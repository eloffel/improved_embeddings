speech and language processing

an introduction to natural language processing,
computational linguistics, and id103

third edition draft

daniel jurafsky
stanford university

james h. martin

university of colorado at boulder

copyright c(cid:13)2018

draft of september 23, 2018. comments and typos welcome!

summary of contents

introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
9
2 id157, text id172, id153 . . . . . . . . . 10
3 id165 language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4 naive bayes and sentiment classi   cation . . . . . . . . . . . . . . . . . . . . . . . 63
5 id28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6 vector semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7 neural networks and neural language models . . . . . . . . . . . . . . . . . 131
8 part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
9 sequence processing with recurrent networks. . . . . . . . . . . . . . . . . . 177
10 formal grammars of english . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
11 syntactic parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
12 statistical parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
13 id33 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
14 the representation of sentence meaning . . . . . . . . . . . . . . . . . . . . . . . 295
15 computational semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
16 id29 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326
17 information extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
18 id14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
19 lexicons for sentiment, affect, and connotation . . . . . . . . . . . . . . . . 378
20 coreference resolution and entity linking . . . . . . . . . . . . . . . . . . . . . 399
21 discourse coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
22 machine translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
23 id53 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
24 id71 and chatbots. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
25 advanced id71 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
26 id103 and synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
463
a id48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
b id147 and the noisy channel . . . . . . . . . . . . . . . . . . . . . 480
c id138: word relations, senses, and disambiguation . . . . . . . . . 493
bibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517
author index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
subject index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551

appendix

2

contents

1 introduction

2 id157, text id172, id153

.

.
.

.
.

. . . .
. . . .

id157 . . . . . . . . . . . . . . . . . . . . . . . . .
2.1
. . . . . . . . . . . . . . . . . . . . . . . . .
2.2 words
corpora
. . . . . . . . . . . . . . . . . . . . . . . . .
2.3
2.4
text id172 . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 minimum id153 . . . . . . . . . . . . . . . . . . . . . . .
2.6
. . . . . . . . . . . . . . . . . . . . . . . . .
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . .
exercises
. . . . . . . . . . . . . . . . . . . . . . .

summary .

. . . .

. . . .

. . .

.

.

.

.

.

3 id165 language models
. . . .

.

. . . .

id165s .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.1
evaluating language models . . . . . . . . . . . . . . . . . . . .
3.2
generalization and zeros
. . . . . . . . . . . . . . . . . . . . . .
3.3
smoothing .
. . . . . . . . . . . . . . . . . . . . . . . . .
3.4
kneser-ney smoothing . . . . . . . . . . . . . . . . . . . . . . .
3.5
the web and stupid backoff
. . . . . . . . . . . . . . . . . . . .
3.6
advanced: perplexity   s relation to id178 . . . . . . . . . . . .
3.7
3.8
. . . . . . . . . . . . . . . . . . . . . . . . .
summary .
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . .
exercises
. . . . . . . . . . . . . . . . . . . . . . .

. . . .

. . . .

. . .

.

.

.

.

.

4 naive bayes and sentiment classi   cation

naive bayes classi   ers . . . . . . . . . . . . . . . . . . . . . . .
4.1
training the naive bayes classi   er . . . . . . . . . . . . . . . . .
4.2
. . . . . . . . . . . . . . . . . . . . . . . . .
4.3 worked example . .
optimizing for id31 . . . . . . . . . . . . . . . . .
4.4
naive bayes for other text classi   cation tasks
. . . . . . . . . . .
4.5
naive bayes as a language model
. . . . . . . . . . . . . . . . .
4.6
evaluation: precision, recall, f-measure . . . . . . . . . . . . . .
4.7
test sets and cross-validation . . . . . . . . . . . . . . . . . . . .
4.8
4.9
statistical signi   cance testing . . . . . . . . . . . . . . . . . . .
4.10 advanced: feature selection . . . . . . . . . . . . . . . . . . . .
4.11 summary .
. . . . . . . . . . . . . . . . . . . . . . . . .
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . .
exercises
. . . . . . . . . . . . . . . . . . . . . . .

. . . .

. . . .

. . .

.

.

.

.

.

5 id28

classi   cation: the sigmoid . . . . . . . . . . . . . . . . . . . . .
5.1
learning in id28 . . . . . . . . . . . . . . . . . . .
5.2
the cross-id178 id168 . . . . . . . . . . . . . . . . . . .
5.3
. . . . . . . . . . . . . . . . . . . . . . . . .
id119
.
5.4
5.5
id173 . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
5.6 multinomial id28 . . . . . . . . . . . . . . . . . . .
interpreting models
5.7
. . . . . . . . . . . . . . . . . . . . . . . . .
advanced: deriving the gradient equation . . . . . . . . . . . . .
5.8
5.9
summary .
. . . . . . . . . . . . . . . . . . . . . . . . .

. . . .

.

3

9

10
11
19
21
22
30
34
34
35

37
38
43
45
49
53
55
56
60
60
61

63
65
67
68
70
71
72
73
76
77
79
79
80
81

82
83
87
88
89
93
95
97
98
99

4 contents

bibliographical and historical notes . . . . . . . . . . . . . . . . . . . .
exercises

99
. . . . . . . . . . . . . . . . . . . . . . . . . 100

. . .

.

.

.

.

.

.

6 vector semantics

lexical semantics . .
vector semantics
. .

101
. . . . . . . . . . . . . . . . . . . . . . . . 102
6.1
. . . . . . . . . . . . . . . . . . . . . . . . 106
6.2
. . . . . . . . . . . . . . . . . . . . . . . 108
6.3 words and vectors . . .
cosine for measuring similarity . . . . . . . . . . . . . . . . . . . 111
6.4
. . . . . . . . . . . . . . . 112
tf-idf: weighing terms in the vector
6.5
applications of the tf-idf vector model
. . . . . . . . . . . . . . . 115
6.6
optional: pointwise mutual information (pmi) . . . . . . . . . . . 116
6.7
. . . . . . . . . . . . . . . . . . . . . . . . 118
6.8 id97 .
6.9
visualizing embeddings . . . . . . . . . . . . . . . . . . . . . . . 123
6.10 semantic properties of embeddings . . . . . . . . . . . . . . . . . 123
6.11 bias and embeddings . . . . . . . . . . . . . . . . . . . . . . . . 125
6.12 evaluating vector models . . . . . . . . . . . . . . . . . . . . . . 126
6.13 summary .
. . . . . . . . . . . . . . . . . . . . . . . . 127
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 127
exercises
. . . . . . . . . . . . . . . . . . . . . . . . . 130

. . .

. . .

. . .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . .

7 neural networks and neural language models

131
. . . . . . . . . . . . . . . . . . . . . . . . . 132
units .
7.1
. . . . . . . . . . . . . . . . . . . . . . . . 134
the xor problem . .
7.2
feed-forward neural networks . . . . . . . . . . . . . . . . . . . 137
7.3
. . . . . . . . . . . . . . . . . . . . . . . . 140
training neural nets
7.4
. . . . . . . . . . . . . . . . . . . . . . 145
neural language models
7.5
7.6
summary .
. . . . . . . . . . . . . . . . . . . . . . . . 149
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 149

. . .

.

.

.

8 part-of-speech tagging

151
(mostly) english word classes . . . . . . . . . . . . . . . . . . . 151
8.1
the id32 part-of-speech tagset
. . . . . . . . . . . . . 154
8.2
part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . . . 156
8.3
8.4
id48 part-of-speech tagging . . . . . . . . . . . . . . . . . . . 157
8.5 maximum id178 markov models . . . . . . . . . . . . . . . . . 167
8.6
. . . . . . . . . . . . . . . . . . . . . . . . . 171
bidirectionality . .
part-of-speech tagging for other languages . . . . . . . . . . . . 172
8.7
8.8
summary .
. . . . . . . . . . . . . . . . . . . . . . . . 173
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 174
exercises
. . . . . . . . . . . . . . . . . . . . . . . . . 175

. . .

. . .

.

.

.

.

.

.

.

.

.

9 sequence processing with recurrent networks

177
9.1
simple recurrent networks . . . . . . . . . . . . . . . . . . . . . 177
9.2
applications of id56s . . . . . . . . . . . . . . . . . . . . . . . . 184
9.3
deep networks: stacked and bidirectional id56s
. . . . . . . . . 186
9.4 managing context in id56s: lstms and grus . . . . . . . . . . 188
9.5 words, characters and byte-pairs . . . . . . . . . . . . . . . . . . 192
9.6
. . . . . . . . . . . . . . . . . . . . . . . . 193

summary .

. . .

.

.

.

10 formal grammars of english

194
10.1 constituency .
. . . . . . . . . . . . . . . . . . . . . . . . . 194
10.2 context-free grammars . . . . . . . . . . . . . . . . . . . . . . . 195

. . .

contents

5

. . .

10.3 some grammar rules for english . . . . . . . . . . . . . . . . . . 200
10.4 treebanks
. . . . . . . . . . . . . . . . . . . . . . . . . . . 207
10.5 grammar equivalence and normal form . . . . . . . . . . . . . . 213
10.6 lexicalized grammars . . . . . . . . . . . . . . . . . . . . . . . . 214
10.7 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 220
. . . . . . . . . . . . . . . . . . . . . . . . 221
exercises

. . . . .

. .

.

.

.

.

.

11 syntactic parsing

223
11.1 ambiguity . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
11.2 cky parsing: a id145 approach . . . . . . . . 225
. . . . . . . . . . . . . . . . . . . . . . . . 231
11.3 partial parsing . . . .
11.4 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 235
exercises
. . . . . . . . . . . . . . . . . . . . . . . . 236

. . . . .

. .

.

.

.

.

.

12 statistical parsing

237
12.1 id140 . . . . . . . . . . . . . . . . 238
12.2 probabilistic cky parsing of pid18s . . . . . . . . . . . . . . . . 242
12.3 ways to learn pid18 rule probabilities
. . . . . . . . . . . . . . 243
12.4 problems with pid18s . . . . . . . . . . . . . . . . . . . . . . . . 245
12.5
improving pid18s by splitting non-terminals . . . . . . . . . . . 248
12.6 probabilistic lexicalized id18s . . . . . . . . . . . . . . . . . . . 250
12.7 probabilistic id35 parsing . . . . . . . . . . . . . . . . . . . . . . 255
12.8 evaluating parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 263
12.9 human parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
12.10 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 267
. . . . . . . . . . . . . . . . . . . . . . . . 268
exercises

. . . . .

. .

.

.

.

.

.

13 id33

270
13.1 dependency relations . . . . . . . . . . . . . . . . . . . . . . . . 271
13.2 dependency formalisms . . . . . . . . . . . . . . . . . . . . . . . 273
13.3 dependency treebanks
. . . . . . . . . . . . . . . . . . . . . . . 274
13.4 transition-based id33 . . . . . . . . . . . . . . . 275
13.5 graph-based id33 . . . . . . . . . . . . . . . . . 286
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
13.6 evaluation . .
13.7 summary .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 292
exercises
. . . . . . . . . . . . . . . . . . . . . . . . 294

. . . . .

.

.

.

.

.

14 the representation of sentence meaning

295
14.1 computational desiderata for representations . . . . . . . . . . . 297
14.2 model-theoretic semantics . . . . . . . . . . . . . . . . . . . . . 301
14.3 id85 . .
. . . . . . . . . . . . . . . . . . . . . . . . 304
14.4 event and state representations . . . . . . . . . . . . . . . . . . . 311
14.5 description logics . . . . . . . . . . . . . . . . . . . . . . . . . . 316
14.6 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 322
. . . . . . . . . . . . . . . . . . . . . . . . 323
exercises

. . . . .

. .

.

.

.

.

.

15 computational semantics

325

6 contents

16 id29

326

17 information extraction

327
17.1 id39 . . . . . . . . . . . . . . . . . . . . . 328
. . . . . . . . . . . . . . . . . . . . . . . 334
17.2 id36 . .
17.3 extracting times . .
. . . . . . . . . . . . . . . . . . . . . . . . . 344
17.4 extracting events and their times . . . . . . . . . . . . . . . . . . 348
. . . . . . . . . . . . . . . . . . . . . . . . . 351
17.5 template filling . .
17.6 summary .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . 353
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 354
exercises
. . . . . . . . . . . . . . . . . . . . . . . 355

. . .

. .

.

.

.

.

.

.

.

.

. . .

18 id14
.

356
18.1 semantic roles
. . . . . . . . . . . . . . . . . . . . . . . 357
18.2 diathesis alternations . . . . . . . . . . . . . . . . . . . . . . . . 358
18.3 semantic roles: problems with thematic roles . . . . . . . . . . 359
18.4 the proposition bank . . . . . . . . . . . . . . . . . . . . . . . . 360
18.5 framenet
. . . . . . . . . . . . . . . . . . . . . . . . . 362
18.6 id14 . . . . . . . . . . . . . . . . . . . . . . . 364
18.7 selectional restrictions . . . . . . . . . . . . . . . . . . . . . . . 368
18.8 primitive decomposition of predicates . . . . . . . . . . . . . . . 372
18.9 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . 374
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 374
exercises
. . . . . . . . . . . . . . . . . . . . . . . 377

. . .

. . .

. . .

. .

.

.

.

.

.

.

.

.

.

.

19 lexicons for sentiment, affect, and connotation

378
19.1 de   ning emotion .
. . . . . . . . . . . . . . . . . . . . . . . . . 379
19.2 available sentiment and affect lexicons . . . . . . . . . . . . . . 381
19.3 creating affect lexicons by human labeling . . . . . . . . . . . . . 382
19.4 semi-supervised induction of affect lexicons . . . . . . . . . . . . 384
19.5 supervised learning of word sentiment
. . . . . . . . . . . . . . . 387
19.6 using lexicons for sentiment recognition . . . . . . . . . . . . . 391
19.7 other tasks: personality . . . . . . . . . . . . . . . . . . . . . . . 392
19.8 affect recognition .
. . . . . . . . . . . . . . . . . . . . . . . . . 393
19.9 connotation frames . . . . . . . . . . . . . . . . . . . . . . . . . 395
19.10 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . 396
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 397

. . .

.

.

20 coreference resolution and entity linking

21 discourse coherence

22 machine translation

399

400

401

23 id53

402
23.1
ir-based factoid id53 . . . . . . . . . . . . . . . 403
23.2 knowledge-based id53 . . . . . . . . . . . . . . . 411
23.3 using multiple information sources: ibm   s watson . . . . . . . . 415
23.4 evaluation of factoid answers
. . . . . . . . . . . . . . . . . . . 418
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 420
. . . . . . . . . . . . . . . . . . . . . . . 421
exercises

. . .

. .

.

.

.

.

.

.

24 id71 and chatbots

422

contents

7

.

. . . . .

24.1 chatbots .
. . . . . . . . . . . . . . . . . . . . . . . . 425
24.2 frame based dialog agents . . . . . . . . . . . . . . . . . . . . . 430
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
24.3 voicexml . .
24.4 evaluating id71
. . . . . . . . . . . . . . . . . . . . . 441
24.5 dialog system design . . . . . . . . . . . . . . . . . . . . . . . . 442
24.6 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 444
exercises
. . . . . . . . . . . . . . . . . . . . . . . . 445

. . . . .

. .

.

.

.

.

.

25 advanced id71

.

446
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
25.1 dialog acts
25.2 dialog state: interpreting dialog acts
. . . . . . . . . . . . . . . 452
25.3 dialog policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
25.4 a simple policy based on local context
. . . . . . . . . . . . . . . 456
25.5 id86 in the dialog-state model
. . . . . . . 456
25.6 deep id23 for dialog . . . . . . . . . . . . . . 459
25.7 summary .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 459

. .

26 id103 and synthesis

461

appendix
a id48

463
464
a.1 markov chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
the hidden markov model
a.2
. . . . . . . . . . . . . . . . . . . . . 465
a.3
likelihood computation: the forward algorithm . . . . . . . . . 467
a.4 decoding: the viterbi algorithm . . . . . . . . . . . . . . . . . . 471
a.5 id48 training: the forward-backward algorithm . . . . . . . . 473
a.6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 479

summary .

. .

b id147 and the noisy channel

480
the id87 . . . . . . . . . . . . . . . . . . . . . . 481
b.1
b.2
real-word spelling errors . . . . . . . . . . . . . . . . . . . . . . 486
b.3 id87: the state of the art . . . . . . . . . . . . . 488
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 491
exercises
. . . . . . . . . . . . . . . . . . . . . . . . 492

. . . . .

.

.

.

.

.

c id138: word relations, senses, and disambiguation

493
c.1 word senses .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
c.2 id138: a database of lexical relations . . . . . . . . . . . . . 497
c.3 word similarity: thesaurus methods . . . . . . . . . . . . . . . . 498
c.4 id51: overview . . . . . . . . . . . . . . . 504
supervised id51 . . . . . . . . . . . . . . 505
c.5
c.6 wsd: dictionary and thesaurus methods
. . . . . . . . . . . . . 507
c.7
semi-supervised wsd: id64 . . . . . . . . . . . . . . . 510
c.8 unsupervised word sense induction . . . . . . . . . . . . . . . . 511
c.9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
bibliographical and historical notes . . . . . . . . . . . . . . . . . . . . 513
exercises
. . . . . . . . . . . . . . . . . . . . . . . . 514

summary .

. . . . .

. .

.

.

.

.

.

8 contents

bibliography

author index

subject index

517

543

551

chapter

1 introduction

placeholder

9

10 chapter 2

    id157, text id172, id153

chapter

2 id157, text

id172, id153

you are like my father in some ways.

user:
eliza: what resemblance do you see
user:

you are not very aggressive but i think you don   t want me to
notice that.

eliza

chatbots

text
id172

id121

you don   t argue with me.

eliza: what makes you think i am not very aggressive
user:
eliza: why do you think i don   t argue with you
user:
eliza: does it please you to believe i am afraid of you

you are afraid of me.

weizenbaum (1966)
the dialogue above is from eliza, an early natural language processing sys-
tem that could carry on a limited conversation with a user by imitating the responses
of a rogerian psychotherapist (weizenbaum, 1966). eliza is a surprisingly simple
program that uses pattern matching to recognize phrases like    you are x    and trans-
late them into suitable outputs like    what makes you think i am x?   . this simple
technique succeeds in this domain because eliza doesn   t actually need to know
anything to mimic a rogerian psychotherapist. as weizenbaum notes, this is one
of the few dialogue genres where listeners can act as if they know nothing of the
world. eliza   s mimicry of human conversation was remarkably successful: many
people who interacted with eliza came to believe that it really understood them
and their problems, many continued to believe in eliza   s abilities even after the
program   s operation was explained to them (weizenbaum, 1976), and even today
such chatbots are a fun diversion.

of course modern conversational agents are much more than a diversion; they
can answer questions, book    ights, or    nd restaurants, functions for which they rely
on a much more sophisticated understanding of the user   s intent, as we will see in
chapter 24. nonetheless, the simple pattern-based methods that powered eliza
and other chatbots play a crucial role in natural language processing.

we   ll begin with the most important tool for describing text patterns: the regular
expression. id157 can be used to specify strings we might want to
extract from a document, from transforming    you are x    in eliza above, to de   ning
strings like $199 or $24.99 for extracting tables of prices from a document.

we   ll then turn to a set of tasks collectively called text id172, in which
id157 play an important part. normalizing text means converting it
to a more convenient, standard form. for example, most of what we are going to
do with language relies on    rst separating out or tokenizing words from running
text, the task of id121. english words are often separated from each other
by whitespace, but whitespace is not always suf   cient. new york and rock    n    roll
are sometimes treated as large words despite the fact that they contain spaces, while
sometimes we   ll need to separate i   m into the two words i and am. for processing
tweets or texts we   ll need to tokenize emoticons like :) or hashtags like #nlproc.
some languages, like chinese, don   t have spaces between words, so word tokeniza-
tion becomes more dif   cult.

2.1

    id157

11

lemmatization

id30

sentence
segmentation

another part of text id172 is lemmatization, the task of determining
that two words have the same root, despite their surface differences. for example,
the words sang, sung, and sings are forms of the verb sing. the word sing is the
common lemma of these words, and a lemmatizer maps from all of these to sing.
lemmatization is essential for processing morphologically complex languages like
arabic. id30 refers to a simpler version of lemmatization in which we mainly
just strip suf   xes from the end of the word. text id172 also includes sen-
tence segmentation: breaking up a text into individual sentences, using cues like
periods or exclamation points.

finally, we   ll need to compare words and other strings. we   ll introduce a metric
called id153 that measures how similar two strings are based on the number
of edits (insertions, deletions, substitutions) it takes to change one string into the
other. id153 is an algorithm with applications throughout language process-
ing, from id147 to id103 to coreference resolution.

2.1 id157

regular
expression

corpus

sir andrew: her c   s, her u   s and her t   s: why that?
shakespeare, twelfth night
one of the unsung successes in standardization in computer science has been the
regular expression (re), a language for specifying text search strings. this prac-
tical language is used in every computer language, word processor, and text pro-
cessing tools like the unix tools grep or emacs. formally, a regular expression is
an algebraic notation for characterizing a set of strings. they are particularly use-
ful for searching in texts, when we have a pattern to search for and a corpus of
texts to search through. a regular expression search function will search through the
corpus, returning all texts that match the pattern. the corpus can be a single docu-
ment or a collection. for example, the unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
a search can be designed to return every match on a line, if there are more than
one, or just the    rst match. in the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the    rst
match. we   ll show id157 delimited by slashes but note that slashes are
not part of the id157.

id157 come in many variants. we   ll be describing extended regu-
lar expressions; different regular expression parsers may only recognize subsets of
these, or treat some expressions slightly differently. using an online regular expres-
sion tester is a handy way to test out your expressions and explore these variations.

2.1.1 basic regular expression patterns
the simplest kind of regular expression is a sequence of simple characters. to search
for woodchuck, we type /woodchuck/. the expression /buttercup/ matches any
string containing the substring buttercup; grep with that expression would return the
line i   m called little buttercup. the search string can consist of a single character
(like /!/) or a sequence of characters (like /urgl/).

id157 are case sensitive; lower case /s/ is distinct from upper
case /s/ (/s/ matches a lower case s but not an upper case s). this means that
the pattern /woodchucks/ will not match the string woodchucks. we can solve this

12 chapter 2

    id157, text id172, id153

re
/woodchucks/
/a/
/!/
figure 2.1 some simple regex searches.

example patterns matched
   interesting links to woodchucks and lemurs   
   mary ann stopped by mona   s   
   you   ve left the burglar behind again!    said nori

problem with the use of the square braces [ and ]. the string of characters inside the
braces speci   es a disjunction of characters to match. for example, fig. 2.2 shows
that the pattern /[ww]/ matches patterns containing either w or w.

match

re
/[ww]oodchuck/ woodchuck or woodchuck
/[abc]/
/[1234567890]/

   a   ,    b   , or    c   
any digit

example patterns
   woodchuck   
   in uomini, in soldati   
   plenty of 7 to 5   

figure 2.2 the use of the brackets [] to specify a disjunction of characters.

the regular expression /[1234567890]/ speci   ed any single digit. while such
classes of characters as digits or letters are important building blocks in expressions,
they can get awkward (e.g., it   s inconvenient to specify

/[abcdefghijklmnopqrstuvwxyz]/

range

to mean    any capital letter   ). in cases where there is a well-de   ned sequence asso-
ciated with a set of characters, the brackets can be used with the dash (-) to specify
any one character in a range. the pattern /[2-5]/ speci   es any one of the charac-
ters 2, 3, 4, or 5. the pattern /[b-g]/ speci   es one of the characters b, c, d, e, f, or
g. some other examples are shown in fig. 2.3.

re
/[a-z]/
/[a-z]/
/[0-9]/

match
an upper case letter
a lower case letter
a single digit

example patterns matched
   we should call it    drenched blossoms       
   my beans were impatient to be hoed!   
   chapter 1: down the rabbit hole   

figure 2.3 the use of the brackets [] plus the dash - to specify a range.

the square braces can also be used to specify what a single character cannot be,
by use of the caret   . if the caret    is the    rst symbol after the open square brace [,
the resulting pattern is negated. for example, the pattern /[  a]/ matches any single
character (including special characters) except a. this is only true when the caret
is the    rst symbol after the open square brace. if it occurs anywhere else, it usually
stands for a caret; fig. 2.4 shows some examples.

re
/[  a-z]/
/[  ss]/
/[  \.]/
/[e  ]/
/a  b/

match (single characters)
not an upper case letter
neither    s    nor    s   
not a period
either    e    or         
the pattern    a  b   

example patterns matched
   oyfn pripetchik   
   i have no exquisite reason for   t   
   our resident djinn   
   look up    now   
   look up a   b now   

figure 2.4 the caret    for negation or just to mean   . see below re: the backslash for escaping the period.

how can we talk about optional elements, like an optional s in woodchuck and
woodchucks? we can   t use the square brackets, because while they allow us to say
   s or s   , they don   t allow us to say    s or nothing   . for this we use the question mark
/?/, which means    the preceding character or nothing   , as shown in fig. 2.5.

    id157
example patterns matched
   woodchuck   
   colour   
figure 2.5 the question mark ? marks optionality of the previous expression.

match
woodchuck or woodchucks
color or colour

re
/woodchucks?/
/colou?r/

2.1

13

we can think of the question mark as meaning    zero or one instances of the
previous character   . that is, it   s a way of specifying how many of something that
we want, something that is very important in id157. for example,
consider the language of certain sheep, which consists of strings that look like the
following:

baa!
baaa!
baaaa!
baaaaa!
. . .

kleene *

kleene +

this language consists of strings with a b, followed by at least two a   s, followed
by an exclamation point. the set of operators that allows us to say things like    some
number of as    are based on the asterisk or *, commonly called the kleene * (gen-
erally pronounced    cleany star   ). the kleene star means    zero or more occurrences
of the immediately previous character or regular expression   . so /a*/ means    any
string of zero or more as   . this will match a or aaaaaa, but it will also match off
minor since the string off minor has zero a   s. so the regular expression for matching
one or more a is /aa*/, meaning one a followed by zero or more as. more complex
patterns can also be repeated. so /[ab]*/ means    zero or more a   s or b   s    (not
   zero or more right square braces   ). this will match strings like aaaa or ababab or
bbbb.

for specifying multiple digits (useful for    nding prices) we can extend /[0-9]/,
the regular expression for a single digit. an integer (a string of digits) is thus
/[0-9][0-9]*/. (why isn   t it just /[0-9]*/?)

sometimes it   s annoying to have to write the regular expression for digits twice,
so there is a shorter way to specify    at least one    of some character. this is the
kleene +, which means    one or more occurrences of the immediately preceding
character or regular expression   . thus, the expression /[0-9]+/ is the normal way
to specify    a sequence of digits   . there are thus two ways to specify the sheep
language: /baaa*!/ or /baa+!/.

one very important special character is the period (/./), a wildcard expression

that matches any single character (except a carriage return), as shown in fig. 2.6.

re
/beg.n/

match
any character between beg and n

example matches
begin, beg   n, begun

figure 2.6 the use of the period . to specify any character.

the wildcard is often used together with the kleene star to mean    any string of
characters   . for example, suppose we want to    nd any line in which a particular
word, for example, aardvark, appears twice. we can specify this with the regular
expression /aardvark.*aardvark/.

anchors are special characters that anchor id157 to particular places
in a string. the most common anchors are the caret    and the dollar sign $. the caret
   matches the start of a line. the pattern /  the/ matches the word the only at the

anchors

14 chapter 2

    id157, text id172, id153

start of a line. thus, the caret    has three uses: to match the start of a line, to in-
dicate a negation inside of square brackets, and just to mean a caret. (what are the
contexts that allow grep or python to know which function a given caret is supposed
to have?) the dollar sign $ matches the end of a line. so the pattern (cid:32)$ is a useful
pattern for matching a space at the end of a line, and /  the dog\.$/ matches a
line that contains only the phrase the dog. (we have to use the backslash here since
we want the . to mean    period    and not the wildcard.)

there are also two other anchors: \b matches a word boundary, and \b matches
a non-boundary. thus, /\bthe\b/ matches the word the but not the word other.
more technically, a    word    for the purposes of a regular expression is de   ned as any
sequence of digits, underscores, or letters; this is based on the de   nition of    words   
in programming languages. for example, /\b99\b/ will match the string 99 in
there are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in
there are 299 bottles of beer on the wall (since 99 follows a number). but it will
match 99 in $99 (since 99 follows a dollar sign ($), which is not a digit, underscore,
or letter).

2.1.2 disjunction, grouping, and precedence

suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. in such a case, we might want to search for either the string cat or
the string dog. since we can   t use the square brackets to search for    cat or dog    (why
can   t we say /[catdog]/?), we need a new operator, the disjunction operator, also
called the pipe symbol |. the pattern /cat|dog/ matches either the string cat or
the string dog.

sometimes we need to use this disjunction operator in the midst of a larger se-
quence. for example, suppose i want to search for information about pet    sh for
my cousin david. how can i specify both guppy and guppies? we cannot simply
say /guppy|ies/, because that would match only the strings guppy and ies. this
is because sequences like guppy take precedence over the disjunction operator |.
to make the disjunction operator apply only to a speci   c pattern, we need to use the
parenthesis operators ( and ). enclosing a pattern in parentheses makes it act like
a single character for the purposes of neighboring operators like the pipe | and the
kleene*. so the pattern /gupp(y|ies)/ would specify that we meant the disjunc-
tion only to apply to the suf   xes y and ies.

the parenthesis operator ( is also useful when we are using counters like the
kleene*. unlike the | operator, the kleene* operator applies by default only to
a single character, not to a whole sequence. suppose we want to match repeated
instances of a string. perhaps we have a line that has column labels of the form
column 1 column 2 column 3. the expression /column(cid:32)[0-9]+(cid:32)*/ will not
match any number of columns; instead, it will match a single column followed by
any number of spaces! the star here applies only to the space (cid:32) that precedes it,
not to the whole sequence. with the parentheses, we could write the expression
/(column(cid:32)[0-9]+(cid:32)*)*/ to match the word column, followed by a number and
optional spaces, the whole pattern repeated any number of times.

this idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for id157. the following table gives the order
of re operator precedence, from highest precedence to lowest precedence.

disjunction

precedence

operator
precedence

2.1

    id157

15

parenthesis
counters
sequences and anchors
disjunction

()
* + ? {}

the   my end$

|

thus,

because

than sequences,
/the*/ matches theeeee but not thethe. because sequences have a higher prece-
dence than disjunction, /the|any/ matches the or any but not theny.

a higher precedence

counters have

patterns can be ambiguous in another way. consider the expression /[a-z]*/
when matching against the text once upon a time. since /[a-z]*/ matches zero or
more letters, this expression could match nothing, or just the    rst letter o, on, onc,
or once. in these cases id157 always match the largest string they can;
we say that patterns are greedy, expanding to cover as much of a string as they can.
there are, however, ways to enforce non-greedy matching, using another mean-
ing of the ? quali   er. the operator *? is a kleene star that matches as little text as
possible. the operator +? is a kleene plus that matches as little text as possible.

greedy
non-greedy
*?
+?

2.1.3 a simple example
suppose we wanted to write a re to    nd cases of the english article the. a simple
(but incorrect) pattern might be:

/the/

one problem is that this pattern will miss the word when it begins a sentence

and hence is capitalized (i.e., the). this might lead us to the following pattern:

/[tt]he/

but we will still incorrectly return texts with the embedded in other words (e.g.,
other or theology). so we need to specify that we want instances with a word bound-
ary on both sides:

/\b[tt]he\b/

suppose we wanted to do this without the use of /\b/. we might want this since
/\b/ won   t treat underscores and numbers as word boundaries; but we might want
to    nd the in some context where it might also have underlines or numbers nearby
(the or the25). we need to specify that we want instances in which there are no
alphabetic letters on either side of the the:

/[  a-za-z][tt]he[  a-za-z]/

but there is still one more problem with this pattern: it won   t    nd the word the
when it begins a line. this is because the regular expression [  a-za-z], which
we used to avoid embedded instances of the, implies that there must be some single
(although non-alphabetic) character before the the. we can avoid this by specify-
ing that before the the we require either the beginning-of-line or a non-alphabetic
character, and the same at the end of the line:

/(  |[  a-za-z])[tt]he([  a-za-z]|$)/

false positives
false negatives

the process we just went through was based on    xing two kinds of errors: false
positives, strings that we incorrectly matched like other or there, and false nega-
tives, strings that we incorrectly missed, like the. addressing these two kinds of

16 chapter 2

    id157, text id172, id153

errors comes up again and again in implementing speech and language processing
systems. reducing the overall error rate for an application thus involves two antag-
onistic efforts:

    increasing precision (minimizing false positives)
    increasing recall (minimizing false negatives)

2.1.4 a more complex example
let   s try out a more signi   cant example of the power of res. suppose we want to
build an application to help a user buy a computer on the web. the user might want
   any machine with at least 6 ghz and 500 gb of disk space for less than $1000   .
to do this kind of retrieval, we    rst need to be able to look for expressions like 6
ghz or 500 gb or mac or $999.99. in the rest of this section we   ll work out some
simple id157 for this task.

first, let   s complete our regular expression for prices. here   s a regular expres-

sion for a dollar sign followed by a string of digits:

/$[0-9]+/

note that the $ character has a different function here than the end-of-line function
we discussed earlier. most regular expression parsers are smart enough to realize
that $ here doesn   t mean end-of-line. (as a thought experiment, think about how
regex parsers might    gure out the function of $ from the context.)

now we just need to deal with fractions of dollars. we   ll add a decimal point

and two digits afterwards:

/$[0-9]+\.[0-9][0-9]/

this pattern only allows $199.99 but not $199. we need to make the cents

optional and to make sure we   re at a word boundary:

/(  |\w)$[0-9]+(\.[0-9][0-9])?\b/

one last catch! this pattern allows prices like $199999.99 which would be far

too expensive! we need to limit the dollar

/(  |\w)$[0-9]{0,3}(\.[0-9][0-9])?\b/

how about speci   cations for > 6ghz processor speed? here   s a pattern for that:

/\b[6-9]+(cid:32)*(ghz|[gg]igahertz)\b/

note that we use /(cid:32)*/ to mean    zero or more spaces    since there might always
be extra spaces lying around. for disk space, we   ll need to allow for optional frac-
tions again (5.5 gb); note the use of ? for making the    nal s optional:

/\b[0-9]+(\.[0-9]+)?(cid:32)*(gb|[gg]igabytes?)\b/

modifying this regular expression so that it only matches more than 500 gb is

left as an exercise for the reader.

2.1.5 more operators
figure 2.7 shows some aliases for common ranges, which can be used mainly to
save typing. besides the kleene * and kleene + we can also use explicit numbers as

2.1

    id157

17

counters, by enclosing them in curly brackets. the regular expression /{3}/ means
   exactly 3 occurrences of the previous character or expression   . so /a\.{24}z/
will match a followed by 24 dots followed by z (but not a followed by 23 or 25 dots
followed by a z).

re
\d
\d
\w
\w
\s
\s

match
any digit
any non-digit
any alphanumeric/underscore
a non-alphanumeric
whitespace (space, tab)
non-whitespace
figure 2.7 aliases for common sets of characters.

expansion
[0-9]
[  0-9]
[a-za-z0-9_]
[  \w]
[(cid:32)\r\t\n\f]
[  \s]

first matches
party(cid:32)of(cid:32)5
blue(cid:32)moon
daiyu
!!!!

in(cid:32)concord

a range of numbers can also be speci   ed. so /{n,m}/ speci   es from n to m
occurrences of the previous char or expression, and /{n,}/ means at least n occur-
rences of the previous expression. res for counting are summarized in fig. 2.8.

re
*
+
?
{n}
{n,m}
{n,}
{,m}

match
zero or more occurrences of the previous char or expression
one or more occurrences of the previous char or expression
exactly zero or one occurrence of the previous char or expression
n occurrences of the previous char or expression
from n to m occurrences of the previous char or expression
at least n occurrences of the previous char or expression
up to m occurrences of the previous char or expression

figure 2.8 regular expression operators for counting.

newline

finally, certain special characters are referred to by special notation based on the
backslash (\) (see fig. 2.9). the most common of these are the newline character
\n and the tab character \t. to refer to characters that are special themselves (like
., *, [, and \), precede them with a backslash, (i.e., /\./, /\*/, /\[/, and /\\/).

re
\*
\.
\?
\n
\t

match
an asterisk    *   
a period    .   
a question mark
a newline
a tab

first patterns matched
   k*a*p*l*a*n   
   dr. livingston, i presume   
   why don   t they come and lend a hand?   

figure 2.9 some characters that need to be backslashed.

substitution

2.1.6 regular expression substitution, capture groups, and eliza
an important use of id157 is in substitutions. for example, the substi-
tution operator s/regexp1/pattern/ used in python and in unix commands like
vim or sed allows a string characterized by a regular expression to be replaced by
another string:
s/colour/color/

it is often useful to be able to refer to a particular subpart of the string matching
the    rst pattern. for example, suppose we wanted to put angle brackets around all

18 chapter 2

    id157, text id172, id153

integers in a text, for example, changing the 35 boxes to the <35> boxes. we   d
like a way to refer to the integer we   ve found so that we can easily add the brackets.
to do this, we put parentheses ( and ) around the    rst pattern and use the number
operator \1 in the second pattern to refer back. here   s how it looks:

s/([0-9]+)/<\1>/

the parenthesis and number operators can also specify that a certain string or
expression must occur twice in the text. for example, suppose we are looking for
the pattern    the xer they were, the xer they will be   , where we want to constrain
the two x   s to be the same string. we do this by surrounding the    rst x with the
parenthesis operator, and replacing the second x with the number operator \1, as
follows:

/the (.*)er they were, the \1er they will be/

here the \1 will be replaced by whatever string matched the    rst item in paren-
theses. so this will match the bigger they were, the bigger they will be but not the
bigger they were, the faster they will be.

this use of parentheses to store a pattern in memory is called a capture group.
every time a capture group is used (i.e., parentheses surround a pattern), the re-
sulting match is stored in a numbered register. if you match two different sets of
parentheses, \2 means whatever matched the second capture group. thus

/the (.*)er they (.*), the \1er we \2/
will match the faster they ran, the faster we ran but not the faster they ran, the faster
we ate. similarly, the third capture group is stored in \3, the fourth is \4, and so on.
parentheses thus have a double function in id157; they are used to
group terms for specifying the order in which operators should apply, and they are
used to capture something in a register. occasionally we might want to use parenthe-
ses for grouping, but don   t want to capture the resulting pattern in a register. in that
case we use a non-capturing group, which is speci   ed by putting the commands
?: after the open paren, in the form (?: pattern ).

/(?:some|a few) (people|cats) like some \1/
will match some cats like some cats but not some cats like some a few.

substitutions and capture groups are very useful in implementing simple chat-
bots like eliza (weizenbaum, 1966). recall that eliza simulates a rogerian
psychologist by carrying on conversations like the following:

capture group

register

non-capturing
group

they   re always bugging us about something or other.

user1: men are all alike.
eliza1: in what way
user2:
eliza2: can you think of a specific example
user3: well, my boyfriend made me come here.
eliza3: your boyfriend made you come here
user4:
eliza4: i am sorry to hear you are depressed

he says i   m depressed much of the time.

eliza works by having a series or cascade of regular expression substitutions
each of which matches and changes some part of the input lines. input lines are
   rst uppercased. the    rst substitutions then change all instances of my to your,
and i   m to you are, and so on. the next set of substitutions matches and replaces
other patterns in the input. here are some examples:

2.2

    words

19

s/.* i   m (depressed|sad) .*/i am sorry to hear you are \1/
s/.* i am (depressed|sad) .*/why do you think you are \1/
s/.* all .*/in what way/
s/.* always .*/can you think of a specific example/

since multiple substitutions can apply to a given input, substitutions are assigned
a rank and applied in order. creating patterns is the topic of exercise 2.3, and we
return to the details of the eliza architecture in chapter 24.

2.1.7 lookahead assertions
finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not advance the match cursor, so that we can
then deal with the pattern if it occurs.

these lookahead assertions make use of the (? syntax that we saw in the previ-
ous section for non-capture groups. the operator (?= pattern) is true if pattern
occurs, but is zero-width, i.e.
the match pointer doesn   t advance. the operator
(?! pattern) only returns true if a pattern does not match, but again is zero-width
and doesn   t advance the cursor. negative lookahead is commonly used when we
are parsing some complex pattern but want to rule out a special case. for example
suppose we want to match, at the beginning of a line, any single word that doesn   t
start with    volcano   . we can use negative lookahead to do this:

/  (?!volcano)[a-za-z]+/

lookahead

zero-width

2.2 words

corpus
corpora

before we talk about processing words, we need to decide what counts as a word.
let   s start by looking at one particular corpus (plural corpora), a computer-readable
collection of text or speech. for example the brown corpus is a million-word col-
lection of samples from 500 written english texts from different genres (newspa-
per,    ction, non-   ction, academic, etc.), assembled at brown university in 1963   64
(ku  cera and francis, 1967). how many words are in the following brown sentence?

he stepped out into the hall, was delighted to encounter a water brother.

this sentence has 13 words if we don   t count punctuation marks as words, 15
if we count punctuation. whether we treat period (   .   ), comma (   ,   ), and so on as
words depends on the task. punctuation is critical for    nding boundaries of things
(commas, periods, colons) and for identifying some aspects of meaning (question
marks, exclamation marks, quotation marks). for some tasks, like part-of-speech
tagging or parsing or id133, we sometimes treat punctuation marks as if
they were separate words.

the switchboard corpus of american english telephone conversations between
strangers was collected in the early 1990s; it contains 2430 conversations averaging
6 minutes each, totaling 240 hours of speech and about 3 million words (godfrey
et al., 1992). such corpora of spoken language don   t have punctuation but do intro-
duce other complications with regard to de   ning words. let   s look at one utterance
from switchboard; an utterance is the spoken correlate of a sentence:

i do uh main- mainly business data processing

utterance

20 chapter 2

    id157, text id172, id153
this utterance has two kinds of dis   uencies. the broken-off word main- is
called a fragment. words like uh and um are called    llers or    lled pauses. should
we consider these to be words? again, it depends on the application. if we are
building a speech transcription system, we might want to eventually strip out the
dis   uencies.

but we also sometimes keep dis   uencies around. dis   uencies like uh or um
are actually helpful in id103 in predicting the upcoming word, because
they may signal that the speaker is restarting the clause or idea, and so for speech
recognition they are treated as regular words. because people use different dis   u-
encies they can also be a cue to speaker identi   cation. in fact clark and fox tree
(2002) showed that uh and um have different meanings. what do you think they are?
are capitalized tokens like they and uncapitalized tokens like they the same
word? these are lumped together in some tasks (id103), while for part-
of-speech or named-entity tagging, capitalization is a useful feature and is retained.
how about in   ected forms like cats versus cat? these two words have the same
lemma cat but are different wordforms. a lemma is a set of lexical forms having
the same stem, the same major part-of-speech, and the same word sense. the word-
form is the full in   ected or derived form of the word. for morphologically complex
languages like arabic, we often need to deal with lemmatization. for many tasks in
english, however, wordforms are suf   cient.

how many words are there in english? to answer this question we need to
distinguish two ways of talking about words. types are the number of distinct words
in a corpus; if the set of words in the vocabulary is v , the number of types is the
vocabulary size |v|. tokens are the total number n of running words. if we ignore
punctuation, the following brown sentence has 16 tokens and 14 types:

dis   uency
fragment
   lled pause

lemma

wordform

word type

word token

they picnicked by the pool, then lay back on the grass and looked at the stars.

when we speak about the number of words in the language, we are generally

referring to word types.

corpus
shakespeare
brown corpus
switchboard telephone conversations
coca
google id165s

tokens = n types = |v|
884 thousand 31 thousand
1 million 38 thousand
2.4 million 20 thousand
2 million
440 million
1 trillion
13 million

figure 2.10 rough numbers of types and tokens for some english language corpora. the
largest, the google id165s corpus, contains 13 million types, but this count only includes
types appearing 40 or more times, so the true number would be much larger.

fig. 2.10 shows the rough numbers of types and tokens computed from some
popular english corpora. the larger the corpora we look at, the more word types
we    nd, and in fact this relationship between the number of types |v| and number
of tokens n is called herdan   s law (herdan, 1960) or heaps    law (heaps, 1978)
after its discoverers (in linguistics and information retrieval respectively). it is shown
in eq. 2.1, where k and    are positive constants, and 0 <    < 1.

herdan   s law
heaps    law

the value of    depends on the corpus size and the genre, but at least for the
large corpora in fig. 2.10,    ranges from .67 to .75. roughly then we can say that

|v| = kn  

(2.1)

2.3

    corpora

21

the vocabulary size for a text goes up signi   cantly faster than the square root of its
length in words.

another measure of the number of words in the language is the number of lem-
mas instead of wordform types. dictionaries can help in giving lemma counts; dic-
tionary entries or boldface forms are a very rough upper bound on the number of
lemmas (since some lemmas have multiple boldface forms). the 1989 edition of the
oxford english dictionary had 615,000 entries.

2.3 corpora

words don   t appear out of nowhere. any particular piece of text that we study
is produced by one or more speci   c speakers or writers, in a speci   c dialect of a
speci   c language, at a speci   c time, in a speci   c place, for a speci   c function.

perhaps the most important dimension of variation is the language. nlp algo-
rithms are most useful when they apply across many languages. the world has 7097
languages at the time of this writing, according to the online ethnologue catalog
(simons and fennig, 2018). most nlp tools tend to be developed for the of   cial
languages of large industrialized nations (chinese, english, spanish, arabic, etc.),
but we don   t want to limit tools to just these few languages. furthermore, most lan-
guages also have multiple varieties, such as dialects spoken in different regions or
by different social groups. thus, for example, if we   re processing text in african
american vernacular english (aave), a dialect spoken by millions of people in the
united states, it   s important to make use of nlp tools that function with that dialect.
twitter posts written in aave make use of constructions like iont (i don   t in stan-
dard american english (sae)), or talmbout corresponding to sae talking about,
both examples that in   uence id40 (blodgett et al. 2016, jones 2015).
it   s also quite common for speakers or writers to use multiple languages in a
single communicative act, a phenomenon called code switching. code switch-
ing is enormously common across the world; here are examples showing spanish
and (transliterated) hindi code switching with english (solorio et al. 2014, jurgens
et al. 2017):
(2.2) por primera vez veo a @username actually being hateful! it was beautiful:)

[for the    rst time i get to see @username actually being hateful! it was
beautiful:]

(2.3) dost tha or ra- hega ... dont wory ... but dherya rakhe

[   he was and will remain a friend ... don   t worry ... but have faith   ]

aave

sae

code switching

another dimension of variation is the genre. the text that our algorithms must
process might come from newswire,    ction or non-   ction books, scienti   c articles,
wikipedia, or religious texts.
it might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. it might come from work situations
like doctors    notes, legal text, or parliamentary or congressional proceedings.

text also re   ects the demographic characteristics of the writer (or speaker): their
age, gender, race, socio-economic class can all in   uence the linguistic properties of
the text we are processing.

and    nally, time matters too. language changes over time, and for some lan-

guages we have good corpora of texts from different historical periods.

because language is so situated, when developing computational models for lan-

22 chapter 2

    id157, text id172, id153

guage processing, it   s important to consider who produced the language, in what
context, for what purpose, and make sure that the models are    t to the data.

2.4 text id172

before almost any natural language processing of a text, the text has to be normal-
ized. at least three tasks are commonly applied as part of any id172 process:

1. segmenting/tokenizing words from running text
2. normalizing word formats
3. segmenting sentences in running text.
in the next sections we walk through each of these tasks.

2.4.1 unix tools for crude id121 and id172
let   s begin with an easy, if somewhat naive version of word id121 and nor-
malization (and frequency computation) that can be accomplished for english solely
in a single unix command-line, inspired by church (1994). we   ll make use of some
unix commands: tr, used to systematically change particular characters in the in-
put; sort, which sorts input lines in alphabetical order; and uniq, which collapses
and counts adjacent identical lines.

for example let   s begin with the    complete words    of shakespeare in one text   le,
sh.txt. we can use tr to tokenize the words by changing every sequence of non-
alphabetic characters to a newline (   a-za-z    means alphabetic, the -c option com-
plements to non-alphabet, and the -s option squeezes all sequences into a single
character):

tr -sc    a-za-z       \n    < sh.txt

the output of this command will be:

the
sonnets
by
william
shakespeare
from
fairest
creatures
we
...

now that there is one word per line, we can sort the lines, and pass them to uniq

-c which will collapse and count them:

tr -sc    a-za-z       \n    < sh.txt | sort | uniq -c

with the following output:

1945 a
72 aaron
19 abbess
25 aaron

2.4

    text id172

23

6 abate
1 abates
5 abbess
6 abbey
3 abbot
...

alternatively, we can collapse all the upper case to lower case:

tr -sc    a-za-z       \n    < sh.txt | tr a-z a-z | sort | uniq -c
whose output is
14725 a

97 aaron

1 abaissiez

10 abandon

2 abandoned
2 abase
1 abash
14 abate

3 abated
3 abatement
...
now we can sort again to    nd the frequent words. the -n option to sort means
to sort numerically rather than alphabetically, and the -r option means to sort in
reverse order (highest-to-lowest):

tr -sc    a-za-z       \n    < sh.txt | tr a-z a-z | sort | uniq -c | sort -n -r
the results show that the most frequent words in shakespeare, as in any other

corpus, are the short function words like articles, pronouns, prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
12489 my
11318 that
11112 in

...
unix tools of this sort can be very handy in building quick word count statistics

for any corpus.

2.4.2 word id121 and id172
the simple unix tools above were    ne for getting rough word statistics but more
sophisticated algorithms are generally necessary for id121, the task of seg-
menting running text into words, and id172, the task of putting words/to-
kens in a standard format.

while the unix command sequence just removed all the numbers and punctu-
ation, for most nlp applications we   ll need to keep these in our id121. we

id121
id172

24 chapter 2

    id157, text id172, id153

often want to break off punctuation as a separate token; commas are a useful piece of
information for parsers, periods help indicate sentence boundaries. but we   ll often
want to keep the punctuation that occurs word internally, in examples like m.p.h,,
ph.d., at&t, cap   n. special characters and numbers will need to be kept in prices
($45.55) and dates (01/02/06); we don   t want to segment that price into separate to-
kens of    45    and    55   . and there are urls (http://www.stanford.edu), twitter
hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).

number expressions introduce other complications as well; while commas nor-
mally appear at word boundaries, commas are used inside numbers in english, every
three digits: 555,500.50. languages, and hence id121 requirements, differ
on this; many continental european languages like spanish, french, and german, by
contrast, use a comma to mark the decimal point, and spaces (or sometimes periods)
where english puts commas, for example, 555 500,50.

a tokenizer can also be used to expand clitic contractions that are marked by
apostrophes, for example, converting what   re to the two tokens what are, and
we   re to we are. a clitic is a part of a word that can   t stand on its own, and can only
occur when it is attached to another word. some such contractions occur in other
alphabetic languages, including articles and pronouns in french (j   ai, l   homme).
depending on the application, id121 algorithms may also tokenize mul-
tiword expressions like new york or rock    n    roll as a single token, which re-
quires a multiword expression dictionary of some sort. id121 is thus inti-
mately tied up with named entity detection, the task of detecting names, dates, and
organizations (chapter 17).

one commonly used id121 standard is known as the id32 to-
kenization standard, used for the parsed corpora (treebanks) released by the lin-
guistic data consortium (ldc), the source of many useful datasets. this standard
separates out clitics (doesn   t becomes does plus n   t), keeps hyphenated words to-
gether, and separates out all punctuation:

input:
output:

   the san francisco-based restaurant,    they said,    doesn   t charge $10   .
   
said

francisco-based
charge

restaurant
10
$

san
   

the
,

does

they

n   t

   

   

,

.

tokens can also be normalized, in which a single normalized form is chosen for
words with multiple forms like usa and us or uh-huh and uhhuh. this standard-
ization may be valuable, despite the spelling information that is lost in the normal-
ization process. for information retrieval, we might want a query for us to match a
document that has usa; for information extraction we might want to extract coherent
information that is consistent across differently-spelled instances.

case folding is another kind of id172. for tasks like id103
and information retrieval, everything is mapped to lower case. for sentiment anal-
ysis and other text classi   cation tasks, information extraction, and machine transla-
tion, by contrast, case is quite helpful and case folding is generally not done (losing
the difference, for example, between us the country and us the pronoun can out-
weigh the advantage in generality that case folding provides).

in practice, since id121 needs to be run before any other language process-
ing, it is important for it to be very fast. the standard method for id121/nor-
malization is therefore to use deterministic algorithms based on id157
compiled into very ef   cient    nite state automata. carefully designed deterministic
algorithms can deal with the ambiguities that arise, such as the fact that the apos-
trophe needs to be tokenized differently when used as a genitive marker (as in the

clitic

id32
id121

case folding

2.4

    text id172

25

book   s cover), a quotative as in    the other class   , she said, or in clitics like they   re.

2.4.3 id40 in chinese: the maxmatch algorithm
some languages, including written chinese, japanese, and thai, do not use spaces to
mark potential word-boundaries, and so require alternative segmentation methods.
in chinese, for example, words are composed of characters known as hanzi. each
character generally represents a single morpheme and is pronounceable as a single
syllable. words are about 2.4 characters long on average. a simple algorithm that
does remarkably well for segmenting chinese, and often used as a baseline com-
parison for more advanced methods, is a version of greedy search called maximum
matching or sometimes maxmatch. the algorithm requires a dictionary (wordlist)
of the language.

the maximum matching algorithm starts by pointing at the beginning of a string.
it chooses the longest word in the dictionary that matches the input at the current
position. the pointer is then advanced to the end of that word in the string.
if
no word matches, the pointer is instead advanced one character (creating a one-
character word). the algorithm is then iteratively applied again starting from the
new pointer position. fig. 2.11 shows a version of the algorithm.

hanzi

maximum
matching

function maxmatch(sentence, dictionary) returns word sequence w

if sentence is empty

return empty list

for i   length(sentence) downto 1
   rstword =    rst i chars of sentence
remainder = rest of sentence
if indictionary(   rstword, dictionary)

return list(   rstword, maxmatch(remainder,dictionary) )

# no word was found, so make a one-character word
   rstword =    rst char of sentence
remainder = rest of sentence
return list(   rstword, maxmatch(remainder,dictionary) )
figure 2.11 the maxmatch algorithm for id40.

maxmatch works very well on chinese; the following example shows an appli-
cation to a simple chinese sentence using a simple chinese lexicon available from
the linguistic data consortium:

input:                            
output:           

                   

he especially likes peking duck

   he especially likes peking duck   

maxmatch doesn   t work as well on english. to make the intuition clear, we   ll
create an example by removing the spaces from the beginning of turing   s famous
quote    we can only see a short distance ahead   , producing    wecanonlyseeashortdis-
tanceahead   . the maxmatch results are shown below.
wecanonlyseeashortdistanceahead

input:
output: we canon l y see ash ort distance ahead
on english the algorithm incorrectly chose canon instead of stopping at can,
which left the algorithm confused and having to create single-character words l and

26 chapter 2

    id157, text id172, id153

word error rate

morpheme
stem
af   x

y and use the very rare word ort.

the algorithm works better in chinese than english, because chinese has much
shorter words than english. we can quantify how well a segmenter works using a
metric called word error rate. we compare our output segmentation with a perfect
hand-segmented (   gold   ) sentence, seeing how many words differ. the word error
rate is then the normalized minimum id153 in words between our output and
the gold: the number of word insertions, deletions, and substitutions divided by the
length of the gold sentence in words; we   ll see in section 2.5 how to compute edit
distance. even in chinese, however, maxmatch has problems, for example dealing
with unknown words (words not in the dictionary) or genres that differ a lot from
the assumptions made by the dictionary builder.

the most accurate chinese segmentation algorithms generally use statistical se-
quence models trained via supervised machine learning on hand-segmented training
sets; we   ll introduce sequence models in chapter 8.

2.4.4 collapsing words: lemmatization and id30
for many natural language processing situations we want two different forms of
a word to behave similarly. for example in web search, someone may type the
string woodchucks but a useful system might want to also return pages that mention
woodchuck with no s. this is especially common in morphologically complex lan-
guages like russian, where for example the word moscow has different endings in
the phrases moscow, of moscow, from moscow, and so on.

lemmatization is the task of determining that two words have the same root,
despite their surface differences. the words am, are, and is have the shared lemma
be; the words dinner and dinners both have the lemma dinner.

lemmatizing each of these forms to the same lemma will let us    nd all mentions
of words like moscow. the the lemmatized form of a sentence like he is reading
detective stories would thus be he be read detective story.

how is lemmatization done? the most sophisticated methods for lemmatization
involve complete morphological parsing of the word. morphology is the study of
the way words are built up from smaller meaning-bearing units called morphemes.
two broad classes of morphemes can be distinguished: stems   the central mor-
pheme of the word, supplying the main meaning    and af   xes   adding    additional   
meanings of various kinds. so, for example, the word fox consists of one morpheme
(the morpheme fox) and the word cats consists of two: the morpheme cat and the
morpheme -s. a morphological parser takes a word like cats and parses it into the
two morphemes cat and s, or a spanish word like amaren (   if in the future they
would love   ) into the morphemes amar    to love   , 3pl, and future subjunctive.

the porter stemmer
lemmatization algorithms can be complex. for this reason we sometimes make use
of a simpler but cruder method, which mainly consists of chopping off word-   nal
af   xes. this naive version of morphological analysis is called id30. one of
the most widely used id30 algorithms is the porter (1980). the porter stemmer
applied to the following paragraph:

id30
porter stemmer

this was not the map we found in billy bones   s chest, but
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.

2.4

    text id172

27

produces the following stemmed output:

thi wa not the map we found in billi bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
the algorithm is based on series of rewrite rules run in series, as a cascade, in
which the output of each pass is fed as input to the next pass; here is a sampling of
the rules:

cascade

ational     ate (e.g., relational     relate)

ing      
sses     ss (e.g., grasses     grass)

if stem contains vowel (e.g., motoring     motor)

detailed rule lists for the porter stemmer, as well as code (in java, python, etc.)
can be found on martin porter   s homepage; see also the original paper (porter, 1980).
simple stemmers can be useful in cases where we need to collapse across differ-
ent variants of the same lemma. nonetheless, they do tend to commit errors of both
over- and under-generalizing, as shown in the table below (krovetz, 1993):

errors of commission
organization organ
doing
numerical
policy

doe
numerous
police

errors of omission
european europe
analyzes
analysis
noisy
noise
sparse
sparsity

unknown
words

byte-pair
encoding
bpe

2.4.5 byte-pair encoding
id30 or lemmatizing has another side-bene   t. by treating two similar words
identically, these id172 methods help deal with the problem of unknown
words, words that a system has not seen before.

unknown words are particularly relevant for machine learning systems. as we
will see in the next chapter, machine learning systems often learn some facts about
words in one corpus (a training corpus) and then use these facts to make decisions
about a separate test corpus and its words. thus if our training corpus contains, say
the words low, and lowest, but not lower, but then the word lower appears in our
test corpus, our system will not know what to do with it. id30 or lemmatizing
everything to low can solve the problem, but has the disadvantage that sometimes
we don   t want words to be completely collapsed. for some purposes (for example
part-of-speech tagging) the words low and lower need to remain distinct.

a solution to this problem is to use a different kind of id121 in which
most tokens are words, but some tokens are frequent word parts like -er, so that an
unseen word can be represented by combining the parts.

the simplest such algorithm is byte-pair encoding, or bpe (sennrich et al.,
2016). byte-pair encoding is based on a method for text compression (gage, 1994),
but here we use it for id121 instead. the intuition of the algorithm is to
iteratively merge frequent pairs of characters,

the algorithm begins with the set of symbols equal to the set of characters. each
word is represented as a sequence of characters plus a special end-of-word symbol
  . at each step of the algorithm, we count the number of symbol pairs,    nd the
most frequent pair (   a   ,    b   ), and replace it with the new merged symbol (   ab   ). we
continue to count and merge, creating new longer and longer character strings, until

28 chapter 2

    id157, text id172, id153

we   ve done k merges; k is a parameter of the algorithm. the resulting symbol set
will consist of the original set of characters plus k new symbols.

the algorithm is run inside words (we don   t merge across word boundaries).
for this reason, the algorithm can take as input a dictionary of words together with
counts. for example, consider the following tiny input dictionary:

we    rst count all pairs of symbols: the most frequent is the pair r    because it
occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occur-
rences. we then merge these symbols, treating r   as one symbol, and count again:

now the most frequent pair is e r  , which we merge:
word
frequency
5
l o w   
l o w e s t    2
6
n e w er  
3
w i d er  
2
n e w   

our system has learned that there should be a token for word-   nal er, repre-

sented as er  . if we continue, the next merges are

word
frequency
5
l o w   
l o w e s t    2
6
n e w e r   
3
w i d e r   
2
n e w   

word
frequency
5
l o w   
l o w e s t    2
6
n e w e r  
3
w i d e r  
2
n e w   

(   e   ,    w   )
(   n   ,    ew   )
(   l   ,    o   )
(   lo   ,    w   )
(   new   ,    er     )
(   low   ,         )

the current set of symbols is thus {  , d, e, i, l, n, o, r, s, t, w,
r  , er  , ew, new, lo, low, newer  , low  }
when we need to tokenize a test sentence, we just run the merges we have
learned, greedily, in the order we learned them, on the test data.
(thus the fre-
quencies in the test data don   t play a role, just the frequencies in the training data).
so    rst we segment each test sentence word into characters. then we apply the    rst
rule: replace every instance of r    in the test corpus with r  , and then the second
rule: replace every instance of e r   in the test corpus with er  , and so on. by the
end, if the test corpus contained the word n e w e r   , it would be tokenized as a
full word. but a new (unknown) word like l o w e r    would be merged into the
two tokens low er  .
of course in real algorithms bpe is run with many thousands of merges on a
very large input dictionary. the result is that most words will be represented as

2.4

    text id172

29

full symbols, and only the very rare words (and unknown words) will have to be
represented by their parts.

the full bpe learning algorithm is given in fig. 2.12.

i m p o r t

re ,

c o l l e c t i o n s

d e f g e t s t a t s ( vocab ) :

p a i r s = c o l l e c t i o n s . d e f a u l t d i c t ( i n t )
f o r word ,

f r e q i n vocab . i t e m s ( ) :

symbols = word . s p l i t ( )
f o r

i n r a n g e ( l e n ( symbols )    1):

i
p a i r s [ symbols [ i ] , symbols [ i + 1 ] ] += f r e q

r e t u r n p a i r s

d e f merge vocab ( p a i r , v i n ) :

v o u t = {}
bigram = r e . e s c a p e (    
p = r e . c o m p i l e ( r     (? <!\s )     + bigram + r     ( ? !\ s )     )
f o r word i n v i n :

    . j o i n ( p a i r ) )

w out = p . sub (         . j o i n ( p a i r ) , word )
v o u t [ w out ] = v i n [ word ]

r e t u r n v o u t

vocab = {     l o w </w>   

: 5 ,

    l o w e s

t </w>   

: 2 ,

    n e w e r </w>    : 6 ,

   w i d e r </w>    : 3 ,

    n e w </w>    : 2}

num merges = 8

f o r

i n r a n g e ( num merges ) :

i
p a i r s = g e t s t a t s ( vocab )
b e s t = max ( p a i r s , key= p a i r s . g e t )
vocab = merge vocab ( b e s t , vocab )
p r i n t ( b e s t )

figure 2.12 python code for bpe learning algorithm from sennrich et al. (2016).

sentence
segmentation

2.4.6 sentence segmentation
sentence segmentation is another important step in text processing. the most use-
ful cues for segmenting a text into sentences are punctuation, like periods, question
marks, exclamation points. question marks and exclamation points are relatively
unambiguous markers of sentence boundaries. periods, on the other hand, are more
ambiguous. the period character    .    is ambiguous between a sentence boundary
marker and a marker of abbreviations like mr. or inc. the previous sentence that
you just read showed an even more complex case of this ambiguity, in which the    nal
period of inc. marked both an abbreviation and the sentence boundary marker. for
this reason, sentence id121 and word id121 may be addressed jointly.
in general, sentence id121 methods work by building a binary classi   er
(based on a sequence of rules or on machine learning) that decides if a period is part
of the word or is a sentence-boundary marker. in making this decision, it helps to
know if the period is attached to a commonly used abbreviation; thus, an abbrevia-
tion dictionary is useful.

state-of-the-art methods for sentence id121 are based on machine learning

and are introduced in later chapters.

30 chapter 2

    id157, text id172, id153

2.5 minimum id153

much of natural language processing is concerned with measuring how similar two
strings are. for example in id147, the user typed some erroneous
string   let   s say graffe   and we want to know what the user meant. the user prob-
ably intended a word that is similar to graffe. among candidate similar words,
the word giraffe, which differs by only one letter from graffe, seems intuitively
to be more similar than, say grail or graf, which differ in more letters. another
example comes from coreference, the task of deciding whether two strings such as
the following refer to the same entity:

stanford president john hennessy
stanford university president john hennessy

minimum edit
distance

alignment

again, the fact that these two strings are very similar (differing by only one word)
seems like useful evidence for deciding that they might be coreferent.

id153 gives us a way to quantify both of these intuitions about string sim-
ilarity. more formally, the minimum id153 between two strings is de   ned
as the minimum number of editing operations (operations like insertion, deletion,
substitution) needed to transform one string into another.

the gap between intention and execution, for example, is 5 (delete an i, substi-
tute e for n, substitute x for t, insert c, substitute u for n). it   s much easier to see
this by looking at the most important visualization for string distances, an alignment
between the two strings, shown in fig. 2.13. given two sequences, an alignment is
a correspondence between substrings of the two sequences. thus, we say i aligns
with the empty string, n with e, and so on. beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
top string into the bottom string: d for deletion, s for substitution, i for insertion.

i n t e * n t i o n
| | | | | | | | | |
* e x e c u t i o n
d s s

i s

figure 2.13 representing the minimum id153 between two strings as an alignment.
the    nal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.

we can also assign a particular cost or weight to each of these operations. the
levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (levenshtein, 1966)   we assume
that the substitution of a letter for itself, for example, t for t, has zero cost. the lev-
enshtein distance between intention and execution is 5. levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (this is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). using this version, the levenshtein distance between
intention and execution is 8.

2.5

    minimum id153

31

2.5.1 the minimum id153 algorithm
how do we    nd the minimum id153? we can think of this as a search task, in
which we are searching for the shortest path   a sequence of edits   from one string
to another.

dynamic
programming

figure 2.14 finding the id153 viewed as a search problem

the space of all possible edits is enormous, so we can   t search naively. however,
lots of distinct edit paths will end up in the same state (string), so rather than recom-
puting all those paths, we could just remember the shortest path to a state each time
we saw it. we can do this by using id145. id145
is the name for a class of algorithms,    rst introduced by bellman (1957), that apply
a table-driven method to solve problems by combining solutions to sub-problems.
some of the most commonly used algorithms in natural language processing make
use of id145, such as the viterbi algorithm (chapter 8) and the
cky algorithm for parsing (chapter 11).

the intuition of a id145 problem is that a large problem can
be solved by properly combining the solutions to various sub-problems. consider
the shortest path of transformed words that represents the minimum id153
between the strings intention and execution shown in fig. 2.15.

figure 2.15 path from intention to execution.

minimum edit
distance

imagine some string (perhaps it is exention) that is in this optimal path (whatever
it is). the intuition of id145 is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention to exention. why? if there were a shorter path from intention to exention,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn   t be optimal, thus leading to a contradiction.

the minimum id153 algorithm was named by wagner and fischer (1974)
but independently discovered by many people (see the historical notes section of
chapter 8).

let   s    rst de   ne the minimum id153 between two strings. given two
strings, the source string x of length n, and target string y of length m, we   ll de   ne
d(i, j) as the id153 between x[1..i] and y [1.. j], i.e., the    rst i characters of x
and the    rst j characters of y . the id153 between x and y is thus d(n,m).

n t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o nn t e n t i o ni n t e n t i o ne t e n t i o ne x e n t i o ne x e n u t i o ne x e c u t i o ndelete isubstitute n by esubstitute t by xinsert usubstitute n by c32 chapter 2

    id157, text id172, id153
we   ll use id145 to compute d(n,m) bottom up, combining so-
lutions to subproblems. in the base case, with a source substring of length i but an
empty target string, going from i characters to 0 requires i deletes. with a target
substring of length j but an empty source going from 0 characters to j characters
requires j inserts. having computed d(i, j) for small i, j we then compute larger
d(i, j) based on previously computed smaller values. the value of d(i, j) is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:

d[i    1, j] + del-cost(source[i])
d[i, j    1] + ins-cost(target[ j])
d[i    1, j    1] + sub-cost(source[i],target[ j])

d[i, j] = min         

d[i, j] = min                     

if we assume the version of levenshtein distance in which the insertions and
deletions each have a cost of 1 (ins-cost(  ) = del-cost(  ) = 1), and substitutions have
a cost of 2 (except substitution of identical letters have zero cost), the computation
for d(i, j) becomes:

d[i    1, j] + 1
d[i, j    1] + 1
d[i    1, j    1] +(cid:26) 2;

0;

if source[i] (cid:54)= target[ j]
if source[i] = target[ j]

(2.4)

the algorithm is summarized in fig. 2.16; fig. 2.17 shows the results of applying
the algorithm to the distance between intention and execution with the version of
levenshtein in eq. 2.4.

knowing the minimum id153 is useful for algorithms like    nding poten-
tial spelling error corrections. but the id153 algorithm is important in another
way; with a small change, it can also provide the minimum cost alignment between
two strings. aligning two strings is useful throughout speech and language process-
ing. in id103, minimum id153 alignment is used to compute
the word error rate (chapter 26). alignment plays a role in machine translation, in
which sentences in a parallel corpus (a corpus with a text in two languages) need to
be matched to each other.

to extend the id153 algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the id153 matrix. figure 2.18
shows this path with the boldfaced cell. each boldfaced cell represents an alignment
of a pair of letters in the two strings. if two boldfaced cells occur in the same row,
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.

figure 2.18 also shows the intuition of how to compute this alignment path. the
computation proceeds in two steps. in the    rst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. the backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
we   ve shown a schematic of these backpointers in fig. 2.18. some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. in the second step, we perform a backtrace. in a backtrace, we start
from the last cell (at the    nal row and column), and follow the pointers back through
the id145 matrix. each complete path between the    nal cell and the
initial cell is a minimum distance alignment. exercise 2.7 asks you to modify the

backtrace

2.5

    minimum id153

33

function min-edit-distance(source, target) returns min-distance

n    length(source)
m    length(target)
create a distance matrix distance[n+1,m+1]

# initialization: the zeroth row and column is the distance from the empty string

d[0,0] = 0
for each row i from 1 to n do

for each column j from 1 to m do

d[i,0]   d[i-1,0] + del-cost(source[i])
d[0,j]   d[0, j-1] + ins-cost(target[j])

# recurrence relation:
for each row i from 1 to n do

for each column j from 1 to m do

d[i, j]    min( d[i   1, j] + del-cost(source[i]),
d[i   1, j   1] + sub-cost(source[i], target[j]),
d[i, j   1] + ins-cost(target[j]))

# termination
return d[n,m]

figure 2.16 the minimum id153 algorithm, an example of the class of dynamic
programming algorithms. the various costs can either be    xed (e.g.,    x,ins-cost(x) = 1)
or can be speci   c to the letter (to model the fact that some letters are more likely to be in-
serted than others). we assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost(x,x) = 0).

src\tar
#
i
n
t
e
n
t
i
o
n

#
0
1
2
3
4
5
6
7
8
9

e
1
2
3
4
3
4
5
6
7
8

x
2
3
4
5
4
5
6
7
8
9

e
3
4
5
6
5
6
7
8
9
10

c
4
5
6
7
6
7
8
9
10
11

u
5
6
7
8
7
8
9
10
11
12

t
6
7
8
7
8
9
8
9
10
11

i
7
6
7
8
9
10
9
8
9
10

o
8
7
8
9
10
11
10
9
8
9

n
9
8
7
8
9
10
11
10
9
8

figure 2.17 computation of minimum id153 between intention and execution with
the algorithm of fig. 2.16, using levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.

minimum id153 algorithm to store the pointers and compute the backtrace to
output an alignment.

while we worked our example with simple levenshtein distance, the algorithm
in fig. 2.16 allows arbitrary weights on the operations. for id147, for
example, substitutions are more likely to happen between letters that are next to
each other on the keyboard. the viterbi algorithm is a probabilistic extension of
minimum id153. instead of computing the    minimum id153    between
two strings, viterbi computes the    maximum id203 alignment    of one string
with another. we   ll discuss this more in chapter 8.

34 chapter 2

x
    2

e
    3

e
    1

c
    4

u
    5

    id157, text id172, id153
o

t
#
#
    6
0
    1 (cid:45)       2 (cid:45)       3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7
i
n     2 (cid:45)       3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8
t
    3 (cid:45)       4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8
(cid:45) 7
e
    4
n     5
t
    6
i
    7
o     8
n     9

n
i
    8     9
    7
(cid:45) 6
    7     8
    7 (cid:45)       8 (cid:45) 7
       8 (cid:45)       9
    8
    7        8 (cid:45)       9 (cid:45)       10
    9
(cid:45) 3
    4 (cid:45)       5 (cid:45)       6 (cid:45)       7 (cid:45)       8 (cid:45)       9 (cid:45)       10 (cid:45)       11 (cid:45)    10
    10        11
    5 (cid:45)       6 (cid:45)       7 (cid:45)       8 (cid:45)       9
    6 (cid:45)       7 (cid:45)       8 (cid:45)       9 (cid:45)       10
    9     10
(cid:45) 8     9
    7 (cid:45)       8 (cid:45)       9 (cid:45)       10 (cid:45)       11
    9 (cid:45) 8
    8 (cid:45)       9 (cid:45)       10 (cid:45)       11 (cid:45)       12
figure 2.18 when entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. after the table is full we compute an alignment
(minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and
following the arrows back. the sequence of bold cells represents one possible minimum cost
alignment between the two strings. diagram design after gus   eld (1997).

(cid:45) 8
    9
    10
    11

    9
(cid:45) 8
    9
    10

    4 (cid:45)    5

    6

2.6 summary

this chapter introduced a fundamental tool in language processing, the regular ex-
pression, and showed how to perform basic text id172 tasks including
id40 and id172, sentence segmentation, and id30.
we also introduce the important minimum id153 algorithm for comparing
strings. here   s a summary of the main points we covered about these ideas:

    the regular expression language is a powerful tool for pattern-matching.
    basic operations in id157 include concatenation of symbols,
disjunction of symbols ([], |, and .), counters (*, +, and {n,m}), anchors
(  , $) and precedence operators ((,)).

simple id157 substitutions or    nite automata.

    word id121 and id172 are generally done by cascades of
    the porter algorithm is a simple and ef   cient way to do id30, stripping
off af   xes. it does not have high accuracy but may be useful for some tasks.
    the minimum id153 between two strings is the minimum number of
operations it takes to edit one into the other. minimum id153 can be
computed by id145, which also results in an alignment of
the two strings.

bibliographical and historical notes

kleene (1951) and (1956)    rst de   ned id157 and the    nite automaton,
based on the mcculloch-pitts neuron. ken thompson was one of the    rst to build
id157 compilers into editors for text searching (thompson, 1968). his
editor ed included a command    g/regular expression/p   , or global regular expres-
sion print, which later became the unix grep utility.

text id172 algorithms has been applied since the beginning of the    eld.
one of the earliest widely-used stemmers was lovins (1968). id30 was also
applied early to the digital humanities, by packard (1973), who built an af   x-stripping
morphological parser for ancient greek. currently a wide variety of code for tok-

exercises

35

enization and id172 is available, such as the stanford tokenizer (http://
nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for
twitter (o   connor et al., 2010), or for sentiment (http://sentiment.christopherpotts.
net/tokenizing.html). see palmer (2012) for a survey of text preprocessing.
while the max-match algorithm we describe is commonly used as a segmentation
baseline in languages like chinese, higher accuracy algorithms like the stanford
crf segmenter, are based on sequence models; see tseng et al. (2005a) and chang
et al. (2008). nltk is an essential tool that offers both useful python libraries
(http://www.nltk.org) and textbook descriptions (bird et al., 2009) of many al-
gorithms including text id172 and corpus interfaces.

for more on herdan   s law and heaps    law, see herdan (1960, p. 28), heaps
(1978), egghe (2007) and baayen (2001); yasseri et al. (2012) discuss the relation-
ship with other measures of linguistic complexity. for more on id153, see the
excellent gus   eld (1997). our example measuring the id153 from    intention   
to    execution    was adapted from kruskal (1983). there are various publicly avail-
able packages to compute id153, including unix diff and the nist sclite
program (nist, 2005).

in his autobiography bellman (1984) explains how he originally came up with

the term id145:

i decided therefore to use the word,    programming   .

   ...the 1950s were not good years for mathematical research. [the]
secretary of defense ...had a pathological fear and hatred of the word,
research...
i
wanted to get across the idea that this was dynamic, this was multi-
stage... i thought, let   s ...
take a word that has an absolutely precise
meaning, namely dynamic... it   s impossible to use the word, dynamic,
in a pejorative sense. try thinking of some combination that will pos-
sibly give it a pejorative meaning.
it   s impossible. thus, i thought
id145 was a good name. it was something not even a
congressman could object to.   

exercises

2.1 write id157 for the following languages.

1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a,b such that each a is immedi-

ately preceded by and immediately followed by a b;

2.2 write id157 for the following languages. by    word   , we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.

1. the set of all strings with two consecutive repeated words (e.g.,    hum-

bert humbert    and    the the    but not    the bug    or    the big bug   );

2. all strings that start at the beginning of the line with an integer and that

end at the end of the line with a word;

3. all strings that have both the word grotto and the word raven in them

(but not, e.g., words like grottos that merely contain the word grotto);

36 chapter 2

    id157, text id172, id153

4. write a pattern that places the    rst word of an english sentence in a

register. deal with punctuation.

2.3

implement an eliza-like program, using substitutions such as those described
on page 18. you might want to choose a different domain than a rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.

2.4 compute the id153 (using insertion cost 1, deletion cost 1, substitution

cost 1) of    leda    to    deal   . show your work (using the id153 grid).
figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. you may use any version of distance that you like.

2.5

2.6 now implement a minimum id153 algorithm and use your hand-computed

results to check your code.

2.7 augment the minimum id153 algorithm to output an alignment; you

2.8
2.9

will need to store pointers and add a stage to compute the backtrace.
implement the maxmatch algorithm.
to test how well your maxmatch algorithm works, create a test set by remov-
ing spaces from a set of sentences. implement the word error rate metric (the
number of word insertions + deletions + substitutions, divided by the length
in words of the correct string) and compute the wer for your test set.

chapter

3 id165 language models

   you are uniformly charming!    cried he, with a smile of associating and now
and then i bowed and they perceived a chaise and four to wish for.

random sentence generated from a jane austen trigram model

being able to predict the future is not always a good thing. cassandra of troy
had the gift of foreseeing but was cursed by apollo that no one would believe her
predictions. her warnings of the destruction of troy were ignored and   well, let   s
just say that things didn   t turn out great for her.

in this chapter we take up the somewhat less fraught topic of predicting words.

what word, for example, is likely to follow

please turn your homework ...

hopefully, most of you concluded that a very likely word is in, or possibly over,
but probably not refrigerator or the. in the following sections we will formalize
this intuition by introducing models that assign a id203 to each possible next
word. the same models will also serve to assign a id203 to an entire sentence.
such a model, for example, could predict that the following sequence has a much
higher id203 of appearing in a text:

all of a sudden i notice three guys standing on the sidewalk

than does this same set of words in a different order:

on guys all i of notice sidewalk three a sudden standing the

why would you want to predict upcoming words, or assign probabilities to sen-
tences? probabilities are essential in any task in which we have to identify words
in noisy, ambiguous input, like id103 or handwriting recognition. in
the movie take the money and run, woody allen tries to rob a bank with a sloppily
written hold-up note that the teller incorrectly reads as    i have a gub   . as rus-
sell and norvig (2002) point out, a language processing system could avoid making
this mistake by using the knowledge that the sequence    i have a gun    is far more
probable than the non-word    i have a gub    or even    i have a gull   .

in id147, we need to    nd and correct spelling errors like their
are two midterms in this class, in which there was mistyped as their. a sentence
starting with the phrase there are will be much more probable than one starting with
their are, allowing a spellchecker to both detect and correct these errors.

assigning probabilities to sequences of words is also essential in machine trans-

lation. suppose we are translating a chinese source sentence:

              
he to reporters introduced main content

                       

38 chapter 3

    id165 language models
as part of the process we might have built the following set of potential rough

english translations:

he introduced reporters to the main contents of the statement
he briefed to reporters the main contents of the statement
he briefed reporters on the main contents of the statement

a probabilistic model of word sequences could suggest that briefed reporters on
is a more probable english phrase than briefed to reporters (which has an awkward
to after briefed) or introduced reporters to (which uses a verb that is less    uent
english in this context), allowing us to correctly select the boldfaced sentence above.
probabilities are also important for augmentative communication (newell et al.,
1998) systems. people like the late physicist stephen hawking who are unable to
physically talk or sign can instead use simple movements to select words from a
menu to be spoken by the system. word prediction can be used to suggest likely
words for the menu.

models that assign probabilities to sequences of words are called language mod-
els or lms. in this chapter we introduce the simplest model that assigns probabilities
to sentences and sequences of words, the id165. an id165 is a sequence of n
words: a 2-gram (or bigram) is a two-word sequence of words like    please turn   ,
   turn your   , or    your homework   , and a 3-gram (or trigram) is a three-word se-
quence of words like    please turn your   , or    turn your homework   . we   ll see how
to use id165 models to estimate the id203 of the last word of an id165 given
the previous words, and also to assign probabilities to entire sequences. in a bit of
terminological ambiguity, we usually drop the word    model   , and thus the term n-
gram is used to mean either the word sequence itself or the predictive model that
assigns it a id203.

language model
lm
id165

3.1 id165s

let   s begin with the task of computing p(w|h), the id203 of a word w given
some history h. suppose the history h is    its water is so transparent that    and we
want to know the id203 that the next word is the:

p(the|its water is so transparent that).

(3.1)

one way to estimate this id203 is from relative frequency counts: take a
very large corpus, count the number of times we see its water is so transparent that,
and count the number of times this is followed by the. this would be answering the
question    out of the times we saw the history h, how many times was it followed by
the word w   , as follows:

p(the|its water is so transparent that) =
c(its water is so transparent that the)
c(its water is so transparent that)

(3.2)

with a large enough corpus, such as the web, we can compute these counts and
estimate the id203 from eq. 3.2. you should pause now, go to the web, and
compute this estimate for yourself.

while this method of estimating probabilities directly from counts works    ne in
many cases, it turns out that even the web isn   t big enough to give us good estimates

3.1

    id165s

39

in most cases. this is because language is creative; new sentences are created all the
time, and we won   t always be able to count entire sentences. even simple extensions
of the example sentence may have counts of zero on the web (such as    walden
pond   s water is so transparent that the   ).

similarly, if we wanted to know the joint id203 of an entire sequence of
words like its water is so transparent, we could do it by asking    out of all possible
sequences of    ve words, how many of them are its water is so transparent?    we
would have to get the count of its water is so transparent and divide by the sum of
the counts of all possible    ve word sequences. that seems rather a lot to estimate!
for this reason, we   ll need to introduce cleverer ways of estimating the proba-
bility of a word w given a history h, or the id203 of an entire word sequence w .
let   s start with a little formalizing of notation. to represent the id203 of a par-
ticular random variable xi taking on the value    the   , or p(xi =    the   ), we will use
the simpli   cation p(the). we   ll represent a sequence of n words either as w1 . . .wn
or wn
1 means the string w1,w2, ...,wn   1). for the joint prob-
ability of each word in a sequence having a particular value p(x = w1,y = w2,z =
w3, ...,w = wn) we   ll use p(w1,w2, ...,wn).

1 (so the expression wn   1

now how can we compute probabilities of entire sequences like p(w1,w2, ...,wn)?
one thing we can do is decompose this id203 using the chain rule of proba-
bility:

p(x1...xn) = p(x1)p(x2|x1)p(x3|x 2

1 ) . . .p(xn|x n   1

1

)

=

n(cid:89)k=1

p(xk|x k   1

1

)

applying the chain rule to words, we get

p(wn

1) = p(w1)p(w2|w1)p(w3|w2

1) . . .p(wn|wn   1

1

=

n(cid:89)k=1

p(wk|wk   1

1

)

(3.3)

(3.4)

)

the chain rule shows the link between computing the joint id203 of a se-
quence and computing the id155 of a word given previous words.
equation 3.4 suggests that we could estimate the joint id203 of an entire se-
quence of words by multiplying together a number of conditional probabilities. but
using the chain rule doesn   t really seem to help us! we don   t know any way to
compute the exact id203 of a word given a long sequence of preceding words,
p(wn|wn   1
). as we said above, we can   t just estimate by counting the number of
times every word occurs following every long string, because language is creative
and any particular context might have never occurred before!

1

the intuition of the id165 model is that instead of computing the id203 of
a word given its entire history, we can approximate the history by just the last few
words.

the bigram model, for example, approximates the id203 of a word given
all the previous words p(wn|wn   1
) by using only the id155 of the
preceding word p(wn|wn   1). in other words, instead of computing the id203
(3.5)

p(the|walden pond   s water is so transparent that)

1

bigram

40 chapter 3

    id165 language models
we approximate it with the id203

p(the|that)

(3.6)

markov

id165

maximum
likelihood
estimation

normalize

when we use a bigram model to predict the id155 of the next

word, we are thus making the following approximation:
)     p(wn|wn   1)

p(wn|wn   1

1

(3.7)

the assumption that the id203 of a word depends only on the previous word
is called a markov assumption. markov models are the class of probabilistic models
that assume we can predict the id203 of some future unit without looking too
far into the past. we can generalize the bigram (which looks one word into the past)
to the trigram (which looks two words into the past) and thus to the id165 (which
looks n    1 words into the past).
id203 of the next word in a sequence is

thus, the general equation for this id165 approximation to the conditional

(3.8)
given the bigram assumption for the id203 of an individual word, we can
compute the id203 of a complete word sequence by substituting eq. 3.7 into
eq. 3.4:

)     p(wn|wn   1

p(wn|wn   1

n   n+1)

1

p(wn

1)    

p(wk|wk   1)

(3.9)

n(cid:89)k=1

how do we estimate these bigram or id165 probabilities? an intuitive way to
estimate probabilities is called id113 or id113. we get
the id113 estimate for the parameters of an id165 model by getting counts from a
corpus, and normalizing the counts so that they lie between 0 and 1.1

for example, to compute a particular bigram id203 of a word y given a
previous word x, we   ll compute the count of the bigram c(xy) and normalize by the
sum of all the bigrams that share the same    rst word x:
c(wn   1wn)

(3.10)

p(wn|wn   1) =

we can simplify this equation, since the sum of all bigram counts that start with
a given word wn   1 must be equal to the unigram count for that word wn   1 (the reader
should take a moment to be convinced of this):

(cid:80)wc(wn   1w)

p(wn|wn   1) =

c(wn   1wn)
c(wn   1)

(3.11)

let   s work through an example using a mini-corpus of three sentences. we   ll
   rst need to augment each sentence with a special symbol <s> at the beginning
of the sentence, to give us the bigram context of the    rst word. we   ll also need a
special end-symbol. </s>2

1 for probabilistic models, normalizing means dividing by some total count so that the resulting prob-
abilities fall legally between 0 and 1.
2 we need the end-symbol to make the bigram grammar a true id203 distribution. without an
end-symbol, the sentence probabilities for all sentences of a given length would sum to one. this model
would de   ne an in   nite set of id203 distributions, with one distribution per sentence length. see
exercise 3.5.

3.1

    id165s

41

<s> i am sam </s>
<s> sam i am </s>
<s> i do not like green eggs and ham </s>

here are the calculations for some of the bigram probabilities from this corpus
p(i|<s>) = 2
p(</s>|sam) = 1
for the general case of id113 id165 parameter estimation:

p(sam|<s>) = 1
p(sam|am) = 1

p(am|i) = 2
p(do|i) = 1

3 = .33
2 = .5

3 = .67
3 = .33

2 = 0.5

3 = .67

relative
frequency

p(wn|wn   1

n   n+1) =

c(wn   1
c(wn   1

n   n+1wn)
n   n+1)

(3.12)

equation 3.12 (like eq. 3.11) estimates the id165 id203 by dividing the
observed frequency of a particular sequence by the observed frequency of a pre   x.
this ratio is called a relative frequency. we said above that this use of relative
frequencies as a way to estimate probabilities is an example of maximum likelihood
estimation or id113. in id113, the resulting parameter set maximizes the likelihood
of the training set t given the model m (i.e., p(t|m)). for example, suppose the
word chinese occurs 400 times in a corpus of a million words like the brown corpus.
what is the id203 that a random word selected from some other text of, say,
a million words will be the word chinese? the id113 of its id203 is
1000000
or .0004. now .0004 is not the best possible estimate of the id203 of chinese
occurring in all situations; it might turn out that in some other corpus or context
chinese is a very unlikely word. but it is the id203 that makes it most likely
that chinese will occur 400 times in a million-word corpus. we present ways to
modify the id113 estimates slightly to get better id203 estimates in section 3.4.
let   s move on to some examples from a slightly larger corpus than our 14-word
example above. we   ll use data from the now-defunct berkeley restaurant project,
a dialogue system from the last century that answered questions about a database
of restaurants in berkeley, california (jurafsky et al., 1994). here are some text-
normalized sample user queries (a sample of 9332 sentences is on the website):

400

can you tell me about any good cantonese restaurants close by
mid priced thai food is what i   m looking for
tell me about chez panisse
can you give me a listing of the kinds of food that are available
i   m looking for a good place to eat breakfast
when is caffe venezia open during the day

figure 3.1 shows the bigram counts from a piece of a bigram grammar from the
berkeley restaurant project. note that the majority of the values are zero. in fact,
we have chosen the sample words to cohere with each other; a matrix selected from
a random set of seven words would be even more sparse.

figure 3.2 shows the bigram probabilities after id172 (dividing each cell
in fig. 3.1 by the appropriate unigram for its row, taken from the following set of
unigram probabilities):

want to

i
2533 927

eat chinese food lunch spend

2417 746 158

1093 341

278

here are a few other useful probabilities:

42 chapter 3

    id165 language models
eat
9
1
686
0
0
0
0
0

i
want
to
eat
chinese
food
lunch
spend
figure 3.1 bigram counts for eight of the words (out of v = 1446) in the berkeley restau-
rant project corpus of 9332 sentences. zero counts are in gray.

chinese
0
6
2
16
0
1
0
0

spend
2
1
211
0
0
0
0
0

lunch
0
5
6
42
1
0
0
0

want
827
0
0
0
0
0
0
0

food
0
6
0
2
82
4
1
0

to
0
608
4
2
0
15
0
1

i
5
2
2
0
1
15
2
1

chinese

lunch
0

eat
0.0036 0
0.0011 0.0065

spend
food
0
0.00079
0.0065 0.0054 0.0011
0.0025 0.087

i
want
0.002
0.33
0.0022
0
0.00083 0
0
0
0
0
0
0

i
want
to
eat
chinese 0.0063
food
0.014
lunch
0.0059
spend
0.0036
figure 3.2 bigram probabilities for eight words in the berkeley restaurant project corpus
of 9332 sentences. zero probabilities are in gray.

0.00083 0
0.021
0
0.00092 0.0037 0
0.0029 0
0
0
0
0

0
0.0063 0
0
0
0

to
0
0.66
0.0017 0.28
0.0027 0
0
0
0.014
0
0
0
0.0036 0

0.0027 0.056
0.52

p(i|<s>) = 0.25
p(food|english) = 0.5

p(english|want) = 0.0011
p(</s>|food) = 0.68

now we can compute the id203 of sentences like i want english food or
i want chinese food by simply multiplying the appropriate bigram probabilities to-
gether, as follows:

p(<s> i want english food </s>)

= p(i|<s>)p(want|i)p(english|want)

p(food|english)p(</s>|food)

= .25   .33   .0011   0.5   0.68
= .000031

we leave it as exercise 3.2 to compute the id203 of i want chinese food.
what kinds of linguistic phenomena are captured in these bigram statistics?
some of the bigram probabilities above encode some facts that we think of as strictly
syntactic in nature, like the fact that what comes after eat is usually a noun or an
adjective, or that what comes after to is usually a verb. others might be a fact about
the personal assistant task, like the high id203 of sentences beginning with
the words i. and some might even be cultural rather than linguistic, like the higher
id203 that people are looking for chinese versus english food.

trigram
4-gram
5-gram

some practical issues: although for pedagogical purposes we have only described
bigram models, in practice it   s more common to use trigram models, which con-
dition on the previous two words rather than the previous word, or 4-gram or even
5-gram models, when there is suf   cient training data. note that for these larger n-
grams, we   ll need to assume extra context for the contexts to the left and right of the

log
probabilities

3.2

    evaluating language models

43

sentence end. for example, to compute trigram probabilities at the very beginning of
the sentence, we can use two pseudo-words for the    rst trigram (i.e., p(i|<s><s>).
we always represent and compute language model probabilities in log format
as log probabilities. since probabilities are (by de   nition) less than or equal to
1, the more probabilities we multiply together, the smaller the product becomes.
multiplying enough id165s together would result in numerical under   ow. by using
log probabilities instead of raw probabilities, we get numbers that are not as small.
adding in log space is equivalent to multiplying in linear space, so we combine log
probabilities by adding them. the result of doing all computation and storage in log
space is that we only need to convert back into probabilities if we need to report
them at the end; then we can just take the exp of the logprob:

p1    p2    p3    p4 = exp(log p1 + log p2 + log p3 + log p4)

(3.13)

3.2 evaluating language models

extrinsic
evaluation

intrinsic
evaluation

training set

test set
held out

the best way to evaluate the performance of a language model is to embed it in
an application and measure how much the application improves. such end-to-end
evaluation is called extrinsic evaluation. extrinsic evaluation is the only way to
know if a particular improvement in a component is really going to help the task
at hand. thus, for id103, we can compare the performance of two
language models by running the speech recognizer twice, once with each language
model, and seeing which gives the more accurate transcription.

unfortunately, running big nlp systems end-to-end is often very expensive. in-
stead, it would be nice to have a metric that can be used to quickly evaluate potential
improvements in a language model. an intrinsic evaluation metric is one that mea-
sures the quality of a model independent of any application.

for an intrinsic evaluation of a language model we need a test set. as with many
of the statistical models in our    eld, the probabilities of an id165 model come from
the corpus it is trained on, the training set or training corpus. we can then measure
the quality of an id165 model by its performance on some unseen data called the
test set or test corpus. we will also sometimes call test sets and other datasets that
are not in our training sets held out corpora because we hold them out from the
training data.

so if we are given a corpus of text and want to compare two different id165
models, we divide the data into training and test sets, train the parameters of both
models on the training set, and then compare how well the two trained models    t the
test set.

but what does it mean to       t the test set   ? the answer is simple: whichever
model assigns a higher id203 to the test set   meaning it more accurately
predicts the test set   is a better model. given two probabilistic models, the better
model is the one that has a tighter    t to the test data or that better predicts the details
of the test data, and hence will assign a higher id203 to the test data.

since our evaluation metric is based on test set id203, it   s important not to
let the test sentences into the training set. suppose we are trying to compute the
id203 of a particular    test    sentence. if our test sentence is part of the training
corpus, we will mistakenly assign it an arti   cially high id203 when it occurs
in the test set. we call this situation training on the test set. training on the test
set introduces a bias that makes the probabilities all look too high, and causes huge

44 chapter 3

    id165 language models

development
test

inaccuracies in perplexity, the id203-based metric we introduce below.

sometimes we use a particular test set so often that we implicitly tune to its
characteristics. we then need a fresh test set that is truly unseen. in such cases, we
call the initial test set the development test set or, devset. how do we divide our
data into training, development, and test sets? we want our test set to be as large
as possible, since a small test set may be accidentally unrepresentative, but we also
want as much training data as possible. at the minimum, we would want to pick
the smallest test set that gives us enough statistical power to measure a statistically
signi   cant difference between two potential models. in practice, we often just divide
our data into 80% training, 10% development, and 10% test. given a large corpus
that we want to divide into training and test, test data can either be taken from some
continuous sequence of text inside the corpus, or we can remove smaller    stripes   
of text from randomly selected parts of our corpus and combine them into a test set.

perplexity

3.2.1 perplexity
in practice we don   t use raw id203 as our metric for evaluating language mod-
els, but a variant called perplexity. the perplexity (sometimes called pp for short)
of a language model on a test set is the inverse id203 of the test set, normalized
by the number of words. for a test set w = w1w2 . . .wn,:

pp(w ) = p(w1w2 . . .wn)    1

n

= n(cid:115)

1

p(w1w2 . . .wn)

we can use the chain rule to expand the id203 of w :

pp(w ) = n(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:89)i=1

1

p(wi|w1 . . .wi   1)

(3.14)

(3.15)

thus, if we are computing the perplexity of w with a bigram language model,

we get:

pp(w ) = n(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:89)i=1

1

p(wi|wi   1)

(3.16)

note that because of the inverse in eq. 3.15, the higher the conditional probabil-
ity of the word sequence, the lower the perplexity. thus, minimizing perplexity is
equivalent to maximizing the test set id203 according to the language model.
what we generally use for word sequence in eq. 3.15 or eq. 3.16 is the entire se-
quence of words in some test set. since this sequence will cross many sentence
boundaries, we need to include the begin- and end-sentence markers <s> and </s>
in the id203 computation. we also need to include the end-of-sentence marker
</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-
kens n.

there is another way to think about perplexity: as the weighted average branch-
ing factor of a language. the branching factor of a language is the number of possi-
ble next words that can follow any word. consider the task of recognizing the digits

3.3

    generalization and zeros

45

in english (zero, one, two,..., nine), given that each of the 10 digits occurs with equal
id203 p = 1
10. the perplexity of this mini-language is in fact 10. to see that,
imagine a string of digits of length n. by eq. 3.15, the perplexity will be

pp(w ) = p(w1w2 . . .wn)    1

n

n

= (

1
10
   1
1
=
10
= 10

)    1

n

(3.17)

but suppose that the number zero is really frequent and occurs 10 times more
often than other numbers. now we should expect the perplexity to be lower since
most of the time the next number will be zero. thus, although the branching factor
is still 10, the perplexity or weighted branching factor is smaller. we leave this
calculation as an exercise to the reader.

we see in section 3.7 that perplexity is also closely related to the information-

theoretic notion of id178.

finally, let   s look at an example of how perplexity can be used to compare dif-
ferent id165 models. we trained unigram, bigram, and trigram grammars on 38
million words (including start-of-sentence tokens) from the wall street journal, us-
ing a 19,979 word vocabulary. we then computed the perplexity of each of these
models on a test set of 1.5 million words with eq. 3.16. the table below shows the
perplexity of a 1.5 million word wsj test set according to each of these grammars.

unigram bigram trigram

perplexity 962

170

109

as we see above, the more information the id165 gives us about the word
sequence, the lower the perplexity (since as eq. 3.15 showed, perplexity is related
inversely to the likelihood of the test sequence according to the model).

note that in computing perplexities, the id165 model p must be constructed
without any knowledge of the test set or any prior knowledge of the vocabulary of
the test set. any kind of knowledge of the test set can cause the perplexity to be
arti   cially low. the perplexity of two language models is only comparable if they
use identical vocabularies.

an (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-
provement in the performance of a language processing task like id103
or machine translation. nonetheless, because perplexity often correlates with such
improvements, it is commonly used as a quick check on an algorithm. but a model   s
improvement in perplexity should always be con   rmed by an end-to-end evaluation
of a real task before concluding the evaluation of the model.

3.3 generalization and zeros

the id165 model, like many statistical models, is dependent on the training corpus.
one implication of this is that the probabilities often encode speci   c facts about a
given training corpus. another implication is that id165s do a better and better job
of modeling the training corpus as we increase the value of n.

46 chapter 3

    id165 language models
we can visualize both of these facts by borrowing the technique of shannon
(1951) and miller and selfridge (1950) of generating random sentences from dif-
ferent id165 models.
it   s simplest to visualize how this works for the unigram
case. imagine all the words of the english language covering the id203 space
between 0 and 1, each word covering an interval proportional to its frequency. we
choose a random value between 0 and 1 and print the word whose interval includes
this chosen value. we continue choosing random numbers and generating words
until we randomly generate the sentence-   nal token </s>. we can use the same
technique to generate bigrams by    rst generating a random bigram that starts with
<s> (according to its bigram id203). let   s say the second word of that bigram
is w. we next chose a random bigram starting with w (again, drawn according to its
bigram id203), and so on.

to give an intuition for the increasing power of higher-order id165s, fig. 3.3
shows random sentences generated from unigram, bigram, trigram, and 4-gram
models trained on shakespeare   s works.

gram

gram

rote life have
   hill he late speaks; or! a more to leg less    rst you enter

king. follow.
   what means, sir. i confess she? then all sorts, he is trim, captain.

1    to him swallowed confess hear both. which. of save on trail for are ay device and
2    why dost stand forth thy canopy, forsooth; he is this palpable hit the king henry. live
3    fly, and will rid me these news of price. therefore the sadness of parting, as they say,
4    king henry. what! i will go seek the traitor gloucester. exeunt some of the watch. a

   tis done.
   this shall forbid it should be branded, if renown made it empty.

great banquet serv   d in;
   it cannot be but so.

gram

gram

figure 3.3 eight sentences randomly generated from four id165s computed from shakespeare   s works. all
characters were mapped to lower-case and punctuation marks were treated as words. output is hand-corrected
for capitalization to improve readability.

the longer the context on which we train the model, the more coherent the sen-
tences. in the unigram sentences, there is no coherent relation between words or any
sentence-   nal punctuation. the bigram sentences have some local word-to-word
coherence (especially if we consider that punctuation counts as a word). the tri-
gram and 4-gram sentences are beginning to look a lot like shakespeare. indeed, a
careful investigation of the 4-gram sentences shows that they look a little too much
like shakespeare. the words it cannot be but so are directly from king john. this is
because, not to put the knock on shakespeare, his oeuvre is not very large as corpora
go (n = 884,647,v = 29,066), and our id165 id203 matrices are ridiculously
sparse. there are v 2 = 844,000,000 possible bigrams alone, and the number of pos-
sible 4-grams is v 4 = 7  1017. thus, once the generator has chosen the    rst 4-gram
(it cannot be but), there are only    ve possible continuations (that, i, he, thou, and
so); indeed, for many 4-grams, there is only one continuation.

to get an idea of the dependence of a grammar on its training set, let   s look at an
id165 grammar trained on a completely different corpus: the wall street journal
(wsj) newspaper. shakespeare and the wall street journal are both english, so
we might expect some overlap between our id165s for the two genres. fig. 3.4

3.3

    generalization and zeros

47

shows sentences generated by unigram, bigram, and trigram grammars trained on
40 million words from wsj.

gram

were recession exchange new endorsed a acquire to six executives

1 months the my and issue of year foreign new exchange   s september
2 last december through the way to preserve the hudson corporation n.

b. e. c. taylor would seem to complete the major central planners one
point    ve percent of u. s. e. has already old m. x. corporation of living
on information such as more frequently    shing to keep her

gram

3 they also point to ninety nine point six billion dollars from two hundred

four oh six three percent of the rates of interest stores as mexico and
brazil on market conditions

gram

figure 3.4 three sentences randomly generated from three id165 models computed from
40 million words of the wall street journal, lower-casing all characters and treating punctua-
tion as words. output was then hand-corrected for capitalization to improve readability.

compare these examples to the pseudo-shakespeare in fig. 3.3. while they both
model    english-like sentences   , there is clearly no overlap in generated sentences,
and little overlap even in small phrases. statistical models are likely to be pretty use-
less as predictors if the training sets and the test sets are as different as shakespeare
and wsj.

how should we deal with this problem when we build id165 models? one step
is to be sure to use a training corpus that has a similar genre to whatever task we are
trying to accomplish. to build a language model for translating legal documents,
we need a training corpus of legal documents. to build a language model for a
question-answering system, we need a training corpus of questions.

it is equally important to get training data in the appropriate dialect, especially
when processing social media posts or spoken transcripts. thus tweets in aave
(african american vernacular english) often use words like    nna   an auxiliary
verb that markes immediate future tense    that don   t occur in other dialects, or
spellings like den for then, in tweets like this one (blodgett and o   connor, 2017):
(3.18) bored af den my phone    nna die!!!
while tweets from varieties like nigerian english have markedly different vocabu-
lary and id165 patterns from american english (jurgens et al., 2017):
(3.19) @username r u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u

tweet, nyt gan u dey tweet. beta get ur it placement wiv twitter

matching genres and dialects is still not suf   cient. our models may still be
subject to the problem of sparsity. for any id165 that occurred a suf   cient number
of times, we might have a good estimate of its id203. but because any corpus is
limited, some perfectly acceptable english word sequences are bound to be missing
from it. that is, we   ll have many cases of putative    zero id203 id165s    that
should really have some non-zero id203. consider the words that follow the
bigram denied the in the wsj treebank3 corpus, together with their counts:

denied the allegations: 5
denied the speculation: 2
denied the rumors:
1
1
denied the report:

but suppose our test set has phrases like:

48 chapter 3

    id165 language models

denied the offer
denied the loan

zeros

our model will incorrectly estimate that the p(offer|denied the) is 0!
these zeros    things that don   t ever occur in the training set but do occur in
the test set   are a problem for two reasons. first, their presence means we are
underestimating the id203 of all sorts of words that might occur, which will
hurt the performance of any application we want to run on this data.

second, if the id203 of any word in the test set is 0, the entire id203
of the test set is 0. by de   nition, perplexity is based on the inverse id203 of the
test set. thus if some words have zero id203, we can   t compute perplexity at
all, since we can   t divide by 0!

closed
vocabulary

oov
open
vocabulary

3.3.1 unknown words
the previous section discussed the problem of words whose bigram id203 is
zero. but what about words we simply have never seen before?

sometimes we have a language task in which this can   t happen because we know
all the words that can occur. in such a closed vocabulary system the test set can
only contain words from this lexicon, and there will be no unknown words. this is
a reasonable assumption in some domains, such as id103 or machine
translation, where we have a pronunciation dictionary or a phrase table that are    xed
in advance, and so the language model can only use the words in that dictionary or
phrase table.

in other cases we have to deal with words we haven   t seen before, which we   ll
call unknown words, or out of vocabulary (oov) words. the percentage of oov
words that appear in the test set is called the oov rate. an open vocabulary system
is one in which we model these potential unknown words in the test set by adding a
pseudo-word called <unk>.

there are two common ways to train the probabilities of the unknown word
model <unk>. the    rst one is to turn the problem back into a closed vocabulary one
by choosing a    xed vocabulary in advance:

1. choose a vocabulary (word list) that is    xed in advance.
2. convert in the training set any word that is not in this set (any oov word) to

the unknown word token <unk> in a text id172 step.

3. estimate the probabilities for <unk> from its counts just like any other regular

word in the training set.

the second alternative, in situations where we don   t have a prior vocabulary in ad-
vance, is to create such a vocabulary implicitly, replacing words in the training data
by <unk> based on their frequency. for example we can replace by <unk> all words
that occur fewer than n times in the training set, where n is some small number, or
equivalently select a vocabulary size v in advance (say 50,000) and choose the top
v words by frequency and replace the rest by unk. in either case we then proceed
to train the language model as before, treating <unk> like a regular word.

the exact choice of <unk> model does have an effect on metrics like perplexity.
a language model can achieve low perplexity by choosing a small vocabulary and
assigning the unknown word a high id203. for this reason, perplexities should
only be compared across language models with the same vocabularies (buck et al.,
2014).

3.4 smoothing

3.4

    smoothing

49

smoothing
discounting

laplace
smoothing

what do we do with words that are in our vocabulary (they are not unknown words)
but appear in a test set in an unseen context (for example they appear after a word
they never appeared after in training)? to keep a language model from assigning
zero id203 to these unseen events, we   ll have to shave off a bit of id203
mass from some more frequent events and give it to the events we   ve never seen.
this modi   cation is called smoothing or discounting. in this section and the fol-
lowing ones we   ll introduce a variety of ways to do smoothing: add-1 smoothing,
add-k smoothing, stupid backoff, and kneser-ney smoothing.

3.4.1 laplace smoothing
the simplest way to do smoothing is to add one to all the bigram counts, before
we normalize them into probabilities. all the counts that used to be zero will now
have a count of 1, the counts of 1 will be 2, and so on. this algorithm is called
laplace smoothing. laplace smoothing does not perform well enough to be used
in modern id165 models, but it usefully introduces many of the concepts that we
see in other smoothing algorithms, gives a useful baseline, and is also a practical
smoothing algorithm for other tasks like text classi   cation (chapter 4).

let   s start with the application of laplace smoothing to unigram probabilities.
recall that the unsmoothed maximum likelihood estimate of the unigram id203
of the word wi is its count ci normalized by the total number of word tokens n:

p(wi) =

ci
n

add-one

laplace smoothing merely adds one to each count (hence its alternate name add-
one smoothing). since there are v words in the vocabulary and each one was incre-
mented, we also need to adjust the denominator to take into account the extra v
observations. (what happens to our p values if we don   t increase the denominator?)

plaplace(wi) =

ci + 1
n +v

(3.20)

instead of changing both the numerator and denominator, it is convenient to
describe how a smoothing algorithm affects the numerator, by de   ning an adjusted
count c   . this adjusted count is easier to compare directly with the id113 counts and
can be turned into a id203 like an id113 count by normalizing by n. to de   ne
this count, since we are only changing the numerator in addition to adding 1 we   ll
also need to multiply by a id172 factor n

n+v :
n

n +v

c   i = (ci + 1)

(3.21)

discounting

discount

we can now turn c   i into a id203 p   i by normalizing by n.

a related way to view smoothing is as discounting (lowering) some non-zero
counts in order to get the id203 mass that will be assigned to the zero counts.
thus, instead of referring to the discounted counts c   , we might describe a smooth-
ing algorithm in terms of a relative discount dc, the ratio of the discounted counts to
the original counts:

50 chapter 3

    id165 language models

dc =

c   
c

now that we have the intuition for the unigram case, let   s smooth our berkeley
restaurant project bigrams. figure 3.5 shows the add-one smoothed counts for the
bigrams in fig. 3.1.

i
6
3
3
1
2
16
3
2

i
want
to
eat
chinese
food
lunch
spend
figure 3.5 add-one smoothed bigram counts for eight of the words (out of v = 1446) in
the berkeley restaurant project corpus of 9332 sentences. previously-zero counts are in gray.

chinese
1
7
3
17
1
2
1
1

spend
3
2
212
1
1
1
1
1

lunch
1
6
7
43
2
1
1
1

want
828
1
1
1
1
1
1
1

food
1
7
1
3
83
5
2
1

to
1
609
5
3
1
16
1
2

eat
10
2
687
1
1
1
1
1

figure 3.6 shows the add-one smoothed probabilities for the bigrams in fig. 3.2.
recall that normal bigram probabilities are computed by normalizing each row of
counts by the unigram count:

c(wn   1wn)
c(wn   1)
the number of total word types in the vocabulary v :

p(wn|wn   1) =

for add-one smoothed bigram counts, we need to augment the unigram count by

p   laplace(wn|wn   1) =

c(wn   1wn) + 1

(cid:80)w (c(wn   1w) + 1)

=

c(wn   1wn) + 1
c(wn   1) +v

thus, each of the unigram counts given in the previous section will need to be
augmented by v = 1446. the result is the smoothed bigram probabilities in fig. 3.6.

(3.22)

(3.23)

i
0.0015
0.0013
0.00078
0.00046
0.0012
0.0063
0.0017
0.0012

spend
0.00075
i
0.00084
want
0.055
to
0.00046
eat
0.00062
chinese
0.00039
food
0.00056
lunch
spend
0.00058
figure 3.6 add-one smoothed bigram probabilities for eight of the words (out of v = 1446) in the berp
corpus of 9332 sentences. previously-zero probabilities are in gray.

want
0.21
0.00042
0.00026
0.00046
0.00062
0.00039
0.00056
0.00058

eat
0.0025
0.00084
0.18
0.00046
0.00062
0.00039
0.00056
0.00058

chinese
0.00025
0.0029
0.00078
0.0078
0.00062
0.00079
0.00056
0.00058

food
0.00025
0.0029
0.00026
0.0014
0.052
0.002
0.0011
0.00058

to
0.00025
0.26
0.0013
0.0014
0.00062
0.0063
0.00056
0.0012

lunch
0.00025
0.0025
0.0018
0.02
0.0012
0.00039
0.00056
0.00058

it is often convenient to reconstruct the count matrix so we can see how much a
smoothing algorithm has changed the original counts. these adjusted counts can be
computed by eq. 3.24. figure 3.7 shows the reconstructed counts.

c   (wn   1wn) =

[c(wn   1wn) + 1]  c(wn   1)

c(wn   1) +v

(3.24)

51

3.4

i
want
to
eat
chinese
food
lunch
spend
figure 3.7 add-one reconstituted counts for eight words (of v = 1446) in the berp corpus
of 9332 sentences. previously-zero counts are in gray.

    smoothing
food
0.64
2.7
0.63
1
8.2
2.2
0.38
0.16

lunch
0.64
2.3
4.4
15
0.2
0.43
0.19
0.16

spend
1.9
0.78
133
0.34
0.098
0.43
0.19
0.16

chinese
0.64
2.7
1.9
5.8
0.098
0.86
0.19
0.16

eat
6.4
0.78
430
0.34
0.098
0.43
0.19
0.16

want
527
0.39
0.63
0.34
0.098
0.43
0.19
0.16

to
0.64
238
3.1
1
0.098
6.9
0.19
0.32

i
3.8
1.2
1.9
0.34
0.2
6.9
0.57
0.32

note that add-one smoothing has made a very big change to the counts. c(want to)
changed from 608 to 238! we can see this in id203 space as well: p(to|want)
decreases from .66 in the unsmoothed case to .26 in the smoothed case. looking at
the discount d (the ratio between new and old counts) shows us how strikingly the
counts for each pre   x word have been reduced; the discount for the bigram want to
is .39, while the discount for chinese food is .10, a factor of 10!

the sharp change in counts and probabilities occurs because too much probabil-

ity mass is moved to all the zeros.

3.4.2 add-k smoothing
one alternative to add-one smoothing is to move a bit less of the id203 mass
from the seen to the unseen events. instead of adding 1 to each count, we add a frac-
tional count k (.5? .05? .01?). this algorithm is therefore called add-k smoothing.

add-k

p   add-k(wn|wn   1) =

c(wn   1wn) + k
c(wn   1) + kv

(3.25)

add-k smoothing requires that we have a method for choosing k; this can be
done, for example, by optimizing on a devset. although add-k is useful for some
tasks (including text classi   cation), it turns out that it still doesn   t work well for
id38, generating counts with poor variances and often inappropriate
discounts (gale and church, 1994).

3.4.3 backoff and interpolation
the discounting we have been discussing so far can help solve the problem of zero
frequency id165s. but there is an additional source of knowledge we can draw on.
if we are trying to compute p(wn|wn   2wn   1) but we have no examples of a particular
trigram wn   2wn   1wn, we can instead estimate its id203 by using the bigram
id203 p(wn|wn   1). similarly, if we don   t have counts to compute p(wn|wn   1),
we can look to the unigram p(wn).
in other words, sometimes using less context is a good thing, helping to general-
ize more for contexts that the model hasn   t learned much about. there are two ways
to use this id165    hierarchy   . in backoff, we use the trigram if the evidence is
suf   cient, otherwise we use the bigram, otherwise the unigram. in other words, we
only    back off    to a lower-order id165 if we have zero evidence for a higher-order
id165. by contrast, in interpolation, we always mix the id203 estimates from
all the id165 estimators, weighing and combining the trigram, bigram, and unigram
counts.

backoff

interpolation

52 chapter 3

    id165 language models
in simple linear interpolation, we combine different order id165s by linearly in-
terpolating all the models. thus, we estimate the trigram id203 p(wn|wn   2wn   1)
by mixing together the unigram, bigram, and trigram probabilities, each weighted
by a    :

  p(wn|wn   2wn   1) =   1p(wn|wn   2wn   1)

+  2p(wn|wn   1)
+  3p(wn)

(3.26)

(3.27)

such that the    s sum to 1:

  i = 1

(cid:88)i

in a slightly more sophisticated version of linear interpolation, each    weight is
computed by conditioning on the context. this way, if we have particularly accurate
counts for a particular bigram, we assume that the counts of the trigrams based on
this bigram will be more trustworthy, so we can make the    s for those trigrams
higher and thus give that trigram more weight in the interpolation. equation 3.28
shows the equation for interpolation with context-conditioned weights:

  p(wn|wn   2wn   1) =   1(wn   1
+  2(wn   1
+   3(wn   1

n   2)p(wn|wn   2wn   1)
n   2)p(wn|wn   1)
n   2)p(wn)

(3.28)

how are these    values set? both the simple interpolation and conditional inter-
polation    s are learned from a held-out corpus. a held-out corpus is an additional
training corpus that we use to set hyperparameters like these    values, by choosing
the    values that maximize the likelihood of the held-out corpus. that is, we    x
the id165 probabilities and then search for the    values that   when plugged into
eq. 3.26   give us the highest id203 of the held-out set. there are various ways
to    nd this optimal set of    s. one way is to use the em algorithm, an iterative
learning algorithm that converges on locally optimal    s (jelinek and mercer, 1980).
in a backoff id165 model, if the id165 we need has zero counts, we approxi-
mate it by backing off to the (n-1)-gram. we continue backing off until we reach a
history that has some counts.

in order for a backoff model to give a correct id203 distribution, we have
to discount the higher-order id165s to save some id203 mass for the lower
order id165s. just as with add-one smoothing, if the higher-order id165s aren   t
discounted and we just used the undiscounted id113 id203, then as soon as we
replaced an id165 which has zero id203 with a lower-order id165, we would
be adding id203 mass, and the total id203 assigned to all possible strings
by the language model would be greater than 1! in addition to this explicit discount
factor, we   ll need a function    to distribute this id203 mass to the lower order
id165s.

this kind of backoff with discounting is also called katz backoff. in katz back-
off we rely on a discounted id203 p    if we   ve seen this id165 before (i.e., if
we have non-zero counts). otherwise, we recursively back off to the katz probabil-
ity for the shorter-history (n-1)-gram. the id203 for a backoff id165 pbo is

held-out

discount

katz backoff

thus computed as follows:

3.5

    kneser-ney smoothing

53

pbo(wn|wn   1

n   n+1) =         

p   (wn|wn   1
  (wn   1

n   n+1),

n   n+1)pbo(wn|wn   1

n   n+2),

n   n+1) > 0

if c(wn
otherwise.

good-turing

(3.29)
katz backoff is often combined with a smoothing method called good-turing.
the combined good-turing backoff algorithm involves quite detailed computation
for estimating the good-turing smoothing and the p    and    values.

3.5 kneser-ney smoothing

kneser-ney

one of the most commonly used and best performing id165 smoothing methods
is the interpolated kneser-ney algorithm (kneser and ney 1995, chen and good-
man 1998).

kneser-ney has its roots in a method called absolute discounting. recall that
discounting of the counts for frequent id165s is necessary to save some id203
mass for the smoothing algorithm to distribute to the unseen id165s.

to see this, we can use a clever idea from church and gale (1991). consider
an id165 that has count 4. we need to discount this count by some amount. but
how much should we discount it? church and gale   s clever idea was to look at a
held-out corpus and just see what the count is for all those bigrams that had count
4 in the training set. they computed a bigram grammar from 22 million words of
ap newswire and then checked the counts of each of these bigrams in another 22
million words. on average, a bigram that occurred 4 times in the    rst 22 million
words occurred 3.23 times in the next 22 million words. the following table from
church and gale (1991) shows these counts for bigrams with c from 0 to 9:

bigram count in bigram count in

training set heldout set
0 0.0000270
1 0.448
2 1.25
3 2.24
4 3.23
5 4.21
6 5.23
7 6.21
8 7.21
9 8.26

figure 3.8 for all bigrams in 22 million words of ap newswire of count 0, 1, 2,...,9, the
counts of these bigrams in a held-out corpus also of 22 million words.

absolute
discounting

the astute reader may have noticed that except for the held-out counts for 0
and 1, all the other bigram counts in the held-out set could be estimated pretty well
by just subtracting 0.75 from the count in the training set! absolute discounting
formalizes this intuition by subtracting a    xed (absolute) discount d from each count.
the intuition is that since we have good estimates already for the very high counts, a
small discount d won   t affect them much. it will mainly modify the smaller counts,

54 chapter 3

    id165 language models

for which we don   t necessarily trust the estimate anyway, and fig. 3.8 suggests that
in practice this discount is actually a good one for bigrams with counts 2 through 9.
the equation for interpolated absolute discounting applied to bigrams:

c(wi   1wi)    d
(cid:80)vc(wi   1 v)

pabsolutediscounting(wi|wi   1) =

+    (wi   1)p(wi)

(3.30)

the    rst term is the discounted bigram, and the second term is the unigram with
an interpolation weight    . we could just set all the d values to .75, or we could keep
a separate discount value of 0.5 for the bigrams with counts of 1.

kneser-ney discounting (kneser and ney, 1995) augments absolute discount-
ing with a more sophisticated way to handle the lower-order unigram distribution.
consider the job of predicting the next word in this sentence, assuming we are inter-
polating a bigram and a unigram model.

i can   t see without my reading

.

the word glasses seems much more likely to follow here than, say, the word
kong, so we   d like our unigram model to prefer glasses. but in fact it   s kong that is
more common, since hong kong is a very frequent word. a standard unigram model
will assign kong a higher id203 than glasses. we would like to capture the
intuition that although kong is frequent, it is mainly only frequent in the phrase hong
kong, that is, after the word hong. the word glasses has a much wider distribution.
in other words, instead of p(w), which answers the question    how likely is
w?   , we   d like to create a unigram model that we might call pcontinuation, which
answers the question    how likely is w to appear as a novel continuation?   . how can
we estimate this id203 of seeing the word w as a novel continuation, in a new
unseen context? the kneser-ney intuition is to base our estimate of pcontinuation
on the number of different contexts word w has appeared in, that is, the number of
bigram types it completes. every bigram type was a novel continuation the    rst time
it was seen. we hypothesize that words that have appeared in more contexts in the
past are more likely to appear in some new context as well. the number of times a
word w appears as a novel continuation can be expressed as:

pcontinuation(w)     |{v : c(vw) > 0}|

to turn this count into a id203, we normalize by the total number of word
bigram types. in summary:

pcontinuation(w) =

|{v : c(vw) > 0}|

|{(u(cid:48),w(cid:48)) : c(u(cid:48)w(cid:48)) > 0}|

an alternative metaphor for an equivalent formulation is to use the number of

word types seen to precede w (eq. 3.31 repeated):

pcontinuation(w)     |{v : c(vw) > 0}|

normalized by the number of words preceding all words, as follows:

pcontinuation(w) = |{v : c(vw) > 0}|
(cid:80)w(cid:48) |{v : c(vw(cid:48)) > 0}|

a frequent word (kong) occurring in only one context (hong) will have a low

continuation id203.

(3.31)

(3.32)

(3.33)

(3.34)

interpolated
kneser-ney

the    nal equation for interpolated kneser-ney smoothing for bigrams is then:

3.6

    the web and stupid backoff

55

max(c(wi   1wi)    d,0)

pkn(wi|wi   1) =
the    is a normalizing constant that is used to distribute the id203 mass

+    (wi   1)pcontinuation(wi)

c(wi   1)

(3.35)

we   ve discounted.:

d

   (wi   1) =
d(cid:80)
v c(wi   1v) is the normalized discount. the second term |{w : c(wi   1w) > 0}|

(cid:80)vc(wi   1v)|{w : c(wi   1w) > 0}|

(3.36)

the    rst term

is the number of word types that can follow wi   1 or, equivalently, the number of
word types that we discounted; in other words, the number of times we applied the
normalized discount.

the general recursive formulation is as follows:

max(ckn(wi

i   n+1) =

i   n+1)    d,0)
(cid:80)v ckn(wi   1
i   n+1v)

pkn(wi|wi   1
i   n+2) (3.37)
where the de   nition of the count ckn depends on whether we are counting the
highest-order id165 being interpolated (for example trigram if we are interpolating
trigram, bigram, and unigram) or one of the lower-order id165s (bigram or unigram
if we are interpolating trigram, bigram, and unigram):

i   n+1)pkn(wi|wi   1

+    (wi   1

ckn(  ) =(cid:26) count(  )

continuationcount(  )

for the highest order

for lower orders

(3.38)

the continuation count is the number of unique single word contexts for   .
distribution, where the parameter   is the empty string:

at the termination of the recursion, unigrams are interpolated with the uniform

pkn(w) =

max(ckn(w)    d,0)
(cid:80)w(cid:48) ckn(w(cid:48))

+    ( )

1
v

(3.39)

if we want to include an unknown word <unk>, it   s just included as a regular vo-
cabulary entry with count zero, and hence its id203 will be a lambda-weighted
uniform distribution    ( )
v .

the best-performing version of kneser-ney smoothing is called modi   ed kneser-

ney smoothing, and is due to chen and goodman (1998). rather than use a single
   xed discount d, modi   ed kneser-ney uses three different discounts d1, d2, and
d3+ for id165s with counts of 1, 2 and three or more, respectively. see chen and
goodman (1998, p. 19) or hea   eld et al. (2013) for the details.

modi   ed
kneser-ney

3.6 the web and stupid backoff

by using text from the web, it is possible to build extremely large language mod-
els. in 2006 google released a very large set of id165 counts, including id165s
(1-grams through 5-grams) from all the    ve-word sequences that appear at least
40 times from 1,024,908,267,229 words of running text on the web; this includes

56 chapter 3

    id165 language models

1,176,470,663    ve-word sequences using over 13 million unique words types (franz
and brants, 2006). some examples:

4-gram
serve as the incoming
serve as the incubator
serve as the independent
serve as the index
serve as the indication
serve as the indicator
serve as the indicators
serve as the indispensable
serve as the indispensible
serve as the individual

count
92
99
794
223
72
120
45
111
40
234

ef   ciency considerations are important when building language models that use
such large sets of id165s. rather than store each word as a string, it is generally
represented in memory as a 64-bit hash number, with the words themselves stored
on disk. probabilities are generally quantized using only 4-8 bits (instead of 8-byte
   oats), and id165s are stored in reverse tries.

id165s can also be shrunk by pruning, for example only storing id165s with
counts greater than some threshold (such as the count threshold of 40 used for the
google id165 release) or using id178 to prune less-important id165s (stolcke,
1998). another option is to build approximate language models using techniques
like bloom    lters (talbot and osborne 2007, church et al. 2007). finally, ef   -
cient language model toolkits like kenlm (hea   eld 2011, hea   eld et al. 2013) use
sorted arrays, ef   ciently combine probabilities and backoffs in a single value, and
use merge sorts to ef   ciently build the id203 tables in a minimal number of
passes through a large corpus.

although with these toolkits it is possible to build web-scale language models
using full kneser-ney smoothing, brants et al. (2007) show that with very large lan-
guage models a much simpler algorithm may be suf   cient. the algorithm is called
stupid backoff. stupid backoff gives up the idea of trying to make the language
model a true id203 distribution. there is no discounting of the higher-order
probabilities. if a higher-order id165 has a zero count, we simply backoff to a
lower order id165, weighed by a    xed (context-independent) weight. this algo-
rithm does not produce a id203 distribution, so we   ll follow brants et al. (2007)
in referring to it as s:

bloom    lters

stupid backoff

s(wi|wi   1

i   k+1) =         

count(wi
i   k+1)
if count(wi
count(wi   1
i   k+1)
   s(wi|wi   1
i   k+2) otherwise

i   k+1) > 0

the backoff terminates in the unigram, which has id203 s(w) = count(w)
et al. (2007)    nd that a value of 0.4 worked well for    .

n

(3.40)

. brants

3.7 advanced: perplexity   s relation to id178

we introduced perplexity in section 3.2.1 as a way to evaluate id165 models on
a test set. a better id165 model is one that assigns a higher id203 to the

3.7

    advanced: perplexity   s relation to id178

57

id178

test data, and perplexity is a normalized version of the id203 of the test set.
the perplexity measure actually arises from the information-theoretic concept of
cross-id178, which explains otherwise mysterious properties of perplexity (why
the inverse id203, for example?) and its relationship to id178. id178 is a
measure of information. given a random variable x ranging over whatever we are
predicting (words, letters, parts of speech, the set of which we   ll call   ) and with a
particular id203 function, call it p(x), the id178 of the random variable x is:

h(x) =    (cid:88)x     

p(x)log2 p(x)

(3.41)

the log can, in principle, be computed in any base. if we use log base 2, the

resulting value of id178 will be measured in bits.

one intuitive way to think about id178 is as a lower bound on the number of
bits it would take to encode a certain decision or piece of information in the optimal
coding scheme.

consider an example from the standard id205 textbook cover and
thomas (1991). imagine that we want to place a bet on a horse race but it is too
far to go all the way to yonkers racetrack, so we   d like to send a short message to
the bookie to tell him which of the eight horses to bet on. one way to encode this
message is just to use the binary representation of the horse   s number as the code;
thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded
as 000. if we spend the whole day betting and each horse is coded with 3 bits, on
average we would be sending 3 bits per race.

can we do better? suppose that the spread is the actual distribution of the bets

placed and that we represent it as the prior id203 of each horse as follows:

horse 1 1
horse 5 1
2
64
horse 2 1
horse 6 1
4
64
horse 3 1
horse 7 1
64
8
horse 4 1
16 horse 8 1
64

the id178 of the random variable x that ranges over horses gives us a lower

bound on the number of bits and is

p(i)log p(i)

i=8(cid:88)i=1

h(x) =    
2 log 1
=     1
= 2 bits

2    1

4 log 1

4    1

8 log 1

8    1

16 log 1

16   4( 1

64 log 1
64 )

(3.42)
a code that averages 2 bits per race can be built with short encodings for more
probable horses, and longer encodings for less probable horses. for example, we
could encode the most likely horse with the code 0, and the remaining horses as 10,
then 110, 1110, 111100, 111101, 111110, and 111111.

what if the horses are equally likely? we saw above that if we used an equal-
length binary code for the horse numbers, each horse took 3 bits to code, so the
average was 3.
is the id178 the same? in this case each horse would have a
id203 of 1

8. the id178 of the choice of horses is then

h(x) =    

1
8

i=8(cid:88)i=1

log

1
8

=    log

1
8

= 3 bits

(3.43)

58 chapter 3

    id165 language models
until now we have been computing the id178 of a single variable. but most of
what we will use id178 for involves sequences. for a grammar, for example, we
will be computing the id178 of some sequence of words w = {w0,w1,w2, . . . ,wn}.
one way to do this is to have a variable that ranges over sequences of words. for
example we can compute the id178 of a random variable that ranges over all    nite
sequences of words of length n in some language l as follows:

h(w1,w2, . . . ,wn) =    (cid:88)w n

1    l

p(w n

1 )log p(w n
1 )

(3.44)

id178 rate

we could de   ne the id178 rate (we could also think of this as the per-word

id178) as the id178 of this sequence divided by the number of words:

1
n

h(w n

1 ) =    

1

n (cid:88)w n

1    l

p(w n

1 )log p(w n
1 )

(3.45)

but to measure the true id178 of a language, we need to consider sequences of
in   nite length. if we think of a language as a stochastic process l that produces a
sequence of words, and allow w to represent the sequence of words w1, . . . ,wn, then
l   s id178 rate h(l) is de   ned as

1
h(l) = lim
n
n      
=     lim
n      

h(w1,w2, . . . ,wn)
1

n(cid:88)w   l

p(w1, . . . ,wn)log p(w1, . . . ,wn)

(3.46)

the shannon-mcmillan-breiman theorem (algoet and cover 1988, cover and
thomas 1991) states that if the language is regular in certain ways (to be exact, if it
is both stationary and ergodic),

h(l) = lim
n         

1
n

log p(w1w2 . . .wn)

(3.47)

that is, we can take a single sequence that is long enough instead of summing
over all possible sequences. the intuition of the shannon-mcmillan-breiman the-
orem is that a long-enough sequence of words will contain in it many other shorter
sequences and that each of these shorter sequences will reoccur in the longer se-
quence according to their probabilities.

a stochastic process is said to be stationary if the probabilities it assigns to a
sequence are invariant with respect to shifts in the time index. in other words, the
id203 distribution for words at time t is the same as the id203 distribution
at time t + 1. markov models, and hence id165s, are stationary. for example, in
a bigram, pi is dependent only on pi   1. so if we shift our time index by x, pi+x is
still dependent on pi+x   1. but natural language is not stationary, since as we show
in chapter 10, the id203 of upcoming words can be dependent on events that
were arbitrarily distant and time dependent. thus, our statistical models only give
an approximation to the correct distributions and entropies of natural language.

to summarize, by making some incorrect but convenient simplifying assump-
tions, we can compute the id178 of some stochastic process by taking a very long
sample of the output and computing its average log id203.

now we are ready to introduce cross-id178. the cross-id178 is useful when
it

we don   t know the actual id203 distribution p that generated some data.

stationary

cross-id178

3.7

    advanced: perplexity   s relation to id178

59

allows us to use some m, which is a model of p (i.e., an approximation to p). the
cross-id178 of m on p is de   ned by

h(p,m) = lim
n         

1

n(cid:88)w   l

p(w1, . . . ,wn)logm(w1, . . . ,wn)

(3.48)

that is, we draw sequences according to the id203 distribution p, but sum

the log of their probabilities according to m.

again, following the shannon-mcmillan-breiman theorem, for a stationary er-

godic process:

h(p,m) = lim
n         

1
n

logm(w1w2 . . .wn)

(3.49)

this means that, as for id178, we can estimate the cross-id178 of a model
m on some distribution p by taking a single sequence that is long enough instead of
summing over all possible sequences.

what makes the cross-id178 useful is that the cross-id178 h(p,m) is an up-

per bound on the id178 h(p). for any model m:

h(p)     h(p,m)

(3.50)

this means that we can use some simpli   ed model m to help estimate the true en-
tropy of a sequence of symbols drawn according to id203 p. the more accurate
m is, the closer the cross-id178 h(p,m) will be to the true id178 h(p). thus,
the difference between h(p,m) and h(p) is a measure of how accurate a model is.
between two models m1 and m2, the more accurate model will be the one with the
lower cross-id178. (the cross-id178 can never be lower than the true id178, so
a model cannot err by underestimating the true id178.)

we are    nally ready to see the relation between perplexity and cross-id178 as
we saw it in eq. 3.49. cross-id178 is de   ned in the limit, as the length of the
observed word sequence goes to in   nity. we will need an approximation to cross-
id178, relying on a (suf   ciently long) sequence of    xed length. this approxima-
tion to the cross-id178 of a model m = p(wi|wi   n+1...wi   1) on a sequence of
words w is

h(w ) =    

1
n

logp(w1w2 . . .wn)

(3.51)

perplexity

the perplexity of a model p on a sequence of words w is now formally de   ned as
the exp of this cross-id178:

perplexity(w ) = 2h(w )

= p(w1w2 . . .wn)    1

n

1

p(w1w2 . . .wn)

= n(cid:115)
= n(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:89)i=1

1

p(wi|w1 . . .wi   1)

(3.52)

60 chapter 3

    id165 language models

3.8 summary

this chapter introduced id38 and the id165, one of the most widely
used tools in language processing.

sequence of words, and to predict a word from preceding words.

    language models offer a way to assign a id203 to a sentence or other
    id165s are markov models that estimate words from a    xed window of pre-
vious words. id165 probabilities can be estimated by counting in a corpus
and normalizing (the maximum likelihood estimate).

cally using perplexity.

    id165 language models are evaluated extrinsically in some task, or intrinsi-
    the perplexity of a test set according to a language model is the geometric
    smoothing algorithms provide a more sophisticated way to estimate the prob-
ability of id165s. commonly used smoothing algorithms for id165s rely on
lower-order id165 counts through backoff or interpolation.

mean of the inverse test set id203 computed by the model.

tribution.

    both backoff and interpolation require discounting to create a id203 dis-
    kneser-ney smoothing makes use of the id203 of a word being a novel
continuation. the interpolated kneser-ney smoothing algorithm mixes a
discounted id203 with a lower-order continuation id203.

bibliographical and historical notes

the underlying mathematics of the id165 was    rst proposed by markov (1913),
who used what are now called markov chains (bigrams and trigrams) to predict
whether an upcoming letter in pushkin   s eugene onegin would be a vowel or a con-
sonant. markov classi   ed 20,000 letters as v or c and computed the bigram and
trigram id203 that a given letter would be a vowel given the previous one or
two letters. shannon (1948) applied id165s to compute approximations to english
word sequences. based on shannon   s work, markov models were commonly used in
engineering, linguistic, and psychological work on modeling word sequences by the
1950s. in a series of extremely in   uential papers starting with chomsky (1956) and
including chomsky (1957) and miller and chomsky (1963), noam chomsky argued
that       nite-state markov processes   , while a possibly useful engineering heuristic,
were incapable of being a complete cognitive model of human grammatical knowl-
edge. these arguments led many linguists and computational linguists to ignore
work in statistical modeling for decades.

the resurgence of id165 models came from jelinek and colleagues at the ibm
thomas j. watson research center, who were in   uenced by shannon, and baker
at cmu, who was in   uenced by the work of baum and colleagues. independently
these two labs successfully used id165s in their id103 systems (baker 1990,
jelinek 1976, baker 1975, bahl et al. 1983, jelinek 1990). a trigram model was used
in the ibm tangora id103 system in the 1970s, but the idea was not
written up until later.

add-one smoothing derives from laplace   s 1812 law of succession and was    rst
applied as an engineering solution to the zero-frequency problem by jeffreys (1948)

exercises

61

based on an earlier add-k suggestion by johnson (1932). problems with the add-
one algorithm are summarized in gale and church (1994).

a wide variety of different id38 and smoothing techniques were
proposed in the 80s and 90s, including good-turing discounting      rst applied to
the id165 smoothing at ibm by katz (n  adas 1984, church and gale 1991)   
witten-bell discounting (witten and bell, 1991), and varieties of class-based n-
gram models that used information about word classes.

starting in the late 1990s, chen and goodman produced a highly in   uential
series of papers with a comparison of different language models (chen and good-
man 1996, chen and goodman 1998, chen and goodman 1999, goodman 2006).
they performed a number of carefully controlled experiments comparing differ-
ent discounting algorithms, cache models, class-based models, and other language
model parameters. they showed the advantages of modi   ed interpolated kneser-
ney, which has since become the standard baseline for id38, espe-
cially because they showed that caches and class-based models provided only minor
additional improvement. these papers are recommended for any reader with further
interest in id38.

two commonly used toolkits for building language models are srilm (stolcke,
2002) and kenlm (hea   eld 2011, hea   eld et al. 2013). both are publicly available.
srilm offers a wider range of options and types of discounting, while kenlm is
optimized for speed and memory size, making it possible to build web-scale lan-
guage models.

the highest accuracy language models at the time of this writing make use of
neural nets. the problem with standard language models is that the number of pa-
rameters increases exponentially as the id165 order increases, and id165s have no
way to generalize from training to test set. neural networks instead project words
into a continuous space in which words with similar contexts have similar represen-
tations. both feedforward nets bengio et al. 2006, schwenk 2007 and recurrent
nets (mikolov, 2012) are used.

other important classes of language models are maximum id178 language
models (rosenfeld, 1996), based on id28 classi   ers that use lots of
features to help predict upcoming words. these classi   ers can use the standard
features presented in this chapter (i.e., the previous words) but also lots of other
useful predictors, as can other kinds of discriminative language models (roark et al.,
2007). we   ll introduce id28 id38 when we introduce
classi   cation in chapter 4.

another important technique is language model adaptation, where we want to
combine data from multiple domains (for example we might have less in-domain
training data but more general data that we then need to adapt) (bulyko et al. 2003,
bacchiani et al. 2004, bellegarda 2004, bacchiani et al. 2006, hsu 2007, liu et al. 2013).

class-based
id165

neural nets

maximum
id178

adaptation

exercises

3.1 write out the equation for trigram id203 estimation (modifying eq. 3.11).
now write out all the non-zero trigram probabilities for the i am sam corpus
on page 41.

3.2 calculate the id203 of the sentence i want chinese food. give two
probabilities, one using fig. 3.2, and another using the add-1 smoothed table
in fig. 3.6.

62 chapter 3

    id165 language models

3.3 which of the two probabilities you computed in the previous exercise is higher,

unsmoothed or smoothed? explain why.

3.4 we are given the following corpus, modi   ed from the one in the chapter:

<s> i am sam </s>
<s> sam i am </s>
<s> i am sam </s>
<s> i do not like green eggs and sam </s>

using a bigram language model with add-one smoothing, what is p(sam |
am)? include <s> and </s> in your counts just like any other token.
suppose we didn   t use the end-symbol </s>. train an unsmoothed bigram
grammar on the following training corpus without using the end-symbol </s>:

3.5

<s> a b
<s> b b
<s> b a
<s> a a

3.6

demonstrate that your bigram model does not assign a single id203 dis-
tribution across all sentence lengths by showing that the sum of the id203
of the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the
sum of the id203 of all possible 3 word sentences over the alphabet {a,b}
is also 1.0.
suppose we train a trigram language model with add-one smoothing on a
given corpus. the corpus contains v word types. express a formula for esti-
mating p(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),
in terms of various id165 counts and v. use the notation c(w1,w2,w3) to
denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and
so on for bigrams and unigrams.

3.7 we are given the following corpus, modi   ed from the one in the chapter:

<s> i am sam </s>
<s> sam i am </s>
<s> i am sam </s>
<s> i do not like green eggs and sam </s>
if we use linear interpolation smoothing between a maximum-likelihood bi-
gram model and a maximum-likelihood unigram model with   1 = 1
2 and   2 =
1
2, what is p(sam|am)? include <s> and </s>\verb in your counts just like
any other token.

3.8 write a program to compute unsmoothed unigrams and bigrams.
3.9 run your id165 program on two different small corpora of your choice (you
might use email text or newsgroups). now compare the statistics of the two
corpora. what are the differences in the most common unigrams between the
two? how about interesting differences in bigrams?

3.10 add an option to your program to generate random sentences.
3.11 add an option to your program to compute the perplexity of a test set.

chapter

4 naive bayes and sentiment

classi   cation

classi   cation lies at the heart of both human and machine intelligence. deciding
what letter, word, or image has been presented to our senses, recognizing faces
or voices, sorting mail, assigning grades to homeworks; these are all examples of
assigning a category to an input. the potential challenges of this task are highlighted
by the fabulist jorge luis borges (1964), who imagined classifying animals into:

(a) those that belong to the emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classi   cation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very    ne camel   s hair brush, (l) others, (m) those that have just broken
a    ower vase, (n) those that resemble    ies from a distance.

many language processing tasks involve classi   cation, although luckily our classes

are much easier to de   ne than those of borges. in this chapter we introduce the naive
bayes algorithm and apply it to text categorization, the task of assigning a label or
category to an entire text or document.

we focus on one common text categorization task, id31, the ex-
traction of sentiment, the positive or negative orientation that a writer expresses
toward some object. a review of a movie, book, or product on the web expresses the
author   s sentiment toward the product, while an editorial or political text expresses
sentiment toward a candidate or political action. extracting consumer or public sen-
timent is thus relevant for    elds from marketing to politics.

the simplest version of id31 is a binary classi   cation task, and the
words of the review provide excellent cues. consider, for example, the following
phrases extracted from positive and negative reviews of movies and restaurants,.
words like great, richly, awesome, and pathetic, and awful and ridiculously are very
informative cues:

+ ...zany characters and richly applied satire, and some great plot twists
    it was pathetic. the worst part about it was the boxing scenes...
+ ...awesome caramel sauce and sweet toasty almonds. i love this place!
    ...awful pizza and ridiculously overpriced...

spam detection is another important commercial application, the binary clas-
si   cation task of assigning an email to one of the two classes spam or not-spam.
many lexical and other features can be used to perform this classi   cation. for ex-
ample you might quite reasonably be suspicious of an email containing phrases like
   online pharmaceutical    or    without any cost    or    dear winner   .

another thing we might want to know about a text is the language it   s written
in. texts on social media, for example, can be in any number of languages and we   ll
need to apply different processing. the task of language id is thus the    rst step
in most language processing pipelines. related tasks like determining a text   s au-
thor, (authorship attribution), or author characteristics like gender, age, and native

text
categorization

sentiment
analysis

spam detection

language id

authorship
attribution

64 chapter 4

    naive bayes and sentiment classification

supervised
machine
learning

language are text classi   cation tasks that are also relevant to the digital humanities,
social sciences, and forensic linguistics.

finally, one of the oldest tasks in text classi   cation is assigning a library sub-
ject category or topic label to a text. deciding whether a research paper concerns
epidemiology or instead, perhaps, embryology, is an important component of infor-
mation retrieval. various sets of subject categories exist, such as the mesh (medical
subject headings) thesaurus. in fact, as we will see, subject category classi   cation
is the task for which the naive bayes algorithm was invented in 1961.

classi   cation is essential for tasks below the level of the document as well.
we   ve already seen period disambiguation (deciding if a period is the end of a sen-
tence or part of a word), and word id121 (deciding if a character should be
a word boundary). even id38 can be viewed as classi   cation: each
word can be thought of as a class, and so predicting the next word is classifying the
context-so-far into a class for each next word. a part-of-speech tagger (chapter 8)
classi   es each occurrence of a word in a sentence as, e.g., a noun or a verb.

the goal of classi   cation is to take a single observation, extract some useful
features, and thereby classify the observation into one of a set of discrete classes.
one method for classifying text is to use hand-written rules. there are many areas
of language processing where hand-written rule-based classi   ers constitute a state-
of-the-art system, or at least part of it.

rules can be fragile, however, as situations or data change over time, and for
some tasks humans aren   t necessarily good at coming up with the rules. most cases
of classi   cation in language processing are instead done via supervised machine
learning, and this will be the subject of the remainder of this chapter. in supervised
learning, we have a data set of input observations, each associated with some correct
output (a    supervision signal   ). the goal of the algorithm is to learn how to map
from a new observation to a correct output.

formally, the task of supervised classi   cation is to take an input x and a    xed
set of output classes y = y1,y2, ...,ym and return a predicted class y     y . for text
classi   cation, we   ll sometimes talk about c (for    class   ) instead of y as our output
variable, and d (for    document   ) instead of x as our input variable. in the supervised
situation we have a training set of n documents that have each been hand-labeled
with a class: (d1,c1), ...., (dn,cn). our goal is to learn a classi   er that is capable of
mapping from a new document d to its correct class c     c. a probabilistic classi   er
additionally will tell us the id203 of the observation being in the class. this
full distribution over the classes can be useful information for downstream decisions;
avoiding making discrete decisions early on can be useful when combining systems.
many kinds of machine learning algorithms are used to build classi   ers. this
chapter introduces naive bayes; the following one introduces id28.
these exemplify two ways of doing classi   cation. generative classi   ers like naive
bayes build a model of how a class could generate some input data. given an ob-
servation, they return the class most likely to have generated the observation. dis-
criminative classi   ers like id28 instead learn what features from the
input are most useful to discriminate between the different possible classes. while
discriminative systems are often more accurate and hence more commonly used,
generative classi   ers still have a role.

4.1 naive bayes classi   ers

4.1

    naive bayes classifiers

65

naive bayes
classi   er

bag-of-words

in this section we introduce the multinomial naive bayes classi   er, so called be-
cause it is a bayesian classi   er that makes a simplifying (naive) assumption about
how the features interact.

the intuition of the classi   er is shown in fig. 4.1. we represent a text document
as if it were a bag-of-words, that is, an unordered set of words with their position
ignored, keeping only their frequency in the document. in the example in the    gure,
instead of representing the word order in all the phrases like    i love this movie    and
   i would recommend it   , we simply note that the word i occurred 5 times in the
entire excerpt, the word it 6 times, the words love, recommend, and movie once, and
so on.

figure 4.1
words is ignored (the bag of words assumption) and we make use of the frequency of each word.

intuition of the multinomial naive bayes classi   er applied to a movie review. the position of the

naive bayes is a probabilistic classi   er, meaning that for a document d, out of
all classes c     c the classi   er returns the class   c which has the maximum posterior
id203 given the document. in eq. 4.1 we use the hat notation    to mean    our
estimate of the correct class   .

  

  c = argmax

c   c

p(c|d)

(4.1)

bayesian
id136

this idea of bayesian id136 has been known since the work of bayes (1763),
and was    rst applied to text classi   cation by mosteller and wallace (1964). the in-
tuition of bayesian classi   cation is to use bayes    rule to transform eq. 4.1 into other
probabilities that have some useful properties. bayes    rule is presented in eq. 4.2;
it gives us a way to break down any id155 p(x|y) into three other

ititititititiiiiiloverecommendmoviethethethethetototoandandandseenseenyetwouldwithwhowhimsicalwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainabouti love this movie! it's sweet, but with satirical humor. the dialogue is great and the adventure scenes are fun... it manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. i would recommend it to just about anyone. i've seen it several times, and i'm always happy to see it again whenever i have a friend who hasn't seen it yet!it ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat   6 54332111111111111   66 chapter 4

    naive bayes and sentiment classification

probabilities:

p(x|y) =

p(y|x)p(x)

p(y)

we can then substitute eq. 4.2 into eq. 4.1 to get eq. 4.3:
p(d|c)p(c)

  c = argmax

p(c|d) = argmax
c   c

c   c

p(d)

(4.2)

(4.3)

we can conveniently simplify eq. 4.3 by dropping the denominator p(d). this
is possible because we will be computing p(d|c)p(c)
for each possible class. but p(d)
doesn   t change for each class; we are always asking about the most likely class for
the same document d, which must have the same id203 p(d). thus, we can
choose the class that maximizes this simpler formula:

p(d)

  c = argmax

p(c|d) = argmax
c   c

c   c

p(d|c)p(c)

(4.4)

we thus compute the most probable class   c given some document d by choosing
the class which has the highest product of two probabilities: the prior id203
of the class p(c) and the likelihood of the document p(d|c):

prior
id203
likelihood

naive bayes
assumption

without loss of generalization, we can represent a document d as a set of features

f1, f2, ..., fn:

  c = argmax

c   c

likelihood
(cid:122) (cid:125)(cid:124) (cid:123)
p(d|c)

prior

(cid:122)(cid:125)(cid:124)(cid:123)p(c)

  c = argmax

c   c

likelihood

(cid:123)
(cid:122)
p( f1, f2, ...., fn|c)

(cid:125)(cid:124)

prior

(cid:122)(cid:125)(cid:124)(cid:123)p(c)

unfortunately, eq. 4.6 is still too hard to compute directly: without some sim-
plifying assumptions, estimating the id203 of every possible combination of
features (for example, every possible set of words and positions) would require huge
numbers of parameters and impossibly large training sets. naive bayes classi   ers
therefore make two simplifying assumptions.

the    rst is the bag of words assumption discussed intuitively above: we assume
position doesn   t matter, and that the word    love    has the same effect on classi   cation
whether it occurs as the 1st, 20th, or last word in the document. thus we assume
that the features f1, f2, ..., fn only encode word identity and not position.

the second is commonly called the naive bayes assumption: this is the condi-
tional independence assumption that the probabilities p( fi|c) are independent given
the class c and hence can be    naively    multiplied as follows:

p( f1, f2, ...., fn|c) = p( f1|c)   p( f2|c)   ...   p( fn|c)

the    nal equation for the class chosen by a naive bayes classi   er is thus:

cnb = argmax

c   c

p(c)(cid:89)f   f

p( f|c)

to apply the naive bayes classi   er to text, we need to consider word positions,

by simply walking an index through every word position in the document:

(4.5)

(4.6)

(4.7)

(4.8)

4.2

    training the naive bayes classifier

67

positions     all word positions in test document

cnb = argmax

c   c

p(c) (cid:89)i   positions

p(wi|c)

(4.9)

naive bayes calculations, like calculations for id38, are done in
log space, to avoid under   ow and increase speed. thus eq. 4.9 is generally instead
expressed as

cnb = argmax

c   c

logp(c) + (cid:88)i   positions

logp(wi|c)

(4.10)

by considering features in log space eq. 4.10 computes the predicted class as
a linear function of input features. classi   ers that use a linear combination of
the inputs to make a classi   cation decision    like naive bayes and also logistic
regression    are called linear classi   ers.

linear
classi   ers

4.2 training the naive bayes classi   er

how can we learn the probabilities p(c) and p( fi|c)? let   s    rst consider the max-
imum likelihood estimate. we   ll simply use the frequencies in the data. for the
document prior p(c) we ask what percentage of the documents in our training set
are in each class c. let nc be the number of documents in our training data with
class c and ndoc be the total number of documents. then:

  p(c) =

nc
ndoc

(4.11)

to learn the id203 p( fi|c), we   ll assume a feature is just the existence of a
word in the document   s bag of words, and so we   ll want p(wi|c), which we compute
as the fraction of times the word wi appears among all words in all documents of
topic c. we    rst concatenate all documents with category c into one big    category
c    text. then we use the frequency of wi in this concatenated document to give a
maximum likelihood estimate of the id203:

  p(wi|c) =

count(wi,c)

(cid:80)w   v count(w,c)

(4.12)

here the vocabulary v consists of the union of all the word types in all classes,

not just the words in one class c.

there is a problem, however, with maximum likelihood training. imagine we
are trying to estimate the likelihood of the word    fantastic    given class positive, but
suppose there are no training documents that both contain the word    fantastic    and
are classi   ed as positive. perhaps the word    fantastic    happens to occur (sarcasti-
cally?) in the class negative. in such a case the id203 for this feature will be
zero:

68 chapter 4

    naive bayes and sentiment classification

  p(   fantastic   |positive) =

count(   fantastic   ,positive)

(cid:80)w   v count(w,positive)

= 0

(4.13)

but since naive bayes naively multiplies all the feature likelihoods together, zero
probabilities in the likelihood term for any class will cause the id203 of the
class to be zero, no matter the other evidence!

the simplest solution is the add-one (laplace) smoothing introduced in chap-
ter 3. while laplace smoothing is usually replaced by more sophisticated smoothing
algorithms in id38, it is commonly used in naive bayes text catego-
rization:

unknown word

stop words

(4.14)

  p(wi|c) =

count(wi,c) + 1

(cid:80)w   v (count(w,c) + 1)

=

count(wi,c) + 1

(cid:0)(cid:80)w   v count(w,c)(cid:1) +|v|

note once again that it is crucial that the vocabulary v consists of the union of
all the word types in all classes, not just the words in one class c (try to convince
yourself why this must be true; see the exercise at the end of the chapter).

what do we do about words that occur in our test data but are not in our vocab-
ulary at all because they did not occur in any training document in any class? the
solution for such unknown words is to ignore them   remove them from the test
document and not include any id203 for them at all.

finally, some systems choose to completely ignore another class of words: stop
words, very frequent words like the and a. this can be done by sorting the vocabu-
lary by frequency in the training set, and de   ning the top 10   100 vocabulary entries
as stop words, or alternatively by using one of the many pre-de   ned stop word list
available online. then every instance of these stop words are simply removed from
both training and test documents as if they had never occurred. in most text classi-
   cation applications, however, using a stop word list doesn   t improve performance,
and so it is more common to make use of the entire vocabulary and not use a stop
word list.

fig. 4.2 shows the    nal algorithm.

4.3 worked example

let   s walk through an example of training and testing naive bayes with add-one
smoothing. we   ll use a id31 domain with the two classes positive
(+) and negative (-), and take the following miniature training and test documents
simpli   ed from actual movie reviews.

cat

training -
-
-
+
+
?

test

documents

just plain boring
entirely predictable and lacks energy
no surprises and very few laughs
very powerful
the most fun    lm of the summer
predictable with no fun

the prior p(c) for the two classes is computed via eq. 4.11 as nc
ndoc

:

4.3

    worked example

69

function train naive bayes(d, c) returns log p(c) and log p(w|c)
for each class c     c

# calculate p(c) terms

ndoc = number of documents in d
nc = number of documents from d in class c
logprior[c]    log
v   vocabulary of d
bigdoc[c]   append(d) for d     d with class c
for each word w in v

nc
ndoc

# calculate p(w|c) terms
count(w,c)   # of occurrences of w in bigdoc[c]
count(w,c) + 1
loglikelihood[w,c]    log
return logprior, loglikelihood, v

w(cid:48) in v (count (w(cid:48),c) + 1)

(cid:80)

function test naive bayes(testdoc, logprior, loglikelihood, c, v) returns best c
for each class c     c

sum[c]    logprior[c]
for each position i in testdoc
word   testdoc[i]
if word     v

sum[c]   sum[c]+ loglikelihood[word,c]

return argmaxc sum[c]

figure 4.2 the naive bayes algorithm, using add-1 smoothing. to use add-   smoothing
instead, change the +1 to +   for loglikelihood counts in training.

p(   ) =

3
5

p(+) =

2
5

the word with doesn   t occur in the training set, so we drop it completely (as
mentioned above, we don   t use unknown word models for naive bayes). the like-
lihoods from the training set for the remaining three words    predictable   ,    no   , and
   fun   , are as follows, from eq. 4.14 (computing the probabilities for the remainder
of the words in the training set is left as exercise 4.?? (tbd)).

p(   predictable   |   ) =
p(   no   |   ) =
p(   fun   |   ) =

1 + 1
14 + 20
1 + 1
14 + 20
0 + 1
14 + 20

p(   predictable   |+) =
0 + 1
p(   no   |+) =
9 + 20
1 + 1
p(   fun   |+) =
9 + 20

0 + 1
9 + 20

for the test sentence s =    predictable with no fun   , after removing the word

   with   , the chosen class, via eq. 4.9, is therefore computed as follows:

p(   )p(s|   ) =
p(+)p(s|+) =

3
5   
2
5   

2   2   1
1   1   2

343 = 6.1   10   5
293 = 3.2   10   5

70 chapter 4

    naive bayes and sentiment classification
the model thus predicts the class negative for the test sentence.

4.4 optimizing for id31

binary nb

while standard naive bayes text classi   cation can work well for id31,
some small changes are generally employed that improve performance.

first, for sentiment classi   cation and a number of other text classi   cation tasks,
whether a word occurs or not seems to matter more than its frequency. thus it
often improves performance to clip the word counts in each document at 1 (see
the end of the chapter for pointers to these results). this variant is called binary
multinomial naive bayes or binary nb. the variant uses the same eq. 4.10 except
that for each document we remove all duplicate words before concatenating them
into the single big document. fig. 4.3 shows an example in which a set of four
documents (shortened and text-normalized for this example) are remapped to binary,
with the modi   ed counts shown in the table on the right. the example is worked
without add-1 smoothing to make the differences clearer. note that the results counts
need not be 1; the word great has a count of 2 even for binary nb, because it appears
in multiple documents.

four original documents:

boxing scenes

    it was pathetic the worst part was the
    no plot twists or great scenes
+ and satire and great plot twists
+ great scenes great    lm

after per-document binarization:

scenes

    it was pathetic the worst part boxing
    no plot twists or great scenes
+ and satire great plot twists
+ great scenes    lm

nb

binary
counts counts
+     +    
2
0
and
1
0
boxing
0
1
   lm
3
1
great
1
0
it
1
0
no
1
0
or
1
part
0
pathetic 0
1
1
1
plot
0
1
satire
2
1
scenes
1
0
the
twists
1
1
1
0
was
worst
0
1

0
1
0
1
1
1
1
1
1
1
0
2
2
1
2
1

1
0
1
2
0
0
0
0
0
1
1
1
0
1
0
0

figure 4.3 an example of binarization for the binary naive bayes algorithm.

a second important addition commonly made when doing text classi   cation for
sentiment is to deal with negation. consider the difference between i really like this
movie (positive) and i didn   t like this movie (negative). the negation expressed by
didn   t completely alters the id136s we draw from the predicate like. similarly,
negation can modify a negative word to produce a positive review (don   t dismiss this
   lm, doesn   t let us get bored).

a very simple baseline that is commonly used in sentiment to deal with negation
is during text id172 to prepend the pre   x not to every word after a token
of logical negation (n   t, not, no, never) until the next punctuation mark. thus the
phrase

didn   t like this movie , but i

4.5

    naive bayes for other text classification tasks

71

becomes

didn   t not_like not_this not_movie , but i

newly formed    words    like not like, not recommend will thus occur more of-
ten in negative document and act as cues for negative sentiment, while words like
not bored, not dismiss will acquire positive associations. we will return in chap-
ter 15 to the use of parsing to deal more accurately with the scope relationship be-
tween these negation words and the predicates they modify, but this simple baseline
works quite well in practice.

finally, in some situations we might have insuf   cient labeled training data to
train accurate naive bayes classi   ers using all words in the training set to estimate
positive and negative sentiment. in such cases we can instead derive the positive
and negative word features from sentiment lexicons, lists of words that are pre-
annotated with positive or negative sentiment. four popular lexicons are the general
inquirer (stone et al., 1966), liwc (pennebaker et al., 2007), the opinion lexicon
of hu and liu (2004a) and the mpqa subjectivity lexicon (wilson et al., 2005).

for example the mpqa subjectivity lexicon has 6885 words, 2718 positive and
4912 negative, each marked for whether it is strongly or weakly biased. (chapter 19
will discuss how these lexicons can be learned automatically.) some samples of
positive and negative words from the mpqa lexicon include:

sentiment
lexicons

general
inquirer
liwc

+ : admirable, beautiful, con   dent, dazzling, ecstatic, favor, glee, great
    : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate

a common way to use lexicons in a naive bayes classi   er is to add a feature
that is counted whenever a word from that lexicon occurs. thus we might add a
feature called    this word occurs in the positive lexicon   , and treat all instances of
words in the lexicon as counts for that one feature, instead of counting each word
separately. similarly, we might add as a second feature    this word occurs in the
negative lexicon    of words in the negative lexicon. if we have lots of training data,
and if the test data matches the training data, using just two features won   t work as
well as using all the words. but when training data is sparse or not representative of
the test set, using dense lexicon features instead of sparse individual-word features
may generalize better.

4.5 naive bayes for other text classi   cation tasks

spam detection

in the previous section we pointed out that naive bayes doesn   t require that our
classi   er use all the words in the training data as features. in fact features in naive
bayes can express any property of the input text we want.

consider the task of spam detection, deciding if a particular piece of email is
an example of spam (unsolicited bulk email)     and one of the    rst applications of
naive bayes to text classi   cation (sahami et al., 1998).

a common solution here, rather than using all the words as individual features, is
to prede   ne likely sets of words or phrases as features, combined these with features
that are not purely linguistic. for example the open-source spamassassin tool1
prede   nes features like the phrase    one hundred percent guaranteed   , or the feature
mentions millions of dollars, which is a regular expression that matches suspiciously
large sums of money. but it also includes features like html has a low ratio of

1 https://spamassassin.apache.org

72 chapter 4

    naive bayes and sentiment classification

text to image area, that isn   t purely linguistic and might require some sophisticated
computation, or totally non-linguistic features about, say, the path that the email
took to arrive. more sample spamassassin features:

language id

    email subject line is all capital letters
    contains phrases of urgency like    urgent reply   
    email subject line contains    online pharmaceutical   
    html has unbalanced    head    tags
    claims you can be removed from the list
for other tasks, like language id   determining what language a given piece of
text is written in   the most effective naive bayes features are not words at all, but
byte id165s, 2-grams (   zw   ) 3-grams (   nya   ,     vo   ), or 4-grams (   ie z   ,    thei   ).
because spaces count as a byte, byte id165s can model statistics about the begin-
ning or ending of words. 2 a widely used naive bayes system, langid.py (lui
and baldwin, 2012) begins with all possible id165s of lengths 1-4, using feature
selection to winnow down to the most informative 7000    nal features.

language id systems are trained on multilingual text, such as wikipedia (wikipedia

text in 68 different languages were used in (lui and baldwin, 2011)), or newswire.
to make sure that this multilingual text correctly re   ects different regions, dialects,
and socio-economic classes, systems also add twitter text in many languages geo-
tagged to many regions (important for getting world english dialects from countries
with large anglophone populations like nigeria or india), bible and quran transla-
tions, slang websites like urban dictionary, corpora of african american vernacular
english (blodgett et al., 2016), and so on (jurgens et al., 2017).

4.6 naive bayes as a language model

as we saw in the previous section, naive bayes classi   ers can use any sort of fea-
ture: dictionaries, urls, email addresses, network features, phrases, and so on. but
if, as in the previous section, we use only individual word features, and we use all
of the words in the text (not a subset), then naive bayes has an important similar-
ity to id38. speci   cally, a naive bayes model can be viewed as a
set of class-speci   c unigram language models, in which the model for each class
instantiates a unigram language model.

since the likelihood features from the naive bayes model assign a id203 to

each word p(word|c), the model also assigns a id203 to each sentence:

p(s|c) = (cid:89)i   positions

p(wi|c)

(4.15)

thus consider a naive bayes model with the classes positive (+) and negative (-)

and the following model parameters:

it   s also possible to use codepoints, which are multi-byte unicode representations of characters in

2
character sets, but simply using bytes seems to work better.

4.7

    evaluation: precision, recall, f-measure

73

p(w|+) p(w|-)
w
0.2
0.1
i
0.001
love 0.1
0.01
this 0.01
0.005
fun 0.05
   lm 0.1
0.1
...
...
...

each of the two columns above instantiates a language model that can assign a

id203 to the sentence    i love this fun    lm   :

p(   i love this fun    lm   |+) = 0.1   0.1   0.01   0.05   0.1 = 0.0000005
p(   i love this fun    lm   |   ) = 0.2   0.001   0.01   0.005   0.1 = .0000000010
as it happens, the positive model assigns a higher id203 to the sentence:
p(s|pos) > p(s|neg). note that this is just the likelihood part of the naive bayes
model; once we multiply in the prior a full naive bayes model might well make a
different classi   cation decision.

4.7 evaluation: precision, recall, f-measure

gold labels

contingency
table

to introduce the methods for evaluating text classi   cation, let   s    rst consider some
simple binary detection tasks. for example, in spam detection, our goal is to label
every text as being in the spam category (   positive   ) or not in the spam category
(   negative   ). for each item (email document) we therefore need to know whether
our system called it spam or not. we also need to know whether the email is actually
spam or not, i.e. the human-de   ned labels for each document that we are trying to
match. we will refer to these human labels as the gold labels.

or imagine you   re the ceo of the delicious pie company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning delicious pie. here the positive class is tweets about
delicious pie and the negative class is all other tweets.

in both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. to evaluate any system for detecting things, we start
by building a contingency table like the one shown in fig. 4.4. each cell labels a
set of possible outcomes. in the spam detection case, for example, true positives are
documents that are indeed spam (indicated by human-created gold labels) and our
system said they were spam. false negatives are documents that are indeed spam
but our system labeled as non-spam.

to the bottom right of the table is the equation for accuracy, which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. although accuracy might seem a natural
metric, we generally don   t use it. that   s because accuracy doesn   t work well when
the classes are unbalanced (as indeed they are with spam, which is a large majority
of email, or with tweets, which are mainly not about pie).

to make this more explicit, imagine that we looked at a million tweets, and
let   s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. imagine a

74 chapter 4

    naive bayes and sentiment classification

figure 4.4 contingency table

simple classi   er that stupidly classi   ed every tweet as    not about pie   . this classi   er
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! what an amazing accuracy level! surely we should
be happy with this classi   er? but of course this fabulous    no pie    classi   er would
be completely useless, since it wouldn   t    nd a single one of the customer comments
we are looking for. in other words, accuracy is not a good metric when the goal is
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.

that   s why instead of accuracy we generally turn to two other metrics: precision
and recall. precision measures the percentage of the items that the system detected
(i.e., the system labeled as positive) that are in fact positive (i.e., are positive accord-
ing to the human gold labels). precision is de   ned as

precision

precision =

true positives

true positives + false positives

recall

recall measures the percentage of items actually present in the input that were

correctly identi   ed by the system. recall is de   ned as

recall =

true positives

true positives + false negatives

precision and recall will help solve the problem with the useless    nothing is
pie    classi   er. this classi   er, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). you should convince yourself that the precision at    nding relevant
tweets is equally problematic. thus precision and recall, unlike accuracy, emphasize
true positives:    nding the things that we are supposed to be looking for.

there are many ways to de   ne a single metric that incorporates aspects of both
precision and recall. the simplest of these combinations is the f-measure (van
rijsbergen, 1975) , de   ned as:

f-measure

f   =

(   2 + 1)pr
   2p + r

the    parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. values of    > 1 favor recall, while
values of    < 1 favor precision. when    = 1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called f   =1 or just f1:

f1

true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn4.7

    evaluation: precision, recall, f-measure

75

f1 =

2pr
p + r

(4.16)

f-measure comes from a weighted harmonic mean of precision and recall. the
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

harmonicmean(a1,a2,a3,a4, ...,an) =

n
+ 1
a3

1
a1

+ 1
a2

+ ... + 1
an

and hence f-measure is

f =

1

   1
p + (1      ) 1

r

or(cid:18)with    2 =

   (cid:19) f =
1      

(   2 + 1)pr
   2p + r

(4.17)

(4.18)

harmonic mean is used because it is a conservative metric; the harmonic mean of
two values is closer to the minimum of the two values than the arithmetic mean is.
thus it weighs the lower of the two numbers more heavily.

4.7.1 more than two classes
up to now we have been assuming text classi   cation tasks with only two classes.
but lots of classi   cation tasks in language processing have more than two classes.
for id31 we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, id14, emotion detection, and so on.

there are two kinds of multi-class classi   cation tasks. in any-of or multi-label
classi   cation, each document or item can be assigned more than one label. we can
solve any-of classi   cation by building separate binary classi   ers for each class c,
trained on positive examples labeled c and negative examples not labeled c. given
a test document or item d, then each classi   er makes their decision independently,
and we may assign multiple labels to d.

more common in language processing is one-of or multinomial classi   cation,
in which the classes are mutually exclusive and each document or item appears in
exactly one class. here we again build a separate binary classi   er trained on positive
examples from c and negative examples from all other classes. now given a test
document or item d, we run all the classi   ers and choose the label from the classi   er
with the highest score. consider the sample confusion matrix for a hypothetical 3-
way one-of email categorization decision (urgent, normal, spam) shown in fig. 4.5.
the matrix shows, for example, that the system mistakenly labeled 1 spam doc-
ument as urgent, and we have shown how to compute a distinct precision and recall
value for each class. in order to derive a single metric that tells us how well the
system is doing, we can combine these values in two ways. in macroaveraging, we
compute the performance for each class, and then average over classes. in microav-
eraging, we collect the decisions for all classes into a single contingency table, and
then compute precision and recall from that table. fig. 4.6 shows the contingency
table for each class separately, and shows the computation of microaveraged and
macroaveraged precision.

as the    gure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. the macroaverage better re   ects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

any-of

one-of
multinomial
classi   cation

macroaveraging

microaveraging

76 chapter 4

    naive bayes and sentiment classification

figure 4.5 confusion matrix for a three-class categorization task, showing for each pair of
classes (c1,c2), how many documents from c1 were (in)correctly assigned to c2

figure 4.6 separate contingency tables for the 3 classes from the previous    gure, showing the pooled contin-
gency table and the microaveraged and macroaveraged precision.
4.8 test sets and cross-validation

development
test set
devset

cross-validation

10-fold
cross-validation

the training and testing procedure for text classi   cation follows what we saw with
id38 (section 3.2): we use the training set to train the model, then use
the development test set (also called a devset) to perhaps tune some parameters,
and in general decide what the best model is. once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
while the use of a devset avoids over   tting the test set, having a    xed training
set, devset, and test set creates another problem: in order to save lots of data for
training, the test set (or devset) might not be large enough to be representative. it
would be better if we could somehow use all our data both for training and test. we
do this by cross-validation: we randomly choose a training and test set division of
our data, train our classi   er, and then compute the error rate on the test set. then
we repeat with a different randomly selected training set and test set. we do this
sampling process 10 times and average these 10 runs to get an average error rate.
this is called 10-fold cross-validation.

the only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can   t examine any of the data
to suggest possible features and in general see what   s going on. but looking at the
corpus is often important for designing the system. for this reason, it is common

851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+2008811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=pooledclass 3: spamclass 2: normalclass 1: urgent4.9

    statistical significance testing

77

to create a    xed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
fig. 4.7.

figure 4.7

10-fold cross-validation

4.9 statistical signi   cance testing

null hypothesis

in building systems we are constantly comparing the performance of systems. often
we have added some new bells and whistles to our algorithm and want to compare
the new version of the system to the unaugmented version. or we want to compare
our algorithm to a previously published one to know which is better.

we might imagine that to compare the performance of two classi   ers a and b
all we have to do is look at a and b   s score on the same test set   for example we
might choose to compare macro-averaged f1    and see whether it   s a or b that has
the higher score. but just looking at this one difference isn   t good enough, because
a might have a better performance than b on a particular test set just by chance.

let   s say we have a test set x of n observations x = x1,x2, ..,xn on which a   s
performance is better than b by    (x). how can we know if a is really better than b?
to do so we   d need to reject the null hypothesis that a isn   t really better than b and
this difference    (x) occurred purely by chance. if the null hypothesis was correct,
we would expect that if we had many test sets of size n and we measured a and b   s
performance on all of them, that on average a might accidentally still be better than
b by this amount    (x) just by chance.

more formally, if we had a random variable x ranging over test sets, the null
hypothesis h0 expects p(   (x) >    (x)|h0), the id203 that we   ll see similarly
big differences just by chance, to be high.
if we had all these test sets we could just measure all the    (x(cid:48)) for all the x(cid:48). if we
found that those deltas didn   t seem to be bigger than    (x), that is, that p-value(x) was
suf   ciently small, less than the standard thresholds of 0.05 or 0.01, then we might
reject the null hypothesis and agree that    (x) was a suf   ciently surprising difference
and a is really a better algorithm than b. following berg-kirkpatrick et al. (2012)
we   ll refer to p(   (x) >    (x)|h0) as p-value(x).
in language processing we don   t generally use traditional statistical approaches
like paired t-tests to compare system outputs because most metrics are not normally

training iterations13452678910devdevdevdevdevdevdevdevdevdevtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtrainingtest settesting78 chapter 4

    naive bayes and sentiment classification

bootstrap test
approximate
randomization

id64

distributed, violating the assumptions of the tests. the standard approach to comput-
ing p-value(x) in natural language processing is to use non-parametric tests like the
bootstrap test (efron and tibshirani, 1993)    which we will describe below   or a
similar test, approximate randomization (noreen, 1989). the advantage of these
tests is that they can apply to any metric; from precision, recall, or f1 to the id7
metric used in machine translation.

the word id64 refers to repeatedly drawing large numbers of smaller
samples with replacement (called bootstrap samples) from an original larger sam-
ple. the intuition of the bootstrap test is that we can create many virtual test sets
from an observed test set by repeatedly sampling from it. the method only makes
the assumption that the sample is representative of the population.

consider a tiny text classi   cation example with a test set x of 10 documents. the
   rst row of fig. 4.8 shows the results of two classi   ers (a and b) on this test set,
with each document labeled by one of the four possibilities: (a and b both right,
both wrong, a right and b wrong, a wrong and b right); a slash through a letter
( b) means that that classi   er got the answer wrong. on the    rst document both a
and b get the correct class (ab), while on the second document a got it right but b
got it wrong (a b). if we assume for simplicity that our metric is accuracy, a has an
accuracy of .70 and b of .50, so    (x) is .20. to create each virtual test set of size
n = 10, we repeatedly (10 times) select a cell from row x with replacement. fig. 4.8
shows a few examples.

9

8

7

6

5

4

3

2

10 a% b%    ()
1
ab a  b ab   ab a  b   ab a  b ab   a  b a  b .70 .50 .20
x
x   (1) a  b ab a  b   ab   ab a  b   ab ab   a  b ab .60 .60 .00
x   (2) a  b ab   a  b   ab   ab ab   ab a  b ab ab .60 .70 -.10
...
x   (b)
figure 4.8 the bootstrap: examples of b pseudo test sets being created from an initial true
test set x. each pseudo test set is created by sampling n = 10 times with replacement; thus an
individual sample is a single cell, a document with its gold label and the correct or incorrect
performance of classi   ers a and b.

now that we have a sampling distribution, we can do statistics on how how often
a has an accidental advantage. there are various ways to compute this advantage;
here we follow the version laid out in berg-kirkpatrick et al. (2012). we might
think that we should just ask, for each bootstrap sample x   (i), whether a beats b
by more than    (x). but there   s a problem: we didn   t draw these samples from a
distribution with 0 mean. the x   (i) were sampled from x, and so the expected value
of    (x   (i)) lies very close to    (x). that is, about half the time a will be better than
b, so we expect a to beat b by    (x). instead, we want to know how often a beats
these expectations by more than    (x). to correct for the expected success, we need
to zero-center, subtracting    (x) from each pseudo test set. thus we   ll be comparing
for each x   (i) whether    (x   (i)) > 2   (x). the full algorithm for the bootstrap is shown
in fig. 4.9. it is given a test set x, a number of samples b, and counts the percentage
of the b bootstrap test sets in which delta(x   (i)) > 2   (x). this percentage then
acts as a one-sided empirical p-value (more sophisticated ways to get p-values from
con   dence intervals also exist).

4.10

    advanced: feature selection

79

function bootstrap(test set x, num of samples b) returns p-value(x)
calculate    (x) # how much better does algorithm a do than b on x
for i = 1 to b do

for j = 1 to n do

# draw a bootstrap sample x   (i) of size n

select a member of x at random and add it to x   (i)

calculate    (x   (i)) # how much better does algorithm a do than b on x   (i)
for each x   (i)
s   s + 1 if    (x   (i)) > 2   (x)
p-value(x)     s
return p-value(x)

# on what % of the b samples did algorithm a beat expectations?

b

figure 4.9 a version of the bootstrap algorithm after berg-kirkpatrick et al. (2012).

4.10 advanced: feature selection

feature
selection

information
gain

the id173 technique introduced in the previous section is feature selection
is a method of removing features that are unlikely to generalize well. the basis
of feature selection is to assign some metric of goodness to each feature, rank the
features, and keep the best ones. the number of features to keep is a meta-parameter
that can be optimized on a dev set.

features are generally ranked by how informative they are about the classi   ca-
tion decision. a very common metric is information gain. information gain tells
us how many bits of information the presence of the word gives us for guessing the
class, and can be computed as follows (where ci is the ith class and   w means that a
document does not contain the word w):

g(w) =    

c(cid:88)i=1

p(ci)logp(ci)

+p(w)

+p(   w)

4.11 summary

c(cid:88)i=1
c(cid:88)i=1

p(ci|w)logp(ci|w)

p(ci|   w)logp(ci|   w)

(4.19)

this chapter introduced the naive bayes model for classi   cation and applied it to
the text categorization task of id31.

learn to model the class given the observation.

    many language processing tasks can be viewed as tasks of classi   cation.
    text categorization, in which an entire text is assigned a class from a    nite set,
includes such tasks as id31, spam detection, language identi-
   cation, and authorship attribution.

    id31 classi   es a text as re   ecting the positive or negative orien-

tation (sentiment) that a writer expresses toward some object.

80 chapter 4

    naive bayes and sentiment classification
    naive bayes is a generative model that make the bag of words assumption
(position doesn   t matter) and the conditional independence assumption (words
are conditionally independent of each other given the class)

si   cation tasks.

    naive bayes with binarized features seems to work better for many text clas-
    feature selection can be used to automatically remove features that aren   t
    classi   ers are evaluated based on precision and recall.
    classi   ers are trained using distinct training, dev, and test sets, including the

helpful.

use of cross-validation in the training set.

bibliographical and historical notes

multinomial naive bayes text classi   cation was proposed by maron (1961) at the
rand corporation for the task of assigning subject categories to journal abstracts.
his model introduced most of the features of the modern form presented here, ap-
proximating the classi   cation task with one-of categorization, and implementing
add-   smoothing and information-based feature selection.

the conditional independence assumptions of naive bayes and the idea of bayes-
ian analysis of text seem to have been arisen multiple times. the same year as
maron   s paper, minsky (1961) proposed a naive bayes classi   er for vision and other
arti   cial intelligence problems, and bayesian techniques were also applied to the
text classi   cation task of authorship attribution by mosteller and wallace (1963). it
had long been known that alexander hamilton, john jay, and james madison wrote
the anonymously-published federalist papers. in 1787   1788 to persuade new york
to ratify the united states constitution. yet although some of the 85 essays were
clearly attributable to one author or another, the authorship of 12 were in dispute
between hamilton and madison. mosteller and wallace (1963) trained a bayesian
probabilistic model of the writing of hamilton and another model on the writings
of madison, then computed the maximum-likelihood author for each of the disputed
essays. naive bayes was    rst applied to spam detection in heckerman et al. (1998).
metsis et al. (2006), pang et al. (2002), and wang and manning (2012) show
that using boolean attributes with multinomial naive bayes works better than full
counts. binary multinomial naive bayes is sometimes confused with another variant
of naive bayes that also use a binary representation of whether a term occurs in
a document: multivariate bernoulli naive bayes. the bernoulli variant instead
estimates p(w|c) as the fraction of documents that contain a term, and includes a
id203 for whether a term is not in a document. mccallum and nigam (1998)
and wang and manning (2012) show that the multivariate bernoulli variant of naive
bayes doesn   t work as well as the multinomial algorithm for sentiment or other text
tasks.

there are a variety of sources covering the many kinds of text classi   cation
tasks. for id31 see pang and lee (2008), and liu and zhang (2012).
stamatatos (2009) surveys authorship attribute algorithms. on language identi   ca-
tion see jauhiainen et al. (2018); jaech et al. (2016) is an important early neural
system. the task of newswire indexing was often used as a test case for text classi-
   cation algorithms, based on the reuters-21578 collection of newswire articles.

see manning et al. (2008) and aggarwal and zhai (2012) on text classi   cation;
classi   cation in general is covered in machine learning textbooks (hastie et al. 2001,

exercises

81

witten and frank 2005, bishop 2006, murphy 2012).

non-parametric methods for computing statistical signi   cance were used    rst in
nlp in the muc competition (chinchor et al., 1993), and even earlier in speech
recognition (gillick and cox 1989, bisani and ney 2004). our description of the
bootstrap draws on the description in berg-kirkpatrick et al. (2012). recent work
has focused on issues including multiple test sets and multiple metrics (s  gaard
et al. 2014, dror et al. 2017).

metrics besides information gain for feature selection include   2, pointwise mu-
tual information, and gini index; see yang and pedersen (1997) for a comparison
and guyon and elisseeff (2003) for a broad introduction survey of feature selection.

exercises

4.1 assume the following likelihoods for each word being part of a positive or

negative movie review, and equal prior probabilities for each class.

pos neg
i
0.09 0.16
always 0.07 0.06
like
0.29 0.06
foreign 0.04 0.15
   lms
0.08 0.11

what class will naive bayes assign to the sentence    i always like foreign
   lms.   ?

4.2 given the following short movie reviews, each labeled with a genre, either

comedy or action:

comedy

1. fun, couple, love, love
2. fast, furious, shoot action
3. couple,    y, fast, fun, fun comedy
4. furious, shoot, shoot, fun action
5.    y, fast, shoot, love action

and a new document d:

fast, couple, shoot,    y

4.3

compute the most likely class for d. assume a naive bayes classi   er and use
add-1 smoothing for the likelihoods.
train two models, multinominal naive bayes and binarized naive bayes, both
with add-1 smoothing, on the following document counts for key sentiment
words, with positive or negative class assigned as noted.
doc    good       poor       great    (class)
d1. 3
d2. 0
d3. 1
d4. 1
d5. 0
use both naive bayes models to assign a class (pos or neg) to this sentence:

pos
pos
neg
neg
neg

0
1
3
5
2

3
2
0
2
0

a good, good plot and great characters, but poor acting.

do the two models agree or disagree?

82 chapter 5

    id28

chapter

5 id28

logistic
regression

   and how do you know that these    ne begonias are not of equal importance?   
hercule poirot, in agatha christie   s the mysterious affair at styles
detective stories are as littered with clues as texts are with words. yet for the
poor reader it can be challenging to know how to weigh the author   s clues in order
to make the crucial classi   cation task: deciding whodunnit.

in this chapter we introduce an algorithm that is admirably suited for discovering
the link between features or cues and some particular outcome: id28.
indeed, id28 is one of the most important analytic tool in the social and
natural sciences. in natural language processing, id28 is the baseline
supervised machine learning algorithm for classi   cation, and also has a very close
relationship with neural networks. as we will see in chapter 7, a neural network can
be viewed as a series of id28 classi   ers stacked on top of each other.
thus the classi   cation and machine learning techniques introduced here will play
an important role throughout the book.

id28 can be used to classify an observation into one of two classes
(like    positive sentiment    and    negative sentiment   ), or into one of many classes.
because the mathematics for the two-class case is simpler, we   ll describe this special
case of id28    rst in the next few sections, and then brie   y summarize
the use of multinomial id28 for more than two classes in section 5.6.
we   ll introduce the mathematics of id28 in the next few sections.

but let   s begin with some high-level issues.

generative and discriminative classi   ers: the most important difference be-
tween naive bayes and id28 is that id28 is a discrimina-
tive classi   er while naive bayes is a generative classi   er.

these are two very different frameworks for how
to build a machine learning model. consider a visual
metaphor:
imagine we   re trying to distinguish dog
images from cat images. a generative model would
have the goal of understanding what dogs look like
and what cats look like. you might literally ask such
a model to    generate   , i.e. draw, a dog. given a test
image, the system then asks whether it   s the cat model or the dog model that better
   ts (is less surprised by) the image, and chooses that as its label.

a discriminative model, by contrast, is only try-
ing to learn to distinguish the classes (perhaps with-
out learning much about them). so maybe all the
dogs in the training data are wearing collars and the
cats aren   t. if that one feature neatly separates the
classes, the model is satis   ed.
if you ask such a
model what it knows about cats all it can say is that
they don   t wear collars.

generative
model

discriminative
model

5.1

    classification: the sigmoid

83

more formally, recall that the naive bayes assigns a class c to a document d not

by directly computing p(c|d) but by computing a likelihood and a prior

  c = argmax

c   c

likelihood
(cid:122) (cid:125)(cid:124) (cid:123)
p(d|c)

prior

(cid:122)(cid:125)(cid:124)(cid:123)p(c)

(5.1)

a generative model like naive bayes makes use of this likelihood term, which
expresses how to generate the features of a document if we knew it was of class c.

by contrast a discriminative model in this text categorization scenario attempts
to directly compute p(c|d). perhaps it will learn to assign high weight to document
features that directly improve its ability to discriminate between possible classes,
even if it couldn   t generate an example of one of the classes.
components of a probabilistic machine learning classi   er: like naive bayes,
id28 is a probabilistic classi   er that makes use of supervised machine
learning. machine learning classi   ers require a training corpus of m observations
input/output pairs (x(i),y(i)). (we   ll use superscripts in parentheses to refer to indi-
vidual instances in the training set   for sentiment classi   cation each instance might
be an individual document to be classi   ed). a machine learning system for classi   -
cation then has four components:

1. a feature representation of the input. for each input observation x(i), this
will be a vector of features [x1,x2, ...,xn]. we will generally refer to feature
i for input x( j) as x( j)
, sometimes simpli   ed as xi, but we will also see the
i
notation fi, fi(x), or, for multiclass classi   cation, fi(c,x).

2. a classi   cation function that computes   y, the estimated class, via p(y|x). in
the next section we will introduce the sigmoid and softmax tools for classi   -
cation.

3. an objective function for learning, usually involving minimizing error on

training examples. we will introduce the cross-id178 id168

4. an algorithm for optimizing the objective function. we introduce the stochas-

tic id119 algorithm.

id28 has two phases:
training: we train the system (speci   cally the weights w and b) using stochastic

id119 and the cross-id178 loss.

test: given a test example x we compute p(y|x) and return the higher id203

label y = 1 or y = 0.

5.1 classi   cation: the sigmoid

the goal of binary id28 is to train a classi   er that can make a binary
decision about the class of a new input observation. here we introduce the sigmoid
classi   er that will help us make this decision.

consider a single input observation x, which we will represent by a vector of
features [x1,x2, ...,xn] (we   ll show sample features in the next subsection). the clas-
si   er output y can be 1 (meaning the observation is a member of the class) or 0
(the observation is not a member of the class). we want to know the id203
p(y = 1|x) that this observation is a member of the class. so perhaps the decision

84 chapter 5

    id28

is    positive sentiment    versus    negative sentiment   , the features represent counts
of words in a document, and p(y = 1|x) is the id203 that the document has
positive sentiment, while and p(y = 0|x) is the id203 that the document has
negative sentiment.
id28 solves this task by learning, from a training set, a vector of
weights and a bias term. each weight wi is a real number, and is associated with one
of the input features xi. the weight wi represents how important that input feature is
to the classi   cation decision, and can be positive (meaning the feature is associated
with the class) or negative (meaning the feature is not associated with the class).
thus we might expect in a sentiment task the word awesome to have a high positive
weight, and abysmal to have a very negative weight. the bias term, also called the
intercept, is another real number that   s added to the weighted inputs.

to make a decision on a test instance    after we   ve learned the weights in
training    the classi   er    rst multiplies each xi by its weight wi, sums up the weighted
features, and adds the bias term b. the resulting single number z expresses the
weighted sum of the evidence for the class.

z = (cid:32) n(cid:88)i=1

wixi(cid:33) + b

(5.2)

bias term
intercept

dot product

in the rest of the book we   ll represent such sums using the dot product notation from
id202. the dot product of two vectors a and b, written as a   b is the sum of
the products of the corresponding elements of each vector. thus the following is an
equivalent formation to eq. 5.2:

z = w   x + b

(5.3)

but note that nothing in eq. 5.3 forces z to be a legal id203, that is, to lie
between 0 and 1. in fact, since weights are real-valued, the output might even be
negative; z ranges from        to    .

figure 5.1 the sigmoid function y = 1
1+e   z takes a real value and maps it to the range [0,1].
because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash
outlier values toward 0 or 1.

sigmoid

logistic
function

to create a id203, we   ll pass z through the sigmoid function,    (z). the
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives id28 its name. the sigmoid has the following equation,
shown graphically in fig. 5.1:

y =    (z) =

1

1 + e   z

(5.4)

5.1

    classification: the sigmoid

85

the sigmoid has a number of advantages; it take a real-valued number and maps
it into the range [0,1], which is just what we want for a id203. because it is
nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1. and it   s differentiable, which as we   ll see in section 5.8 will
be handy for learning.

we   re almost there. if we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. to make it a id203, we just need to make
sure that the two cases, p(y = 1) and p(y = 0), sum to 1. we can do this as follows:

p(y = 1) =    (w   x + b)
1 + e   (w  x+b)

=

1

p(y = 0) = 1       (w   x + b)
1 + e   (w  x+b)

= 1   

1

=

e   (w  x+b)

1 + e   (w  x+b)

(5.5)

now we have an algorithm that given an instance x computes the id203
p(y = 1|x). how do we make a decision? for a test instance x, we say yes if the
id203 p(y = 1|x) is more than .5, and no otherwise. we call .5 the decision
boundary:

decision
boundary

  y =(cid:26) 1 if p(y = 1|x) > 0.5

0 otherwise

5.1.1 example: sentiment classi   cation
let   s have an example. suppose we are doing binary sentiment classi   cation on
movie review text, and we would like to know whether to assign the sentiment class
+ or     to a review document doc. we   ll represent each input observation by the
following 6 features x1...x6 of the input; fig. 5.2 shows the features in a sample mini
test document.

var de   nition
x1
x2
x3
x4
x5
x6

count(positive lexicon)     doc)
count(negative lexicon)     doc)
(cid:26) 1 if    no        doc
count(1st and 2nd pronouns     doc)
(cid:26) 1 if    !        doc

log(word count of doc)

0 otherwise

0 otherwise

value in fig. 5.2
3
2

1
3

0
ln(64) = 4.15

let   s assume for the moment that we   ve already learned a real-valued weight
for each of these features, and that the 6 weights corresponding to the 6 features
are [2.5,   5.0,   1.2,0.5,2.0,0.7], while b = 0.1. (we   ll discuss in the next section
how the weights are learned.) the weight w1, for example indicates how important

86 chapter 5

    id28

figure 5.2 a sample mini test document showing the extracted features in the vector x.

a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to
a positive sentiment decision, while w2 tells us the importance of negative lexicon
words. note that w1 = 2.5 is positive, while w2 =    5.0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.

given these 6 features and the input review x, p(+|x) and p(   |x) can be com-

puted using eq. 5.5:

p(+|x) = p(y = 1|x) =    (w   x + b)
=    ([2.5,   5.0,   1.2,0.5,2.0,0.7]   [3,2,1,3,0,4.15] + 0.1)
=    (1.805)
= 0.86

p(   |x) = p(y = 0|x) = 1       (w   x + b)

= 0.14

id28 is commonly applied to all sorts of nlp tasks, and any prop-
erty of the input can be a feature. consider the task of period disambiguation:
deciding if a period is the end of a sentence or part of a word, by classifying each
period into one of two classes eos (end-of-sentence) and not-eos. we might use
features like x1 below expressing that the current word is lower case and the class
is eos (perhaps with a positive weight), or that the current word is in our abbrevia-
tions dictionary (   prof.   ) and the class is eos (perhaps with a negative weight). a
feature can also express a quite complex combination of properties. for example a
period following a upper cased word is a likely to be an eos, but if the word itself is
st. and the previous word is capitalized, then the period is likely part of a shortening
of the word street.

0 otherwise

x1 = (cid:26) 1 if    case(wi) = lower   
x2 = (cid:26) 1 if    wi     acronymdict   
x3 = (cid:26) 1 if    wi = st. & case(wi   1) = cap   

0 otherwise

0 otherwise

designing features: features are generally designed by examining the training
set with an eye to linguistic intuitions and the linguistic literature on the domain. a
careful error analysis on the training or dev set. of an early version of a system often
provides insights into features.

 it's hokey. there are virtually no surprises , and the writing is second-rate . so why was it so enjoyable? for one thing , the cast is great . another nice touch is the music . i was overcome with the urge to get off the couch and start dancing .  it sucked me in , and it'll do the same to you .x1=3x6=4.15x3=1x4=3x5=0x2=2feature
interactions

feature
templates

5.2

    learning in id28

87

for some tasks it is especially helpful to build complex features that are combi-
nations of more primitive features. we saw such a feature for period disambiguation
above, where a period on the word st. was less likely to be the end of sentence if
the previous word was capitalized. for id28 and naive bayes these
combination features or feature interactions have to be designed by hand.

for many tasks (especially when feature values can reference speci   c words)
we   ll need large numbers of features. often these are created automatically via fea-
ture templates, abstract speci   cations of features. for example a bigram template
for period disambiguation might create a feature for every pair of words that occurs
before a period in the training set. thus the feature space is sparse, since we only
have to create a feature if that id165 exists in that position in the training set. the
feature is generally created as a hash from the string descriptions. a user description
of a feature as,    bigram(american breakfast)    is hashed into a unique integer i that
becomes the feature number fi.

in order to avoid the extensive human effort of feature design, recent research in
nlp has focused on representation learning: ways to learn features automatically
in an unsupervised way from the input. we   ll introduce methods for representation
learning in chapter 6 and chapter 7.

choosing a classi   er id28 has a number of advantages over naive
bayes. naive bayes has overly strong conditional independence assumptions. con-
sider two features which are strongly correlated; in fact, imagine that we just add the
same feature f1 twice. naive bayes will treat both copies of f1 as if they were sep-
arate, multiplying them both in, overestimating the evidence. by contrast, logistic
regression is much more robust to correlated features; if two features f1 and f2 are
perfectly correlated, regression will simply assign part of the weight to w1 and part
to w2. thus when there are many correlated features, id28 will assign
a more accurate id203 than naive bayes. so id28 generally works
better on larger documents or datasets and is a common default.

despite the less accurate probabilities, naive bayes still often makes the correct
classi   cation decision. furthermore, naive bayes works extremely well (even bet-
ter than id28) on very small datasets (ng and jordan, 2002) or short
documents (wang and manning, 2012). furthermore, naive bayes is easy to imple-
ment and very fast to train (there   s no optimization step). so it   s still a reasonable
approach to use in some situations.

5.2 learning in id28

how are the parameters of the model, the weights w and bias b, learned?

id28 is an instance of supervised classi   cation in which we know
the correct label y (either 0 or 1) for each observation x. what the system produces,
via eq. 5.5 is   y, the system   s estimate of the true y. we want to learn parameters
(meaning w and b) that make   y for each training observation as close as possible to
the true y .

this requires 2 components that we foreshadowed in the introduction to the
chapter. the    rst is a metric for how close the current label (   y) is to the true gold
label y. rather than measure similarity, we usually talk about the opposite of this:
the distance between the system output and the gold output, and we call this distance
the id168 or the cost function. in the next section we   ll introduce the loss

loss

88 chapter 5

    id28

function that is commonly used for id28 and also for neural networks,
the cross-id178 loss.

the second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this id168. the standard algorithm for this is
id119; we   ll introduce the stochastic id119 algorithm in the
following section.

5.3 the cross-id178 id168

we need a id168 that expresses, for an observation x, how close the classi   er
output (   y =    (w   x + b)) is to the correct output (y, which is 0 or 1). we   ll call this:
(5.6)

l(   y,y) = how much   y differs from the true y

you could imagine using a simple id168 that just takes the mean squared
error between   y and y.

lmse(   y,y) =

1
2

(   y    y)2

(5.7)

cross id178
loss

it turns out that this mse loss, which is very useful for some algorithms like
id75, becomes harder to optimize (technically, non-convex), when it   s
applied to probabilistic classi   cation.

instead, we use a id168 that prefers the correct class labels of the training
example to be more likely. this is called conditional maximum likelihood estima-
tion: we choose the parameters w,b that maximize the log id203 of the true
y labels in the training data given the observations x. the resulting id168
is the negative log likelihood loss, generally called the cross id178 loss.

let   s derive this id168, applied to a single observation x. we   d like to
learn weights that maximize the id203 of the correct label p(y|x). since there
are only two discrete outcomes (1 or 0), this is a bernoulli distribution, and we can
express the id203 p(y|x) that our classi   er produces for one observation as
the following (keeping in mind that if y=1, eq. 5.8 simpli   es to   y; if y=0, eq. 5.8
simpli   es to 1      y):

p(y|x) =   y y (1      y)1   y

(5.8)

now we take the log of both sides. this will turn out to be handy mathematically,
and doesn   t hurt us; whatever values maximize a id203 will also maximize the
log of the id203:

log p(y|x) = log(cid:2)   y y (1      y)1   y(cid:3)

= ylog   y + (1    y)log(1      y)

(5.9)

eq. 5.9 describes a log likelihood that should be maximized. in order to turn this
into id168 (something that we need to minimize), we   ll just    ip the sign on
eq. 5.9. the result is the cross-id178 loss lce:

lce (   y,y) =    log p(y|x) =     [ylog   y + (1    y)log(1      y)]

finally, we can plug in the de   nition of   y =    (w   x) + b:

lce (w,b) =     [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]

(5.10)

(5.11)

5.4

    id119

89

why does minimizing this negative log id203 do what we want? a perfect
classi   er would assign id203 1 to the correct outcome (y=1 or y=0) and prob-
ability 0 to the incorrect outcome. that means the higher   y (the closer it is to 1), the
better the classi   er; the lower   y is (the closer it is to 0), the worse the classi   er. the
negative log of this id203 is a convenient loss metric since it goes from 0 (neg-
ative log of 1, no loss) to in   nity (negative log of 0, in   nite loss). this id168
also insures that as id203 of the correct answer is maximized, the id203
of the incorrect answer is minimized; since the two sum to one, any increase in the
id203 of the correct answer is coming at the expense of the incorrect answer.
it   s called the cross-id178 loss, because eq. 5.9 is also the formula for the cross-
id178 between the true id203 distribution y and our estimated distribution
  y.

let   s now extend eq. 5.10 from one example to the whole training set: we   ll con-
tinue to use the notation that x(i) and y(i) mean the ith training features and training
label, respectively. we make the assumption that the training examples are indepen-
dent:

log p(training labels) = log

p(y(i)|x(i))

m(cid:89)i=1
m(cid:88)i=1
log p(y(i)|x(i))
m(cid:88)i=1

lce (   y(i),y(i))

=

=    

(5.12)

(5.13)

(5.14)

we   ll de   ne the cost function for the whole dataset as the average loss for each

example:

cost(w,b) =

1
m

=    

m(cid:88)i=1
m(cid:88)i=1

1
m

lce (   y(i),y(i))

y(i) log   (w   x(i) + b) + (1    y(i))log(cid:16)1       (w   x(i) + b)(cid:17)

(5.15)

now we know what we want to minimize; in the next section, we   ll see how to

   nd the minimum.

5.4 id119

our goal with id119 is to    nd the optimal weights: minimize the loss
function we   ve de   ned for the model. in eq. 5.16 below, we   ll explicitly represent
the fact that the id168 l is parameterized by the weights, which we   ll refer to
in machine learning in general as    (in the case of id28    = w,b):

     = argmin

  

1
m

m(cid:88)i=1

lce (y(i),x(i);   )

(5.16)

90 chapter 5

convex

    id28
how shall we    nd the minimum of this (or any) id168? id119
is a method that    nds a minimum of a function by    guring out in which direction
(in the space of the parameters   ) the function   s slope is rising the most steeply,
and moving in the opposite direction. the intuition is that if you are hiking in a
canyon and trying to descend most quickly down to the river at the bottom, you might
look around yourself 360 degrees,    nd the direction where the ground is sloping the
steepest, and walk downhill in that direction.

for id28, this id168 is conveniently convex. a convex func-
tion has just one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to    nd the minimum.

although the algorithm (and the concept of gradient) are designed for direction
vectors, let   s    rst consider a visualization of the the case where the parameter of our
system, is just a single scalar w, shown in fig. 5.3.

given a random initialization of w at some value w1, and assuming the loss
function l happened to have the shape in fig. 5.3, we need the algorithm to tell us
whether at the next iteration, we should move left (making w2 smaller than w1) or
right (making w2 bigger than w1) to reach the minimum.

gradient

learning rate

figure 5.3 the    rst step in iteratively    nding the minimum of this id168, by moving
w in the reverse direction from the slope of the function. since the slope is negative, we need
to move w in a positive direction, to the right. here superscripts are used for learning steps,
so w1 means the initial value of w (which is 0), w2 at the second step, and so on.

the id119 algorithm answers this question by    nding the gradient
of the id168 at the current point and moving in the opposite direction. the
gradient of a function of many variables is a vector pointing in the direction the
greatest increase in a function. the gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in fig. 5.3, we can informally
think of the gradient as the slope. the dotted line in fig. 5.3 shows the slope of this
hypothetical id168 at point w = w1. you can see that the slope of this dotted
line is negative. thus to    nd the minimum, id119 tells us to go in the
opposite direction: moving w in a positive direction.

the magnitude of the amount to move in id119 is the value of the slope
d
dw f (x;w) weighted by a learning rate   . a higher (faster) learning rate means that
we should move w more on each step. the change we make in our parameter is the
learning rate times the gradient (or the slope, in our single-variable example):

wt+1 = wt        d

dw

f (x;w)

(5.17)

now let   s extend the intuition from a function of one scalar variable w to many

wloss0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent5.4

    id119

91

variables, because we don   t just want to move left or right, we want to know where
in the n-dimensional space (of the n parameters that make up   ) we should move.
the gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those n dimensions. if we   re just imagining two weight
dimension (say for one weight w and one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
the w dimension and in the b dimension. fig. 5.4 shows a visualization:

figure 5.4 visualization of the gradient vector in two dimensions w and b.

in an actual id28, the parameter vector w is much longer than 1 or
2, since the input feature vector x can be quite long, and we need a weight wi for
each xi for each dimension/variable wi in w (plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. essentially we   re
asking:    how much would a small change in that variable wi in   uence the total loss
function l?   

in each dimension wi, we express the slope as a partial derivative    
    wi

of the loss
function. the gradient is then de   ned as a vector of these partials. we   ll represent   y
as f (x;   ) to make the dependence on    more obvious:

      l( f (x;   ),y)) =

(5.18)

   
    w1
   
    w2

   
    wn

                  

l( f (x;   ),y)
l( f (x;   ),y)

...

l( f (x;   ),y)

                  

the    nal equation for updating    based on the gradient is thus

  t+1 =   t          l( f (x;   ),y)
5.4.1 the gradient for id28
in order to update   , we need a de   nition for the gradient    l( f (x;   ),y). recall that
for id28, the cross-id178 id168 is:

(5.19)

lce (w,b) =     [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]

(5.20)
it turns out that the derivative of this function for one observation vector x is
eq. 5.21 (the interested reader can see section 5.8 for the derivation of this equation):

    lce (w,b)

    w j

= [   (w   x + b)    y]x j

(5.21)

cost(w,b)wb92 chapter 5

    id28
note in eq. 5.21 that the gradient with respect to a single weight w j represents a
very intuitive value: the difference between the true y and our estimated   y =    (w  
x + b) for that observation, multiplied by the corresponding input value x j.
the loss for a batch of data or an entire dataset is just the average loss over the

m examples:

cost(w,b) =    

1
m

m(cid:88)i=1

y(i) log   (w   x(i) + b) + (1    y(i))log(cid:16)1       (w   x(i) + b)(cid:17)

(5.22)

and the gradient for multiple data points is the sum of the individual gradients::

   cost(w,b)

    w j

=

m(cid:88)i=1(cid:104)   (w   x(i) + b)    y(i)(cid:105)x(i)

j

(5.23)

5.4.2 the stochastic id119 algorithm
stochastic id119 is an online algorithm that minimizes the id168
by computing its gradient after each training example, and nudging    in the right
direction (the opposite direction of the gradient). fig. 5.5 shows the algorithm.

function stochastic id119(l(), f (), x, y) returns   

f is a function parameterized by   
x is the set of training inputs x(1), x(2), ..., x(n)
y is the set of training outputs (labels) y(1), y(2), ..., y(n)

# where: l is the id168
#
#
#
      0
repeat t times
for each training tuple (x(i), y(i)) (in random order)
compute   y (i) = f (x(i);   )
compute the loss l(   y (i),y(i)) # how far off is   y(i)) from the true output y(i)?
g         l( f (x(i);   ),y(i))
# how should we move    to maximize loss ?
                g
return   

# what is our estimated output   y?

# go the other way instead

minibatch

figure 5.5 the stochastic id119 algorithm

stochastic id119 is called stochastic because it chooses a single ran-
dom example at a time, moving the weights so as to improve performance on that
single example. that can result in very choppy movements, so it   s also common to
do minibatch id119, which computes the gradient over batches of train-
ing instances rather than a single instance.

the learning rate    is a parameter that must be adjusted. if it   s too high, the
learner will take steps that are too large, overshooting the minimum of the loss func-
tion. if it   s too low, the learner will take steps that are too small, and take too long to
get to the minimum. it is most common to begin the learning rate at a higher value,
and then slowly decrease it, so that it is a function of the iteration k of training; you
will sometimes see the notation   k to mean the value of the learning rate at iteration
k.

5.5

    id173

93

5.4.3 working through an example
let   s walk though a single step of the id119 algorithm. we   ll use a sim-
pli   ed version of the example in fig. 5.2 as it sees a single observation x, whose
correct value is y = 1 (this is a positive review), and with only two features:

x1 = 3
x2 = 2

(count of positive lexicon words)
(count of negative lexicon words)

let   s assume the initial weights and bias in    0 are all set to 0, and the initial learning
rate    is 0.1:

w1 = w2 = b = 0

   = 0.1

the single update step requires that we compute the gradient, multiplied by the
learning rate

   t+1 =    t             l( f (x(i);   ),y(i))

in our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1, w2, and b. we can compute the    rst gradient as follows:

   w,b =         

    lce (w,b)
    lce (w,b)
    lce (w,b)

    w1
    w2
    b

          =      

(   (w   x + b)    y)x1
(   (w   x + b)    y)x2
   (w   x + b)    y

       =      

(   (0)    1)x1
(   (0)    1)x2
   (0)    1

       =      

   0.5x1
   0.5x2

   0.5        =      

   1.5
   1.0

   0.5      

now that we have a gradient, we compute the new parameter vector    2 by mov-

ing    1 in the opposite direction from the gradient:

   2 =      

w1
w2

b                   

   1.5
   1.0

   0.5       =      

.15
.1

.05      

so after one step of id119, the weights have shifted to be: w1 = .15,

w2 = .1, and b = .05.

note that this observation x happened to be a positive example. we would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2 would shift to have a negative value.

5.5 id173

numquam ponenda est pluralitas sine necessitate
   plurality should never be proposed unless needed   
william of occam

94 chapter 5

over   tting
generalize

id173

l2
id173

    id28
there is a problem with learning weights that make the model perfectly match
the training data. if a feature is perfectly predictive of the outcome because it hap-
pens to only occur in one class, it will be assigned a very high weight. the weights
for features will attempt to perfectly    t details of the training set, in fact too per-
fectly, modeling noisy factors that just accidentally correlate with the class. this
problem is called over   tting. a good model should be able to generalize well from
the training data to the unseen test set, but a model that over   ts will have poor gen-
eralization.

to avoid over   tting, a id173 term is added to the objective function in

eq. 5.16, resulting in the following objective:

  w = argmax

w

m(cid:88)1=1

logp(y(i)|x(i))      r(w)

(5.24)

the new component, r(w) is called a id173 term, and is used to penalize
large weights. thus a setting of the weights that matches the training data perfectly,
but uses many weights with high values to do so, will be penalized more than a
setting that matches the data a little less well, but does so using smaller weights.

there are two common id173 terms r(w). l2 id173 is a quad-
ratic function of the weight values, named because it uses the (square of the) l2
norm of the weight values. the l2 norm, ||w||2, is the same as the euclidean
distance:

r(w ) = ||w||2

2 =

w2
j

n(cid:88)j=1

the l2 regularized objective function becomes:

  w = argmax

w

logp(y(i)|x(i))(cid:35)      

(cid:34) m(cid:88)1=i

w2
j

n(cid:88)j=1

(5.25)

(5.26)

l1
id173

l1 id173 is a linear function of the weight values, named after the l1
norm ||w||1, the sum of the absolute values of the weights, or manhattan distance
(the manhattan distance is the distance you   d have to walk between two points in a
city with a street grid like new york):

r(w ) = ||w||1 =

n(cid:88)i=1

|wi|

the l1 regularized objective function becomes:

  w = argmax

w

logp(y(i)|x(i))(cid:35)      

(cid:34) m(cid:88)1=i

n(cid:88)j=1

|w j|

(5.27)

(5.28)

these kinds of id173 come from statistics, where l1 id173 is
called the    lasso    or lasso regression (tibshirani, 1996) and l2 regression is called

5.6

    multinomial id28

95

ridge regression, and both are commonly used in language processing. l2 regu-
larization is easier to optimize because of its simple derivative (the derivative of w2
is just 2w), while l1 id173 is more complex (the derivative of |w| is non-
continuous at zero). but where l2 prefers weight vectors with many small weights,
l1 prefers sparse solutions with some larger weights but many more weights set to
zero. thus l1 id173 leads to much sparser weight vectors, that is, far fewer
features.

both l1 and l2 id173 have bayesian interpretations as constraints on
the prior of how weights should look. l1 id173 can be viewed as a laplace
prior on the weights. l2 id173 corresponds to assuming that weights are
distributed according to a gaussian distribution with mean    = 0.
in a gaussian
or normal distribution, the further away a value is from the mean, the lower its
id203 (scaled by the variance   ). by using a gaussian prior on the weights, we
are saying that weights prefer to have the value 0. a gaussian for a weight w j is

exp(cid:32)   

(w j        j)2

2   2
j

(cid:33)

(5.29)

1

(cid:113)2     2

j

if we multiply each weight by a gaussian prior on the weight, we are thus maxi-

mizing the following constraint:

  w = argmax

w

m(cid:89)i=1

p(y(i)|x(i))  

n(cid:89)j=1

1

(cid:113)2     2

j

exp(cid:32)   

(w j        j)2

2   2
j

(cid:33)

which in log space, with    = 0, and assuming 2   2 = 1, corresponds to

  w = argmax

w

m(cid:88)i=1

logp(y(i)|x(i))      

w2
j

n(cid:88)j=1

which is in the same form as eq. 5.26.

5.6 multinomial id28

(5.30)

(5.31)

multinominal
logistic
regression

softmax

sometimes we need more than two classes. perhaps we might want to do 3-way
sentiment classi   cation (positive, negative, or neutral). or we could be classifying
the part of speech of a word (choosing from 10, 30, or even 50 different parts of
speech), or assigning semantic labels like the named entities or semantic relations
we will introduce in chapter 17.

in such cases we use multinominal id28, also called softmax re-
gression (or, historically, the maxent classi   er). in multinominal id28
the target y is a variable that ranges over more than two classes; we want to know
the id203 of y being in each potential class c     c, p(y = c|x).
the multinominal logistic classi   er uses a generalization of the sigmoid, called
the softmax function, to compute the id203 p(y = c|x). the softmax function
takes a vector z = [z1,z2, ...,zk] of k arbitrary values and maps them to a id203
distribution, with each value in the range (0,1], and all the values summing to 1.
like the sigmoid, it is an exponential function;

96 chapter 5

    id28
for a vector z of dimensionality k, the softmax is de   ned as:

softmax(zi) =

1     i     k

ezi
j=1 ez j

(cid:80)k

the softmax of an input vector z = [z1,z2, ...,zk] is thus a vector itself:

(5.32)

(5.33)

softmax(z) = (cid:34)

ez1
i=1 ezi

,

(cid:80)k

ez2
i=1 ezi

(cid:80)k

, ...,

ezk

i=1 ezi(cid:35)
(cid:80)k

i=1 ezi is used to normalize all the values into probabilities.

the denominator(cid:80)k

thus for example given a vector:

z = [0.6,1.1,   1.5,1.2,3.2,   1.1]

the result softmax(z) is

[0.055,0.090,0.0067,0.10,0.74,0.010]

again like the sigmoid, the input to the softmax will be the dot product between
a weight vector w and an input vector x (plus a bias). but now we   ll need separate
weight vectors (and bias) for each of the k classes.

p(y = c|x) =

ewc    x + bc
ew j    x + b j

k(cid:88)j=1

(5.34)

like the sigmoid, the softmax has the property of squashing values toward 0 or
1. thus if one of the inputs is larger than the others, will tend to push its id203
toward 1, and suppress the probabilities of the smaller inputs.

5.6.1 features in multinomial id28
for multiclass classi   cation the input features need to be a function of both the
observation x and the candidate output class c. thus instead of the notation xi, fi
or fi(x), when we   re discussing features we will use the notation fi(c,x), meaning
feature i for a particular class c for a given observation x.

in binary classi   cation, a positive weight on a feature pointed toward y=1 and
a negative weight toward y=0... but in multiclass a feature could be evidence for or
against an individual class.

let   s look at some sample features for a few nlp tasks to help understand this
perhaps unintuitive use of features that are functions of both the observation x and
the class c,

suppose we are doing text classi   cation, and instead of binary classi   cation our
task is to assign one of the 3 classes +,    , or 0 (neutral) to a document. now a
feature related to exclamation marks might have a negative weight for 0 documents,
and a positive weight for + or     documents:

5.7

   

interpreting models

97

var

de   nition

0 otherwise

f1(0,x) (cid:26) 1 if    !        doc
f1(+,x) (cid:26) 1 if    !        doc
f1(0,x) (cid:26) 1 if    !        doc

0 otherwise

0 otherwise

wt
   4.5
2.6

1.3

5.6.2 learning in multinomial id28
multinomial id28 has a slightly different id168 than binary lo-
gistic regression because it uses the softmax rather than sigmoid classi   er, the loss
function for a single example x is the sum of the logs of the k output classes:

lce (   y,y) =    

=    

k(cid:88)k=1
k(cid:88)k=1

1{y = k}log p(y = k|x)
ewk  x+bk
j=1 ew j  x+b j

1{y = k}log

(cid:80)k

(5.35)

this makes use of the function 1{} which evaluates to 1 if the condition in the
brackets is true and to 0 otherwise.
the gradient for a single example turns out to be very similar to the gradient for
id28, although we don   t show the derivation here. it is the different
between the value for the true class k (which is 1) and the id203 the classi   er
outputs for class k, weighted by the value of the input xk:

    lce
    wk

5.7

interpreting models

= (1{y = k}    p(y = k|x))xk
= (cid:32)1{y = k}   
ewk  x+bk

j=1 ew j  x+b j(cid:33)xk
(cid:80)k

(5.36)

interpretable

often we want to know more than just the correct classi   cation of an observation.
we want to know why the classi   er made the decision it did. that is, we want our
decision to be interpretable. interpretability can be hard to de   ne strictly, but the
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. because the features to id28 are often human-designed,
one way to understand a classi   er   s decision is to understand the role each feature it
plays in the decision. id28 can be combined with statistical tests (the
likelihood ratio test, or the wald test); investigating whether a particular feature is
signi   cant by one of these tests, or inspecting its magnitude (how large is the weight
w associated with the feature?) can help us interpret why the classi   er made the
decision it makes. this is enormously important for building transparent models.

furthermore, in addition to its use as a classi   er, id28 in nlp and
many other    elds is widely used as an analytic tool for testing hypotheses about the

98 chapter 5

    id28

effect of various explanatory variables (features). in text classi   cation, perhaps we
want to know if logically negative words (no, not, never) are more likely to be asso-
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. however, in doing so it   s necessary to control for po-
tential confounds: other factors that might in   uence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). or we might be study-
ing the relationship between nlp-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). in such cases, id28 allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.

5.8 advanced: deriving the gradient equation

in this section we give the derivation of the gradient of the cross-id178 loss func-
tion lce for id28. let   s start with some quick calculus refreshers.
first, the derivative of ln(x):

d
dx

ln(x) =

1
x

second, the (very elegant) derivative of the sigmoid:

d   (z)

dz

=    (z)(1       (z))

(5.37)

(5.38)

chain rule

finally, the chain rule of derivatives. suppose we are computing the derivative
of a composite function f (x) = u(v(x)). the derivative of f (x) is the derivative of
u(x) with respect to v(x) times the derivative of v(x) with respect to x:

d f
dx

=

du
dv   

dv
dx

(5.39)

first, we want to know the derivative of the id168 with respect to a single

weight w j (we   ll need to compute it for each weight, and for the bias):

    ll(w,b)

    w j

=

   
    w j     [ylog   (w   x + b) + (1    y)log (1       (w   x + b))]
=    (cid:20)    

(1    y)log [1       (w   x + b)](cid:21)

ylog   (w   x + b) +

   
    w j

    w j

(5.40)

next, using the chain rule, and relying on the derivative of log:
    ll(w,b)

y

    w j

=    

   (w   x + b)

   
    w j

   (w   x + b)   

1    y

1       (w   x + b)

   
    w j

1       (w   x + b)
(5.41)

rearranging terms:
    ll(w,b)

    w j

=    (cid:20)

y

   (w   x + b)    

1    y

1       (w   x + b)(cid:21)    

    w j

   (w   x + b)

(5.42)

and now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with eq. 5.43:

5.9

    summary

99

    ll(w,b)

    w j

5.9 summary

y       (w   x + b)
y       (w   x + b)

   (w   x + b)[1       (w   x + b)](cid:21)   (w   x + b)[1       (w   x + b)]
   (w   x + b)[1       (w   x + b)](cid:21)   (w   x + b)[1       (w   x + b)]x j

=    (cid:20)
=    (cid:20)
=    [y       (w   x + b)]x j
= [   (w   x + b)    y]x j

    (w   x + b)

    w j

(5.43)

this chapter introduced the id28 model of classi   cation.

    id28 is a supervised machine learning classi   er that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a id203. a
threshold is used to make a decision.

    id28 can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes (multinomial id28, for ex-
ample for n-ary text classi   cation, part-of-speech labeling, etc.).

bilities.

id168, such as the cross-id178 loss, that must be minimized.

    multinomial id28 uses the softmax function to compute proba-
    the weights (vector w and bias b) are learned from a labeled training set via a
    minimizing this id168 is a id76 problem, and iterative
    id173 is used to avoid over   tting.
    id28 is also one of the most useful analytic tools, because of its

algorithms like id119 are used to    nd the optimal weights.

ability to transparently study the importance of individual features.

bibliographical and historical notes

id28 was developed in the    eld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(cox, 1969). starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (sankoff and labov,
1979).

nonetheless, id28 didn   t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. the    rst source was the neighboring    elds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to nlp. indeed a very early use of logistic
regression for document routing was one of the    rst nlp applications to use (lsi)
embeddings as word representations (sch  utze et al., 1995).

at the same time in the early 1990s id28 was developed and ap-
plied to nlp at ibm research under the name maximum id178 modeling or

maximum
id178

100 chapter 5

    id28

maxent (berger et al., 1996), seemingly independent of the statistical literature. un-
der that name it was applied to id38 (rosenfeld, 1996), part-of-speech
tagging ((ratnaparkhi, 1996)), parsing (ratnaparkhi, 1997), and text classi   cation
(nigam et al., 1999).

more on classi   cation can be found in machine learning textbooks (hastie et al. 2001,

witten and frank 2005, bishop 2006, murphy 2012).

exercises

chapter

6 vector semantics

the asphalt that los angeles is famous for occurs mainly on its freeways. but in the
middle of the city is another patch of asphalt, the la brea tar pits, and this asphalt
preserves millions of fossil bones from the last of the ice ages of the pleistocene
epoch. one of these fossils is the smilodon, or sabre-toothed tiger, instantly rec-
ognizable by its long canines. five million years ago or so, a completely different
sabre-tooth tiger called thylacosmilus lived
in argentina and other parts of south amer-
ica. thylacosmilus was a marsupial whereas
smilodon was a placental mammal, but thy-
lacosmilus had the same long upper canines
and, like smilodon, had a protective bone
   ange on the lower jaw. the similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (gould, 1980).

the role of context is also important in the similarity of a less biological kind
of organism: the word. words that occur in similar contexts tend to have similar
meanings. this link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis. the hypothesis was
   rst formulated in the 1950s by linguists like joos (1950), harris (1954), and firth
(1957), who noticed that words which are synonyms (like oculist and eye-doctor)
tended to occur in the same environment (e.g., near words like eye or examined)
with the amount of meaning difference between two words    corresponding roughly
to the amount of difference in their environments    (harris, 1954, 157).

in this chapter we introduce a model known as vector semantics, which instan-
tiates this linguistic hypothesis by learning representations of the meaning of words
directly from their distributions in texts. these representations are used in every
natural language processing application that makes use of meaning. these word
representations are also the    rst example we will see in the book of representation
learning, automatically learning useful representations of the input text. finding
such unsupervised ways to learn representations of the input, instead of creating
representations by hand via feature engineering, is an important focus of recent
nlp research (bengio et al., 2013).

we   ll begin, however, by introducing some basic principles of word meaning,
which will motivate the vector semantic models of this chapter as well as extensions
that we   ll return to in appendix c, chapter 19, and chapter 18.

distributional
hypothesis

vector
semantics

representation
learning

102 chapter 6

    vector semantics

6.1 lexical semantics

how should we represent the meaning of a word? in the id165 models we saw in
chapter 3, and in many traditional nlp applications, our only representation of a
word is as a string of letters, or perhaps as an index in a vocabulary list. this repre-
sentation is not that different from a tradition in philosophy, perhaps you   ve seen it
in introductory logic classes, in which the meaning of words is often represented by
just spelling the word with small capital letters; representing the meaning of    dog   
as dog, and    cat    as cat).

representing the meaning of a word by capitalizing it is a pretty unsatisfactory

model. you might have seen the old philosophy joke:

q: what   s the meaning of life?
a: life

lexical
semantics

lemma
citation form

wordform

surely we can do better than this! after all, we   ll want a model of word meaning
to do all sorts of things for us. it should tell us that some words have similar mean-
ings (cat is similar to dog), other words are antonyms (cold is the opposite of hot). it
should know that some words have positive connotations (happy) while others have
negative connotations (sad). it should represent the fact that the meanings of buy,
sell, and pay offer differing perspectives on the same underlying purchasing event
(if i buy something from you, you   ve probably sold it to me, and i likely paid you).
more generally, a model of word meaning should allow us to draw useful infer-
ences that will help us solve meaning-related tasks like question-answering, sum-
marization, paraphrase or plagiarism detection, and dialogue.

in this section we summarize some of these desiderata, drawing on results in the

linguistic study of word meaning, which is called lexical semantics.

lemmas and senses let   s start by looking at how one word (we   ll choose mouse)
might be de   ned in a dictionary: 1

mouse (n)
1.
2.

any of numerous small rodents...
a hand-operated device that controls a cursor...

here the form mouse is the lemma, also called the citation form. the form
mouse would also be the lemma for the word mice; dictionaries don   t have separate
de   nitions for in   ected forms like mice. similarly sing is the lemma for sing, sang,
sung. in many languages the in   nitive form is used as the lemma for the verb, so
spanish dormir    to sleep    is the lemma for duermes    you sleep   . the speci   c forms
sung or carpets or sing or duermes are called wordforms.

as the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. we call each
of these aspects of the meaning of mouse a word sense. the fact that lemmas can be
homonymous (have multiple senses) can make interpretation dif   cult (is someone
who types    mouse info    to a search engine looking for a pet or a tool?). appendix c
will discuss the problem of homonymy, and introduce id51,
the task of determining which sense of a word is being used in a particular context.

relationships between words or senses one important component of word mean-
ing is the relationship between word senses. for example when one word has a sense

1 this example shortened from the online dictionary id138, discussed in appendix c.

6.1

    lexical semantics

103

synonym

whose meaning is identical to a sense of another word, or nearly identical, we say
the two senses of those two words are synonyms. synonyms include such pairs as

couch/sofa vomit/throw up    lbert/hazelnut car/automobile

propositional
meaning

principle of
contrast

antonym

reversives

similarity

a more formal de   nition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable one for the other in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true. we often say in this case that the two words have the same
propositional meaning.

while substitutions between some pairs of words like car / automobile or water /
h2o are truth preserving, the words are still not identical in meaning. indeed, proba-
bly no two words are absolutely identical in meaning. one of the fundamental tenets
of semantics, called the principle of contrast (br  eal 1897, ?, clark 1987), is the as-
sumption that a difference in linguistic form is always associated with at least some
difference in meaning. for example, the word h2o is used in scienti   c contexts and
would be inappropriate in a hiking guide   water would be more appropriate    and
this difference in genre is part of the meaning of the word. in practice, the word
synonym is therefore commonly used to describe a relationship of approximate or
rough synonymy.

where synonyms are words with identical or similar meanings, antonyms are

words with an opposite meaning, like:

long/short big/little fast/slow cold/hot dark/light
rise/fall

up/down in/out

two senses can be antonyms if they de   ne a binary opposition or are at opposite
ends of some scale. this is the case for long/short, fast/slow, or big/little, which are
at opposite ends of the length or size scale. another group of antonyms, reversives,
describe change or movement in opposite directions, such as rise/fall or up/down.

antonyms thus differ completely with respect to one aspect of their meaning   
their position on a scale or their direction   but are otherwise very similar, sharing
almost all other aspects of meaning. thus, automatically distinguishing synonyms
from antonyms can be dif   cult.

word similarity: while words don   t have many synonyms, most words do have
lots of similar words. cat is not a synonym of dog, but cats and dogs are certainly
similar words. in moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.

the notion of word similarity is very useful in larger semantic tasks. for exam-
ple knowing how similar two words are is helpful if we are trying to decide if two
phrases or sentences mean similar things. phrase or sentence similarity is useful in
such natural language understanding tasks as id53, id141, and
summarization.

one way of getting values for word similarity is to ask humans to judge how
similar one word is to another. a number of datasets have resulted from such ex-
periments. for example the siid113x-999 dataset (hill et al., 2015) gives values on
a scale from 0 to 10, like the examples below, which range from near-synonyms
(vanish, disappear) to pairs that scarcely seem to have anything in common (hole,
agreement):

104 chapter 6

    vector semantics

vanish disappear
behave obey
belief
muscle bone
modest    exible
hole

9.8
7.3
impression 5.95
3.65
0.98
agreement 0.3

relatedness
association

semantic    eld

topic models

semantic frame

hyponym

hypernym

superordinate

word relatedness: the meaning of two words can be related in ways others than
similarity. one such class of connections is called word relatedness (budanitsky
and hirst, 2006), also traditionally called word association in psychology.

consider the meanings of the words coffee and cup; coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is an
manufactured object with a particular shape).

but coffee and cup are clearly related; they are associated in the world by com-
monly co-participating in a shared event (the event of drinking coffee out of a cup).
similarly the nouns scalpel and surgeon are not similar but are related eventively (a
surgeon tends to make use of a scalpel).

one common kind of relatedness between words is if they belong to the same
semantic    eld. a semantic    eld is a set of words which cover a particular semantic
domain and bear structured relations with each other.

for example, words might be related by being in the semantic    eld of hospitals
(surgeon, scalpel, nurse, anaesthetic, hospital), restaurants (waiter, menu, plate,
food, chef), or houses (door, roof, kitchen, family, bed).

semantic    elds are also related to topic models, like latent dirichlet alloca-
tion, lda, which apply unsupervised learning on large sets of texts to induce sets
of associated words from text. semantic    elds and topic models are a very useful
tool for discovering topical structure in documents.
semantic frames and roles: closely related to semantic    elds is the idea of a
semantic frame. a semantic frame is a set of words that denote perspectives or
participants in a particular type of event. a commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service
is performed. this event can be encoded lexically by using verbs like buy (the
event from the perspective of the buyer) sell (from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer. frames have semantic roles
(like buyer, seller, goods, money), and words in a sentence can take on these roles.
knowing that buy and sell have this relation makes it possible for a system to
know that a sentence like sam bought the book from ling could be paraphrased as
ling sold the book to sam, and that sam has the role of the buyer in the frame and
ling the seller. being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
taxonomic relations: another way word senses can be related is taxonomically.
a word (or sense) is a hyponym of another word or sense if the    rst is more speci   c,
denoting a subclass of the other. for example, car is a hyponym of vehicle; dog is
a hyponym of animal, and mango is a hyponym of fruit. conversely, we say that
vehicle is a hypernym of car, and animal is a hypernym of dog. it is unfortunate that
the two words (hypernym and hyponym) are very similar and hence easily confused;
for this reason, the word superordinate is often used instead of hypernym.

superordinate vehicle fruit
subordinate

car

furniture mammal

mango chair

dog

is-a

connotations

sentiment

6.1

    lexical semantics

105

we can de   ne hypernymy more formally by saying that the class denoted by the
superordinate extensionally includes the class denoted by the hyponym. thus, the
class of animals includes as members all dogs, and the class of moving actions in-
cludes all walking actions. hypernymy can also be de   ned in terms of entailment.
under this de   nition, a sense a is a hyponym of a sense b if everything that is a is
also b, and hence being an a entails being a b, or    x a(x)     b(x). hyponymy/hy-
pernymy is usually a transitive relation; if a is a hyponym of b and b is a hyponym
of c, then a is a hyponym of c. another name for the hypernym/hyponym structure
is the is-a hierarchy, in which we say a is-a b, or b subsumes a.

hypernymy is useful for tasks like id123 or id53;
knowing that leukemia is a type of cancer, for example, would certainly be useful in
answering questions about leukemia.

connotation: finally, words have affective meanings or connotations. the word
connotation has different meanings in different    elds, but here we use it to mean
the aspects of a word   s meaning that are related to a writer or reader   s emotions,
sentiment, opinions, or evaluations. for example some words have positive conno-
tations (happy) while others have negative connotations (sad). some words describe
positive evaluation (great, love) and others negative evaluation (terrible, hate). pos-
itive or negative evaluation expressed through language is called sentiment, as we
saw in chapter 4, and word sentiment plays a role in important tasks like sentiment
analysis, stance detection, and many aspects of natural language processing to the
language of politics and consumer reviews.

early work on affective meaning (osgood et al., 1957) found that words varied
along three important dimensions of affective meaning. these are now generally
called valence, arousal, and dominance, de   ned as follows:

valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus

thus words like happy or satis   ed are high on valence, while unhappy or an-
noyed are low on valence. excited or frenzied are high on arousal, while relaxed
or calm are low on arousal. important or controlling are high on dominance, while
awed or in   uenced are low on dominance. each word is thus represented by three
numbers, corresponding to its value on each of the three dimensions, like the exam-
ples below:

valence arousal dominance

courageous 8.05
music
7.67
heartbreak 2.45
6.71
cub
life
6.68

5.5
5.57
5.65
3.95
5.59

7.38
6.5
3.58
4.24
5.89

osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word   s
rating on the three scales. this revolutionary idea that word meaning word could
be represented as a point in space (e.g., that part of the meaning of heartbreak can
be represented as the point [2.45,5.65,3.58]) was the    rst expression of the vector
semantics models that we introduce next.

106 chapter 6

    vector semantics

6.2 vector semantics

vector
semantics

how can we build a computational model that successfully deals with the different
aspects of word meaning we saw in the previous section (word senses, word simi-
larity and relatedness, lexical    elds and frames, connotation)?

a perfect model that completely deals with each of these aspects of word mean-
ing turns out to be elusive. but the current best model, called vector semantics,
draws its inspiration from linguistic and philosophical work of the 1950   s.

during that period, the philosopher ludwig wittgenstein, skeptical of the possi-
bility of building a completely formal theory of meaning de   nitions for each word,
suggested instead that    the meaning of a word is its use in the language    (wittgen-
stein, 1953, pi 43). that is, instead of using some logical language to de   ne each
word, we should de   ne words by some representation of how the word was used by
actual people in speaking and understanding.

linguists of the period like joos (1950), harris (1954), and firth (1957) (the
linguistic distributionalists), came up with a speci   c idea for realizing wittgenstein   s
intuition: de   ne a word by the environment or distribution it occurs in in language
use. a word   s distribution is the set of contexts in which it occurs, the neighboring
words or grammatical environments. the idea is that two words that occur in very
similar distributions (that occur together with very similar words) are likely to have
the same meaning.

let   s see an example illustrating this distributionalist approach. suppose you
didn   t know what the cantonese word ongchoi meant, but you do see it in the fol-
lowing sentences or contexts:
(6.1) ongchoi is delicious sauteed with garlic.
(6.2) ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...

and furthermore let   s suppose that you had seen many of these context words

occurring in contexts like:
(6.4) ...spinach sauteed with garlic over rice...
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens

the fact that ongchoi occurs with words like rice and garlic and delicious and
salty, as do words like spinach, chard, and collard greens might suggest to the reader
that ongchoi is a leafy green similar to these other leafy greens.2

vector semantics thus combines two intuitions:

we can do the same thing computationally by just counting words in the context
of ongchoi; we   ll tend to see words like sauteed and eaten and garlic. the fact that
these words and other similar context words also occur around the word spinach or
collard greens can help us discover the similarity between these words and ongchoi.
the distributionalist intuition
(de   ning a word by counting what other words occur in its environment), and the
vector intuition of osgood et al. (1957) we saw in the last section on connotation:
de   ning the meaning of a word w as a vector, a list of numbers, a point in n-
dimensional space. there are various versions of vector semantics, each de   ning
the numbers in the vector somewhat differently, but in each case the numbers are
based in some way on counts of neighboring words.

2

it   s in fact ipomoea aquatica, a relative of morning glory sometimes called water spinach in english.

embeddings

6.2

    vector semantics

107

figure 6.1 a two-dimensional (id167) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. the original 60-
dimensional embeddings were trained for a id31 task. simpli   ed from li et al.
(2015).

the idea of vector semantics is thus to represent a word as a point in some multi-
dimensional semantic space. vectors for representing words are generally called
embeddings, because the word is embedded in a particular vector space. fig. 6.1
displays a visualization of embeddings that were learned for a id31
task, showing the location of some selected words projected down from the original
60-dimensional space into a two dimensional space.

notice that positive and negative words seem to be located in distinct portions of
the space (and different also from the neutral function words). this suggests one of
the great advantages of vector semantics: it offers a    ne-grained model of meaning
that lets us also implement word similarity (and phrase similarity). for example,
the id31 classi   er we saw in chapter 4 only works if enough of the
important sentimental words that appear in the test set also appeared in the training
set. but if words were represented as embeddings, we could assign sentiment as
long as words with similar meanings as the test set words occurred in the training
set. vector semantic models are also extremely practical because they can be learned
automatically from text without any complex labeling or supervision.

as a result of these advantages, vector models of meaning are now the standard
way to represent the meaning of words in nlp. in this chapter we   ll introduce the
two most commonly used models. . first is the tf-idf model, often used a a baseline,
in which the meaning of a word is de   ned by a simple function of the counts of
nearby words. we will see that this method results in very long vectors that are
sparse, i.e. contain mostly zeros (since most words simply never occur in the context
of others).

then we   ll introduce the id97 model, one of a family of models that are

ways of constructing short, dense vectors that have useful semantic properties.

we   ll also introduce the cosine, the standard way to use embeddings (vectors)
to compute functions like semantic similarity, the similarity between two words,
two sentences, or two documents, an important tool in practical applications like
id53, summarization, or automatic essay grading.

goodnicebadworstnot goodwonderfulamazingterrificdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto   sareisathan108 chapter 6

    vector semantics

6.3 words and vectors

vector or distributional models of meaning are generally based on a co-occurrence
matrix, a way of representing how often words co-occur. this matrix can be con-
structed in various ways; let   s s begin by looking at one such co-occurrence matrix,
a term-document matrix.

6.3.1 vectors and documents
in a term-document matrix, each row represents a word in the vocabulary and each
column represents a document from some collection of documents. fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by shakespeare. each cell in this matrix represents the number of times
a particular word (de   ned by the row) occurs in a particular document (de   ned by
the column). thus fool appeared 58 times in twelfth night.

term-document
matrix

as you like it

twelfth night

julius caesar

henry v

1
114
36
20

battle
good
fool
wit
figure 6.2 the term-document matrix for four words in four shakespeare plays. each cell
contains the number of times the (row) word occurs in the (column) document.

0
80
58
15

13
89
4
3

7
62
1
2

vector space
model

vector

vector space
dimension

the term-document matrix of fig. 6.2 was    rst de   ned as part of the vector
space model of information retrieval (salton, 1971). in this model, a document is
represented as a count vector, a column in fig. 6.3.

to review some basic id202, a vector is, at heart, just a list or array
of numbers. so as you like it is represented as the list [1,114,36,20] and julius
caesar is represented as the list [7,62,1,2]. a vector space is a collection of vectors,
characterized by their dimension.
in the example in fig. 6.3, the vectors are of
dimension 4, just so they    t on the page; in real term-document matrices, the vectors
representing each document would have dimensionality |v|, the vocabulary size.
the ordering of the numbers in a vector space is not arbitrary; each position
indicates a meaningful dimension on which the documents can vary. thus the    rst
dimension for both these vectors corresponds to the number of times the word battle
occurs, and we can compare each dimension, noting for example that the vectors for
as you like it and twelfth night have similar values (1 and 0, respectively) for the
   rst dimension.

as you like it

twelfth night

julius caesar

henry v

battle
good
fool
wit
figure 6.3 the term-document matrix for four words in four shakespeare plays. the red
boxes show that each document is represented as a column vector of length four.

1
114
36
20

0
80
58
15

13
89
4
3

7
62
1
2

we can think of the vector for a document as identifying a point in |v|-dimensional
space; thus the documents in fig. 6.3 are points in 4-dimensional space. since 4-
dimensional spaces are hard to draw in textbooks, fig. 6.4 shows a visualization in

6.3

    words and vectors

109

two dimensions; we   ve arbitrarily chosen the dimensions corresponding to the words
battle and fool.

figure 6.4 a spatial visualization of the document vectors for the four shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle and fool.
the comedies have high values for the fool dimension and low values for the battle dimension.

term-document matrices were originally de   ned as a means of    nding similar
documents for the task of document information retrieval. two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. the vectors for the comedies as you
like it [1,114,36,20] and twelfth night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they do like julius caesar [7,62,1,2] or henry
v [13,89,4,3]. we can see the intuition with the raw numbers; in the    rst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in fig. 6.4; we   ll see very shortly how to quantify this intuition
more formally.

a real term-document matrix, of course, wouldn   t just have 4 rows and columns,
let alone 2. more generally, the term-document matrix x has |v| rows (one for each
word type in the vocabulary) and d columns (one for each document in the collec-
tion); as we   ll see, vocabulary sizes are generally at least in the tens of thousands,
and the number of documents can be enormous (think about all the pages on the
web).

information retrieval (ir) is the task of    nding the document d from the d
documents in some collection that best matches a query q. for ir we   ll therefore also
represent a query by a vector, also of length |v|, and we   ll need a way to compare
two vectors to    nd how similar they are. (doing ir will also require ef   cient ways
to store and manipulate these vectors, which is accomplished by making use of the
convenient fact that these vectors are sparse, i.e., mostly zeros).

later in the chapter we   ll introduce some of the components of this vector com-

parison process: the tf-idf term weighting, and the cosine similarity metric.

information
retrieval

6.3.2 words as vectors
we   ve seen that documents can be represented as vectors in a vector space. but
vector semantics can also be used to represent the meaning of words, by associating
each word with a vector.

the word vector is now a row vector rather than a column vector, and hence the
dimensions of the vector are different. the four dimensions of the vector for fool,

row vector

51015202530510henry v [4,13]as you like it [36,1]julius caesar [1,7]battle fooltwelfth night [58,0]1540354045505560110 chapter 6

    vector semantics

term-term
matrix
word-word
matrix

[36,58,1,4], correspond to the four shakespeare plays. the same four dimensions are
used to form the vectors for the other 3 words: wit, [20, 15, 2, 3]; battle, [1,0,7,13];
and good [114,80,62,89]. each entry in the vector thus represents the counts of the
word   s occurrence in the document corresponding to that dimension.

for documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. this same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
the term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.

however, it is most common to use a different kind of context for the dimensions
of a word   s vector representation. rather than the term-document matrix we use the
term-term matrix, more commonly called the word-word matrix or the term-
context matrix, in which the columns are labeled by words rather than documents.
this matrix is thus of dimensionality |v|  |v| and each cell records the number of
times the row (target) word and the column (context) word co-occur in some context
in some training corpus. the context could be the document, in which case the cell
represents the number of times the two words appear in the same document. it is
most common, however, to use smaller contexts, generally a window around the
word, for example of 4 words to the left and 4 words to the right, in which case
the cell represents the number of times (in some training corpus) the column word
occurs in such a   4 word window around the row word.
brown corpus (just one example of each word):

for example here are 7-word windows surrounding four sample words from the

sugar, a sliced lemon, a tablespoonful of apricot

their enjoyment. cautiously she sampled her    rst pineapple
well suited to programming on the digital computer.

jam, a pinch each of,
and another fruit whose taste she likened
in    nding the optimal r-stage policy from

for the purpose of gathering data and information necessary for the study authorized in the

for each word we collect the counts (from the windows around each occurrence)
of the occurrences of context words. fig. 6.5 shows a selection from the word-word
co-occurrence matrix computed from the brown corpus for these four words.

aardvark

computer

data

pinch

result

sugar

...

apricot
pineapple

digital

...
...
...
...
...

0
0
0
0

0
0
2
1

0
0
1
6

1
1
0
0

0
0
1
4

1
1
0
0

information
figure 6.5 co-occurrence vectors for four words, computed from the brown corpus, show-
ing only six of the dimensions (hand-picked for pedagogical purposes). the vector for the
word digital is outlined in red. note that a real vector would have vastly more dimensions
and thus be much sparser.

note in fig. 6.5 that the two words apricot and pineapple are more similar to
each other (both pinch and sugar tend to occur in their window) than they are to
other words like digital; conversely, digital and information are more similar to each
other than, say, to apricot. fig. 6.6 shows a spatial visualization.

note that |v|, the length of the vector, is generally the size of the vocabulary,
usually between 10,000 and 50,000 words (using the most frequent words in the
training corpus; keeping words after about the most frequent 50,000 or so is gener-
ally not helpful). but of course since most of these numbers are zero these are sparse
vector representations, and there are ef   cient algorithms for storing and computing
with sparse matrices.

6.4

    cosine for measuring similarity

111

figure 6.6 a spatial visualization of word vectors for digital and information, showing just
two of the dimensions, corresponding to the words data and result.

now that we have some intuitions, let   s move on to examine the details of com-
puting word similarity. afterwards we   ll discuss the tf-idf method of weighting
cells.

6.4 cosine for measuring similarity

to de   ne similarity between two target words v and w, we need a measure for taking
two such vectors and giving a measure of vector similarity. by far the most common
similarity metric is the cosine of the angle between the vectors.

the cosine   like most measures for vector similarity used in nlp   is based on

the dot product operator from id202, also called the inner product:

dot-product((cid:126)v,(cid:126)w) =(cid:126)v   (cid:126)w =

n(cid:88)i=1

viwi = v1w1 + v2w2 + ... + vnwn

(6.7)

as we will see, most metrics for similarity between vectors are based on the dot
product. the dot product acts as a similarity metric because it will tend to be high
just when the two vectors have large values in the same dimensions. alternatively,
vectors that have zeros in different dimensions   orthogonal vectors   will have a
dot product of 0, representing their strong dissimilarity.

this raw dot-product, however, has a problem as a similarity metric: it favors

long vectors. the vector length is de   ned as

dot product
inner product

vector length

|(cid:126)v| =(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:88)i=1

v2
i

(6.8)

the dot product is higher if a vector is longer, with higher values in each dimension.
more frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. the raw dot product
thus will be higher for frequent words. but this is a problem; we   d like a similarity
metric that tells us how similar two words are regardless of their frequency.

the simplest way to modify the dot product to normalize for the vector length is
to divide the dot product by the lengths of each of the two vectors. this normalized
dot product turns out to be the same as the cosine of the angle between the two

12345612digital [1,1]result datainformation [6,4] 34112 chapter 6

    vector semantics

vectors, following from the de   nition of the dot product between two vectors (cid:126)a and
(cid:126)b:

(cid:126)a  (cid:126)b = |(cid:126)a||(cid:126)b|cos  
(cid:126)a  (cid:126)b
|(cid:126)a||(cid:126)b|

= cos  

(6.9)

cosine

unit vector

the cosine similarity metric between two vectors (cid:126)v and (cid:126)w thus can be computed as:

cosine((cid:126)v,(cid:126)w) =

(cid:126)v   (cid:126)w
|(cid:126)v||(cid:126)w|

=

(6.10)

w2
i

viwi

n(cid:88)i=1
(cid:118)(cid:117)(cid:117)(cid:116)
i(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:88)i=1
n(cid:88)i=1

v2

for some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. thus we could compute a unit vector from (cid:126)a by
dividing it by |(cid:126)a|. for unit vectors, the dot product is the same as the cosine.
the cosine value ranges from 1 for vectors pointing in the same direction, through
0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.
but raw frequency values are non-negative, so the cosine for these vectors ranges
from 0   1.

let   s see how the cosine computes which of the words apricot or digital is closer
in meaning to information, just using raw counts from the following simpli   ed table:

apricot
digital

information

large data computer

2
0
1

0
1
6

0
2
1

cos(apricot,information) =

cos(digital,information) =

2 + 0 + 0

   4 + 0 + 0   1 + 36 + 1
   0 + 1 + 4   1 + 36 + 1
=

0 + 6 + 2

=

2
2   38
8
   38   5

= .16

= .58

(6.11)

the model decides that information is closer to digital than it is to apricot, a

result that seems sensible. fig. 6.7 shows a visualization.

6.5 tf-idf: weighing terms in the vector

the co-occurrence matrix in fig. 6.5 represented each cell by the raw frequency of
the co-occurrence of two words.

it turns out, however, that simple frequency isn   t the best measure of association
between words. one problem is that raw frequency is very skewed and not very
discriminative. if we want to know what kinds of contexts are shared by apricot
and pineapple but not by digital and information, we   re not going to get good dis-
crimination from words like the, it, or they, which occur frequently with all sorts of
words and aren   t informative about any particular word. we saw this also in fig. 6.3
for the shakespeare corpus; the dimension for the word good is not very discrimina-
tive between plays; good is simply a frequent word and has roughly equivalent high
frequencies in each of the plays.

6.5

    tf-idf: weighing terms in the vector

113

figure 6.7 a graphical demonstration of cosine similarity, showing vectors for three words
(apricot, digital, and information) in the two dimensional space de   ned by counts of the
words data and large in the neighborhood. note that the angle between digital and informa-
tion is smaller than the angle between apricot and information. when two vectors are more
similar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the
angle between two vectors is smallest (0   ); the cosine of all other angles is less than 1.

it   s a bit of a paradox. word that occur nearby frequently (maybe sugar appears
often in our corpus near apricot) are more important than words that only appear
once or twice. yet words that are too frequent   ubiquitous, like the or good    are
unimportant. how can we balance these two con   icting constraints?

the tf-idf algorithm (the    -    here is a hyphen, not a minus sign) algorithm is the

product of two terms, each term capturing one of these two intuitions:

term frequency

1. the    rst is the term frequency (luhn, 1957): the frequency of the word in the
document. normally we want to downweight the raw frequency a bit, since
a word appearing 100 times in a document doesn   t make that word 100 times
more likely to be relevant to the meaning of the document. so we generally
use the log10 of the frequency, resulting in the following de   nition for the term
frequency weight:

tft,d =(cid:26) 1 + log10 count(t,d)

0

if count(t,d) > 0
otherwise

thus terms which occur 10 times in a document would have a tf=2, 100 times
in a document tf=3, 1000 times tf=4, and so on.

2. the second factor is used to give a higher weight to words that occur only
in a few documents. terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that
occur frequently across the entire collection aren   t as helpful. the document
frequency dft of a term t is simply the number of documents it occurs in. by
contrast, the collection frequency of a term is the total number of times the
word appears in the whole collection in any document. consider in the col-
lection shakespeare   s 37 plays the two words romeo and action. the words
have identical collection frequencies of 113 (they both occur 113 times in all
the plays) but very different document frequencies, since romeo only occurs
in a single play. if our goal is    nd documents about the romantic tribulations
of romeo, the word romeo should be highly weighted:

document
frequency

1234567123digitalapricotinformationdimension 1:    large   dimension 2:    data   114 chapter 6

    vector semantics

collection frequency document frequency

romeo 113
113
action

1
31

idf

we assign importance to these more discriminative words like romeo via
the inverse document frequency or idf term weight (sparck jones, 1972).
the idf is de   ned using the fraction n/dft, where n is the total number of
documents in the collection, and dft is the number of documents in which
term t occurs. the fewer documents in which a term occurs, the higher this
weight. the lowest weight of 1 is assigned to terms that occur in all the
documents.
in shakespeare
we would use a play; when processing a collection of encyclopedia articles
like wikipedia, the document is a wikipedia page; in processing newspaper
articles, the document is a single article. occasionally your corpus might
not have appropriate document divisions and you might need to break up the
corpus into documents yourself for the purposes of computing idf.

it   s usually clear what counts as a document:

because of the large number of documents in many collections, this mea-
sure is usually squashed with a log function. the resulting de   nition for in-
verse document frequency (idf) is thus

dft(cid:19)
idft = log10(cid:18) n

(6.12)

here are some idf values for some words in the shakespeare corpus, ranging
from extremely informative words which occur in only one play like romeo, to
those that occur in a few like salad or falstaff, to those which are very common like
fool or so common as to be completely non-discriminative since they occur in all 37
plays like good or sweet.3

word
romeo
salad
falstaff
forest
battle
fool
good
sweet

df
1
2
4
12
21
36
37
37

idf
1.57
1.27
0.967
0.489
0.074
0.012
0
0

tf-idf

the tf-idf weighting of the value for word t in document d, wt,d thus combines

term frequency with idf:

wt,d = tft,d    idft

(6.13)

fig. 6.8 applies tf-idf weighting to the shakespeare term-document matrix in fig. 6.2.
note that the tf-idf values for the dimension corresponding to the word good have
now all become 0; since this word appears in every document, the tf-idf algorithm
leads it to be ignored in any comparison of the plays. similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.

the tf-idf weighting is by far the dominant way of weighting co-occurrence ma-
trices in information retrieval, but also plays a role in many other aspects of natural

3 sweet was one of shakespeare   s favorite adjectives, a fact probably related to the increased use of
sugar in european recipes around the turn of the 16th century (jurafsky, 2014, p. 175).

6.6

    applications of the tf-idf vector model
henry v

twelfth night

julius caesar

115

as you like it

0

0
0

0.22

0.074

0.019
0.049

battle
good
fool
wit
figure 6.8 a tf-idf weighted term-document matrix for four words in four shakespeare
plays, using the counts in fig. 6.2. note that the idf weighting has eliminated the importance
of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word
fool.

0.0036
0.018

0.0083
0.022

0.021
0.044

0.28

0

0

language processing. it   s also a great baseline, the simple thing to try    rst. we   ll
look at other weightings like ppmi (positive pointwise mutual information) in sec-
tion 6.7.

6.6 applications of the tf-idf vector model

in summary, the vector semantics model we   ve described so far represents a target
word as a vector with dimensions corresponding to all the words in the vocabulary
(length |v|, with vocabularies of 20,000 to 50,000), which is also sparse (most values
are zero). the values in each dimension are the frequency with which the target
word co-occurs with each neighboring context word, weighted by tf-idf. the model
computes the similarity between two words x and y by taking the cosine of their
tf-idf vectors; high cosine, high similarity. this entire model is sometimes referred
to for short as the tf-idf model, after the weighting function.

one common use for a tf-idf model is to compute word similarity, a useful tool
for tasks like    nding word paraphrases, tracking changes in word meaning, or au-
tomatically discovering meanings of words in different corpora. for example, we
can    nd the 10 most similar words to any target word w by computing the cosines
between w and each of the v     1 other words, sorting, and looking at the top 10.
the tf-idf vector model can also be used to decide if two documents are similar.
we represent a document by taking the vectors of all the words in the document, and
computing the centroid of all those vectors. the centroid is the multidimensional
version of the mean; the centroid of a set of vectors is a single vector that has the
minimum sum of squared distances to each of the vectors in the set. given k word
vectors w1,w2, ...,wk, the centroid document vector d is:

d =

w1 + w2 + ... + wk

k

(6.14)

centroid

document
vector

given two documents, we can then compute their document vectors d1 and d2,

and estimate the similarity between the two documents by cos(d1,d2).

document similarity is also useful for all sorts of applications; information re-
trieval, plagiarism detection, news recommender systems, and even for digital hu-
manities tasks like comparing different versions of a text to see which are similar to
each other.

116 chapter 6

    vector semantics

6.7 optional: pointwise mutual information (pmi)

an alternative weighting function to tf-idf is called ppmi (positive pointwise mutual
information). ppmi draws on the intuition that best way to weigh the association
between two words is to ask how much more the two words co-occur in our corpus
than we would have a priori expected them to appear by chance.

pointwise mutual information (fano, 1961)4 is one of the most important con-
cepts in nlp. it is a measure of how often two events x and y occur, compared with
what we would expect if they were independent:

pointwise
mutual
information

i(x,y) = log2

p(x,y)
p(x)p(y)

(6.16)

the pointwise mutual information between a target word w and a context word

c (church and hanks 1989, church and hanks 1990) is then de   ned as:

pmi(w,c) = log2

p(w,c)
p(w)p(c)

(6.17)

the numerator tells us how often we observed the two words together (assuming
we compute id203 by using the id113). the denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the id203 of two independent events both occurring is just
the product of the probabilities of the two events. thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. pmi is
a useful tool whenever we need to    nd words that are strongly associated.

pmi values range from negative to positive in   nity. but negative pmi values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. to distinguish whether two
words whose individual id203 is each 10   6 occur together more often than
chance, we would need to be certain that the id203 of the two occurring to-
gether is signi   cantly different than 10   12, and this kind of granularity would require
an enormous corpus. furthermore it   s not clear whether it   s even possible to evalu-
ate such scores of    unrelatedness    with human judgments. for this reason it is more
common to use positive pmi (called ppmi) which replaces all negative pmi values
with zero (church and hanks 1989, dagan et al. 1993, niwa and nitta 1994)5:

ppmi(w,c) = max(log2

p(w,c)
p(w)p(c)

,0)

(6.18)

more formally, let   s assume we have a co-occurrence matrix f with w rows (words)
and c columns (contexts), where fi j gives the number of times word wi occurs in

ppmi

4 pointwise mutual information is based on the mutual information between two random variables x
and y , which is de   ned as:

i(x,y ) =

p(x,y)log2

p(x,y)
p(x)p(y)

(6.15)

(cid:88)

(cid:88)

x

y

in a confusion of terminology, fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information
5 positive pmi also cleanly solves the problem of what to do with zero counts, using 0 to replace the
       from log(0).

6.7

    optional: pointwise mutual information (pmi)

117

context c j. this can be turned into a ppmi matrix where ppmii j gives the ppmi
value of word wi with context c j as follows:

(cid:80)w

(cid:80)c

fi j

i=1

j=1 fi j

pi j =

pi    =

(cid:80)c
(cid:80)w
(cid:80)c

j=1 fi j

i=1

j=1 fi j

(cid:80)w
(cid:80)w
(cid:80)c

i=1 fi j

i=1

j=1 fi j

ppmii j = max(log2

p    j =

,0)

pi j
pi    p    j

(6.19)

(6.20)

thus for example we could compute ppmi(w=information,c=data), assuming we
pretended that fig. 6.5 encompassed all the relevant word contexts/dimensions, as
follows:

p(w=information,c=data) =

p(w=information) =

p(c=data) =

6
19
11
19
7
19

= .316

= .579

= .368

ppmi(information,data) = log2(.316/(.368    .579)) = .568

fig. 6.9 shows the joint probabilities computed from the counts in fig. 6.5, and
fig. 6.10 shows the ppmi values.

computer

0
0

0.11
0.05

p(w,context)

pinch
0.05
0.05

0
0

data

0
0

0.05
.32

result

0
0

0.05
0.21

sugar
0.05
0.05

0
0

p(w)
p(w)
0.11
0.11
0.21
0.58

apricot
pineapple

digital

information

p(context)
figure 6.9 replacing the counts in fig. 6.5 with joint probabilities, showing the marginals
around the outside.

0.26

0.11

0.37

0.16

0.11

computer

data

apricot
pineapple

digital

0
0

1.66

0
0
0

pinch
2.25
2.25

0
0

result

0
0
0

sugar
2.25
2.25

0
0

0

0.57

0.47

information
figure 6.10 the ppmi matrix showing the association between words and context words,
computed from the counts in fig. 6.5 again showing    ve dimensions. note that the 0
ppmi values are ones that had a negative pmi; for example pmi(information,computer) =
log2(.05/(.16     .58)) =    0.618, meaning that information and computer co-occur in this
mini-corpus slightly less often than we would expect by chance, and with ppmi we re-
place negative values by zero. many of the zero ppmi values had a pmi of       , like
pmi(apricot,computer) = log2(0/(0.16    0.11)) = log2(0) =       .

pmi has the problem of being biased toward infrequent events; very rare words
tend to have very high pmi values. one way to reduce this bias toward low frequency
events is to slightly change the computation for p(c), using a different function p   (c)
that raises contexts to the power of   :

ppmi   (w,c) = max(log2

p(w,c)

p(w)p   (c)

,0)

(6.21)

118 chapter 6

    vector semantics

p   (c) =

count(c)  

(cid:80)c count(c)  

(6.22)

levy et al. (2015) found that a setting of    = 0.75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in eq. 6.31). this works because raising the id203 to
   = 0.75 increases the id203 assigned to rare contexts, and hence lowers their
pmi (p   (c) > p(c) when c is rare).

another possible solution is laplace smoothing: before computing pmi, a small
constant k (values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. the larger the k, the more the non-zero counts
are discounted.

computer

data

pinch

result

sugar

information
figure 6.11 laplace (add-2) smoothing of the counts in fig. 6.5.

apricot
pineapple

digital

apricot
pineapple

digital

2
2
4
3

0
0

0.62

computer

data

2
2
3
8

0
0
0

3
3
2
2

2
2
3
6

pinch
0.56
0.56

0
0

result

0
0
0

3
3
2
2

sugar
0.56
0.56

0
0

information
figure 6.12 the add-2 laplace smoothed ppmi matrix from the add-2 smoothing counts
in fig. 6.11.

0.37

0.58

0

6.8 id97

in the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to the words in the vocabulary, and whose values were tf-
idf or ppmi functions of the count of the word co-occurring with each neighboring
word. in this section we turn to an alternative method for representing a word: the
use of vectors that are short (of length perhaps 50-500) and dense (most values are
non-zero).

it turns out that dense vectors work better in every nlp task than sparse vec-
tors. while we don   t complete understand all the reasons for this, we have some
intuitions. first, dense vectors may be more successfully included as features in
machine learning systems; for example if we use 100-dimensional word embed-
dings as features, a classi   er can just learn 100 weights to represent a function of
word meaning; if we instead put in a 50,000 dimensional vector, a classi   er would
have to learn tens of thousands of weights for each of the sparse dimensions. sec-
ond, because they contain fewer parameters than sparse vectors of explicit counts,
dense vectors may generalize better and help avoid over   tting. finally, dense vec-
tors may do a better job of capturing synonymy than sparse vectors. for example,
car and automobile are synonyms; but in a typical sparse vector representation, the
car dimension and the automobile dimension are distinct dimensions. because the

skip-gram
sgns
id97

6.8

    id97

119

relationship between these two dimensions is not modeled, sparse vectors may fail
to capture the similarity between a word with car as a neighbor and a word with
automobile as a neighbor.

in this section we introduce one method for very dense, short vectors, skip-
gram with negative sampling, sometimes called sgns. the skip-gram algorithm
is one of two algorithms in a software package called id97, and so sometimes
the algorithm is loosely referred to as id97 (mikolov et al. 2013, mikolov
et al. 2013a). the id97 methods are fast, ef   cient to train, and easily avail-
able online with code and pretrained embeddings. we point to other embedding
methods, like the equally popular glove (pennington et al., 2014), at the end of the
chapter.

the intuition of id97 is that instead of counting how often each word w oc-
curs near, say, apricot, we   ll instead train a classi   er on a binary prediction task:    is
word w likely to show up near apricot?    we don   t actually care about this prediction
task; instead we   ll take the learned classi   er weights as the id27s.

the revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classi   er; a word s that occurs near the target
word apricot acts as gold    correct answer    to the question    is word w likely to show
up near apricot?    this avoids the need for any sort of hand-labeled supervision
signal. this idea was    rst proposed in the task of neural id38, when
bengio et al. (2003) and collobert et al. (2011) showed that a neural language model
(a neural network that learned to predict the next word from prior words) could just
use the next word in running text as its supervision signal, and could be used to learn
an embedding representation for each word as part of doing this prediction task.

we   ll see how to do neural networks in the next chapter, but id97 is a
much simpler model than the neural network language model, in two ways. first,
id97 simpli   es the task (making it binary classi   cation instead of word pre-
diction). second, id97 simpli   es the architecture (training a id28
classi   er instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). the intuition of skip-gram is:

1. treat the target word and a neighboring context word as positive examples.
2. randomly sample other words in the lexicon to get negative samples
3. use id28 to train a classi   er to distinguish those two cases
4. use the regression weights as the embeddings

6.8.1 the classi   er
let   s start by thinking about the classi   cation task, and then turn to how to train.
imagine a sentence like the following, with a target word apricot and assume we   re
using a window of   2 context words:

... lemon,

a [tablespoon of apricot jam,

c1

c2

t

c3

a] pinch ...
c4

our goal is to train a classi   er such that, given a tuple (t,c) of a target word
t paired with a candidate context word c (for example (apricot, jam), or perhaps
(apricot, aardvark) it will return the id203 that c is a real context word (true
for jam, false for aardvark):

p(+|t,c)

(6.23)

120 chapter 6

    vector semantics
the id203 that word c is not a real context word for t is just 1 minus

eq. 6.23:

p(   |t,c) = 1    p(+|t,c)

(6.24)
how does the classi   er compute the id203 p? the intuition of the skip-
gram model is to base this id203 on similarity: a word is likely to occur near
the target if its embedding is similar to the target embedding. how can we compute
similarity between embeddings? recall that two vectors are similar if they have a
high dot product (cosine, the most popular similarity metric, is just a normalized dot
product). in other words:

similarity(t,c)     t    c

(6.25)
of course, the dot product t    c is not a id203, it   s just a number ranging from
0 to    . (recall, for that matter, that cosine isn   t a id203 either). to turn the
dot product into a id203, we   ll use the logistic or sigmoid function    (x), the
fundamental core of id28:

   (x) =

1

1 + e   x

(6.26)

the id203 that word c is a real context word for target word t is thus computed
as:

p(+|t,c) =

1

1 + e   t  c

(6.27)

the sigmoid function just returns a number between 0 and 1, so to make it a proba-
bility we   ll need to make sure that the total id203 of the two possible events (c
being a context word, and c not being a context word) sum to 1.

the id203 that word c is not a real context word for t is thus:

p(   |t,c) = 1    p(+|t,c)

=

e   t  c

1 + e   t  c

(6.28)

equation 6.27 give us the id203 for one word, but we need to take account of
the multiple context words in the window. skip-gram makes the strong but very
useful simplifying assumption that all context words are independent, allowing us to
just multiply their probabilities:

p(+|t,c1:k) =

logp(+|t,c1:k) =

1

1 + e   t  ci

log

1

1 + e   t  ci

(6.29)

(6.30)

k(cid:89)i=1
k(cid:88)i=1

in summary, skip-gram trains a probabilistic classi   er that, given a test target word
t and its context window of k words c1:k, assigns a id203 based on how similar
this context window is to the target word. the id203 is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. we could thus compute this id203 if only we had
embeddings for each word target and context word in the vocabulary. let   s now turn
to learning these embeddings (which is the real goal of training this classi   er in the
   rst place).

6.8

    id97

121

6.8.2 learning skip-gram embeddings
id97 learns embeddings by starting with an initial set of embedding vectors
and then iteratively shifting the embedding of each word w to be more like the em-
beddings of words that occur nearby in texts, and less like the embeddings of words
that don   t occur nearby.

let   s start by considering a single piece of the training data, from the sentence

above:

... lemon,

a [tablespoon of apricot jam,

c1

c2

t

c3

a] pinch ...
c4

this example has a target word t (apricot), and 4 context words in the l =   2

window, resulting in 4 positive training instances (on the left below):
negative examples -
c
twelve

c

t

t
apricot aardvark apricot
apricot puddle
apricot where
apricot coaxial

apricot hello
apricot dear
apricot forever

c
tablespoon

positive examples +
t
apricot
apricot of
apricot preserves
apricot or

for training a binary classi   er we also need negative examples, and in fact skip-
gram uses more negative examples than positive examples, the ratio set by a param-
eter k. so for each of these (t,c) training instances we   ll create k negative samples,
each consisting of the target t plus a    noise word   . a noise word is a random word
from the lexicon, constrained not to be the target word t. the right above shows the
setting where k = 2, so we   ll have 2 negative examples in the negative training set
    for each positive example t,c.
the noise words are chosen according to their weighted unigram frequency
p   (w), where    is a weight. if we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram id203 p(   the   ) we would choose
the word the as a noise word, with unigram id203 p(   aardvark   ) we would
choose aardvark, and so on. but in practice it is common to set    = .75, i.e. use the
weighting p 3

4 (w):

p   (w) =

count(w)  

(cid:80)w(cid:48) count(w(cid:48))  

(6.31)

setting    = .75 gives better performance because it gives rare noise words slightly
higher id203: for rare words, p   (w) > p(w). to visualize this intuition, it
might help to work out the probabilities for an example with two events, p(a) = .99
and p(b) = .01:

p   (a) =

p   (b) =

.99.75

.99.75 + .01.75 = .97
.99.75 + .01.75 = .03

.01.75

(6.32)

given the set of positive and negative training instances, and an initial set of
embeddings, the goal of the learning algorithm is to adjust those embeddings such
that we

    maximize the similarity of the target word, context word pairs (t,c) drawn

from the positive examples

122 chapter 6

    vector semantics
    minimize the similarity of the (t,c) pairs drawn from the negative examples.
we can express this formally over the whole training set as:

l(   ) = (cid:88)(t,c)   +

logp(+|t,c) + (cid:88)(t,c)      

logp(   |t,c)

(6.33)

or, focusing in on one word/context pair (t,c) with its k noise words n1...nk, the

learning objective l is:

logp(   |t,ni)

k(cid:88)i=1
l(   ) = logp(+|t,c) +
k(cid:88)i=1
log   (   ni   t)
k(cid:88)i=1

= log   (c  t) +

1 + e   c  t +

1 + eni  t

= log

log

1

1

(6.34)

that is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the k negative sampled non-
neighbor words.

we can then use stochastic id119 to train to this objective, iteratively
modifying the parameters (the embeddings for each target word t and each context
word or noise word c in the vocabulary) to maximize the objective.

note that the skip-gram model thus actually learns two separate embeddings
for each word w: the target embedding t and the context embedding c. these
embeddings are stored in two matrices, the target matrix t and the context matrix
c. so each row i of the target matrix t is the 1   d vector embedding ti for word
i in the vocabulary v , and each column i of the context matrix c is a d    1 vector
embedding ci for word i in v . fig. 6.13 shows an intuition of the learning task for
the embeddings encoded in these two matrices.

target
embedding
context
embedding

figure 6.13 the skip-gram model tries to shift embeddings so the target embedding (here
for apricot) are closer to (have a higher dot product with) context embeddings for nearby
words (here jam) and further from (have a lower dot product with) context embeddings for
words that don   t occur nearby (here aardvark).

just as in id28, then, the learning algorithm starts with randomly
initialized w and c matrices, and then walks through the training corpus using gra-
dient descent to move w and c so as to maximize the objective in eq. 6.34. thus
the matrices w and c function as the parameters    that id28 is tuning.

1.k.n.v1.2      .j         v1...dwc1. ..          dincreasesimilarity( apricot , jam)wj . ckjamapricotaardvarkdecreasesimilarity( apricot , aardvark)wj . cn      apricot jam      neighbor wordrandom noiseword6.9

    visualizing embeddings

123

once the embeddings are learned, we   ll have two embeddings for each word wi:
ti and ci. we can choose to throw away the c matrix and just keep w , in which case
each word i will be represented by the vector ti.

alternatively we can add the two embeddings together, using the summed em-
bedding ti + ci as the new d-dimensional embedding, or we can concatenate them
into an embedding of dimensionality 2d.

as with the simple count-based methods like tf-idf, the context window size l
effects the performance of skip-gram embeddings, and experiments often tune the
parameter l on a dev set. one difference from the count-based methods is that for
skip-grams, the larger the window size the more computation the algorithm requires
for training (more neighboring words must be predicted).

6.9 visualizing embeddings

visualizing embeddings is an important goal in helping understands, apply, and im-
prove these models of word meaning. but how can we visualize a (for example)
100-dimensional vector?

the simplest way to visualize the meaning of a word w embedded in a space
is to list the most similar words to w sorting all words in the vocabulary by their
cosines. for example the 7 closest words to frog using the glove embeddings are:
frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (pennington
et al., 2014)

yet another visualization method is to use a clus-
tering algorithm to show a hierarchical representa-
tion of which words are similar to others in the em-
bedding space. the example on the right uses hi-
erarchical id91 of some embedding vectors for
nouns as a visualization method (rohde et al., 2006).
probably the most common visualization method,
however, is to project the 100 dimensions of a word
down into 2 dimensions. fig. 6.1 showed one such
visualization, using a projection method called t-
sne (van der maaten and hinton, 2008).

6.10 semantic properties of embeddings

vector semantic models have a number of parameters. one parameter that is relevant
to both sparse tf-idf vectors and dense id97 vectors is the size of the context
window used to collect counts. this is generally between 1 and 10 words on each
side of the target word (for a total context of 3-20 words).

the choice depends on on the goals of the representation. shorter context win-
dows tend to lead to representations that are a bit more syntactic, since the infor-
mation is coming from immediately nearby words. when the vectors are computed
from short context windows, the most similar words to a target word w tend to be
semantically similar words with the same parts of speech. when vectors are com-
puted from long context windows, the highest cosine words to a target word w tend
to be words that are topically related but not similar.

rohde,gonnerman,plautmodelingwordmeaningusinglexicalco-occurrenceheadhandfacedogamericacateyeeuropefootchinafrancechicagoarmfingernoselegrussiamouseafricaatlantaearshoulderasiacowbullpuppylionhawaiimontrealtokyotoemoscowtoothnashvillebrazilwristkittenankleturtleoysterfigure8:multidimensionalscalingforthreenounclasses.wristankleshoulderarid113ghandfootheadnosefingertoefaceeareyetoothdogcatpuppykittencowmouseturtleoysterlionbullchicagoatlantamontrealnashvilletokyochinarussiaafricaasiaeuropeamericabrazilmoscowfrancehawaiifigure9:hierarchicalid91forthreenounclassesusingdistancesbasedonvectorcorrelations.20124 chapter 6

    vector semantics
for example levy and goldberg (2014a) showed that using skip-gram with a
window of   2, the most similar words to the word hogwarts (from the harry potter
series) were names of other    ctional schools: sunnydale (from buffy the vampire
slayer) or evernight (from a vampire series). with a window of   5, the most similar
words to hogwarts were other words topically related to the harry potter series:
dumbledore, malfoy, and half-blood.

it   s also often useful to distinguish two kinds of similarity or association between
words (sch  utze and pedersen, 1993). two words have    rst-order co-occurrence
(sometimes called syntagmatic association) if they are typically nearby each other.
thus wrote is a    rst-order associate of book or poem. two words have second-order
co-occurrence (sometimes called paradigmatic association) if they have similar
neighbors. thus wrote is a second-order associate of words like said or remarked.

   rst-order
co-occurrence

second-order
co-occurrence

analogy another semantic property of embeddings is their ability to capture re-
lational meanings. mikolov et al. (2013b) and levy and goldberg (2014b) show
that the offsets between vector embeddings can capture some analogical relations
between words. for example, the result of the expression vector(   king   ) - vec-
tor(   man   ) + vector(   woman   ) is a vector close to vector(   queen   ); the left panel
in fig. 6.14 visualizes this, again projected down into 2 dimensions. similarly, they
found that the expression vector(   paris   ) - vector(   france   ) + vector(   italy   ) results
in a vector that is very close to vector(   rome   ).

(a)

(b)

figure 6.14 relational properties of the vector space, shown by projecting vectors onto two dimensions. (a)
   king    -    man    +    woman    is close to    queen    (b) offsets seem to capture comparative and superlative morphology
(pennington et al., 2014).

embeddings and historical semantics: embeddings can also be a useful tool
for studying how meaning changes over time, by computing multiple embedding
spaces, each from texts written in a particular time period. for example fig. 6.15
shows a visualization of changes in meaning in english words over the last two
centuries, computed by building separate embedding spaces for each decade from
historical corpora like google id165s (lin et al., 2012) and the corpus of histori-
cal american english (davies, 2012).

6.11

    bias and embeddings

125

figure 6.15 a id167 visualization of the semantic change of 3 words in english using
id97 vectors. the modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. earlier points are com-
puted from earlier historical embedding spaces. the visualizations show the changes in the
word gay from meanings related to    cheerful    or    frolicsome    to referring to homosexuality,
the development of the modern    transmission    sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning    full of awe   
to meaning    terrible or appalling    (hamilton et al., 2016b).

6.11 bias and embeddings

in addition to their ability to learn word meaning from text, embeddings, alas, also
reproduce the implicit biases and stereotypes that were latent in the text. recall that
embeddings model analogical relations;    queen    as the closest word to    king    -    man   
+    woman    implies the analogy man:woman::king:queen. but embedding analogies
also exhibit gender stereotypes. for example bolukbasi et al. (2016)    nd that the
closest occupation to    man    -    computer programmer    +    woman    in id97 em-
beddings trained on news text is    homemaker   , and that the embeddings similarly
suggest the analogy    father    is to    doctor    as    mother    is to    nurse   . algorithms that
used embeddings as part of an algorithm to search for potential programmers or
doctors might thus incorrectly downweight documents with women   s names.

embeddings also encode the implicit associations that are a property of human
reasoning. the implicit association test (greenwald et al., 1998) measures peo-
ple   s associations between concepts (like       owers    or    insects   ) and attributes (like
   pleasantness    and    unpleasantness   ) by measuring differences in the latency with
which they label words in the various categories.6 using such methods, people
in the united states have been shown to associate african-american names with
unpleasant words (more than european-american names), male names more with
mathematics and female names with the arts, and old people   s names with unpleas-
ant words (greenwald et al. 1998, nosek et al. 2002a, nosek et al. 2002b). caliskan
et al. (2017) replicated all these    ndings of implicit associations using glove vec-
tors and cosine similarity instead of human latencies. for example afrian american
names like    leroy    and    shaniqua    had a higher glove cosine with unpleasant words
while european american names (   brad   ,    greg   ,    courtney   ) had a higher cosine
with pleasant words. any embedding-aware algorithm that made use of word senti-
ment could thus lead to bias against african americans.

6 roughly speaking, if humans associate       owers    with    pleasantness    and    insects    with    unpleasant-
ness   , when they are instructed to push a red button for       owers    (daisy, iris, lilac) and    pleasant words   
(love, laughter, pleasure) and a green button for    insects    (   ea, spider, mosquito) and    unpleasant words   
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
      owers    and    unpleasant words    and a green button for    insects    and    pleasant words   .

chapter5.dynamicsocialrepresentationsofwordmeaning79figure5.1:two-dimensionalvisualizationofsemanticchangeinenglishusingsgnsvectors(seesection5.8forthevisualizationalgorithm).a,thewordgayshiftedfrommeaning   cheerful   or   frolicsome   toreferringtohomosexuality.a,intheearly20thcenturybroadcastreferredto   castingoutseeds   ;withtheriseoftelevisionandradioitsmeaningshiftedto   transmittingsignals   .c,awfulunderwentaprocessofpejoration,asitshiftedfrommeaning   fullofawe   tomeaning   terribleorappalling   [212].thatadverbials(e.g.,actually)haveageneraltendencytoundergosubjecti   cationwheretheyshiftfromobjectivestatementsabouttheworld(e.g.,   sorry,thecarisactuallybroken   )tosubjectivestatements(e.g.,   ican   tbelieveheactuallydidthat   ,indicatingsurprise/disbelief).5.2.2computationallinguisticstudiestherearealsoanumberofrecentworksanalyzingsemanticchangeusingcomputationalmethods.[200]uselatentsemanticanalysistoanalyzehowwordmeaningsbroadenandnarrowovertime.[113]userawco-occurrencevectorstoperformanumberofhistoricalcase-studiesonsemanticchange,and[252]performasimilarsetoid122all-scalecase-studiesusingtemporaltopicmodels.[87]constructpoint-wisemutualinformation-basedembeddingsandfoundthatsemanticchangesuncoveredbytheirmethodhadreasonableagreementwithhumanjudgments.[129]and[119]use   neural   word-embeddingmethodstodetectlinguisticchangepoints.finally,[257]analyzehistoricalco-occurrencestotestwhethersynonymstendtochangeinsimilarways.126 chapter 6

    vector semantics
recent research focuses on ways to try to remove the kinds of biases, for example
by developing a transformation of the embedding space that removes gender stereo-
types but preserves de   nitional gender (bolukbasi et al. 2016, zhao et al. 2017).

historical embeddings are also being used to measure biases in the past. garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women   s names versus
men   s to occupation words like    librarian    or    carpenter   ) across the 20th century.
they found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupation. historical embeddings also replicated
old surveys of ethnic stereotypes; the tendency of experimental participants in 1933
to associate adjectives like    industrious    or    superstitious    with, e.g., chinese eth-
nicity, correlates with the cosine between chinese last names and those adjectives
using embeddings trained on 1930s text. they also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(   smart   ,    wise   ,    thoughtful   ,    resourceful   ) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960.

we will return in later chapters to this question about the role of bias in natural

language processing and machine learning in general.

6.12 evaluating vector models

the most important evaluation metric for vector models is extrinsic evaluation on
tasks; adding them as features into any nlp task and seeing whether this improves
performance over some other model.

nonetheless it is useful to have intrinsic evaluations. the most common metric
is to test their performance on similarity, computing the correlation between an
algorithm   s word similarity scores and word similarity ratings assigned by humans.
wordsim-353 (finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77.
siid113x-999 (hill et al., 2015) is a more dif   cult dataset that quanti   es similarity
(cup, mug) rather than relatedness (cup, coffee), and including both concrete and
abstract adjective, noun and verb pairs. the toefl dataset is a set of 80 questions,
each consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: levied is closest in meaning to:
imposed, believed, requested, correlated (landauer and dumais, 1997). all of these
datasets present words without context.

slightly more realistic are intrinsic similarity tasks that include context. the
stanford contextual word similarity (scws) dataset (huang et al., 2012) offers a
richer evaluation scenario, giving human judgments on 2,003 pairs of words in their
sentential context, including nouns, verbs, and adjectives. this dataset enables the
evaluation of word similarity algorithms that can make use of context words. the
semantic textual similarity task (agirre et al. 2012, agirre et al. 2015) evaluates the
performance of sentence-level similarity algorithms, consisting of a set of pairs of
sentences, each pair with human-labeled similarity scores.

another task used for evaluate is an analogy task, where the system has to solve
problems of the form a is to b as c is to d, given a, b, and c and having to    nd d.
, the system must    ll in the word
thus given athens is to greece as oslo is to
norway. or more syntactically-oriented examples: given mouse, mice, and dollar

the system must return dollars. large sets of such tuples have been created (mikolov
et al. 2013, mikolov et al. 2013b).

6.13

    summary

127

6.13 summary

space, also called an embedding.

    in vector semantics, a word is modeled as a vector   a point in high-dimensional
    vector semantic models fall into two classes: sparse and dense. in sparse
models like tf-idf each dimension corresponds to a word in the vocabulary v ;
    cell in sparse models are functions of co-occurrence counts. the term-
document matrix has rows for each word (term) in the vocabulary and a
column for each document.

and a column for each context term in the vocabulary.

    the word-context matrix has a row for each (target) word in the vocabulary
    a common sparse weighting is tf-idf, which weights each cell by its term
    word and document similarity is computed by computing the dot product
between vectors. the cosine of two vectors   a normalized dot product   is
the most popular such metric.

frequency and inverse document frequency.

to interpret.

scheme to tf-idf.

    ppmi (pointwise positive mutual information) is an alternative weighting
    dense vector models have dimensionality 50-300 and the dimensions are harder
    the id97 family of models, including skip-gram and cbow, is a pop-
    skip-gram trains a id28 classi   er to compute the id203 that
two words are    likely to occur nearby in text   . this id203 is computed
from the dot product between the embeddings for the two words,

ular ef   cient way to compute dense embeddings.

    skip-gram uses stochastic id119 to train the classi   er, by learning
embeddings that have a high dot-product with embeddings of words that occur
nearby and a low dot-product with noise words.

    other important embedding algorithms include glove, a method based on ra-
tios of word co-occurrence probabilities, and fasttext, an open-source library
for computing id27s by summing embeddings of the bag of char-
acter id165s that make up a word.

bibliographical and historical notes

the idea of vector semantics arose out of research in the 1950s in three distinct
   elds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.

the idea that meaning was related to distribution of words in context was widespread

in linguistic theory of the 1950s, among distributionalists like zellig harris, martin
joos, and j. r. firth, and semioticians like thomas sebeok. as joos (1950) put it,

128 chapter 6

    vector semantics

the linguist   s    meaning    of a morpheme. . . is by de   nition the set of conditional
probabilities of its occurrence in context with all other morphemes.

mechanical
indexing

semantic
feature

the idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like charles e. osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad, or hard/soft. osgood et al. (1957) proposed that
the meaning of a word in general could be modeled as a point in a multidimensional
euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.

a    nal intellectual source in the 1950s and early 1960s was the    eld then called
mechanical indexing, now known as information retrieval. in what became known
as the vector space model for information retrieval (salton 1971,sparck jones 1986),
researchers demonstrated new ways to de   ne the meaning of words in terms of vec-
tors (switzer, 1965), and re   ned methods for word similarity based on measures
of statistical association between words like mutual information (giuliano, 1965)
and idf (sparck jones, 1972), and showed that the meaning of documents could be
represented in the same vector spaces used for words.

more distantly related is the idea of de   ning words by a vector of discrete fea-
tures, which has a venerable history in our    eld, with roots at least as far back as
descartes and leibniz (wierzbicka 1992, wierzbicka 1996). by the middle of the
20th century, beginning with the work of hjelmslev (hjelmslev, 1969) and    eshed
out in early models of generative grammar (katz and fodor, 1963), the idea arose of
representing meaning with semantic features, symbols that represent some sort of
primitive meaning. for example words like hen, rooster, or chick, have something
in common (they all describe chickens) and something different (their age and sex),
representable as:

+chicken, -adult

hen
+female, +chicken, +adult
rooster -female, +chicken, +adult
chick
the dimensions used by vector models of meaning to de   ne words, however, are
only abstractly related to this idea of a small    xed number of hand-built dimensions.
nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speci   c compositional aspect of meaning like
these early semantic features.

the    rst use of dense vectors to model word meaning was the latent seman-
tic indexing (lsi) model (deerwester et al., 1988) recast as lsa (latent semantic
analysis) (deerwester et al., 1990). in lsa svd is applied to a term-document ma-
trix (each cell weighted by log frequency and normalized by id178), and then using
the    rst 300 dimensions as the embedding. lsa was then quickly widely applied:
as a cognitive model landauer and dumais (1997), and tasks like spell checking
(jones and martin, 1997), id38 (bellegarda 1997, coccaro and ju-
rafsky 1998, bellegarda 2000) morphology induction (schone and jurafsky 2000,
schone and jurafsky 2001), and essay grading (rehder et al., 1998). related mod-
els were simultaneously developed and applied to id51 by
sch  utze (1992b). lsa also led to the earliest use of embeddings to represent words
in a probabilistic classi   er, in the id28 document router of sch  utze
et al. (1995). the idea of svd on the term-term matrix (rather than the term-
document matrix) as a model of meaning for nlp was proposed soon after lsa
by sch  utze (1992b). sch  utze applied the low-rank (97-dimensional) embeddings
produced by svd to the task of id51, analyzed the result-

bibliographical and historical notes

129

ing semantic space, and also suggested possible techniques like dropping high-order
dimensions. see sch  utze (1997a).

a number of alternative matrix models followed on from the early svd work,
including probabilistic id45 (plsi) (hofmann, 1999) latent
dirichlet allocation (lda) (blei et al., 2003). nonnegative id105
(nmf) (lee and seung, 1999).

by the next decade, bengio et al. (2003) and bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. collobert and weston (2007), collobert and weston (2008), and
collobert et al. (2011) then demonstrated that embeddings could play a role for rep-
resenting word meanings for a number of nlp tasks. turian et al. (2010) compared
the value of different kinds of embeddings for different nlp tasks. mikolov et al.
(2011) showed that recurrent neural nets could be used as language models. the
idea of simplifying the hidden layer of these neural net language models to create
the skip-gram and cbow algorithms was proposed by mikolov et al. (2013). the
negative sampling training algorithm was proposed in mikolov et al. (2013a).

studies of embeddings include results showing an elegant mathematical relation-
ship between sparse and dense embeddings (levy and goldberg, 2014c), as well
as numerous surveys of embeddings and their parameterizations. (bullinaria and
levy 2007, bullinaria and levy 2012, lapesa and evert 2014, kiela and clark 2014,
levy et al. 2015).

there are many other embedding algorithms, using methods like non-negative
id105 (fyshe et al., 2015), or by converting sparse ppmi embeddings
to dense vectors by using svd (levy and goldberg, 2014c). the most widely-
used embedding model besides id97 is glove (pennington et al., 2014). the
name stands for global vectors, because the model is based on capturing global
corpus statistics. glove is based on ratios of probabilities from the word-word co-
occurrence matrix, combining the intuitions of count-based models like ppmi while
also capturing the linear structures used by methods like id97.

an extension of id97, fasttext (bojanowski et al., 2017), deals with un-
known words and sparsity in languages with rich morphology, by using subword
models. each word in fasttext is represented as itself plus a bag of constituent n-
grams, with special boundary symbols < and > added to each word. for example,
with n = 3 the word where would be represented by the character id165s:

fasttext

<wh, whe, her, ere, re>

plus the sequence

<where>

then a skipgram embedding is learned for each constituent id165, and the word
where is represented by the sum of all of the embeddings of its constituent id165s.
a fasttext open-source library, including pretrained embeddings for 157 languages,
is available at https://fasttext.cc.

see manning et al. (2008) for a deeper understanding of the role of vectors in in-
formation retrieval, including how to compare queries with documents, more details
on tf-idf, and issues of scaling to very large datasets.

cruse (2004) is a useful introductory linguistic text on lexical semantics.

130 chapter 6

    vector semantics

exercises

chapter

7 neural networks and neural

language models

   [m]achines of this character can behave in a very complicated manner when
the number of units is large.   

alan turing (1948)    intelligent machines   , page 6

deep learning
deep

neural networks are an essential computational tool for language processing, and
a very old one. they are called neural because their origins lie in the mcculloch-
pitts neuron (mcculloch and pitts, 1943), a simpli   ed model of the human neuron
as a kind of computing element that could be described in terms of propositional
logic. but the modern use in language processing no longer draws on these early
biological inspirations.

instead, a modern neural network is a network of small computing units, each
of which takes a vector of input values and produces a single output value. in this
chapter we introduce the neural net applied to classi   cation. the architecture we
introduce is called a feed-forward network because the computation proceeds iter-
atively from one layer of units to the next. the use of modern neural nets is often
called deep learning, because modern networks are often deep (have many layers).
neural networks share much of the same mathematics as id28. but
neural networks are a more powerful classi   er than id28, and indeed a
minimal neural network (technically one with a single    hidden layer   ) can be shown
to learn any function.

neural net classi   ers are different from id28 in another way. with
id28, we applied the regression classi   er to many different tasks by
developing many rich kinds of feature templates based on domain knowledge. when
working with neural networks, it is more common to avoid the use of rich hand-
derived features, instead building neural networks that take raw words as inputs
and learn to induce features as part of the process of learning to classify. we saw
examples of this kind of representation learning for embeddings in chapter 6. nets
that are very deep are particularly good at representation learning for that reason
deep neural nets are the right tool for large scale problems that offer suf   cient data
to learn features automatically.

in this chapter we   ll see feedforward networks as classi   ers, and apply them to
the simple task of id38: assigning probabilities to word sequences and
predicting upcoming words. in later chapters we   ll introduce many other aspects of
neural models, such as the recurrent neural network and the encoder-decoder
model.

132 chapter 7

    neural networks and neural language models

7.1 units

the building block of a neural network is a single computational unit. a unit takes
a set of real valued numbers as input, performs some computation on them, and
produces an output.

at its heart, a neural unit is taking a weighted sum of its inputs, with one addi-
tional term in the sum called a bias term. thus given a set of inputs x1...xn, a unit
has a set of corresponding weights w1...wn and a bias b, so the weighted sum z can
be represented as:

z = b +(cid:88)i

wixi

(7.1)

often it   s more convenient to express this weighted sum using vector notation;
recall from id202 that a vector is, at heart, just a list or array of numbers.
thus we   ll talk about z in terms of a weight vector w, a scalar bias b, and an input
vector x, and we   ll replace the sum with the convenient dot product:

z = w   x + b

(7.2)

as de   ned in eq. 7.2, z is just a real valued number.
finally, instead of using z, a linear function of x, as the output, neural units
apply a non-linear function f to z. we will refer to the output of this function as
the activation value for the unit, a. since we are just modeling a single unit, the
activation for the node is in fact the    nal output of the network, which we   ll generally
call y. so the value y is de   ned as:

bias term

vector

activation

y = a = f (z)

(7.3)

we   ll discuss three popular non-linear functions f () below (the sigmoid, the
tanh, and the recti   ed linear relu) but it   s pedagogically convenient to start with
the sigmoid function since we saw it in chapter 5:

sigmoid

y =    (z) =

1

1 + e   z

(7.4)

(7.5)

the sigmoid (shown in fig. 7.1) has a number of advantages; it maps the output
into the range [0,1], which is useful in squashing outliers toward 0 or 1. and it   s
differentiable, which as we saw in section 5.8 will be handy for learning.

substituting the sigmoid equation into eq. 7.2 gives us the    nal value for the

output of a neural unit:

y =    (w   x + b) =

1

1 + exp(   (w   x + b))

fig. 7.2 shows a    nal schematic of a basic neural unit. in this example the unit
takes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each
value by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then
passes the resulting sum through a sigmoid function to result in a number between 0
and 1.

let   s walk through an example just to get an intuition. let   s suppose we have a

unit with the following weight vector and bias:

7.1

    units

133

figure 7.1 the sigmoid function takes a real value and maps it to the range [0,1]. because
it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squash outlier
values toward 0 or 1.

figure 7.2 a neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a
weight for an input clamped at +1) and producing an output y. we include some convenient
intermediate variables: the output of the summation, z, and the output of the sigmoid, a. in
this case the output of the unit y is the same as a, but in deeper networks we   ll reserve y to
mean the    nal output of the entire network, leaving a as the activation of an individual node.

w = [0.2,0.3,0.9]
b = 0.5

what would this unit do with the following input vector:

x = [0.5,0.6,0.1]

the resulting output y would be:

1

1

1 + e   (w  x+b)

y =    (w   x + b) =
in practice, the sigmoid is not commonly used as an activation function. a
function that is very similar but almost always better is the tanh function shown
in fig. 7.3a; tanh is a variant of the sigmoid that ranges from -1 to +1:

1 + e   (.5   .2+.6   .3+.1   .9+.5)

= e   0.87 = .70

=

tanh

y =

ez     e   z
ez + e   z

(7.6)

the simplest activation function, and perhaps the most commonly used, is the
recti   ed linear unit, also called the relu, shown in fig. 7.3b. it   s just the same as x

relu

x1x2x3yw1w2w3   b  +1za134 chapter 7

    neural networks and neural language models

when x is positive, and 0 otherwise:

y = max(x,0)

(7.7)

(a)

(b)

figure 7.3 the tanh and relu id180.

saturated

these id180 have different properties that make them useful for
different language applications or network architectures. for example the recti   er
function has nice properties that result from it being very close to linear. in the sig-
moid or tanh functions, very high values of z result in values of y that are saturated,
i.e., extremely close to 1, which causes problems for learning. recti   ers don   t have
this problem, since the output of values close to 1 also approaches 1 in a nice gentle
linear way. by contrast, the tanh function has the nice properties of being smoothly
differentiable and mapping outlier values toward the mean.

7.2 the xor problem

early in the history of neural networks it was realized that the power of neural net-
works, as with the real neurons that inspired them, comes from combining these
units into larger networks.

one of the most clever demonstrations of the need for multi-layer networks was
the proof by minsky and papert (1969) that a single neural unit cannot compute
some very simple functions of its input. consider the very simple task of computing
simple logical functions of two inputs, like and, or, and xor. as a reminder,
here are the truth tables for those functions:

and

x1 x2 y
0
0
0
0
0
1
1
1

0
1
0
1

or

x1 x2 y
0
0
1
0
1
1
1
1

0
1
0
1

xor

x1 x2 y
0
0
1
0
1
1
1
0

0
1
0
1

id88

this example was    rst shown for the id88, which is a very simple neural
unit that has a binary output and no non-linear activation function. the output y of

    the xor problem 135
a id88 is 0 or 1, and just computed as follows (using the same weight w, input
x, and bias b as in eq. 7.2):

7.2

y =(cid:26) 0,

1,

if w   x + b     0
if w   x + b > 0

(7.8)

it   s very easy to build a id88 that can compute the logical and and or

functions of its binary inputs; fig. 7.4 shows the necessary weights.

decision
boundary

linearly
separable

(a)

(b)

figure 7.4 the weights w and bias b for id88s for computing logical functions. the
inputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied
with the bias weight b. (a) logical and, showing weights w1 = 1 and w2 = 1 and bias weight
b =    1. (b) logical or, showing weights w1 = 1 and w2 = 1 and bias weight b = 0. these
weights/biases are just one from an in   nite number of possible sets of weights and biases that
would implement the functions.

it turns out, however, that it   s not possible to build a id88 to compute

logical xor! (it   s worth spending a moment to give it a try!)

the intuition behind this important result relies on understanding that a percep-
tron is a linear classi   er. for a two-dimensional input x0 and x1, the perception
equation, w1x1 + w2x2 + b = 0 is the equation of a line (we can see this by putting
it in the standard linear format: x2 =    (w1/w2)x1     b.) this line acts as a decision
boundary in two-dimensional space in which the output 0 is assigned to all inputs
lying on one side of the line, and the output 1 to all input points lying on the other
side of the line. if we had more than 2 inputs, the decision boundary becomes a
hyperplane instead of a line, but the idea is the same, separating the space into two
categories.

fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn
by one possible set of parameters for an and and an or classi   er. notice that there
is simply no way to draw a line that separates the positive cases of xor (01 and 10)
from the negative cases (00 and 11). we say that xor is not a linearly separable
function. of course we could draw a boundary with a curve, or some other function,
but not a single line.

7.2.1 the solution: neural networks
while the xor function cannot be calculated by a single id88, it can be cal-
culated by a layered network of units. let   s see an example of how to do this from
goodfellow et al. (2016) that computes xor using two layers of relu-based units.
fig. 7.6 shows a    gure with the input being processed by two layers of neural units.
the middle layer (called h) has two units, and the output layer (called y) has one
unit. a set of weights and biases are shown for each relu that correctly computes
the xor function

let   s walk through what happens with the input x = [0 0]. if we multiply each
input value by the appropriate weight, sum, and then add the bias b, we get the vector

x1x2+1-111x1x2+1011136 chapter 7

    neural networks and neural language models

figure 7.5 the functions and, or, and xor, represented with input x0 on the x-axis and input x1 on the
y axis, filled circles represent id88 outputs of 1, and white circles id88 outputs of 0. there is no
way to draw a line that correctly separates the two categories for xor. figure styled after russell and norvig
(2002).

figure 7.6 xor solution after goodfellow et al. (2016). there are three relu units, in
two layers; we   ve called them h1, h2 (h for    hidden layer   ) and y1. as before, the numbers
on the arrows represent the weights w for each unit, and we represent the bias b as a weight
on a unit clamped to +1, with the bias weights/units in gray.

[0 -1], and we then we apply the recti   ed linear transformation to give the output
of the h layer as [0 0]. now we once again multiply by the weights, sum, and add
the bias (0 in this case) resulting in the value 0. the reader should work through the
computation of the remaining 3 possible input pairs to see that the resulting y values
correctly are 1 for the inputs [0 1] and [1 0] and 0 for [0 0] and [1 1].

it   s also instructive to look at the intermediate results, the outputs of the two
hidden nodes h0 and h1. we showed in the previous paragraph that the h vector for
the inputs x = [0 0] was [0 0]. fig. 7.7b shows the values of the h layer for all 4
inputs. notice that hidden representations of the two input points x = [0 1] and x
= [1 0] (the two cases with xor output = 1) are merged to the single point h = [1
0]. the merger makes it easy to linearly separate the positive and negative cases
of xor. in other words, we can view the hidden layer of the network is forming a
representation for the input.

in this example we just stipulated the weights in fig. 7.6. but for real exam-
ples the weights for neural networks are learned automatically using the error back-
propagation algorithm to be introduced in section 7.4. that means the hidden layers
will learn to form useful representations. this intuition, that neural networks can au-
tomatically learn useful representations of the input, is one of their key advantages,

0011x1x20011x1x20011x1x2a)  x1 and x2b)  x1 or x2c)  x1 xor x2?x1x2h1h2y1+11-1111-201+107.3

    feed-forward neural networks

137

figure 7.7 the hidden layer forming a new representation of the input. here is the rep-
resentation of the hidden layer, h, compared to the original input representation x. notice
that the input point [0 1] has been collapsed with the input point [1 0], making it possible to
linearly separate the positive and negative cases of xor. after goodfellow et al. (2016).

and one that we will return to again and again in later chapters.

note that the solution to the xor problem requires a network of units with non-
linear id180. a network made up of simple linear (id88) units
cannot solve the xor problem. this is because a network formed by many layers
of purely linear units can always be reduced (shown to be computationally identical
to) a single layer of linear units with appropriate weights, and we   ve already shown
(visually, in fig. 7.5) that a single unit cannot solve the xor problem.

7.3 feed-forward neural networks

feed-forward
network

multi-layer
id88s
mlp

hidden layer

fully-connected

let   s now walk through a slightly more formal presentation of the simplest kind of
neural network, the feed-forward network. a feed-forward network is a multilayer
network in which the units are connected with no cycles; the outputs from units in
each layer are passed to units in the next higher layer, and no outputs are passed
back to lower layers. (in chapter 9 we   ll introduce networks with cycles, called
recurrent neural networks.)

for historical reasons multilayer networks, especially feedforward networks, are
sometimes called multi-layer id88s (or mlps); this is a technical misnomer,
since the units in modern multilayer networks aren   t id88s (id88s are
purely linear, but modern networks are made up of units with non-linearities like
sigmoids), but at some point the name stuck.

simple feed-forward networks have three kinds of nodes: input units, hidden

units, and output units. fig. 7.8 shows a picture.

the input units are simply scalar values just as we saw in fig. 7.2.
the core of the neural network is the hidden layer formed of hidden units,
each of which is a neural unit as described in section 7.1, taking a weighted sum of
its inputs and then applying a non-linearity. in the standard architecture, each layer
is fully-connected, meaning that each unit in each layer takes as input the outputs
from all the units in the previous layer, and there is a link between every pair of units
from two adjacent layers. thus each hidden unit sums over all the input units.

0011x0x1a) the original x space0011h0h12b) the new h space138 chapter 7

    neural networks and neural language models

figure 7.8 a simple 2-layer feed-forward network, with one hidden layer, one output layer,
and one input layer (the input layer is usually not counted when enumerating layers).

recall that a single hidden unit has parameters w (the weight vector) and b (the
bias scalar). we represent the parameters for the entire hidden layer by combining
the weight vector wi and bias bi for each unit i into a single weight matrix w and
a single bias vector b for the whole layer (see fig. 7.8). each element wi j of the
weight matrix w represents the weight of the connection from the ith input unit xi to
the the jth hidden unit h j.

the advantage of using a single matrix w for the weights of the entire layer is
that now that hidden layer computation for a feedforward network can be done very
ef   ciently with simple matrix operations. in fact, the computation only has three
steps: multiplying the weight matrix by the input vector x, adding the bias vector b,
and applying the activation function g (such as the sigmoid, tanh, or relu activation
function de   ned above).

the output of the hidden layer, the vector h, is thus the following, using the

sigmoid function   :

h =    (w x + b)

(7.9)

notice that we   re applying the    function here to a vector, while in eq. 7.4 it was
applied to a scalar. we   re thus allowing    (  ), and indeed any activation function
g(  ), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].
let   s introduce some constants to represent the dimensionalities of these vectors
and matrices. we   ll refer to the input layer as layer 0 of the network, and use have
n0 represent the number of inputs, so x is a vector of real numbers of dimension
n0, or more formally x     rn0. let   s call the hidden layer layer 1 and the output
layer layer 2. the hidden layer has dimensionality n1, so h     rn1 and also b     rn1
(since each hidden unit can take a different bias value). and the weight matrix w
has dimensionality w     rn1  n0.
compute the value of each hi j as(cid:80)nx

as we saw in section 7.2, the resulting value h (for hidden but also for hypoth-
esis) forms a representation of the input. the role of the output layer is to take
this new representation h and compute a    nal output. this output could be a real-
valued number, but in many cases the goal of the network is to make some sort of
classi   cation decision, and so we will focus on the case of classi   cation.

take a moment to convince yourself that the id127 in eq. 7.9 will

i=1 wi jxi + b j.

if we are doing a binary task like sentiment classi   cation, we might have a single

x1x2h1h2y1xn0   h3hn1   +1b   uwy2yn27.3

    feed-forward neural networks

139

output node, and its value y is the id203 of positive versus negative sentiment.
if we are doing multinomial classi   cation, such as assigning a part-of-speech tag, we
might have one output node for each potential part-of-speech, whose output value
is the id203 of that part-of-speech, and the values of all the output nodes must
sum to one. the output layer thus gives a id203 distribution across the output
nodes.

let   s see how this happens. like the hidden layer, the output layer has a weight
matrix (let   s call it u), but output layers may not t have a bias vector b, so we   ll sim-
plify by eliminating the bias vector in this example. the weight matrix is multiplied
by its input vector (h) to produce the intermediate output z.

z = uh

there are n2 output nodes, so z     rn2, weight matrix u has dimensionality u    
rn2  n1, and element ui j is the weight from unit j in the hidden layer to unit i in the
output layer.

however, z can   t be the output of the classi   er, since it   s a vector of real-valued
numbers, while what we need for classi   cation is a vector of probabilities. there is
a convenient function for normalizing a vector of real values, by which we mean
converting it to a vector that encodes a id203 distribution (all the numbers lie
between 0 and 1 and sum to 1): the softmax function that we saw on page 96 of
chapter 5. for a vector z of dimensionality d, the softmax is de   ned as:

normalizing

softmax

ezi
j=1 ez j

(cid:80)d

softmax(zi) =

1     i     d

(7.10)

thus for example given a vector z=[0.6 1.1 -1.5 1.2 3.2 -1.1], softmax(z) is [ 0.055
0.090 0.0067 0.10 0.74 0.010].

you may recall that softmax was exactly what is used to create a id203
distribution from a vector of real-valued numbers (computed from summing weights
times features) in id28 in chapter 5.

that means we can think of a neural network classi   er with one hidden layer
as building a vector h which is a hidden layer representation of the input, and then
running standard id28 on the features that the network develops in h.
by contrast, in chapter 5 the features were mainly designed by hand via feature
templates. so a neural network is like id28, but (a) with many layers,
since a deep neural network is like layer after layer of id28 classi   ers,
and (b) rather than forming the features by feature templates, the prior layers of the
network induce the feature representations themselves.

here are the    nal equations for a feed-forward network with a single hidden
layer, which takes an input vector x, outputs a id203 distribution y, and is pa-
rameterized by weight matrices w and u and a bias vector b:

h =    (w x + b)
z = uh
y = softmax(z)

(7.11)

we   ll call this network a 2-layer network (we traditionally don   t count the input
layer when numbering layers, but do count the output layer). so by this terminology
id28 is a 1-layer network.

let   s now set up some notation to make it easier to talk about deeper networks
of depth more than 2. we   ll use superscripts in square brackets to mean layer num-
bers, starting at 0 for the input layer. so w [1] will mean the weight matrix for the

140 chapter 7

    neural networks and neural language models

(   rst) hidden layer, and b[1] will mean the bias vector for the (   rst) hidden layer. n j
will mean the number of units at layer j. we   ll use g(  ) to stand for the activation
function, which will tend to be relu or tanh for intermediate layers and softmax
for output layers. we   ll use a[i] to mean the output from layer i, and z[i] to mean the
combination of weights and biases w [i]a[i   1] +b[i]. the 0th layer is for inputs, so the
inputs x we   ll refer to more generally as a[0].

thus we   ll represent a 3-layer net as follows:

z[1] = w [1]a[0] + b[1]
a[1] = g[1](z[1])
z[2] = w [2]a[1] + b[2]
a[2] = g[2](z[2])

  y = a[2]

(7.12)

note that with this notation, the equations for the computation done at each layer are
the same. the algorithm for computing the forward step in an n-layer feed-forward
network, given the input vector a[0] is thus simply:

for i in 1..n

z[i] = w [i] a[i   1] + b[i]
a[i] = g[i](z[i])

  y = a[n]

the

id180 g(  ) are generally different at the    nal layer. thus g[2] might
be softmax for multinomial classi   cation or sigmoid for binary classi   cation, while
relu or tanh might be the activation function g() at the internal layers.

7.4 training neural nets

a feedforward neural net is an instance of supervised machine learning in which we
know the correct output y for each observation x. what the system produces, via
eq. 7.12, is   y, the system   s estimate of the true y. the goal of the training procedure
is to learn parameters w [i] and b[i] for each layer i that make   y for each training
observation as close as possible to the true y .

in general, we do all this by drawing on the methods we introduced in chapter 5
for id28, so the reader should be comfortable with that chapter before
proceeding.

first, we   ll need a id168 that models the distance between the system
output and the gold output, and it   s common to use the loss used for logistic regres-
sion, the cross-id178 loss.

second, to    nd the parameters that minimize this id168, we   ll use the
id119 optimization algorithm introduced in chapter 5. there are some
differences

third, id119 requires knowing the gradient of the id168, the
vector that contains the partial derivative of the id168 with respect to each of
the parameters. here is one part where learning for neural networks is more complex
than for logistic id28. in id28, for each observation we
could directly compute the derivative of the id168 with respect to an individ-
ual w or b. but for neural networks, with millions of parameters in many layers, it   s

7.4

    training neural nets

141

much harder to see how to compute the partial derivative of some weight in layer 1
when the loss is attached to some much later layer. how do we partial out the loss
over all those intermediate layers?

the answer is the algorithm called error back-propagation or reverse differ-

entiation.

cross id178
loss

7.4.1 id168
the cross id178 loss, that is used in neural networks is the same one we saw for
id28.

in fact, if the neural network is being used as a binary classi   er, with the sig-
moid at the    nal layer, the id168 is exactly the same as we saw with logistic
regression in eq. 5.10:

lce (   y,y) =    log p(y|x) =     [ylog   y + (1    y)log(1      y)]

(7.13)

what about if the neural network is being used as a multinomial classi   er? let
y be a vector over the c classes representing the true output id203 distribution.
the cross id178 loss here is

lce (   y,y) =    

yi log   yi

c(cid:88)i=1

(7.14)

we can simplify this equation further. assume this is a hard classi   cation task,
meaning that only one class is the correct one, and that there is one output unit in y
for each class. if the true class is i, then y is a vector where yi = 1 and y j = 0     j (cid:54)= i.
a vector like this, with one value=1 and the rest 0, is called a one-hot vector. now
let   y be the vector output from the network. the sum in eq. 7.14 will be 0 except
for the true class. hence the cross-id178 loss is simply the log id203 of the
correct class, and we therefore also call this the negative log likelihood loss:

plugging in the softmax formula from eq. 7.10, and with k the number of classes:

lce (   y,y) =    log   yi

(7.15)

negative log
likelihood loss

lce (   y,y) =    log

ezi
(cid:80)k
j   1 ez j

(7.16)

7.4.2 computing the gradient
how do we compute the gradient of this id168? computing the gradient
requires the partial derivative of the id168 with respect to each parameter.
for a network with one weight layer and sigmoid output (which is what logistic
regression is), we could simply use the derivative of the loss that we used for logistic
regression in: eq. 7.17 (and derived in section 5.8):

    lce (w,b)

    w j

= (   y    y) x j
= (   (w   x + b)    y) x j

(7.17)

142 chapter 7

    neural networks and neural language models

or for a network with one hidden layer and softmax output, we could use the deriva-
tive of the softmax loss from eq. 5.36:

    lce
    wk

= (1{y = k}    p(y = k|x))xk
= (cid:32)1{y = k}   
ewk  x+bk

j=1 ew j  x+b j(cid:33)xk
(cid:80)k

(7.18)

but these derivatives only give correct updates for one weight layer: the last one!
for deep networks, computing the gradients for each weight is much more complex,
since we are computing the derivative with respect to weight parameters that appear
all the way back in the very early layers of the network, even though the loss is
computed only at the very end of the network.

the solution to computing this gradient is an algorithm called error backprop-
agation or backprop (rumelhart et al., 1986). while backprop was invented spe-
cially for neural networks, it turns out to be the same as a more general procedure
called backward differentiation, which depends on the notion of computation
graphs. let   s see how that works in the next subsection.

7.4.3 computation graphs
a computation graph is a representation of the process of computing a mathematical
expression, in which the computation is broken down into separate operations, each
of which is modeled as a node in a graph.

consider computing the function l(a,b,c) = c(a + 2b). if we make each of the
component addition and multiplication operations explicit, and add names (d and e)
for the intermediate outputs, the resulting series of computations is:

d = 2    b
e = a + d
l = c    e

we can now represent this as a graph, with nodes for each operation, and di-
rected edges showing the outputs from each operation as the inputs to the next, as
in fig. 7.9. the simplest use of computation graphs is to compute the value of the
function with some given inputs. in the    gure, we   ve assumed the inputs a = 3,
b = 1, c =    1, and we   ve shown the result of the forward pass to compute the re-
sult l(3,1,   1) = 10. in the forward pass of a computation graph, we apply each
operation left to right, passing the outputs of each computation as the input to the
next node.

7.4.4 backward differentiation on computation graphs
the importance of the computation graph comes from the backward pass, which
is used to compute the derivatives that we   ll need for the weight update. in this
example our goal is to compute the derivative of the output function l with respect
to each of the input variables, i.e.,     l
    a , tells us how
much a small change in a affects l.

    c . the derivative     l

    b , and     l

    a ,     l

backwards differentiation makes use of the chain rule in calculus. suppose we
are computing the derivative of a composite function f (x) = u(v(x)). the derivative

error back-
propagation

chain rule

7.4

    training neural nets

143

figure 7.9 computation graph for the function l(a,b,c) = c(a + 2b), with values for input
nodes a = 3, b = 1, c =    1, showing the forward pass computation of l.
of f (x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with
respect to x:

d f
dx

=

du
dv   

dv
dx

(7.19)

the chain rule extends to more than two functions. if computing the derivative of a
composite function f (x) = u(v(w(x))), the derivative of f (x) is:

d f
dx

=

du
dv   

dv
dw   

dw
dx

(7.20)

let   s now compute the 3 derivatives we need. since in the computation graph

l = ce, we can directly compute the derivative     l
    c :

    l
    c

= e

for the other two, we   ll need to use the chain rule:

    l
    a
    l
    b

=

=

    l
    e
    l
    e

    e
    a
    e
    d

    d
    b

(7.21)

(7.22)

eq. 7.22 thus requires four intermediate derivatives:     l

    b , which
are as follows (making use of the fact that the derivative of a sum is the sum of the
derivatives):

    d , and     d

    a,     e

    e ,     e

l = ce :

e = a + d :

d = 2b :

    l
    c
    e
    d

= e

= 1

    l
    e
    e
    a
    d
    b

= c,

= 1,

= 2

(7.23)

in the backward pass, we compute each of these partials along each edge of
the graph from right to left, multiplying the necessary partials to result in the    nal
derivative we need. thus we begin by annotating the    nal node with     l
    l = 1. moving
to the left, we then compute     l
    e , and so on, until we have annotated the graph

    c and     l

e=d+ad = 2bl=ce31-2e=5d=2l=-10forward passabc144 chapter 7

    neural networks and neural language models

all the way to the input variables. the forward pass conveniently already will have
computed the values of the forward intermediate variables we need (like d and e)
to compute these derivatives. fig. 7.10 shows the backward pass. at each node we
need to compute the local partial derivative with respect to the parent, multiply it by
the partial derivative that is being passed down from the parent, and then pass it to
the child.

figure 7.10 computation graph for the function l(a,b,c) = c(a + 2b), showing the back-
ward pass computation of     l

    a ,     l

    b , and     l
    c .

of course computation graphs for real neural networks are much more complex.
fig. 7.11 shows a sample computation graph for a 2-layer neural network with n0 =
2, n1 = 2, and n2 = 1, assuming binary classi   cation and hence using a sigmoid
output unit for simplicity. the weights that need updating (those for which we need
to know the partial derivative of the id168) are shown in orange.

figure 7.11 sample computation graph for a simple 2-layer neural net (= 1 hidden layer)
with two input dimensions and 2 hidden dimensions.

in order to do the backward pass, we   ll need to know the derivatives of all the
functions in the graph. we already saw in section 5.8 the derivative of the sigmoid
  :

d   (z)

dz

=    (z)(1    z)

(7.24)

we   ll also need the derivatives of each of the other id180. the

derivative of tanh is:

d tanh(z)

dz

= 1    tanh2(z)

(7.25)

e=d+ad = 2bl=cea=3b=1e=5d=2l=-10    l=1   l   l=-4   b   l=-2   dabc   a=-2   b   l=5   c   l=-2   e   l=-2   e   e=1   d   l=5   c   d=2   b   e=1   abackward passc=-2z = +a[2] =    a[1] = reluz[1] = +b[1]****x1x2a[1] = reluz[1] = +b[1]**w[2]11w[1]11w[1]21w[1]12w[1]22b[1]w[2]21l (a[2],y)the derivative of the relu is

d relu(z)

dz

7.5

    neural language models
=(cid:26) 0 f or x < 0
1 f or x     0

145

(7.26)

dropout

hyperparameter

7.4.5 more details on learning
optimization in neural networks is a non-id76 problem, more com-
plex than for id28, and for that and other reasons there are many best
practices for successful learning.

for id28 we can initialize id119 with all the weights and
biases having the value 0. in neural networks, by contrast, we need to initialize the
weights with small random numbers. it   s also helpful to normalize the input values
to have 0 mean and unit variance.

various forms of id173 are used to prevent over   tting. one of the most
important is dropout: randomly dropping some units and their connections from the
network during training (hinton et al. 2012, srivastava et al. 2014).

hyperparameter tuning is also important. the parameters of a neural network
are the weights w and biases b; those are learned by id119. the hyperpa-
rameters are things that are set by the algorithm designer and not learned in the same
way, although they must be tuned. hyperparameters include the learning rate   , the
minibatch size, the model architecture (the number of layers, the number of hidden
nodes per layer, the choice of id180), how to regularize, and so on.
id119 itself also has many architectural variants such as adam (kingma
and ba, 2015).

finally, most modern neural networks are built using computation graph for-
malisms that make all the work of gradient computation and parallelization onto
vector-based gpus (graphic processing units) very easy and natural. pytorch (paszke
et al., 2017) and tensorflow (abadi et al., 2015) are two of the most popular. the
interested reader should consult a neural network textbook for further details; some
suggestions are at the end of the chapter.

7.5 neural language models

as our    rst application of neural networks, let   s consider id38: pre-
dicting upcoming words from prior word context.

neural net-based language models turn out to have many advantages over the n-
gram language models of chapter 3. among these are that neural language models
don   t need smoothing, they can handle much longer histories, and they can general-
ize over contexts of similar words. for a training set of a given size, a neural lan-
guage model has much higher predictive accuracy than an id165 language model
furthermore, neural language models underlie many of the models we   ll introduce
for tasks like machine translation, dialog, and language generation.

on the other hand, there is a cost for this improved performance: neural net
language models are strikingly slower to train than traditional language models, and
so for many tasks an id165 language model is still the right tool.

in this chapter we   ll describe simple feedforward neural language models,    rst
introduced by bengio et al. (2003). modern neural language models are generally
not feedforward but recurrent, using the technology that we will introduce in chap-
ter 9.

146 chapter 7

    neural networks and neural language models
a feedforward neural lm is a standard feedforward network that takes as input
at time t a representation of some number of previous words (wt   1,wt   2, etc) and
outputs a id203 distribution over possible next words. thus   like the id165
lm   the feedforward neural lm approximates the id203 of a word given the
entire prior context p(wt|wt   1

) by approximating based on the n previous words:
p(wt|wt   1

(7.27)
in the following examples we   ll use a 4-gram example, so we   ll show a net to

)     p(wt|wt   1

t   n+1)

1

1

estimate the id203 p(wt = i|wt   1,wt   2,wt   3).
7.5.1 embeddings
in neural language models, the prior context is represented by embeddings of the
previous words. representing the prior context as embeddings, rather than by ex-
act words as used in id165 language models, allows neural language models to
generalize to unseen data much better than id165 language models. for example,
suppose we   ve seen this sentence in training:

i have to make sure when i get home to feed the cat.

but we   ve never seen the word    dog    after the words    feed the   . in our test set we
are trying to predict what comes after the pre   x    i forgot when i got home to feed
the   .

an id165 language model will predict    cat   , but not    dog   . but a neural lm,
which can make use of the fact that    cat    and    dog    have similar embeddings, will
be able to assign a reasonably high id203 to    dog    as well as    cat   , merely
because they have similar vectors.

let   s see how this works in practice. let   s assume we have an embedding dic-
tionary e that gives us, for each word in our vocabulary v , the embedding for that
word, perhaps precomputed by an algorithm like id97 from chapter 6.

fig. 7.12 shows a sketch of this simpli   ed ffnnlm with n=3; we have a mov-
ing window at time t with an embedding vector representing each of the 3 previous
words (words wt   1, wt   2, and wt   3). these 3 vectors are concatenated together to
produce x, the input layer of a neural network whose output is a softmax with a
id203 distribution over words. thus y42, the value of output node 42 is the
id203 of the next word wt being v42, the vocabulary word with index 42.

the model shown in fig. 7.12 is quite suf   cient, assuming we learn the embed-
dings separately by a method like the id97 methods of chapter 6. the method
of using another algorithm to learn the embedding representations we use for input
words is called pretraining. if those pretrained embeddings are suf   cient for your
purposes, then this is all you need.

however, often we   d like to learn the embeddings simultaneously with training
the network. this is true when whatever task the network is designed for (sentiment
classi   cation, or translation, or parsing) places strong constraints on what makes a
good representation.

let   s therefore show an architecture that allows the embeddings to be learned.
to do this, we   ll add an extra layer to the network, and propagate the error all the
way back to the embedding vectors, starting with embeddings with random values
and slowly moving toward sensible representations.

for this to work at the input layer, instead of pre-trained embeddings, we   re
going to represent each of the n previous words as a one-hot vector of length |v|, i.e.,
with one dimension for each word in the vocabulary. a one-hot vector is a vector

pretraining

one-hot vector

7.5

    neural language models

147

figure 7.12 a simpli   ed view of a feedforward neural language model moving through a text. at each
timestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and concate-
nates the 3 embeddings together to get the 1   nd unit input layer x for the network. these units are multiplied
by a weight matrix w and bias vector b and then an activation function to produce a hidden layer h, which
is then multiplied by another weight matrix u. (for graphic simplicity we don   t show b in this and future
pictures). finally, a softmax output layer predicts at each node i the id203 that the next word wt will be
vocabulary word vi. (this picture is simpli   ed because it assumes we just look up in an embedding dictionary
e the d-dimensional embedding vector for each word, precomputed by an algorithm like id97.)

that has one element equal to 1   in the dimension corresponding to that word   s
index in the vocabulary    while all the other elements are set to zero.

thus in a one-hot representation for the word    toothpaste   , supposing it happens
to have index 5 in the vocabulary, x5 is one and and xi = 0    i (cid:54)= 5, as shown here:

[0 0 0 0 1 0 0 ... 0 0 0 0]
... |v|

1 2 3 4 5 6 7 ...

fig. 7.13 shows the additional layers needed to learn the embeddings during lm
training. here the n=3 context words are represented as 3 one-hot vectors, fully
connected to the embedding layer via 3 instantiations of the e embedding matrix.
note that we don   t want to learn separate weight matrices for mapping each of the 3
previous words to the projection layer, we want one single embedding dictionary e
that   s shared among these three. that   s because over time, many different words will
appear as wt   2 or wt   1, and we   d like to just represent each word with one vector,
whichever context position it appears in. the embedding weight matrix e thus has
a row for each word, each a vector of d dimensions, and hence has dimensionality
v    d.

let   s walk through the forward pass of fig. 7.13.
1. select three embeddings from e: given the three previous words, we look
up their indices, create 3 one-hot vectors, and then multiply each by the em-
bedding matrix e. consider wt   3. the one-hot vector for    the    is (index 35) is
multiplied by the embedding matrix e, to give the    rst part of the    rst hidden
layer, called the projection layer. since each row of the input matrix e is just
an embedding for a word, and the input is a one-hot columnvector xi for word

projection layer

h1h2y1h3hdh      uwy42y|v|projection layer1   3dconcatenated embeddingsfor context wordshidden layeroutput layer p(w|u)   inthehole......groundtherelivedword 42embedding forword 35embedding for word 9925embedding for word 45180wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|148 chapter 7

    neural networks and neural language models

figure 7.13
the 3 context words.

learning all the way back to embeddings. notice that the embedding matrix e is shared among

vi, the projection layer for input w will be exi = ei, the embedding for word i.
we now concatenate the three embeddings for the context words.

2. multiply by w: we now multiply by w (and add b) and pass through the

recti   ed linear (or other) activation function to get the hidden layer h.

3. multiply by u: h is now multiplied by u
4. apply softmax: after the softmax, each node i in the output layer estimates

the id203 p(wt = i|wt   1,wt   2,wt   3)

in summary, if we use e to represent the projection layer, formed by concatenat-
ing the 3 embedding for the three context vectors, the equations for a neural language
model become:

e = (ex1,ex2, ...,ex)
h =    (we + b)
z = uh
y = softmax(z)

(7.28)
(7.29)
(7.30)
(7.31)

7.5.2 training the neural language model
to train the model, i.e. to set all the parameters    = e,w,u,b, we do gradient de-
scent (fig. 5.5), using error back propagation on the computation graph to compute
the gradient. training thus not only sets the weights w and u of the network, but
also as we   re predicting upcoming words, we   re learning the embeddings e for each
words that best predict upcoming words.

h1h2y1h3hdh      uwy42y|v|projection layer1   3dhidden layeroutput layer p(w|context)   inthehole......groundtherelivedword 42wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|input layerone-hot vectorsindexword 35001001|v|35001001|v|45180001001|v|992500index word 9925index word 45180e1   |v|d   |v|e is sharedacross words7.6

    summary

149

generally training proceedings by taking as input a very long text, concatenating
all the sentences, start with random weights, and then iteratively moving through
the text predicting each word wt. at each word wt, the cross-id178 (negative log
likelihood) loss is:

the gradient is for this loss is then:

l =    log p(wt|wt   1, ...,wt   n+1)

  t+1 =   t       

        log p(wt|wt   1, ...,wt   n+1)

     

(7.32)

(7.33)

this gradient can be computed in any standard neural network framework which

will then backpropagate through u, w , b, e.

training the parameters to minimize loss will result both in an algorithm for
id38 (a word predictor) but also a new set of embeddings e that can
be used as word representations for other tasks.

7.6 summary

neurons but now simple an abstract computational device.

    neural networks are built out of neural units, originally inspired by human
    each neural unit multiplies input values by a weight vector, adds a bias, and
then applies a non-linear activation function like sigmoid, tanh, or recti   ed
linear.

scent.

to each unit in layer i + 1, and there are no cycles.

representations that can be utilized by later layers in the network.

    in a fully-connected, feedforward network, each unit in layer i is connected
    the power of neural networks comes from the ability of early layers to learn
    neural networks are trained by optimization algorithms like gradient de-
    error back propagation, backward differentiation on a computation graph,
    neural language models use a neural network as a probabilistic classi   er, to
    neural language models can use pretrained embeddings, or can learn embed-

compute the id203 of the next word given the previous n words.

is used to compute the gradients of the id168 for a network.

dings from scratch in the process of id38.

bibliographical and historical notes

the origins of neural networks lie in the 1940s mcculloch-pitts neuron (mccul-
loch and pitts, 1943), a simpli   ed model of the human neuron as a kind of com-
puting element that could be described in terms of id118. by the late
1950s and early 1960s, a number of labs (including frank rosenblatt at cornell and
bernard widrow at stanford) developed research into neural networks; this phase
saw the development of the id88 (rosenblatt, 1958), and the transformation
of the threshold into a bias, a notation we still use (widrow and hoff, 1960).

150 chapter 7

connectionist

    neural networks and neural language models
the    eld of neural networks declined after it was shown that a single percep-
tron unit was unable to model functions as simple as xor (minsky and papert,
1969). while some small amount of work continued during the next two decades,
a major revival for the    eld didn   t come until the 1980s, when practical tools for
building deeper networks like error back propagation became widespread (rumel-
hart et al., 1986). during the 1980s a wide variety of neural network and related
architectures were developed, particularly for applications in psychology and cog-
nitive science (rumelhart and mcclelland 1986b, mcclelland and elman 1986,
rumelhart and mcclelland 1986a,elman 1990), for which the term connection-
ist or parallel distributed processing was often used (feldman and ballard 1982,
smolensky 1988). many of the principles and techniques developed in this period
are foundational to modern work, including the ideas of distributed representations
(hinton, 1986), recurrent networks (elman, 1990), and the use of tensors for com-
positionality (smolensky, 1990).

by the 1990s larger neural networks began to be applied to many practical lan-
guage processing tasks as well, like handwriting recognition (lecun et al. 1989,
lecun et al. 1990) and id103 (morgan and bourlard 1989, morgan
and bourlard 1990). by the early 2000s, improvements in computer hardware and
advances in optimization and training techniques made it possible to train even larger
and deeper networks, leading to the modern term deep learning (hinton et al. 2006,
bengio et al. 2007). we cover more related history in chapter 9.

there are a number of excellent books on the subject. goldberg (2017) has a
superb and comprehensive coverage of neural networks for natural language pro-
cessing. for neural networks in general see goodfellow et al. (2016) and nielsen
(2015).

chapter

8 part-of-speech tagging

parts-of-speech

pos

dionysius thrax of alexandria (c. 100 b.c.), or perhaps someone else (it was a long
time ago), wrote a grammatical sketch of greek (a    techn  e   ) that summarized the
linguistic knowledge of his day. this work is the source of an astonishing proportion
of modern linguistic vocabulary, including words like syntax, diphthong, clitic, and
analogy. also included are a description of eight parts-of-speech: noun, verb,
pronoun, preposition, adverb, conjunction, participle, and article. although earlier
scholars (including aristotle as well as the stoics) had their own lists of parts-of-
speech, it was thrax   s set of eight that became the basis for practically all subsequent
part-of-speech descriptions of most european languages for the next 2000 years.

schoolhouse rock was a series of popular animated educational television clips
from the 1970s. its grammar rock sequence included songs about exactly 8 parts-
of-speech, including the late great bob dorough   s conjunction junction:

conjunction junction, what   s your function?
hooking up words and phrases and clauses...

although the list of 8 was slightly modi   ed from thrax   s original, the astonishing
durability of the parts-of-speech through two millenia is an indicator of both the
importance and the transparency of their role in human language.1

parts-of-speech (also known as pos, word classes, or syntactic categories) are
useful because they reveal a lot about a word and its neighbors. knowing whether a
word is a noun or a verb tells us about likely neighboring words (nouns are preceded
by determiners and adjectives, verbs by nouns) and syntactic structure word (nouns
are generally part of noun phrases), making part-of-speech tagging a key aspect
of parsing (chapter 11). parts of speech are useful features for labeling named
entities like people or organizations in information extraction (chapter 17), or for
coreference resolution (chapter 20). a word   s part-of-speech can even play a role
in id103 or synthesis, e.g., the word content is pronounced content
when it is a noun and content when it is an adjective.

this chapter introduces parts-of-speech, and then introduces two algorithms for
part-of-speech tagging, the task of assigning parts-of-speech to words. one is
generative    hidden markov model (id48)   and one is discriminative   the max-
imum id178 markov model (memm). chapter 9 then introduces a third algorithm
based on the recurrent neural network (id56). all three have roughly equal perfor-
mance but, as we   ll see, have different tradeoffs.

8.1

(mostly) english word classes

until now we have been using part-of-speech terms like noun and verb rather
freely. in this section we give a more complete de   nition of these and other classes.
while word classes do have semantic tendencies   adjectives, for example, often

1 nonetheless, eight isn   t very many and, as we   ll see, recent tagsets have more.

152 chapter 8

    part-of-speech tagging

closed class
open class

function word

noun

proper noun

common noun
count noun
mass noun

verb

adjective

adverb

describe properties and nouns people    parts-of-speech are traditionally de   ned in-
stead based on syntactic and morphological function, grouping words that have sim-
ilar neighboring words (their distributional properties) or take similar af   xes (their
morphological properties).

parts-of-speech can be divided into two broad supercategories: closed class
types and open class types. closed classes are those with relatively    xed member-
ship, such as prepositions   new prepositions are rarely coined. by contrast, nouns
and verbs are open classes   new nouns and verbs like iphone or to fax are contin-
ually being created or borrowed. any given speaker or corpus may have different
open class words, but all speakers of a language, and suf   ciently large corpora,
likely share the set of closed class words. closed class words are generally function
words like of, it, and, or you, which tend to be very short, occur frequently, and
often have structuring uses in grammar.

four major open classes occur in the languages of the world: nouns, verbs,
adjectives, and adverbs. english has all four, although not every language does.
the syntactic class noun includes the words for most people, places, or things, but
others as well. nouns include concrete terms like ship and chair, abstractions like
bandwidth and relationship, and verb-like terms like pacing as in his pacing to and
fro became quite annoying. what de   nes a noun in english, then, are things like its
ability to occur with determiners (a goat, its bandwidth, plato   s republic), to take
possessives (ibm   s annual revenue), and for most but not all nouns to occur in the
plural form (goats, abaci).

open class nouns fall into two classes. proper nouns, like regina, colorado,
and ibm, are names of speci   c persons or entities. in english, they generally aren   t
preceded by articles (e.g., the book is upstairs, but regina is upstairs). in written
english, proper nouns are usually capitalized. the other class, common nouns, are
divided in many languages, including english, into count nouns and mass nouns.
count nouns allow grammatical enumeration, occurring in both the singular and plu-
ral (goat/goats, relationship/relationships) and they can be counted (one goat, two
goats). mass nouns are used when something is conceptualized as a homogeneous
group. so words like snow, salt, and communism are not counted (i.e., *two snows
or *two communisms). mass nouns can also appear without articles where singular
count nouns cannot (snow is white but not *goat is white).

verbs refer to actions and processes, including main verbs like draw, provide,
and go. english verbs have in   ections (non-third-person-sg (eat), third-person-sg
(eats), progressive (eating), past participle (eaten)). while many researchers believe
that all human languages have the categories of noun and verb, others have argued
that some languages, such as riau indonesian and tongan, don   t even make this
distinction (broschart 1997; evans 2000; gil 2000) .

the third open class english form is adjectives, a class that includes many terms
for properties or qualities. most languages have adjectives for the concepts of color
(white, black), age (old, young), and value (good, bad), but there are languages
without adjectives.
in korean, for example, the words corresponding to english
adjectives act as a subclass of verbs, so what is in english an adjective    beautiful   
acts in korean like a verb meaning    to be beautiful   .

the    nal open class form, adverbs, is rather a hodge-podge in both form and

meaning. in the following all the italicized words are adverbs:

actually, i ran home extremely quickly yesterday

what coherence the class has semantically may be solely that each of these
words can be viewed as modifying something (often verbs, hence the name    ad-

8.1

   

(mostly) english word classes

153

verb   , but also other adverbs and entire verb phrases). directional adverbs or loca-
tive adverbs (home, here, downhill) specify the direction or location of some action;
degree adverbs (extremely, very, somewhat) specify the extent of some action, pro-
cess, or property; manner adverbs (slowly, slinkily, delicately) describe the manner
of some action or process; and temporal adverbs describe the time that some ac-
tion or event took place (yesterday, monday). because of the heterogeneous nature
of this class, some adverbs (e.g., temporal adverbs like monday) are tagged in some
tagging schemes as nouns.

the closed classes differ more from language to language than do the open

classes. some of the important closed classes in english include:
prepositions: on, under, over, near, by, at, from, to, with
particles: up, down, on, off, in, out, at, by
determiners: a, an, the
conjunctions: and, but, or, as, if, when
pronouns: she, who, i, others
auxiliary verbs: can, may, should, are
numerals: one, two, three,    rst, second, third

prepositions occur before noun phrases. semantically they often indicate spatial
or temporal relations, whether literal (on it, before then, by the house) or metaphor-
ical (on time, with gusto, beside herself), but often indicate other relations as well,
like marking the agent in (haid113t was written by shakespeare, a particle resembles
a preposition or an adverb and is used in combination with a verb. particles often
have extended meanings that aren   t quite the same as the prepositions they resemble,
as in the particle over in she turned the paper over.

a verb and a particle that act as a single syntactic and/or semantic unit are
called a phrasal verb. the meaning of phrasal verbs is often problematically non-
compositional   not predictable from the distinct meanings of the verb and the par-
ticle. thus, turn down means something like    reject   , rule out    eliminate   ,    nd out
   discover   , and go on    continue   .

a closed class that occurs with nouns, often marking the beginning of a noun
phrase, is the determiner. one small subtype of determiners is the article: english
has three articles: a, an, and the. other determiners include this and that (this chap-
ter, that page). a and an mark a noun phrase as inde   nite, while the can mark it
as de   nite; de   niteness is a discourse property (chapter 21). articles are quite fre-
quent in english; indeed, the is the most frequently occurring word in most corpora
of written english, and a and an are generally right behind.

conjunctions join two phrases, clauses, or sentences. coordinating conjunc-
tions like and, or, and but join two elements of equal status. subordinating conjunc-
tions are used when one of the elements has some embedded status. for example,
that in    i thought that you might like some milk    is a subordinating conjunction
that links the main clause i thought with the subordinate clause you might like some
milk. this clause is called subordinate because this entire clause is the    content    of
the main verb thought. subordinating conjunctions like that which link a verb to its
argument in this way are also called complementizers.

pronouns are forms that often act as a kind of shorthand for referring to some
noun phrase or entity or event. personal pronouns refer to persons or entities (you,
she, i, it, me, etc.). possessive pronouns are forms of personal pronouns that in-
dicate either actual possession or more often just an abstract relation between the
person and some object (my, your, his, her, its, one   s, our, their). wh-pronouns
(what, who, whom, whoever) are used in certain question forms, or may also act as

locative
degree
manner
temporal

preposition

particle

phrasal verb

determiner
article

conjunctions

complementizer
pronoun
personal
possessive

wh

154 chapter 8

    part-of-speech tagging

auxiliary

copula
modal

interjection
negative

complementizers (frida, who married diego. . . ).

a closed class subtype of english verbs are the auxiliary verbs. cross-linguist-
ically, auxiliaries mark semantic features of a main verb: whether an action takes
place in the present, past, or future (tense), whether it is completed (aspect), whether
it is negated (polarity), and whether an action is necessary, possible, suggested, or
desired (mood). english auxiliaries include the copula verb be, the two verbs do and
have, along with their in   ected forms, as well as a class of modal verbs. be is called
a copula because it connects subjects with certain kinds of predicate nominals and
adjectives (he is a duck). the verb have can mark the perfect tenses (i have gone, i
had gone), and be is used as part of the passive (we were robbed) or progressive (we
are leaving) constructions. modals are used to mark the mood associated with the
event depicted by the main verb: can indicates ability or possibility, may permission
or possibility, must necessity. there is also a modal use of have (e.g., i have to go).
english also has many words of more or less unique function, including inter-
jections (oh, hey, alas, uh, um), negatives (no, not), politeness markers (please,
thank you), greetings (hello, goodbye), and the existential there (there are two on
the table) among others. these classes may be distinguished or lumped together as
interjections or adverbs depending on the purpose of the labeling.

8.2 the id32 part-of-speech tagset

an important tagset for english is the 45-tag id32 tagset (marcus et al.,
1993), shown in fig. 8.1, which has been used to label many corpora.
in such
labelings, parts-of-speech are generally represented by placing the tag after each
word, delimited by a slash:

example
tag description
and, but, or pdt predeterminer

example tag description example
all, both

vbp verb non-3sg

eat

tag
cc

description
coordinating
conjunction
cardinal number
determiner
existential    there   

adverb

rbr comparative

pos possessive ending    s
prp
personal pronoun
prp$ possess. pronoun

one, two
a, the
there
mea culpa rb
of, in, by

cd
dt
ex
fw foreign word
in
preposition/
subordin-conj
adjective
comparative adj
superlative adj
list item marker
modal
sing or mass noun llama
llamas
noun, plural
ibm
proper noun, sing.

jj
jjr
jjs
ls
md
nn
nns
nnp
nnps proper noun, plu. carolinas vbn verb past part.
figure 8.1 id32 part-of-speech tags (including punctuation).

rbs superlatv. adverb
yellow
rp
bigger
particle
sym symbol
wildest
1, 2, one
to
   to   
can, should uh
interjection
vb
verb base form
vbd verb past tense
vbg verb gerund

fastest
up, off
+,%, &
to
ah, oops
eat
ate
eating
eaten

adverb

present

vbz verb 3sg pres

i, you, he wdt wh-determ.
your, one   s wp
wh-pronoun
wp$ wh-possess.
quickly
faster
wrb wh-adverb

eats
which, that
what, who
whose
how, where

$
#
   
   
(
)
,
.
:

$
#
    or    
    or    
[, (, {, <
], ), }, >
,

dollar sign
pound sign
left quote
right quote
left paren
right paren
comma
sent-end punc . ! ?
sent-mid punc : ; ...     -

(8.1) the/dt grand/jj jury/nn commented/vbd on/in a/dt number/nn of/in

other/jj topics/nns ./.

(8.2) there/ex are/vbp 70/cd children/nns there/rb

brown

wsj
switchboard

8.2

    the id32 part-of-speech tagset
(8.3) preliminary/jj    ndings/nns were/vbd reported/vbn in/in today/nn
   s/pos new/nnp england/nnp journal/nnp of/in medicine/nnp ./.

155

example (8.1) shows the determiners the and a, the adjectives grand and other,
the common nouns jury, number, and topics, and the past tense verb commented.
example (8.2) shows the use of the ex tag to mark the existential there construction
in english, and, for comparison, another use of there which is tagged as an adverb
(rb). example (8.3) shows the segmentation of the possessive morpheme    s a pas-
sive construction,    were reported   , in which reported is marked as a past participle
(vbn). note that since new england journal of medicine is a proper noun, the tree-
bank tagging chooses to mark each noun in it separately as nnp, including journal
and medicine, which might otherwise be labeled as common nouns (nn).

corpora labeled with parts-of-speech are crucial training (and testing) sets for
statistical tagging algorithms. three main tagged corpora are consistently used for
training and testing part-of-speech taggers for english. the brown corpus is a mil-
lion words of samples from 500 written texts from different genres published in the
united states in 1961. the wsj corpus contains a million words published in the
wall street journal in 1989. the switchboard corpus consists of 2 million words
of telephone conversations collected in 1990-1991. the corpora were created by
running an automatic part-of-speech tagger on the texts and then human annotators
hand-corrected each tag.

there are some minor differences in the tagsets used by the corpora. for example
in the wsj and brown corpora, the single penn tag to is used for both the in   nitive
to (i like to race) and the preposition to (go to the store), while in switchboard the
tag to is reserved for the in   nitive use of to and the preposition is tagged in:

well/uh ,/, i/prp ,/, i/prp want/vbp to/to go/vb to/in a/dt restauran-
t/nn

finally, there are some idiosyncracies inherent in any tagset. for example, be-
cause the penn 45 tags were collapsed from a larger 87-tag tagset, the original
brown tagset, some potential useful distinctions were lost. the penn tagset was
designed for a treebank in which sentences were parsed, and so it leaves off syntac-
tic information recoverable from the parse tree. thus for example the penn tag in is
used for both subordinating conjunctions like if, when, unless, after:
after/in spending/vbg a/dt day/nn at/in the/dt beach/nn

and prepositions like in, on, after:

after/in sunrise/nn

words are generally tokenized before tagging. the id32 and the

british national corpus split contractions and the    s-genitive from their stems:2

would/md n   t/rb
children/nns    s/pos

the treebank tagset assumes that id121 of multipart words like new
york is done at whitespace, thus tagging. a new york city    rm as a/dt new/nnp
york/nnp city/nnp    rm/nn.

another commonly used tagset, the universal pos tag set of the universal de-
pendencies project (nivre et al., 2016a), is used when building systems that can tag
many languages. see section 8.7.

2

indeed, the treebank tag pos is used only for    s, which must be segmented in id121.

156 chapter 8

    part-of-speech tagging

8.3 part-of-speech tagging

part-of-speech
tagging

ambiguous

ambiguity
resolution

part-of-speech tagging is the process of assigning a part-of-speech marker to each
word in an input text.3 the input to a tagging algorithm is a sequence of (tokenized)
words and a tagset, and the output is a sequence of tags, one per token.

tagging is a disambiguation task; words are ambiguous    have more than one
possible part-of-speech   and the goal is to    nd the correct tag for the situation.
for example, book can be a verb (book that    ight) or a noun (hand me that book).
that can be a determiner (does that    ight serve dinner) or a complementizer (i
thought that your    ight was earlier). the goal of pos-tagging is to resolve these
ambiguities, choosing the proper tag for the context. how common is tag ambiguity?
fig. 8.2 shows that most word types (80-86%) are unambiguous (janet is always
nnp, funniest jjs, and hesitantly rb). but the ambiguous words, though accounting
for only 14-15% of the vocabulary, are very common words, and hence 55-67% of
word tokens in running text are ambiguous.4

types:

tokens:

unambiguous (1 tag)
ambiguous

(2+ tags)

unambiguous (1 tag)
ambiguous

(2+ tags)

wsj

44,432 (86%)
7,025 (14%)

brown

45,799 (85%)
8,050 (15%)

577,421 (45%) 384,349 (33%)
711,780 (55%) 786,646 (67%)

figure 8.2
tagging. punctuation were treated as words, and words were kept in their original case.

tag ambiguity for word types in brown and wsj, using treebank-3 (45-tag)

some of the most ambiguous frequent words are that, back, down, put and set;

here are some examples of the 6 different parts-of-speech for the word back:

earnings growth took a back/jj seat
a small building in the back/nn
a clear majority of senators back/vbp the bill
dave began to back/vb toward the door
enable the country to buy back/rp about debt
i was twenty-one back/rb then

nonetheless, many words are easy to disambiguate, because their different tags
aren   t equally likely. for example, a can be a determiner or the letter a, but the
determiner sense is much more likely. this idea suggests a simplistic baseline algo-
rithm for part-of-speech tagging: given an ambiguous word, choose the tag which is
most frequent in the training corpus. this is a key concept:

most frequent class baseline: always compare a classi   er against a baseline at
least as good as the most frequent class baseline (assigning each token to the class
it occurred in most often in the training set).

accuracy

how good is this baseline? a standard way to measure the performance of part-
of-speech taggers is accuracy: the percentage of tags correctly labeled (matching

3 tags are also applied to punctuation, so assumes tokenzing of commas, quotation marks, etc., and
disambiguating end-of-sentence periods from periods inside words (e.g., etc.).
4 note the large differences across the two genres, especially in token frequency. tags in the wsj corpus
are less ambiguous; its focus on    nancial news leads to a more limited distribution of word usages than
the diverse genres of the brown corpus.

8.4

    id48 part-of-speech tagging

157

human labels on a test set). if we train on the wsj training corpus and test on sec-
tions 22-24 of the same corpus the most-frequent-tag baseline achieves an accuracy
of 92.34%. by contrast, the state of the art in part-of-speech tagging on this dataset
is around 97% tag accuracy, a performance that is achievable by most algorithms
(id48s, memms, neural networks, rule-based algorithms). see section 8.7 on
other languages and genres.

8.4 id48 part-of-speech tagging

sequence model

markov chain

in this section we introduce the use of the hidden markov model for part-of-speech
tagging. the id48 is a sequence model. a sequence model or sequence classi-
   er is a model whose job is to assign a label or class to each unit in a sequence,
thus mapping a sequence of observations to a sequence of labels. an id48 is a
probabilistic sequence model: given a sequence of units (words, letters, morphemes,
sentences, whatever), it computes a id203 distribution over possible sequences
of labels and chooses the best label sequence.

8.4.1 markov chains
the id48 is based on augmenting the markov chain. a markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. these sets can be words, or
tags, or symbols representing anything, for example the weather. a markov chain
makes a very strong assumption that if we want to predict the future in the sequence,
all that matters is the current state. all the states before the current state have no im-
pact on the future except via the current state. it   s as if to predict tomorrow   s weather
you could examine today   s weather but you weren   t allowed to look at yesterday   s
weather.

(a)

(b)

figure 8.3 a markov chain for weather (a) and one for words (b), showing states and
transitions. a start distribution    is required; setting    = [0.1, 0.7, 0.2] for (a) would mean a
id203 0.7 of starting in state 2 (cold), id203 0.1 of starting in state 1 (hot), etc.

markov
assumption

more formally, consider a sequence of state variables q1,q2, ...,qi. a markov
model embodies the markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn   t matter, only the present.

markov assumption: p(qi = a|q1...qi   1) = p(qi = a|qi   1)

(8.4)

figure 8.3a shows a markov chain for assigning a id203 to a sequence of
weather events, for which the vocabulary consists of hot, cold, and warm. the

warm3hot1cold2.8.6.1.1.3.6.1.1.3charminguniformlyare.1.4.5.5.5.2.6.2158 chapter 8

    part-of-speech tagging

states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. the transitions are probabilities: the values of arcs leaving a given
state must sum to 1. figure 8.3b shows a markov chain for assigning a id203 to
a sequence of words w1...wn. this markov chain should be familiar; in fact, it repre-
sents a bigram language model, with each edge expressing the id203 p(wi|w j)!
given the two models in fig. 8.3, we can assign a id203 to any sequence from
our vocabulary.

formally, a markov chain is speci   ed by the following components:

q = q1q2 . . .qn
a = a11a12 . . .an1 . . .ann

   =   1,  2, ...,  n

a set of n states
a transition id203 matrix a, each ai j represent-
ing the id203 of moving from state i to state j, s.t.

an initial id203 distribution over states.   i is the
id203 that the markov chain will start in state i.
some states j may have    j = 0, meaning that they cannot

(cid:80)n
j=1 ai j = 1    i
be initial states. also,(cid:80)n

i=1   i = 1

before you go on, use the sample probabilities in fig. 8.3a (with    = [.1, .7.,2])

to compute the id203 of each of the following sequences:

(8.5) hot hot hot hot
(8.6) cold hot cold hot

what does the difference in these probabilities tell you about a real-world weather
fact encoded in fig. 8.3a?

8.4.2 the hidden markov model
a markov chain is useful when we need to compute a id203 for a sequence
of observable events. in many cases, however, the events we are interested in are
hidden: we don   t observe them directly. for example we don   t normally observe
part-of-speech tags in a text. rather, we see words, and must infer the tags from the
word sequence. we call the tags hidden because they are not observed.

a hidden markov model (id48) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. an id48 is speci   ed by
the following components:

hidden

hidden
markov model

q = q1q2 . . .qn
a = a11 . . .ai j . . .ann

o = o1o2 . . .ot

b = bi(ot )

   =   1,  2, ...,  n

j=1 ai j = 1    i

a set of n states
a transition id203 matrix a, each ai j representing the id203

of moving from state i to state j, s.t.(cid:80)n

a sequence of t observations, each one drawn from a vocabulary v =
v1,v2, ...,vv
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the id203 of an observation ot being generated
from a state i
an initial id203 distribution over states.   i is the id203 that
the markov chain will start in state i. some states j may have    j = 0,

meaning that they cannot be initial states. also,(cid:80)n

i=1   i = 1

8.4

    id48 part-of-speech tagging

159

a    rst-order hidden markov model instantiates two simplifying assumptions.
first, as with a    rst-order markov chain, the id203 of a particular state depends
only on the previous state:

markov assumption: p(qi|q1...qi   1) = p(qi|qi   1)

(8.7)

second, the id203 of an output observation oi depends only on the state that

produced the observation qi and not on any other states or any other observations:

output independence: p(oi|q1 . . .qi, . . . ,qt ,o1, . . . ,oi, . . . ,ot ) = p(oi|qi)

(8.8)

8.4.3 the components of an id48 tagger
let   s start by looking at the pieces of an id48 tagger, and then we   ll see how to use
it to tag. an id48 has two components, the a and b probabilities.

the a matrix contains the tag transition probabilities p(ti|ti   1) which represent
the id203 of a tag occurring given the previous tag. for example, modal verbs
like will are very likely to be followed by a verb in the base form, a vb, like race, so
we expect this id203 to be high. we compute the maximum likelihood estimate
of this transition id203 by counting, out of the times we see the    rst tag in a
labeled corpus, how often the    rst tag is followed by the second:

p(ti|ti   1) =

c(ti   1,ti)
c(ti   1)

(8.9)

in the wsj corpus, for example, md occurs 13124 times of which it is followed

by vb 10471, for an id113 estimate of

p(v b|md) =

c(md,v b)

c(md)

=

10471
13124

= .80

(8.10)

let   s walk through an example, seeing how these probabilities are estimated and

used in a sample tagging task, before we return to the algorithm for decoding.

in id48 tagging, the probabilities are estimated by counting on a tagged training

corpus. for this example we   ll use the tagged wsj corpus.

the b emission probabilities, p(wi|ti), represent the id203, given a tag (say
md), that it will be associated with a given word (say will). the id113 of the emis-
sion id203 is

p(wi|ti) =

c(ti,wi)

c(ti)

(8.11)

of the 13124 occurrences of md in the wsj corpus, it is associated with will 4046
times:

p(will|md) =

c(md,will)

c(md)

=

4046
13124

= .31

(8.12)

we saw this kind of bayesian modeling in chapter 4; recall that this likelihood
term is not asking    which is the most likely tag for the word will?    that would be
the posterior p(md|will). instead, p(will|md) answers the slightly counterintuitive
question    if we were going to generate a md, how likely is it that this modal would
be will?   

the a transition probabilities, and b observation likelihoods of the id48 are
illustrated in fig. 8.4 for three states in an id48 part-of-speech tagger; the full
tagger would have one state for each tag.

160 chapter 8

    part-of-speech tagging

figure 8.4 an illustration of the two parts of an id48 representation:
the a transition
probabilities used to compute the prior id203, and the b observation likelihoods that are
associated with each state, one likelihood for each possible observation word.
8.4.4 id48 tagging as decoding
for any model, such as an id48, that contains hidden variables, the task of deter-
mining the hidden variables sequence corresponding to the sequence of observations
is called decoding. more formally,

decoding: given as input an id48    = (a,b) and a sequence of ob-
servations o = o1,o2, ...,ot ,    nd the most probable sequence of states
q = q1q2q3 . . .qt .

decoding

for id52, the goal of id48 decoding is to choose the tag
1 that is most probable given the observation sequence of n words words

sequence tn
wn
1:

  tn
1 = argmax

tn
1

p(tn

1|wn
1)

(8.13)

the way we   ll do this in the id48 is to use bayes    rule to instead compute:

  tn
1 = argmax

tn
1

p(wn

1 )p(tn
1|tn
1 )
p(wn
1)

furthermore, we simplify eq. 8.14 by dropping the denominator p(wn
1):

  tn
1 = argmax

tn
1

1 )p(tn
p(wn
1|tn
1 )

(8.14)

(8.15)

id48 taggers make two further simplifying assumptions. the    rst is that the
id203 of a word appearing depends only on its own tag and is independent of
neighboring words and tags:

p(wn
1|tn
1 )    

p(wi|ti)

(8.16)

n(cid:89)i=1

the second assumption, the bigram assumption, is that the id203 of a tag

is dependent only on the previous tag, rather than the entire tag sequence;

p(tn

1 )    

n(cid:89)i=1

p(ti|ti   1)

(8.17)

nn3vb1md2a22a11a12a21a13a33a32a23a31p("aardvark" | nn)...p(   will    | nn)...p("the" | nn)...p(   back    | nn)...p("zebra" | nn)b3p("aardvark" | vb)...p(   will    | vb)...p("the" | vb)...p(   back    | vb)...p("zebra" | vb)b1p("aardvark" | md)...p(   will    | md)...p("the" | md)...p(   back    | md)...p("zebra" | md)b28.4

    id48 part-of-speech tagging

161

plugging the simplifying assumptions from eq. 8.16 and eq. 8.17 into eq. 8.15
results in the following equation for the most probable tag sequence from a bigram
tagger:

  tn
1 = argmax

tn
1

p(tn

1|wn

1)     argmax

tn
1

emission
(cid:122) (cid:125)(cid:124) (cid:123)
p(wi|ti)

transition
(cid:123)
(cid:125)(cid:124)
(cid:122)
p(ti|ti   1)

n(cid:89)i=1

(8.18)

the two parts of eq. 8.18 correspond neatly to the b emission id203 and

a transition id203 that we just de   ned above!

viterbi
algorithm

8.4.5 the viterbi algorithm
the decoding algorithm for id48s is the viterbi algorithm shown in fig. 8.5. as
an instance of id145, viterbi resembles the dynamic program-
ming minimum id153 algorithm of chapter 2.

function viterbi(observations of len t,state-graph of len n) returns best-path, path-prob

create a path id203 matrix viterbi[n,t]
for each state s from 1 to n do
viterbi[s,1]     s     bs(o1)
backpointer[s,1]   0

for each time step t from 2 to t do
for each state s from 1 to n do

; initialization step

; recursion step

nmax
viterbi[s,t]   
s(cid:48) =1
backpointer[s,t]   

nmax

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )
nargmax
s(cid:48) =1

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )
; termination step

s=1

viterbi[s,t ]

bestpathprob   
bestpathpointer   
bestpath   the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob

; termination step

viterbi[s,t ]

nargmax

s=1

figure 8.5 viterbi algorithm for    nding the optimal sequence of tags. given an observation sequence and an
id48    = (a,b), the algorithm returns the state path through the id48 that assigns maximum likelihood to
the observation sequence.

the viterbi algorithm    rst sets up a id203 matrix or lattice, with one col-
umn for each observation ot and one row for each state in the state graph. each col-
umn thus has a cell for each state qi in the single combined automaton. figure 8.6
shows an intuition of this lattice for the sentence janet will back the bill.

each cell of the trellis, vt ( j), represents the id203 that the id48 is in state
j after seeing the    rst t observations and passing through the most probable state
sequence q1, ...,qt   1, given the id48    . the value of each cell vt ( j) is computed
by recursively taking the most probable path that could lead us to this cell. formally,
each cell expresses the id203

vt ( j) = max
q1,...,qt   1

p(q1...qt   1,o1,o2 . . .ot ,qt = j|   )

(8.19)

we represent the most probable path by taking the maximum over all possible
. like other id145 algorithms,

previous state sequences max
q1,...,qt   1

162 chapter 8

    part-of-speech tagging

figure 8.6 a sketch of the lattice for janet will back the bill, showing the possible tags (qi)
for each word and highlighting the path corresponding to the correct tag sequence through the
hidden states. states (parts-of-speech) which have a zero id203 of generating a particular
word according to the b matrix (such as the id203 that a determiner dt will be realized
as janet) are greyed out.

viterbi    lls each cell recursively. given that we had already computed the probabil-
ity of being in every state at time t    1, we compute the viterbi id203 by taking
the most probable of the extensions of the paths that lead to the current cell. for a
given state q j at time t, the value vt ( j) is computed as

vt ( j) =

nmax
i=1

vt   1(i) ai j b j(ot )

(8.20)

the three factors that are multiplied in eq. 8.20 for extending the previous paths to
compute the viterbi id203 at time t are

vt   1(i)
ai j
b j(ot )

the previous viterbi path id203 from the previous time step
the transition id203 from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

8.4.6 working through an example
let   s tag the sentence janet will back the bill; the goal is the correct series of tags
(see also fig. 8.6):
(8.21) janet/nnp will/md back/vb the/dt bill/nn

let the id48 be de   ned by the two tables in fig. 8.7 and fig. 8.8. figure 8.7
lists the ai j probabilities for transitioning between the hidden states (part-of-speech
tags). figure 8.8 expresses the bi(ot ) probabilities, the observation likelihoods of
words given tags. this table is (slightly simpli   ed) from counts in the wsj corpus.
so the word janet only appears as an nnp, back has 4 possible parts of speech, and
the word the can appear as a determiner or as an nnp (in titles like    somewhere
over the rainbow    all words are tagged as nnp).

figure 8.9 shows a    eshed-out version of the sketch we saw in fig. 8.6, the
viterbi trellis for computing the best hidden state sequence for the observation se-
quence janet will back the bill.

jjnnpnnpnnpmdmdmdmdvbvbjjjjjjnnnnrbrbrbrbdtdtdtdtnnpjanetwillbackthebillnnvbmdnnvbjjrbnnpdtnnvb163

jj

8.4

rb

dt

nn

    id48 part-of-speech tagging
vb
0.0031 0.0453 0.0449 0.0510 0.2026
0.0009 0.0084 0.0584 0.0090 0.0025
0.7968 0.0005 0.0008 0.1698 0.0041
0.0050 0.0837 0.0615 0.0514 0.2231
0.0001 0.0733 0.4509 0.0036 0.0036
0.0014 0.0086 0.1216 0.0177 0.0068
0.1011 0.1012 0.0120 0.0728 0.0479
0.0002 0.2157 0.4744 0.0102 0.0017

<s >
nnp
md
vb
jj
nn
rb
dt
figure 8.7 the a transition probabilities p(ti|ti   1) computed from the wsj corpus without
smoothing. rows are labeled with the conditioning event; thus p(v b|md) is 0.7968.

nnp
0.2767
0.3777
0.0008
0.0322
0.0366
0.0096
0.0068
0.1147

md
0.0006
0.0110
0.0002
0.0005
0.0004
0.0176
0.0102
0.0021

nnp
md
vb
jj
nn
rb
dt

bill

will

janet
0.000032 0
0
0
0
0
0
0

back
the
0.000048 0
0
0.308431 0
0
0
0.000028
0.000028 0.000672 0
0
0
0.000340 0
0.002337
0.000200 0.000223 0
0
0.010446 0
0
0.506099 0
0
0

figure 8.8 observation likelihoods b computed from the wsj corpus without smoothing,
simpli   ed slightly.

there are n = 5 state columns. we begin in column 1 (for the word janet) by
setting the viterbi value in each cell to the product of the    transition id203
(the start id203 for that state i, which we get from the < s > entry of fig. 8.7),
and the observation likelihood of the word janet given the tag for that cell. most of
the cells in the column are zero since the word janet cannot be any of those tags.
the reader should    nd this in fig. 8.9.

next, each cell in the will column gets updated. for each state, we compute the
value viterbi[s,t] by taking the maximum over the extensions of all the paths from the
previous column that lead to the current cell according to eq. 8.20. we have shown
the values for the md, vb, and nn cells. each cell gets the max of the 7 values
from the previous column, multiplied by the appropriate transition id203; as it
happens in this case, most of them are zero from the previous column. the remaining
value is multiplied by the relevant observation id203, and the (trivial) max is
taken. in this case the    nal value, .0000002772, comes from the nnp state at the
previous column. the reader should    ll in the rest of the trellis in fig. 8.9 and
backtrace to reconstruct the correct state sequence nnp md vb dt nn.

8.4.7 extending the id48 algorithm to trigrams
practical id48 taggers have a number of extensions of this simple model. one
important missing feature is a wider tag context. in the tagger described above the
id203 of a tag depends only on the previous tag:

p(tn

1 )    

n(cid:89)i=1

p(ti|ti   1)

(8.22)

in practice we use more of the history, letting the id203 of a tag depend on

164 chapter 8

    part-of-speech tagging

figure 8.9 the    rst few entries in the individual state columns for the viterbi algorithm. each cell keeps the
id203 of the best path so far and a pointer to the previous cell along that path. we have only    lled out
columns 1 and 2; to avoid clutter most cells with value 0 are left empty. the rest is left as an exercise for the
reader. after the cells are    lled in, backtracing from the end state, we should be able to reconstruct the correct
state sequence nnp md vb dt nn.

the two previous tags:

p(tn

1 )    

n(cid:89)i=1

p(ti|ti   1,ti   2)

(8.23)

extending the algorithm from bigram to trigram taggers gives a small (perhaps a
half point) increase in performance, but conditioning on two previous tags instead of
one requires a signi   cant change to the viterbi algorithm. for each cell, instead of
taking a max over transitions from each cell in the previous column, we have to take
a max over paths through the cells in the previous two columns, thus considering n2
rather than n hidden states at every observation.

in addition to increasing the context window, id48 taggers have a number of
other advanced features. one is to let the tagger know the location of the end of the
sentence by adding dependence on an end-of-sequence marker for tn+1. this gives
the following equation for part-of-speech tagging:

  tn
1 = argmax

tn
1

p(tn

1|wn

1)     argmax

tn
1

p(wi|ti)p(ti|ti   1,ti   2)(cid:35)p(tn+1|tn)

(cid:34) n(cid:89)i=1

(8.24)

in tagging any sentence with eq. 8.24, three of the tags used in the context will
fall off the edge of the sentence, and hence will not match regular words. these tags,

  p(nnp|start) = .28* p(md|md)= 0*  p(md|nnp).000009*.01  = .0000009 v1(2)=.0006 x 0 = 0v1(1) = .28* .000032 = .000009tmdq2q1o1janetbillwillo2o3backvbjjv1(3)=.0031 x 0 = 0v1(4)= .045*0=0o4  *  p(md|vb) = 0 * p(md|jj)= 0p(vb|start) = .0031p(jj |start) =.045backtraceq3q4thennq5rbq6dtq7v2(2) =max * .308 =.0000002772v2(5)=max * .0002 = .0000000001v2(3)=max * .000028 =     2.5e-11 v3(6)=max * .0104v3(5)=max * .000223v3(4)=max * .00034v3(3)=max * .00067v1(5)v1(6)v1(7)v2(1)v2(4)v2(6)v2(7)backtrace* p(rb|nn)* p(nn|nn)startstartstartstartstarto5nnpp(md|start) = .00068.4

    id48 part-of-speech tagging

165

t   1, t0, and tn+1, can all be set to be a single special    sentence boundary    tag that is
added to the tagset, which assumes sentences boundaries have already been marked.
one problem with trigram taggers as instantiated in eq. 8.24 is data sparsity.
any particular sequence of tags ti   2,ti   1,ti that occurs in the test set may simply
never have occurred in the training set. that means we cannot compute the tag
trigram id203 just by the maximum likelihood estimate from counts, following
eq. 8.25:

p(ti|ti   1,ti   2) =

c(ti   2,ti   1,ti)
c(ti   2,ti   1)

(8.25)

just as we saw with id38, many of these counts will be zero
in any training set, and we will incorrectly predict that a given tag sequence will
never occur! what we need is a way to estimate p(ti|ti   1,ti   2) even if the sequence
ti   2,ti   1,ti never occurs in the training data.
the standard approach to solving this problem is the same interpolation idea
we saw in id38: estimate the id203 by combining more robust,
but weaker estimators. for example, if we   ve never seen the tag sequence prp vb
to, and so can   t compute p(to|prp,vb) from this frequency, we still could rely
on the bigram id203 p(to|vb), or even the unigram id203 p(to). the
id113 of each of these probabilities can be computed from
a corpus with the following counts:

trigrams

bigrams

  p(ti|ti   1,ti   2) =
  p(ti|ti   1) =
  p(ti) =

unigrams

c(ti   2,ti   1,ti)
c(ti   2,ti   1)
c(ti   1,ti)
c(ti   1)
c(ti)

n

(8.26)

(8.27)

(8.28)

the standard way to combine these three estimators to estimate the trigram probabil-
ity p(ti|ti   1,ti   2) is via linear interpolation. we estimate the id203 p(ti|ti   1ti   2)
by a weighted sum of the unigram, bigram, and trigram probabilities:
p(ti|ti   1ti   2) =   3   p(ti|ti   1ti   2) +   2   p(ti|ti   1) +   1   p(ti)

(8.29)

we require   1 +   2 +   3 = 1, ensuring that the resulting p is a id203 distri-
bution. the    s are set by deleted interpolation (jelinek and mercer, 1980): we
successively delete each trigram from the training corpus and choose the    s so as to
maximize the likelihood of the rest of the corpus. the deletion helps to set the    s
in such a way as to generalize to unseen data and not over   t. figure 8.10 gives a
deleted interpolation algorithm for tag trigrams.

8.4.8 id125
when the number of states grows very large, the vanilla viterbi algorithm be slow.
the complexity of the algorithm is o(n2t ); n (the number of states) can be large
for trigram taggers, which have to consider every previous pair of the 45 tags, re-
sulting in 453 = 91,125 computations per column. n can be even larger for other
applications of viterbi, for example to decoding in neural networks, as we will see
in future chapters.

one common solution to the complexity problem is the use of id125
decoding. in id125, instead of keeping the entire column of states at each

deleted
interpolation

id125

166 chapter 8

    part-of-speech tagging

function deleted-interpolation(corpus) returns   1,  2,  3
  1,   2,   3   0
foreach trigram t1,t2,t3 with c(t1,t2,t3) > 0
depending on the maximum of the following three values

case c(t1,t2,t3)   1
case c(t2,t3)   1
case c(t3)   1

c(t1,t2)   1 : increment   3 by c(t1,t2,t3)
c(t2)   1 : increment   2 by c(t1,t2,t3)
n   1 : increment   1 by c(t1,t2,t3)

end

end
normalize   1,  2,  3
return   1,  2,  3

figure 8.10 the deleted interpolation algorithm for setting the weights for combining un-
igram, bigram, and trigram tag probabilities. if the denominator is 0 for any case, we de   ne
the result of that case to be 0. n is the number of tokens in the corpus. after brants (2000).

time point t, we just keep the best few hypothesis at that point. at time t this requires
computing the viterbi score for each of the n cells, sorting the scores, and keeping
only the best-scoring states. the rest are pruned out and not continued forward to
time t + 1.

one way to implement id125 is to keep a    xed number of states instead of
all n current states. here the beam width    is a    xed number of states. alternatively
   can be modeled as a    xed percentage of the n states, or as a id203 threshold.
figure 8.11 shows the search lattice using a beam width of 2 states.

beam width

figure 8.11 a id125 version of fig. 8.6, showing a beam width of 2. at each time
t, all (non-zero) states are computed, but then they are sorted and only the best 2 states are
propagated forward and the rest are pruned, shown in orange.

jjnnpnnpnnpmdmdmdmdvbvbjjjjjjnnnnrbrbrbrbdtdtdtdtnnpjanetwillbackthebillnnvbmdnnvbjjrbnnpdtnnvbunknown
words

8.5

    maximum id178 markov models

167

8.4.9 unknown words

words people
never use    
could be
only i
know them

ishikawa takuboku 1885   1912

to achieve high accuracy with part-of-speech taggers, it is also important to have
a good model for dealing with unknown words. proper names and acronyms are
created very often, and even new common nouns and verbs enter the language at a
surprising rate. one useful feature for distinguishing parts of speech is word shape:
words starting with capital letters are likely to be proper nouns (nnp).

but the strongest source of information for guessing the part-of-speech of un-
known words is morphology. words that end in -s are likely to be plural nouns
(nns), words ending with -ed tend to be past participles (vbn), words ending with
-able adjectives (jj), and so on. we store for each    nal letter sequence (for sim-
plicity referred to as word suf   xes) of up to 10 letters the statistics of the tag it was
associated with in training. we are thus computing for each suf   x of length i the
id203 of the tag ti given the suf   x letters (samuelsson 1993, brants 2000):

p(ti|ln   i+1 . . .ln)

(8.30)

back-off is used to smooth these probabilities with successively shorter suf   xes.
because unknown words are unlikely to be closed-class words like prepositions,
suf   x probabilities can be computed only for words whose training set frequency is
    10, or only for open-class words. separate suf   x tries are kept for capitalized and
uncapitalized words.
finally, because eq. 8.30 gives a posterior estimate p(ti|wi), we can compute
the likelihood p(wi|ti) that id48s require by using bayesian inversion (i.e., using
bayes rule and computation of the two priors p(ti) and p(ti|ln   i+1 . . .ln)).
in addition to using capitalization information for unknown words, brants (2000)
also uses capitalization for known words by adding a capitalization feature to each
tag. thus, instead of computing p(ti|ti   1,ti   2) as in eq. 8.26, the algorithm com-
putes the id203 p(ti,ci|ti   1,ci   1,ti   2,ci   2). this is equivalent to having a cap-
italized and uncapitalized version of each tag, doubling the size of the tagset.
combining all these features, a trigram id48 like that of brants (2000) has a
tagging accuracy of 96.7% on the id32, perhaps just slightly below the
performance of the best memm and neural taggers.

8.5 maximum id178 markov models

while an id48 can achieve very high accuracy, we saw that it requires a number of
architectural innovations to deal with unknown words, backoff, suf   xes, and so on.
it would be so much easier if we could add arbitrary features directly into the model
in a clean way, but that   s hard for generative models like id48s. luckily, we   ve
already seen a model for doing this: the id28 model of chapter 5! but
id28 isn   t a sequence model; it assigns a class to a single observation.
however, we could turn id28 into a discriminative sequence model
simply by running it on successive words, using the class assigned to the prior word

168 chapter 8

    part-of-speech tagging

memm

let the sequence of words be w = wn

as a feature in the classi   cation of the next word. when we apply id28
in this way, it   s called the maximum id178 markov model or memm5
1 and the sequence of tags t = tn

1. in an
id48 to compute the best tag sequence that maximizes p(t|w ) we rely on bayes   
rule and the likelihood p(w|t ):
  t = argmax

= argmax

= argmax

t

t

p(t|w )
p(w|t )p(t )
p(wordi|tagi)(cid:89)i
t (cid:89)i

p(tagi|tagi   1)

(8.31)

in an memm, by contrast, we compute the posterior p(t|w ) directly, training it to
discriminate among the possible tag sequences:
p(t|w )
t (cid:89)i

p(ti|wi,ti   1)

  t = argmax

= argmax

(8.32)

t

consider tagging just one word. a multinomial id28 classi   er could
compute the single id203 p(ti|wi,ti   1) in a different way that an id48. fig. 8.12
shows the intuition of the difference via the direction of the arrows; id48s compute
likelihood (observation word conditioned on tags) but memms compute posterior
(tags conditioned on observation words).

figure 8.12 a schematic view of the id48 (top) and memm (bottom) representation of
the id203 computation for the correct sequence of tags for the back sentence. the id48
computes the likelihood of the observation given the hidden state, while the memm computes
the posterior of each state, conditioned on the previous state and current observation.

8.5.1 features in a memm
of course we don   t build memms that condition just on wi and ti   1. the reason to
use a discriminative sequence model is that it   s easier to incorporate a lots of fea-
tures.6 figure 8.13 shows a graphical intuition of some of these additional features.

5

   maximum id178 model    is an outdated name for id28; see the history section.

6 because in id48s all computation is based on the two probabilities p(tag|tag) and p(word|tag), if
we want to include some source of knowledge into the tagging process, we must    nd a way to encode
the knowledge into one of these two probabilities. each time we add a feature we have to do a lot of
complicated conditioning which gets harder and harder as we have more and more such features.

willmdvbdtnnjanetbackthebillnnpwillmdvbdtnnjanetbackthebillnnp8.5

    maximum id178 markov models

169

figure 8.13 an memm for part-of-speech tagging showing the ability to condition on
more features.

a basic memm part-of-speech tagger conditions on the observation word it-
self, neighboring words, and previous tags, and various combinations, using feature
templates like the following:

templates

(cid:104)ti,wi   2(cid:105),(cid:104)ti,wi   1(cid:105),(cid:104)ti,wi(cid:105),(cid:104)ti,wi+1(cid:105),(cid:104)ti,wi+2(cid:105)
(cid:104)ti,ti   1(cid:105),(cid:104)ti,ti   2,ti   1(cid:105),
(cid:104)ti,ti   1,wi(cid:105),(cid:104)ti,wi   1,wi(cid:105)(cid:104)ti,wi,wi+1(cid:105),

(8.33)

recall from chapter 5 that feature templates are used to automatically populate the
set of features from every instance in the training and test set. thus our example
janet/nnp will/md back/vb the/dt bill/nn, when wi is the word back, would gen-
erate the following features:

ti = vb and wi   2 = janet
ti = vb and wi   1 = will
ti = vb and wi = back
ti = vb and wi+1 = the
ti = vb and wi+2 = bill
ti = vb and ti   1 = md
ti = vb and ti   1 = md and ti   2 = nnp
ti = vb and wi = back and wi+1 = the

also necessary are features to deal with unknown words, expressing properties of
the word   s spelling or shape:

wi contains a particular pre   x (from all pre   xes of length     4)
wi contains a particular suf   x (from all suf   xes of length     4)
wi contains a number
wi contains an upper-case letter
wi contains a hyphen
wi is all upper case
wi   s word shape
wi   s short word shape
wi is upper case and has a digit and a dash (like cfc-12)
wi is upper case and followed within 3 words by co., inc., etc.

word shape

word shape features are used to represent the abstract letter pattern of the word
by mapping lower-case letters to    x   , upper-case to    x   , numbers to    d   , and retaining
punctuation. thus for example i.m.f would map to x.x.x. and dc10-30 would
map to xxdd-dd. a second class of shorter word shape features is also used. in these
features consecutive character types are removed, so dc10-30 would be mapped to
xd-d but i.m.f would still map to x.x.x. for example the word well-dressed would
generate the following non-zero valued feature values:

willmdvbjanetbackthebillnnp<s>wiwi+1wi-1ti-1ti-2wi-1170 chapter 8

    part-of-speech tagging
pre   x(wi) = w
pre   x(wi) = we
pre   x(wi) = wel
pre   x(wi) = well
suf   x(wi) = ssed
suf   x(wi) = sed
suf   x(wi) = ed
suf   x(wi) = d
has-hyphen(wi)
word-shape(wi) = xxxx-xxxxxxx
short-word-shape(wi) = x-x

features for known words, like the templates in eq. 8.33, are computed for every
word seen in the training set. the unknown word features can also be computed for
all words in training, or only on training words whose frequency is below some
threshold. the result of the known-word templates and word-signature features is a
very large set of features. generally a feature cutoff is used in which features are
thrown out if they have count < 5 in the training set.

8.5.2 decoding and training memms
the most likely sequence of tags is then computed by combining these features of
the input word wi, its neighbors within l words wi+l
i   k as
follows (using    to refer to feature weights instead of w to avoid the confusion with
w meaning words):

i   l, and the previous k tags ti   1

  t = argmax

= argmax

t

p(t|w )
t (cid:89)i

= argmax

t (cid:89)i

p(ti|wi+l

i   l,ti   1
i   k )

exp      (cid:88)j
exp      (cid:88)j
(cid:88)t(cid:48)   tagset

   j f j(ti,wi+l

i   l,ti   1

   j f j(t(cid:48),wi+l

i   k )      
i   k )      

i   l,ti   1

(8.34)

how should we decode to    nd this optimal tag sequence   t ? the simplest way
to turn id28 into a sequence model is to build a local classi   er that
classi   es each word left to right, making a hard classi   cation of the    rst word in
the sentence, then a hard decision on the second word, and so on. this is called a
greedy decoding algorithm, because we greedily choose the best tag for each word,
as shown in fig. 8.14.

greedy

function greedy sequence decoding(words w, model p) returns tag sequence t

for i = 1 to length(w)

  ti = argmax
t(cid:48)    t

p(t(cid:48) | wi+l

i   l,ti   1
i   k )

figure 8.14
each time making a hard decision of which is the best tag.

in greedy decoding we simply run the classi   er on each token, left to right,

8.6

    bidirectionality

171

viterbi

the problem with the greedy algorithm is that by making a hard decision on
each word before moving on to the next word, the classi   er can   t use evidence from
future decisions. although the greedy algorithm is very fast, and occasionally has
suf   cient accuracy to be useful, in general the hard decision causes too much a drop
in performance, and we don   t use it.

instead we decode an memm with the viterbi algorithm just as with the id48,

   nding the sequence of part-of-speech tags that is optimal for the whole sentence.

for example, assume that our memm is only conditioning on the previous tag
ti   1 and observed word wi. concretely, this involves    lling an n    t array with
the appropriate values for p(ti|ti   1,wi), maintaining backpointers as we proceed. as
with id48 viterbi, when the table is    lled, we simply follow pointers back from the
maximum value in the    nal column to retrieve the desired set of labels. the requisite
changes from the id48-style application of viterbi have to do only with how we
   ll each cell. recall from eq. 8.20 that the recursive step of the viterbi equation
computes the viterbi value of time t for state j as

vt ( j) =

nmax
i=1

vt   1(i)ai j b j(ot ); 1     j     n,1 < t     t

(8.35)

which is the id48 implementation of

vt ( j) =

nmax
i=1

vt   1(i) p(s j|si) p(ot|s j) 1     j     n,1 < t     t

(8.36)

the memm requires only a slight change to this latter formula, replacing the a and
b prior and likelihood probabilities with the direct posterior:

vt ( j) =

nmax
i=1

vt   1(i) p(s j|si,ot ) 1     j     n,1 < t     t

(8.37)

learning in memms relies on the same supervised learning algorithms we presented
for id28. given a sequence of observations, feature functions, and cor-
responding hidden states, we use id119 to train the weights to maximize
the log-likelihood of the training corpus.

8.6 bidirectionality

label bias
observation
bias

the one problem with the memm and id48 models as presented is that they are
exclusively run left-to-right. while the viterbi algorithm still allows present deci-
sions to be in   uenced indirectly by future decisions, it would help even more if a
decision about word wi could directly use information about future tags ti+1 and ti+2.
adding bidirectionality has another useful advantage. memms have a theoret-
ical weakness, referred to alternatively as the label bias or observation bias prob-
lem (lafferty et al. 2001, toutanova et al. 2003). these are names for situations
when one source of information is ignored because it is explained away by another
source. consider an example from toutanova et al. (2003), the sequence will/nn
to/to    ght/vb. the tag to is often preceded by nn but rarely by modals (md),
and so that tendency should help predict the correct nn tag for will. but the previ-
ous transition p(twill|(cid:104)s(cid:105)) prefers the modal, and because p(to|to,twill) is so close
to 1 regardless of twill the model cannot make use of the transition id203 and
incorrectly chooses md. the strong information that to must have the tag to has ex-
plained away the presence of to and so the model doesn   t learn the importance of

172 chapter 8

    part-of-speech tagging

crf

stanford tagger

the previous nn tag for predicting to. bidirectionality helps the model by making
the link between to available when tagging the nn.

one way to implement bidirectionality is to switch to a more powerful model
called a conditional random    eld or crf. the crf is an undirected graphical
model, which means that it   s not computing a id203 for each tag at each time
step. instead, at each time step the crf computes log-linear functions over a clique,
a set of relevant features. unlike for an memm, these might include output features
of words in future time steps. the id203 of the best sequence is similarly
computed by the viterbi algorithm. because a crf normalizes probabilities over all
tag sequences, rather than over all the tags at an individual time t, training requires
computing the sum over all possible labelings, which makes crf training quite slow.
simpler methods can also be used; the stanford tagger uses a bidirectional
version of the memm called a cyclic dependency network (toutanova et al., 2003).
alternatively, any sequence model can be turned into a bidirectional model by
using multiple passes. for example, the    rst pass would use only part-of-speech
features from already-disambiguated words on the left. in the second pass, tags for
all words, including those on the right, can be used. alternately, the tagger can be run
twice, once left-to-right and once right-to-left. in greedy decoding, for each word
the classi   er chooses the highest-scoring of the tag assigned by the left-to-right and
right-to-left classi   er. in viterbi decoding, the classi   er chooses the higher scoring
of the two sequences (left-to-right or right-to-left). these bidirectional models lead
directly into the bi-lstm models that we will introduce in chapter 9 as a standard
neural sequence model.

8.7 part-of-speech tagging for other languages

augmentations to tagging algorithms become necessary when dealing with lan-
guages with rich morphology like czech, hungarian and turkish.

these productive word-formation processes result in a large vocabulary for these
languages: a 250,000 word token corpus of hungarian has more than twice as many
word types as a similarly sized corpus of english (oravecz and dienes, 2002), while
a 10 million word token corpus of turkish contains four times as many word types
as a similarly sized english corpus (hakkani-t  ur et al., 2002). large vocabular-
ies mean many unknown words, and these unknown words cause signi   cant per-
formance degradations in a wide variety of languages (including czech, slovene,
estonian, and romanian) (haji  c, 2000).

highly in   ectional languages also have much more information than english
coded in word morphology, like case (nominative, accusative, genitive) or gender
(masculine, feminine). because this information is important for tasks like pars-
ing and coreference resolution, part-of-speech taggers for morphologically rich lan-
guages need to label words with case and gender information. tagsets for morpho-
logically rich languages are therefore sequences of morphological tags rather than a
single primitive tag. here   s a turkish example, in which the word izin has three pos-
sible morphological/part-of-speech tags and meanings (hakkani-t  ur et al., 2002):

1. yerdeki izin temizlenmesi gerek.

the trace on the    oor should be cleaned.
  uzerinde parmak izin kalmis  
your    nger print is left on (it).

2.

iz + noun+a3sg+pnon+gen

iz + noun+a3sg+p2sg+nom

3. ic  eri girmek ic  in izin alman gerekiyor.

you need a permission to enter.

8.8

    summary

173

izin + noun+a3sg+pnon+nom

using a morphological parse sequence like noun+a3sg+pnon+gen as the part-
of-speech tag greatly increases the number of parts-of-speech, and so tagsets can
be 4 to 10 times larger than the 50   100 tags we have seen for english. with such
large tagsets, each word needs to be morphologically analyzed to generate the list
of possible morphological tag sequences (part-of-speech tags) for the word. the
role of the tagger is then to disambiguate among these tags. this method also helps
with unknown words since morphological parsers can accept unknown stems and
still segment the af   xes properly.

for non-word-space languages like chinese, id40 (chapter 2) is
either applied before tagging or done jointly. although chinese words are on aver-
age very short (around 2.4 characters per unknown word compared with 7.7 for en-
glish) the problem of unknown words is still large. while english unknown words
tend to be proper nouns in chinese the majority of unknown words are common
nouns and verbs because of extensive compounding. tagging models for chinese
use similar unknown word features to english, including character pre   x and suf-
   x features, as well as novel features like the radicals of each character in a word.
(tseng et al., 2005b).

a stanford for multilingual tagging is the universal pos tag set of the universal
dependencies project, which contains 16 tags plus a wide variety of features that
can be added to them to create a large tagset for any language (nivre et al., 2016a).

8.8 summary

this chapter introduced parts-of-speech and part-of-speech tagging:

    languages generally have a small set of closed class words that are highly
frequent, ambiguous, and act as function words, and open-class words like
nouns, verbs, adjectives. various part-of-speech tagsets exist, of between 40
and 200 tags.

    part-of-speech tagging is the process of assigning a part-of-speech label to

each of a sequence of words.

    two common approaches to sequence modeling are a generative approach,
id48 tagging, and a discriminative approach, memm tagging. we will see
a third, discriminative neural approach in chapter 9.

    the probabilities in id48 taggers are estimated by maximum likelihood es-
timation on tag-labeled training corpora. the viterbi algorithm is used for
decoding,    nding the most likely tag sequence

    id125 is a variant of viterbi decoding that maintains only a fraction of

high scoring states rather than all states during decoding.

    maximum id178 markov model or memm taggers train logistic regres-
sion models to pick the best tag given an observation word and its context and
the previous tags, and then use viterbi to choose the best sequence of tags.

    modern taggers are generally run bidirectionally.

174 chapter 8

    part-of-speech tagging

bibliographical and historical notes

what is probably the earliest part-of-speech tagger was part of the parser in zellig
harris   s transformations and discourse analysis project (tdap), implemented be-
tween june 1958 and july 1959 at the university of pennsylvania (harris, 1962),
although earlier systems had used part-of-speech dictionaries. tdap used 14 hand-
written rules for part-of-speech disambiguation; the use of part-of-speech tag se-
quences and the relative frequency of tags for a word pre   gures all modern algo-
rithms. the parser was implemented essentially as a cascade of    nite-state trans-
ducers; see joshi and hopely (1999) and karttunen (1999) for a reimplementation.
the computational grammar coder (cgc) of klein and simmons (1963) had
three components: a lexicon, a morphological analyzer, and a context disambiguator.
the small 1500-word lexicon listed only function words and other irregular words.
the morphological analyzer used in   ectional and derivational suf   xes to assign part-
of-speech classes. these were run over words to produce candidate parts-of-speech
which were then disambiguated by a set of 500 context rules by relying on sur-
rounding islands of unambiguous words. for example, one rule said that between an
article and a verb, the only allowable sequences were adj-noun, noun-
adverb, or noun-noun. the taggit tagger (greene and rubin, 1971) used
the same architecture as klein and simmons (1963), with a bigger dictionary and
more tags (87). taggit was applied to the brown corpus and, according to francis
and ku  cera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the
brown corpus was then tagged by hand. all these early algorithms were based on
a two-stage architecture in which a dictionary was    rst used to assign each word a
set of potential parts-of-speech, and then lists of hand-written disambiguation rules
winnowed the set down to a single part-of-speech per word.

soon afterwards probabilistic architectures began to be developed. probabili-
ties were used in tagging by stolz et al. (1965) and a complete probabilistic tagger
with viterbi decoding was sketched by bahl and mercer (1976). the lancaster-
oslo/bergen (lob) corpus, a british english equivalent of the brown corpus, was
tagged in the early 1980   s with the claws tagger (marshall 1983; marshall 1987;
garside 1987), a probabilistic algorithm that approximated a simpli   ed id48 tag-
ger. the algorithm used tag bigram probabilities, but instead of storing the word
likelihood of each tag, the algorithm marked tags either as rare (p(tag|word) < .01)
infrequent (p(tag|word) < .10) or normally frequent (p(tag|word) > .10).
derose (1988) developed a quasi-id48 algorithm, including the use of dy-
namic programming, although computing p(t|w)p(w) instead of p(w|t)p(w). the
same year, the probabilistic parts tagger of church (1988), (1989) was probably
the    rst implemented id48 tagger, described correctly in church (1989), although
church (1988) also described the computation incorrectly as p(t|w)p(w) instead
of p(w|t)p(w). church (p.c.) explained that he had simpli   ed for pedagogical pur-
poses because using the id203 p(t|w) made the idea seem more understandable
as    storing a lexicon in an almost standard form   .
later taggers explicitly introduced the use of the hidden markov model (ku-
piec 1992; weischedel et al. 1993; sch  utze and singer 1994). merialdo (1994)
showed that fully unsupervised em didn   t work well for the tagging task and that
reliance on hand-labeled data was important. charniak et al. (1993) showed the im-
portance of the most frequent tag baseline; the 92.3% number we give above was
from abney et al. (1999). see brants (2000) for many implementation details of an
id48 tagger whose performance is still roughly close to state of the art taggers.

exercises

175

ratnaparkhi (1996) introduced the memm tagger, called mxpost, and the

modern formulation is very much based on his work.

the idea of using letter suf   xes for unknown words is quite old; the early klein
and simmons (1963) system checked all    nal letter suf   xes of lengths 1-5. the
probabilistic formulation we described for id48s comes from samuelsson (1993).
the unknown word features described on page 169 come mainly from (ratnaparkhi,
1996), with augmentations from toutanova et al. (2003) and manning (2011).

state of the art taggers use neural algorithms or (bidirectional) id148
toutanova et al. (2003). id48 (brants 2000; thede and harper 1999) and memm
tagger accuracies are likely just a tad lower.

an alternative modern formalism, the english constraint grammar systems (karls-

son et al. 1995; voutilainen 1995; voutilainen 1999), uses a two-stage formalism
much like the early taggers from the 1950s and 1960s. a morphological analyzer
with tens of thousands of english word stem entries returns all parts-of-speech for a
word, using a large feature-based tagset. so the word occurred is tagged with the op-
tions (cid:104)v pcp2 sv(cid:105) and (cid:104)v past vfin sv(cid:105), meaning it can be a participle (pcp2)
for an intransitive (sv) verb, or a past (past)    nite (vfin) form of an intransitive
(sv) verb. a set of 3,744 constraints are then applied to the input sentence to rule
out parts-of-speech inconsistent with the context. for example here   s a rule for the
ambiguous word that that eliminates all tags except the adv (adverbial intensi   er)
sense (this is the sense in the sentence it isn   t that odd):

adverbial-that rule given input:    that   
if (+1 a/adv/quant); /* if next word is adj, adverb, or quanti   er */

(+2 sent-lim);
(not -1 svoc/a); /* and the previous word is not a verb like */

/* and following which is a sentence boundary, */

then eliminate non-adv tags else eliminate adv tag

/*    consider    which allows adjs as object complements */

manning (2011) investigates the remaining 2.7% of errors in a state-of-the-art
tagger, the bidirectional memm-style model described above (toutanova et al.,
2003). he suggests that a third or half of these remaining errors are due to errors or
inconsistencies in the training data, a third might be solvable with richer linguistic
models, and for the remainder the task is underspeci   ed or unclear.

supervised tagging relies heavily on in-domain training data hand-labeled by
experts. ways to relax this assumption include unsupervised algorithms for cluster-
ing words into part-of-speech-like classes, summarized in christodoulopoulos et al.
(2010), and ways to combine labeled and unlabeled data, for example by co-training
(clark et al. 2003; s  gaard 2010).

see householder (1995) for historical notes on parts-of-speech, and sampson
(1987) and garside et al. (1997) on the provenance of the brown and other tagsets.

exercises

8.1

find one tagging error in each of the following sentences that are tagged with
the id32 tagset:

1. i/prp need/vbp a/dt    ight/nn from/in atlanta/nn
2. does/vbz this/dt    ight/nn serve/vb dinner/nns
3. i/prp have/vb a/dt friend/nn living/vbg in/in denver/nnp
4. can/vbp you/prp list/vb the/dt nonstop/jj afternoon/nn    ights/nns

176 chapter 8

    part-of-speech tagging

8.2 use the id32 tagset to tag each word in the following sentences
from damon runyon   s short stories. you may ignore punctuation. some of
these are quite dif   cult; do your best.

1. it is a nice night.
2. this crap game is over a garage in fifty-second street. . .
3.
4. he is a tall, skinny guy with a long, sad, mean-looking kisser, and a

. . . nobody ever takes the newspapers she sells . . .

5.

mournful voice.
. . . i am sitting in mindy   s restaurant putting on the ge   llte    sh, which is
a dish i am very fond of, . . .

6. when a guy and a doll get to taking peeks back and forth at each other,

why there you are indeed.

8.4

8.3 now compare your tags from the previous exercise with one or two friend   s

answers. on which words did you disagree the most? why?
implement the    most likely tag    baseline. find a pos-tagged training set,
and use it to compute for each word the tag that maximizes p(t|w). you will
need to implement a simple tokenizer to deal with sentence boundaries. start
by assuming that all unknown words are nn and compute your error rate on
known and unknown words. now write at least    ve rules to do a better job of
tagging unknown words, and show the difference in error rates.

8.5 build a bigram id48 tagger. you will need a part-of-speech-tagged corpus.
first split the corpus into a training set and test set. from the labeled training
set, train the transition and observation probabilities of the id48 tagger di-
rectly on the hand-tagged data. then implement the viterbi algorithm so that
you can label an arbitrary test sentence. now run your algorithm on the test
set. report its error rate and compare its performance to the most frequent tag
baseline.

8.6 do an error analysis of your tagger. build a confusion matrix and investigate
the most frequent errors. propose some features for improving the perfor-
mance of your tagger on these errors.

chapter

9 sequence

recurrent networks

processing with

time will explain.
jane austin, persuasion

in chapter 7, we explored feedforward neural networks along with their applications
to neural language models and text classi   cation. in the case of language models,
we saw that such networks can be trained to make predictions about the next word in
a sequence given a limited context of preceding words     an approach that is remi-
niscent of the markov approach to id38 discussed in chapter 3. these
models operated by accepting a small    xed-sized window of tokens as input; longer
sequences are processed by sliding this window over the input making incremental
predictions, with the end result being a sequence of predictions spanning the input.
fig. 9.1, reproduced here from chapter 7, illustrates this approach with a window
of size 3. here, we   re predicting which word will come next given the window the
ground there. subsequent words are predicted by sliding the window forward one
word at a time.

unfortunately, the sliding window approach is problematic for a number of rea-
sons. first, it shares the primary weakness of markov approaches in that it limits
the context from which information can be extracted; anything outside the context
window has no impact on the decision being made. this is problematic since there
are many language tasks that require access to information that can be arbitrarily dis-
tant from the point at which processing is happening. second, the use of windows
makes it dif   cult for networks to learn systematic patterns arising from phenomena
like constituency. for example, in fig. 9.1 the phrase the ground appears twice in
different windows: once, as shown, in the    rst and second positions in the window,
and in in the preceding step in the second and third slots, thus forcing the network
to learn two separate patterns for a single constituent.

the subject of this chapter is recurrent neural networks, a class of networks
designed to address these problems by processing sequences explicitly as sequences,
allowing us to handle variable length inputs without the use of arbitrary    xed-sized
windows.

9.1 simple recurrent networks

a recurrent neural network is any network that contains is a cycle within its network
connections. that is, any network where the value of a unit is directly, or indirectly,
dependent on its own output as an input. in general, such networks are dif   cult to
reason about, and to train. however, within the general class of recurrent networks
there are constrained architectures that have proven to be extremely useful when

177

178 chapter 9

    sequence processing with recurrent networks

figure 9.1 a simpli   ed view of a feedforward neural language model moving through a text. at each
timestep t the network takes the 3 context words, converts each to a d-dimensional embeddings, and con-
catenates the 3 embeddings together to get the 1   nd unit input layer x for the network.

simple
recurrent
networks
elman
networks

applied to language problems. in this section, we   ll introduce a class of recurrent
networks referred to as simple recurrent networks (srns) or elman networks
(elman, 1990). these networks are useful in their own right, and will serve as the
basis for more complex approaches to be discussed later in this chapter and again in
chapter 22.

fig. 9.2 abstractly illustrates the recurrent structure of an srn. as with ordinary
feed-forward networks, an input vector representing the current input element, xt,
is multiplied by a weight matrix and then passed through an activation function to
compute an activation value for a layer of hidden of units. this hidden layer is,
in turn, used to calculate a corresponding output, yt. sequences are processed by
presenting one element at a time to the network. the key difference from a feed-

figure 9.2 simple recurrent neural network after elman (elman, 1990). the hidden layer
includes a recurrent connection as part of its input. that is, the activation value of the hidden
layer depends on the current input as well as the activation value of the hidden layer from the
previous timestep.

h1h2y1h3hdh      uwy42y|v|projection layer1   3dconcatenated embeddingsfor context wordshidden layeroutput layer p(w|u)   inthehole......groundtherelivedword 42embedding forword 35embedding for word 9925embedding for word 45180wt-1wt-2wtwt-3dh   3d1   dh|v|   dhp(wt=v42|wt-3,wt-2,wt-3)1   |v|htytxt9.1

    simple recurrent networks

179

figure 9.3 simple recurrent neural network illustrated as a feed-forward network.

forward network lies in the recurrent link shown in the    gure with the dashed line.
this link augments the input to the hidden layer with the activation value of the
hidden layer from the preceding point in time.

the hidden layer from the previous timestep provides a form of memory, or
context, that encodes earlier processing and informs the decisions to be made at later
points in time. importantly, the architecture does not impose a    xed-length limit
on this prior context; the context embodied in the previous hidden layer includes
information extending back to the beginning of the sequence.

adding this temporal dimension makes recurrent networks appear to be more
exotic than non-recurrent architectures. but in reality, they   re not all that different.
given an input vector and the values for the hidden layer from the previous time
step, we   re still performing the standard feed-forward calculation. to see this, con-
sider fig. 9.3 which clari   es the nature of the recurrence and how it factors into the
computation at the hidden layer. the most signi   cant addition lies in the new set of
weights, u, that connect the hidden layer from the previous timestep to the current
hidden layer. these weights determine how the network should make use of past
context in calculating the output for the current input. as with the other weights in
the network, these connections will be trained via id26.

id136 in simple id56s

9.1.1
forward id136 (mapping a sequence of inputs to a sequence of outputs) in an
srn is nearly identical to what we   ve already seen with feedforward networks. to
compute an output yt for an input xt, we need the activation value for the hidden
layer ht. to calculate this, we compute the dot product of the input xt with the weight
matrix w , and the dot product of the hidden layer from the previous time step ht   1
with the weight matrix u. we add these values together and pass them through a
suitable activation function, g, to arrive at the activation value for the current hidden
layer, ht. once we have the values for the hidden layer, we proceed with the usual
computation to generate the output vector.

ht = g(uht   1 +w xt )
yt = f (v ht )

in the commonly encountered case of soft classi   cation,    nding yt consists of
a softmax computation that provides a normalized id203 distribution over the

uvwytxththt-1180 chapter 9

    sequence processing with recurrent networks

figure 9.4 a simple recurrent neural network shown unrolled in time. network layers are copied for each
timestep, while the weights u, v and w are shared in common across all timesteps.

possible output classes.

yt = softmax(v ht )

the sequential nature of simple recurrent networks can be illustrated by un-
rolling the network in time as is shown in fig. 9.4.
in    gures such as this, the
various layers of units are copied for each time step to illustrate that they will have
differing values over time. however the weights themselves are shared across the
various timesteps. finally, the fact that the computation at time t requires the value
of the hidden layer from time t   1 mandates an incremental id136 algorithm that
proceeds from the start of the sequence to the end as shown in fig. 9.5.

function forwardid56(x, network) returns output sequence y

h0   0
for i   1 to length(x) do
hi   g(u hi   1 + w xi)
yi    f (v hi)

return y

figure 9.5 forward id136 in a simple recurrent network.

9.1.2 training
as we did with feed-forward networks, we   ll use a training set, a id168, and
id26 to adjust the sets of weights in these recurrent networks. as shown
in fig. 9.3, we now have 3 sets of weights to update: w , the weights from the input

uvwuvwuvwx1x2x3y1y2y3h1h3h2h09.1

    simple recurrent networks

181

layer to the hidden layer, u, the weights from the previous hidden layer to the current
hidden layer, and    nally v , the weights from the hidden layer to the output layer.

before going on, let   s    rst review some of the notation that we introduced in
chapter 7. assuming a network with an input layer x and a non-linear activation
function g, we   ll use a[i] to refer to the activation value from a layer i, which is the
result of applying g to z[i], the weighted sum of the inputs to that layer. a simple
two-layer feedforward network with w and v as the    rst and second sets of weights
respectively, would be characterized as follows.
z[1] = w x
a[1] = g(z[1])
z[2] = ua[1]
a[2] = g(z[2])

y = a[2]

fig. 9.4 illustrates the two considerations that we didn   t have to worry about with
id26 in feed-forward networks. first, to compute the id168 for
the output at time t we need the hidden layer from time t     1. second, the hidden
layer at time t in   uences both the output at time t and the hidden layer at time t + 1
(and hence the output and loss at t + 1). it follows from this that to assess the error
accruing to ht, we   ll need to know its in   uence on both the current output as well as
the next one.

consider the situation where we are examining an input/output pair at time 2 as
shown in fig. 9.4. what do we need to compute the gradients needed to update the
weights u, v , and w here? let   s start by reviewing how we compute the gradients
required to update v (this computation is unchanged from feed-forward networks).
to review from chapter 7, we need to compute the derivative of the id168 l
with respect to the weights v . however, since the loss is not expressed directly in
terms of the weights, we apply the chain rule to get there indirectly.

    l
   v

=

    l
    a

    a
    z

    z
   v

the    rst term is just the derivative of the id168 with respect to the network
output, which is just the activation of the output layer, a. the second term is the
derivative of the network output with respect to the intermediate network activation
z, which is a function of the activation function g. the    nal term in our application of
the chain rule is the derivative of the network activation with respect to the weights
v , which is just the activation value of the current hidden layer ht.

it   s useful here to use the    rst two terms to de   ne    , an error term that represents

how much of the scalar loss is attributable to each of the units in the output layer.

    l
    a

    a
  out =
    z
  out = l(cid:48)g(cid:48)(z)

therefore, the    nal gradient we need to update the weight matrix v is just:

    l
   v

=   outht

(9.1)

(9.2)

(9.3)

moving on, we need to compute the corresponding gradients for the weight ma-
   u . here we encounter the    rst substantive change from

trices w and u:     l

   w and     l

182 chapter 9

    sequence processing with recurrent networks

figure 9.6 the id26 of errors in an srn. the ti vectors represent the targets for each element
of the sequence from the training data. the red arrows illustrate the    ow of backpropagated errors required to
calculate the updates for u, v and w at time 2. the two incoming arrows converging on h2 signal that these
errors need to be summed.

feed-forward networks. the hidden state at time t contributes to the output and asso-
ciated error at time t and to the output and error at the next timestep, t +1. therefore,
the error term,   h, for the hidden layer must be the sum of the error term from the
current output and its error from the next time step.

  h = g(cid:48)(z)v  out +   next

given this total error term for the hidden layer, we can compute the gradients for

the weights u and w in the usual way using the chain rule as we did in chapter 7.

dl
dw
dl
du

=

=

dl
dz
dl
dz

dz
da
dz
da

da
dw
da
du

    l
   w
    l
   u

=   hxt

=   hht   1

these gradients provide us with the information needed to update the matrices u
and w through ordinary id26.

we   re not quite done yet, we still need to assign proportional blame (compute
the error term) back to the previous hidden layer ht   1 for use in further processing.

uvwuvwuvwx1x2x3y1y2y3h1h3h2h0t1t2t39.1

    simple recurrent networks

183

function backpropthroughtime(sequence, network) returns gradients for weight
updates
forward pass to gather the loss
backward pass compute error terms and assess blame

figure 9.7 id26 training through time. the forward pass computes the re-
quired loss values at each time step. the backward pass computes the gradients using the
values from the forward pass.

this involves backpropagating the error from   h to ht   1 proportionally based on the
weights in u.

  next = g(cid:48)(z)u  h

(9.4)

at this point we have all the gradients needed to perform weight updates for each
of our three sets of weights. note that in this simple case there is no need to back-
propagate the error through w to the input x, since the input training data is assumed
to be    xed. if we wished to update our input word or character embeddings we
would backpropagate the error through to them as well. we   ll discuss this more in
section 9.5.

taken together, all of these considerations lead to a two-pass algorithm for train-
ing the weights in srns. in the    rst pass, we perform forward id136, computing
ht, yt, and an loss at each step in time, saving the value of the hidden layer at each
step for use at the next time step. in the second phase, we process the sequence
in reverse, computing the required error terms gradients as we go, computing and
saving the error term for use in the hidden layer for each step backward.

unfortunately, computing the gradients and updating weights for each item of a
sequence individually would be extremely time-consuming. instead, much as we did
with mini-batch training in chapter 7, we will accumulate gradients for the weights
incrementally over the sequence, and then use those accumulated gradients in per-
forming weight updates.

9.1.3 unrolled networks as computational graphs
we used the unrolled network shown in fig. 9.4 as a way to understand the dynamic
behavior of these networks over time. however, with modern computational frame-
works and adequate computing resources, explicitly unrolling a recurrent network
into a deep feed-forward computational graph is quite practical for word-by-word
approaches to sentence-level processing. in such an approach, we provide a tem-
plate that speci   es the basic structure of the srn, including all the necessary pa-
rameters for the input, output, and hidden layers, the weight matrices, as well as the
activation and output functions to be used. then, when provided with an input se-
quence such as a training sentence, we can compile a feed-forward graph speci   c to
that input, and use that graph to perform forward id136 or training via ordinary
id26.

for applications that involve much longer input sequences, such as speech recog-
nition, character-by-character sentence processing, or streaming of continuous in-
puts, unrolling an entire input sequence may not be feasible. in these cases, we can
unroll the input into manageable    xed-length segments and treat each segment as a
distinct training item. this approach is called truncated id26 through
time (tbtt).

184 chapter 9

    sequence processing with recurrent networks

figure 9.8 part-of-speech tagging as sequence labeling with a simple id56. pre-trained
id27s serve as inputs and a softmax layer provides a id203 distribution over
the part-of-speech tags as output at each time step.

9.2 applications of id56s

simple recurrent networks have proven to be an effective approach to language mod-
eling, sequence labeling tasks such as part-of-speech tagging, as well as sequence
classi   cation tasks such as id31 and topic classi   cation. and as we   ll
see in chapter 22, they form the basic building blocks for sequence to sequence
approaches to applications such as summarization and machine translation.

9.2.1 generation with neural language models
[coming soon]

9.2.2 sequence labeling
in sequence labeling, the network   s job is to assign a label to each element of a
sequence chosen from a small    xed set of labels. the canonical example of such a
task is part-of-speech tagging, discussed in chapter 8. in a recurrent network-based
approach to id52, inputs are words and the outputs are tag probabilities
generated by a softmax layer over the pos tagset, as illustrated in fig. 9.8.

in this    gure, the inputs at each time step are pre-trained id27s cor-
responding to the input tokens. the id56 block is an abstraction that represents
an unrolled simple recurrent network consisting of an input layer, hidden layer, and
output layer at each time step, as well as the shared u, v and w weight matrices that
comprise the network. the outputs of the network at each time step represent the
distribution over the pos tagset generated by a softmax layer. to generate an actual
tag sequence as output, we can run forward id136 over the input sequence and
select the most likely tag from the softmax at each step. since we   re using a softmax
layer to generate the id203 distribution over the output tagset at each timestep,
we   ll rely on the cross id178 loss introduced in chapter 7 to train the network.

a closely related, and extremely useful, application of sequence labeling is to
   nd and classify spans of text corresponding to items of interest in some task do-
main. an example of such a task is id39     the problem of

named entity
recognition

janetwillbackid56thebill9.2

    applications of id56s

185

   nding all the spans in a text that correspond to names of people, places or organi-
zations (a problem we   ll study in gory detail in chapter 17).

to turn a problem like this into a per-word sequence labeling task, we   ll use a
technique called iob encoding (ramshaw and marcus, 1995). in its simplest form,
we   ll label any token that begins a span of interest with the label b, tokens that occur
inside a span are tagged with an i, and any tokens outside of any span of interest are
labeled o. consider the following example:
(9.5) united

cancelled
o

the
o

   ight
o

from
o

denver
b

to
o

san
b

francisco.
i

b

here, the spans of interest are united, denver and san francisco.

in applications where we are interested in more than one class of entity (e.g.,
   nding and distinguishing names of people, locations, or organizations), we can
specialize the b and i tags to represent each of the more speci   c classes, thus ex-
panding the tagset from 3 tags to 2    n + 1 where n is the number of classes we   re
interested in.
(9.6) united
b-org

francisco.
i-loc

cancelled
o

denver
b-loc

san
b-loc

   ight
o

from
o

the
o

to
o

with such an encoding, the inputs are the usual id27s and the output
consistes of a sequence of softmax distributions over the tags at each point in the
sequence.

9.2.3 viterbi and id49 (crfs)
as we saw with applying id28 to part-of-speech tagging, choosing the
maximum id203 label for each element in a sequence does not necessarily re-
sult in an optimal (or even very good) tag sequence. in the case of iob tagging, it
doesn   t even guarantee that the resulting sequence will be well-formed. for exam-
ple, nothing in approach described in the last section prevents an output sequence
from containing an i following an o, even though such a transition is illegal. simi-
larly, when dealing with multiple classes nothing would prevent an i-loc tag from
following a b-per tag.

a simple solution to this problem is to use combine the sequence of id203
distributions provided by the softmax outputs with a tag-level language model as we
did with memms in chapter 8. thereby allowing the use of the viterbi algorithm
to select the most likely tag sequence.
[or a crf layer... coming soon]

9.2.4 id56s for sequence classi   cation
another use of id56s is to classify entire sequences rather than the tokens within
a sequence. we   ve already encountered this task in chapter 4 with our discussion
of id31. other examples include document-level topic classi   cation,
spam detection, message routing for customer service applications, and deception
detection. in all of these applications, sequences of text are classi   ed as belonging
to one of a small number of categories.

to apply id56s in this setting, the hidden layer from the    nal state of the network
is taken to constitute a compressed representation of the entire sequence. this com-
pressed sequence representation can then in turn serve as the input to a feed-forward
network trained to select the correct class. fig. 9.10 illustrates this approach.

186 chapter 9

    sequence processing with recurrent networks

figure 9.9 sequence classi   cation using a simple id56 combined with a feedforward net-
work.

note that in this approach, there are no intermediate outputs for the items in the
sequence preceding the last element, and therefore there are no loss terms associ-
ated with those individual items. instead, the loss used to train the network weights
is based on the loss from the    nal classi   cation task. speci   cally, we use the output
from the softmax layer from the    nal classi   er along with a cross-id178 loss func-
tion to drive our network training. the loss is backpropagated all the way through
the weights in the feedforward classi   er through to its input, and then through to the
three sets of weights in the id56 as described earlier in section 9.1.2. this combina-
tion of a simple recurrent network with a feedforward classi   er is our    rst example
of a deep neural network.

9.3 deep networks: stacked and bidirectional id56s

as suggested by the sequence classi   cation architecture shown in fig. 9.9, recur-
rent networks are in fact quite    exible. combining the feedforward nature of un-
rolled computational graphs with vectors as common inputs and outputs, complex
networks can be treated as modules that can be combined in creative ways. this
section introduces two of the more common network architectures used in language
processing with id56s.

9.3.1 stacked id56s
in our examples thus far, the inputs to our id56s have consisted of sequences of
word or character embeddings (vectors) and the outputs have been vectors useful for
predicting words, tags or sequence labels. however, nothing prevents us from using
the entire sequence of outputs from one id56 as an input sequence to another one.
stacked id56s consist of multiple networks where the output of one layer serves as
the input to a subsequent layer, as shown in fig. 9.10.

it has been demonstrated across numerous tasks that stacked id56s can outper-

stacked id56s

x1x2x3xnid56hnsoftmax9.3

    deep networks: stacked and bidirectional id56s

187

figure 9.10 stacked recurrent networks. the output of a lower level serves as the input to
higher levels with the output of the last network serving as the    nal output.

form single-layer networks. one reason for this success has to do with the networks
ability to induce representations at differing levels of abstraction across layers. just
as the early stages of the human visual system detects edges that are then used for
   nding larger regions and shapes, the initial layers of stacked networks can induce
representations that serve as useful abstractions for further layers     representations
that might prove dif   cult to induce in a single id56.

9.3.2 bidirectional id56s
in an simple recurrent network, the hidden state at a given time t represents every-
thing the network knows about the sequence up to that point in the sequence. that
is, the hidden state at time t is the result of a function of the inputs from the start up
through time t. we can think of this as the context of the network to the left of the
current time.

h f orward
t

= srnf orward(x1 : xt )

where h f orward
everything the network has gleaned from the sequence to that point.

corresponds to the normal hidden state at time t, and represents

t

of course, in text-based applications we have access to the entire input sequence
all at once. we might ask whether its helpful to take advantage of the context to
the right of the current input as well. one way to recover such information is to
train a recurrent network on an input sequence in reverse, using the same kind of
network that we   ve been discussing. with this approach, the hidden state at time t
now represents information about the sequence to the right of the current input.

hbackward
t

= srnbackward(xn : xt )

bidirectional
id56

here, the hidden state hbackward
about the sequence from t to the end of the sequence.

t

represents all the information we have discerned

putting these networks together results in a bidirectional id56. a bi-id56 con-
sists of two independent recurrent networks, one where the input is processed from

y1y2y3ynx1x2x3xnid56 1id56 3id56 2188 chapter 9

    sequence processing with recurrent networks

figure 9.11 a bidirectional id56. separate models are trained in the forward and backward
directions with the output of each model at each time point concatenated to represent the state
of affairs at that point in time. the box wrapped around the forward and backward network
emphasizes the modular nature of this architecture.

the start to the end, and the other from the end to the start. we can then combine the
outputs of the two networks into a single representation that captures the both the
left and right contexts of an input at each point in time.
    hbackward

ht = h f orward

(9.7)

t

t

fig. 9.11 illustrates a bidirectional network where the outputs of the forward and
backward pass are concatenated. other simple ways to combine the forward and
backward contexts include element-wise addition or multiplication. the output at
each step in time thus captures information to the left and to the right of the current
input. in sequence labeling applications, these concatenated outputs can serve as the
basis for a local labeling decision.

bidirectional id56s have also proven to be quite effective for sequence classi-
   cation. recall from fig. 9.10, that for sequence classi   cation we used the    nal
hidden state of the id56 as the input to a subsequent feedforward classi   er. a dif-
   culty with this approach is that the    nal state naturally re   ects more information
about the end of the sentence than its beginning. bidirectional id56s provide a
simple solution to this problem; as shown in fig. 9.12, we simply combine the    nal
hidden states from the forward and backward passes and use that as input for follow-
on processing. again, concatenation is a common approach to combining the two
outputs but element-wise summation, multiplication or averaging are also used.

9.4 managing context in id56s: lstms and grus

in practice, it is quite dif   cult to train simple id56s for tasks that require a network
to make use of information distant from the current point of processing. despite hav-
ing access to the entire preceding sequence, the information encoded in hidden states
tends to be fairly local, more relevant to the most recent parts of the input sequence
and recent decisions. however, it is often the case that long-distance information is
critical to many language applications.

y1x1x2x3xnid56 1 (left to right)id56 2 (right to left)+y2+y3+yn+9.4

    managing context in id56s: lstms and grus

189

figure 9.12 a bidirectional id56 for sequence classi   cation. the    nal hidden units from
the forward and backward passes are combined to represent the entire sequence. this com-
bined representation serves as input to the subsequent classi   er.

consider the following example in the context of language models.

(9.8) the    ights the airline was cancelling were full.
assigning a high id203 to was following airline is straightforward since was
provides a strong local context for the singular agreement. however, assigning an
appropriate id203 to were is quite dif   cult, not only because the plural    ights
is quite distant, but also because the more recent context contains singular con-
stituents. ideally, a network should be able to retain the distant information about
plural    ights until it is needed, all the while processing intermediate parts of the
sequence correctly.

one reason for the inability of srns to carry forward critical information is that
the hidden layer in srns, and, by extension, the weights that determine the values
in the hidden layer, are being asked to perform two tasks simultaneously: provide
information useful to the decision being made in the current context, and updating
and carrying forward information useful for future decisions.

a second dif   culty to successfully training simple recurrent networks arises
from the need to backpropagate training error back in time through the hidden lay-
ers. recall from section 9.1.2 that the hidden layer at time t contributes to the loss
at the next time step since it takes part in that calculation. as a result, during the
backward pass of training, the hidden layers are subject to repeated dot products, as
determined by the length of the sequence. a frequent result of this process is that
the gradients are either driven to zero or saturate. situations that are referred to as
vanishing gradients or exploding gradients, respectively.

to address these issues more complex network architectures have been designed
to explicitly manage the task of maintaining contextual information over time. these
approaches treat context as a kind of memory unit that needs to be managed explic-
itly. more speci   cally, the network needs to forget information that is no longer
needed and to remember information as needed for later decisions.

x1x2x3xnid56 1 (left to right)id56 2 (right to left)+hn_forwh1_backsoftmax190 chapter 9

    sequence processing with recurrent networks

figure 9.13 a single lstm memory unit displayed as a computation graph.

9.4.1 long short-term memory
long short-term memory (lstm) networks, divide the context management prob-
lem into two sub-problems: removing information no longer needed from the con-
text, and adding information likely to be needed for later decision making. the key
to the approach is to learn how to manage this context rather than hard-coding a
strategy into the architecture.

lstms accomplish this through the use of specialized neural units that make use
of gates that control the    ow of information into and out of the units that comprise
the network layers. these gates are implemented through the use of additional sets
of weights that operate sequentially on the context layer.

gt = tanh(ught   1 +wgxt )
it =    (uiht   1 +wixt )
ft =    (uf ht   1 +wf xt )
ot =    (uoht   1 +woxt )
ct = ft (cid:12) ct   1 + it (cid:12) gt
ht = ot (cid:12)tanh(ct )

[more on this]

+ost-1ifgx tht-1yyystht9.4

    managing context in id56s: lstms and grus

191

figure 9.14 basic neural units used in feed-forward, simple recurrent networks (srn),
long short-term memory (lstm) and gate recurrent units.

9.4.2 id149

while relatively easy to deploy, lstms introduce a considerable number of param-
eters to our networks, and hence carry a much larger training burden. gated recur-
rent units (grus) try to ease this burden by collapsing the forget and add gates of
lstms into a single update gate with a single set of weights.

[coming soon]

9.4.3 gated units, layers and networks

the neural units used in lstms and grus are obviously much more complex than
basic feed-forward networks. fortunately, this complexity is largely encapsulated
within the basic processing units, allowing us to maintain modularity and to eas-
ily experiment with different architectures. to see this, consider fig. 9.14 which
illustrates the inputs/outputs and weights associated with each kind of unit.

at the far left, (a) is the basic feed-forward unit h = g(w x + b). a single set of
weights and a single activation function determine its output, and when arranged in
a layer there is no connection between the units in the layer. next, (b) represents the
unit in an srn. now there are two inputs and additional set of weights to go with it.
however, there is still a single activation function and output. when arranged as a
layer the hidden layer from each unit feeds in as an input to the next.

fortunately, the increased complexity of the lstm and gru units is encapsu-
lated within the units themselves. the only additional external complexity over the
basic recurrent unit (b) is the presence of the additional context vector input and out-
put. this modularity is key to the power and widespread applicability of lstm and
gru units. speci   cally, lstm and gru units can be substituted into any of the
network architectures described in section 9.3. and, as with srns, multi-layered
networks making use of gated units can be unrolled into deep feed-forward networks
and trained in the usual fashion with id26.

hxxtxtht-1hthtct-1ctht-1xthtct-1ctht-1(b)(a)(c)(d)   gza   gzlstmunitgruunita192 chapter 9

    sequence processing with recurrent networks

figure 9.15 sequence labeling id56 that accepts distributional id27s aug-
mented with character-level id27s.

9.5 words, characters and byte-pairs

to this point, we   ve assumed that the inputs to our networks would be either pre-
trained or trained id27s. as we   ve seen, word-based embeddings are
great at    nding distributional (syntactic and semantic) similarity between words.
however, there are signi   cant issues with any solely word-based approach:

    for some languages and applications, the lexicon is simply too large to prac-
tically represent every possible word as an embedding. some means of com-
posing words from smaller bits is needed.

    no matter how large the lexicon, we will always encounter unknown words
due to new words entering the language, misspellings and borrowings from
other languages.

    morphological information, below the word level, is clearly an important
source of information for many applications. word-based methods are blind
to such regularities.

we can overcome some of these issues by augmenting our input word repre-
sentations with embeddings derived from the characters that make up the words.
fig. 9.15 illustrates an approach in the context of part-of-speech tagging. the upper
part of the diagram consists of an id56 that accepts an input sequence and outputs
a softmax distribution over the tags for each element of the input. note that this
id56 can be arbitrarily complex, consisting of stacked and/or bidirectional network
layers.

the inputs to this network consist of ordinary id27s enriched with
character information. speci   cally, each input consists of the concatenation of the
normal id27 with embeddings derived from a bidirectional id56 that
accepts the character sequences for each word as input, as shown in the lower part
of the    gure.

the character sequence for each word in the input is run through a bidirectional
id56 consisting of two independent id56s     one that processes the sequence left-

janetid56bi-id56jante+willbi-id56will+      9.6

    summary

193

figure 9.16 bi-id56 accepts word character sequences and emits embeddings derived
from a forward and backward pass over the sequence. the network itself is trained in the
context of a larger end-application where the loss is propagated all the way through to the
character vector embeddings.

to-right and the other right-to-left. as discussed in section ??, the    nal hidden
states of the left-to-right and right-to-left networks are concatenated to represent the
composite character-level representation of each word. critically, these character
embeddings are trained in the context of the overall task; the loss from the part-of-
speech softmax layer is propagated all the way back to the character embeddings.

[more on byte-pair encoding approach]

9.6 summary

    simple recurrent networks
    id136 and training in srns.
    common use cases for id56s

    id38
    sequence labeling
    sequence classi   cation

    lstms and grus
    characters as inputs

janecharacter projection layerlstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2right-to-left lstid113ft-to-right lstmtlstm2lstm1concatenationcharacter-level id27character embeddings194 chapter 10

    formal grammars of english

chapter

10 formal grammars of english

syntax

the study of grammar has an ancient pedigree; panini   s grammar of sanskrit was
written over two thousand years ago and is still referenced today in teaching san-
skrit. despite this history, knowledge of grammar remains spotty at best. in this
chapter, we make a preliminary stab at addressing some of these gaps in our knowl-
edge of grammar and syntax, as well as introducing some of the formal mechanisms
that are available for capturing this knowledge in a computationally useful manner.
the word syntax comes from the greek s  yntaxis, meaning    setting out together
or arrangement   , and refers to the way words are arranged together. we have seen
various syntactic notions in previous chapters. the regular languages introduced
in chapter 2 offered a simple way to represent the ordering of strings of words, and
chapter 3 showed how to compute probabilities for these word sequences. chapter 8
showed that part-of-speech categories could act as a kind of equivalence class for
words. in this chapter and next few we introduce a variety of syntactic phenomena
and models for syntax and grammar that go well beyond these simpler approaches.

the bulk of this chapter is devoted to the topic of context-free grammars. context-
free grammars are the backbone of many formal models of the syntax of natural
language (and, for that matter, of computer languages). as such, they are integral to
many computational applications, including grammar checking, semantic interpreta-
tion, dialogue understanding, and machine translation. they are powerful enough to
express sophisticated relations among the words in a sentence, yet computationally
tractable enough that ef   cient algorithms exist for parsing sentences with them (as
we show in chapter 11). in chapter 12, we show that adding id203 to context-
free grammars gives us a powerful model of disambiguation. and in chapter 15 we
show how they provide a systematic framework for semantic interpretation.

in addition to an introduction to this grammar formalism, this chapter also pro-
vides a brief overview of the grammar of english. to illustrate our grammars, we
have chosen a domain that has relatively simple sentences, the air traf   c informa-
tion system (atis) domain (hemphill et al., 1990). atis systems were an early
example of spoken language systems for helping book airline reservations. users
try to book    ights by conversing with the system, specifying constraints like i   d like
to    y from atlanta to denver.

10.1 constituency

the fundamental notion underlying the idea of constituency is that of abstraction    
groups of words behaving as a single units, or constituents. a signi   cant part of
developing a grammar involves discovering the inventory of constituents present in
the language.

how do words group together in english? consider the noun phrase, a sequence
of words surrounding at least one noun. here are some examples of noun phrases

noun phrase

10.2

    context-free grammars

195

(thanks to damon runyon):

harry the horse
the broadway coppers
they

a high-class spot such as mindy   s
the reason he comes into the hot box
three parties from brooklyn

what evidence do we have that these words group together (or    form constituents   )?

one piece of evidence is that they can all appear in similar syntactic environments,
for example, before a verb.

three parties from brooklyn arrive. . .
a high-class spot such as mindy   s attracts. . .
the broadway coppers love. . .
they sit

but while the whole noun phrase can occur before a verb, this is not true of each
of the individual words that make up a noun phrase. the following are not grammat-
ical sentences of english (recall that we use an asterisk (*) to mark fragments that
are not grammatical english sentences):

*from arrive. . . *as attracts. . .
*the is. . .

*spot sat. . .

preposed
postposed

thus, to correctly describe facts about the ordering of these words in english, we
must be able to say things like    noun phrases can occur before verbs   .

other kinds of evidence for constituency come from what are called preposed or
postposed constructions. for example, the prepositional phrase on september sev-
enteenth can be placed in a number of different locations in the following examples,
including at the beginning (preposed) or at the end (postposed):

on september seventeenth, i   d like to    y from atlanta to denver
i   d like to    y on september seventeenth from atlanta to denver
i   d like to    y from atlanta to denver on september seventeenth

but again, while the entire phrase can be placed differently, the individual words

making up the phrase cannot be

*on september, i   d like to    y seventeenth from atlanta to denver
*on i   d like to    y september seventeenth from atlanta to denver
*i   d like to    y on september from atlanta to denver seventeenth

see radford (1988) for further examples of groups of words behaving as a single

constituent.

10.2 context-free grammars

the most widely used formal system for modeling constituent structure in english
and other natural languages is the context-free grammar, or id18. context-
free grammars are also called phrase-structure grammars, and the formalism
is equivalent to backus-naur form, or bnf. the idea of basing a grammar on
constituent structure dates back to the psychologist wilhelm wundt (1900) but was
not formalized until chomsky (1956) and, independently, backus (1959).

a context-free grammar consists of a set of rules or productions, each of which

id18

rules

196 chapter 10

    formal grammars of english

lexicon
np

expresses the ways that symbols of the language can be grouped and ordered to-
gether, and a lexicon of words and symbols. for example, the following productions
express that an np (or noun phrase) can be composed of either a propernoun or
a determiner (det) followed by a nominal; a nominal in turn can consist of one or
more nouns.

np     det nominal
np     propernoun

nominal     noun | nominal noun

context-free rules can be hierarchically embedded, so we can combine the pre-

vious rules with others, like the following, that express facts about the lexicon:

det     a
det     the
noun        ight

terminal

non-terminal

derivation
parse tree

dominates

start symbol

verb phrase

the symbols that are used in a id18 are divided into two classes. the symbols
that correspond to words in the language (   the   ,    nightclub   ) are called terminal
symbols; the lexicon is the set of rules that introduce these terminal symbols. the
symbols that express abstractions over these terminals are called non-terminals. in
each context-free rule, the item to the right of the arrow (   ) is an ordered list of one
or more terminals and non-terminals; to the left of the arrow is a single non-terminal
symbol expressing some cluster or generalization. notice that in the lexicon, the
non-terminal associated with each word is its lexical category, or part-of-speech,
which we de   ned in chapter 8.

a id18 can be thought of in two ways: as a device for generating sentences
and as a device for assigning a structure to a given sentence. viewing a id18 as a
generator, we can read the     arrow as    rewrite the symbol on the left with the string
of symbols on the right   .
so starting from the symbol:
we can use our    rst rule to rewrite np as:
and then rewrite nominal as:
and    nally rewrite these parts-of-speech as:

np
det nominal
det noun
a    ight

we say the string a    ight can be derived from the non-terminal np. thus, a id18
can be used to generate a set of strings. this sequence of rule expansions is called a
derivation of the string of words. it is common to represent a derivation by a parse
tree (commonly shown inverted with the root at the top). figure 10.1 shows the tree
representation of this derivation.

in the parse tree shown in fig. 10.1, we can say that the node np dominates
all the nodes in the tree (det, nom, noun, a,    ight). we can say further that it
immediately dominates the nodes det and nom.

the formal language de   ned by a id18 is the set of strings that are derivable
from the designated start symbol. each grammar must have one designated start
symbol, which is often called s. since context-free grammars are often used to de   ne
sentences, s is usually interpreted as the    sentence    node, and the set of strings that
are derivable from s is the set of sentences in some simpli   ed version of english.

let   s add a few additional rules to our inventory. the following rule expresses

the fact that a sentence can consist of a noun phrase followed by a verb phrase:

s     np vp i prefer a morning    ight

10.2

    context-free grammars

197

np

det

nom

a

noun

   ight

figure 10.1 a parse tree for    a    ight   .

a verb phrase in english consists of a verb followed by assorted other things;
for example, one kind of verb phrase consists of a verb followed by a noun phrase:

vp     verb np prefer a morning    ight

or the verb may be followed by a noun phrase and a prepositional phrase:

vp     verb np pp leave boston in the morning

or the verb phrase may have a verb followed by a prepositional phrase alone:

vp     verb pp leaving on thursday

a prepositional phrase generally has a preposition followed by a noun phrase.
for example, a common type of prepositional phrase in the atis corpus is used to
indicate location or direction:

pp     preposition np from los angeles

the np inside a pp need not be a location; pps are often used with times and
dates, and with other nouns as well; they can be arbitrarily complex. here are ten
examples from the atis corpus:

on these    ights
about the ground transportation in chicago
of the round trip    ight on united airlines
of the ap    fty seven    ight
with a stopover in nashville

to seattle
in minneapolis
on wednesday
in the evening
on the ninth of july
figure 10.2 gives a sample lexicon, and fig. 10.3 summarizes the grammar rules
we   ve seen so far, which we   ll call l0. note that we can use the or-symbol | to
indicate that a non-terminal has alternate possible expansions.
we can use this grammar to generate sentences of this    atis-language   . we
start with s, expand it to np vp, then choose a random expansion of np (let   s say, to
i), and a random expansion of vp (let   s say, to verb np), and so on until we generate
the string i prefer a morning    ight. figure 10.4 shows a parse tree that represents a
complete derivation of i prefer a morning    ight.

it is sometimes convenient to represent a parse tree in a more compact format
called bracketed notation; here is the bracketed representation of the parse tree of
fig. 10.4:
(10.1)

[s [np [pro i]] [vp [v prefer] [np [det a] [nom [n morning] [nom [n    ight]]]]]]

bracketed
notation

198 chapter 10

    formal grammars of english

noun        ights | breeze | trip | morning
verb     is | prefer | like | need | want |    y
adjective     cheapest | non-stop |    rst | latest
| other | direct
pronoun     me | i | you | it

proper-noun     alaska | baltimore | los angeles
| chicago | united | american
determiner     the | a | an | this | these | that
preposition     from | to | on | near
conjunction     and | or | but

figure 10.2 the lexicon for l0.

grammar rules
s     np vp
np     pronoun

| proper-noun
| det nominal
| noun
vp     verb

| verb np
| verb np pp
| verb pp

examples

i + want a morning    ight

i
los angeles
a +    ight

do
want + a    ight
leave + boston + in the morning
leaving + on thursday

nominal     nominal noun morning +    ight

   ights

pp     preposition np from + los angeles

figure 10.3 the grammar for l0, with example phrases for each rule.

a id18 like that of l0 de   nes a formal language. we saw in chapter 2 that a for-
mal language is a set of strings. sentences (strings of words) that can be derived by a
grammar are in the formal language de   ned by that grammar, and are called gram-
matical sentences. sentences that cannot be derived by a given formal grammar are
not in the language de   ned by that grammar and are referred to as ungrammatical.
this hard line between    in    and    out    characterizes all formal languages but is only
a very simpli   ed model of how natural languages really work. this is because de-
termining whether a given sentence is part of a given natural language (say, english)
often depends on the context. in linguistics, the use of formal languages to model
natural languages is called generative grammar since the language is de   ned by
the set of possible sentences    generated    by the grammar.

grammatical
ungrammatical

generative
grammar

10.2.1 formal de   nition of context-free grammar

we conclude this section with a quick, formal description of a context-free gram-
mar and the language it generates. a context-free grammar g is de   ned by four
parameters: n,   , r, s (technically this is a    4-tuple   ).

10.2

    context-free grammars

199

s

np

vp

pro

verb

np

i

prefer

det

nom

a

nom

noun

noun

   ight

morning

figure 10.4 the parse tree for    i prefer a morning    ight    according to grammar l0.

n a set of non-terminal symbols (or variables)
   a set of terminal symbols (disjoint from n)
r a set of rules or productions, each of the form a        ,

where a is a non-terminal,
   is a string of symbols from the in   nite set of strings (      n)   

s a designated start symbol and a member of n

for the remainder of the book we adhere to the following conventions when dis-
cussing the formal properties of context-free grammars (as opposed to explaining
particular facts about english or other languages).

capital letters like a, b, and s
s
lower-case greek letters like   ,    , and   
lower-case roman letters like u, v, and w

non-terminals
the start symbol
strings drawn from (      n)   
strings of terminals

a language is de   ned through the concept of derivation. one string derives an-
other one if it can be rewritten as the second one by some series of rule applications.
more formally, following hopcroft and ullman (1979),

if a        is a production of r and    and    are any strings in the set
(      n)   , then we say that   a   directly derives       , or   a             .

derivation is then a generalization of direct derivation:

let   1,   2, . . . ,   m be strings in (      n)   ,m     1, such that

  1       2,  2       3, . . . ,  m   1       m

we say that   1 derives   m, or   1          m.
we can then formally de   ne the language lg generated by a grammar g as the
set of strings composed of terminal symbols that can be derived from the designated

directly derives

derives

200 chapter 10

    formal grammars of english

start symbol s.

lg = {w|w is in       and s        w}

syntactic
parsing

the problem of mapping from a string of words to its parse tree is called syn-

tactic parsing; we de   ne algorithms for parsing in chapter 11.

10.3 some grammar rules for english

in this section, we introduce a few more aspects of the phrase structure of english;
for consistency we will continue to focus on sentences from the atis domain. be-
cause of space limitations, our discussion is necessarily limited to highlights. read-
ers are strongly advised to consult a good reference grammar of english, such as
huddleston and pullum (2002).

10.3.1 sentence-level constructions
in the small grammar l0, we provided only one sentence-level construction for
declarative sentences like i prefer a morning    ight. among the large number of
constructions for english sentences, four are particularly common and important:
declaratives, imperatives, yes-no questions, and wh-questions.

sentences with declarative structure have a subject noun phrase followed by
a verb phrase, like    i prefer a morning    ight   . sentences with this structure have
a great number of different uses that we follow up on in chapter 24. here are a
number of examples from the atis domain:
i want a    ight from ontario to chicago
the    ight should be eleven a.m. tomorrow
the return    ight should leave at around seven p.m.

sentences with imperative structure often begin with a verb phrase and have
no subject. they are called imperative because they are almost always used for
commands and suggestions; in the atis domain they are commands to the system.

show the lowest fare
give me sunday   s    ights arriving in las vegas from new york city
list all    ights between    ve and seven p.m.

we can model this sentence structure with another rule for the expansion of s:

declarative

imperative

s     vp

yes-no question

sentences with yes-no question structure are often (though not always) used to
ask questions; they begin with an auxiliary verb, followed by a subject np, followed
by a vp. here are some examples. note that the third example is not a question at
all but a request; chapter 24 discusses the uses of these question forms to perform
different pragmatic functions such as asking, requesting, or suggesting.

do any of these    ights have stops?
does american   s    ight eighteen twenty    ve serve dinner?
can you give me the same information for united?

here   s the rule:

s     aux np vp

10.3

    some grammar rules for english

201

wh-phrase
wh-word

the most complex sentence-level structures we examine here are the various wh-
structures. these are so named because one of their constituents is a wh-phrase, that
is, one that includes a wh-word (who, whose, when, where, what, which, how, why).
these may be broadly grouped into two classes of sentence-level structures. the
wh-subject-question structure is identical to the declarative structure, except that
the    rst noun phrase contains some wh-word.

what airlines    y from burbank to denver?
which    ights depart burbank after noon and arrive in denver by six p.m?
whose    ights serve breakfast?

here is a rule. exercise 10.7 discusses rules for the constituents that make up the
wh-np.

s     wh-np vp

wh-non-subject-
question

in the wh-non-subject-question structure, the wh-phrase is not the subject of the
sentence, and so the sentence includes another subject. in these types of sentences
the auxiliary appears before the subject np, just as in the yes-no question structures.
here is an example followed by a sample rule:

what    ights do you have from burbank to tacoma washington?

long-distance
dependencies

clause

s     wh-np aux np vp

constructions like the wh-non-subject-question contain what are called long-
distance dependencies because the wh-np what    ights is far away from the predi-
cate that it is semantically related to, the main verb have in the vp. in some models
of parsing and understanding compatible with the grammar rule above, long-distance
dependencies like the relation between    ights and have are thought of as a semantic
relation. in such models, the job of    guring out that    ights is the argument of have
is done during semantic interpretation. in other models of parsing, the relationship
between    ights and have is considered to be a syntactic relation, and the grammar is
modi   ed to insert a small marker called a trace or empty category after the verb.
we return to such empty-category models when we introduce the id32 on
page 208.

10.3.2 clauses and sentences
before we move on, we should clarify the status of the s rules in the grammars we
just described. s rules are intended to account for entire sentences that stand alone
as fundamental units of discourse. however, s can also occur on the right-hand side
of grammar rules and hence can be embedded within larger sentences. clearly then,
there   s more to being an s than just standing alone as a unit of discourse.

what differentiates sentence constructions (i.e., the s rules) from the rest of the
grammar is the notion that they are in some sense complete. in this way they corre-
spond to the notion of a clause, which traditional grammars often describe as form-
ing a complete thought. one way of making this notion of    complete thought    more
precise is to say an s is a node of the parse tree below which the main verb of the s
has all of its arguments. we de   ne verbal arguments later, but for now let   s just see
an illustration from the tree for i prefer a morning    ight in fig. 10.4 on page 199.
the verb prefer has two arguments: the subject i and the object a morning    ight.
one of the arguments appears below the vp node, but the other one, the subject np,
appears only below the s node.

202 chapter 10

    formal grammars of english

10.3.3 the noun phrase
our l0 grammar introduced three of the most frequent types of noun phrases that
occur in english: pronouns, proper nouns and the np     det nominal construction.
the central focus of this section is on the last type since that is where the bulk of
the syntactic complexity resides. these noun phrases consist of a head, the central
noun in the noun phrase, along with various modi   ers that can occur before or after
the head noun. let   s take a close look at the various parts.

the determiner
noun phrases can begin with simple lexical determiners, as in the following exam-
ples:

a stop
those    ights

the    ights
any    ights

this    ight
some    ights

the role of the determiner in english noun phrases can also be    lled by more

complex expressions, as follows:

united   s    ight
united   s pilot   s union
denver   s mayor   s mother   s canceled    ight

in these examples, the role of the determiner is    lled by a possessive expression
consisting of a noun phrase followed by an    s as a possessive marker, as in the
following rule.

det     np (cid:48)s

the fact that this rule is recursive (since an np can start with a det) helps us
model the last two examples above, in which a sequence of possessive expressions
serves as a determiner.

under some circumstances determiners are optional in english. for example,

determiners may be omitted if the noun they modify is plural:

(10.2) show me    ights from san francisco to denver on weekdays
as we saw in chapter 8, mass nouns also don   t require determination. recall that
mass nouns often (not always) involve something that is treated like a substance
(including e.g., water and snow), don   t take the inde   nite article    a   , and don   t tend
to pluralize. many abstract nouns are mass nouns (music, homework). mass nouns
in the atis domain include breakfast, lunch, and dinner:

(10.3) does this    ight serve dinner?

the nominal
the nominal construction follows the determiner and contains any pre- and post-
head noun modi   ers. as indicated in grammar l0, in its simplest form a nominal
can consist of a single noun.

nominal     noun

as we   ll see, this rule also provides the basis for the bottom of various recursive
rules used to capture more complex nominal constructions.

10.3

    some grammar rules for english

203

cardinal
numbers

ordinal
numbers
quanti   ers

before the head noun
a number of different kinds of word classes can appear before the head noun (the
   postdeterminers   ) in a nominal. these include cardinal numbers, ordinal num-
bers, quanti   ers, and adjectives. examples of cardinal numbers:

two friends

one stop

ordinal numbers include    rst, second, third, and so on, but also words like next,

last, past, other, and another:

the    rst one
the last    ight

the next day
the other american    ight

the second leg

some quanti   ers (many, (a) few, several) occur only with plural count nouns:

many fares

adjectives occur after quanti   ers but before nouns.
a non-stop    ight
the earliest lunch    ight

a    rst-class fare
the longest layover
adjectives can also be grouped into a phrase called an adjective phrase or ap.
aps can have an adverb before the adjective (see chapter 8 for de   nitions of adjec-
tives and adverbs):

adjective
phrase

the least expensive fare

after the head noun
a head noun can be followed by postmodi   ers. three kinds of nominal postmodi-
   ers are common in english:

prepositional phrases
non-   nite clauses
relative clauses
common in the atis corpus since they are used to mark the origin and destina-

all    ights from cleveland
any    ights arriving after eleven a.m.
a    ight that serves breakfast

tion of    ights.

here are some examples of prepositional phrase postmodi   ers, with brackets
inserted to show the boundaries of each pp; note that two or more pps can be strung
together within a single np:

all    ights [from cleveland] [to newark]
arrival [in san jose] [before seven p.m.]
a reservation [on    ight six oh six] [from tampa] [to montreal]

here   s a new nominal rule to account for postnominal pps:

nominal     nominal pp

non-   nite

gerundive

the three most common kinds of non-   nite postmodi   ers are the gerundive (-

ing), -ed, and in   nitive forms.

gerundive postmodi   ers are so called because they consist of a verb phrase that

begins with the gerundive (-ing) form of the verb. here are some examples:

any of those [leaving on thursday]
any    ights [arriving after eleven a.m.]
   ights [arriving within thirty minutes of each other]

204 chapter 10

    formal grammars of english

we can de   ne the nominals with gerundive modi   ers as follows, making use of

a new non-terminal gerundvp:

nominal     nominal gerundvp

we can make rules for gerundvp constituents by duplicating all of our vp pro-

ductions, substituting gerundv for v.

gerundvp     gerundv np

| gerundv pp | gerundv | gerundv np pp

gerundv can then be de   ned as

gerundv     being | arriving | leaving | . . .

the phrases in italics below are examples of the two other common kinds of

non-   nite clauses, in   nitives and -ed forms:

the last    ight to arrive in boston
i need to have dinner served
which is the aircraft used by this    ight?

relative
pronoun

a postnominal relative clause (more correctly a restrictive relative clause), is
a clause that often begins with a relative pronoun (that and who are the most com-
mon). the relative pronoun functions as the subject of the embedded verb in the
following examples:

a    ight that serves breakfast
   ights that leave in the morning
the one that leaves at ten thirty    ve

we might add rules like the following to deal with these:
nominal     nominal relclause
relclause     (who | that) vp

the relative pronoun may also function as the object of the embedded verb, as
in the following example; we leave for the reader the exercise of writing grammar
rules for more complex relative clauses of this kind.

the earliest american airlines    ight that i can get

various postnominal modi   ers can be combined, as the following examples

show:

a    ight [from phoenix to detroit] [leaving monday evening]
evening    ights [from nashville to houston] [that serve dinner]
a friend [living in denver] [that would like to visit me here in washington dc]

predeterminers

before the noun phrase
word classes that modify and appear before nps are called predeterminers. many
of these have to do with number or amount; a common predeterminer is all:

all the    ights
the example noun phrase given in fig. 10.5 illustrates some of the complexity

all non-stop    ights

all    ights

that arises when these rules are combined.

10.3

    some grammar rules for english

205

np

predet

np

all

det

the

nom

nom

gerundivevp

nom

pp

leaving before 10

nom

pp

to tampa

nom

noun

from denver

noun

   ights

morning

figure 10.5 a parse tree for    all the morning    ights from denver to tampa leaving before 10   .

10.3.4 the verb phrase
in the
the verb phrase consists of the verb and a number of other constituents.
simple rules we have built so far, these other constituents include nps and pps and
combinations of the two:

disappear

vp     verb
vp     verb np prefer a morning    ight
vp     verb np pp leave boston in the morning
vp     verb pp leaving on thursday

verb phrases can be signi   cantly more complicated than this. many other kinds
of constituents, such as an entire embedded sentence, can follow the verb. these are
called sentential complements:

sentential
complements

you [vp [v said [s you had a two hundred sixty six dollar fare]]
[vp [v tell] [np me] [s how to get from the airport in philadelphia to down-
town]]
i [vp [v think [s i would like to take the nine thirty    ight]]

here   s a rule for these:

vp     verb s

similarly, another potential constituent of the vp is another vp. this is often the

case for verbs like want, would like, try, intend, need:
i want [vp to    y from milwaukee to orlando]
hi, i want [vp to arrange three    ights]

206 chapter 10

    formal grammars of english

frame
/0
np
np np
ppfrom ppto
np ppwith
vpto
vpbrst
s

verb
eat, sleep
prefer,    nd, leave
show, give
   y, travel
help, load
prefer, want, need
can, would, might
mean

example
i ate
find [np the    ight from pittsburgh to boston]
show [np me] [np airlines with    ights from pittsburgh]
i would like to    y [pp from boston] [pp to philadelphia]
can you help [np me] [pp with a    ight]
i would prefer [vpto to go by united airlines]
i can [vpbrst go from boston]
does this mean [s aa has a hub in boston]

figure 10.6 subcategorization frames for a set of example verbs.

transitive
intransitive

subcategorize

subcategorizes
for

complements

subcategorization

frame

while a verb phrase can have many possible kinds of constituents, not every
verb is compatible with every verb phrase. for example, the verb want can be used
either with an np complement (i want a    ight . . . ) or with an in   nitive vp comple-
ment (i want to    y to . . . ). by contrast, a verb like    nd cannot take this sort of vp
complement (* i found to    y to dallas).

this idea that verbs are compatible with different kinds of complements is a very
old one; traditional grammar distinguishes between transitive verbs like    nd, which
take a direct object np (i found a    ight), and intransitive verbs like disappear,
which do not (*i disappeared a    ight).

where traditional grammars subcategorize verbs into these two categories (tran-
sitive and intransitive), modern grammars distinguish as many as 100 subcategories.
we say that a verb like    nd subcategorizes for an np, and a verb like want sub-
categorizes for either an np or a non-   nite vp. we also call these constituents the
complements of the verb (hence our use of the term sentential complement above).
so we say that want can take a vp complement. these possible sets of complements
are called the subcategorization frame for the verb. another way of talking about
the relation between the verb and these other constituents is to think of the verb as
a logical predicate and the constituents as logical arguments of the predicate. so we
can think of such predicate-argument relations as find(i, a flight) or want(i, to
fly). we talk more about this view of verbs and arguments in chapter 14 when we
talk about predicate calculus representations of verb semantics. subcategorization
frames for a set of example verbs are given in fig. 10.6.

we can capture the association between verbs and their complements by making
separate subtypes of the class verb (e.g., verb-with-np-complement, verb-with-inf-
vp-complement, verb-with-s-complement, and so on):

verb-with-np-complement        nd | leave | repeat | . . .
verb-with-s-complement     think | believe | say | . . .

verb-with-inf-vp-complement     want | try | need | . . .

each vp rule could then be modi   ed to require the appropriate verb subtype:

vp     verb-with-no-complement
vp     verb-with-np-comp np prefer a morning    ight
vp     verb-with-s-comp s said there were two    ights

disappear

a problem with this approach is the signi   cant increase in the number of rules

and the associated loss of generality.

conjunctions
coordinate

10.3.5 coordination
the major phrase types discussed here can be conjoined with conjunctions like and,
or, and but to form larger constructions of the same type. for example, a coordinate
noun phrase can consist of two other noun phrases separated by a conjunction:

10.4

    treebanks

207

please repeat [np [np the    ights] and [np the costs]]
i need to know [np [np the aircraft] and [np the    ight number]]

here   s a rule that allows these structures:

np     np and np

note that the ability to form coordinate phrases through conjunctions is often
used as a test for constituency. consider the following examples, which differ from
the ones given above in that they lack the second determiner.

please repeat the [nom [nom    ights] and [nom costs]]
i need to know the [nom [nom aircraft] and [nom    ight number]]

the fact that these phrases can be conjoined is evidence for the presence of the
underlying nominal constituent we have been making use of. here   s a new rule for
this:

nominal     nominal and nominal

the following examples illustrate conjunctions involving vps and ss.

what    ights do you have [vp [vp leaving denver] and [vp arriving in
san francisco]]
[s [s i   m interested in a    ight from dallas to washington] and [s i   m
also interested in going to baltimore]]

the rules for vp and s conjunctions mirror the np one given above.

vp     vp and vp
s     s and s

metarules

since all the major phrase types can be conjoined in this fashion, it is also pos-
sible to represent this conjunction fact more generally; a number of grammar for-
malisms such as gpsg ((gazdar et al., 1985)) do this using metarules such as the
following:

x     x and x

this metarule simply states that any non-terminal can be conjoined with the same
non-terminal to yield a constituent of the same type. of course, the variable x
must be designated as a variable that stands for any non-terminal rather than a non-
terminal itself.

10.4 treebanks

suf   ciently robust grammars consisting of context-free grammar rules can be used
to assign a parse tree to any sentence. this means that it is possible to build a
corpus where every sentence in the collection is paired with a corresponding parse

208 chapter 10

    formal grammars of english

treebank

id32

tree. such a syntactically annotated corpus is called a treebank. treebanks play
an important role in parsing, as we discuss in chapter 11, as well as in linguistic
investigations of syntactic phenomena.

a wide variety of treebanks have been created, generally through the use of
parsers (of the sort described in the next few chapters) to automatically parse each
sentence, followed by the use of humans (linguists) to hand-correct the parses. the
id32 project (whose pos tagset we introduced in chapter 8) has pro-
duced treebanks from the brown, switchboard, atis, and wall street journal cor-
pora of english, as well as treebanks in arabic and chinese. a number of treebanks
use the dependency representation we will introduce in chapter 13, including many
that are part of the universal dependencies project (nivre et al., 2016b).

10.4.1 example: the id32 project
figure 10.7 shows sentences from the brown and atis portions of the penn tree-
bank.1 note the formatting differences for the part-of-speech tags; such small dif-
ferences are common and must be dealt with in processing treebanks. the penn
treebank part-of-speech tagset was de   ned in chapter 8. the use of lisp-style
parenthesized notation for trees is extremely common and resembles the bracketed
notation we saw earlier in (10.1). for those who are not familiar with it we show a
standard node-and-line tree representation in fig. 10.8.

((s

(np-sbj (dt that)
(jj cold) (, ,)
(jj empty) (nn sky) )

(vp (vbd was)

(adjp-prd (jj full)

(pp (in of)

(np (nn fire)

(cc and)
(nn light) ))))

(. .) ))

(a)

((s

(np-sbj the/dt flight/nn )
(vp should/md

(vp arrive/vb

(pp-tmp at/in

(np eleven/cd a.m/rb ))
(np-tmp tomorrow/nn )))))

(b)

traces
syntactic
movement

figure 10.7 parsed sentences from the ldc treebank3 version of the brown (a) and atis
(b) corpora.

figure 10.9 shows a tree from the wall street journal. this tree shows an-
the use of traces (-none- nodes) to mark
other feature of the id32s:
long-distance dependencies or syntactic movement. for example, quotations often
follow a quotative verb like say. but in this example, the quotation    we would have
to wait until we have collected on those assets    precedes the words he said. an
empty s containing only the node -none- marks the position after said where the
quotation sentence often occurs. this empty node is marked (in treebanks ii and
iii) with the index 2, as is the quotation s at the beginning of the sentence. such
co-indexing may make it easier for some parsers to recover the fact that this fronted
or topicalized quotation is the complement of the verb said. a similar -none- node

1 the id32 project released treebanks in multiple languages and in various stages; for ex-
ample, there were treebank i (marcus et al., 1993), treebank ii (marcus et al., 1994), and treebank iii
releases of english treebanks. we use treebank iii for our examples.

10.4

    treebanks

209

np-sbj

s

vp

dt

jj

that

cold

,

,

jj

nn

vbd

adjp-prd

empty

sky

was

jj

pp

.

.

full

in

np

of

nn

cc

nn

   re

and

light

figure 10.8 the tree corresponding to the brown corpus sentence in the previous    gure.

marks the fact that there is no syntactic subject right before the verb to wait; instead,
the subject is the earlier np we. again, they are both co-indexed with the index 1.

( (s (             )
(s-tpc-2

(np-sbj-1 (prp we) )
(vp (md would)

(vp (vb have)

(s

(np-sbj (-none- *-1) )
(vp (to to)

(vp (vb wait)

(sbar-tmp (in until)

(s

(np-sbj (prp we) )
(vp (vbp have)

(vp (vbn collected)

(pp-clr (in on)

(np (dt those)(nns assets)))))))))))))

(, ,) (             )
(np-sbj (prp he) )
(vp (vbd said)

(s (-none- *t*-2) ))

(. .) ))

figure 10.9 a sentence from the wall street journal portion of the ldc id32.
note the use of the empty -none- nodes.

the id32 ii and treebank iii releases added further information to
make it easier to recover the relationships between predicates and arguments. cer-

210 chapter 10

    formal grammars of english

lexicon

prp     we | he
dt     the | that | those
jj     cold | empty | full
nn     sky |    re | light |    ight | tomorrow
nns     assets
cc     and
in     of | at | until | on
cd     eleven
rb     a.m.
vb     arrive | have | wait
vbd     was | said
vbp     have
vbn     collected
md     should | would
to     to

grammar
s     np vp .
s     np vp
s         s     , np vp .
s     -none-
np     dt nn
np     dt nns
np     nn cc nn
np     cd rb
np     dt jj , jj nn
np     prp
np     -none-
vp     md vp
vp     vbd adjp
vp     vbd s
vp     vbn pp
vp     vb s
vp     vb sbar
vp     vbp vp
vp     vbn pp
vp     to vp
sbar     in s
adjp     jj pp
pp     in np

figure 10.10 a sample of the id18 grammar rules and lexical entries that would be ex-
tracted from the three treebank sentences in fig. 10.7 and fig. 10.9.

tain phrases were marked with tags indicating the grammatical function of the phrase
(as surface subject, logical topic, cleft, non-vp predicates) its presence in particular
text categories (headlines, titles), and its semantic function (temporal phrases, lo-
cations) (marcus et al. 1994, bies et al. 1995). figure 10.9 shows examples of the
-sbj (surface subject) and -tmp (temporal phrase) tags. figure 10.8 shows in addi-
tion the -prd tag, which is used for predicates that are not vps (the one in fig. 10.8
is an adjp). we   ll return to the topic of grammatical function when we consider
dependency grammars and parsing in chapter 13.

10.4.2 treebanks as grammars
the sentences in a treebank implicitly constitute a grammar of the language repre-
sented by the corpus being annotated. for example, from the three parsed sentences
in fig. 10.7 and fig. 10.9, we can extract each of the id18 rules in them. for sim-
plicity, let   s strip off the rule suf   xes (-sbj and so on). the resulting grammar is
shown in fig. 10.10.

the grammar used to parse the id32 is relatively    at, resulting in very
many and very long rules. for example, among the approximately 4,500 different
rules for expanding vps are separate rules for pp sequences of any length and every
possible arrangement of verb arguments:

vp     vbd pp
vp     vbd pp pp
vp     vbd pp pp pp
vp     vbd pp pp pp pp
vp     vb advp pp
vp     vb pp advp
vp     advp vb pp

10.4

    treebanks

211

as well as even longer rules, such as

vp     vbp pp pp pp pp pp advp pp

which comes from the vp marked in italics:

this mostly happens because we go from football in the fall to lifting in the
winter to football again in the spring.

some of the many thousands of np rules include

np     dt jj nn
np     dt jj nns
np     dt jj nn nn
np     dt jj jj nn
np     dt jj cd nns
np     rb dt jj nn nn
np     rb dt jj jj nns
np     dt jj jj nnp nns
np     dt nnp nnp nnp nnp jj nn
np     dt jj nnp cc jj jj nn nns
np     rb dt jjs nn nn sbar
np     dt vbg jj nnp nnp cc nnp
np     dt jj nns , nns cc nn nns nn
np     dt jj jj vbg nn nnp nnp fw nnp
np     np jj , jj        sbar        nns

the last two of those rules, for example, come from the following two noun phrases:

[dt the] [jj state-owned] [jj industrial] [vbg holding] [nn company] [nnp instituto]
[nnp nacional] [fw de] [nnp industria]
[np shearson   s] [jj easy-to-   lm], [jj black-and-white]    [sbar where we stand]   
[nns commercials]

viewed as a large grammar in this way, the id32 iii wall street journal
corpus, which contains about 1 million words, also has about 1 million non-lexical
rule tokens, consisting of about 17,500 distinct rule types.

various facts about the treebank grammars, such as their large numbers of    at
rules, pose problems for probabilistic parsing algorithms. for this reason, it is com-
mon to make various modi   cations to a grammar extracted from a treebank. we
discuss these further in chapter 12.

10.4.3 heads and head finding
we suggested informally earlier that syntactic constituents could be associated with
a lexical head; n is the head of an np, v is the head of a vp. this idea of a head for
each constituent dates back to bloom   eld (1914). it is central to constituent-based
grammar formalisms such as head-driven phrase structure grammar (pollard and
sag, 1994), as well as the dependency-based approaches to grammar we   ll discuss
in chapter 13. heads and head-dependent relations have also come to play a central
role in computational linguistics with their use in probabilistic parsing (chapter 12)
and in id33 (chapter 13).

in one simple model of lexical heads, each context-free rule is associated with
a head (charniak 1997, collins 1999). the head is the word in the phrase that is
grammatically the most important. heads are passed up the parse tree; thus, each
non-terminal in a parse tree is annotated with a single word, which is its lexical head.

212 chapter 10

    formal grammars of english

s(dumped)

np(workers)

vp(dumped)

nns(workers)

vbd(dumped)

np(sacks)

pp(into)

workers

dumped

nns(sacks)

p

np(bin)

sacks

into

dt(a)

nn(bin)

a

bin

figure 10.11 a lexicalized tree from collins (1999).

figure 10.11 shows an example of such a tree from collins (1999), in which each
non-terminal is annotated with its head.

for the generation of such a tree, each id18 rule must be augmented to identify
one right-side constituent to be the head daughter. the headword for a node is
then set to the headword of its head daughter. choosing these head daughters is
simple for textbook examples (nn is the head of np) but is complicated and indeed
controversial for most phrases. (should the complementizer to or the verb be the
head of an in   nite verb-phrase?) modern linguistic theories of syntax generally
include a component that de   nes heads (see, e.g., (pollard and sag, 1994)).

an alternative approach to    nding a head is used in most practical computational
systems. instead of specifying head rules in the grammar itself, heads are identi   ed
dynamically in the context of trees for speci   c sentences.
in other words, once
a sentence is parsed, the resulting tree is walked to decorate each node with the
appropriate head. most current systems rely on a simple set of hand-written rules,
such as a practical one for id32 grammars given in collins (1999) but
developed originally by magerman (1995). for example, the rule for    nding the
head of an np is as follows (collins, 1999, p. 238):

or jjr.

    if the last word is tagged pos, return last-word.
    else search from right to left for the    rst child which is an nn, nnp, nnps, nx, pos,
    else search from left to right for the    rst child which is an np.
    else search from right to left for the    rst child which is a $, adjp, or prn.
    else search from right to left for the    rst child which is a cd.
    else search from right to left for the    rst child which is a jj, jjs, rb or qp.
    else return the last word

selected other rules from this set are shown in fig. 10.12. for example, for vp
rules of the form vp     y1        yn, the algorithm would start from the left of y1       
yn looking for the    rst yi of type to; if no tos are found, it would search for the
   rst yi of type vbd; if no vbds are found, it would search for a vbn, and so on.
see collins (1999) for more details.

10.5

    grammar equivalence and normal form 213

parent direction
adjp

left

priority list
nns qp nn $ advp jj vbn vbg adjp jjr np jjs dt fw rbr rbs
sbar rb
rb rbr rbs fw advp to cd jjr jj in np jjs nn

right
left
right
left
left
left
left

advp
prn
prt
qp
s
sbar
vp
figure 10.12 selected head rules from collins (1999). the set of head rules is often called a head percola-
tion table.
10.5 grammar equivalence and normal form

rp
$ in nns nn jj rb dt cd ncd qp jjr jjs
to in vp s sbar adjp ucp np
whnp whpp whadvp whadjp in dt s sq sinv sbar frag
to vbd vbn md vbz vb vbg vbp vp adjp nn nns np

normal form

chomsky
normal form

binary
branching

a formal language is de   ned as a (possibly in   nite) set of strings of words. this
suggests that we could ask if two grammars are equivalent by asking if they gener-
ate the same set of strings. in fact, it is possible to have two distinct context-free
grammars generate the same language.

we usually distinguish two kinds of grammar equivalence: weak equivalence
and strong equivalence. two grammars are strongly equivalent if they generate the
same set of strings and if they assign the same phrase structure to each sentence
(allowing merely for renaming of the non-terminal symbols). two grammars are
weakly equivalent if they generate the same set of strings but do not assign the same
phrase structure to each sentence.

it is sometimes useful to have a normal form for grammars, in which each of
the productions takes a particular form. for example, a context-free grammar is in
chomsky normal form (cnf) (chomsky, 1963) if it is  -free and if in addition
each production is either of the form a     b c or a     a. that is, the right-hand side
of each rule either has two non-terminal symbols or one terminal symbol. chomsky
normal form grammars are binary branching, that is they have binary trees (down
to the prelexical nodes). we make use of this binary branching property in the cky
parsing algorithm in chapter 11.

any context-free grammar can be converted into a weakly equivalent chomsky

normal form grammar. for example, a rule of the form

a     b c d

can be converted into the following two cnf rules (exercise 10.8 asks the reader to
formulate the complete algorithm):

a     b x
x     c d

sometimes using binary branching can actually produce smaller grammars. for

example, the sentences that might be characterized as

vp -> vbd np pp*

are represented in the id32 by this series of rules:

vp     vbd np pp
vp     vbd np pp pp

214 chapter 10

    formal grammars of english

vp     vbd np pp pp pp
vp     vbd np pp pp pp pp
...

but could also be generated by the following two-rule grammar:

vp     vbd np pp
vp     vp pp

the generation of a symbol a with a potentially in   nite sequence of symbols b with
a rule of the form a     a b is known as chomsky-adjunction.

chomsky-
adjunction

10.6 lexicalized grammars

the approach to grammar presented thus far emphasizes phrase-structure rules while
minimizing the role of the lexicon. however, as we saw in the discussions of
agreement, subcategorization, and long distance dependencies, this approach leads
to solutions that are cumbersome at best, yielding grammars that are redundant,
hard to manage, and brittle. to overcome these issues, numerous alternative ap-
proaches have been developed that all share the common theme of making bet-
ter use of the lexicon. among the more computationally relevant approaches are
lexical-functional grammar (lfg) (bresnan, 1982), head-driven phrase structure
grammar (hpsg) (pollard and sag, 1994), tree-adjoining grammar (tag) (joshi,
1985), and id35 (id35). these approaches differ with
respect to how lexicalized they are     the degree to which they rely on the lexicon
as opposed to phrase structure rules to capture facts about the language.

the following section provides an introduction to id35, a heavily lexicalized
approach motivated by both syntactic and semantic considerations, which we will
return to in chapter 14. chapter 13 discusses dependency grammars, an approach
that eliminates phrase-structure rules entirely.

categorial
grammar

10.6.1 id35
in this section, we provide an overview of categorial grammar (ajdukiewicz 1935,
bar-hillel 1953), an early lexicalized grammar model, as well as an important mod-
ern extension, id35, or id35 (steedman 1996,steed-

combinatory
categorial
grammar man 1989,steedman 2000).

the categorial approach consists of three major elements: a set of categories,
a lexicon that associates words with categories, and a set of rules that govern how
categories combine in context.

categories
categories are either atomic elements or single-argument functions that return a cat-
egory as a value when provided with a desired category as argument. more formally,
we can de   ne c , a set of categories for a grammar as follows:

    a     c , where a is a given set of atomic elements
    (x/y), (x\y)     c , if x, y     c
the slash notation shown here is used to de   ne the functions in the grammar.
it speci   es the type of the expected argument, the direction it is expected be found,
and the type of the result. thus, (x/y) is a function that seeks a constituent of type

10.6

    lexicalized grammars

215

y to its right and returns a value of x; (x\y) is the same except it seeks its argument
to the left.
the set of atomic categories is typically very small and includes familiar el-
ements such as sentences and noun phrases. functional categories include verb
phrases and complex noun phrases among others.

the lexicon
the lexicon in a categorial approach consists of assignments of categories to words.
these assignments can either be to atomic or functional categories, and due to lexical
ambiguity words can be assigned to multiple categories. consider the following
sample lexical entries.

n
np

   ight :
miami :
cancel : (s\np)/np

nouns and proper nouns like    ight and miami are assigned to atomic categories,
re   ecting their typical role as arguments to functions. on the other hand, a transitive
verb like cancel is assigned the category (s\np)/np: a function that seeks an np on
its right and returns as its value a function with the type (s\np). this function can,
in turn, combine with an np on the left, yielding an s as the result. this captures the
kind of subcategorization information discussed in section 10.3.4, however here the
information has a rich, computationally useful, internal structure.

ditransitive verbs like give, which expect two arguments after the verb, would
have the category ((s\np)/np)/np: a function that combines with an np on its
right to yield yet another function corresponding to the transitive verb (s\np)/np
category such as the one given above for cancel.

rules
the rules of a categorial grammar specify how functions and their arguments com-
bine. the following two rule templates constitute the basis for all categorial gram-
mars.

x/y y     x
y x\y     x

(10.4)
(10.5)

the    rst rule applies a function to its argument on the right, while the second
looks to the left for its argument. we   ll refer to the    rst as forward function appli-
cation, and the second as backward function application. the result of applying
either of these rules is the category speci   ed as the value of the function being ap-
plied.

given these rules and a simple lexicon, let   s consider an analysis of the sentence
united serves miami. assume that serves is a transitive verb with the category
(s\np)/np and that united and miami are both simple nps. using both forward
and backward function application, the derivation would proceed as follows:

united

np

serves

miami
(s\np)/np np

>

s\np

s

<

216 chapter 10

    formal grammars of english

categorial grammar derivations are illustrated growing down from the words,
rule applications are illustrated with a horizontal line that spans the elements in-
volved, with the type of the operation indicated at the right end of the line. in this
example, there are two function applications: one forward function application indi-
cated by the > that applies the verb serves to the np on its right, and one backward
function application indicated by the < that applies the result of the    rst to the np
united on its left.

with the addition of another rule, the categorial approach provides a straight-
forward way to implement the coordination metarule described earlier on page 207.
recall that english permits the coordination of two constituents of the same type,
resulting in a new constituent of the same type. the following rule provides the
mechanism to handle such examples.

x conj x     x

(10.6)

this rule states that when two constituents of the same category are separated by a
constituent of type conj they can be combined into a single larger constituent of
the same type. the following derivation illustrates the use of this rule.

to

flew

we
np (s\np)/pp pp/np
pp

geneva

np

>

>

s\np

drove

and
conj (s\np)/pp pp/np

to

s\np

s\np
s

chamonix

np

pp

>

>

<  >

<

here the two s\np constituents are combined via the conjunction operator <  >
to form a larger constituent of the same type, which can then be combined with the
subject np via backward function application.

these examples illustrate the lexical nature of the categorial grammar approach.
the grammatical facts about a language are largely encoded in the lexicon, while the
rules of the grammar are boiled down to a set of three rules. unfortunately, the basic
categorial approach does not give us any more expressive power than we had with
traditional id18 rules; it just moves information from the grammar to the lexicon. to
move beyond these limitations id35 includes operations that operate over functions.

the    rst pair of operators permit us to compose adjacent functions.

x/y y /z     x/z
y\z x\y     x\z

(10.7)
(10.8)

forward
composition

backward
composition

type raising

the    rst rule, called forward composition, can be applied to adjacent con-
stituents where the    rst is a function seeking an argument of type y to its right, and
the second is a function that providesy as a result. this rule allows us to compose
these two functions into a single one with the type of the    rst constituent and the
argument of the second. although the notation is a little awkward, the second rule,
backward composition is the same, except that we   re looking to the left instead of
to the right for the relevant arguments. both kinds of composition are signalled by a
b in id35 diagrams, accompanied by a < or > to indicate the direction.

the next operator is type raising. type raising elevates simple categories to the
status of functions. more speci   cally, type raising takes a category and converts
it to function that seeks as an argument a function that takes the original category

10.6

    lexicalized grammars

217

as its argument. the following schema show two versions of type raising: one for
arguments to the right, and one for the left.

x     t /(t\x)
x     t\(t /x)

(10.9)
(10.10)

the category t in these rules can correspond to any of the atomic or functional
categories already present in the grammar.

a particularly useful example of type raising transforms a simple np argument
in subject position to a function that can compose with a following vp. to see how
this works, let   s revisit our earlier example of united serves miami. instead of clas-
sifying united as an np which can serve as an argument to the function attached to
serve, we can use type raising to reinvent it as a function in its own right as follows.

np     s/(s\np)

combining this type-raised constituent with the forward composition rule (10.7)
permits the following alternative to our previous derivation.

united

np

>t

s/(s\np)

serves

miami
(s\np)/np np

s/np

>b

s

>

by type raising united to s/(s\np), we can compose it with the transitive verb
serves to yield the (s/np) function needed to complete the derivation.
there are several interesting things to note about this derivation. first, is it
provides a left-to-right, word-by-word derivation that more closely mirrors the way
humans process language. this makes id35 a particularly apt framework for psy-
cholinguistic studies. second, this derivation involves the use of an intermediate
unit of analysis, united serves, that does not correspond to a traditional constituent
in english. this ability to make use of such non-constituent elements provides id35
with the ability to handle the coordination of phrases that are not proper constituents,
as in the following example.
(10.11) we    ew icelandair to geneva and swissair to london.

here, the segments that are being coordinated are icelandair to geneva and
swissair to london, phrases that would not normally be considered constituents, as
can be seen in the following standard derivation for the verb phrase    ew icelandair
to geneva.

   ew

icelandair

to

geneva

(vp/pp)/np

np

vp/pp

vp

>

pp/np

np

pp

>

>

in this derivation, there is no single constituent that corresponds to icelandair
to geneva, and hence no opportunity to make use of the <  > operator. note that
complex id35 categories can can get a little cumbersome, so we   ll use vp as a
shorthand for (s\np) in this and the following derivations.
the following alternative derivation provides the required element through the
use of both backward type raising (10.10) and backward function composition (10.8).

218 chapter 10

    formal grammars of english
icelandair

   ew

(v p/pp)/np

np

(v p/pp)\((v p/pp)/np)

<t

to

geneva

pp/np

np

pp

v p\(v p/pp)

<b

>

<t

<

v p\((v p/pp)/np)
v p

applying the same analysis to swissair to london satis   es the requirements
for the <  > operator, yielding the following derivation for our original example
(10.11).

   ew

(v p/pp)/np

icelandair

np

(v p/pp)\((v p/pp)/np)

<t

to

geneva

pp/np

np

pp

>

<t

v p\(v p/pp)

<

and
conj

swissair

np

(v p/pp)\((v p/pp)/np)

<t

v p\((v p/pp)/np)

v p\((v p/pp)/np)

to

london

pp/np

np

pp

v p\(v p/pp)
<  >

<

>

<t

<

v p\((v p/pp)/np)
v p

finally, let   s examine how these advanced operators can be used to handle long-
distance dependencies (also referred to as syntactic movement or extraction). as
mentioned in section 10.3.1, long-distance dependencies arise from many english
constructions including wh-questions, relative clauses, and topicalization. what
these constructions have in common is a constituent that appears somewhere dis-
tant from its usual, or expected, location. consider the following relative clause as
an example.

the    ight that united diverted

here, divert is a transitive verb that expects two np arguments, a subject np to its
left and a direct object np to its right; its category is therefore (s\np)/np. however,
in this example the direct object the    ight has been    moved    to the beginning of the
clause, while the subject united remains in its normal position. what is needed is a
way to incorporate the subject argument, while dealing with the fact that the    ight is
not in its expected location.

the following derivation accomplishes this, again through the combined use of

type raising and function composition.
that

   ight

the
np/n n (np\np)/(s/np)

>

np

diverted
(s\np)/np

united

np

>t

s/(s\np)

s/np

>b

>

<

np\np

np

as we saw with our earlier examples, the    rst step of this derivation is type raising
united to the category s/(s\np) allowing it to combine with diverted via forward
composition. the result of this composition is s/np which preserves the fact that we
are still looking for an np to    ll the missing direct object. the second critical piece
is the lexical category assigned to the word that: (np\np)/(s/np). this function
seeks a verb phrase missing an argument to its right, and transforms it into an np
seeking a missing element to its left, precisely where we    nd the    ight.

10.7

    summary

219

id35bank

as with phrase-structure approaches, treebanks play an important role in id35-
based approaches to parsing. id35bank (hockenmaier and steedman, 2007) is the
largest and most widely used id35 treebank. it was created by automatically trans-
lating phrase-structure trees from the id32 via a rule-based approach. the
method produced successful translations of over 99% of the trees in the penn tree-
bank resulting in 48,934 sentences paired with id35 derivations. it also provides
a lexicon of 44,000 words with over 1200 categories. chapter 12 will discuss how
these resources can be used to train id35 parsers.

10.7 summary

this chapter has introduced a number of fundamental concepts in syntax through
the use of context-free grammars.

    in many languages, groups of consecutive words act as a group or a con-
stituent, which can be modeled by context-free grammars (which are also
known as phrase-structure grammars).

    a context-free grammar consists of a set of rules or productions, expressed
over a set of non-terminal symbols and a set of terminal symbols. formally,
a particular context-free language is the set of strings that can be derived
from a particular context-free grammar.

guage that is used to model the grammar of a natural language.

    a generative grammar is a traditional name in linguistics for a formal lan-
    there are many sentence-level grammatical constructions in english; declar-
ative, imperative, yes-no question, and wh-question are four common types;
these can be modeled with context-free rules.

    an english noun phrase can have determiners, numbers, quanti   ers, and
adjective phrases preceding the head noun, which can be followed by a num-
ber of postmodi   ers; gerundive vps, in   nitives vps, and past participial
vps are common possibilities.

    subjects in english agree with the main verb in person and number.
    verbs can be subcategorized by the types of complements they expect. sim-
ple subcategories are transitive and intransitive; most grammars include
many more categories than these.

languages. treebanks can be searched with tree-search tools.

    treebanks of parsed sentences exist for many genres of english and for many
    any context-free grammar can be converted to chomsky normal form, in
which the right-hand side of each rule has either two non-terminals or a single
terminal.

    lexicalized grammars place more emphasis on the structure of the lexicon,

lessening the burden on pure phrase-structure rules.

    combinatorial categorial grammar (id35) is an important computationally

relevant lexicalized approach.

220 chapter 10

    formal grammars of english

bibliographical and historical notes

[the origin of the idea of phrasal constituency, cited in percival (1976)]:
den sprachlichen ausdruck f  ur die willk  urliche
gliederung einer gesammtvorstellung in ihre
in logische beziehung zueinander gesetzten bestandteile   
[the linguistic expression for the arbitrary division of a total idea
into its constituent parts placed in logical relations to one another]
w. wundt

according to percival (1976), the idea of breaking up a sentence into a hierar-
chy of constituents appeared in the v  olkerpsychologie of the groundbreaking psy-
chologist wilhelm wundt (wundt, 1900). wundt   s idea of constituency was taken
up into linguistics by leonard bloom   eld in his early book an introduction to the
study of language (bloom   eld, 1914). by the time of his later book, language
(bloom   eld, 1933a), what was then called    immediate-constituent analysis    was a
well-established method of syntactic study in the united states. by contrast, tra-
ditional european grammar, dating from the classical period, de   ned relations be-
tween words rather than constituents, and european syntacticians retained this em-
phasis on such dependency grammars, the subject of chapter 13.

american structuralism saw a number of speci   c de   nitions of the immediate
constituent, couched in terms of their search for a    discovery procedure   : a method-
ological algorithm for describing the syntax of a language. in general, these attempt
to capture the intuition that    the primary criterion of the immediate constituent is the
degree in which combinations behave as simple units    (bazell, 1966, p. 284). the
most well known of the speci   c de   nitions is harris    idea of distributional similarity
to individual units, with the substitutability test. essentially, the method proceeded
by breaking up a construction into constituents by attempting to substitute simple
structures for possible constituents   if a substitution of a simple form, say, man,
was substitutable in a construction for a more complex set (like intense young man),
then the form intense young man was probably a constituent. harris   s test was the
beginning of the intuition that a constituent is a kind of equivalence class.

the    rst formalization of this idea of hierarchical constituency was the phrase-
structure grammar de   ned in chomsky (1956) and further expanded upon (and
argued against) in chomsky (1957) and chomsky (1975). from this time on, most
generative linguistic theories were based at least in part on context-free grammars or
generalizations of them (such as head-driven phrase structure grammar (pollard
and sag, 1994), lexical-functional grammar (bresnan, 1982), government and
binding (chomsky, 1981), and construction grammar (kay and fillmore, 1999),
inter alia); many of these theories used schematic context-free templates known as
x-bar schemata, which also relied on the notion of syntactic head.

shortly after chomsky   s initial work, the context-free grammar was reinvented
by backus (1959) and independently by naur et al. (1960) in their descriptions of
the algol programming language; backus (1996) noted that he was in   uenced by
the productions of emil post and that naur   s work was independent of his (backus   )
own. (recall the discussion on page ?? of multiple invention in science.) after this
early work, a great number of computational models of natural language processing
were based on context-free grammars because of the early development of ef   cient
algorithms to parse these grammars (see chapter 11).

x-bar
schemata

exercises

221

as we have already noted, grammars based on context-free rules are not ubiqui-
tous. various classes of extensions to id18s are designed speci   cally to handle long-
distance dependencies. we noted earlier that some grammars treat long-distance-
dependent items as being related semantically but not syntactically; the surface syn-
tax does not represent the long-distance link (kay and fillmore 1999, culicover and
jackendoff 2005). but there are alternatives.

one extended formalism is id34 (tag)

(joshi, 1985).
the primary tag data structure is the tree, rather than the rule. trees come in two
kinds: initial trees and auxiliary trees. initial trees might, for example, represent
simple sentential structures, and auxiliary trees add recursion into a tree. trees are
combined by two operations called substitution and adjunction. the adjunction
operation handles long-distance dependencies. see joshi (1985) for more details.
an extension of id34, called lexicalized tree adjoining gram-
mars is discussed in chapter 12. id34 is a member of the family
of mildly context-sensitive languages.

we mentioned on page 208 another way of handling long-distance dependencies,
based on the use of empty categories and co-indexing. the id32 uses
this model, which draws (in various treebank corpora) from the extended standard
theory and minimalism (radford, 1997).

readers interested in the grammar of english should get one of the three large
reference grammars of english: huddleston and pullum (2002), biber et al. (1999),
and quirk et al. (1985). another useful reference is mccawley (1998).

there are many good introductory textbooks on syntax from different perspec-
tives. sag et al. (2003) is an introduction to syntax from a generative perspective,
focusing on the use of phrase-structure rules, uni   cation, and the type hierarchy in
head-driven phrase structure grammar. van valin, jr. and la polla (1997) is an
introduction from a functional perspective, focusing on cross-linguistic data and on
the functional motivation for syntactic structures.

generative

functional

exercises

10.1 draw tree structures for the following atis phrases:

1. dallas
2. from denver
3. after    ve p.m.
4. arriving in washington
5. early    ights
6. all redeye    ights
7. on thursday
8. a one-way fare
9. any delays in denver

10.2 draw tree structures for the following atis sentences:

1. does american airlines have a    ight between    ve a.m. and six a.m.?
2. i would like to    y on american airlines.
3. please repeat that.
4. does american 487 have a    rst-class section?
5. i need to    y between philadelphia and atlanta.
6. what is the fare from atlanta to denver?

222 chapter 10

    formal grammars of english
7. is there an american airlines    ight from philadelphia to dallas?

10.3 assume a grammar that has many vp rules for different subcategorizations,
as expressed in section 10.3.4, and differently subcategorized verb rules like
verb-with-np-complement. how would the rule for postnominal relative clauses
(10.4) need to be modi   ed if we wanted to deal properly with examples like
the earliest    ight that you have? recall that in such examples the pronoun
that is the object of the verb get. your rules should allow this noun phrase but
should correctly rule out the ungrammatical s *i get.

10.4 does your solution to the previous problem correctly model the np the earliest
   ight that i can get? how about the earliest    ight that i think my mother
wants me to book for her? hint: this phenomenon is called long-distance
dependency.

possessive
genitive

10.5 write rules expressing the verbal subcategory of english auxiliaries; for ex-

ample, you might have a rule verb-with-bare-stem-vp-complement     can.

10.6 nps like fortune   s of   ce or my uncle   s marks are called possessive or genitive
noun phrases. we can model possessive noun phrases by treating the sub-np
like fortune   s or my uncle   s as a determiner of the following head noun. write
grammar rules for english possessives. you may treat    s as if it were a separate
word (i.e., as if there were always a space before    s).

10.7 page 201 discussed the need for a wh-np constituent. the simplest wh-np
is one of the wh-pronouns (who, whom, whose, which). the wh-words what
and which can be determiners: which four will you have?, what credit do you
have with the duke? write rules for the different types of wh-nps.

10.8 write an algorithm for converting an arbitrary context-free grammar into chom-

sky normal form.

chapter

11 syntactic parsing

one morning i shot an elephant in my pajamas.
how he got into my pajamas i don   t know.

groucho marx, animal crackers, 1930

syntactic parsing is the task of recognizing a sentence and assigning a syntactic
structure to it. this chapter focuses on the structures assigned by context-free gram-
mars of the kind described in chapter 10. since they are based on a purely declar-
ative formalism, context-free grammars don   t specify how the parse tree for a given
sentence should be computed. we therefore need to specify algorithms that employ
these grammars to ef   ciently produce correct trees.

parse trees are directly useful in applications such as grammar checking in
word-processing systems: a sentence that cannot be parsed may have grammatical
errors (or at least be hard to read). more typically, however, parse trees serve as an
important intermediate stage of representation for semantic analysis (as we show in
chapter 15) and thus play an important role in applications like id53
and information extraction. for example, to answer the question

what books were written by british women authors before 1800?

we   ll need to know that the subject of the sentence was what books and that the by-
adjunct was british women authors to help us    gure out that the user wants a list of
books (and not a list of authors).

before presenting any algorithms, we begin by discussing how the ambiguity
arises again in this context and the problems it presents. the section that fol-
lows then presents the cocke-kasami-younger (cky) algorithm (kasami 1965,
younger 1967), the standard id145 approach to syntactic parsing.
recall that we   ve already seen applications of id145 algorithms in
the minimum-edit-distance and viterbi algorithms of earlier chapters. finally, we
discuss partial parsing methods, for use in situations in which a super   cial syntac-
tic analysis of an input may be suf   cient.

11.1 ambiguity

structural
ambiguity

ambiguity is perhaps the most serious problem faced by syntactic parsers. chap-
ter 8 introduced the notions of part-of-speech ambiguity and part-of-speech dis-
ambiguation. here, we introduce a new kind of ambiguity, called structural ambi-
guity, which arises from many commonly used rules in phrase-structure grammars.
to illustrate the issues associated with structural ambiguity, we   ll make use of a new
toy grammar l1, shown in figure 11.1, which consists of the l0 grammar from the
last chapter augmented with a few additional rules.

structural ambiguity occurs when the grammar can assign more than one parse
to a sentence. groucho marx   s well-known line as captain spaulding in animal

224 chapter 11

    syntactic parsing

lexicon
det     that | this | the | a
noun     book |    ight | meal | money
verb     book | include | prefer
pronoun     i | she | me
proper-noun     houston | nwa
aux     does
preposition     from | to | on | near | through

grammar

s     np vp
s     aux np vp
s     vp
np     pronoun
np     proper-noun
np     det nominal
nominal     noun
nominal     nominal noun
nominal     nominal pp
vp     verb
vp     verb np
vp     verb np pp
vp     verb pp
vp     vp pp
pp     preposition np
figure 11.1 the l1 miniature english grammar and lexicon.

s

s

np

vp

np

vp

pronoun

verb

np

pronoun

vp

pp

i

shot

det

nominal

i

verb

np

in my pajamas

an

nominal

pp

shot

det

nominal

noun

in my pajamas

an

noun

elephant

elephant

figure 11.2 two parse trees for an ambiguous sentence. the parse on the left corresponds to the humorous
reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which
captain spaulding did the shooting in his pajamas.

attachment
ambiguity

crackers is ambiguous because the phrase in my pajamas can be part of the np
headed by elephant or a part of the verb phrase headed by shot. figure 11.2 illus-
trates these two analyses of marx   s line using rules from l1.

structural ambiguity, appropriately enough, comes in many forms. two common

kinds of ambiguity are attachment ambiguity and coordination ambiguity.

a sentence has an attachment ambiguity if a particular constituent can be at-
tached to the parse tree at more than one place. the groucho marx sentence is
an example of pp-attachment ambiguity. various kinds of adverbial phrases are
also subject to this kind of ambiguity. for instance, in the following example the
gerundive-vp    ying to paris can be part of a gerundive sentence whose subject is
the eiffel tower or it can be an adjunct modifying the vp headed by saw:

coordination
ambiguity

syntactic
disambiguation

11.2

    cky parsing: a id145 approach

225

(11.1) we saw the eiffel tower    ying to paris.

in coordination ambiguity different sets of phrases can be conjoined by a con-
junction like and. for example, the phrase old men and women can be bracketed as
[old [men and women]], referring to old men and old women, or as [old men] and
[women], in which case it is only the men who are old.

these ambiguities combine in complex ways in real sentences. a program that
summarized the news, for example, would need to be able to parse sentences like
the following from the brown corpus:
(11.2) president kennedy today pushed aside other white house business to

devote all his time and attention to working on the berlin crisis address he
will deliver tomorrow night to the american people over nationwide
television and radio.

this sentence has a number of ambiguities, although since they are semantically
unreasonable, it requires a careful reading to see them. the last noun phrase could be
parsed [nationwide [television and radio]] or [[nationwide television] and radio].
the direct object of pushed aside should be other white house business but could
also be the bizarre phrase [other white house business to devote all his time and
attention to working] (i.e., a structure like kennedy af   rmed [his intention to propose
a new budget to address the de   cit]). then the phrase on the berlin crisis address he
will deliver tomorrow night to the american people could be an adjunct modifying
the verb pushed. a pp like over nationwide television and radio could be attached
to any of the higher vps or nps (e.g., it could modify people or night).

the fact that there are many grammatically correct but semantically unreason-
able parses for naturally occurring sentences is an irksome problem that affects all
parsers. ultimately, most natural language processing systems need to be able to
choose a single correct parse from the multitude of possible parses through a process
of syntactic disambiguation. effective disambiguation algorithms require statisti-
cal, semantic, and contextual knowledge sources that vary in how well they can be
integrated into parsing algorithms.

fortunately, the cky algorithm presented in the next section is designed to ef   -
ciently handle structural ambiguities of the kind we   ve been discussing. and as we   ll
see in chapter 12, there are straightforward ways to integrate statistical techniques
into the basic cky framework to produce highly accurate parsers.

11.2 cky parsing: a id145 approach

the previous section introduced some of the problems associated with ambiguous
grammars. fortunately, id145 provides a powerful framework for
addressing these problems, just as it did with the minimum id153, viterbi,
and forward algorithms. recall that id145 approaches systemati-
cally    ll in tables of solutions to sub-problems. when complete, the tables contain
the solution to all the sub-problems needed to solve the problem as a whole.
in
the case of syntactic parsing, these sub-problems represent parse trees for all the
constituents detected in the input.

the id145 advantage arises from the context-free nature of our
grammar rules     once a constituent has been discovered in a segment of the input
we can record its presence and make it available for use in any subsequent derivation
that might require it. this provides both time and storage ef   ciencies since subtrees

226 chapter 11

    syntactic parsing

can be looked up in a table, not reanalyzed. this section presents the cocke-kasami-
younger (cky) algorithm, the most widely used dynamic-programming based ap-
proach to parsing. related approaches include the earley algorithm (earley, 1970)
and chart parsing (kaplan 1973, kay 1982).

unit
productions

11.2.1 conversion to chomsky normal form
we begin our investigation of the cky algorithm by examining the requirement
that grammars used with it must be in chomsky normal form (cnf). recall from
chapter 10 that grammars in cnf are restricted to rules of the form a     b c or
a     w. that is, the right-hand side of each rule must expand either to two non-
terminals or to a single terminal. restricting a grammar to cnf does not lead to
any loss in expressiveness, since any context-free grammar can be converted into
a corresponding cnf grammar that accepts exactly the same set of strings as the
original grammar.

let   s start with the process of converting a generic id18 into one represented in
cnf. assuming we   re dealing with an  -free grammar, there are three situations we
need to address in any generic grammar: rules that mix terminals with non-terminals
on the right-hand side, rules that have a single non-terminal on the right-hand side,
and rules in which the length of the right-hand side is greater than 2.

the remedy for rules that mix terminals and non-terminals is to simply introduce
a new dummy non-terminal that covers only the original terminal. for example, a
rule for an in   nitive verb phrase such as inf-vp     to vp would be replaced by the
two rules inf-vp     to vp and to     to.
rules with a single non-terminal on the right are called unit productions. we
can eliminate unit productions by rewriting the right-hand side of the original rules
with the right-hand side of all the non-unit production rules that they ultimately lead
to. more formally, if a        b by a chain of one or more unit productions and b       
is a non-unit production in our grammar, then we add a        for each such rule in
the grammar and discard all the intervening unit productions. as we demonstrate
with our toy grammar, this can lead to a substantial    attening of the grammar and a
consequent promotion of terminals to fairly high levels in the resulting trees.

rules with right-hand sides longer than 2 are normalized through the introduc-
tion of new non-terminals that spread the longer sequences over several new rules.
formally, if we have a rule like

we replace the leftmost pair of non-terminals with a new non-terminal and introduce
a new production result in the following new rules:

a     b c   

a     x1   
x1     b c

in the case of longer right-hand sides, we simply iterate this process until the of-
fending rule has been replaced by rules of length 2. the choice of replacing the
leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results
in binary rules would suf   ce.

rules s     x1 vp and x1     aux np.

in our current grammar, the rule s     aux np vp would be replaced by the two
the entire conversion process can be summarized as follows:
1. copy all conforming rules to the new grammar unchanged.

11.2

    cky parsing: a id145 approach

227

l1 in cnf

l1 grammar

s     np vp
s     aux np vp
s     vp

s     np vp
s     x1 vp
x1     aux np
s     book | include | prefer
s     verb np
s     x2 pp
s     verb pp
s     vp pp
np     i | she | me
np     twa | houston
np     det nominal
nominal     book |    ight | meal | money
nominal     nominal noun
nominal     nominal pp
vp     book | include | prefer
vp     verb np
vp     x2 pp
x2     verb np
vp     verb pp
vp     vp pp
pp     preposition np

np     pronoun
np     proper-noun
np     det nominal
nominal     noun
nominal     nominal noun
nominal     nominal pp
vp     verb
vp     verb np
vp     verb np pp
vp     verb pp
vp     vp pp
pp     preposition np
figure 11.3 l1 grammar and its conversion to cnf. note that although they aren   t shown
here, all the original lexical entries from l1 carry over unchanged as well.

2. convert terminals within rules to dummy non-terminals.
3. convert unit-productions.
4. make all rules binary and add them to new grammar.
figure 11.3 shows the results of applying this entire conversion procedure to
the l1 grammar introduced earlier on page 224. note that this    gure doesn   t show
the original lexical rules; since these original lexical rules are already in cnf, they
all carry over unchanged to the new grammar. figure 11.3 does, however, show
the various places where the process of eliminating unit productions has, in effect,
created new lexical rules. for example, all the original verbs have been promoted to
both vps and to ss in the converted grammar.

11.2.2 cky recognition
with our grammar now in cnf, each non-terminal node above the part-of-speech
level in a parse tree will have exactly two daughters. a two-dimensional matrix can
be used to encode the structure of an entire tree. for a sentence of length n, we will
work with the upper-triangular portion of an (n + 1)   (n + 1) matrix. each cell [i, j]
in this matrix contains the set of non-terminals that represent all the constituents that
span positions i through j of the input. since our indexing scheme begins with 0,
it   s natural to think of the indexes as pointing at the gaps between the input words
(as in 0 book 1 that 2    ight 3). it follows then that the cell that represents the entire
input resides in position [0,n] in the matrix.

since each non-terminal entry in our table has two daughters in the parse, it fol-
lows that for each constituent represented by an entry [i, j], there must be a position
in the input, k, where it can be split into two parts such that i < k < j. given such

228 chapter 11

    syntactic parsing

a position k, the    rst constituent [i,k] must lie to the left of entry [i, j] somewhere
along row i, and the second entry [k, j] must lie beneath it, along column j.

to make this more concrete, consider the following example with its completed

parse matrix, shown in fig. 11.4.

(11.3) book the    ight through houston.

the superdiagonal row in the matrix contains the parts of speech for each input word
in the input. the subsequent diagonals above that superdiagonal contain constituents
that cover all the spans of increasing length in the input.

figure 11.4 completed parse table for book the    ight through houston.

given this setup, cky recognition consists of    lling the parse table in the right
way. to do this, we   ll proceed in a bottom-up fashion so that at the point where
we are    lling any cell [i, j], the cells containing the parts that could contribute to
this entry (i.e., the cells to the left and the cells below) have already been    lled.
the algorithm given in fig. 11.5    lls the upper-triangular matrix a column at a time
working from left to right, with each column    lled from bottom to top, as the right
side of fig. 11.4 illustrates. this scheme guarantees that at each point in time we
have all the information we need (to the left, since all the columns to the left have
already been    lled, and below since we   re    lling bottom to top). it also mirrors on-
line parsing since    lling the columns from left to right corresponds to processing
each word one at a time.

function cky-parse(words, grammar) returns table
for j   from 1 to length(words) do
for all {a | a     words[ j]     grammar}
table[ j    1, j]   table[ j    1, j]     a
for i   from j    2 downto 0 do
for k   i + 1 to j    1 do
for all {a | a     bc     grammar and b     table[i,k] and c     table[k, j]}

figure 11.5 the cky algorithm.

table[i,j]   table[i,j]     a

bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2s,vp,x2detnpnpnominal,nounnominalprepppnp,proper-noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]11.2

    cky parsing: a id145 approach

229

figure 11.6 all the ways to    ll the [i, j]th cell in the cky table.

the outermost loop of the algorithm given in fig. 11.5 iterates over the columns,
and the second loop iterates over the rows, from the bottom up. the purpose of the
innermost loop is to range over all the places where a substring spanning i to j in
the input might be split in two. as k ranges over the places where the string can be
split, the pairs of cells we consider move, in lockstep, to the right along row i and
down along column j. figure 11.6 illustrates the general case of    lling cell [i, j]. at
each such split, the algorithm considers whether the contents of the two cells can be
combined in a way that is sanctioned by a rule in the grammar. if such a rule exists,
the non-terminal on its left-hand side is entered into the table.

figure 11.7 shows how the    ve cells of column 5 of the table are    lled after the
word houston is read. the arrows point out the two spans that are being used to add
an entry to the table. note that the action in cell [0,5] indicates the presence of three
alternative parses for this input, one where the pp modi   es the    ight, one where
it modi   es the booking, and one that captures the second argument in the original
vp     verb np pp rule, now captured indirectly with the vp     x2 pp rule.

......[0,n][i,i+1][i,i+2][i,j-2][i,j-1][i+1,j][i+2,j][j-1,j][j-2,j][i,j]...[0,1][n-1, n]230 chapter 11

    syntactic parsing

figure 11.7 filling the cells of column 5 after reading the word houston.

bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2detnpnominal,nounnominalprepnp,proper-noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2detnpnpnominal,nounprepppnp,proper-noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2detnpnpnominal,nounnominalprepppnp,proper-noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2detnpnpnominal,nounnominalprepppnp,proper-noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]bookthe flight throughhoustons, vp, verb, nominal, nouns,vp,x2detnpnpnominal,nounnominalprepppnp,proper-noun[0,1][0,2][0,3][0,4][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]s2, vps3s1,vp, x211.3

    partial parsing

231

11.2.3 cky parsing
the algorithm given in fig. 11.5 is a recognizer, not a parser; for it to succeed, it
simply has to    nd an s in cell [0,n]. to turn it into a parser capable of returning all
possible parses for a given input, we can make two simple changes to the algorithm:
the    rst change is to augment the entries in the table so that each non-terminal is
paired with pointers to the table entries from which it was derived (more or less as
shown in fig. 11.7), the second change is to permit multiple versions of the same
non-terminal to be entered into the table (again as shown in fig. 11.7). with these
changes, the completed table contains all the possible parses for a given input. re-
turning an arbitrary single parse consists of choosing an s from cell [0,n] and then
recursively retrieving its component constituents from the table.

of course, returning all the parses for a given input may incur considerable cost
since an exponential number of parses may be associated with a given input. in such
cases, returning all the parses will have an unavoidable exponential cost. looking
forward to chapter 12, we can also think about retrieving the best parse for a given
input by further augmenting the table to contain the probabilities of each entry. re-
trieving the most probable parse consists of running a suitably modi   ed version of
the viterbi algorithm from chapter 8 over the completed parse table.

11.2.4 cky in practice
finally, we should note that while the restriction to cnf does not pose a prob-
lem theoretically, it does pose some non-trivial problems in practice. obviously, as
things stand now, our parser isn   t returning trees that are consistent with the grammar
given to us by our friendly syntacticians. in addition to making our grammar devel-
opers unhappy, the conversion to cnf will complicate any syntax-driven approach
to semantic analysis.

one approach to getting around these problems is to keep enough information
around to transform our trees back to the original grammar as a post-processing step
of the parse. this is trivial in the case of the transformation used for rules with length
greater than 2. simply deleting the new dummy non-terminals and promoting their
daughters restores the original tree.

in the case of unit productions, it turns out to be more convenient to alter the ba-
sic cky algorithm to handle them directly than it is to store the information needed
to recover the correct trees. exercise 11.3 asks you to make this change. many of
the probabilistic parsers presented in chapter 12 use the cky algorithm altered in
just this manner. another solution is to adopt a more complex dynamic program-
ming solution that simply accepts arbitrary id18s. the next section presents such an
approach.

11.3 partial parsing

partial parse
shallow parse

many language processing tasks do not require complex, complete parse trees for all
inputs. for these tasks, a partial parse, or shallow parse, of input sentences may
be suf   cient. for example, information extraction systems generally do not extract
all the possible information from a text: they simply identify and classify the seg-
ments in a text that are likely to contain valuable information. similarly, information
retrieval systems may index texts according to a subset of the constituents found in

232 chapter 11

    syntactic parsing

them.

there are many different approaches to partial parsing. some make use of
cascades of    nite state transducers to produce tree-like representations. these ap-
proaches typically produce    atter trees than the ones we   ve been discussing in this
chapter and the previous one. this    atness arises from the fact that    nite state trans-
ducer approaches generally defer decisions that may require semantic or contex-
tual factors, such as prepositional phrase attachments, coordination ambiguities, and
nominal compound analyses. nevertheless, the intent is to produce parse trees that
link all the major constituents in an input.

an alternative style of partial parsing is known as chunking. chunking is the
process of identifying and classifying the    at, non-overlapping segments of a sen-
tence that constitute the basic non-recursive phrases corresponding to the major
content-word parts-of-speech: noun phrases, verb phrases, adjective phrases, and
prepositional phrases. the task of    nding all the base noun phrases in a text is
particularly common. since chunked texts lack a hierarchical structure, a simple
bracketing notation is suf   cient to denote the location and the type of the chunks in
a given example:
(11.4)
this bracketing notation makes clear the two fundamental tasks that are involved
in chunking: segmenting (   nding the non-overlapping extents of the chunks) and
labeling (assigning the correct tag to the discovered chunks).

[np the morning    ight] [pp from] [np denver] [vp has arrived.]

some input words may not be part of any chunk, particularly in tasks like base

np:
(11.5)

[np the morning    ight] from [np denver] has arrived.

what constitutes a syntactic base phrase depends on the application (and whether
the phrases come from a treebank). nevertheless, some standard guidelines are fol-
lowed in most systems. first and foremost, base phrases of a given type do not
recursively contain any constituents of the same type. eliminating this kind of recur-
sion leaves us with the problem of determining the boundaries of the non-recursive
phrases. in most approaches, base phrases include the headword of the phrase, along
with any pre-head material within the constituent, while crucially excluding any
post-head material. eliminating post-head modi   ers obviates the need to resolve at-
tachment ambiguities. this exclusion does lead to certain oddities, such as pps and
vps often consisting solely of their heads. thus, our earlier example a    ight from
indianapolis to houston on nwa is reduced to the following:
(11.6) [np a    ight] [pp from] [np indianapolis][pp to][np houston][pp on][np

nwa]

11.3.1 machine learning-based approaches to chunking
state-of-the-art approaches to chunking use supervised machine learning to train a
chunker by using annotated data as a training set and training any sequence labeler.
it   s common to model chunking as iob tagging. in iob tagging we introduce a tag
for the beginning (b) and inside (i) of each chunk type, and one for tokens outside
(o) any chunk. the number of tags is thus 2n + 1 tags, where n is the number
of chunk types.
iob tagging can represent exactly the same information as the
bracketed notation. the following example shows the bracketing notation of (11.4)
on page 232 reframed as a tagging task:
(11.7) the

morning
i np

   ight
i np

from
b pp

denver
b np

has
b vp

arrived
i vp

b np

chunking

iob

11.3

    partial parsing

233

figure 11.8 a sequence model for chunking. the chunker slides a context window over the sentence, clas-
sifying words as it proceeds. at this point, the classi   er is attempting to label    ight, using features like words,
embeddings, part-of-speech tags and previously assigned chunk tags.

the same sentence with only the base-nps tagged illustrates the role of the o tags.

(11.8) the

b np

morning
i np

   ight
i np

from
o

denver
b np

has
o

arrived.
o

there is no explicit encoding of the end of a chunk in iob tagging; the end of any
chunk is implicit in any transition from an i or b to a b or o tag. this encoding
re   ects the notion that when sequentially labeling words, it is generally easier (at
least in english) to detect the beginning of a new chunk than it is to know when a
chunk has ended.

since annotation efforts are expensive and time consuming, chunkers usually
rely on existing treebanks like the id32 (chapter 10), extracting syntactic
phrases from the full parse constituents of a sentence,    nding the appropriate heads
and then including the material to the left of the head, ignoring the text to the right.
this is somewhat error-prone since it relies on the accuracy of the head-   nding rules
described in chapter 10.

given a training set, any sequence model can be used. figure 11.8 shows an
illustration of a simple feature-based model, using features like the words and parts-
of-speech within a 2 word window, and the chunk tags of the preceding inputs in the
window. in training, each training vector would consist of the values of 13 features;
the two words to the left of the decision point, their parts-of-speech and chunk tags,
the word to be tagged along with its part-of-speech, the two words that follow along
with their parts-of speech, and the correct chunk tag, in this case, i np. during
classi   cation, the classi   er is given the same vector without the answer and assigns
the most appropriate tag from its tagset. viterbi decoding is commonly used.

b_npi_np?     theflightfromdenverhasarrived classifierdtnnnninnnpcorresponding feature representationthe, dt, b_np, morning, nn, i_np, flight, nn, from, in, denver, nnplabeli_npmorning234 chapter 11

    syntactic parsing

11.3.2 chunking-system evaluations
as with the evaluation of part-of-speech taggers, the evaluation of chunkers pro-
ceeds by comparing chunker output with gold-standard answers provided by human
annotators. however, unlike part-of-speech tagging, word-by-word accuracy mea-
sures are not appropriate. instead, chunkers are evaluated according to the notions of
precision, recall, and the f-measure borrowed from the    eld of information retrieval.
precision measures the percentage of system-provided chunks that were correct.
correct here means that both the boundaries of the chunk and the chunk   s label are
correct. precision is therefore de   ned as
precision: = number of correct chunks given by system
total number of chunks given by system

recall measures the percentage of chunks actually present in the input that were

correctly identi   ed by the system. recall is de   ned as
recall: = number of correct chunks given by system
total number of actual chunks in the text

precision

recall

f-measure

the f-measure (van rijsbergen, 1975) provides a way to combine these two

measures into a single metric. the f-measure is de   ned as

f   =

(   2 + 1)pr
   2p + r

the    parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. values of    > 1 favor recall, while
values of    < 1 favor precision. when    = 1, precision and recall are equally bal-
anced; this is sometimes called f   =1 or just f1:
2pr
p + r

(11.9)

f1 =

f-measure comes from a weighted harmonic mean of precision and recall. the
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-
rocals:

harmonicmean(a1,a2,a3,a4, ...,an) =

n
+ 1
a3

1
a1

+ 1
a2

+ ... + 1
an

and hence f-measure is

f =

1

p + (1      ) 1
   1

r

or(cid:18)with    2 =

   (cid:19) f =
1      

(   2 + 1)pr
   2p + r

(11.10)

(11.11)

11.4 summary

the two major ideas introduced in this chapter are those of parsing and partial
parsing. here   s a summary of the main points we covered about these ideas:

    structural ambiguity is a signi   cant problem for parsers. common sources
of structural ambiguity include pp-attachment, coordination ambiguity,
and noun-phrase bracketing ambiguity.

    id145 parsing algorithms, such as cky, use a table of

partial parses to ef   ciently parse ambiguous sentences.

bibliographical and historical notes

235

solved without full parsing.

    cky restricts the form of the grammar to chomsky normal form (cnf).
    many practical problems, including information extraction problems, can be
    partial parsing and chunking are methods for identifying shallow syntactic
    state-of-the-art methods for partial parsing use supervised machine learning

constituents in a text.

techniques.

bibliographical and historical notes

writing about the history of compilers, knuth notes:

in this    eld there has been an unusual amount of parallel discovery of
the same technique by people working independently.

wfst

well, perhaps not unusual, since multiple discovery is the norm in science (see
page ??). but there has certainly been enough parallel publication that this his-
tory errs on the side of succinctness in giving only a characteristic early mention of
each algorithm; the interested reader should see aho and ullman (1972).

bottom-up parsing seems to have been    rst described by yngve (1955), who
gave a breadth-   rst, bottom-up parsing algorithm as part of an illustration of a ma-
chine translation procedure. top-down approaches to parsing and translation were
described (presumably independently) by at least glennie (1960), irons (1961), and
kuno and oettinger (1963). id145 parsing, once again, has a his-
tory of independent discovery. according to martin kay (personal communication),
a id145 parser containing the roots of the cky algorithm was    rst
implemented by john cocke in 1960. later work extended and formalized the algo-
rithm, as well as proving its time complexity (kay 1967,younger 1967,kasami 1965).
the related well-formed substring table (wfst) seems to have been indepen-
dently proposed by kuno (1965) as a data structure that stores the results of all pre-
vious computations in the course of the parse. based on a generalization of cocke   s
work, a similar data structure had been independently described in kay 1967, kay 1973.
the top-down application of id145 to parsing was described in
earley   s ph.d. dissertation (earley 1968, earley 1970). sheil (1976) showed the
equivalence of the wfst and the earley algorithm. norvig (1991) shows that the
ef   ciency offered by id145 can be captured in any language with a
memoization function (such as in lisp) simply by wrapping the memoization oper-
ation around a simple top-down parser.

while parsing via cascades of    nite-state automata had been common in the
early history of parsing (harris, 1962), the focus shifted to full id18 parsing quite
soon afterward. church (1980) argued for a return to    nite-state grammars as a
processing model for natural language understanding; other early    nite-state parsing
models include ejerhed (1988). abney (1991) argued for the important practical role
of id66.

the classic reference for parsing algorithms is aho and ullman (1972); although
the focus of that book is on computer languages, most of the algorithms have been
applied to natural language. a good programming languages textbook such as aho
et al. (1986) is also useful.

236 chapter 11

    syntactic parsing

exercises

11.1 implement the algorithm to convert arbitrary context-free grammars to cnf.

apply your program to the l1 grammar.

11.2 implement the cky algorithm and test it with your converted l1 grammar.
11.3 rewrite the cky algorithm given in fig. 11.5 on page 228 so that it can accept

grammars that contain unit productions.

11.4 discuss the relative advantages and disadvantages of partial versus full pars-

ing.

11.5 discuss how to augment a parser to deal with input that may be incorrect, for
example, containing spelling errors or mistakes arising from automatic speech
recognition.

chapter

12 statistical parsing

the characters in damon runyon   s short stories are willing to bet    on any propo-
sition whatever   , as runyon says about sky masterson in the idyll of miss sarah
brown, from the id203 of getting aces back-to-back to the odds against a man
being able to throw a peanut from second base to home plate. there is a moral here
for language processing: with enough knowledge we can    gure the id203 of
just about anything. the last two chapters have introduced sophisticated models of
syntactic structure and its parsing. here, we show that it is possible to build proba-
bilistic models of syntactic knowledge and use some of this probabilistic knowledge
to build ef   cient probabilistic parsers.

one crucial use of probabilistic parsing is to solve the problem of disambigua-
tion. recall from chapter 11 that sentences on average tend to be syntactically
ambiguous because of phenomena like coordination ambiguity and attachment
ambiguity. the cky parsing algorithm can represent these ambiguities in an ef   -
cient way but is not equipped to resolve them. a probabilistic parser offers a solution
to the problem: compute the id203 of each interpretation and choose the most
probable interpretation. thus, due to the prevalence of ambiguity, most modern
parsers used for natural language understanding tasks (semantic analysis, summa-
rization, question-answering, machine translation) are of necessity probabilistic.

the most commonly used probabilistic grammar formalism is the probabilistic
context-free grammar (pid18), a probabilistic augmentation of context-free gram-
mars in which each rule is associated with a id203. we introduce pid18s in the
next section, showing how they can be trained on treebank grammars and how they
can be parsed. we present the most basic parsing algorithm for pid18s, which is the
probabilistic version of the cky algorithm that we saw in chapter 11.

we then show a number of ways that we can improve on this basic id203
model (pid18s trained on treebank grammars). one method of improving a trained
treebank grammar is to change the names of the non-terminals. by making the
non-terminals sometimes more speci   c and sometimes more general, we can come
up with a grammar with a better id203 model that leads to improved parsing
scores. another augmentation of the pid18 works by adding more sophisticated
conditioning factors, extending pid18s to handle probabilistic subcategorization
information and probabilistic lexical dependencies.

heavily lexicalized grammar formalisms such as lexical-functional grammar
(lfg) (bresnan, 1982), head-driven phrase structure grammar (hpsg) (pollard
and sag, 1994), tree-adjoining grammar (tag) (joshi, 1985), and combinatory
categorial grammar (id35) pose additional problems for probabilistic parsers. sec-
tion 12.7 introduces the task of id55 and the use of heuristic search methods
based on the a* algorithm in the context of id35 parsing.

finally, we describe the standard techniques and metrics for evaluating parsers

and discuss some relevant psychological results on human parsing.

238 chapter 12

    statistical parsing

12.1 id140

pid18
sid18

the simplest augmentation of the context-free grammar is the probabilistic context-
free grammar (pid18), also known as the stochastic context-free grammar
(sid18),    rst proposed by booth (1969). recall that a context-free grammar g is
de   ned by four parameters (n,   , r, s); a probabilistic context-free grammar is also
de   ned by four parameters, with a slight augmentation to each of the rules in r:

n a set of non-terminal symbols (or variables)
   a set of terminal symbols (disjoint from n)
r a set of rules or productions, each of the form a        [p],

where a is a non-terminal,
   is a string of symbols from the in   nite set of strings (      n)   ,
and p is a number between 0 and 1 expressing p(  |a)

s a designated start symbol

that is, a pid18 differs from a standard id18 by augmenting each rule in r with

a id155:

a        [p]

(12.1)

here p expresses the id203 that the given non-terminal a will be expanded
to the sequence    . that is, p is the id155 of a given expansion   
given the left-hand-side (lhs) non-terminal a. we can represent this id203 as

or as

or as

p(a        )

p(a       |a)

thus, if we consider all the possible expansions of a non-terminal, the sum of their
probabilities must be 1:

p(rhs|lhs)
(cid:88)  

p(a        ) = 1

consistent

figure 12.1 shows a pid18: a probabilistic augmentation of the l1 miniature en-
glish id18 grammar and lexicon. note that the probabilities of all of the expansions
of each non-terminal sum to 1. also note that these probabilities were made up
for pedagogical purposes. a real grammar has a great many more rules for each
non-terminal; hence, the probabilities of any particular rule would tend to be much
smaller.

a pid18 is said to be consistent if the sum of the probabilities of all sentences
in the language equals 1. certain kinds of recursive rules cause a grammar to be
inconsistent by causing in   nitely looping derivations for some sentences. for ex-
ample, a rule s     s with id203 1 would lead to lost id203 mass due to
derivations that never terminate. see booth and thompson (1973) for more details
on consistent and inconsistent grammars.

12.1

    id140

239

grammar

lexicon

det     that [.10] | a [.30] | the [.60]
noun     book [.10] |    ight [.30]
| meal [.015] | money [.05]
|    ight [.40] | dinner [.10]
verb     book [.30] | include [.30]
| prefer [.40]
pronoun     i [.40] | she [.05]
| me [.15] | you [.40]
proper-noun     houston [.60]
| nwa [.40]
aux     does [.60] | can [40]
preposition     from [.30] | to [.30]

s     np vp
[.80]
s     aux np vp
[.15]
s     vp
[.05]
np     pronoun
[.35]
np     proper-noun
[.30]
np     det nominal
[.20]
np     nominal
[.15]
nominal     noun
[.75]
nominal     nominal noun [.20]
nominal     nominal pp
[.05]
[.35]
vp     verb
[.20]
vp     verb np
[.10]
vp     verb np pp
vp     verb pp
[.15]
vp     verb np np
[.05]
[.15]
vp     vp pp
pp     preposition np
[1.0]
figure 12.1 a pid18 that is a probabilistic augmentation of the l1 miniature english id18
grammar and lexicon of fig. 11.1. these probabilities were made up for pedagogical purposes
and are not based on a corpus (since any real corpus would have many more rules, so the true
probabilities of each rule would be much smaller).

| on [.20] | near [.15]
| through [.05]

how are pid18s used? a pid18 can be used to estimate a number of useful
probabilities concerning a sentence and its parse tree(s), including the id203 of
a particular parse tree (useful in disambiguation) and the id203 of a sentence
or a piece of a sentence (useful in id38). let   s see how this works.

12.1.1 pid18s for disambiguation
a pid18 assigns a id203 to each parse tree t (i.e., each derivation) of a sen-
tence s. this attribute is useful in disambiguation. for example, consider the two
parses of the sentence    book the dinner    ight    shown in fig. 12.2. the sensible
parse on the left means    book a    ight that serves dinner   . the nonsensical parse
on the right, however, would have to mean something like    book a    ight on behalf
of    the dinner       just as a structurally similar sentence like    can you book john a
   ight?    means something like    can you book a    ight on behalf of john?   

the id203 of a particular parse t is de   ned as the product of the probabil-
ities of all the n rules used to expand each of the n non-terminal nodes in the parse
tree t, where each rule i can be expressed as lhsi     rhsi:

p(t,s) =

n(cid:89)i=1

p(rhsi|lhsi)

(12.2)

the resulting id203 p(t,s) is both the joint id203 of the parse and the
sentence and also the id203 of the parse p(t ). how can this be true? first, by
the de   nition of joint id203:

p(t,s) = p(t )p(s|t )

(12.3)

240 chapter 12

    statistical parsing

but since a parse tree includes all the words of the sentence, p(s|t ) is 1. thus,
(12.4)

p(t,s) = p(t )p(s|t ) = p(t )

s

vp

s

vp

verb

np

verb

np

np

book

det

nominal

book

det

nominal

nominal

the

nominal

noun

the

noun

noun

noun

   ight

dinner

   ight

dinner

rules
    vp
    verb np
    det nominal

p
s
.05
vp
.20
np
.20
nominal     nominal noun .20
nominal     noun
.75
    book
verb
det
    the
noun     dinner
noun        ight

.30
.60
.10
.40

rules
p
    vp
.05
    verb np np .10
.20
    det nominal
    nominal
.15
.75
.75
.30
.60
.10
.40

s
vp
np
np
nominal     noun
nominal     noun
    book
verb
det
    the
noun     dinner
noun        ight

figure 12.2 two parse trees for an ambiguous sentence. the parse on the left corresponds
to the sensible meaning    book a    ight that serves dinner   , while the parse on the right corre-
sponds to the nonsensical meaning    book a    ight on behalf of    the dinner       .

we can compute the id203 of each of the trees in fig. 12.2 by multiplying
the probabilities of each of the rules used in the derivation. for example, the proba-
bility of the left tree in fig. 12.2a (call it tle f t) and the right tree (fig. 12.2b or tright)
can be computed as follows:

p(tle f t ) = .05    .20    .20    .20    .75    .30    .60    .10    .40 = 2.2   10   6
p(tright ) = .05    .10    .20    .15    .75    .75    .30    .60    .10    .40 = 6.1   10   7
we can see that the left tree in fig. 12.2 has a much higher id203 than the
tree on the right. thus, this parse would correctly be chosen by a disambiguation
algorithm that selects the parse with the highest pid18 id203.

let   s formalize this intuition that picking the parse with the highest id203
is the correct way to do disambiguation. consider all the possible parse trees for a
given sentence s. the string of words s is called the yield of any parse tree over s.

yield

12.1

    id140

241

thus, out of all parse trees with a yield of s, the disambiguation algorithm picks the
parse tree that is most probable given s:

  t (s) = argmax

t s.t.s=yield(t )

p(t|s)

(12.5)

by de   nition, the id203 p(t|s) can be rewritten as p(t,s)/p(s), thus lead-

ing to

  t (s) = argmax

t s.t.s=yield(t )

p(t,s)
p(s)

(12.6)

since we are maximizing over all parse trees for the same sentence, p(s) will be

a constant for each tree, so we can eliminate it:

  t (s) = argmax

t s.t.s=yield(t )

p(t,s)

(12.7)

furthermore, since we showed above that p(t,s) = p(t ), the    nal equation
for choosing the most likely parse neatly simpli   es to choosing the parse with the
highest id203:

  t (s) = argmax

t s.t.s=yield(t )

p(t )

(12.8)

12.1.2 pid18s for id38
a second attribute of a pid18 is that it assigns a id203 to the string of words
constituting a sentence. this is important in id38, whether for use
in id103, machine translation, id147, augmentative com-
munication, or other applications. the id203 of an unambiguous sentence is
p(t,s) = p(t ) or just the id203 of the single parse tree for that sentence. the
id203 of an ambiguous sentence is the sum of the probabilities of all the parse
trees for the sentence:

p(s) = (cid:88)t s.t.s=yield(t )
= (cid:88)t s.t.s=yield(t )

p(t,s)

p(t )

(12.9)

(12.10)

an additional feature of pid18s that is useful for id38 is their
ability to assign a id203 to substrings of a sentence. for example, suppose we
want to know the id203 of the next word wi in a sentence given all the words
we   ve seen so far w1, ...,wi   1. the general formula for this is

p(wi|w1,w2, ...,wi   1) =

p(w1,w2, ...,wi   1,wi)
p(w1,w2, ...,wi   1)

(12.11)

we saw in chapter 3 a simple approximation of this id203 using id165s,
conditioning on only the last word or two instead of the entire context; thus, the
bigram approximation would give us

p(wi|w1,w2, ...,wi   1)    

p(wi   1,wi)
p(wi   1)

(12.12)

242 chapter 12

    statistical parsing

but the fact that the id165 model can only make use of a couple words of
context means it is ignoring potentially useful prediction cues. consider predicting
the word after in the following sentence from chelba and jelinek (2000):
(12.13) the contract ended with a loss of 7 cents after trading as low as 9 cents
a trigram grammar must predict after from the words 7 cents, while it seems clear
that the verb ended and the subject contract would be useful predictors that a pid18-
based parser could help us make use of. indeed, it turns out that pid18s allow us to
condition on the entire previous context w1,w2, ...,wi   1 shown in eq. 12.11.
in summary, this section and the previous one have shown that pid18s can be
applied both to disambiguation in syntactic parsing and to word prediction in lan-
guage modeling. both of these applications require that we be able to compute the
id203 of parse tree t for a given sentence s. the next few sections introduce
some algorithms for computing this id203.

12.2 probabilistic cky parsing of pid18s

the parsing problem for pid18s is to produce the most-likely parse   t for a given
sentence s, that is,

  t (s) = argmax

t s.t.s=yield(t )

p(t )

(12.14)

probabilistic
cky

the algorithms for computing the most likely parse are simple extensions of the
standard algorithms for parsing; most modern probabilistic parsers are based on the
probabilistic cky algorithm,    rst described by ney (1991).

as with the cky algorithm, we assume for the probabilistic cky algorithm that
the pid18 is in chomsky normal form. recall from page 213 that grammars in cnf
are restricted to rules of the form a     b c, or a     w. that is, the right-hand side
of each rule must expand to either two non-terminals or to a single terminal.
for the cky algorithm, we represented each sentence as having indices between

the words. thus, an example sentence like
(12.15) book the    ight through houston.
would assume the following indices between each word:
(12.16)

0(cid:13) book 1(cid:13) the 2(cid:13)    ight 3(cid:13) through 4(cid:13) houston 5(cid:13)

using these indices, each constituent in the cky parse tree is encoded in a
two-dimensional matrix. speci   cally, for a sentence of length n and a grammar
that contains v non-terminals, we use the upper-triangular portion of an (n + 1)  
(n + 1) matrix. for cky, each cell table[i, j] contained a list of constituents that
could span the sequence of words from i to j. for probabilistic cky, it   s slightly
simpler to think of the constituents in each cell as constituting a third dimension of
maximum length v . this third dimension corresponds to each non-terminal that can
be placed in this cell, and the value of the cell is then a id203 for that non-
terminal/constituent rather than a list of constituents. in summary, each cell [i, j,a]
in this (n + 1)   (n + 1)  v matrix is the id203 of a constituent of type a that
spans positions i through j of the input.
figure 12.3 gives pseudocode for this probabilistic cky algorithm, extending

the basic cky algorithm from fig. 11.5.

12.3

    ways to learn pid18 rule probabilities

243

function probabilistic-cky(words,grammar) returns most probable parse

and its id203

for j   from 1 to length(words) do

for all { a | a     words[ j]     grammar}
for i   from j    2 downto 0 do

table[ j    1, j,a]   p(a     words[ j])
for k   i + 1 to j    1 do

for all { a | a     bc     grammar,

and table[i,k,b] > 0 and table[k, j,c] > 0 }

if (table[i,j,a] < p(a     bc)    table[i,k,b]    table[k,j,c]) then

table[i,j,a]   p(a     bc)    table[i,k,b]    table[k,j,c]
back[i,j,a]   {k,b,c}

return build tree(back[1, length(words), s]), table[1, length(words), s]

figure 12.3 the probabilistic cky algorithm for    nding the maximum id203 parse
of a string of num words words given a pid18 grammar with num rules rules in chomsky
normal form. back is an array of backpointers used to recover the best parse. the build tree
function is left as an exercise to the reader.

like the basic cky algorithm, the probabilistic cky algorithm as shown in
fig. 12.3 requires a grammar in chomsky normal form. converting a probabilistic
grammar to cnf requires that we also modify the probabilities so that the id203
of each parse remains the same under the new cnf grammar. exercise 12.2 asks
you to modify the algorithm for conversion to cnf in chapter 11 so that it correctly
handles rule probabilities.

in practice, a generalized cky algorithm that handles unit productions directly
is typically used. recall that exercise 13.3 asked you to make this change in cky;
exercise 12.3 asks you to extend this change to probabilistic cky.

let   s see an example of the probabilistic cky chart, using the following mini-

grammar, which is already in cnf:

s     np vp
.80
np     det n
.30
v p     v np
.20
v     includes .05

det     the
det     a
n     meal
n     f light

.40
.40
.01
.02

given this grammar, fig. 12.4 shows the    rst steps in the probabilistic cky

parse of the following example:
(12.17) the    ight includes a meal

12.3 ways to learn pid18 rule probabilities

where do pid18 rule probabilities come from? there are two ways to learn proba-
bilities for the rules of a grammar. the simplest way is to use a treebank, a corpus
of already parsed sentences. recall that we introduced in chapter 10 the idea of
treebanks and the commonly used id32 (marcus et al., 1993), a collec-
tion of parse trees in english, chinese, and other languages that is distributed by the
linguistic data consortium. given a treebank, we can compute the id203 of
each expansion of a non-terminal by counting the number of times that expansion

244 chapter 12

    statistical parsing

figure 12.4 the beginning of the probabilistic cky matrix. filling out the rest of the chart
is left as exercise 12.4 for the reader.

occurs and then normalizing.

p(         |  ) =

count(          )
(cid:80)   count(         )

=

count(          )

count(  )

(12.18)

if we don   t have a treebank but we do have a (non-probabilistic) parser, we can
generate the counts we need for computing pid18 rule probabilities by    rst parsing
a corpus of sentences with the parser. if sentences were unambiguous, it would be
as simple as this: parse the corpus, increment a counter for every rule in the parse,
and then normalize to get probabilities.

but wait! since most sentences are ambiguous, that is, have multiple parses, we
don   t know which parse to count the rules in. instead, we need to keep a separate
count for each parse of a sentence and weight each of these partial counts by the
id203 of the parse it appears in. but to get these parse probabilities to weight
the rules, we need to already have a probabilistic parser.

the intuition for solving this chicken-and-egg problem is to incrementally im-
prove our estimates by beginning with a parser with equal rule probabilities, then
parse the sentence, compute a id203 for each parse, use these probabilities to

theflight[0,1][0,2][0,3][1,2][1,3][2,3]det: .40includesameal[3,4][4,5]n: .02v: .05np: .30 *.40 *.02= .0024[0,4][1,4][2,4][3,5][2,5][1,5][0,5]det: .40n: .01inside-outside

expectation
step
maximization
step

12.4

    problems with pid18s

245

weight the counts, re-estimate the rule probabilities, and so on, until our proba-
bilities converge. the standard algorithm for computing this solution is called the
inside-outside algorithm; it was proposed by baker (1979) as a generalization of the
forward-backward algorithm for id48s. like forward-backward, inside-outside is
a special case of the expectation maximization (em) algorithm, and hence has two
steps: the expectation step, and the maximization step. see lari and young (1990)
or manning and sch  utze (1999) for a complete description of the algorithm.

this use of the inside-outside algorithm to estimate the rule probabilities for
a grammar is actually a kind of limited use of inside-outside. the inside-outside
algorithm can actually be used not only to set the rule probabilities but even to induce
the grammar rules themselves. it turns out, however, that grammar induction is so
dif   cult that inside-outside by itself is not a very successful grammar inducer; see
the historical notes at the end of the chapter for pointers to other grammar induction
algorithms.

12.4 problems with pid18s

while id140 are a natural extension to context-free
grammars, they have two main problems as id203 estimators:
poor independence assumptions: id18 rules impose an independence assumption
on probabilities, resulting in poor modeling of structural dependencies across
the parse tree.

lack of lexical conditioning: id18 rules don   t model syntactic facts about speci   c
words, leading to problems with subcategorization ambiguities, preposition
attachment, and coordinate structure ambiguities.

because of these problems, most current probabilistic parsing models use some
augmented version of pid18s, or modify the treebank-based grammar in some way.
in the next few sections after discussing the problems in more detail we introduce
some of these augmentations.

12.4.1

independence assumptions miss structural dependencies
between rules

let   s look at these problems in more detail. recall that in a id18 the expansion of a
non-terminal is independent of the context, that is, of the other nearby non-terminals
in the parse tree. similarly, in a pid18, the id203 of a particular rule like
np     det n is also independent of the rest of the tree. by de   nition, the id203
of a group of independent events is the product of their probabilities. these two facts
explain why in a pid18 we compute the id203 of a tree by just multiplying the
probabilities of each non-terminal expansion.

unfortunately, this id18 independence assumption results in poor id203
estimates. this is because in english the choice of how a node expands can after all
depend on the location of the node in the parse tree. for example, in english it turns
out that nps that are syntactic subjects are far more likely to be pronouns, and nps
that are syntactic objects are far more likely to be non-pronominal (e.g., a proper
noun or a determiner noun sequence), as shown by these statistics for nps in the

246 chapter 12

    statistical parsing

switchboard corpus (francis et al., 1999):1

pronoun non-pronoun

subject 91%
object 34%

9%
66%

unfortunately, there is no way to represent this contextual difference in the prob-
abilities in a pid18. consider two expansions of the non-terminal np as a pronoun
or as a determiner+noun. how shall we set the probabilities of these two rules? if
we set their probabilities to their overall id203 in the switchboard corpus, the
two rules have about equal id203.

np     dt nn .28
.25
np     prp

because pid18s don   t allow a rule id203 to be conditioned on surrounding
context, this equal id203 is all we get; there is no way to capture the fact that in
subject position, the id203 for np     prp should go up to .91, while in object
position, the id203 for np     dt nn should go up to .66.
these dependencies could be captured if the id203 of expanding an np as
a pronoun (e.g., np     prp) versus a lexical np (e.g., np     dt nn) were condi-
tioned on whether the np was a subject or an object. section 12.5 introduces the
technique of parent annotation for adding this kind of conditioning.

12.4.2 lack of sensitivity to lexical dependencies
a second class of problems with pid18s is their lack of sensitivity to the words in
the parse tree. words do play a role in pid18s since the parse id203 includes
the id203 of a word given a part-of-speech (i.e., from rules like v     sleep,
nn     book, etc.).
but it turns out that lexical information is useful in other places in the grammar,
such as in resolving prepositional phrase (pp) attachment ambiguities. since prepo-
sitional phrases in english can modify a noun phrase or a verb phrase, when a parser
   nds a prepositional phrase, it must decide where to attach it into the tree. consider
the following example:
(12.19) workers dumped sacks into a bin.

figure 12.5 shows two possible parse trees for this sentence; the one on the left is
the correct parse; fig. 12.6 shows another perspective on the preposition attachment
problem, demonstrating that resolving the ambiguity in fig. 12.5 is equivalent to
deciding whether to attach the prepositional phrase into the rest of the tree at the
np or vp nodes; we say that the correct parse requires vp attachment, and the
incorrect parse implies np attachment.

why doesn   t a pid18 already deal with pp attachment ambiguities? note that
the two parse trees in fig. 12.5 have almost exactly the same rules; they differ only
in that the left-hand parse has this rule:

v p     v bd np pp

1 distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sen-
tences. this tendency is caused by the use of subject position to realize the topic or old information
in a sentence (giv  on, 1990). pronouns are a way to talk about old information, while non-pronominal
(   lexical   ) noun-phrases are often used to introduce new referents. we talk more about new and old
information in chapter 21.

vp attachment
np attachment

12.4

    problems with pid18s

247

s

s

np

vp

np

vp

nns

vbd

np

pp

nns

vbd

np

workers

dumped

nns

p

np

workers

dumped

np

pp

sacks

into

dt

nn

nns

p

np

a

bin

sacks

into

dt

nn

figure 12.5 two possible parse trees for a prepositional phrase attachment ambiguity. the left parse is
the sensible one, in which    into a bin    describes the resulting location of the sacks. in the right incorrect parse,
the sacks to be dumped are the ones which are already    into a bin   , whatever that might mean.

a

bin

s

np

vp

nns

vbd

np

workers

dumped

nns

sacks

pp

p

np

into

dt

nn

a

bin

figure 12.6 another view of the preposition attachment problem. should the pp on the right attach to the
vp or np nodes of the partial parse tree on the left?

while the right-hand parse has these:

v p     v bd np
np     np pp

depending on how these probabilities are set, a pid18 will always either prefer
np attachment or vp attachment. as it happens, np attachment is slightly more
common in english, so if we trained these rule probabilities on a corpus, we might
always prefer np attachment, causing us to misparse this sentence.

but suppose we set the probabilities to prefer the vp attachment for this sen-
tence. now we would misparse the following sentence, which requires np attach-
ment:
(12.20)    shermen caught tons of herring

248 chapter 12

    statistical parsing

lexical
dependency

what information in the input sentence lets us know that (12.20) requires np

attachment while (12.19) requires vp attachment?

it should be clear that these preferences come from the identities of the verbs,
nouns, and prepositions. it seems that the af   nity between the verb dumped and the
preposition into is greater than the af   nity between the noun sacks and the preposi-
tion into, thus leading to vp attachment. on the other hand, in (12.20) the af   nity
between tons and of is greater than that between caught and of, leading to np attach-
ment.

thus, to get the correct parse for these kinds of examples, we need a model that
somehow augments the pid18 probabilities to deal with these lexical dependency
statistics for different verbs and prepositions.

coordination ambiguities are another case in which lexical dependencies are
the key to choosing the proper parse. figure 12.7 shows an example from collins
(1999) with two parses for the phrase dogs in houses and cats. because dogs is
semantically a better conjunct for cats than houses (and because most dogs can   t    t
inside cats), the parse [dogs in [np houses and cats]] is intuitively unnatural and
should be dispreferred. the two parses in fig. 12.7, however, have exactly the same
pid18 rules, and thus a pid18 will assign them the same id203.

np

np

np

conj

np

np

pp

np

pp

and

noun

noun

prep

np

noun

prep

np

cats

dogs

in

np

conj

np

dogs

in

noun

houses

noun

and

noun

houses

cats

figure 12.7 an instance of coordination ambiguity. although the left structure is intu-
itively the correct one, a pid18 will assign them identical probabilities since both structures
use exactly the same set of rules. after collins (1999).

in summary, we have shown in this section and the previous one that probabilistic
context-free grammars are incapable of modeling important structural and lexical
dependencies. in the next two sections we sketch current methods for augmenting
pid18s to deal with both these issues.

12.5

improving pid18s by splitting non-terminals

let   s start with the    rst of the two problems with pid18s mentioned above: their
inability to model structural dependencies, like the fact that nps in subject position
tend to be pronouns, whereas nps in object position tend to have full lexical (non-
pronominal) form. how could we augment a pid18 to correctly model this fact?
one idea would be to split the np non-terminal into two versions: one for sub-

split

parent
annotation

12.5

   

improving pid18s by splitting non-terminals

249

jects, one for objects. having two nodes (e.g., npsubject and npobject) would allow
us to correctly model their different distributional properties, since we would have
different probabilities for the rule npsubject     prp and the rule npobject     prp.
one way to implement this intuition of splits is to do parent annotation (john-
son, 1998), in which we annotate each node with its parent in the parse tree. thus,
an np node that is the subject of the sentence and hence has parent s would be anno-
tated np  s, while a direct object np whose parent is vp would be annotated np  vp.
figure 12.8 shows an example of a tree produced by a grammar that parent-annotates
the phrasal non-terminals (like np and vp).

a)

s

b)

s

np

vp

np  s

vp  s

prp

vbd

np

prp

vbd

np  vp

i

need

dt

nn

i

need

dt

nn

a

   ight

a

   ight

figure 12.8 a standard pid18 parse tree (a) and one which has parent annotation on the
nodes which aren   t pre-terminal (b). all the non-terminal nodes (except the pre-terminal
part-of-speech nodes) in parse (b) have been annotated with the identity of their parent.

in addition to splitting these phrasal nodes, we can also improve a pid18 by
splitting the pre-terminal part-of-speech nodes (klein and manning, 2003b). for ex-
ample, different kinds of adverbs (rb) tend to occur in different syntactic positions:
the most common adverbs with advp parents are also and now, with vp parents
n   t and not, and with np parents only and just. thus, adding tags like rb  advp,
rb  vp, and rb  np can be useful in improving pid18 modeling.

similarly, the id32 tag in can mark a wide variety of parts-of-speech,
including subordinating conjunctions (while, as, if), complementizers (that, for), and
prepositions (of, in, from). some of these differences can be captured by parent an-
notation (subordinating conjunctions occur under s, prepositions under pp), while
others require speci   cally splitting the pre-terminal nodes. figure 12.9 shows an ex-
ample from klein and manning (2003b) in which even a parent-annotated grammar
incorrectly parses works as a noun in to see if advertising works. splitting pre-
terminals to allow if to prefer a sentential complement results in the correct verbal
parse.

to deal with cases in which parent annotation is insuf   cient, we can also hand-
write rules that specify a particular node split based on other features of the tree. for
example, to distinguish between complementizer in and subordinating conjunction
in, both of which can have the same parent, we could write rules conditioned on
other aspects of the tree such as the lexical identity (the lexeme that is likely to be a
complementizer, as a subordinating conjunction).

node-splitting is not without problems; it increases the size of the grammar and
hence reduces the amount of training data available for each grammar rule, leading
to over   tting. thus, it is important to split to just the correct level of granularity for a
particular training set. while early models employed hand-written rules to try to    nd
an optimal number of non-terminals (klein and manning, 2003b), modern models

250 chapter 12

    statistical parsing

split and merge

automatically search for the optimal splits. the split and merge algorithm of petrov
et al. (2006), for example, starts with a simple x-bar grammar, alternately splits the
non-terminals, and merges non-terminals,    nding the set of annotated nodes that
maximizes the likelihood of the training set treebank. as of the time of this writing,
the performance of the petrov et al. (2006) algorithm was the best of any known
parsing algorithm on the id32.

12.6 probabilistic lexicalized id18s

the previous section showed that a simple probabilistic cky algorithm for pars-
ing raw pid18s can achieve extremely high parsing accuracy if the grammar rule
symbols are redesigned by automatic splits and merges.

in this section, we discuss an alternative family of models in which instead of
modifying the grammar rules, we modify the probabilistic model of the parser to
allow for lexicalized rules. the resulting family of lexicalized parsers includes the
well-known collins parser (collins, 1999) and charniak parser (charniak, 1997),
both of which are publicly available and widely used throughout natural language
processing.

we saw in section 10.4.3 that syntactic constituents could be associated with a
lexical head, and we de   ned a lexicalized grammar in which each non-terminal
in the tree is annotated with its lexical head, where a rule like v p     v bd np pp
would be extended as

collins parser
charniak
parser

lexicalized
grammar

vp(dumped)     vbd(dumped) np(sacks) pp(into)

(12.21)

head tag

in the standard type of lexicalized grammar, we actually make a further exten-
sion, which is to associate the head tag, the part-of-speech tags of the headwords,
with the non-terminal symbols as well. each rule is thus lexicalized by both the

vp  s

vp  s

to

vp  vp

to  vp

vp  vp

to

vb

pp  vp

to

vb  vp

sbar  vp

see

in

np  pp

see

in  sbar

s  sbar

if

nn

nns

if

np  s

vp  s

advertising

works

nn  np

vbz  vp

figure 12.9 an incorrect parse even with a parent-annotated parse (left). the correct parse (right), was
produced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to
capture the fact that if prefers sentential complements. adapted from klein and manning (2003b).

advertising

works

12.6

    probabilistic lexicalized id18s

251

headword and the head tag of each constituent resulting in a format for lexicalized
rules like

vp(dumped,vbd)     vbd(dumped,vbd) np(sacks,nns) pp(into,p)

(12.22)

we show a lexicalized parse tree with head tags in fig. 12.10, extended from fig. 10.11.

top

s(dumped,vbd)

np(workers,nns)

vp(dumped,vbd)

nns(workers,nns)

vbd(dumped,vbd)

np(sacks,nns)

pp(into,p)

workers

dumped

nns(sacks,nns)

p(into,p)

np(bin,nn)

sacks

into

dt(a,dt)

nn(bin,nn)

a

bin

internal rules
top
    s(dumped,vbd)
s(dumped,vbd)     np(workers,nns)
np(workers,nns)     nns(workers,nns)
vp(dumped,vbd)     vbd(dumped, vbd) np(sacks,nns) pp(into,p) p(into,p)
dt(a,dt)
pp(into,p)
np(bin,nn)
nn(bin,nn)

np(bin,nn)
nn(bin,nn)

vp(dumped,vbd)

    p(into,p)
    dt(a,dt)

lexical rules
nns(workers,nns)     workers
vbd(dumped,vbd)     dumped
nns(sacks,nns)

    sacks
    into
    a
    bin

figure 12.10 a lexicalized tree, including head tags, for a wsj sentence, adapted from collins (1999). below
we show the pid18 rules that would be needed for this parse tree, internal rules on the left, and lexical rules on
the right.

to generate such a lexicalized tree, each pid18 rule must be augmented to iden-
tify one right-hand constituent to be the head daughter. the headword for a node is
then set to the headword of its head daughter, and the head tag to the part-of-speech
tag of the headword. recall that we gave in fig. 10.12 a set of hand-written rules for
identifying the heads of particular constituents.

a natural way to think of a lexicalized grammar is as a parent annotation, that
is, as a simple context-free grammar with many copies of each rule, one copy for
each possible headword/head tag for each constituent. thinking of a probabilistic
lexicalized id18 in this way would lead to the set of simple pid18 rules shown below
the tree in fig. 12.10.

note that fig. 12.10 shows two kinds of rules:

lexical rules, which express
the expansion of a pre-terminal to a word, and internal rules, which express the
other rule expansions. we need to distinguish these kinds of rules in a lexicalized
grammar because they are associated with very different kinds of probabilities. the
lexical rules are deterministic, that is, they have id203 1.0 since a lexicalized
pre-terminal like nn(bin,nn) can only expand to the word bin. but for the internal
rules, we need to estimate probabilities.

lexical rules
internal rules

252 chapter 12

    statistical parsing

suppose we were to treat a probabilistic lexicalized id18 like a really big id18
that just happened to have lots of very complex non-terminals and estimate the
probabilities for each rule from maximum likelihood estimates. thus, according
to eq. 12.18, the id113 estimate for the id203 for the rule p(vp(dumped,vbd)
    vbd(dumped, vbd) np(sacks,nns) pp(into,p)) would be

count(vp(dumped,vbd)     vbd(dumped, vbd) np(sacks,nns) pp(into,p))

count(vp(dumped,vbd))

(12.23)

but there   s no way we can get good estimates of counts like those in (12.23)
because they are so speci   c: we   re unlikely to see many (or even any) instances of a
sentence with a verb phrase headed by dumped that has one np argument headed by
sacks and a pp argument headed by into. in other words, counts of fully lexicalized
pid18 rules like this will be far too sparse, and most rule probabilities will come out
0.

the idea of lexicalized parsing is to make some further independence assump-

tions to break down each rule so that we would estimate the id203

p(vp(dumped,vbd)     vbd(dumped, vbd) np(sacks,nns) pp(into,p))

as the product of smaller independent id203 estimates for which we could
acquire reasonable counts. the next section summarizes one such method, the
collins parsing method.

12.6.1 the collins parser
modern statistical parsers differ in exactly which independence assumptions they
make. in this section we describe a simpli   ed version of collins   s worth knowing
about; see the summary at the end of the chapter.

the    rst intuition of the collins parser is to think of the right-hand side of every
(internal) id18 rule as consisting of a head non-terminal, together with the non-
terminals to the left of the head and the non-terminals to the right of the head. in the
abstract, we think about these rules as follows:

lhs     ln ln   1 ...l1 h r1 ...rn   1 rn

(12.24)

since this is a lexicalized grammar, each of the symbols like l1 or r3 or h or
lhs is actually a complex symbol representing the category and its head and head
tag, like vp(dumped,vp) or np(sacks,nns).

now, instead of computing a single id113 id203 for this rule, we are going
to break down this rule via a neat generative story, a slight simpli   cation of what is
called collins model 1. this new generative story is that given the left-hand side,
we    rst generate the head of the rule and then generate the dependents of the head,
one by one, from the inside out. each of these generation steps will have its own
id203.

we also add a special stop non-terminal at the left and right edges of the rule;
this non-terminal allows the model to know when to stop generating dependents on a
given side. we generate dependents on the left side of the head until we   ve generated
stop on the left side of the head, at which point we move to the right side of the
head and start generating dependents there until we generate stop. so it   s as if we

12.6

    probabilistic lexicalized id18s

253

are generating a rule augmented as follows:

p(vp(dumped,vbd)    

stop vbd(dumped, vbd) np(sacks,nns) pp(into,p) stop)

(12.25)

let   s see the generative story for this augmented rule. we make use of three
kinds of probabilities: ph for generating heads, pl for generating dependents on the
left, and pr for generating dependents on the right.

1. generate the head vbd(dumped,vbd) with id203
p(h|lhs) = p(vbd(dumped,vbd) | vp(dumped,vbd))

2. generate the left dependent (which is stop, since there isn   t
one) with id203
p(stop| vp(dumped,vbd) vbd(dumped,vbd))

3. generate right dependent np(sacks,nns) with id203
pr(np(sacks,nns| vp(dumped,vbd), vbd(dumped,vbd))

4. generate the right dependent pp(into,p) with id203
pr(pp(into,p) | vp(dumped,vbd), vbd(dumped,vbd))

stop

vp(dumped,vbd)

vbd(dumped,vbd)

vp(dumped,vbd)

stop

vbd(dumped,vbd)

vp(dumped,vbd)

stop

vbd(dumped,vbd)

np(sacks,nns)

vp(dumped,vbd)

vbd(dumped,vbd)

np(sacks,nns)

pp(into,p)

vp(dumped,vbd)

5) generate the right dependent stop with id203
pr(stop | vp(dumped,vbd), vbd(dumped,vbd))

stop

vbd(dumped,vbd)

np(sacks,nns)

pp(into,p)

stop

in summary, the id203 of this rule

p(vp(dumped,vbd)    

vbd(dumped, vbd) np(sacks,nns) pp(into,p))

is estimated as

ph(vbd|vp, dumped)    pl(stop|vp,vbd,dumped)

   pr(np(sacks,nns)|vp,vbd,dumped)
   pr(pp(into,p)|vp,vbd,dumped)
   pr(stop|vp,vbd,dumped)

(12.26)

(12.27)

each of these probabilities can be estimated from much smaller amounts of data
than the full id203 in (12.26). for example, the maximum likelihood estimate

254 chapter 12

    statistical parsing

for the component id203 pr(np(sacks,nns)|vp,vbd,dumped) is

count( vp(dumped,vbd) with nns(sacks)as a daughter somewhere on the right )

count( vp(dumped,vbd) )

(12.28)
these counts are much less subject to sparsity problems than are complex counts
like those in (12.26).

more generally, if h is a head with head word hw and head tag ht, lw/lt and
rw/rt are the word/tag on the left and right respectively, and p is the parent, then the
id203 of an entire rule can be expressed as follows:

1. generate the head of the phrase h(hw,ht) with id203:

2. generate modi   ers to the left of the head with total id203

ph (h(hw,ht)|p,hw,ht)

pl(li(lwi,lti)|p,h,hw,ht)

such that ln+1(lwn+1,ltn+1) =stop, and we stop generating once we   ve gen-
erated a stop token.

3. generate modi   ers to the right of the head with total id203:

pp(ri(rwi,rti)|p,h,hw,ht)

such that rn+1(rwn+1,rtn+1) = stop, and we stop generating once we   ve
generated a stop token.

n+1(cid:89)i=1

n+1(cid:89)i=1

12.6.2 advanced: further details of the collins parser
the actual collins parser models are more complex (in a couple of ways) than the
simple model presented in the previous section. collins model 1 includes a distance
feature. thus, instead of computing pl and pr as follows,

distance

pl(li(lwi,lti)|p,h,hw,ht)
pr(ri(rwi,rti)|p,h,hw,ht)
collins model 1 conditions also on a distance feature:

pl(li(lwi,lti)|p,h,hw,ht,distancel(i    1))
pr(ri(rwi,rti)|p,h,hw,ht,distancer(i    1))

(12.29)
(12.30)

(12.31)
(12.32)

the distance measure is a function of the sequence of words below the previous
modi   ers (i.e., the words that are the yield of each modi   er non-terminal we have
already generated on the left).

the simplest version of this distance measure is just a tuple of two binary fea-
tures based on the surface string below these previous dependencies: (1) is the string
of length zero? (i.e., were no previous words generated?) (2) does the string contain
a verb?

12.7

    probabilistic id35 parsing

255

collins model 2 adds more sophisticated features, conditioning on subcatego-

rization frames for each verb and distinguishing arguments from adjuncts.

finally, smoothing is as important for statistical parsers as it was for id165
models. this is particularly true for lexicalized parsers, since the lexicalized rules
will otherwise condition on many lexical items that may never occur in training
(even using the collins or other methods of independence assumptions).

consider the id203 pr(ri(rwi,rti)|p,hw,ht). what do we do if a particular
right-hand constituent never occurs with this head? the collins model addresses this
problem by interpolating three backed-off models: fully lexicalized (conditioning on
the headword), backing off to just the head tag, and altogether unlexicalized.
backoff pr(ri(rwi,rti|...)
1
2
3

example
pr(np(sacks,nns)|vp, vbd, dumped)
pr(np(sacks,nns)|v p,v bd)
pr(np(sacks,nns)|v p)

pr(ri(rwi,rti)|p,hw,ht)
pr(ri(rwi,rti)|p,ht)
pr(ri(rwi,rti)|p)

similar backoff models are built also for pl and ph. although we   ve used the
word    backoff   , in fact these are not backoff models but interpolated models. the
three models above are linearly interpolated, where e1, e2, and e3 are the maximum
likelihood estimates of the three backoff models above:

(12.33)
the values of   1and  2 are set to implement witten-bell discounting (witten and

pr(...) =   1e1 + (1      1)(  2e2 + (1      2)e3)

bell, 1991) following bikel et al. (1997).

the collins model deals with unknown words by replacing any unknown word
in the test set, and any word occurring less than six times in the training set, with a
special unknown word token. unknown words in the test set are assigned a part-
of-speech tag in a preprocessing step by the ratnaparkhi (1996) tagger; all other
words are tagged as part of the parsing process.

the parsing algorithm for the collins model is an extension of probabilistic
cky; see collins (2003a). extending the cky algorithm to handle basic lexicalized
probabilities is left as exercises 14.5 and 14.6 for the reader.

12.7 probabilistic id35 parsing

lexicalized grammar frameworks such as id35 pose problems for which the phrase-
based methods we   ve been discussing are not particularly well-suited. to quickly
review, id35 consists of three major parts: a set of categories, a lexicon that asso-
ciates words with categories, and a set of rules that govern how categories combine
in context. categories can be either atomic elements, such as s and np, or functions
such as (s\np)/np which speci   es the transitive verb category. rules specify how
functions, their arguments, and other functions combine. for example, the following
rule templates, forward and backward function application, specify the way that
functions apply to their arguments.

x/y y     x
y x\y     x

the    rst rule applies a function to its argument on the right, while the second
looks to the left for its argument. the result of applying either of these rules is the

256 chapter 12

    statistical parsing

category speci   ed as the value of the function being applied. for the purposes of
this discussion, we   ll rely on these two rules along with the forward and backward
composition rules and type-raising, as described in chapter 10.

12.7.1 ambiguity in id35
as is always the case in parsing, managing ambiguity is the key to successful id35
parsing. the dif   culties with id35 parsing arise from the ambiguity caused by the
large number of complex lexical categories combined with the very general nature of
the grammatical rules. to see some of the ways that ambiguity arises in a categorial
framework, consider the following example.

(12.34) united diverted the    ight to reno.

our grasp of the role of the    ight in this example depends on whether the prepo-
sitional phrase to reno is taken as a modi   er of the    ight, as a modi   er of the entire
verb phrase, or as a potential second argument to the verb divert. in a context-free
grammar approach, this ambiguity would manifest itself as a choice among the fol-
lowing rules in the grammar.

nominal     nominal pp

vp     vp pp
vp     verb np pp

in a phrase-structure approach we would simply assign the word to to the cate-
gory p allowing it to combine with reno to form a prepositional phrase. the sub-
sequent choice of grammar rules would then dictate the ultimate derivation. in the
categorial approach, we can associate to with distinct categories to re   ect the ways
in which it might interact with other elements in a sentence. the fairly abstract
combinatoric rules would then sort out which derivations are possible. therefore,
the source of ambiguity arises not from the grammar but rather from the lexicon.

let   s see how this works by considering several possible derivations for this
example. to capture the case where the prepositional phrase to reno modi   es the
   ight, we assign the preposition to the category (np\np)/np, which gives rise to
the following derivation.

united

np

the

diverted
reno
(s\np)/np np/n n (np\np)/np np

   ight

to

>

>

np\np

np

np

s\np

s

<

>

<

here, the category assigned to to expects to    nd two arguments: one to the right as
with a traditional preposition, and one to the left that corresponds to the np to be
modi   ed.

alternatively, we could assign to to the category (s\s)/np, which permits the

following derivation where to reno modi   es the preceding verb phrase.

12.7

    probabilistic id35 parsing
the

   ight

257

united

np

diverted
reno
(s\np)/np np/n n (s\s)/np np

to

>

>

s\np

np

>

s\s

s\np

s

<b

<

a third possibility is to view divert as a ditransitive verb by assigning it to the
category ((s\np)/pp)/np, while treating to reno as a simple prepositional phrase.

united

np

diverted

reno
((s\np)/pp)/np np/n n pp/np np

   ight

the

to

>

np

>

>

pp

(s\np)/pp

s\np

s

>

<

while id35 parsers are still subject to ambiguity arising from the choice of
grammar rules, including the kind of spurious ambiguity discussed in chapter 10,
it should be clear that the choice of lexical categories is the primary problem to be
addressed in id35 parsing.

12.7.2 id35 parsing frameworks
since the rules in combinatory grammars are either binary or unary, a bottom-up,
tabular approach based on the cky algorithm should be directly applicable to id35
parsing. recall from fig. 12.3 that pcky employs a table that records the location,
category and id203 of all valid constituents discovered in the input. given an
appropriate id203 model for id35 derivations, the same kind of approach can
work for id35 parsing.

unfortunately, the large number of lexical categories available for each word,
combined with the promiscuity of id35   s combinatoric rules, leads to an explosion
in the number of (mostly useless) constituents added to the parsing table. the key
to managing this explosion of zombie constituents is to accurately assess and ex-
ploit the most likely lexical categories possible for each word     a process called
id55.

the following sections describe two approaches to id35 parsing that make use of
supertags. section 12.7.4, presents an approach that structures the parsing process
as a heuristic search through the use of the a* algorithm. the following section
then brie   y describes a more traditional maximum id178 approach that manages
the search space complexity through the use of adaptive id55     a process
that iteratively considers more and more tags until a parse is found.

id55

12.7.3 id55
chapter 8 introduced the task of part-of-speech tagging, the process of assigning the
correct lexical category to each word in a sentence. id55 is the correspond-
ing task for highly lexicalized grammar frameworks, where the assigned tags often
dictate much of the derivation for a sentence.

258 chapter 12

    statistical parsing

id35 supertaggers rely on treebanks such as id35bank to provide both the over-
all set of lexical categories as well as the allowable category assignments for each
word in the lexicon. id35bank includes over 1000 lexical categories, however, in
practice, most supertaggers limit their tagsets to those tags that occur at least 10
times in the training corpus. this results in an overall total of around 425 lexical
categories available for use in the lexicon. note that even this smaller number is
large in contrast to the 45 pos types used by the id32 tagset.

as with traditional part-of-speech tagging, the standard approach to building a
id35 supertagger is to use supervised machine learning to build a sequence classi-
   er using labeled training data. a common approach is to use the maximum id178
markov model (memm), as described in chapter 8, to    nd the most likely sequence
of tags given a sentence. the features in such a model consist of the current word
wi, its surrounding words within l words wi+l
i   l, as well as the k previously assigned
supertags ti   1
i   k . this type of model is summarized in the following equation from
chapter 8. training by maximizing log-likelihood of the training corpus and decod-
ing via the viterbi algorithm are the same as described in chapter 8.

  t = argmax

= argmax

= argmax

t

p(t|w )
t (cid:89)i
t (cid:89)i

p(ti|wi+l

i   l,ti   1
i   k )

exp(cid:32)(cid:88)i
exp(cid:32)(cid:88)i
(cid:88)t(cid:48)   tagset

wi fi(ti,wi+l

i   k )(cid:33)
i   l,ti   1
i   k )(cid:33)
i   l,ti   1

wi fi(t(cid:48),wi+l

(12.35)

word and tag-based features with k and l both set to 2 provides reasonable results
given suf   cient training data. additional features such as pos tags and short char-
acter suf   xes are also commonly used to improve performance.

unfortunately, even with additional features the large number of possible su-
pertags combined with high per-word ambiguity leads to error rates that are too
high for practical use in a parser. more speci   cally, the single best tag sequence
  t will typically contain too many incorrect tags for effective parsing to take place.
to overcome this, we can instead return a id203 distribution over the possible
supertags for each word in the input. the following table illustrates an example dis-
tribution for a simple example sentence. in this table, each column represents the
id203 of each supertag for a given word in the context of the input sentence.
the    ...    represent all the remaining supertags possible for each word.

serves

united
denver
n/n: 0.4 (s\np)/np: 0.8 np: 0.9
n/n: 0.05
np: 0.3
s/s: 0.1
s\s: .05

n: 0.1

...

...

...

in a memm framework, the id203 of the optimal tag sequence de   ned in
eq. 12.35 is ef   ciently computed with a suitably modi   ed version of the viterbi
algorithm. however, since viterbi only    nds the single best tag sequence it doesn   t

12.7

    probabilistic id35 parsing

259

provide exactly what we need here; we need to know the id203 of each pos-
sible word/tag pair. the id203 of any given tag for a word is the sum of the
probabilities of all the supertag sequences that contain that tag at that location. a
table representing these values can be computed ef   ciently by using a version of the
forward-backward algorithm used for id48s.

the same result can also be achieved through the use of deep learning approaches
based on recurrent neural networks (id56s). recent efforts have demonstrated con-
siderable success with id56s as alternatives to id48-based methods. these ap-
proaches differ from traditional classi   er-based methods in the following ways:

based feature functions.

    the use of vector-based word representations (embeddings) rather than word-
    input representations that span the entire sentence, as opposed to size-limited
    avoiding the use of high-level features, such as part of speech tags, since

errors in tag assignment can propagate to errors in supertags.

sliding windows.

as with the forward-backward algorithm, id56-based methods can provide a prob-
ability distribution over the lexical categories for each word in the input.

12.7.4 id35 parsing using the a* algorithm
the a* algorithm is a heuristic search method that employs an agenda to    nd an
optimal solution. search states representing partial solutions are added to an agenda
based on a cost function, with the least-cost option being selected for further ex-
ploration at each iteration. when a state representing a complete solution is    rst
selected from the agenda, it is guaranteed to be optimal and the search terminates.

the a* cost function, f (n), is used to ef   ciently guide the search to a solution.
the f -cost has two components: g(n), the exact cost of the partial solution repre-
sented by the state n, and h(n) a heuristic approximation of the cost of a solution
that makes use of n. when h(n) satis   es the criteria of not overestimating the actual
cost, a* will    nd an optimal solution. not surprisingly, the closer the heuristic can
get to the actual cost, the more effective a* is at    nding a solution without having
to explore a signi   cant portion of the solution space.

when applied to parsing, search states correspond to edges representing com-
pleted constituents. as with the pcky algorithm, edges specify a constituent   s start
and end positions, its grammatical category, and its f -cost. here, the g component
represents the current cost of an edge and the h component represents an estimate
of the cost to complete a derivation that makes use of that edge. the use of a*
for phrase structure parsing originated with (klein and manning, 2003a), while the
id35 approach presented here is based on (lewis and steedman, 2014).

using information from a supertagger, an agenda and a parse table are initial-
ized with states representing all the possible lexical categories for each word in the
input, along with their f -costs. the main loop removes the lowest cost edge from
the agenda and tests to see if it is a complete derivation. if it re   ects a complete
derivation it is selected as the best solution and the loop terminates. otherwise, new
states based on the applicable id35 rules are generated, assigned costs, and entered
into the agenda to await further processing. the loop continues until a complete
derivation is discovered, or the agenda is exhausted, indicating a failed parse. the
algorithm is given in fig. 12.11.

260 chapter 12

    statistical parsing

function id35-astar-parse(words) returns table or failure

supertags    supertagger(words)
for i   from 1 to length(words) do

for all {a | (words[i], a, score)     supertags}

edge    makeedge(i    1, i, a, score)
table    insertedge(table, edge)
agenda    insertedge(agenda, edge)

loop do

if empty?(agenda) return failure
current    pop(agenda)
if completedparse?(current) return table
table    insertedge(chart, edge)
for each rule in applicablerules(edge) do
successor    apply(rule, edge)
if successor not     in agenda or chart
else if successor     agenda with higher cost

agenda    insertedge(agenda, successor)
agenda    replaceedge(agenda, successor)

figure 12.11 a*-based id35 parsing.

heuristic functions
before we can de   ne a heuristic function for our id67, we need to decide how
to assess the quality of id35 derivations. for the generic pid18 model, we de   ned
the id203 of a tree as the product of the id203 of the rules that made up
the tree. given id35   s lexical nature, we   ll make the simplifying assumption that the
id203 of a id35 derivation is just the product of the id203 of the supertags
assigned to the words in the derivation, ignoring the rules used in the derivation.
more formally, given a sentence s and derivation d that contains suptertag sequence
t , we have:

p(d,s) = p(t,s)

=

n(cid:89)i=1

p(ti|si)

(12.36)

(12.37)

to better    t with the traditional a* approach, we   d prefer to have states scored
by a cost function where lower is better (i.e., we   re trying to minimize the cost of
a derivation). to achieve this, we   ll use negative log probabilities to score deriva-
tions; this results in the following equation, which we   ll use to score completed id35
derivations.

p(d,s) = p(t,s)

=

n(cid:88)i=1

   logp(ti|si)

(12.38)

(12.39)

given this model, we can de   ne our f -cost as follows. the f -cost of an edge is
the sum of two components: g(n), the cost of the span represented by the edge, and

12.7

    probabilistic id35 parsing

261

h(n), the estimate of the cost to complete a derivation containing that edge (these
are often referred to as the inside and outside costs). we   ll de   ne g(n) for an edge
using equation 12.39. that is, it is just the sum of the costs of the supertags that
comprise the span.

for h(n), we need a score that approximates but never overestimates the actual
cost of the    nal derivation. a simple heuristic that meets this requirement assumes
that each of the words in the outside span will be assigned its most probable su-
pertag. if these are the tags used in the    nal derivation, then its score will equal
the heuristic. if any other tags are used in the    nal derivation the f -cost will be
higher since the new tags must have higher costs, thus guaranteeing that we will not
overestimate.

putting this all together, we arrive at the following de   nition of a suitable f -cost

for an edge.

f (wi, j,ti, j) = g(wi, j) + h(wi, j)

(12.40)

=

j(cid:88)k=i
i   1(cid:88)k=1

   logp(tk|wk) +

max
t   tags

(   logp(t|wk)) +

n(cid:88)k= j+1

max
t   tags

(   logp(t|wk))

as an example, consider an edge representing the word serves with the supertag

n in the following example.
(12.41) united serves denver.

the g-cost for this edge is just the negative log id203 of the tag, or x. the
outside h-cost consists of the most optimistic supertag assignments for united and
denver. the resulting f -cost for this edge is therefore x+y+z = 1.494.

an example
fig. 12.12 shows the initial agenda and the progress of a complete parse for this
example. after initializing the agenda and the parse table with information from the
supertagger, it selects the best edge from the agenda     the entry for united with
the tag n/n and f -cost 0.591. this edge does not constitute a complete parse and is
therefore used to generate new states by applying all the relevant grammar rules. in
this case, applying forward application to united: n/n and serves: n results in the
creation of the edge united serves: n[0,2], 1.795 to the agenda.

skipping ahead, at the the third iteration an edge representing the complete
derivation united serves denver, s[0,3], .716 is added to the agenda. however,
the algorithm does not terminate at this point since the cost of this edge (.716) does
not place it at the top of the agenda. instead, the edge representing denver with the
category np is popped. this leads to the addition of another edge to the agenda
(type-raising denver). only after this edge is popped and dealt with does the ear-
lier state representing a complete derivation rise to the top of the agenda where it is
popped, goal tested, and returned as a solution.

the effectiveness of the a* approach is re   ected in the coloring of the states
in fig. 12.12 as well as the    nal parsing table. the edges shown in blue (includ-
ing all the initial lexical category assignments not explicitly shown) re   ect states in
the search space that never made it to the top of the agenda and, therefore, never

262 chapter 12

    statistical parsing

figure 12.12 example of an id67 for the example    united serves denver   . the circled numbers on the
white boxes indicate the order in which the states are popped from the agenda. the costs in each state are based
on f-costs using negative log10 probabilities.

contributed any edges to the    nal table. this is in contrast to the pcky approach
where the parser systematically    lls the parse table with all possible constituents for
all possible spans in the input,    lling the table with myriad constituents that do not
contribute to the    nal analysis.

united serves: n[0,2]1.795united: n/n.591denver: n/n2.494denver: n1.795serves: n1.494united: s\s1.494united: s/s1.1938united: np.716denver: np.591serves: (s\np)/np.591serves denver: s\np[1,3].591united serves denver: s[0,3].716denver: s/(s\np)[0,1].591123456initial agendagoal state   s: 0.716s/np: 0.591unitedserves[0,1][0,2][0,3][1,2][1,3][2,3]n/n: 0.591np: 0.716s/s: 1.1938s\s: 1.494   denver(s\np)/np: 0.591n: 1.494   np: 0.591n: 1.795n/n: 2.494   n: 1.79512.8 evaluating parsers

12.8

    evaluating parsers

263

the standard techniques for evaluating parsers and grammars are called the par-
seval measures; they were proposed by black et al. (1991) and were based on
the same ideas from signal-detection theory that we saw in earlier chapters. the
intuition of the parseval metric is to measure how much the constituents in the
hypothesis parse tree look like the constituents in a hand-labeled, gold-reference
parse. parseval thus assumes we have a human-labeled    gold standard    parse
tree for each sentence in the test set; we generally draw these gold-standard parses
from a treebank like the id32.

given these gold-standard reference parses for a test set, a given constituent in
a hypothesis parse ch of a sentence s is labeled    correct    if there is a constituent in
the reference parse cr with the same starting point, ending point, and non-terminal
symbol.

we can then measure the precision and recall just as we did for chunking in the

previous chapter.
labeled recall: = # of correct constituents in hypothesis parse of s
# of correct constituents in reference parse of s

labeled precision: = # of correct constituents in hypothesis parse of s
# of total constituents in hypothesis parse of s

f-measure

as with other uses of precision and recall, instead of reporting them separately,
we often report a single number, the f-measure (van rijsbergen, 1975): the f-
measure is de   ned as

f   =

(   2 + 1)pr
   2p + r

the    parameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. values of    > 1 favor recall and values
of    < 1 favor precision. when    = 1, precision and recall are equally balanced;
this is sometimes called f   =1 or just f1:

f1 =

2pr
p + r

(12.42)

the f-measure derives from a weighted harmonic mean of precision and recall.
remember that the harmonic mean of a set of numbers is the reciprocal of the arith-
metic mean of the reciprocals:

harmonicmean(a1,a2,a3,a4, ...,an) =

n
+ 1
a3

1
a1

+ 1
a2

+ ... + 1
an

and hence the f-measure is

f =

1

   1
p + (1      ) 1

r

or (cid:18)with    2 =

   (cid:19) f =
1      

(   2 + 1)pr
   2p + r

(12.43)

(12.44)

we additionally use a new metric, crossing brackets, for each sentence s:

cross-brackets: the number of constituents for which the reference parse has a
bracketing such as ((a b) c) but the hypothesis parse has a bracketing such
as (a (b c)).

264 chapter 12

    statistical parsing

evalb

as of the time of this writing, the performance of modern parsers that are trained
and tested on the wall street journal treebank was somewhat higher than 90% recall,
90% precision, and about 1% cross-bracketed constituents per sentence.

for comparing parsers that use different grammars, the parseval metric in-
cludes a canonicalization algorithm for removing information likely to be grammar-
speci   c (auxiliaries, pre-in   nitival    to   , etc.) and for computing a simpli   ed score
(black et al., 1991). the canonical implementation of the parseval metrics is
called evalb (sekine and collins, 1997).

nonetheless, phrasal constituents are not always an appropriate unit for parser
evaluation. in lexically-oriented grammars, such as id35 and lfg, the ultimate goal
is to extract the appropriate predicate-argument relations or grammatical dependen-
cies, rather than a speci   c derivation. such relations are also more directly relevant
to further semantic processing. for these purposes, we can use alternative evaluation
metrics based on measuring the precision and recall of labeled dependencies, where
the labels indicate the grammatical relations (lin 1995, carroll et al. 1998, collins
et al. 1999).

finally, you might wonder why we don   t evaluate parsers by measuring how
many sentences are parsed correctly instead of measuring component accuracy in
the form of constituents or dependencies. the reason we use components is that it
gives us a more    ne-grained metric. this is especially true for long sentences, where
most parsers don   t get a perfect parse. if we just measured sentence accuracy, we
wouldn   t be able to distinguish between a parse that got most of the parts wrong and
one that just got one part wrong.

12.9 human parsing

human
sentence
processing

reading time

are the kinds of probabilistic parsing models we have been discussing also used by
humans when they are parsing? the answer to this question lies in a    eld called
human sentence processing. recent studies suggest that there are at least two
ways in which humans apply probabilistic parsing algorithms, although there is still
disagreement on the details.

one family of studies has shown that when humans read, the predictability of a
word seems to in   uence the reading time; more predictable words are read more
quickly. one way of de   ning predictability is from simple bigram measures. for
example, scott and shillcock (2003) used an eye-tracker to monitor the gaze of
participants reading sentences. they constructed the sentences so that some would
have a verb-noun pair with a high bigram id203 (such as (12.45a)) and others
a verb-noun pair with a low bigram id203 (such as (12.45b)).
(12.45)

a) high prob: one way to avoid confusion is to make the changes

during vacation

b) low prob: one way to avoid discovery is to make the changes

during vacation

they found that the higher the bigram predictability of a word, the shorter the

time that participants looked at the word (the initial-   xation duration).

while this result provides evidence only for id165 probabilities, more recent
experiments have suggested that the id203 of an upcoming word given the
syntactic parse of the preceding sentence pre   x also predicts word reading time
(hale 2001, levy 2008).

12.9

    human parsing

265

garden-path

the second family of studies has examined how humans disambiguate sentences
that have multiple possible parses, suggesting that humans prefer whichever parse
is more probable. these studies often rely on a speci   c class of temporarily am-
biguous sentences called garden-path sentences. these sentences,    rst described
by bever (1970), are sentences that are cleverly constructed to have three properties
that combine to make them very dif   cult for people to parse:

1. they are temporarily ambiguous: the sentence is unambiguous, but its ini-

tial portion is ambiguous.

2. one of the two or more parses in the initial portion is somehow preferable to

the human parsing mechanism.

3. but the dispreferred parse is the correct one for the sentence.

the result of these three properties is that people are    led down the garden path   
toward the incorrect parse and then are confused when they realize it   s the wrong
one. sometimes this confusion is quite conscious, as in bever   s example (12.46);
in fact, this sentence is so hard to parse that readers often need to be shown the
correct structure. in the correct structure, raced is part of a reduced relative clause
modifying the horse, and means    the horse [which was raced past the barn] fell   ;
this structure is also present in the sentence    students taught by the berlitz method
do worse when they get to france   .

(12.46) the horse raced past the barn fell.

(a)

s

np

vp

?

v

(b)

s

np

det

n

v

pp

fell

np

vp

the

horse

raced

p

np

det

n

v

pp

vp

v

fell

past

det

n

the

horse

raced

p

np

the

barn

past

det

n

the

barn

other times, the confusion caused by a garden-path sentence is so subtle that it
can only be measured by a slight increase in reading time. thus, in (12.47) readers
often misparse the solution as the direct object of forgot rather than as the subject
of an embedded sentence. this misparse is subtle, and is only noticeable because
experimental participants take longer to read the word was than in control sentences.
this    mini garden path    effect at the word was suggests that subjects had chosen the
direct object parse and had to reanalyze or rearrange their parse now that they realize
they are in a sentential complement.

(12.47) the student forgot the solution was in the back of the book.

266 chapter 12

    statistical parsing

s

s

np

vp

np

vp

det

n

v

np

det

n

v

s

the

students

forgot

det

n

the

students

forgot

np

vp

the

solution

det

n

v

the

solution

was
while many factors seem to play a role in these preferences for a particular (in-
correct) parse, at least one factor seems to be syntactic probabilities, especially lex-
icalized (subcategorization) probabilities. for example, the id203 of the verb
forgot taking a direct object (vp     v np) is higher than the id203 of it taking a
sentential complement (vp     v s); this difference causes readers to expect a direct
object after forget and be surprised (longer reading times) when they encounter a
sentential complement. by contrast, a verb which prefers a sentential complement
(like hope) didn   t cause extra reading time at was. the garden path in (12.46) is at
least partially caused by the low id203 of the reduced relative clause construc-
tion.

12.10 summary

this chapter has sketched the basics of probabilistic parsing, concentrating on
id140 and probabilistic lexicalized context-free
grammars.

    probabilistic grammars assign a id203 to a sentence or string of words
while attempting to capture more sophisticated syntactic information than the
id165 grammars of chapter 3.

    a probabilistic

context-free grammar

context-free
grammar in which every rule is annotated with the id203 of that rule
being chosen. each pid18 rule is treated as if it were conditionally inde-
pendent; thus, the id203 of a sentence is computed by multiplying the
probabilities of each rule in the parse of the sentence.

(pid18)

is

a

    the probabilistic cky (cocke-kasami-younger) algorithm is a probabilistic
version of the cky parsing algorithm. there are also probabilistic versions
of other parsers like the earley algorithm.

    pid18 probabilities can be learned by counting in a parsed corpus or by pars-
ing a corpus. the inside-outside algorithm is a way of dealing with the fact
that the sentences being parsed are ambiguous.

of sensitivity to lexical dependencies.

    raw pid18s suffer from poor independence assumptions among rules and lack
    one way to deal with this problem is to split and merge non-terminals (auto-

matically or by hand).

bibliographical and historical notes

267

    probabilistic lexicalized id18s are another solution to this problem in which
the basic pid18 model is augmented with a lexical head for each rule. the
id203 of a rule can then be conditioned on the lexical head or nearby
heads.

based on extensions to probabilistic cky parsing.

    parsers for lexicalized pid18s (like the charniak and collins parsers) are
    parsers are evaluated with three metrics: labeled recall, labeled precision,
    evidence from garden-path sentences and other on-line sentence-processing
experiments suggest that the human parser uses some kinds of probabilistic
information about grammar.

and cross-brackets.

bibliographical and historical notes

many of the formal properties of id140 were    rst
worked out by booth (1969) and salomaa (1969). baker (1979) proposed the inside-
outside algorithm for unsupervised training of pid18 probabilities, and used a cky-
style parsing algorithm to compute inside probabilities. jelinek and lafferty (1991)
extended the cky algorithm to compute probabilities for pre   xes. stolcke (1995)
drew on both of these algorithms in adapting the earley algorithm to use with
pid18s.

a number of researchers starting in the early 1990s worked on adding lexical de-
pendencies to pid18s and on making pid18 rule probabilities more sensitive to sur-
rounding syntactic structure. for example, schabes et al. (1988) and schabes (1990)
presented early work on the use of heads. many papers on the use of lexical depen-
dencies were    rst presented at the darpa speech and natural language workshop
in june 1990. a paper by hindle and rooth (1990) applied lexical dependencies
to the problem of attaching prepositional phrases; in the question session to a later
paper, ken church suggested applying this method to full parsing (marcus, 1990).
early work on such probabilistic id18 parsing augmented with probabilistic depen-
dency information includes magerman and marcus (1991), black et al. (1992), bod
(1993), and jelinek et al. (1994), in addition to collins (1996), charniak (1997), and
collins (1999) discussed above. other recent pid18 parsing models include klein
and manning (2003a) and petrov et al. (2006).

this early lexical probabilistic work led initially to work focused on solving
speci   c parsing problems like preposition-phrase attachment by using methods in-
cluding transformation-based learning (tbl) (brill and resnik, 1994), maximum
id178 (ratnaparkhi et al., 1994), memory-based learning (zavrel and daelemans,
1997), id148 (franz, 1997), id90 that used semantic distance
between heads (computed from id138) (stetina and nagao, 1997), and boosting
(abney et al., 1999).

another direction extended the lexical probabilistic parsing work to build prob-
abilistic formulations of grammars other than pid18s, such as probabilistic tag
grammar (resnik 1992, schabes 1992), based on the tag grammars discussed in
chapter 10, probabilistic lr parsing (briscoe and carroll, 1993), and probabilistic
link grammar (lafferty et al., 1992). an approach to probabilistic parsing called
id55 extends the part-of-speech tagging metaphor to parsing by using very
complex tags that are, in fact, fragments of lexicalized parse trees (bangalore and

id55

268 chapter 12

    statistical parsing

joshi 1999, joshi and srinivas 1994), based on the lexicalized tag grammars of
schabes et al. (1988). for example, the noun purchase would have a different tag
as the    rst noun in a noun compound (where it might be on the left of a small tree
dominated by nominal) than as the second noun (where it might be on the right).

goodman (1997), abney (1997), and johnson et al. (1999) gave early discus-
sions of probabilistic treatments of feature-based grammars. other recent work
on building statistical models of feature-based grammar formalisms like hpsg and
lfg includes (riezler et al. 2002, kaplan et al. 2004), and toutanova et al. (2005).
we mentioned earlier that discriminative approaches to parsing fall into the two
broad categories of id145 methods and discriminative reranking
methods. recall that discriminative reranking approaches require n-best parses.
parsers based on id67 can easily be modi   ed to generate n-best lists just by
continuing the search past the    rst-best parse (roark, 2001). id145
algorithms like the ones described in this chapter can be modi   ed by the elimina-
tion of the id145 with heavy pruning (collins 2000, collins and
koo 2005, bikel 2004), or through new algorithms (jim  enez and marzal 2000,char-
niak and johnson 2005,huang and chiang 2005), some adapted from speech recog-
nition algorithms such as those of schwartz and chow (1990) (see section ??).

in id145 methods, instead of outputting and then reranking an
n-best list, the parses are represented compactly in a chart, and log-linear and other
methods are applied for decoding directly from the chart. such modern methods
include (johnson 2001, clark and curran 2004), and taskar et al. (2004). other
reranking developments include changing the optimization criterion (titov and hen-
derson, 2006).

collins    (1999) dissertation includes a very readable survey of the    eld and an
introduction to his parser. manning and sch  utze (1999) extensively cover proba-
bilistic parsing.

the    eld of grammar induction is closely related to statistical parsing, and a
parser is often used as part of a grammar induction algorithm. one of the earliest
statistical works in grammar induction was horning (1969), who showed that pid18s
could be induced without negative evidence. early modern probabilistic grammar
work showed that simply using em was insuf   cient (lari and young 1990, carroll
and charniak 1992). recent probabilistic work, such as yuret (1998), clark (2001),
klein and manning (2002), and klein and manning (2004), are summarized in klein
(2005) and adriaans and van zaanen (2004). work since that summary includes
smith and eisner (2005), haghighi and klein (2006), and smith and eisner (2007).

exercises

12.1 implement the cky algorithm.
12.2 modify the algorithm for conversion to cnf from chapter 11 to correctly
handle rule probabilities. make sure that the resulting cnf assigns the same
total id203 to each parse tree.

12.3 recall that exercise 13.3 asked you to update the cky algorithm to han-
dle unit productions directly rather than converting them to cnf. extend this
change to probabilistic cky.

12.4 fill out the rest of the probabilistic cky chart in fig. 12.4.

exercises

269

12.5 sketch how the cky algorithm would have to be augmented to handle lexi-

calized probabilities.

12.6 implement your lexicalized extension of the cky algorithm.
12.7 implement the parseval metrics described in section 12.8. next, either
use a treebank or create your own hand-checked parsed test set. now use your
id18 (or other) parser and grammar, parse the test set and compute labeled
recall, labeled precision, and cross-brackets.

270 chapter 13

    id33

chapter

13 id33

dependency
grammars

typed
dependency

free word order

the focus of the three previous chapters has been on context-free grammars and
their use in automatically generating constituent-based representations. here we
present another family of grammar formalisms called dependency grammars that
are quite important in contemporary speech and language processing systems. in
these formalisms, phrasal constituents and phrase-structure rules do not play a direct
role. instead, the syntactic structure of a sentence is described solely in terms of the
words (or lemmas) in a sentence and an associated set of directed binary grammatical
relations that hold among the words.

the following diagram illustrates a dependency-style analysis using the standard

graphical method favored in the dependency-parsing community.

root

dobj

det

nsubj

nmod

nmod

case

i prefer the morning    ight through denver

(13.1)

relations among the words are illustrated above the sentence with directed, la-
beled arcs from heads to dependents. we call this a typed dependency structure
because the labels are drawn from a    xed inventory of grammatical relations. it also
includes a root node that explicitly marks the root of the tree, the head of the entire
structure.

figure 13.1 shows the same dependency analysis as a tree alongside its corre-
sponding phrase-structure analysis of the kind given in chapter 10. note the ab-
sence of nodes corresponding to phrasal constituents or lexical categories in the
dependency parse; the internal structure of the dependency parse consists solely
of directed relations between lexical items in the sentence. these relationships di-
rectly encode important information that is often buried in the more complex phrase-
structure parses. for example, the arguments to the verb prefer are directly linked to
it in the dependency structure, while their connection to the main verb is more dis-
tant in the phrase-structure tree. similarly, morning and denver, modi   ers of    ight,
are linked to it directly in the dependency structure.

a major advantage of dependency grammars is their ability to deal with lan-
guages that are morphologically rich and have a relatively free word order. for
example, word order in czech can be much more    exible than in english; a gram-
matical object might occur before or after a location adverbial. a phrase-structure
grammar would need a separate rule for each possible place in the parse tree where
such an adverbial phrase could occur. a dependency-based approach would just
have one link type representing this particular adverbial relation. thus, a depen-
dency grammar approach abstracts away from word-order information, representing
only the information that is necessary for the parse.

an additional practical motivation for a dependency-based approach is that the
head-dependent relations provide an approximation to the semantic relationship be-

13.1

    dependency relations

271

prefer

s

i

   ight

np

vp

the

morning

denver

pro

verb

np

i

prefer

det

nom

through

the

nom

pp

nom

noun

p

np

noun

   ight

through

pro

morning

denver

figure 13.1 a dependency-style parse alongside the corresponding constituent-based analysis for i prefer the
morning    ight through denver.

tween predicates and their arguments that makes them directly useful for many ap-
plications such as coreference resolution, id53 and information ex-
traction. constituent-based approaches to parsing provide similar information, but it
often has to be distilled from the trees via techniques such as the head    nding rules
discussed in chapter 10.

in the following sections, we   ll discuss in more detail the inventory of relations
used in id33, as well as the formal basis for these dependency struc-
tures. we   ll then move on to discuss the dominant families of algorithms that are
used to automatically produce these structures. finally, we   ll discuss how to eval-
uate dependency parsers and point to some of the ways they are used in language
processing applications.

13.1 dependency relations

grammatical
relation

head
dependent

grammatical
function

the traditional linguistic notion of grammatical relation provides the basis for the
binary relations that comprise these dependency structures. the arguments to these
relations consist of a head and a dependent. we   ve already discussed the notion of
heads in chapter 10 and chapter 12 in the context of constituent structures. there,
the head word of a constituent was the central organizing word of a larger constituent
(e.g, the primary noun in a noun phrase, or verb in a verb phrase). the remaining
words in the constituent are either direct, or indirect, dependents of their head. in
dependency-based approaches, the head-dependent relationship is made explicit by
directly linking heads to the words that are immediately dependent on them, bypass-
ing the need for constituent structures.

in addition to specifying the head-dependent pairs, dependency grammars allow
us to further classify the kinds of grammatical relations, or grammatical function,

272 chapter 13

    id33

nominal subject
direct object
indirect object
clausal complement
open clausal complement

clausal argument relations description
nsubj
dobj
iobj
ccomp
xcomp
nominal modi   er relations description
nmod
amod
nummod
appos
det
case
other notable relations
conj
cc
figure 13.2 selected dependency relations from the universal dependency set. (de marn-
effe et al., 2014)

nominal modi   er
adjectival modi   er
numeric modi   er
appositional modi   er
determiner
prepositions, postpositions and other case markers
description
conjunct
coordinating conjunction

universal
dependencies

in terms of the role that the dependent plays with respect to its head. familiar notions
such as subject, direct object and indirect object are among the kind of relations we
have in mind. in english these notions strongly correlate with, but by no means de-
termine, both position in a sentence and constituent type and are therefore somewhat
redundant with the kind of information found in phrase-structure trees. however, in
more    exible languages the information encoded directly in these grammatical rela-
tions is critical since phrase-based constituent syntax provides little help.

not surprisingly, linguists have developed taxonomies of relations that go well
beyond the familiar notions of subject and object. while there is considerable vari-
ation from theory to theory, there is enough commonality that efforts to develop a
computationally useful standard are now possible. the universal dependencies
project (nivre et al., 2016b) provides an inventory of dependency relations that are
linguistically motivated, computationally useful, and cross-linguistically applicable.
fig. 13.2 shows a subset of the relations from this effort. fig. 13.3 provides some
example sentences illustrating selected relations.

the motivation for all of the relations in the universal dependency scheme is
beyond the scope of this chapter, but the core set of frequently used relations can be
broken into two sets: clausal relations that describe syntactic roles with respect to a
predicate (often a verb), and modi   er relations that categorize the ways that words
that can modify their heads.

consider the following example sentence:

root

dobj

det

nsubj

nmod

nmod

case

united canceled the morning    ights to houston

(13.2)

the clausal relations nsubj and dobj identify the subject and direct object of
the predicate cancel, while the nmod, det, and case relations denote modi   ers of
the nouns    ights and houston.

13.2

    dependency formalisms

273

relation
nsubj
dobj

iobj
nmod
amod
nummod
appos
det

examples with head and dependent
united canceled the    ight.
united diverted the    ight to reno.
we booked her the    rst    ight to miami.
we booked her the    ight to miami.
we took the morning    ight.
book the cheapest    ight.
before the storm jetblue canceled 1000    ights.
united, a unit of ual, matched the fares.
the    ight was canceled.
which    ight was delayed?
we    ew to denver and drove to steamboat.
we    ew to denver and drove to steamboat.
book the    ight through houston.

conj
cc
case
figure 13.3 examples of core universal dependency relations.

13.2 dependency formalisms

dependency
tree

in their most general form, the dependency structures we   re discussing are simply
directed graphs. that is, structures g = (v,a) consisting of a set of vertices v , and
a set of ordered pairs of vertices a, which we   ll refer to as arcs.

for the most part we will assume that the set of vertices, v , corresponds exactly
to the set of words in a given sentence. however, they might also correspond to
punctuation, or when dealing with morphologically complex languages the set of
vertices might consist of stems and af   xes. the set of arcs, a, captures the head-
dependent and grammatical function relationships between the elements in v .

further constraints on these dependency structures are speci   c to the underlying
grammatical theory or formalism. among the more frequent restrictions are that the
structures must be connected, have a designated root node, and be acyclic or planar.
of most relevance to the parsing approaches discussed in this chapter is the common,
computationally-motivated, restriction to rooted trees. that is, a dependency tree
is a directed graph that satis   es the following constraints:

1. there is a single designated root node that has no incoming arcs.
2. with the exception of the root node, each vertex has exactly one incoming arc.
3. there is a unique path from the root node to each vertex in v .

taken together, these constraints ensure that each word has a single head, that the
dependency structure is connected, and that there is a single root node from which
one can follow a unique directed path to each of the words in the sentence.

13.2.1 projectivity
the notion of projectivity imposes an additional constraint that is derived from the
order of the words in the input, and is closely related to the context-free nature of
human languages discussed in chapter 10. an arc from a head to a dependent is
said to be projective if there is a path from the head to every word that lies between
the head and the dependent in the sentence. a dependency tree is then said to be
projective if all the arcs that make it up are projective. all the dependency trees
we   ve seen thus far have been projective. there are, however, many perfectly valid

274 chapter 13

    id33

constructions which lead to non-projective trees, particularly in languages with a
relatively    exible word order.

consider the following example.

root

mod

nsubj

dobj

det

nmod

det

case

mod

adv

jetblue canceled our    ight this morning which was already late

(13.3)

in this example, the arc from    ight to its modi   er was is non-projective since
there is no path from    ight to the intervening words this and morning. as we can
see from this diagram, projectivity (and non-projectivity) can be detected in the way
we   ve been drawing our trees. a dependency tree is projective if it can be drawn
with no crossing edges. here there is no way to link    ight to its dependent was
without crossing the arc that links morning to its head.

our concern with projectivity arises from two related issues. first, the most
widely used english dependency treebanks were automatically derived from phrase-
structure treebanks through the use of head-   nding rules (chapter 10). the trees
generated in such a fashion are guaranteed to be projective since they   re generated
from context-free grammars.

second, there are computational limitations to the most widely used families of
parsing algorithms. the transition-based approaches discussed in section 13.4 can
only produce projective trees, hence any sentences with non-projective structures
will necessarily contain some errors. this limitation is one of the motivations for
the more    exible graph-based parsing approach described in section 13.5.

13.3 dependency treebanks

as with constituent-based methods, treebanks play a critical role in the development
and evaluation of dependency parsers. dependency treebanks have been created
using similar approaches to those discussed in chapter 10     having human annota-
tors directly generate dependency structures for a given corpus, or using automatic
parsers to provide an initial parse and then having annotators hand correct those
parsers. we can also use a deterministic process to translate existing constituent-
based treebanks into dependency trees through the use of head rules.

for the most part, directly annotated dependency treebanks have been created for
morphologically rich languages such as czech, hindi and finnish that lend them-
selves to dependency grammar approaches, with the prague dependency treebank
(bej  cek et al., 2013) for czech being the most well-known effort. the major english
dependency treebanks have largely been extracted from existing resources such as
the wall street journal sections of the id32(marcus et al., 1993). the
more recent ontonotes project (hovy et al. 2006,weischedel et al. 2011) extends
this approach going beyond traditional news text to include conversational telephone
speech, weblogs, usenet newsgroups, broadcast, and talk shows in english, chinese
and arabic.

the translation process from constituent to dependency structures has two sub-
tasks: identifying all the head-dependent relations in the structure and identifying
the correct dependency relations for these relations. the    rst task relies heavily on

13.4

    transition-based id33

275

the use of head rules discussed in chapter 10    rst developed for use in lexicalized
probabilistic parsers (magerman 1994,collins 1999,collins 2003b). here   s a simple
and effective algorithm from xia and palmer (2001).

1. mark the head child of each node in a phrase structure, using the appropriate

head rules.

2. in the dependency structure, make the head of each non-head child depend on

the head of the head-child.

when a phrase-structure parse contains additional information in the form of
grammatical relations and function tags, as in the case of the id32, these
tags can be used to label the edges in the resulting tree. when applied to the parse
tree in fig. 13.4, this algorithm would produce the dependency structure in fig. 13.4.

root

sbj

aux

dobj

nmod

clr

tmp

case

nmod

amod

num

vinken will join the board as a nonexecutive director nov 29

(13.4)

the primary shortcoming of these extraction methods is that they are limited by
the information present in the original constituent trees. among the most impor-
tant issues are the failure to integrate morphological information with the phrase-
structure trees, the inability to easily represent non-projective structures, and the
lack of internal structure to most noun-phrases, as re   ected in the generally    at
rules used in most treebank grammars. for these reasons, outside of english, most
dependency treebanks are developed directly using human annotators.

13.4 transition-based id33

shift-reduce
parsing

con   guration

our    rst approach to id33 is motivated by a stack-based approach
called id132 originally developed for analyzing programming lan-
guages (aho and ullman, 1972). this classic approach is simple and elegant, em-
ploying a context-free grammar, a stack, and a list of tokens to be parsed. input
tokens are successively shifted onto the stack and the top two elements of the stack
are matched against the right-hand side of the rules in the grammar; when a match is
found the matched elements are replaced on the stack (reduced) by the non-terminal
from the left-hand side of the rule being matched. in adapting this approach for
id33, we forgo the explicit use of a grammar and alter the reduce
operation so that instead of adding a non-terminal to a parse tree, it introduces a
dependency relation between a word and its head. more speci   cally, the reduce ac-
tion is replaced with two possible actions: assert a head-dependent relation between
the word at the top of the stack and the word below it, or vice versa. figure 13.5
illustrates the basic operation of such a parser.

a key element in transition-based parsing is the notion of a con   guration which
consists of a stack, an input buffer of words, or tokens, and a set of relations rep-
resenting a dependency tree. given this framework, the parsing process consists of
a sequence of transitions through the space of possible con   gurations. the goal of

276 chapter 13

    id33

s

np-sbj

vp

nnp

md

vp

vinken

will

vb

np

pp-clr

np-tmp

join

dt

nn

in

np

nnp

cd

the

board

as

dt

jj

nn

nov

29

a

nonexecutive

director

s(join)

np-sbj(vinken)

vp(join)

nnp

md

vp(join)

vinken

will

vb

np(board)

pp-clr(director)

np-tmp(29)

join

dt

nn

in

np(director)

nnp

cd

the

board

as

dt

jj

nn

nov

29

a

nonexecutive

director

join

vinken

will

board

director

29

the

as

a

nonexecutive

nov

figure 13.4 a phrase-structure tree from the wall street journal component of the id32 3.

this process is to    nd a    nal con   guration where all the words have been accounted
for and an appropriate dependency tree has been synthesized.

to implement such a search, we   ll de   ne a set of transition operators, which
when applied to a con   guration produce new con   gurations. given this setup, we
can view the operation of a parser as a search through a space of con   gurations for
a sequence of transitions that leads from a start state to a desired goal state. at the
start of this process we create an initial con   guration in which the stack contains the

13.4

    transition-based id33

277

figure 13.5 basic transition-based parser. the parser examines the top two elements of the
stack and selects an action based on consulting an oracle that examines the current con   gura-
tion.

root node, the word list is initialized with the set of the words or lemmatized tokens
in the sentence, and an empty set of relations is created to represent the parse. in the
   nal goal state, the stack and the word list should be empty, and the set of relations
will represent the    nal parse.

in the standard approach to transition-based parsing, the operators used to pro-
duce new con   gurations are surprisingly simple and correspond to the intuitive ac-
tions one might take in creating a dependency tree by examining the words in a
single pass over the input from left to right (covington, 2001):

    assign the current word as the head of some previously seen word,
    assign some previously seen word as the head of the current word,
    or postpone doing anything with the current word, adding it to a store for later

processing.

to make these actions more precise, we   ll create three transition operators that

will operate on the top two elements of the stack:

    leftarc: assert a head-dependent relation between the word at the top of
stack and the word directly beneath it; remove the lower word from the stack.
    rightarc: assert a head-dependent relation between the second word on
    shift: remove the word from the front of the input buffer and push it onto

the stack and the word at the top; remove the word at the top of the stack;

the stack.

arc standard

this particular set of operators implements what is known as the arc standard
approach to transition-based parsing (covington 2001,nivre 2003). there are two
notable characteristics to this approach: the transition operators only assert relations
between elements at the top of the stack, and once an element has been assigned
its head it is removed from the stack and is not available for further processing.
as we   ll see, there are alternative transition systems which demonstrate different
parsing behaviors, but the arc standard approach is quite effective and is simple to
implement.

dependencyrelationswnw1w2s2...s1snparserinput bufferstackoracle278 chapter 13

    id33

to assure that these operators are used properly we   ll need to add some pre-
conditions to their use. first, since, by de   nition, the root node cannot have any
incoming arcs, we   ll add the restriction that the leftarc operator cannot be ap-
plied when root is the second element of the stack. second, both reduce operators
require two elements to be on the stack to be applied. given these transition opera-
tors and preconditions, the speci   cation of a transition-based parser is quite simple.
fig. 13.6 gives the basic algorithm.

function dependencyparse(words) returns dependency tree
state   {[root], [words], [] } ; initial con   guration
while state not    nal
t    oracle(state)
state    apply(t, state) ; apply it, creating a new state

; choose a transition operator to apply

return state

figure 13.6 a generic transition-based dependency parser

at each step, the parser consults an oracle (we   ll come back to this shortly) that
provides the correct transition operator to use given the current con   guration. it then
applies that operator to the current con   guration, producing a new con   guration.
the process ends when all the words in the sentence have been consumed and the
root node is the only element remaining on the stack.

the ef   ciency of transition-based parsers should be apparent from the algorithm.
the complexity is linear in the length of the sentence since it is based on a single left
to right pass through the words in the sentence. more speci   cally, each word must
   rst be shifted onto the stack and then later reduced.

note that unlike the id145 and search-based approaches dis-
cussed in chapters 12 and 13, this approach is a straightforward greedy algorithm
    the oracle provides a single choice at each step and the parser proceeds with that
choice, no other options are explored, no backtracking is employed, and a single
parse is returned in the end.

figure 13.7 illustrates the operation of the parser with the sequence of transitions

leading to a parse for the following example.

root

iobj

dobj

det

nmod

book me the morning    ight

(13.5)

let   s consider the state of the con   guration at step 2, after the word me has been

pushed onto the stack.

stack

word list

relations

[root, book, me] [the, morning,    ight]

the correct operator to apply here is rightarc which assigns book as the head of
me and pops me from the stack resulting in the following con   guration.

stack

relations
[root, book] [the, morning,    ight] (book     me)

word list

13.4

    transition-based id33

279

step

0
1
2
3
4
5
6
7
8
9
10

stack word list
[root]
[root, book]
[root, book, me]
[root, book]
[root, book, the]
[root, book, the, morning]
[root, book, the, morning,    ight]
[root, book, the,    ight]
[root, book,    ight]
[root, book]
[root]

[book, me, the, morning,    ight]
[me, the, morning,    ight]
[the, morning,    ight]
[the, morning,    ight]
[morning,    ight]
[   ight]
[]
[]
[]
[]
[]

action
shift
shift

rightarc

shift
shift
shift

leftarc
leftarc
rightarc
rightarc

done

relation added

(book     me)

(morning        ight)
(the        ight)
(book        ight)
(root     book)

figure 13.7 trace of a transition-based parse.

after several subsequent applications of the shift and leftarc operators, the con-
   guration in step 6 looks like the following:

stack

word list

[root, book, the, morning,    ight]

[]

relations
(book     me)

here, all the remaining words have been passed onto the stack and all that is left
to do is to apply the appropriate reduce operators. in the current con   guration, we
employ the leftarc operator resulting in the following state.

stack

word list

[root, book, the,    ight]

[]

relations
(book     me)

(morning        ight)

at this point, the parse for this sentence consists of the following structure.

dobj

nmod

book me the morning    ight

(13.6)

there are several important things to note when examining sequences such as
the one in figure 13.7. first, the sequence given is not the only one that might lead
to a reasonable parse. in general, there may be more than one path that leads to the
same result, and due to ambiguity, there may be other transition sequences that lead
to different equally valid parses.

second, we are assuming that the oracle always provides the correct operator
at each point in the parse     an assumption that is unlikely to be true in practice.
as a result, given the greedy nature of this algorithm, incorrect choices will lead to
incorrect parses since the parser has no opportunity to go back and pursue alternative
choices. section 13.4.2 will introduce several techniques that allow transition-based
approaches to explore the search space more fully.

finally, for simplicity, we have illustrated this example without the labels on
the dependency relations. to produce labeled trees, we can parameterize the left-
arc and rightarc operators with dependency labels, as in leftarc(nsubj) or
rightarc(dobj). this is equivalent to expanding the set of transition operators
from our original set of three to a set that includes leftarc and rightarc opera-
tors for each relation in the set of dependency relations being used, plus an additional
one for the shift operator. this, of course, makes the job of the oracle more dif   cult
since it now has a much larger set of operators from which to choose.

280 chapter 13

    id33
13.4.1 creating an oracle
state-of-the-art transition-based systems use supervised machine learning methods
to train classi   ers that play the role of the oracle. given appropriate training data,
these methods learn a function that maps from con   gurations to transition operators.
as with all supervised machine learning methods, we will need access to appro-
priate training data and we will need to extract features useful for characterizing the
decisions to be made. the source for this training data will be representative tree-
banks containing dependency trees. the features will consist of many of the same
features we encountered in chapter 8 for part-of-speech tagging, as well as those
used in chapter 12 for statistical parsing models.

training oracle

generating training data
let   s revisit the oracle from the algorithm in fig. 13.6 to fully understand the learn-
ing problem. the oracle takes as input a con   guration and returns as output a tran-
sition operator. therefore, to train a classi   er, we will need con   gurations paired
with transition operators (i.e., leftarc, rightarc, or shift). unfortunately,
treebanks pair entire sentences with their corresponding trees, and therefore they
don   t directly provide what we need.

to generate the required training data, we will employ the oracle-based parsing
algorithm in a clever way. we will supply our oracle with the training sentences
to be parsed along with their corresponding reference parses from the treebank. to
produce training instances, we will then simulate the operation of the parser by run-
ning the algorithm and relying on a new training oracle to give us correct transition
operators for each successive con   guration.

to see how this works, let   s    rst review the operation of our parser. it begins with
a default initial con   guration where the stack contains the root, the input list is just
the list of words, and the set of relations is empty. the leftarc and rightarc
operators each add relations between the words at the top of the stack to the set of
relations being accumulated for a given sentence. since we have a gold-standard
reference parse for each training sentence, we know which dependency relations are
valid for a given sentence. therefore, we can use the reference parse to guide the
selection of operators as the parser steps through a sequence of con   gurations.

to be more precise, given a reference parse and a con   guration, the training

oracle proceeds as follows:

reference parse and the current con   guration,

    choose leftarc if it produces a correct head-dependent relation given the
    otherwise, choose rightarc if (1) it produces a correct head-dependent re-
lation given the reference parse and (2) all of the dependents of the word at
the top of the stack have already been assigned,

    otherwise, choose shift.
the restriction on selecting the rightarc operator is needed to ensure that a
word is not popped from the stack, and thus lost to further processing, before all its
dependents have been assigned to it.

more formally, during training the oracle has access to the following informa-

tion:

    a current con   guration with a stack s and a set of dependency relations rc
    a reference parse consisting of a set of vertices v and a set of dependency

relations rp

13.4

    transition-based id33

281

step

0
1
2
3
4
5
6
7
8
9
10

stack
[root]
[root, book]
[root, book, the]
[root, book, the,    ight]
[root, book,    ight]
[root, book,    ight, through]
[root, book,    ight, through, houston]
[root, book,    ight, houston ]
[root, book,    ight]
[root, book]
[root]

word list
[book, the,    ight, through, houston]
[the,    ight, through, houston]
[   ight, through, houston]
[through, houston]
[through, houston]
[houston]
[]
[]
[]
[]
[]

predicted action

shift
shift
shift

leftarc

shift
shift

leftarc
rightarc
rightarc
rightarc

done

figure 13.8 generating training items consisting of con   guration/predicted action pairs by
simulating a parse with a given reference parse.

given this information, the oracle chooses transitions as follows:

leftarc(r): if (s1 r s2)     rp
rightarc(r): if (s2 r s1)     rp and    r(cid:48),w s.t.(s1 r(cid:48) w)     rp then (s1 r(cid:48) w)    
rc
shift: otherwise

let   s walk through some the steps of this process with the following example as

shown in fig. 13.8.

root

dobj

det

nmod

case

book the    ight through houston

(13.7)

at step 1, leftarc is not applicable in the initial con   guration since it asserts
a relation, (root     book), not in the reference answer; rightarc does assert a
relation contained in the    nal answer (root     book), however book has not been
attached to any of its dependents yet, so we have to defer, leaving shift as the only
possible action. the same conditions hold in the next two steps. in step 3, leftarc
is selected to link the to its head.

now consider the situation in step 4.

stack
relations
[root, book,    ight] [through, houston] (the        ight)

word buffer

here, we might be tempted to add a dependency relation between book and    ight,
which is present in the reference parse. but doing so now would prevent the later
attachment of houston since    ight would have been removed from the stack. for-
tunately, the precondition on choosing rightarc prevents this choice and we   re
again left with shift as the only viable option. the remaining choices complete the
set of operators needed for this example.

to recap, we derive appropriate training instances consisting of con   guration-
transition pairs from a treebank by simulating the operation of a parser in the con-
text of a reference dependency tree. we can deterministically record correct parser
actions at each step as we progress through each training example, thereby creating
the training set we require.

282 chapter 13

    id33

feature
template

features
having generated appropriate training instances (con   guration-transition pairs), we
need to extract useful features from the con   gurations so what we can train classi-
   ers. the features that are used to train transition-based systems vary by language,
genre, and the kind of classi   er being employed. for example, morphosyntactic
features such as case marking on subjects or direct objects may be more or less im-
portant depending on the language being processed. that said, the basic features that
we have already seen with part-of-speech tagging and partial parsing have proven to
be useful in training dependency parsers across a wide range of languages. word
forms, lemmas and parts of speech are all powerful features, as are the head, and
dependency relation to the head.

in the transition-based parsing framework, such features need to be extracted
from the con   gurations that make up the training data. recall that con   gurations
consist of three elements: the stack, the buffer and the current set of relations. in
principle, any property of any or all of these elements can be represented as features
in the usual way for training. however, to avoid sparsity and encourage generaliza-
tion, it is best to focus the learning algorithm on the most useful aspects of decision
making at each point in the parsing process. the focus of feature extraction for
transition-based parsing is, therefore, on the top levels of the stack, the words near
the front of the buffer, and the dependency relations already associated with any of
those elements.

by combining simple features, such as word forms or parts of speech, with spe-
ci   c locations in a con   guration, we can employ the notion of a feature template
that we   ve already encountered with id31 and part-of-speech tagging.
feature templates allow us to automatically generate large numbers of speci   c fea-
tures from a training set. as an example, consider the following feature templates
that are based on single positions in a con   guration.

(cid:104)s1.w,op(cid:105),(cid:104)s2.w,op(cid:105)(cid:104)s1.t,op(cid:105),(cid:104)s2.t,op(cid:105)
(cid:104)b1.w,op(cid:105),(cid:104)b1.t,op(cid:105)(cid:104)s1.wt,op(cid:105)

(13.8)

in these examples, individual features are denoted as location.property, where s
denotes the stack, b the word buffer, and r the set of relations. individual properties
of locations include w for word forms, l for lemmas, and t for part-of-speech. for
example, the feature corresponding to the word form at the top of the stack would be
denoted as s1.w, and the part of speech tag at the front of the buffer b1.t. we can also
combine individual features via concatenation into more speci   c features that may
prove useful. for example, the feature designated by s1.wt represents the word form
concatenated with the part of speech of the word at the top of the stack. finally, op
stands for the transition operator for the training example in question (i.e., the label
for the training instance).

let   s consider the simple set of single-element feature templates given above
in the context of the following intermediate con   guration derived from a training
oracle for example 13.2.

stack
[root, canceled,    ights] [to houston] (canceled     united)
(   ights     morning)

word buffer

relations

(   ights     the)

the correct transition here is shift (you should convince yourself of this before

13.4

    transition-based id33

283

proceeding). the application of our set of feature templates to this con   guration
would result in the following set of instantiated features.

(13.9)

(cid:104)s1.w =    ights,op = shift(cid:105)
(cid:104)s2.w = canceled,op = shift(cid:105)
(cid:104)s1.t = nns,op = shift(cid:105)
(cid:104)s2.t = vbd,op = shift(cid:105)
(cid:104)b1.w = to,op = shift(cid:105)
(cid:104)b1.t = to,op = shift(cid:105)
(cid:104)s1.wt =    ightsnns,op = shift(cid:105)

given that the left and right arc transitions operate on the top two elements of
the stack, features that combine properties from these positions are even more useful.
for example, a feature like s1.t     s2.t concatenates the part of speech tag of the word
at the top of the stack with the tag of the word beneath it.

(cid:104)s1.t     s2.t = nnsvbd,op = shift(cid:105)

(13.10)
not surprisingly, if two properties are useful then three or more should be even
better. figure 13.9 gives a baseline set of feature templates that have been employed
in various state-of-the-art systems (zhang and clark 2008,huang and sagae 2010,zhang
and nivre 2011).

note that some of these features make use of dynamic features     features such
as head words and dependency relations that have been predicted at earlier steps in
the parsing process, as opposed to features that are derived from static properties of
the input.

feature templates

source
one word s1.w
s2.w
b1.w

two word s1.w    s2.w
s1.t     s2.wt
s1.w    s1.t     s2.t

s1.wt
s2.wt
b0.wt
s1.t     b1.w

s1.t
s2.t
b1.w
s1.t     s2.t
s1.w    s2.w    s2.t s1.w    s1.t     s2.t
s1.w    s1.t

figure 13.9 standard feature templates for training transition-based dependency parsers.
in the template speci   cations sn refers to a location on the stack, bn refers to a location in the
word buffer, w refers to the wordform of the input, and t refers to the part of speech of the
input.

learning
over the years, the dominant approaches to training transition-based dependency
parsers have been multinomial id28 and support vector machines, both
of which can make effective use of large numbers of sparse features of the kind
described in the last section. more recently, neural network, or deep learning,
approaches of the kind described in chapter 8 have been applied successfully to
transition-based parsing (chen and manning, 2014). these approaches eliminate the
need for complex, hand-crafted features and have been particularly effective at over-
coming the data sparsity issues normally associated with training transition-based
parsers.

284 chapter 13

    id33

13.4.2 advanced methods in transition-based parsing
the basic transition-based approach can be elaborated in a number of ways to im-
prove performance by addressing some of the most obvious    aws in the approach.

alternative transition systems
the arc-standard transition system described above is only one of many possible sys-
tems. a frequently used alternative is the arc eager transition system. the arc eager
approach gets its name from its ability to assert rightward relations much sooner
than in the arc standard approach. to see this, let   s revisit the arc standard trace of
example 13.7, repeated here.

arc eager

root

dobj

det

nmod

case

book the    ight through houston

consider the dependency relation between book and    ight in this analysis. as
is shown in fig. 13.8, an arc-standard approach would assert this relation at step 8,
despite the fact that book and    ight    rst come together on the stack much earlier at
step 4. the reason this relation can   t be captured at this point is due to the presence
of the post-nominal modi   er through houston. in an arc-standard approach, depen-
dents are removed from the stack as soon as they are assigned their heads. if    ight
had been assigned book as its head in step 4, it would no longer be available to serve
as the head of houston.

while this delay doesn   t cause any issues in this example, in general the longer
a word has to wait to get assigned its head the more opportunities there are for
something to go awry. the arc-eager system addresses this issue by allowing words
to be attached to their heads as early as possible, before all the subsequent words
dependent on them have been seen. this is accomplished through minor changes to
the leftarc and rightarc operators and the addition of a new reduce operator.
    leftarc: assert a head-dependent relation between the word at the front of
    rightarc: assert a head-dependent relation between the word on the top of
the stack and the word at front of the input buffer; shift the word at the front
of the input buffer to the stack.

the input buffer and the word at the top of the stack; pop the stack.

the stack.

    shift: remove the word from the front of the input buffer and push it onto
    reduce: pop the stack.
the leftarc and rightarc operators are applied to the top of the stack and
the front of the input buffer, instead of the top two elements of the stack as in the
arc-standard approach. the rightarc operator now moves the dependent to the
stack from the buffer rather than removing it, thus making it available to serve as the
head of following words. the new reduce operator removes the top element from
the stack. together these changes permit a word to be eagerly assigned its head and
still allow it to serve as the head for later dependents. the trace shown in fig. 13.10
illustrates the new decision sequence for this example.

in addition to demonstrating the arc-eager transition system, this example demon-
strates the power and    exibility of the overall transition-based approach. we were
able to swap in a new transition system without having to make any changes to the

13.4

    transition-based id33

285

step

0
1
2
3
4
5
6
7
8
9
10

action

stack word list
[root]
[root, book]
[root, book, the]
[root, book]
[root, book,    ight]
[root, book,    ight, through]
[root, book,    ight]
[root, book,    ight, houston]
[root, book,    ight]
[root, book]
[root]

[book, the,    ight, through, houston] rightarc
[the,    ight, through, houston]
[   ight, through, houston]
[   ight, through, houston]
[through, houston]
[houston]
[houston]
[]
[]
[]
[]

leftarc
rightarc
reduce
reduce
reduce

leftarc
rightarc

done

shift

shift

relation added
(root     book)
(the        ight)
(book        ight)

(through     houston)
(   ight     houston)

id125

beam width

figure 13.10 a processing trace of book the    ight through houston using the arc-eager
transition operators.

underlying parsing algorithm. this    exibility has led to the development of a di-
verse set of transition systems that address different aspects of syntax and semantics
including: assigning part of speech tags (choi and palmer, 2011a), allowing the
generation of non-projective dependency structures (nivre, 2009), assigning seman-
tic roles (choi and palmer, 2011b), and parsing texts containing multiple languages
(bhat et al., 2017).

id125
the computational ef   ciency of the transition-based approach discussed earlier de-
rives from the fact that it makes a single pass through the sentence, greedily making
decisions without considering alternatives. of course, this is also the source of its
greatest weakness     once a decision has been made it can not be undone, even in
the face of overwhelming evidence arriving later in a sentence. another approach
is to systematically explore alternative decision sequences, selecting the best among
those alternatives. the key problem for such a search is to manage the large number
of potential sequences. id125 accomplishes this by combining a breadth-   rst
search strategy with a heuristic    lter that prunes the search frontier to stay within a
   xed-size beam width.

in applying id125 to transition-based parsing, we   ll elaborate on the al-
gorithm given in fig. 13.6. instead of choosing the single best transition operator
at each iteration, we   ll apply all applicable operators to each state on an agenda and
then score the resulting con   gurations. we then add each of these new con   gura-
tions to the frontier, subject to the constraint that there has to be room within the
beam. as long as the size of the agenda is within the speci   ed beam width, we can
add new con   gurations to the agenda. once the agenda reaches the limit, we only
add new con   gurations that are better than the worst con   guration on the agenda
(removing the worst element so that we stay within the limit). finally, to insure that
we retrieve the best possible state on the agenda, the while loop continues as long as
there are non-   nal states on the agenda.

the id125 approach requires a more elaborate notion of scoring than we
used with the greedy algorithm. there, we assumed that a classi   er trained using
supervised machine learning would serve as an oracle, selecting the best transition
operator based on features extracted from the current con   guration. regardless of
the speci   c learning approach, this choice can be viewed as assigning a score to all
the possible transitions and picking the best one.

  t (c) = argmaxscore(t,c)

286 chapter 13

    id33

with a id125 we are now searching through the space of decision se-
quences, so it makes sense to base the score for a con   guration on its entire history.
more speci   cally, we can de   ne the score for a new con   guration as the score of its
predecessor plus the score of the operator used to produce it.

con   gscore(c0) = 0.0
con   gscore(ci) = con   gscore(ci   1) + score(ti,ci   1)

this score is used both in    ltering the agenda and in selecting the    nal answer.

the new id125 version of transition-based parsing is given in fig. 13.11.

function dependencybeamparse(words, width) returns dependency tree
state   {[root], [words], [], 0.0}
agenda   (cid:104)state(cid:105);
while agenda contains non-   nal states

;initial con   guration

initial agenda

newagenda   (cid:104)(cid:105)
for each state     agenda do
for all {t | t     validoperators(state)} do
child    apply(t, state)
newagenda    addtobeam(child, newagenda, width)

agenda   newagenda
return bestof(agenda)

function addtobeam(state, agenda, width) returns updated agenda

if length(agenda) < width then

else if score(state) > score(worstof(agenda))

agenda    insert(state, agenda)
agenda    remove(worstof(agenda))
agenda    insert(state, agenda)

return agenda

figure 13.11 id125 applied to transition-based id33.

13.5 graph-based id33

graph-based approaches to id33 search through the space of possible
trees for a given sentence for a tree (or trees) that maximize some score. these
methods encode the search space as directed graphs and employ methods drawn
from id207 to search the space for optimal solutions. more formally, given a
sentence s we   re looking for the best dependency tree in gs, the space of all possible
trees for that sentence, that maximizes some score.

  t (s) = argmax

t   gs

score(t,s)

as with the probabilistic approaches to context-free parsing discussed in chap-
ter 12, the overall score for a tree can be viewed as a function of the scores of the
parts of the tree. the focus of this section is on edge-factored approaches where the

edge-factored

13.5

    graph-based id33

287

score for a tree is based on the scores of the edges that comprise the tree.

score(t,s) =(cid:88)e   t

score(e)

there are several motivations for the use of graph-based methods. first, unlike
transition-based approaches, these methods are capable of producing non-projective
trees. although projectivity is not a signi   cant issue for english, it is de   nitely a
problem for many of the world   s languages. a second motivation concerns parsing
accuracy, particularly with respect to longer dependencies. empirically, transition-
based methods have high accuracy on shorter dependency relations but accuracy de-
clines signi   cantly as the distance between the head and dependent increases (mc-
donald and nivre, 2011). graph-based methods avoid this dif   culty by scoring
entire trees, rather than relying on greedy local decisions.

the following section examines a widely-studied approach based on the use of a
maximum spanning tree (mst) algorithm for weighted, directed graphs. we then
discuss features that are typically used to score trees, as well as the methods used to
train the scoring models.

maximum
spanning tree

13.5.1 parsing
the approach described here uses an ef   cient greedy algorithm to search for optimal
spanning trees in directed graphs. given an input sentence, it begins by constructing
a fully-connected, weighted, directed graph where the vertices are the input words
and the directed edges represent all possible head-dependent assignments. an addi-
tional root node is included with outgoing edges directed at all of the other vertices.
the weights in the graph re   ect the score for each possible head-dependent relation
as provided by a model generated from training data. given these weights, a maxi-
mum spanning tree of this graph emanating from the root represents the preferred
dependency parse for the sentence. a directed graph for the example book that
   ight is shown in fig. 13.12, with the maximum spanning tree corresponding to the
desired parse shown in blue. for ease of exposition, we   ll focus here on unlabeled
id33. graph-based approaches to labeled parsing are discussed in
section 13.5.3.

before describing the algorithm it   s useful to consider two intuitions about di-
rected graphs and their spanning trees. the    rst intuition begins with the fact that
every vertex in a spanning tree has exactly one incoming edge. it follows from this
that every connected component of a spanning tree will also have one incoming edge.
the second intuition is that the absolute values of the edge scores are not critical to
determining its maximum spanning tree. instead, it is the relative weights of the
edges entering each vertex that matters. if we were to subtract a constant amount
from each edge entering a given vertex it would have no impact on the choice of
the maximum spanning tree since every possible spanning tree would decrease by
exactly the same amount.

the    rst step of the algorithm itself is quite straightforward. for each vertex
in the graph, an incoming edge (representing a possible head assignment) with the
highest score is chosen. if the resulting set of edges produces a spanning tree then
we   re done. more formally, given the original fully-connected graph g = (v,e), a
subgraph t = (v,f) is a spanning tree if it has no cycles and each vertex (other than
the root) has exactly one edge entering it. if the greedy selection process produces
such a tree then it is the best possible one.

288 chapter 13

    id33

figure 13.12

initial rooted, directed graph for book that    ight.

unfortunately, this approach doesn   t always lead to a tree since the set of edges
selected may contain cycles. fortunately, in yet another case of multiple discovery,
there is a straightforward way to eliminate cycles generated during the greedy se-
lection phase. chu and liu (1965) and edmonds (1967) independently developed
an approach that begins with greedy selection and follows with an elegant recursive
cleanup phase that eliminates cycles.

the cleanup phase begins by adjusting all the weights in the graph by subtracting
the score of the maximum edge entering each vertex from the score of all the edges
entering that vertex. this is where the intuitions mentioned earlier come into play.
we have scaled the values of the edges so that the weight of the edges in the cycle
have no bearing on the weight of any of the possible spanning trees. subtracting the
value of the edge with maximum weight from each edge entering a vertex results
in a weight of zero for all of the edges selected during the greedy selection phase,
including all of the edges involved in the cycle.

having adjusted the weights, the algorithm creates a new graph by selecting a
cycle and collapsing it into a single new node. edges that enter or leave the cycle
are altered so that they now enter or leave the newly collapsed node. edges that do
not touch the cycle are included and edges within the cycle are dropped.

now, if we knew the maximum spanning tree of this new graph, we would have
what we need to eliminate the cycle. the edge of the maximum spanning tree di-
rected towards the vertex representing the collapsed cycle tells us which edge to
delete to eliminate the cycle. how do we    nd the maximum spanning tree of this
new graph? we recursively apply the algorithm to the new graph. this will either
result in a spanning tree or a graph with a cycle. the recursions can continue as long
as cycles are encountered. when each recursion completes we expand the collapsed
vertex, restoring all the vertices and edges from the cycle with the exception of the
single edge to be deleted.

putting all this together, the maximum spanning tree algorithm consists of greedy
edge selection, re-scoring of edge costs and a recursive cleanup phase when needed.
the full algorithm is shown in fig. 13.13.

fig. 13.14 steps through the algorithm with our book that    ight example. the
   rst row of the    gure illustrates greedy edge selection with the edges chosen shown

rootbookthat   ight124456875713.5

    graph-based id33

289

function maxspanningtree(g=(v,e), root, score) returns spanning tree

f   []
t      []
score      []
for each v     v do

bestinedge   argmaxe=(u,v)    e score[e]
f   f     bestinedge
for each e=(u,v)     e do

score   [e]   score[e]     score[bestinedge]
if t=(v,f) is a spanning tree then return it
else

c   a cycle in f
g       contract(g, c)
t       maxspanningtree(g   , root, score   )
t    expand(t   , c)
return t

function contract(g, c) returns contracted graph

function expand(t, c) returns expanded graph

figure 13.13 the chu-liu edmonds algorithm for    nding a maximum spanning tree in a
weighted directed graph.

in blue (corresponding to the set f in the algorithm). this results in a cycle between
that and    ight. the scaled weights using the maximum value entering each node are
shown in the graph to the right.

collapsing the cycle between that and    ight to a single node (labelled tf) and
recursing with the newly scaled costs is shown in the second row. the greedy selec-
tion step in this recursion yields a spanning tree that links root to book, as well as an
edge that links book to the contracted node. expanding the contracted node, we can
see that this edge corresponds to the edge from book to    ight in the original graph.
this in turn tells us which edge to drop to eliminate the cycle

on arbitrary directed graphs, this version of the cle algorithm runs in o(mn)
time, where m is the number of edges and n is the number of nodes. since this par-
ticular application of the algorithm begins by constructing a fully connected graph
m = n2 yielding a running time of o(n3). gabow et al. (1986) present a more ef   -
cient implementation with a running time of o(m + nlogn).

13.5.2 features and training
given a sentence, s, and a candidate tree, t , edge-factored parsing models reduce
the score for the tree to a sum of the scores of the edges that comprise the tree.

score(s,t ) = (cid:88)e   t

score(s,e)

each edge score can, in turn, be reduced to a weighted sum of features extracted

from it.

score(s,e) =

wi fi(s,e)

n(cid:88)i=1

290 chapter 13

    id33

figure 13.14 chu-liu-edmonds graph-based example for book that    ight

or more succinctly.

score(s,e) = w   f

given this formulation, we are faced with two problems in training our parser:

identifying relevant features and    nding the weights used to score those features.

the features used to train edge-factored models mirror those used in training
transition-based parsers (as shown in fig. 13.9). this is hardly surprising since in
both cases we   re trying to capture information about the relationship between heads
and their dependents in the context of a single relation. to summarize this earlier
discussion, commonly used features include:

the words.

    wordforms, lemmas, and parts of speech of the headword and its dependent.
    corresponding features derived from the contexts before, after and between
    id27s.
    the dependency relation itself.
    the direction of the relation (to the right or left).
    the distance from the head to the dependent.

as with transition-based approaches, pre-selected combinations of these features are
often used as well.

given a set of features, our next problem is to learn a set of weights correspond-
ing to each. unlike many of the learning problems discussed in earlier chapters,

rootbooktfrootbookthat   ight0-3-4-7-1-6-2rootbook12that7   ight8-4-30-2-6-1-700rootbook0tf-10-3-4-7-1-6-2rootbook12that7   ight81244568757deleted from cycleid136-based
learning

13.6

    evaluation

291

here we are not training a model to associate training items with class labels, or
parser actions. instead, we seek to train a model that assigns higher scores to cor-
rect trees than to incorrect ones. an effective framework for problems like this is to
use id136-based learning combined with the id88 learning rule. in this
framework, we parse a sentence (i.e, perform id136) from the training set using
some initially random set of initial weights. if the resulting parse matches the cor-
responding tree in the training data, we do nothing to the weights. otherwise, we
   nd those features in the incorrect parse that are not present in the reference parse
and we lower their weights by a small amount based on the learning rate. we do this
incrementally for each sentence in our training data until the weights converge.

more recently, recurrent neural network (id56) models have demonstrated state-

of-the-art performance in shared tasks on multilingual parsing (zeman et al. 2017,dozat
et al. 2017). these neural approaches rely solely on lexical information in the form
of id27s, eschewing the use of hand-crafted features such as those de-
scribed earlier.

13.5.3 advanced issues in graph-based parsing

13.6 evaluation

as with phrase structure-based parsing, the evaluation of dependency parsers pro-
ceeds by measuring how well they work on a test-set. an obvious metric would be
exact match (em)     how many sentences are parsed correctly. this metric is quite
pessimistic, with most sentences being marked wrong. such measures are not    ne-
grained enough to guide the development process. our metrics need to be sensitive
enough to tell if actual improvements are being made.

for these reasons, the most common method for evaluating dependency parsers
are labeled and unlabeled attachment accuracy. labeled attachment refers to the
proper assignment of a word to its head along with the correct dependency relation.
unlabeled attachment simply looks at the correctness of the assigned head, ignor-
ing the dependency relation. given a system output and a corresponding reference
parse, accuracy is simply the percentage of words in an input that are assigned the
correct head with the correct relation. this metrics are usually referred to as the
labeled attachment score (las) and unlabeled attachment score (uas). finally, we
can make use of a label accuracy score (ls), the percentage of tokens with correct
labels, ignoring where the relations are coming from.

as an example, consider the reference parse and system parse for the following

example shown in fig. 13.15.
(13.11) book me the    ight through houston.

the system correctly    nds 4 of the 6 dependency relations present in the refer-
ence parse and therefore receives an las of 2/3. however, one of the 2 incorrect
relations found by the system holds between book and    ight, which are in a head-
dependent relation in the reference parse; therefore the system therefore achieves an
uas of 5/6.

beyond attachment scores, we may also be interested in how well a system is
performing on a particular kind of dependency relation, for example nsubj, across
a development corpus. here we can make use of the notions of precision and recall
introduced in chapter 8, measuring the percentage of relations labeled nsubj by
the system that were correct (precision), and the percentage of the nsubj relations

292 chapter 13

    id33

root

iobj

obj

det

nmod

case

root

x-comp

nsubj

det

nmod

case

book me the

   ight

reference

through houston

book me the    ight
system

through houston

figure 13.15 reference and system parses for book me the    ight through houston, resulting in an las of
4/6 and an uas of 5/6.

present in the development set that were in fact discovered by the system (recall).
we can employ a confusion matrix to keep track of how often each dependency type
was confused for another.

13.7 summary

this chapter has introduced the concept of dependency grammars and dependency
parsing. here   s a summary of the main points that we covered:

    in dependency-based approaches to syntax, the structure of a sentence is de-
scribed in terms of a set of binary relations that hold between the words in a
sentence. larger notions of constituency are not directly encoded in depen-
dency analyses.

ship among the words in a sentence.

    the relations in a dependency structure capture the head-dependent relation-
    dependency-based analyses provides information directly useful in further
language processing tasks including information extraction, id29
and id53

create dependency structures.

of maximum spanning tree methods from id207.

    transition-based parsing systems employ a greedy stack-based algorithm to
    graph-based methods for creating dependency structures are based on the use
    both transition-based and graph-based approaches are developed using super-
    treebanks provide the data needed to train these systems. dependency tree-
banks can be created directly by human annotators or via automatic transfor-
mation from phrase-structure treebanks.

vised machine learning techniques.

    evaluation of dependency parsers is based on labeled and unlabeled accuracy

scores as measured against withheld development and test corpora.

bibliographical and historical notes

the dependency-based approach to grammar is much older than the relatively re-
cent phrase-structure or constituency grammars that have been the primary focus of
both theoretical and computational linguistics for years. it has its roots in the an-
cient greek and indian linguistic traditions. contemporary theories of dependency

bibliographical and historical notes

293

grammar all draw heavily on the work of tesni`ere (1959). the most in   uential
dependency grammar frameworks include meaning-text theory (mtt) (mel     cuk,
1988), word grammar (hudson, 1984), functional generative description (fdg)
(sgall et al., 1986). these frameworks differ along a number of dimensions in-
cluding the degree and manner in which they deal with morphological, syntactic,
semantic and pragmatic factors, their use of multiple layers of representation, and
the set of relations used to categorize dependency relations.

automatic parsing using dependency grammars was    rst introduced into compu-
tational linguistics by early work on machine translation at the rand corporation
led by david hays. this work on id33 closely paralleled work on
constituent parsing and made explicit use of grammars to guide the parsing process.
after this early period, computational work on id33 remained inter-
mittent over the following decades. notable implementations of dependency parsers
for english during this period include link grammar (sleator and temperley, 1993),
constraint grammar (karlsson et al., 1995), and minipar (lin, 2003).

id33 saw a major resurgence in the late 1990   s with the appear-
ance of large dependency-based treebanks and the associated advent of data driven
approaches described in this chapter. eisner (1996) developed an ef   cient dynamic
programming approach to id33 based on bilexical grammars derived
from the id32. covington (2001) introduced the deterministic word by
word approach underlying current transition-based approaches. yamada and mat-
sumoto (2003) and kudo and matsumoto (2002) introduced both the shift-reduce
paradigm and the use of supervised machine learning in the form of support vector
machines to id33.

nivre (2003) de   ned the modern, deterministic, transition-based approach to de-
pendency parsing. subsequent work by nivre and his colleagues formalized and an-
alyzed the performance of numerous transition systems, training methods, and meth-
ods for dealing with non-projective language nivre and scholz 2004,nivre 2006,nivre
and nilsson 2005,nivre et al. 2007,nivre 2007.

the graph-based maximum spanning tree approach to id33 was

introduced by mcdonald et al. 2005,mcdonald et al. 2005.

the earliest source of data for training and evaluating dependency english parsers
came from the wsj id32 (marcus et al., 1993) described in chapter 10.
the use of head-   nding rules developed for use with probabilistic parsing facili-
tated the automatic extraction of dependency parses from phrase-based ones (xia
and palmer, 2001).

the long-running prague dependency treebank project (haji  c, 1998) is the most
signi   cant effort to directly annotate a corpus with multiple layers of morphological,
syntactic and semantic information. the current pdt 3.0 now contains over 1.5 m
tokens (bej  cek et al., 2013).

universal dependencies (ud) (nivre et al., 2016b) is a project directed at cre-
ating a consistent framework for dependency treebank annotation across languages
with the goal of advancing parser development across the worlds languages. under
the auspices of this effort, treebanks for over 30 languages have been annotated and
made available in a single consistent format. the ud annotation scheme evolved out
of several distinct efforts including stanford dependencies (de marneffe et al. 2006,
de marneffe and manning 2008, de marneffe et al. 2014), google   s universal part-
of-speech tags (petrov et al., 2012), and the interset interlingua for morphosyntactic
tagsets (zeman, 2008). driven in part by the ud framework, dependency treebanks
of a signi   cant size and quality are now available in over 30 languages (nivre et al.,

294 chapter 13

    id33

2016b).

the conference on natural language learning (conll) has conducted an in-
   uential series of shared tasks related to id33 over the years (buch-
holz and marsi 2006, nilsson et al. 2007, surdeanu et al. 2008a, haji  c et al. 2009).
more recent evaluations have focused on parser robustness with respect to morpho-
logically rich languages (seddah et al., 2013), and non-canonical language forms
such as social media, texts, and spoken language (petrov and mcdonald, 2012).
choi et al. (2015) presents a detailed performance analysis of 10 state-of-the-art de-
pendency parsers across an impressive range of metrics, as well as dependable, a
robust parser evaluation tool.

exercises

chapter

14 the representation of sen-

tence meaning

ishmael: surely all this is not without meaning.
herman melville, moby dick

meaning
representations

meaning
representation
languages

the approach to semantics introduced here, and elaborated on in the next two chap-
ters, is based on the idea that the meaning of linguistic expressions can be cap-
tured in formal structures called meaning representations. correspondingly, the
frameworks that specify the syntax and semantics of these representations are called
meaning representation languages. these meaning representations play a role
analogous to that of the syntactic representations introduced in earlier chapters   
they abstract away from surface forms and facilitate downstream processing.

the need for meaning representations arises when neither the raw linguistic in-
puts nor any of the syntactic structures derivable from them facilitate the kind of se-
mantic processing that is required. we need representations that bridge the gap from
linguistic inputs to the knowledge of the world needed to perform tasks. consider
the following ordinary language tasks that require some form of semantic processing
of natural language:

    deciding what to order at a restaurant by reading a menu
    learning to use a new piece of software by reading the manual
    answering essay questions on an exam
    realizing that you   ve been insulted
    following recipes
grammatical representations aren   t suf   cient to accomplish these tasks. what
is required are representations that link the linguistic elements to the non-linguistic
knowledge of the world needed to successfully accomplish the tasks:

    reading a menu and deciding what to order, giving advice about where to go
to dinner, following a recipe, and generating new recipes all require knowl-
edge about food and its preparation, what people like to eat, and what restau-
rants are like.

    answering and grading essay questions requires background knowledge about
the topic of the question, the desired knowledge level of the students, and how
such questions are normally answered.

    learning to use a piece of software by reading a manual, or giving advice
about how to use the software, requires knowledge about current computers,
the speci   c software in question, similar software applications, and knowl-
edge about users in general.

in this chapter, we assume that linguistic expressions have meaning representa-
tions that are made up of the same kind of stuff that is used to represent this kind
of everyday common-sense knowledge of the world. the process whereby such

295

296 chapter 14

    the representation of sentence meaning

   e,y having(e)    haver(e,speaker)    hadt hing(e,y)   car(y)

having:

haver:
hadthing:

speaker

car

computational
semantics

literal meaning

figure 14.1 a list of symbols, two directed graphs, and a record structure: a sampler of
meaning representations for i have a car.

representations are created and assigned to linguistic inputs is called semantic anal-
ysis, and the entire enterprise of designing meaning representations and associated
semantic analyzers is referred to as computational semantics.

to make these notions a bit more concrete, consider fig. 14.1, which shows
example meaning representations for the sentence i have a car using four com-
monly used meaning representation languages. the top row illustrates a sentence
in id85, covered in detail in section 14.3; the directed graph and its
corresponding textual form is an example of an abstract meaning representa-
tion (amr) form, to be discussed in chapter 18, and    nally a frame-based or
slot-filler representation, discussed in section 14.5 and again in chapter 17.

while there are non-trivial differences among these approaches, they all share
the notion that a meaning representation consists of structures composed from a
set of symbols, or representational vocabulary. when appropriately arranged, these
symbol structures are taken to correspond to objects, properties of objects, and rela-
tions among objects in some state of affairs being represented or reasoned about. in
this case, all four representations make use of symbols corresponding to the speaker,
a car, and a relation denoting the possession of one by the other.

importantly, these representations can be viewed from at least two distinct per-
spectives in all of these approaches: as representations of the meaning of the par-
ticular linguistic input i have a car, and as representations of the state of affairs in
some world. it is this dual perspective that allows these representations to be used
to link linguistic inputs to the world and to our knowledge of it.

this chapter introduces the basics of what is needed in a meaning representa-
tion. a number of extremely important issues are therefore deferred to later chap-
ters. the focus of this chapter is on representing the literal meaning of individual
sentences. by this, we mean representations that are derived from the core conven-
tional meanings of words and that do not re   ect much of the context in which they
occur. chapter 15 and chapter 16 introduce techniques for generating these formal
meaning representations for linguistic inputs. chapter 17 focuses on the extraction
of entities, relations events and times, chapter 18 and chapter 19 on semantic struc-
ture of verbs and their arguments, while the task of producing representations for

h / have-01c / cari / i arg0arg1(h / have-01        arg0: (i / i)        arg1: (c / car))14.1

    computational desiderata for representations

297

larger stretches of discourse is deferred to chapter 20 and chapter 21.

there are four major parts to this chapter. section 14.1 explores some of the key
computational requirements for what we need in a meaning representation language.
section 14.2 discusses how we can provide some guarantees that these representa-
tions will actually do what we need them to do   provide a correspondence to the
state of affairs being represented. section 14.3 then introduces id85,
which has historically been the primary technique for investigating issues in natural
language semantics. section 14.4 then describes how fol can be used to capture the
semantics of events and states in english.

14.1 computational desiderata for representations

we begin by considering the issue of why meaning representations are needed and
what they should do for us. to focus this discussion, we use the task of giving advice
about restaurants to tourists. assume that we have a computer system that accepts
spoken language inputs from tourists and constructs appropriate responses using a
knowledge base of relevant domain knowledge. a series of examples will serve to
introduce some of the basic requirements that a meaning representation must ful   ll
and some of the complications that inevitably arise in the process of designing such
meaning representations.

14.1.1 veri   ability
consider the following simple question:

(14.1) does maharani serve vegetarian food?

this example illustrates the most basic requirement for a meaning representation:
it must be possible to use the representation to determine the relationship between
the meaning of a sentence and the state of the world as we know it. in other words,
we need to be able to determine the truth of our representations. section 14.2 ex-
plores the standard approach to this topic in some detail. for now, let   s assume that
computational semantic systems require the ability to compare, or match, meaning
representations associated with linguistic expressions with formal representations in
a knowledge base, its store of information about its world.

in this example, assume that the meaning of this question involves the proposi-
tion maharani serves vegetarian food. for now, we will simply gloss this represen-
tation as

serves(maharani,vegetarianfood)

(14.2)

this representation of the input can be matched against our knowledge base of facts
about a set of restaurants. if the system    nds a representation matching this propo-
sition in its knowledge base, it can return an af   rmative answer. otherwise, it must
either say no if its knowledge of local restaurants is complete, or say that it doesn   t
know if there is reason to believe that its knowledge is incomplete.

this notion, known as veri   ability, describes a system   s ability to compare the
state of affairs described by a representation to the state of affairs in some world as
modeled in a knowledge base.

knowledge base

veri   ability

298 chapter 14

    the representation of sentence meaning

vagueness

14.1.2 unambiguous representations
semantics, like all the other domains we have studied, is subject to ambiguity.
speci   cally, individual linguistic expressions can have different meaning represen-
tations assigned to them based on the circumstances in which they occur. consider
the following example from the berp corpus:
(14.3) i wanna eat someplace that   s close to icsi.
given the allowable argument structures for the verb eat, this sentence can either
mean that the speaker wants to eat at some nearby location, or under a godzilla-as-
speaker interpretation, the speaker may want to devour some nearby location. the
answer generated by the system for this request will depend on which interpretation
is chosen as the correct one.

since ambiguities such as this abound in all genres of all languages, some means
of determining that certain interpretations are preferable (or alternatively, not as
preferable) to others is needed. the various linguistic phenomena that give rise
to such ambiguities and the techniques that can be employed to deal with them are
discussed in detail in the next four chapters.

our concern in this chapter, however, is with the status of our meaning repre-
sentations with respect to ambiguity, and not with the means by which we might
arrive at correct interpretations. since we reason about, and act upon, the semantic
content of linguistic inputs, the    nal representation of an input   s meaning should be
free from any ambiguity.1

a concept closely related to ambiguity is vagueness. like ambiguity, vagueness
can make it dif   cult to determine what to do with a particular input on the basis
of its meaning representation. vagueness, however, does not give rise to multiple
representations. consider the following request as an example:
(14.4) i want to eat italian food.
while the use of the phrase italian food may provide enough information for a
restaurant advisor to provide reasonable recommendations, it is nevertheless quite
vague as to what the user really wants to eat. therefore, a vague representation
of the meaning of this phrase may be appropriate for some purposes, while a more
speci   c representation may be needed for other purposes. it will, therefore, be ad-
vantageous for a meaning representation language to support representations that
maintain a certain level of vagueness. note that it is not always easy to distinguish
ambiguity from vagueness. zwicky and sadock (1975) provide a useful set of tests
that can be used as diagnostics.

14.1.3 canonical form
the notion that single sentences can be assigned multiple meanings leads to the
related phenomenon of distinct inputs that should be assigned the same meaning
representation. consider the following alternative ways of expressing (14.1):
(14.5) does maharani have vegetarian dishes?
(14.6) do they have vegetarian food at maharani?
(14.7) are vegetarian dishes served at maharani?
(14.8) does maharani serve vegetarian fare?

1 this does not preclude the use of intermediate semantic representations that maintain some level of
ambiguity on the way to a single unambiguous form. examples of such representations are discussed in
chapter 15.

canonical form

14.1

    computational desiderata for representations

299

given that these alternatives use different words and have widely varying syn-
tactic analyses, it would not be unreasonable to expect them to have quite different
meaning representations. such a situation would, however, have undesirable con-
sequences for how we determine the truth of our representations. if the system   s
knowledge base contains only a single representation of the fact in question, then
the representations underlying all but one of our alternatives will fail to produce
a match. we could, of course, store all possible alternative representations of the
same fact in the knowledge base, but doing so would lead to an enormous number
of problems related to keeping such a knowledge base consistent.

the way out of this dilemma is motivated by the fact that since the answers given
for each of these alternatives should be the same in all situations, we might say that
they all mean the same thing, at least for the purposes of giving restaurant recom-
mendations. in other words, at least in this domain, we can legitimately consider
assigning the same meaning representation to the propositions underlying each of
these requests. taking such an approach would guarantee that our simple scheme
for answering yes-no questions will still work.

the notion that distinct inputs that mean the same thing should have the same
meaning representation is known as the doctrine of canonical form. this approach
greatly simpli   es various reasoning tasks since systems need only deal with a single
meaning representation for a potentially wide range of expressions.

canonical form does complicate the task of semantic analysis. to see this, note
that the alternatives given above use completely different words and syntax to refer
to vegetarian fare and to what restaurants do with it. to assign the same representa-
tion to all of these requests, our system would have to conclude that vegetarian fare,
vegetarian dishes, and vegetarian food refer to the same thing in this context, that
the use here of having and serving are similarly equivalent, and that the different
syntactic parses underlying these requests are all compatible with the same meaning
representation.

being able to assign the same representation to such diverse inputs is a tall or-
der. fortunately, systematic meaning relationships among word senses and among
grammatical constructions can be exploited to make this task tractable. consider the
issue of the meanings of the words food, dish, and fare in these examples. a little
introspection or a glance at a dictionary reveals that these words have a fair number
of distinct uses. however, it also reveals that at least one sense is shared among them
all. if a system has the ability to choose that shared sense, then an identical meaning
representation can be assigned to the phrases containing these words.

just as there are systematic relationships among the meanings of different words,
there are similar relationships related to the role that syntactic analyses play in as-
signing meanings to sentences. speci   cally, alternative syntactic analyses often have
meanings that are, if not identical, at least systematically related to one another.
consider the following pair of examples:
(14.9) maharani serves vegetarian dishes.
(14.10) vegetarian dishes are served by maharani.

despite the different placement of the arguments to serve in these examples, we
can still assign maharani and vegetarian dishes to the same roles in both of these
examples because of our knowledge of the relationship between active and passive
sentence constructions. in particular, we can use knowledge of where grammatical
subjects and direct objects appear in these constructions to assign maharani to the
role of the server, and vegetarian dishes to the role of thing being served in both
of these examples, despite the fact that they appear in different surface locations.

300 chapter 14

    the representation of sentence meaning

the precise role of the grammar in the construction of meaning representations is
covered in chapter 15.

id136

id136 and variables

14.1.4
continuing with the topic of the computational purposes that meaning representa-
tions should serve, we should consider more complex requests such as the following:
(14.11) can vegetarians eat at maharani?
here, it would be a mistake to invoke canonical form to force our system to as-
sign the same representation to this request as for the previous examples. that this
request results in the same answer as the others arises, not because they mean the
same thing, but because there is a common-sense connection between what vegetar-
ians eat and what vegetarian restaurants serve. this is a fact about the world and
not a fact about any particular kind of linguistic regularity. this implies that no
approach based on canonical form and simple matching will give us an appropriate
answer to this request. what is needed is a systematic way to connect the meaning
representation of this request with the facts about the world as they are represented
in a knowledge base.

we use the term id136 to refer generically to a system   s ability to draw valid
conclusions based on the meaning representation of inputs and its store of back-
ground knowledge. it must be possible for the system to draw conclusions about the
truth of propositions that are not explicitly represented in the knowledge base but
that are nevertheless logically derivable from the propositions that are present.

now consider the following somewhat more complex request:

(14.12) i   d like to    nd a restaurant where i can get vegetarian food.
unlike our previous examples, this request does not make reference to any particular
restaurant. the user is expressing a desire for information about an unknown and
unnamed entity that is a restaurant that serves vegetarian food. since this request
does not mention any particular restaurant, the kind of simple matching-based ap-
proach we have been advocating is not going to work. rather, answering this request
requires a more complex kind of matching that involves the use of variables. we can
gloss a representation containing such variables as follows:

serves(x,vegetarianfood)

(14.13)

matching such a proposition succeeds only if the variable x can be replaced by some
known object in the knowledge base in such a way that the entire proposition will
then match. the concept that is substituted for the variable can then be used to ful   ll
the user   s request. of course, this simple example only hints at the issues involved
in the use of such variables. suf   ce it to say that linguistic inputs contain many
instances of all kinds of inde   nite references, and it is, therefore, critical for any
meaning representation language to be able to handle this kind of expression.

14.1.5 expressiveness
finally, to be useful, a meaning representation scheme must be expressive enough
to handle a wide range of subject matter. the ideal situation would be to have a sin-
gle meaning representation language that could adequately represent the meaning
of any sensible natural language utterance. although this is probably too much to
expect from any single representational system, id85, as described in

14.2

    model-theoretic semantics

301

section 14.3, is expressive enough to handle quite a lot of what needs to be repre-
sented.

14.2 model-theoretic semantics

the last two sections focused on various desiderata for meaning representations and
on some of the ways in which natural languages convey meaning. we haven   t said
much formally about what it is about meaning representation languages that allows
them to do all the things we want them to.
in particular, we might like to have
some kind of guarantee that these representations can do the work that we require of
them: bridge the gap from merely formal representations to representations that tell
us something about some state of affairs in the world.

to see how we might provide such a guarantee, let   s start with the basic notions
shared by most meaning representation schemes. what they all have in common
is the ability to represent objects, properties of objects, and relations among ob-
jects. this ability can be formalized by the notion of a model. a model is a formal
construct that stands for the particular state of affairs in the world. expressions in
a meaning representation language can be mapped in a systematic way to the ele-
ments of the model. if the model accurately captures the facts we   re interested in
concerning some state of affairs, then a consistent mapping between the meaning
representation and the model provides the bridge between the meaning representa-
tion and world being considered. as we show, models provide a surprisingly simple
and powerful way to ground the expressions in meaning representation languages.

first, some terminology. the vocabulary of a meaning representation consists of
two parts: the non-logical vocabulary and the logical vocabulary. the non-logical
vocabulary consists of the open-ended set of names for the objects, properties, and
relations that make up the world we   re trying to represent. these appear in various
schemes as predicates, nodes, labels on links, or labels in slots in frames, the log-
ical vocabulary consists of the closed set of symbols, operators, quanti   ers, links,
etc., that provide the formal means for composing expressions in a given meaning
representation language.

we   ll start by requiring that each element of the non-logical vocabulary have a
denotation in the model. by denotation, we simply mean that every element of the
non-logical vocabulary corresponds to a    xed, well-de   ned part of the model. let   s
start with objects, the most basic notion in most representational schemes. the do-
main of a model is simply the set of objects that are part of the application, or state
of affairs, being represented. each distinct concept, category, or individual in an ap-
plication denotes a unique element in the domain. a domain is therefore formally a
set. note that it isn   t mandatory that every element of the domain have a correspond-
ing concept in our meaning representation; it   s perfectly acceptable to have domain
elements that aren   t mentioned or conceived of in the meaning representation. nor
do we require that elements of the domain have a single denoting concept in the
meaning representation; a given element in the domain might have several distinct
representations denoting it, such as mary, wifeof(abe), or motherof(robert).

we can capture properties of objects in a model by denoting those domain ele-
ments that have the property in question; that is, properties denote sets. similarly,
relations among objects denote sets of ordered lists, or tuples, of domain elements
that take part in the corresponding relations. this approach to properties and rela-
tions is thus an extensional one: the denotation of properties like red is the set of

model

non-logical
vocabulary

logical
vocabulary

denotation

domain

extensional

302 chapter 14

    the representation of sentence meaning

things we think are red, the denotation of a relation like married is simply set of
pairs of domain elements that are married. to summarize:

interpretation

    objects denote elements of the domain
    properties denote sets of elements of the domain
    relations denote sets of tuples of elements of the domain
there is one additional element that we need to make this scheme work. we
need a mapping that systematically gets us from our meaning representation to the
corresponding denotations. more formally, we need a function that maps from the
non-logical vocabulary of our meaning representation to the proper denotations in
the model. we   ll call such a mapping an interpretation.

to make these notions more concrete, let   s return to our restaurant advice appli-
cation. assume that our application domain consists of sets of restaurants, patrons,
and various facts about the likes and dislikes of the patrons, and facts about the
restaurants such as their cuisine, typical cost, and noise level.

to begin populating our domain, d, let   s assume that we   re dealing with four pa-
trons designated by the non-logical symbols matthew, franco, katie, and caroline.
these four symbols will denote four unique domain elements. we   ll use the con-
stants a,b,c and, d to stand for these domain elements. note that we   re deliberately
using meaningless, non-mnemonic names for our domain elements to emphasize the
fact that whatever it is that we know about these entities has to come from the formal
properties of the model and not from the names of the symbols. continuing, let   s
assume that our application includes three restaurants, designated as frasca, med,
and rio in our meaning representation, that denote the domain elements e, f , and g.
finally, let   s assume that we   re dealing with the three cuisines italian, mexican, and
eclectic, denoted by h,i, and j in our model.

having populated the domain, let   s move on to the properties and relations we
believe to be true in this particular state of affairs. for our application, we need to
represent various properties of restaurants such as the fact that some are noisy or
expensive. properties like noisy denote the subset of restaurants from our domain
that are known to be noisy. two-place relational notions, such as which restaurants
individual patrons like, denote ordered pairs, or tuples, of the objects from the do-
main. and, since we decided to represent cuisines as objects in our model, we can
capture which restaurants serve which cuisines as a set of tuples. one possible state
of affairs using this scheme is given in fig. 14.2.

given this simple scheme, we can ground our meaning representations by con-
sulting the appropriate denotations in the corresponding model. for example, we can
evaluate a representation claiming that matthew likes the rio, or that the med serves
italian by mapping the objects in the meaning representations to their corresponding
domain elements and mapping any links, predicates, or slots in the meaning repre-
sentation to the appropriate relations in the model. more concretely, we can verify
a representation asserting that matthew likes frasca by    rst using our interpretation
function to map the symbol matthew to its denotation a, frasca to e, and the likes
relation to the appropriate set of tuples. we then check that set of tuples for the
presence of the tuple (cid:104)a,e(cid:105). if, as it is in this case, the tuple is present in the model,
then we can conclude that matthew likes frasca is true; if it isn   t then we can   t.
this is all pretty straightforward   we   re using sets and operations on sets to
ground the expressions in our meaning representations. of course, the more inter-
esting part comes when we consider more complex examples such as the following:
(14.14) katie likes the rio and matthew likes the med.

14.2

    model-theoretic semantics

303

domain
matthew, franco, katie and caroline
frasca, med, rio
italian, mexican, eclectic

d = {a,b,c,d,e, f ,g,h,i, j}
a,b,c,d
e, f ,g
h,i, j

properties
noisy

frasca, med, and rio are noisy

relations
likes

matthew likes the med
katie likes the med and rio
franco likes frasca
caroline likes the med and rio

serves

med serves eclectic
rio serves mexican
frasca serves italian

figure 14.2 a model of the restaurant world.

noisy = {e, f ,g}

likes = {(cid:104)a, f(cid:105),(cid:104)c, f(cid:105),(cid:104)c,g(cid:105),(cid:104)b,e(cid:105),(cid:104)d, f(cid:105),(cid:104)d,g(cid:105)}

serves = {(cid:104) f , j(cid:105),(cid:104)g,i(cid:105),(cid:104)e,h(cid:105)}

(14.15) katie and caroline like the same restaurants.
(14.16) franco likes noisy, expensive restaurants.
(14.17) not everybody likes frasca.

our simple scheme for grounding the meaning of representations is not adequate
for examples such as these. plausible meaning representations for these examples
will not map directly to individual entities, properties, or relations. instead, they
involve complications such as conjunctions, equality, quanti   ed variables, and nega-
tions. to assess whether these statements are consistent with our model, we   ll have
to tear them apart, assess the parts, and then determine the meaning of the whole
from the meaning of the parts according to the details of how the whole is assem-
bled.

consider the    rst example given above. a meaning representation for an exam-
ple like this will include two distinct propositions expressing the individual patron   s
preferences, conjoined with some kind of implicit or explicit conjunction operator.
our model doesn   t have a relation that encodes pairwise preferences for all of the
patrons and restaurants in our model, nor does it need to. we know from our model
that matthew likes the med and separately that katie likes the rio (that is, the tuples
(cid:104)a, f(cid:105) and (cid:104)c,g(cid:105) are members of the set denoted by the likes relation). all we really
need to know is how to deal with the semantics of the conjunction operator. if we
assume the simplest possible semantics for the english word and, the whole state-
ment is true if it is the case that each of the components is true in our model. in this
case, both components are true since the appropriate tuples are present and therefore
the sentence as a whole is true.

what we   ve done with this example is provide a truth-conditional semantics
for the assumed conjunction operator in some meaning representation. that is,
we   ve provided a method for determining the truth of a complex expression from
the meanings of the parts (by consulting a model) and the meaning of an operator by
consulting a truth table. meaning representation languages are truth-conditional to
the extent that they give a formal speci   cation as to how we can determine the mean-

truth-
conditional
semantics

304 chapter 14

    the representation of sentence meaning

formula     atomicformula

| formula connective formula
| quanti   er variable, . . . formula
|    formula
(formula)
|

atomicformula     predicate(term, . . .)
term     function(term, . . .)

| constant
| variable

connective         |     | =   
quanti   er         |    
constant     a | vegetarianfood | maharani      
variable     x | y |       
predicate     serves | near |       
function     locationof | cuisineof |       

figure 14.3 a context-free grammar speci   cation of the syntax of id85 rep-
resentations. adapted from russell and norvig (2002)
.

ing of complex sentences from the meaning of their parts. in particular, we need to
know the semantics of the entire logical vocabulary of the meaning representation
scheme being used.

note that although the details of how this happens depends on details of the
particular meaning representation being used, it should be clear that assessing the
truth conditions of examples like these involves nothing beyond the simple set op-
erations we   ve been discussing. we return to these issues in the next section, where
we discuss them in the context of the semantics of id85.

14.3 id85

id85 (fol) is a    exible, well-understood, and computationally tractable
meaning representation language that satis   es many of the desiderata given in sec-
tion 14.1. it provides a sound computational basis for the veri   ability, id136,
and expressiveness requirements, as well as a sound model-theoretic semantics.

an additional attractive feature of fol is that it makes very few speci   c com-
mitments as to how things ought to be represented. and, the speci   c commitments
it does make are ones that are fairly easy to live with and that are shared by many of
the schemes mentioned earlier; the represented world consists of objects, properties
of objects, and relations among objects.

the remainder of this section introduces the basic syntax and semantics of fol

and then describes the application of fol to the representation of events.

14.3.1 basic elements of id85
let   s explore fol by    rst examining its various atomic elements and then showing
how they can be composed to create larger meaning representations. figure 14.3,
which provides a complete context-free grammar for the particular syntax of fol
that we will use, is our roadmap for this section.

let   s begin by examining the notion of a term, the fol device for representing

term

constants

functions

variable

14.3

    id85

305

objects. as can be seen from fig. 14.3, fol provides three ways to represent these
basic building blocks: constants, functions, and variables. each of these devices can
be thought of as designating an object in the world under consideration.

constants in fol refer to speci   c objects in the world being described. such
constants are conventionally depicted as either single capitalized letters such as a
and b or single capitalized words that are often reminiscent of proper nouns such as
maharani and harry. like programming language constants, fol constants refer
to exactly one object. objects can, however, have multiple constants that refer to
them.

functions in fol correspond to concepts that are often expressed in english as
genitives such as frasca   s location. a fol translation of such an expression might
look like the following.

locationof (frasca)

(14.18)

fol functions are syntactically the same as single argument predicates. it is im-
portant to remember, however, that while they have the appearance of predicates,
they are in fact terms in that they refer to unique objects. functions provide a con-
venient way to refer to speci   c objects without having to associate a named constant
with them. this is particularly convenient in cases in which many named objects,
like restaurants, have a unique concept such as a location associated with them.

variables are variable our    nal fol mechanism for referring to objects. vari-
ables, depicted as single lower-case letters, let us make assertions and draw infer-
ences about objects without having to make reference to any particular named ob-
ject. this ability to make statements about anonymous objects comes in two    avors:
making statements about a particular unknown object and making statements about
all the objects in some arbitrary world of objects. we return to the topic of variables
after we have presented quanti   ers, the elements of fol that make variables useful.
now that we have the means to refer to objects, we can move on to the fol
mechanisms that are used to state relations that hold among objects. predicates are
symbols that refer to, or name, the relations that hold among some    xed number
of objects in a given domain. returning to the example introduced informally in
section 14.1, a reasonable fol representation for maharani serves vegetarian food
might look like the following formula:

serves(maharani,vegetarianfood)

(14.19)

this fol sentence asserts that serves, a two-place predicate, holds between the
objects denoted by the constants maharani and vegetarianfood.

a somewhat different use of predicates is illustrated by the following fairly typ-

ical representation for a sentence like maharani is a restaurant:

restaurant(maharani)

(14.20)

this is an example of a one-place predicate that is used, not to relate multiple objects,
but rather to assert a property of a single object. in this case, it encodes the category
membership of maharani.

with the ability to refer to objects, to assert facts about objects, and to relate
objects to one another, we can create rudimentary composite representations. these
representations correspond to the atomic formula level in fig. 14.3. this ability
to compose complex representations is, however, not limited to the use of single
predicates. larger composite representations can also be put together through the
use of logical connectives. as can be seen from fig. 14.3, logical connectives let

logical
connectives

306 chapter 14

    the representation of sentence meaning

us create larger representations by conjoining logical formulas using one of three
operators. consider, for example, the following berp sentence and one possible
representation for it:
(14.21) i only have    ve dollars and i don   t have a lot of time.

have(speaker,fivedollars)     have(speaker,lotoftime)

(14.22)

the semantic representation for this example is built up in a straightforward way
from semantics of the individual clauses through the use of the     and    operators.
note that the recursive nature of the grammar in fig. 14.3 allows an in   nite number
of logical formulas to be created through the use of these connectives. thus, as with
syntax, we can use a    nite device to create an in   nite number of representations.

quanti   ers

14.3.2 variables and quanti   ers
we now have all the machinery necessary to return to our earlier discussion of vari-
ables. as noted above, variables are used in two ways in fol: to refer to particular
anonymous objects and to refer generically to all objects in a collection. these two
uses are made possible through the use of operators known as quanti   ers. the two
operators that are basic to fol are the existential quanti   er, which is denoted     and
is pronounced as    there exists   , and the universal quanti   er, which is denoted     and
is pronounced as    for all   .
the need for an existentially quanti   ed variable is often signaled by the presence

of an inde   nite noun phrase in english. consider the following example:
(14.23) a restaurant that serves mexican food near icsi.
here, reference is being made to an anonymous object of a speci   ed category with
particular properties. the following would be a reasonable representation of the
meaning of such a phrase:

   xrestaurant(x)     serves(x,mexicanfood)

    near((locationof (x),locationof (icsi))

(14.24)

the existential quanti   er at the head of this sentence instructs us on how to
interpret the variable x in the context of this sentence. informally, it says that for
this sentence to be true there must be at least one object such that if we were to
substitute it for the variable x, the resulting sentence would be true. for example,
if aycaramba is a mexican restaurant near icsi, then substituting aycaramba for x
results in the following logical formula:

restaurant(aycaramba)    serves(aycaramba,mexicanfood)
   near((locationof (aycaramba),locationof (icsi))

(14.25)

based on the semantics of the     operator, this sentence will be true if all of its
three component atomic formulas are true. these in turn will be true if they are
either present in the system   s knowledge base or can be inferred from other facts in
the knowledge base.

the use of the universal quanti   er also has an interpretation based on substi-
tution of known objects for variables. the substitution semantics for the universal
quanti   er takes the expression for all quite literally; the     operator states that for the
logical formula in question to be true, the substitution of any object in the knowledge
base for the universally quanti   ed variable should result in a true formula. this is in

14.3

    id85

307

marked contrast to the     operator, which only insists on a single valid substitution
for the sentence to be true.

consider the following example:

(14.26) all vegetarian restaurants serve vegetarian food.

a reasonable representation for this sentence would be something like the following:

   xvegetarianrestaurant(x) =    serves(x,vegetarianfood)

(14.27)

for this sentence to be true, it must be the case that every substitution of a known
object for x must result in a sentence that is true. we can divide the set of all possible
substitutions into the set of objects consisting of vegetarian restaurants and the set
consisting of everything else. let us    rst consider the case in which the substituted
object actually is a vegetarian restaurant; one such substitution would result in the
following sentence:

vegetarianrestaurant(maharani) =    serves(maharani,vegetarianfood)
if we assume that we know that the consequent clause

(14.28)

serves(maharani,vegetarianfood)

(14.29)

is true, then this sentence as a whole must be true. both the antecedent and the
consequent have the value true and, therefore, according to the    rst two rows of
fig. 14.4 on page 309 the sentence itself can have the value true. this result will be
the same for all possible substitutions of terms representing vegetarian restaurants
for x.

remember, however, that for this sentence to be true, it must be true for all
possible substitutions. what happens when we consider a substitution from the set
of objects that are not vegetarian restaurants? consider the substitution of a non-
vegetarian restaurant such as ay caramba   s for the variable x:

vegetarianrestaurant(aycaramba) =    serves(aycaramba,vegetarianfood)
since the antecedent of the implication is false, we can determine from fig. 14.4

that the sentence is always true, again satisfying the     constraint.
note that it may still be the case that ay caramba serves vegetarian food with-
out actually being a vegetarian restaurant. note also, that despite our choice of
examples, there are no implied categorical restrictions on the objects that can be
substituted for x by this kind of reasoning. in other words, there is no restriction of
x to restaurants or concepts related to them. consider the following substitution:

vegetarianrestaurant(carburetor) =    serves(carburetor,vegetarianfood)
here the antecedent is still false, and hence, the rule remains true under this kind of
irrelevant substitution.

to review, variables in logical formulas must be either existentially (   ) or uni-
versally (   ) quanti   ed. to satisfy an existentially quanti   ed variable, at least one
substitution must result in a true sentence. sentences with universally quanti   ed
variables must be true under all possible substitutions.

308 chapter 14

    the representation of sentence meaning

lambda
notation

14.3.3 lambda notation
the    nal element we need to complete our discussion of fol is called the lambda
notation (church, 1940). this notation provides a way to abstract from fully speci-
   ed fol formula in a way that will be particularly useful for semantic analysis. the
lambda notation extends the syntax of fol to include expressions of the following
form:

   x.p(x)

(14.30)
such expressions consist of the greek symbol    , followed by one or more variables,
followed by a fol formula that makes use of those variables.

the usefulness of these    -expressions is based on the ability to apply them to
logical terms to yield new fol expressions where the formal parameter variables are
bound to the speci   ed terms. this process is known as   -reduction and consists of
a simple textual replacement of the    variables with the speci   ed fol terms, accom-
panied by the subsequent removal of the    . the following expressions illustrate the
application of a    -expression to the constant a, followed by the result of performing
a    -reduction on this expression:

   -reduction

   x.p(x)(a)

p(a)

(14.31)

an important and useful variation of this technique is the use of one    -expression

as the body of another as in the following expression:

   x.   y.near(x,y)

(14.32)

this fairly abstract expression can be glossed as the state of something being
near something else. the following expressions illustrate a single    -application and
subsequent reduction with this kind of embedded    -expression:

   x.   y.near(x,y)(bacaro)

   y.near(bacaro,y)

(14.33)

the important point here is that the resulting expression is still a    -expression;
the    rst reduction bound the variable x and removed the outer    , thus revealing the
inner expression. as might be expected, this resulting    -expression can, in turn,
be applied to another term to arrive at a fully speci   ed logical formula, as in the
following:

   y.near(bacaro,y)(centro)

near(bacaro,centro)

(14.34)

currying

this general technique, called currying2 (sch  onk   nkel, 1924) is a way of con-
verting a predicate with multiple arguments into a sequence of single-argument pred-
icates.

as we show in chapter 15, the    -notation provides a way to incrementally gather
arguments to a predicate when they do not all appear together as daughters of the
predicate in a parse tree.

2 currying is the standard term, although heim and kratzer (1998) present an interesting argument for
the term sch  onk   nkelization over currying, since curry later built on sch  on   nkel   s work.

14.3

    id85

309

14.3.4 the semantics of id85
the various objects, properties, and relations represented in a fol knowledge base
acquire their meanings by virtue of their correspondence to objects, properties, and
relations out in the external world being modeled. we can accomplish this by em-
ploying the model-theoretic approach introduced in section 14.2. recall that this
approach employs simple set-theoretic notions to provide a truth-conditional map-
ping from the expressions in a meaning representation to the state of affairs being
modeled. we can apply this approach to fol by going through all the elements in
fig. 14.3 on page 304 and specifying how each should be accounted for.

we can start by asserting that the objects in our world, fol terms, denote ele-
ments in a domain, and asserting that atomic formulas are captured either as sets of
domain elements for properties, or as sets of tuples of elements for relations. as an
example, consider the following:
(14.35) centro is near bacaro.

capturing the meaning of this example in fol involves identifying the terms
and predicates that correspond to the various grammatical elements in the sentence
and creating logical formulas that capture the relations implied by the words and
syntax of the sentence. for this example, such an effort might yield something like
the following:

near(centro,bacaro)

(14.36)
the meaning of this logical formula is based on whether the domain elements de-
noted by the terms centro and bacaro are contained among the tuples denoted by
the relation denoted by the predicate near in the current model.

the interpretations of formulas involving logical connectives is based on the
meaning of the components in the formulas combined with the meanings of the
connectives they contain. figure 14.4 gives interpretations for each of the logical
operators shown in fig. 14.3.

p

q

false
false
true
true

false
true
false
true

   p
true
true
false
false

p     q
false
false
false
true

p     q
false
true
true
true

p =   q

true
true
false
true

figure 14.4 truth table giving the semantics of the various logical connectives.

the semantics of the     (and) and    (not) operators are fairly straightforward,
and are correlated with at least some of the senses of the corresponding english
terms. however, it is worth pointing out that the     (or) operator is not disjunctive
in the same way that the corresponding english word is, and that the =    (im-
plies) operator is only loosely based on any common-sense notions of implication
or causation.

the    nal bit we need to address involves variables and quanti   ers. recall that
there are no variables in our set-based models, only elements of the domain and
relations that hold among them. we can provide a model-based account for formulas
with variables by employing the notion of a substitution introduced earlier on page
306. formulas involving     are true if a substitution of terms for variables results
in a formula that is true in the model. formulas involving     must be true under all
possible substitutions.

310 chapter 14

    the representation of sentence meaning

id136

14.3.5
one of the most important desiderata given in section 14.1 for a meaning rep-
resentation language is that it should support id136, or deduction. that is, the
ability to add valid new propositions to a knowledge base or to determine the truth of
propositions not explicitly contained within a knowledge base. this section brie   y
discusses modus ponens, the most widely implemented id136 method provided
by fol. applications of modus ponens to id136 in discourse is discussed in
chapter 21.

modus ponens is a familiar form of id136 that corresponds to what is in-
formally known as if-then reasoning. we can abstractly de   ne modus ponens as
follows, where    and    should be taken as fol formulas:

modus ponens

  
   =      

  

(14.37)

a schema like this indicates that the formula below the line can be inferred from
the formulas above the line by some form of id136. modus ponens simply states
that if the left-hand side of an implication rule is true, then the right-hand side of the
rule can be inferred. in the following discussions, we will refer to the left-hand side
of an implication as the antecedent and the right-hand side as the consequent.

for a typical use of modus ponens, consider the following example, which uses

a rule from the last section:

forward
chaining

backward
chaining

vegetarianrestaurant(leaf )
   xvegetarianrestaurant(x) =    serves(x,vegetarianfood)

serves(leaf ,vegetarianfood)

(14.38)

here, the formula vegetarianrestaurant(leaf ) matches the antecedent of the rule,
thus allowing us to use modus ponens to conclude serves(leaf ,vegetarianfood).
modus ponens can be put to practical use in one of two ways: forward chaining
and backward chaining. in forward chaining systems, modus ponens is used in
precisely the manner just described. as individual facts are added to the knowledge
base, modus ponens is used to    re all applicable implication rules. in this kind of
arrangement, as soon as a new fact is added to the knowledge base, all applicable
implication rules are found and applied, each resulting in the addition of new facts to
the knowledge base. these new propositions in turn can be used to    re implication
rules applicable to them. the process continues until no further facts can be deduced.
the forward chaining approach has the advantage that facts will be present in
the knowledge base when needed, because, in a sense all id136 is performed in
advance. this can substantially reduce the time needed to answer subsequent queries
since they should all amount to simple lookups. the disadvantage of this approach
is that facts that will never be needed may be inferred and stored.

in backward chaining, modus ponens is run in reverse to prove speci   c propo-
sitions called queries. the    rst step is to see if the query formula is true by determin-
ing if it is present in the knowledge base. if it is not, then the next step is to search
for applicable implication rules present in the knowledge base. an applicable rule is
one whereby the consequent of the rule matches the query formula. if there are any
such rules, then the query can be proved if the antecedent of any one them can be
shown to be true. not surprisingly, this can be performed recursively by backward

14.4

    event and state representations

311

chaining on the antecedent as a new query. the prolog programming language is a
backward chaining system that implements this strategy.

to see how this works, let   s assume that we have been asked to verify the truth of
the proposition serves(leaf ,vegetarianfood), assuming the facts given above the
line in (14.38). since this proposition is not present in the knowledge base, a search
for an applicable rule is initiated resulting in the rule given above. after substituting
the constant leaf for the variable x, our next task is to prove the antecedent of the
rule, vegetarianrestaurant(leaf ), which, of course, is one of the facts we are given.
note that it is critical to distinguish between reasoning by backward chaining
from queries to known facts and reasoning backwards from known consequents to
unknown antecedents. to be speci   c, by reasoning backwards we mean that if the
consequent of a rule is known to be true, we assume that the antecedent will be as
well. for example, let   s assume that we know that serves(leaf ,vegetarianfood) is
true. since this fact matches the consequent of our rule, we might reason backwards
to the conclusion that vegetarianrestaurant(leaf ).

while backward chaining is a sound method of reasoning, reasoning backwards
is an invalid, though frequently useful, form of plausible reasoning. plausible rea-
soning from consequents to antecedents is known as abduction, and as we show in
chapter 21, is often useful in accounting for many of the id136s people make
while analyzing extended discourses.

while forward and backward reasoning are sound, neither is complete. this
means that there are valid id136s that cannot be found by systems using these
methods alone. fortunately, there is an alternative id136 technique called reso-
lution that is sound and complete. unfortunately, id136 systems based on res-
olution are far more computationally expensive than forward or backward chaining
systems. in practice, therefore, most systems use some form of chaining and place
a burden on knowledge-base developers to encode the knowledge in a fashion that
permits the necessary id136s to be drawn.

abduction

complete

resolution

14.4 event and state representations

much of the semantics that we wish to capture consists of representations of states
and events. states are conditions, or properties, that remain unchanged over an
extended period of time, and events denote changes in some state of affairs. the
representation of both states and events may involve a host of participants, props,
times and locations.

the representations for events and states that we have used thus far have con-
sisted of single predicates with as many arguments as are needed to incorporate all
the roles associated with a given example. for example, the representation for leaf
serves vegetarian fare consists of a single predicate with arguments for the entity
doing the serving and the thing served.

serves(leaf ,vegetarianfare)

(14.39)

this approach assumes that the predicate used to represent an event verb has the
same number of arguments as are present in the verb   s syntactic subcategorization
frame. unfortunately, this is clearly not always the case. consider the following
examples of the verb eat:
(14.40) i ate.

312 chapter 14

    the representation of sentence meaning

arity

event variable

neo-
davidsonian

(14.41) i ate a turkey sandwich.
(14.42) i ate a turkey sandwich at my desk.
(14.43) i ate at my desk.
(14.44) i ate lunch.
(14.45) i ate a turkey sandwich for lunch.
(14.46) i ate a turkey sandwich for lunch at my desk.

clearly, choosing the correct number of arguments for the predicate represent-
ing the meaning of eat is a tricky problem. these examples introduce    ve distinct
arguments, or roles, in an array of different syntactic forms, locations, and combina-
tions. unfortunately, predicates in fol have    xed arity     they take a    xed number
of arguments.

to address this problem, we introduce the notion of an event variable to allow
us to make assertions about particular events. to do this, we can refactor our event
predicates to have an existentially quanti   ed variable as their    rst, and only, argu-
ment. using this event variable, we can introduce additional predicates to represent
the other information we have about the event. these predicates take an event vari-
able as their    rst argument and related fol terms as their second argument. the
following formula illustrates this scheme with the meaning representation of 14.41
from our earlier discussion.

   e eating(e)     eater(e,speaker)    eaten(e,turkeysandwich)

here, the quanti   ed variable e stands for the eating event and is used to bind the
event predicate with the core information provided via the named roles eater and
eaten. to handle the more complex examples, we simply add additional relations
to capture the provided information, as in the following for 14.46.

   e eating(e)     eater(e,speaker)    eaten(e,turkeysandwich)

(14.47)

    meal(e,lunch)    location(e,desk)

event representations of this sort are referred to as neo-davidsonian event repre-
sentations (davidson, 1967; parsons, 1990) after the philosopher donald davidson
who introduced the notion of an event variable (davidson, 1967). to summarize, in
the neo-davidsonian approach to event representations:

argument.

    events are captured with predicates that take a single event variable as an
    there is no need to specify a    xed number of arguments for a given fol
predicate; rather, as many roles and    llers can be glued on as are provided in
the input.

    no more roles are postulated than are mentioned in the input.
    the logical connections among closely related inputs that share the same pred-
icate are satis   ed without the need for additional id136.
this approach still leaves us with the problem of determining the set of predi-
cates needed to represent roles associated with speci   c events like eater and eaten,
as well as more general concepts like location and time. we   ll return to this prob-
lem in more detail in chapter 18.

14.4.1 representing time
in our discussion of events, we did not seriously address the issue of capturing the
time when the represented events are supposed to have occurred. the representation

temporal logic

tense logic

14.4

    event and state representations

313

of such information in a useful form is the domain of temporal logic. this dis-
cussion introduces the most basic concerns of temporal logic and brie   y discusses
the means by which human languages convey temporal information, which, among
other things, includes tense logic, the ways that verb tenses convey temporal infor-
mation. a more detailed discussion of robust approaches to the representation and
analysis of temporal expressions is presented in chapter 17.

the most straightforward theory of time holds that it    ows inexorably forward
and that events are associated with either points or intervals in time, as on a timeline.
given these notions, we can order distinct events by situating them on the timeline.
more speci   cally, we can say that one event precedes another if the    ow of time
leads from the    rst event to the second. accompanying these notions in most theo-
ries is the idea of the current moment in time. combining this notion with the idea
of a temporal ordering relationship yields the familiar notions of past, present, and
future.

not surprisingly, a large number of schemes can represent this kind of temporal
information. the one presented here is a fairly simple one that stays within the fol
framework of rei   ed events that we have been pursuing. consider the following
examples:
(14.48) i arrived in new york.
(14.49) i am arriving in new york.
(14.50) i will arrive in new york.
these sentences all refer to the same kind of event and differ solely in the tense of
the verb. in our current scheme for representing events, all three would share the
following kind of representation, which lacks any temporal information:
   earriving(e)    arriver(e,speaker)    destination(e,newyork)

(14.51)

the temporal information provided by the tense of the verbs can be exploited
by predicating additional information about the event variable e. speci   cally, we
can add temporal variables representing the interval corresponding to the event, the
end point of the event, and temporal predicates relating this end point to the current
time as indicated by the tense of the verb. such an approach yields the following
representations for our arriving examples:

   e,i,n arriving(e)     arriver(e,speaker)    destination(e,newyork)

    intervalof (e,i)    endpoint(i,n)    precedes(n,now)

   e,i,n arriving(e)     arriver(e,speaker)    destination(e,newyork)

    intervalof (e,i)    memberof (i,now)

   e,i,n arriving(e)     arriver(e,speaker)    destination(e,newyork)

    intervalof (e,i)    endpoint(i,n)    precedes(now,n)

this representation introduces a variable to stand for the interval of time as-
sociated with the event and a variable that stands for the end of that interval. the
two-place predicate precedes represents the notion that the    rst time-point argument
precedes the second in time; the constant now refers to the current time. for past
events, the end point of the interval must precede the current time. similarly, for fu-
ture events the current time must precede the end of the event. for events happening
in the present, the current time is contained within the event interval.

unfortunately, the relation between simple verb tenses and points in time is by

no means straightforward. consider the following examples:

314 chapter 14

    the representation of sentence meaning

reference point

(14.52) ok, we    y from san francisco to boston at 10.
(14.53) flight 1390 will be at the gate an hour now.
in the    rst example, the present tense of the verb    y is used to refer to a future event,
while in the second the future tense is used to refer to a past event.

more complications occur when we consider some of the other verb tenses. con-

sider the following examples:
(14.54) flight 1902 arrived late.
(14.55) flight 1902 had arrived late.
although both refer to events in the past, representing them in the same way seems
wrong. the second example seems to have another unnamed event lurking in the
background (e.g., flight 1902 had already arrived late when something else hap-
pened). to account for this phenomena, reichenbach (1947) introduced the notion
of a reference point. in our simple temporal scheme, the current moment in time
is equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). in reichenbach   s approach, the notion of
the reference point is separated from the utterance time and the event time. the
following examples illustrate the basics of this approach:
(14.56) when mary   s    ight departed, i ate lunch.
(14.57) when mary   s    ight departed, i had eaten lunch.

in both of these examples, the eating event has happened in the past, that is, prior
to the utterance. however, the verb tense in the    rst example indicates that the eating
event began when the    ight departed, while the second example indicates that the
eating was accomplished prior to the    ight   s departure. therefore, in reichenbach   s
terms the departure event speci   es the reference point. these facts can be accom-
modated by additional constraints relating the eating and departure events. in the
   rst example, the reference point precedes the eating event, and in the second exam-
ple, the eating precedes the reference point. figure 14.5 illustrates reichenbach   s
approach with the primary english tenses. exercise 14.6 asks you to represent these
examples in fol.

figure 14.5 reichenbach   s approach applied to various english tenses. in these diagrams,
time    ows from left to right, an e denotes the time of the event, an r denotes the reference
time, and an u denotes the time of the utterance.

past perfectsimple pastpresent perfectsimple futurefuture perfectpresenteeeerrur,eur,uu,r,eu,ru14.4

    event and state representations

315

this discussion has focused narrowly on the broad notions of past, present, and
future and how they are signaled by various english verb tenses. of course, lan-
guages also have many other more direct and more speci   c ways to convey temporal
information, including the use of a wide variety of temporal expressions, as in the
following atis examples:
(14.58) i   d like to go at 6:45, in the morning.
(14.59) somewhere around noon, please.
as we show in chapter 17, grammars for such temporal expressions are of consid-
erable practical importance to information extraction and question-answering appli-
cations.

finally, we should note that a systematic conceptual organization is re   ected in
examples like these. in particular, temporal expressions in english are frequently ex-
pressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and
near in these examples (lakoff and johnson, 1980; jackendoff, 1983). metaphori-
cal organizations such as these, in which one domain is systematically expressed in
terms of another, are very common in languages of the world.

14.4.2 aspect
in the last section, we discussed ways to represent the time of an event with respect
to the time of an utterance describing it. in this section, we address the notion of
aspect, which concerns a cluster of related topics, including whether an event has
ended or is ongoing, whether it is conceptualized as happening at a point in time or
over some interval, and whether any particular state in the world comes about be-
cause of it. based on these and related notions, event expressions have traditionally
been divided into four general classes illustrated in the following examples:
stative: i know my departure gate.
activity: john is    ying.
accomplishment: sally booked her    ight.
achievement: she found her gate.

although the earliest versions of this classi   cation were discussed by aristotle,

the one presented here is due to vendler (1967).

stative expressions represent the notion of an event participant having a partic-
ular property, or being in a state, at a given point in time. as such, these expressions
can be thought of as capturing an aspect of a world at a single point in time. consider
the following atis examples.
(14.60) i like flight 840 arriving at 10:06.
(14.61) i need the cheapest fare.
(14.62) i want to go    rst class.
in examples like these, the event participant denoted by the subject can be seen as
experiencing something at a speci   c point in time. whether or not the experiencer
was in the same state earlier or will be in the future is left unspeci   ed.

activity expressions describe events undertaken by a participant and have no
particular end point. unlike statives, activities are seen as occurring over some span
of time and are therefore not associated with single points in time. consider the
following examples:
(14.63) she drove a mazda.

aspect

stative
expressions

activity
expressions

316 chapter 14

    the representation of sentence meaning

(14.64) i live in brooklyn.
these examples both specify that the subject is engaged in, or has engaged in, the
activity speci   ed by the verb for some period of time.

the    nal aspectual class, achievement expressions, is similar to accomplish-

ments in that these expressions result in a state. consider the following:
(14.65) she found her gate.
(14.66) i reached new york.
unlike accomplishments, achievement events are thought of as happening in an in-
stant and are not equated with any particular activity leading up to the state. to be
more speci   c, the events in these examples may have been preceded by extended
searching or traveling events, but the events corresponding directly to found and
reach are conceived of as points, not intervals.

note that since both accomplishments and achievements are events that result
in a state, they are sometimes characterized as subtypes of a single aspectual class.
members of this combined class are known as telic eventualities.

achievement
expressions

telic
eventualities

14.5 description logics

as noted at the beginning of this chapter, a fair number of representational schemes
have been invented to capture the meaning of linguistic utterances. it is now widely
accepted that meanings represented in these various approaches can, in principle, be
translated into equivalent statements in fol with relative ease. the dif   culty is that
in many of these approaches the semantics of a statement are de   ned procedurally.
that is, the meaning arises from whatever the system that interprets it does with it.
description logics are an effort to better specify the semantics of these earlier
structured network representations and to provide a conceptual framework that is
especially well suited to certain kinds of domain modeling. formally, the term de-
scription logics refers to a family of logical approaches that correspond to varying
subsets of fol. the restrictions placed on the expressiveness of description logics
serve to guarantee the tractability of various critical kinds of id136. our focus
here, however, will be on the modeling aspects of dls rather than on computational
complexity issues.

when using description logics to model an application domain, the emphasis
is on the representation of knowledge about categories, individuals that belong to
those categories, and the relationships that can hold among these individuals. the
set of categories, or concepts, that make up a particular application domain is called
its terminology. the portion of a knowledge base that contains the terminology is
traditionally called the tbox; this is in contrast to the abox that contains facts about
individuals. the terminology is typically arranged into a hierarchical organization
called an ontology that captures the subset/superset relations among the categories.
returning to our earlier culinary domain, we represented domain concepts like
using unary predicates such as restaurant(x); the dl equivalent simply omits the
variable, so the restaurant category is simply written as restaurant.3 to capture
the fact that a particular domain element, such as frasca, is a restaurant, we assert
restaurant(frasca) in much the same way we would in fol. the semantics of

3 dl statements are conventionally typeset with a sans serif font. we   ll follow that convention here,
reverting to our standard mathematical notation when giving fol equivalents of dl statements.

terminology
tbox
abox
ontology

subsumption

14.5

    description logics

317

these categories are speci   ed in precisely the same way that was introduced earlier in
section 14.2: a category like restaurant simply denotes the set of domain elements
that are restaurants.

once we   ve speci   ed the categories of interest in a particular domain, the next
step is to arrange them into a hierarchical structure. there are two ways to cap-
ture the hierarchical relationships present in a terminology: we can directly assert
relations between categories that are related hierarchically, or we can provide com-
plete de   nitions for our concepts and then rely on id136 to provide hierarchical
relationships. the choice between these methods hinges on the use to which the re-
sulting categories will be put and the feasibility of formulating precise de   nitions for
many naturally occurring categories. we   ll discuss the    rst option here and return to
the notion of de   nitions later in this section.

to directly specify a hierarchical structure, we can assert subsumption relations
between the appropriate concepts in a terminology. the subsumption relation is
conventionally written as c (cid:118) d and is read as c is subsumed by d; that is, all
members of the category c are also members of the category d. not surprisingly, the
formal semantics of this relation are provided by a simple set relation; any domain
element that is in the set denoted by c is also in the set denoted by d.

adding the following statements to the tbox asserts that all restaurants are com-
mercial establishments and, moreover, that there are various subtypes of restaurants.

restaurant (cid:118) commercialestablishment

italianrestaurant (cid:118) restaurant
chineserestaurant (cid:118) restaurant
mexicanrestaurant (cid:118) restaurant

(14.67)
(14.68)
(14.69)
(14.70)

ontologies such as this are conventionally illustrated with diagrams such as the one
shown in fig. 14.6, where subsumption relations are denoted by links between the
nodes representing the categories.

figure 14.6 a graphical network representation of a set of subsumption relations in the
restaurant domain.

note, that it was precisely the vague nature of semantic network diagrams like
this that motivated the development of description logics. for example, from this

restaurantchineserestaurant mexicanrestaurantitalianrestaurantcommercialestablishment318 chapter 14

    the representation of sentence meaning

diagram we can   t tell whether the given set of categories is exhaustive or disjoint.
that is, we can   t tell if these are all the kinds of restaurants that we   ll be dealing with
in our domain or whether there might be others. we also can   t tell if an individual
restaurant must fall into only one of these categories, or if it is possible, for example,
for a restaurant to be both italian and chinese. the dl statements given above are
more transparent in their meaning; they simply assert a set of subsumption relations
between categories and make no claims about coverage or mutual exclusion.

if an application requires coverage and disjointness information, then such in-
formation must be made explicitly. the simplest ways to capture this kind of in-
formation is through the use of negation and disjunction operators. for example,
the following assertion would tell us that chinese restaurants can   t also be italian
restaurants.

chineserestaurant (cid:118) not italianrestaurant

(14.71)

specifying that a set of subconcepts covers a category can be achieved with disjunc-
tion, as in the following:

restaurant (cid:118)

(or italianrestaurant chineserestaurant mexicanrestaurant)

(14.72)

having a hierarchy such as the one given in fig. 14.6 tells us next to nothing
about the concepts in it. we certainly don   t know anything about what makes a
restaurant a restaurant, much less italian, chinese, or expensive. what is needed are
additional assertions about what it means to be a member of any of these categories.
in description logics such statements come in the form of relations between the
concepts being described and other concepts in the domain.
in keeping with its
origins in structured network representations, relations in description logics are
typically binary and are often referred to as roles, or role-relations.

to see how such relations work, let   s consider some of the facts about restaurants
discussed earlier in the chapter. we   ll use the hascuisine relation to capture infor-
mation as to what kinds of food restaurants serve and the haspricerange relation
to capture how pricey particular restaurants tend to be. we can use these relations
to say something more concrete about our various classes of restaurants. let   s start
with our italianrestaurant concept. as a    rst approximation, we might say some-
thing uncontroversial like italian restaurants serve italian cuisine. to capture these
notions, let   s    rst add some new concepts to our terminology to represent various
kinds of cuisine.

mexicancuisine (cid:118) cuisine
italiancuisine (cid:118) cuisine
chinesecuisine (cid:118) cuisine
vegetariancuisine (cid:118) cuisine
next, let   s revise our earlier version of italianrestaurant to capture cuisine

expensiverestaurant (cid:118) restaurant
moderaterestaurant (cid:118) restaurant
cheaprestaurant (cid:118) restaurant

information.

italianrestaurant (cid:118) restaurant(cid:117)   hascuisine.italiancuisine

(14.73)

the correct way to read this expression is that individuals in the category italian-
restaurant are subsumed both by the category restaurant and by an unnamed

14.5

    description logics

319

class de   ned by the existential clause   the set of entities that serve italian cuisine.
an equivalent statement in fol would be

   xitalianrestaurant(x)     restaurant(x)

   (   yserves(x,y)    italiancuisine(y))

(14.74)

this fol translation should make it clear what the dl assertions given above do
and do not entail. in particular, they don   t say that domain entities classi   ed as ital-
ian restaurants can   t engage in other relations like being expensive or even serving
chinese cuisine. and critically, they don   t say much about domain entities that we
know do serve italian cuisine. in fact, inspection of the fol translation makes it
clear that we cannot infer that any new entities belong to this category based on their
characteristics. the best we can do is infer new facts about restaurants that we   re
explicitly told are members of this category.

of course, inferring the category membership of individuals given certain char-
acteristics is a common and critical reasoning task that we need to support. this
brings us back to the alternative approach to creating hierarchical structures in a
terminology: actually providing a de   nition of the categories we   re creating in the
form of necessary and suf   cient conditions for category membership. in this case,
we might explicitly provide a de   nition for italianrestaurant as being those restau-
rants that serve italian cuisine, and moderaterestaurant as being those whose
price range is moderate.

italianrestaurant     restaurant(cid:117)   hascuisine.italiancuisine

(14.75)
moderaterestaurant     restaurant(cid:117) haspricerange.moderateprices (14.76)
while our earlier statements provided necessary conditions for membership in these
categories, these statements provide both necessary and suf   cient conditions.

finally, let   s now consider the super   cially similar case of vegetarian restaurants.
clearly, vegetarian restaurants are those that serve vegetarian cuisine. but they don   t
merely serve vegetarian fare, that   s all they serve. we can accommodate this kind of
constraint by adding an additional restriction in the form of a universal quanti   er to
our earlier description of vegetarianrestaurants, as follows:

vegetarianrestaurant     restaurant

(cid:117)   hascuisine.vegetariancuisine
(cid:117)   hascuisine.vegetariancuisine

(14.77)

id136
paralleling the focus of description logics on categories, relations, and individuals
is a processing focus on a restricted subset of logical id136. rather than employ-
ing the full range of reasoning permitted by fol, dl reasoning systems emphasize
the closely coupled problems of subsumption and instance checking.

subsumption, as a form of id136, is the task of determining, based on the
facts asserted in a terminology, whether a superset/subset relationship exists between
two concepts. correspondingly, instance checking asks if an individual can be a
member of a particular category given the facts we know about both the individual
and the terminology. the id136 mechanisms underlying subsumption and in-
stance checking go beyond simply checking for explicitly stated subsumption rela-
tions in a terminology. they must explicitly reason using the relational information

subsumption

instance
checking

320 chapter 14

    the representation of sentence meaning

figure 14.7 a graphical network representation of the complete set of subsumption rela-
tions in the restaurant domain given the current set of assertions in the tbox.

asserted about the terminology to infer appropriate subsumption and membership
relations.

returning to our restaurant domain, let   s add a new kind of restaurant using the

following statement:

ilfornaio (cid:118) moderaterestaurant(cid:117)   hascuisine.italiancuisine

(14.78)

given this assertion, we might ask whether the ilfornaio chain of restaurants might
be classi   ed as an italian restaurant or a vegetarian restaurant. more precisely, we
can pose the following questions to our reasoning system:

ilfornaio (cid:118) italianrestaurant
ilfornaio (cid:118) vegetarianrestaurant

(14.79)
(14.80)

the answer to the    rst question is positive since ilfornaio meets the criteria we
speci   ed for the category italianrestaurant: it   s a restaurant since we explicitly
classi   ed it as a moderaterestaurant, which is a subtype of restaurant, and it
meets the has.cuisine class restriction since we   ve asserted that directly.

the answer to the second question is negative. recall, that our criteria for veg-
etarian restaurants contains two requirements: it has to serve vegetarian fare, and
that   s all it can serve. our current de   nition for ilfornaio fails on both counts since
we have not asserted any relations that state that ilfornaio serves vegetarian fare,
and the relation we have asserted, hascuisine.italiancuisine, contradicts the sec-
ond criteria.

a related reasoning task, based on the basic subsumption id136, is to derive
the implied hierarchy for a terminology given facts about the categories in the ter-
minology. this task roughly corresponds to a repeated application of the subsump-
tion operator to pairs of concepts in the terminology. given our current collection of
statements, the expanded hierarchy shown in fig. 14.7 can be inferred. you should
convince yourself that this diagram contains all and only the subsumption links that
should be present given our current knowledge.

instance checking is the task of determining whether a particular individual can
be classi   ed as a member of a particular category. this process takes what is known

implied
hierarchy

restaurantchineserestaurant mexicanrestaurantitalianrestaurantexpensiverestaurantcheaprestaurantmoderaterestaurantilfornaiovegetarianrestaurant14.6

    summary

321

about a given individual, in the form of relations and explicit categorical statements,
and then compares that information with what is known about the current terminol-
ogy. it then returns a list of the most speci   c categories to which the individual can
belong.

as an example of a categorization problem, consider an establishment that we   re

told is a restaurant and serves italian cuisine.

restaurant(gondolier)
hascuisine(gondolier, italiancuisine)

here, we   re being told that the entity denoted by the term gondolier is a restau-
rant and serves italian food. given this new information and the contents of our
current tbox, we might reasonably like to ask if this is an italian restaurant, if it is
a vegetarian restaurant, or if it has moderate prices.

assuming the de   nitional statements given earlier, we can indeed categorize
the gondolier as an italian restaurant. that is, the information we   ve been given
about it meets the necessary and suf   cient conditions required for membership in
this category. and as with the ilfornaio category, this individual fails to match the
stated criteria for the vegetarianrestaurant. finally, the gondolier might also
turn out to be a moderately priced restaurant, but we can   t tell at this point since
we don   t know anything about its prices. what this means is that given our current
knowledge the answer to the query moderaterestaurant(gondolier) would be false
since it lacks the required haspricerange relation.

the implementation of subsumption, instance checking, as well as other kinds of
id136s needed for practical applications, varies according to the expressivity of
the description logic being used. however, for a description logic of even modest
power, the primary implementation techniques are based on satis   ability methods
that in turn rely on the underlying model-based semantics introduced earlier in this
chapter.

owl and the semantic web
the highest-pro   le role for description logics, to date, has been as a part of the
development of the semantic web. the semantic web is an ongoing effort to pro-
vide a way to formally specify the semantics of the contents of the web (fensel
et al., 2003). a key component of this effort involves the creation and deployment
of ontologies for various application areas of interest. the meaning representation
language used to represent this knowledge is the web ontology language (owl)
(mcguiness and van harmelen, 2004). owl embodies a description logic that
corresponds roughly to the one we   ve been describing here.

web ontology
language

14.6 summary

this chapter has introduced the representational approach to meaning. the follow-
ing are some of the highlights of this chapter:

    a major approach to meaning in computational linguistics involves the cre-
ation of formal meaning representations that capture the meaning-related
content of linguistic inputs. these representations are intended to bridge the
gap from language to common-sense knowledge of the world.

322 chapter 14

    the representation of sentence meaning

    the frameworks that specify the syntax and semantics of these representa-
tions are called meaning representation languages. a wide variety of such
languages are used in natural language processing and arti   cial intelligence.
    such representations need to be able to support the practical computational
requirements of semantic processing. among these are the need to determine
the truth of propositions, to support unambiguous representations, to rep-
resent variables, to support id136, and to be suf   ciently expressive.

    human languages have a wide variety of features that are used to convey

meaning. among the most important of these is the ability to convey a predicate-
argument structure.

    id85 is a well-understood, computationally tractable meaning
representation language that offers much of what is needed in a meaning rep-
resentation language.

can be captured in fol.

    important elements of semantic representation including states and events
    semantic networks and frames can be captured within the fol framework.
    modern description logics consist of useful and computationally tractable
subsets of full id85. the most prominent use of a description
logic is the web ontology language (owl), used in the speci   cation of the
semantic web.

bibliographical and historical notes

the earliest computational use of declarative meaning representations in natural lan-
guage processing was in the context of question-answering systems (green et al.,
1961; raphael, 1968; lindsey, 1963). these systems employed ad hoc representa-
tions for the facts needed to answer questions. questions were then translated into
a form that could be matched against facts in the knowledge base. simmons (1965)
provides an overview of these early efforts.

woods (1967) investigated the use of fol-like representations in question an-
swering as a replacement for the ad hoc representations in use at the time. woods
(1973) further developed and extended these ideas in the landmark lunar system.
interestingly, the representations used in lunar had both truth-conditional and pro-
cedural semantics. winograd (1972) employed a similar representation based on the
micro-planner language in his shrdlu system.

during this same period, researchers interested in the cognitive modeling of lan-
guage and memory had been working with various forms of associative network
representations. masterman (1957) was the    rst to make computational use of a
semantic network-like id99, although semantic networks are
generally credited to quillian (1968). a considerable amount of work in the se-
mantic network framework was carried out during this era (norman and rumelhart,
1975; schank, 1972; wilks, 1975c, 1975b; kintsch, 1974). it was during this pe-
riod that a number of researchers began to incorporate fillmore   s notion of case roles
(fillmore, 1968) into their representations. simmons (1973) was the earliest adopter
of case roles as part of representations for natural language processing.

detailed analyses by woods (1975) and brachman (1979) aimed at    guring out
what semantic networks actually mean led to the development of a number of more

exercises

323

sophisticated network-like languages including krl (bobrow and winograd, 1977)
and kl-one (brachman and schmolze, 1985). as these frameworks became more
sophisticated and well de   ned, it became clear that they were restricted variants of
fol coupled with specialized indexing id136 procedures. a useful collection of
papers covering much of this work can be found in brachman and levesque (1985).
russell and norvig (2002) describe a modern perspective on these representational
efforts.

linguistic efforts to assign semantic structures to natural language sentences in
the generative era began with the work of katz and fodor (1963). the limitations
of their simple feature-based representations and the natural    t of logic to many
of the linguistic problems of the day quickly led to the adoption of a variety of
predicate-argument structures as preferred semantic representations (lakoff, 1972;
mccawley, 1968). the subsequent introduction by montague (1973) of the truth-
conditional model-theoretic framework into linguistic theory led to a much tighter
integration between theories of formal syntax and a wide range of formal semantic
frameworks. good introductions to montague semantics and its role in linguistic
theory can be found in dowty et al. (1981) and partee (1976).

the representation of events as rei   ed objects is due to davidson (1967). the
approach presented here, which explicitly rei   es event participants, is due to parsons
(1990).

most current computational approaches to temporal reasoning are based on allen   s

notion of temporal intervals (allen, 1984); see chapter 17. ter meulen (1995) pro-
vides a modern treatment of tense and aspect. davis (1990) describes the use of fol
to represent knowledge across a wide range of common-sense domains including
quantities, space, time, and beliefs.

a recent comprehensive treatment of logic and language can be found in van
benthem and ter meulen (1997). a classic semantics text is lyons (1977). mccaw-
ley (1993) is an indispensable textbook covering a wide range of topics concerning
logic and language. chierchia and mcconnell-ginet (1991) also broadly covers
semantic issues from a linguistic perspective. heim and kratzer (1998) is a more
recent text written from the perspective of current generative theory.

exercises

14.1 peruse your daily newspaper for three examples of ambiguous sentences or

headlines. describe the various sources of the ambiguities.

14.2 consider a domain in which the word coffee can refer to the following con-
cepts in a knowledge-based system: a caffeinated or decaffeinated beverage,
ground coffee used to make either kind of beverage, and the beans themselves.
give arguments as to which of the following uses of coffee are ambiguous and
which are vague.

1. i   ve had my coffee for today.
2. buy some coffee on your way home.
3. please grind some more coffee.

14.3 the following rule, which we gave as a translation for example 14.26, is not

a reasonable de   nition of what it means to be a vegetarian restaurant.

   xvegetarianrestaurant(x) =    serves(x,vegetarianfood)

324 chapter 14

    the representation of sentence meaning
give a fol rule that better de   nes vegetarian restaurants in terms of what they
serve.

14.4 give fol translations for the following sentences:

1. vegetarians do not eat meat.
2. not all vegetarians eat eggs.

14.5 give a set of facts and id136s necessary to prove the following assertions:

1. mcdonald   s is not a vegetarian restaurant.
2. some vegetarians can eat at mcdonald   s.

don   t just place these facts in your knowledge base. show that they can be
inferred from some more general facts about vegetarians and mcdonald   s.

14.6 for the following sentences, give fol translations that capture the temporal

relationships between the events.

1. when mary   s    ight departed, i ate lunch.
2. when mary   s    ight departed, i had eaten lunch.

14.7 on page 309, we gave the representation near(centro,bacaro) as a transla-
tion for the sentence centro is near bacaro. in a truth-conditional semantics,
this formula is either true or false given some model. critique this truth-
conditional approach with respect to the meaning of words like near.

chapter

15 computational semantics

placeholder

325

chapter

16 id29

placeholder

326

chapter

17 information extraction

i am the very model of a modern major-general,
i   ve information vegetable, animal, and mineral,
i know the kings of england, and i quote the    ghts historical
from marathon to waterloo, in order categorical...
gilbert and sullivan, pirates of penzance

imagine that you are an analyst with an investment    rm that tracks airline stocks.
you   re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. his-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? you will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. fortunately, these can be all found in news articles like this one:
citing high fuel prices, united airlines said friday it has increased fares
by $6 per round trip on    ights to some cities also served by lower-
cost carriers. american airlines, a unit of amr corp., immediately
matched the move, spokesman tim wagner said. united, a unit of ual
corp., said the increase took effect thursday and applies to most routes
where it competes against discount carriers, such as chicago to dallas
and denver to san francisco.

information
extraction

named entity
recognition

relation
extraction

event
extraction

this chapter presents techniques for extracting limited kinds of semantic con-
tent from text. this process of information extraction (ie), turns the unstructured
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.

we begin with the    rst step in most ie tasks,    nding the proper names or named
entities in a text. the task of id39 (ner) is to    nd each
mention of a named entity in the text and label its type. what constitutes a named
entity type is task speci   c; people, places, and organizations are common, but gene
or protein names (cohen and demner-fushman, 2014) or    nancial asset classes
might be relevant for some tasks. once all the named entities in a text have been
extracted, they can be linked together in sets corresponding to real-world entities,
inferring, for example, that mentions of united airlines and united refer to the same
company. this is the joint task of coreference resolution and entity linking which
we defer til chapter 20.

next, we turn to the task of id36:    nding and classifying semantic
relations among the text entities. these are often binary relations like child-of, em-
ployment, part-whole, and geospatial relations. id36 has close links
to populating a relational database.

finally, we discuss three tasks related to events. event extraction is    nding
events in which these entities participate, like, in our sample text, the fare increases

328 chapter 17

   

information extraction

temporal
expression

temporal
id172

template    lling

by united and american and the reporting events said and cite. event coreference
(chapter 20) is needed to    gure out which event mentions in a text refer to the same
event; in our running example the two instances of increase and the phrase the move
all refer to the same event.

to    gure out when the events in a text happened we extract temporal expres-
sions like days of the week (friday and thursday), relative expressions like two
days from now or next year and times such as 3:30 p.m.. these expressions must be
normalized onto speci   c calendar dates or times of day to situate events in time. in
our sample task, this will allow us to link friday to the time of united   s announce-
ment, and thursday to the previous day   s fare increase, and produce a timeline in
which united   s announcement follows the fare increase and american   s announce-
ment follows both of those events.

finally, many texts describe recurring stereotypical events or situations. the task
of template    lling is to    nd such situations in documents and    ll in the template
slots. these slot-   llers may consist of text segments extracted directly from the text,
or concepts like times, amounts, or ontology entities that have been inferred from
text elements through additional processing.

our airline text is an example of this kind of stereotypical situation since airlines
often raise fares and then wait to see if competitors follow along.
in this situa-
tion, we can identify united as a lead airline that initially raised its fares, $6 as the
amount, thursday as the increase date, and american as an airline that followed
along, leading to a    lled template like the following.

fare-raise attempt:                

lead airline:
amount:
effective date:
follower:

united airlines
$6
2006-10-26
american airlines

               

17.1 id39

named entity

temporal
expressions

the    rst step in information extraction is to detect the entities in the text. a named
entity is, roughly speaking, anything that can be referred to with a proper name:
a person, a location, an organization. the term is commonly extended to include
things that aren   t entities per se, including dates, times, and other kinds of temporal
expressions, and even numerical expressions like prices. here   s the sample text
introduced earlier with the named entities marked:

citing high fuel prices, [org united airlines] said [time friday] it
has increased fares by [money $6] per round trip on    ights to some
cities also served by lower-cost carriers. [org american airlines], a
unit of [org amr corp.], immediately matched the move, spokesman
[per tim wagner] said. [org united], a unit of [org ual corp.],
said the increase took effect [time thursday] and applies to most
routes where it competes against discount carriers, such as [loc chicago]
to [loc dallas] and [loc denver] to [loc san francisco].

the text contains 13 mentions of named entities including 5 organizations, 4 loca-
tions, 2 times, 1 person, and 1 mention of money.

in addition to their use in extracting events and the relationship between par-
ticipants, named entities are useful for many other language processing tasks. in

17.1

    id39

329

id31 we might want to know a consumer   s sentiment toward a partic-
ular entity. entities are a useful    rst stage in id53, or for linking text
to information in structured knowledge sources like wikipedia.

figure 17.1 shows typical generic named entity types. many applications will
also need to use speci   c entity types like proteins, genes, commercial products, or
works of art.

tag sample categories
per people, characters

type
people
organization org companies, sports teams
location
loc regions, mountains, seas
gpe countries, states, provinces palo alto is raising the fees for parking.
geo-political
entity
facility
it was a classic ford falcon.
vehicles
figure 17.1 a list of generic named entity types with the kinds of entities they refer to.

fac bridges, buildings, airports consider the golden gate bridge.
veh planes, trains, automobiles

example sentences
turing is a giant of computer science.
the ipcc warned about the cyclone.
the mt. sanitas loop is in sunshine canyon.

id39 means    nding spans of text that constitute proper
names and then classifying the type of the entity. recognition is dif   cult partly be-
cause of the ambiguity of segmentation; we need to decide what   s an entity and what
isn   t, and where the boundaries are. another dif   culty is caused by type ambiguity.
the mention jfk can refer to a person, the airport in new york, or any number of
schools, bridges, and streets around the united states. some examples of this kind
of cross-type confusion are given in figures 17.2 and 17.3.

name
washington
downing st.
ira
louis vuitton

possible categories
person, location, political entity, organization, vehicle
location, organization
person, organization, monetary instrument
person, organization, commercial product

figure 17.2 common categorical ambiguities associated with various proper names.

[per washington] was born into slavery on the farm of james burroughs.
[org washington] went up 2 games to 1 in the four-game series.
blair arrived in [loc washington] for what may well be his last state visit.
in june, [gpe washington] passed a primary seatbelt law.
the [veh washington] had proved to be a leaky ship, every passage i made...

figure 17.3 examples of type ambiguities in the use of the name washington.

17.1.1 ner as sequence labeling
the standard algorithm for id39 is as a word-by-word sequence
labeling task, in which the assigned tags capture both the boundary and the type. a
sequence classi   er like an memm/crf or a bi-lstm is trained to label the tokens
in a text with tags that indicate the presence of particular kinds of named entities.
consider the following simpli   ed excerpt from our running example.

[org american airlines], a unit of [org amr corp.], immediately matched
the move, spokesman [per tim wagner] said.

330 chapter 17

   

information extraction

iob

figure 17.4 shows the same excerpt represented with iob tagging. in iob tag-
ging we introduce a tag for the beginning (b) and inside (i) of each entity type,
and one for tokens outside (o) any entity. the number of tags is thus 2n + 1 tags,
where n is the number of entity types. iob tagging can represent exactly the same
information as the bracketed notation.

words
american
airlines
,
a
unit
of
amr
corp.
,
immediately
matched
the
move
,
spokesman
tim
wagner
said
.

iob label
b-org
i-org
o
o
o
o
b-org
i-org
o
o
o
o
o
o
o
b-per
i-per
o
o

io label
i-org
i-org
o
o
o
o
i-org
i-org
o
o
o
o
o
o
o
i-per
i-per
o
o

figure 17.4 named entity tagging as a sequence model, showing iob and io encodings.

we   ve also shown io tagging, which loses some information by eliminating the
b tag. without the b tag io tagging is unable to distinguish between two entities of
the same type that are right next to each other. since this situation doesn   t arise very
often (usually there is at least some punctuation or other deliminator), io tagging
may be suf   cient, and has the advantage of using only n + 1 tags.

in the following three sections we introduce the three standard families of al-
gorithms for ner tagging: feature based (memm/crf), neural (bi-lstm), and
rule-based.

17.1.2 a feature-based algorithm for ner

identity of wi, identity of neighboring words
embeddings for wi, embeddings for neighboring words
part of speech of wi, part of speech of neighboring words
base-phrase syntactic chunk label of wi and neighboring words
presence of wi in a gazetteer
wi contains a particular pre   x (from all pre   xes of length     4)
wi contains a particular suf   x (from all suf   xes of length     4)
wi is all upper case
word shape of wi, word shape of neighboring words
short word shape of wi, short word shape of neighboring words
presence of hyphen
figure 17.5 typical features for a feature-based ner system.

word shape

gazetteer

17.1

    id39

331

the    rst approach is to extract features and train an memm or crf sequence
model of the type we saw for part-of-speech tagging in chapter 8. figure 17.5 lists
standard features used in such feature-based systems. we   ve seen many of these
features before in the context of part-of-speech tagging, particularly for tagging un-
known words. this is not surprising, as many unknown words are in fact named
entities. word shape features are thus particularly important in the context of ner.
recall that word shape features are used to represent the abstract letter pattern of
the word by mapping lower-case letters to    x   , upper-case to    x   , numbers to    d   , and
retaining punctuation. thus for example i.m.f would map to x.x.x. and dc10-30
would map to xxdd-dd. a second class of shorter word shape features is also used.
in these features consecutive character types are removed, so dc10-30 would be
mapped to xd-d but i.m.f would still map to x.x.x. this feature by itself accounts
for a considerable part of the success of feature-based ner systems for english
news text. shape features are also particularly important in recognizing names of
proteins and genes in biological texts.

for example the named entity token l   occitane would generate the following

non-zero valued feature values:

pre   x(wi) = l
pre   x(wi) = l   
pre   x(wi) = l   o
pre   x(wi) = l   oc
word-shape(wi) = x   xxxxxxxx

suf   x(wi) = tane
suf   x(wi) = ane
suf   x(wi) = ne
suf   x(wi) = e
short-word-shape(wi) = x   xx

a gazetteer is a list of place names, often providing millions of entries for lo-
cations with detailed geographical and political information.1 a related resource
is name-lists; the united states census bureau also provides extensive lists of    rst
names and surnames derived from its decadal census in the u.s.2 similar lists of cor-
porations, commercial products, and all manner of things biological and mineral are
also available from a variety of sources. gazetteer and name features are typically
implemented as a binary feature for each name list. unfortunately, such lists can
be dif   cult to create and maintain, and their usefulness varies considerably. while
gazetteers can be quite effective, lists of persons and organizations are not always
helpful (mikheev et al., 1999).

feature effectiveness depends on the application, genre, media, and language.
for example, shape features, critical for english newswire texts, are of little use
with automatic id103 transcripts, or other non-edited or informally-
edited sources, or for languages like chinese that don   t use orthographic case. the
features in fig. 17.5 should therefore be thought of as only a starting point.

figure 17.6 illustrates the result of adding part-of-speech tags, syntactic base-

phrase chunk tags, and some shape information to our earlier example.

given such a training set, a sequence classi   er like an memm can be trained to
label new sentences. figure 17.7 illustrates the operation of such a sequence labeler
at the point where the token corp. is next to be labeled. if we assume a context win-
dow that includes the two preceding and following words, then the features available
to the classi   er are those shown in the boxed area.

1 www.geonames.org
2 www.census.gov

332 chapter 17

   

information extraction

word
american
airlines
,
a
unit
of
amr
corp.
,
immediately rb
vbd b-vp
matched
b-np
dt
the
i-np
nn
move
o
,
,
b-np
spokesman nn
i-np
nnp
tim
nnp
wagner
i-np
vbd b-vp
said
.
,

short shape
pos chunk
xx
nnp b-np
xx
nnps i-np
,
,
o
x
dt
b-np
x
nn
i-np
x
in
b-pp
x
nnp b-np
xx.
i-np
nnp
o
,
,
b-advp x
x
x
x
,
x
xx
xx
x
.

o

label
b-org
i-org
o
o
o
o
b-org
i-org
o
o
o
o
o
o
o
b-per
i-per
o
o

figure 17.6 word-by-word feature encoding for ner.

figure 17.7 id39 as sequence labeling. the features available to the classi   er during
training and classi   cation are those in the boxed area.

17.1.3 a neural algorithm for ner
the standard neural algorithm for ner is based on the bi-lstm introduced in chap-
ter 9. recall that in that model, word and character embeddings are computed for
input word wi. these are passed through a left-to-right lstm and a right-to-left
lstm, whose outputs are concatenated (or otherwise combined) to produce a sin-
gle output layer at position i. in the simplest method, this layer can then be directly
passed onto a softmax that creates a id203 distribution over all ner tags, and
the most likely tag is chosen as ti.

for named entity tagging this greedy approach to decoding is insuf   cient, since
it doesn   t allow us to impose the strong constraints neighboring tokens have on each
other (e.g., the tag i-per must follow another i-per or b-per). instead a crf layer
is normally used on top of the bi-lstm output, and the viterbi decoding algorithm
is used to decode. fig. 17.8 shows a sketch of the algorithm

classifierinnnpnnprbunitofa...xb-pp...amrcorp.immediatelyb-npxi-npx.b-advpxob-org?......,,o,matched17.1

    id39

333

figure 17.8 putting it all together: character embeddings and words together a bi-lstm
sequence model. after (lample et al., 2016)

17.1.4 rule-based ner
while machine learned (neural or memm/crf) sequence models are the norm in
academic research, commercial approaches to ner are often based on pragmatic
combinations of lists and rules, with some smaller amount of supervised machine
learning (chiticariu et al., 2013). for example ibm system t is a text understand-
ing architecture in which a user speci   es complex declarative constraints for tagging
tasks in a formal query language that includes id157, dictionaries, se-
mantic constraints, nlp operators, and table structures, all of which the system
compiles into an ef   cient extractor (chiticariu et al., 2018)

one common approach is to make repeated rule-based passes over a text, allow-
ing the results of one pass to in   uence the next. the stages typically    rst involve
the use of rules that have extremely high precision but low recall. subsequent stages
employ more error-prone statistical methods that take the output of the    rst pass into
account.

1. first, use high-precision rules to tag unambiguous entity mentions.
2. then, search for substring matches of the previously detected names.
3. consult application-speci   c name lists to identify likely name entity mentions

from the given domain.

4. finally, apply probabilistic sequence labeling techniques that make use of the

tags from previous stages as additional features.

the intuition behind this staged approach is twofold. first, some of the entity
mentions in a text will be more clearly indicative of a given entity   s class than others.
second, once an unambiguous entity mention is introduced into a text, it is likely that
subsequent shortened versions will refer to the same entity (and thus the same type
of entity).

17.1.5 evaluation of id39
the familiar metrics of recall, precision, and f1 measure are used to evaluate ner
systems. remember that recall is the ratio of the number of correctly labeled re-
sponses to the total that should have been labeled; precision is the ratio of the num-

markwatneyvisitsmarslstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2concatenationright-to-left lstid113ft-to-right lstmb-peri-perob-loccrf layerchar lstmchar lstmchar lstmchar lstmgloveglovegloveglovechar+gloveembeddings334 chapter 17

   

information extraction

figure 17.9 the 17 relations used in the ace id36 task.

ber of correctly labeled responses to the total labeled; and f-measure is the harmonic
mean of the two. for named entities, the entity rather than the word is the unit of
response. thus in the example in fig. 17.6, the two entities tim wagner and amr
corp. and the non-entity said would each count as a single response.

the fact that named entity tagging has a segmentation component which is not
present in tasks like text categorization or part-of-speech tagging causes some prob-
lems with evaluation. for example, a system that labeled american but not american
airlines as an organization would cause two errors, a false positive for o and a false
negative for i-org. in addition, using entities as the unit of response but words as
the unit of training means that there is a mismatch between the training and test
conditions.

17.2 id36

next on our list of tasks is to discern the relationships that exist among the detected
entities. let   s return to our sample airline text:

citing high fuel prices, [org united airlines] said [time friday] it
has increased fares by [money $6] per round trip on    ights to some
cities also served by lower-cost carriers. [org american airlines], a
unit of [org amr corp.], immediately matched the move, spokesman
[per tim wagner] said. [org united], a unit of [org ual corp.],
said the increase took effect [time thursday] and applies to most
routes where it competes against discount carriers, such as [loc chicago]
to [loc dallas] and [loc denver] to [loc san francisco].

the text tells us, for example, that tim wagner is a spokesman for american
airlines, that united is a unit of ual corp., and that american is a unit of amr.
these binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. figure 17.9 lists the 17 relations
used in the ace id36 evaluations and fig. 17.10 shows some sample
relations. we might also extract more domain-speci   c relation such as the notion of
an airline route. for example from this text we can conclude that united has routes
to chicago, dallas, denver, and san francisco.

artifactgeneralaffiliationorgaffiliationpart-wholeperson-socialphysicallocatednearbusinessfamilylasting personalcitizen-resident-ethnicity-religionorg-location-originfounderemploymentmembershipownershipstudent-aluminvestoruser-owner-inventor-manufacturergeographicalsubsidiarysports-affiliation17.2

    id36
examples
relations
he was in tennessee
physical-located
xyz, the parent company of abc
part-whole-subsidiary
yoko   s husband john
person-social-family
steve jobs, co-founder of apple...
org-aff-founder
figure 17.10 semantic relations with examples and the named entity types they involve.

types
per-gpe
org-org
per-per
per-org

335

domain
united, ual, american airlines, amr
tim wagner
chicago, dallas, denver, and san francisco

classes
united, ual, american, and amr are organizations
tim wagner is a person
chicago, dallas, denver, and san francisco are places

d = {a,b,c,d,e, f ,g,h,i}
a,b,c,d
e
f ,g,h,i

org = {a,b,c,d}
pers = {e}
loc = { f ,g,h,i}

relations
united is a unit of ual
american is a unit of amr
tim wagner works for american airlines
united serves chicago, dallas, denver, and san francisco
figure 17.11 a model-based view of the relations and entities in our sample text.

partof = {(cid:104)a,b(cid:105),(cid:104)c,d(cid:105)}
orgaff = {(cid:104)c,e(cid:105)}
serves = {(cid:104)a, f(cid:105),(cid:104)a,g(cid:105),(cid:104)a,h(cid:105),(cid:104)a,i(cid:105)}

these relations correspond nicely to the model-theoretic notions we introduced
in chapter 14 to ground the meanings of the logical forms. that is, a relation consists
of a set of ordered tuples over elements of a domain. in most standard information-
extraction applications, the domain elements correspond to the named entities that
occur in the text, to the underlying entities that result from co-reference resolution, or
to entities selected from a domain ontology. figure 17.11 shows a model-based view
of the set of entities and relations that can be extracted from our running example.
notice how this model-theoretic view subsumes the ner task as well; named entity
recognition corresponds to the identi   cation of a class of unary relations.

sets of relations have been de   ned for many other domains as well. for example
umls, the uni   ed medical language system from the us national library of
medicine has a network that de   nes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:

entity
relation
injury
disrupts
bodily location
location-of biologic function
anatomical structure
part-of
pharmacologic substance causes
pharmacologic substance treats

organism
pathological function
pathologic function

entity
physiological function

given a medical sentence like this one:
(17.1) doppler echocardiography can be used to diagnose left anterior descending

artery stenosis in patients with type 2 diabetes

we could thus extract the umls relation:

echocardiography, doppler diagnoses acquired stenosis

infoboxes

wikipedia also offers a large supply of relations, drawn from infoboxes, struc-
tured tables associated with certain wikipedia articles. for example, the wikipedia

336 chapter 17

   

information extraction

rdf
rdf triple

infobox for stanford includes structured facts like state = "california" or
president = "mark tessier-lavigne". these facts can be turned into rela-
tions like president-of or located-in. or into relations in a metalanguage called rdf
(resource description framework). an rdf triple is a tuple of entity-relation-
entity, called a subject-predicate-object expression. here   s a sample rdf triple:

subject
golden gate park location

predicate object

san francisco

for example the crowdsourced dbpedia (bizer et al., 2009) is an ontology de-
rived from wikipedia containing over 2 billion rdf triples. another dataset from
wikipedia infoboxes, freebase (bollacker et al., 2008), has relations like

freebase

people/person/nationality
location/location/contains

is-a
hypernym

id138 or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. for example id138 has the is-a or
hypernym relation between classes,

giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...

id138 also has instance-of relation between individuals and classes, so that for
example san francisco is in the instance-of relation with city. extracting these
relations is an important step in extending or building ontologies.

there are    ve main classes of algorithms for id36: hand-written
patterns, supervised machine learning, semi-supervised (via id64 and
via distant supervision), and unsupervised. we   ll introduce each of these in the
next sections.

17.2.1 using patterns to extract relations
the earliest and still common algorithm for id36 is lexico-syntactic
patterns,    rst developed by hearst (1992a). consider the following sentence:
agar is a substance prepared from a mixture of red algae, such as ge-
lidium, for laboratory or industrial use.

hearst points out that most human readers will not know what gelidium is, but that
they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is.
she suggests that the following lexico-syntactic pattern

np0 such as np1{,np2 . . . , (and|or)npi},i     1

implies the following semantics

allowing us to infer

   npi,i     1,hyponym(npi,np0)

hyponym(gelidium,red algae)

(17.2)

(17.3)

(17.4)

figure 17.12 shows    ve patterns hearst (1992a, 1998) suggested for inferring
the hyponym relation; we   ve shown nph as the parent/hyponym. modern versions
of the pattern-based approach extend it by adding named entity constraints. for
example if our goal is to answer questions about    who holds what of   ce in which
organization?   , we can use patterns like the following:

17.2

    id36

337

np {, np}* {,} (and|or) other nph
temples, treasuries, and other important civic buildings
nph such as {np,}* {(or|and)} np
red algae such as gelidium
such nph as {np,}* {(or|and)} np
such authors as herrick, goldsmith, and shakespeare
nph {,} including {np,}* {(or|and)} np
common-law countries, including canada and england
nph {,} especially {np}* {(or|and)} np
european countries, especially france, england, and spain
figure 17.12 hand-built lexico-syntactic patterns for    nding hypernyms, using {} to mark optionality
(hearst 1992a, hearst 1998).

per, position of org:
george marshall, secretary of state of the united states

per (named|appointed|chose|etc.) per prep? position
truman appointed marshall secretary of state

per [be]? (named|appointed|etc.) prep? org position
george marshall was named us secretary of state

hand-built patterns have the advantage of high-precision and they can be tailored
to speci   c domains. on the other hand, they are often low-recall, and it   s a lot of
work to create them for all possible patterns.

17.2.2 id36 via supervised learning
supervised machine learning approaches to id36 follow a scheme that
should be familiar by now. a    xed set of relations and entities is chosen, a training
corpus is hand-annotated with the relations and entities, and the annotated texts are
then used to train classi   ers to annotate an unseen test set.

the most straightforward approach has three steps, illustrated in fig. 17.13. step
one is to    nd pairs of named entities (usually in the same sentence). in step two, a
   ltering classi   er is trained to make a binary decision as to whether a given pair of
named entities are related (by any relation). positive examples are extracted directly
from all relations in the annotated corpus, and negative examples are generated from
within-sentence entity pairs that are not annotated with a relation. in step 3, a classi-
   er is trained to assign a label to the relations that were found by step 2. the use of
the    ltering classi   er can speed up the    nal classi   cation and also allows the use of
distinct feature-sets appropriate for each task. for each of the two classi   ers, we can
use any of the standard classi   cation techniques (id28, neural network,
id166, etc.)

function findrelations(words) returns relations

relations   nil
entities    findentities(words)
forall entity pairs (cid:104)e1, e2(cid:105) in entities do

if related?(e1, e2)

relations   relations+classifyrelation(e1, e2)

figure 17.13 finding and classifying the relations among entities in a text.

for the feature-based classi   ers like id28 or id79s the
most important step is to identify useful features. let   s consider features for clas-

338 chapter 17

   

information extraction

sifying the relationship between american airlines (mention 1, or m1) and tim
wagner (mention 2, m2) from this sentence:
(17.5) american airlines, a unit of amr, immediately matched the move,

spokesman tim wagner said

useful word features include

airlines wagner

    the headwords of m1 and m2 and their concatenation
airlines-wagner
    bag-of-words and bigrams in m1 and m2
    words or bigrams in particular positions

american, airlines, tim, wagner, american airlines, tim wagner

m2: -1 spokesman
m2: +1 said

a, amr, of, immediately, matched, move, spokesman, the, unit

    bag of words or bigrams between m1 and m2:
    stemmed versions of the same

embeddings can be used to represent words in any of these features. useful named
entity features include

    named-entity types and their concatenation
(m1: org, m2: per, m1m2: org-per)
    entity level of m1 and m2 (from the set name, nominal, pronoun)

m1: name [it or he would be pronoun]
m2: name [the company would be nominal]

    number of entities between the arguments (in this case 1, for amr)
the syntactic structure of a sentence can also signal relationships among its
entities. syntax is often featured by using strings representing syntactic paths: the
(dependency or constituency) path traversed through the tree in getting from one
entity to the other.

np np pp vp np np

    base syntactic chunk sequence from m1 to m2
    constituent paths between m1 and m2
np     np     s     s     np
    dependency-tree paths
airlines    sub j matched    comp said    sub j wagner
figure 17.14 summarizes many of the features we have discussed that could be
used for classifying the relationship between american airlines and tim wagner
from our example text.

neural models for id36 similarly treat the task as supervised clas-
si   cation. one option is to use a similar architecture as we saw for named entity
tagging: a bi-lstm model with id27s as inputs and a single softmax
classi   cation of the sentence output as a 1-of-n relation label. because relations
often hold between entities that are far part in a sentence (or across sentences), it
may be possible to get higher performance from algorithms like convolutional nets
(dos santos et al., 2015) or chain or tree lstms (miwa and bansal 2016, peng
et al. 2017).

in general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised id36 systems can get high ac-
curacies. but labeling a large training set is extremely expensive and supervised

17.2

    id36

339

m1 headword
m2 headword
word(s) before m1
word(s) after m2
bag of words between
m1 type
m2 type
concatenated types
constituent path
base phrase path
typed-dependency path airlines    sub j matched    comp said    sub j wagner
figure 17.14 sample of features extracted during classi   cation of the <american airlines, tim wagner>
tuple; m1 is the    rst mention, m2 the second.

airlines (as a word token or an embedding)
wagner
none
said
{a, unit, of, amr, inc., immediately, matched, the, move, spokesman }
org
pers
org-pers
np     np     s     s     np
np     np     pp     np     v p     np     np

models are brittle: they don   t generalize well to different text genres. for this rea-
son, much research in id36 has focused on the semi-supervised and
unsupervised approaches we turn to next.

17.2.3 semisupervised id36 via id64
supervised machine learning assumes that we have lots of labeled data. unfortu-
nately, this is expensive. but suppose we just have a few high-precision seed pat-
terns, like those in section 17.2.1, or perhaps a few seed tuples. that   s enough
to bootstrap a classi   er! id64 proceeds by taking the entities in the seed
pair, and then    nding sentences (on the web, or whatever dataset we are using) that
contain both entities. from all such sentences, we extract and generalize the context
around the entities to learn new patterns. fig. 17.15 sketches a basic algorithm.

seed patterns
seed tuples
id64

function bootstrap(relation r) returns new relation tuples

tuples   gather a set of seed tuples that have relation r
iterate

sentences      nd sentences that contain entities in tuples
patterns   generalize the context between and around entities in sentences
newpairs   use patterns to grep for more tuples
newpairs   newpairs with high con   dence
tuples   tuples + newpairs

return tuples

figure 17.15 id64 from seed entity pairs to learn relations.

suppose, for example, that we need to create a list of airline/hub pairs, and we
know only that ryanair has a hub at charleroi. we can use this seed fact to discover
new patterns by    nding other mentions of this relation in our corpus. we search
for the terms ryanair, charleroi and hub in some proximity. perhaps we    nd the
following set of sentences:
(17.6) budget airline ryanair, which uses charleroi as a hub, scrapped all

weekend    ights out of the airport.

(17.7) all    ights in and out of ryanair   s belgian hub at charleroi airport were

grounded on friday...

340 chapter 17

   

information extraction

con   dence
values
semantic drift

noisy-or

(17.8) a spokesman at charleroi, a main hub for ryanair, estimated that 8000

passengers had already been affected.

from these results, we can use the context of words between the entity mentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
such as the following:

/ [org], which uses [loc] as a hub /
/ [org]   s hub at [loc] /
/ [loc] a main hub for [org] /

these new patterns can then be used to search for additional tuples.

id64 systems also assign con   dence values to new tuples to avoid se-
mantic drift. in semantic drift, an erroneous pattern leads to the introduction of
erroneous tuples, which, in turn, lead to the creation of problematic patterns and the
meaning of the extracted relations    drifts   . consider the following example:
(17.9) sydney has a ferry hub at circular quay.
if accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple (cid:104)sydney,circularquay(cid:105). patterns based on this tuple could
propagate further errors into the database.
con   dence values for patterns are based on balancing two factors: the pattern   s
performance with respect to the current set of tuples and the pattern   s productivity
in terms of the number of matches it produces in the document collection. more
formally, given a document collection d, a current set of tuples t , and a proposed
pattern p, we need to track two factors:

    hits: the set of tuples in t that p matches while looking in d
    f inds: the total set of tuples that p    nds in d
the following equation balances these considerations (riloff and jones, 1999).

hitsp
   ndsp    log(   ndsp)
this metric is generally normalized to produce a id203.

conf rlogf (p) =

(17.10)

we can assess the con   dence in a proposed new tuple by combining the evidence
supporting it from all the patterns p(cid:48) that match that tuple in d (agichtein and
gravano, 2000). one way to combine such evidence is the noisy-or technique.
assume that a given tuple is supported by a subset of the patterns in p, each with
its own con   dence assessed as above. in the noisy-or model, we make two basic
assumptions. first, that for a proposed tuple to be false, all of its supporting patterns
must have been in error, and second, that the sources of their individual failures are
all independent. if we loosely treat our con   dence measures as probabilities, then
the id203 of any individual pattern p failing is 1    conf (p); the id203 of
all of the supporting patterns for a tuple being wrong is the product of their individual
failure probabilities, leaving us with the following equation for our con   dence in a
new tuple.

conf (t) = 1   (cid:89)p   p(cid:48)

(1    conf (p))

(17.11)

setting conservative con   dence thresholds for the acceptance of new patterns
and tuples during the id64 process helps prevent the system from drifting
away from the targeted relation.

distant
supervision

17.2

    id36

341

17.2.4 distant supervision for id36
although text that has been hand-labeled with relation labels is extremely expensive
to produce, there are ways to    nd indirect sources of training data. the distant
supervision method of mintz et al. (2009) combines the advantages of id64
with supervised learning. instead of just a handful of seeds, distant supervision uses
a large database to acquire a huge number of seed examples, creates lots of noisy
pattern features from all these examples and then combines them in a supervised
classi   er.

for example suppose we are trying to learn the place-of-birth relationship be-
tween people and their birth cities. in the seed-based approach, we might have only
5 examples to start with. but wikipedia-based databases like dbpedia or freebase
have tens of thousands of examples of many relations; including over 100,000 ex-
amples of place-of-birth, (<edwin hubble, marshfield>, <albert einstein,
ulm>, etc.,). the next step is to run named entity taggers on large amounts of text   
mintz et al. (2009) used 800,000 articles from wikipedia   and extract all sentences
that have two named entities that match the tuple, like the following:

...hubble was born in marsh   eld...
...einstein, born (1879), ulm...
...hubble   s birthplace in marsh   eld...
training instances can now be extracted from this data, one training instance
for each identical tuple <relation, entity1, entity2>. thus there will be one
training instance for each of:

<born-in, edwin hubble, marshfield>
<born-in, albert einstein, ulm>
<born-year, albert einstein, 1879>
and so on.

we can then apply feature-based or neural classi   cation. for feature-based clas-
si   cation, standard supervised id36 features like the named entity la-
bels of the two mentions, the words and dependency paths in between the mentions,
and neighboring words. each tuple will have features collected from many training
instances; the feature vector for a single training instance like (<born-in,albert
einstein, ulm> will have lexical and syntactic features from many different sen-
tences that mention einstein and ulm.

because distant supervision has very large training sets, it is also able to use very
rich features that are conjunctions of these individual features. so we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependency paths like these:

per was born in loc
per, born (xxxx), loc
per   s birthplace in loc
to return to our running example, for this sentence:

(17.12) american airlines, a unit of amr, immediately matched the move,

spokesman tim wagner said

we would learn rich conjunction features like this one:

m1 = org & m2 = per & nextword=   said   & path= np     np     s     s     np
the result is a supervised classi   er that has a huge rich set of features to use
in detecting relations. since not every test sentence will have one of the training

342 chapter 17

   

information extraction

relations, the classi   er will also need to be able to label an example as no-relation.
this label is trained by randomly selecting entity pairs that do not appear in any
freebase relation, extracting features for them, and building a feature vector for
each such tuple. the    nal algorithm is sketched in fig. 17.16.

function distant supervision(database d, text t) returns relation classi   er c

foreach relation r

foreach tuple (e1,e2) of entities with relation r in d
sentences   sentences in t that contain e1 and e2
f   frequent features in sentences
observations   observations + new training tuple (e1, e2, f, r)

c   train supervised classi   er on observations
return c

figure 17.16 the distant supervision algorithm for id36. a neural classi   er
might not need to use the feature set f .

distant supervision shares advantages with each of the methods we   ve exam-
ined. like supervised classi   cation, distant supervision uses a classi   er with lots
of features, and supervised by detailed hand-created knowledge. like pattern-based
classi   ers, it can make use of high-precision evidence for the relation between en-
tities. indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. for example the is-a or hypernym extraction
system of snow et al. (2005) used hypernym/hyponym np pairs from id138 as
distant supervision, and then learned new patterns from large amounts of text. their
system induced exactly the original 5 template patterns of hearst (1992a), but also
70,000 additional patterns including these four:

nph like np many hormones like leptin...
nph called np ...using a markup language called xhtml
np is a nph
np, a nph

ruby is a programming language...
ibm, a company with a long...

this ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there   s no semantic
drift. like unsupervised classi   cation, it doesn   t use a labeled training corpus of
texts, so it isn   t sensitive to genre issues in the training corpus, and relies on very
large amounts of unlabeled data. distant supervision also has the advantage that it
can create training tuples to be used with neural classi   ers, where features are not
required.

but distant supervision can only help in extracting relations for which a large
enough database already exists. to extract new relations without datasets, or rela-
tions for new domains, purely unsupervised methods must be used.

open
information
extraction

17.2.5 unsupervised id36
the goal of unsupervised id36 is to extract relations from the web
when we have no labeled training data, and not even any list of relations. this task
is often called id10 or open ie. in open ie, the relations
are simply strings of words (usually beginning with a verb).

for example, the reverb system (fader et al., 2011) extracts a relation from a

sentence s in 4 steps:

17.2

    id36

343

1. run a part-of-speech tagger and entity chunker over s
2. for each verb in s,    nd the longest sequence of words w that start with a verb

and satisfy syntactic and lexical constraints, merging adjacent matches.

3. for each phrase w,    nd the nearest noun phrase x to the left which is not a
relative pronoun, wh-word or existential    there   . find the nearest noun phrase
y to the right.

4. assign con   dence c to the relation r = (x,w,y) using a con   dence classi   er

and return it.

a relation is only accepted if it meets syntactic and lexical constraints. the
syntactic constraints ensure that it is a verb-initial sequence that might also include
nouns (relations that begin with light verbs like make, have, or do often express the
core of the relation with a noun, like have a hub in):

v | vp | vw*p
v = verb particle? adv?
w = (noun | adj | adv | pron | det )
p = (prep | particle | inf. marker)

the lexical constraints are based on a dictionary d that is used to prune very rare,
long relation strings. the intuition is to eliminate candidate relations that don   t oc-
cur with suf   cient number of distinct argument types and so are likely to be bad
examples. the system    rst runs the above id36 algorithm of   ine on
500 million web sentences and extracts a list of all the relations that occur after nor-
malizing them (removing in   ection, auxiliary verbs, adjectives, and adverbs). each
relation r is added to the dictionary if it occurs with at least 20 different arguments.
fader et al. (2011) used a dictionary of 1.7 million normalized relations.

finally, a con   dence value is computed for each relation using a logistic re-
gression classi   er. the classi   er is trained by taking 1000 random web sentences,
running the extractor, and hand labelling each extracted relation as correct or incor-
rect. a con   dence classi   er is then trained on this hand-labeled data, using features
of the relation and the surrounding words. fig. 17.17 shows some sample features
used in the classi   cation.

(x,r,y) covers all words in s
the last preposition in r is for
the last preposition in r is on
len(s)     10
there is a coordinating conjunction to the left of r in s
r matches a lone v in the syntactic constraints
there is preposition to the left of x in s
there is an np to the right of y in s
figure 17.17 features for the classi   er that assigns con   dence to relations extracted by the
id10 system reverb (fader et al., 2011).

for example the following sentence:

(17.13) united has a hub in chicago, which is the headquarters of united

continental holdings.

has the relation phrases has a hub in and is the headquarters of (it also has has and
is, but longer phrases are preferred). step 3    nds united to the left and chicago to
the right of has a hub in, and skips over which to    nd chicago to the left of is the
headquarters of. the    nal output is:

344 chapter 17

   

information extraction

r1:
r2:

<united, has a hub in, chicago>
<chicago, is the headquarters of, united continental holdings>

the great advantage of unsupervised id36 is its ability to handle
a huge number of relations without having to specify them in advance. the disad-
vantage is the need to map these large sets of strings into some canonical form for
adding to databases or other knowledge sources. current methods focus heavily on
relations expressed with verbs, and so will miss many relations that are expressed
nominally.

17.2.6 evaluation of id36
supervised id36 systems are evaluated by using test sets with human-
annotated, gold-standard relations and computing precision, recall, and f-measure.
labeled precision and recall require the system to classify the relation correctly,
whereas unlabeled methods simply measure a system   s ability to detect entities that
are related.

semi-supervised and unsupervised methods are much more dif   cult to evalu-
ate, since they extract totally new relations from the web or a large text. because
these methods use very large amounts of text, it is generally not possible to run them
solely on a small labeled test set, and as a result it   s not possible to pre-annotate a
gold set of correct instances of relations.

for these methods it   s possible to approximate (only) precision by drawing a
random sample of relations from the output, and having a human check the accuracy
of each of these relations. usually this approach focuses on the tuples to be extracted
from a body of text rather than on the relation mentions; systems need not detect
every mention of a relation to be scored correctly. instead, the evaluation is based
on the set of tuples occupying the database when the system is    nished. that is,
we want to know if the system can discover that ryanair has a hub at charleroi; we
don   t really care how many times it discovers it. the estimated precision   p is then

  p =

# of correctly extracted relation tuples in the sample

total # of extracted relation tuples in the sample.

(17.14)

another approach that gives us a little bit of information about recall is to com-
pute precision at different levels of recall. assuming that our system is able to
rank the relations it produces (by id203, or con   dence) we can separately com-
pute precision for the top 1000 new relations, the top 10,000 new relations, the top
100,000, and so on. in each case we take a random sample of that set. this will
show us how the precision curve behaves as we extract more and more tuples. but
there is no way to directly evaluate recall.

17.3 extracting times

times and dates are a particularly important kind of named entity that play a role
in id53, in calendar and personal assistant applications. in order to
reason about times and dates, after we extract these temporal expressions they must
be normalized   converted to a standard format so we can reason about them. in this
section we consider both the extraction and id172 of temporal expressions.

17.3

    extracting times

345

absolute
relative

duration

17.3.1 temporal expression extraction
temporal expressions are those that refer to absolute points in time, relative times,
durations, and sets of these. absolute temporal expressions are those that can be
mapped directly to calendar dates, times of day, or both. relative temporal expres-
sions map to particular times through some other reference point (as in a week from
last tuesday). finally, durations denote spans of time at varying levels of granular-
ity (seconds, minutes, days, weeks, centuries, etc.). figure 17.18 lists some sample
temporal expressions in each of these categories.

absolute
april 24, 1916
the summer of    77
10:15 am
the 3rd quarter of 2006
figure 17.18 examples of absolute, relational and durational temporal expressions.

relative
yesterday
next semester
two weeks from yesterday
last quarter

durations
four hours
three weeks
six days
the last three quarters

lexical triggers

temporal expressions are grammatical constructions that have temporal lexical
triggers as their heads. lexical triggers might be nouns, proper nouns, adjectives,
and adverbs; full temporal expressions consist of their phrasal projections: noun
phrases, adjective phrases, and adverbial phrases. figure 17.19 provides examples.

examples
morning, noon, night, winter, dusk, dawn

category
noun
proper noun january, monday, ides, easter, rosh hashana, ramadan, tet
adjective
adverb
figure 17.19 examples of temporal lexical triggers.

recent, past, annual, former
hourly, daily, monthly, yearly

let   s look at the timeml annotation scheme, in which temporal expressions are
annotated with an xml tag, timex3, and various attributes to that tag (pustejovsky
et al. 2005, ferro et al. 2005). the following example illustrates the basic use of this
scheme (we defer discussion of the attributes until section 17.3.2).

a fare increase initiated <timex3>last week</timex3> by ual
corp   s united airlines was matched by competitors over <timex3>the
weekend</timex3>, marking the second successful fare increase in
<timex3>two weeks</timex3>.

the temporal expression recognition task consists of    nding the start and end of
all of the text spans that correspond to such temporal expressions. rule-based ap-
proaches to temporal expression recognition use cascades of automata to recognize
patterns at increasing levels of complexity. tokens are    rst part-of-speech tagged,
and then larger and larger chunks are recognized from the results from previous
stages, based on patterns containing trigger words (e.g., february) or classes (e.g.,
month). figure 17.20 gives a fragment from a rule-based system.

sequence-labeling approaches follow the same iob scheme used for named-
entity tags, marking words that are either inside, outside or at the beginning of a
timex3-delimited temporal expression with the i, o, and b tags as follows:

a
o

fare
o

increase
o

initiated
o

last
b

week
i

by
o

ual
o

corp   s...
o

346 chapter 17

   

information extraction

# yesterday/today/tomorrow
$string =   s/((($ot+the$ct+\s+)?$ot+day$ct+\s+$ot+(before|after)$ct+\s+)?$ot+$tereldayexpr$ct+
(\s+$ot+(morning|afternoon|evening|night)$ct+)?)/<timex$tever type=\"date\">$1
<\/timex$tever>/gio;

$string =   s/($ot+\w+$ct+\s+)<timex$tever type=\"date\"[  >]*>($ot+(today|tonight)$ct+)
<\/timex$tever>/$1$4/gso;

# this (morning/afternoon/evening)
$string =   s/(($ot+(early|late)$ct+\s+)?$ot+this$ct+\s*$ot+(morning|afternoon|evening)$ct+)/

<timex$tever type=\"date\">$1<\/timex$tever>/gosi;

$string =   s/(($ot+(early|late)$ct+\s+)?$ot+last$ct+\s*$ot+night$ct+)/<timex$tever

type=\"date\">$1<\/timex$tever>/gsio;

figure 17.20 perl fragment from the gutime temporal tagging system in tarsqi (verhagen et al., 2005).

features are extracted from the token and its context, and a statistical sequence
labeler is trained (any sequence model can be used). figure 17.21 lists standard
features used in temporal tagging.

explanation
the target token to be labeled

feature
token
tokens in window bag of tokens in the window around a target
shape
pos
chunk tags
lexical triggers
figure 17.21 typical features used to train iob-style temporal expression taggers.

character shape features
parts of speech of target and window words
base-phrase chunk tag for target and words in a window
presence in a list of temporal terms

temporal expression recognizers are evaluated with the usual recall, precision,
and f-measures. a major dif   culty for all of these very lexicalized approaches is
avoiding expressions that trigger false positives:
(17.15) 1984 tells the story of winston smith...
(17.16) ...u2   s classic sunday bloody sunday

17.3.2 temporal id172
temporal id172 is the process of mapping a temporal expression to either
a speci   c point in time or to a duration. points in time correspond to calendar dates,
to times of day, or both. durations primarily consist of lengths of time but may also
include information about start and end points. normalized times are represented
with the value attribute from the iso 8601 standard for encoding temporal values
(iso8601, 2004). fig. 17.22 reproduces our earlier example with the value attributes
added in.

temporal
id172

<timex3 i d =         t 1        

t y p e =   date    v a l u e =   2007   07   02    f u n c t i o n i n d o c u m e n t =   creation time   
i n i t i a t e d <timex3 i d =    t 2     t y p e =   date   

> j u l y 2 , 2007 </ timex3> a f a r e
v a l u e =   2007   w26    anchortimeid=    t 1    > l a s t week</ timex3> by u n i t e d a i r l i n e s was
matched by c o m p e t i t o r s o v e r <timex3 i d =    t 3     t y p e =   duration    v a l u e =   p1we   
anchortimeid=    t 1    > t h e weekend </ timex3> , marking t h e second s u c c e s s f u l
f a r e
i n c r e a s e
weeks </ timex3>.

i n <timex3 i d =    t 4     t y p e =   duration    v a l u e =   p2w    anchortimeid=    t 1    > two

i n c r e a s e

figure 17.22 timeml markup including normalized values for temporal expressions.

the dateline, or document date, for this text was july 2, 2007. the iso repre-
sentation for this kind of expression is yyyy-mm-dd, or in this case, 2007-07-02.

17.3

    extracting times

347

the encodings for the temporal expressions in our sample text all follow from this
date, and are shown here as values for the value attribute.

the    rst temporal expression in the text proper refers to a particular week of the
year. in the iso standard, weeks are numbered from 01 to 53, with the    rst week
of the year being the one that has the    rst thursday of the year. these weeks are
represented with the template yyyy-wnn. the iso week for our document date is
week 27; thus the value for last week is represented as    2007-w26   .

the next temporal expression is the weekend. iso weeks begin on monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. weekends are treated as durations, so the value of the value attribute has
to be a length. durations are represented according to the pattern pnx, where n is
an integer denoting the length and x represents the unit, as in p3y for three years
or p2d for two days. in this example, one weekend is captured as p1we. in this
case, there is also suf   cient information to anchor this particular weekend as part of
a particular week. such information is encoded in the anchortimeid attribute.
finally, the phrase two weeks also denotes a duration captured as p2w. there is a
lot more to the various temporal annotation standards   far too much to cover here.
figure 17.23 describes some of the basic ways that other times and durations are
represented. consult iso8601 (2004), ferro et al. (2005), and pustejovsky et al.
(2005) for more details.

unit
fully speci   ed dates
weeks
weekends
24-hour clock times
dates and times
financial quarters
figure 17.23 sample iso patterns for representing various times and durations.

pattern
yyyy-mm-dd
yyyy-wnn
pnwe
hh:mm:ss
yyyy-mm-ddthh:mm:ss
qn

sample value
1991-09-28
2007-w27
p1we
11:13:45
1991-09-28t11:00:00
1999-q3

fully quali   ed

most current approaches to temporal id172 are rule-based (chang and
manning 2012, str  otgen and gertz 2013). patterns that match temporal expres-
sions are associated with semantic analysis procedures. as in the compositional
rule-to-rule approach introduced in chapter 15, the meaning of a constituent is com-
puted from the meaning of its parts using a method speci   c to the constituent, al-
though here the semantic composition rules involve temporal arithmetic rather than
   -calculus attachments.

fully quali   ed date expressions contain a year, month, and day in some con-
ventional form. the units in the expression must be detected and then placed in the
correct place in the corresponding iso pattern. the following pattern normalizes
expressions like april 24, 1916.

fqte     month date , year

{year.val     month.val     date.val}

the non-terminals month, date, and year represent constituents that have already
been recognized and assigned semantic values, accessed through the *.val notation.
the value of this fqe constituent can, in turn, be accessed as fqte.val during
further processing.

fully quali   ed temporal expressions are fairly rare in real texts. most temporal
expressions in news articles are incomplete and are only implicitly anchored, of-
ten with respect to the dateline of the article, which we refer to as the document   s

348 chapter 17

   

information extraction

temporal
anchor

temporal anchor. the values of temporal expressions such as today, yesterday, or
tomorrow can all be computed with respect to this temporal anchor. the semantic
procedure for today simply assigns the anchor, and the attachments for tomorrow
and yesterday add a day and subtract a day from the anchor, respectively. of course,
given the cyclic nature of our representations for months, weeks, days, and times of
day, our temporal arithmetic procedures must use modulo arithmetic appropriate to
the time unit being used.

unfortunately, even simple expressions such as the weekend or wednesday in-
troduce a fair amount of complexity. in our current example, the weekend clearly
refers to the weekend of the week that immediately precedes the document date. but
this won   t always be the case, as is illustrated in the following example.
(17.17) random security checks that began yesterday at sky harbor will continue

at least through the weekend.

in this case, the expression the weekend refers to the weekend of the week that the
anchoring date is part of (i.e., the coming weekend). the information that signals
this meaning comes from the tense of continue, the verb governing the weekend.

relative temporal expressions are handled with temporal arithmetic similar to
that used for today and yesterday. the document date indicates that our example
article is iso week 27, so the expression last week normalizes to the current week
minus 1. to resolve ambiguous next and last expressions we consider the distance
from the anchoring date to the nearest unit. next friday can refer either to the
immediately next friday or to the friday following that, but the closer the document
date is to a friday, the more likely it is that the phrase will skip the nearest one. such
ambiguities are handled by encoding language and domain-speci   c heuristics into
the temporal attachments.

17.4 extracting events and their times

event
extraction

the task of event extraction is to identify mentions of events in texts. for the
purposes of this task, an event mention is any expression denoting an event or state
that can be assigned to a particular point, or interval, in time. the following markup
of the sample text on page 345 shows all the events in this text.

[event citing] high fuel prices, united airlines [event said] fri-
day it has [event increased] fares by $6 per round trip on    ights to
some cities also served by lower-cost carriers. american airlines, a unit
of amr corp., immediately [event matched]
[event the move],
spokesman tim wagner [event said]. united, a unit of ual corp.,
[event said] [event the increase] took effect thursday and [event
applies] to most routes where it [event competes] against discount
carriers, such as chicago to dallas and denver to san francisco.

in english, most event mentions correspond to verbs, and most verbs introduce
events. however, as we can see from our example, this is not always the case. events
can be introduced by noun phrases, as in the move and the increase, and some verbs
fail to introduce events, as in the phrasal verb took effect, which refers to when the
event began rather than to the event itself. similarly, light verbs such as make, take,
and have often fail to denote events; for light verbs the event is often expressed by
the nominal direct object (took a    ight), and these light verbs just provide a syntactic
structure for the noun   s arguments.

reporting
events

17.4

    extracting events and their times

349

various versions of the event extraction task exist, depending on the goal. for
example in the tempeval shared tasks (verhagen et al. 2009) the goal is to extract
events and aspects like their aspectual and temporal properties. events are to be
classi   ed as actions, states, reporting events (say, report, tell, explain), perception
events, and so on. the aspect, tense, and modality of each event also needs to be
extracted. thus for example the various said events in the sample text would be
annotated as (class=reporting, tense=past, aspect=perfective).

event extraction is generally modeled via supervised learning, detecting events
via sequence models with iob tagging, and assigning event classes and attributes
with multi-class classi   ers. common features include surface information like parts
of speech, lexical items, and verb tense information; see fig. 17.24.

explanation
character-level pre   xes and suf   xes of target word
character level suf   xes for nominalizations (e.g., -tion)
part of speech of the target word
binary feature indicating that the target is governed by a light verb

feature
character af   xes
nominalization suf   x
part of speech
light verb
subject syntactic category syntactic category of the subject of the sentence
morphological stem
verb root
id138 hypernyms
figure 17.24 features commonly used in both rule-based and machine learning approaches to event detec-
tion.

stemmed version of the target word
root form of the verb basis for a nominalization
hypernym set for the target

17.4.1 temporal ordering of events
with both the events and the temporal expressions in a text having been detected, the
next logical task is to use this information to    t the events into a complete timeline.
such a timeline would be useful for applications such as id53 and
summarization. this ambitious task is the subject of considerable current research
but is beyond the capabilities of current systems.

a somewhat simpler, but still useful, task is to impose a partial ordering on the
events and temporal expressions mentioned in a text. such an ordering can provide
many of the same bene   ts as a true timeline. an example of such a partial ordering
is the determination that the fare increase by american airlines came after the fare
increase by united in our sample text. determining such an ordering can be viewed
as a binary relation detection and classi   cation task similar to those described earlier
in section 17.2. the temporal relation between events is classi   ed into one of the
standard set of allen relations shown in fig. 17.25 (allen, 1984), using feature-
based classi   ers as in section 17.2, trained on the timebank corpus with features
like words/embeddings, parse paths, tense and aspect.

the timebank corpus consists of text annotated with much of the information
we   ve been discussing throughout this section (pustejovsky et al., 2003b). time-
bank 1.2 consists of 183 news articles selected from a variety of sources, including
the id32 and propbank collections.

each article in the timebank corpus has had the temporal expressions and event
mentions in them explicitly annotated in the timeml annotation (pustejovsky et al.,
2003a).
in addition to temporal expressions and events, the timeml annotation
provides temporal links between events and temporal expressions that specify the
nature of the relation between them. consider the following sample sentence and

allen relations

timebank

350 chapter 17

   

information extraction

figure 17.25 the 13 temporal relations from allen (1984).

<timex3 tid="t57" type="date" value="1989-10-26"
10/26/89 </timex3>

functionindocument="creation_time">

<timex3 tid="t58" type="date" value="1989-q1" anchortimeid="t57"> the

delta air lines earnings <event eid="e1" class="occurrence"> soared </event> 33% to a
record in
fiscal first quarter </timex3>, <event eid="e3"
the industry trend toward <event eid="e4" class="occurrence">declining</event>
profits.

class="occurrence">bucking</event>

figure 17.26 example from the timebank corpus.

its corresponding markup shown in fig. 17.26, selected from one of the timebank
documents.
(17.18) delta air lines earnings soared 33% to a record in the    scal    rst quarter,

bucking the industry trend toward declining pro   ts.

as annotated, this text includes three events and two temporal expressions. the
events are all in the occurrence class and are given unique identi   ers for use in fur-
ther annotations. the temporal expressions include the creation time of the article,
which serves as the document time, and a single temporal expression within the text.
in addition to these annotations, timebank provides four links that capture the
temporal relations between the events and times in the text, using the allen relations
from fig. 17.25. the following are the within-sentence temporal relations annotated
for this example.

bababaaabbabtime a  before bb after  aa overlaps bb overlaps' aa meets bb meets' aa equals b(b equals a)a starts bb starts' aa finishes bb finishes' aba during bb during' aa17.5

    template filling

351

    soaringe1 is included in the    scal    rst quartert58
    soaringe1 is before 1989-10-26t57
    soaringe1 is simultaneous with the buckinge3
    declininge4 includes soaringe1

17.5 template filling

scripts

templates

template    lling

many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. these abstract
situations or stories, related to what have been called scripts (schank and abel-
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. the strong expectations provided by these scripts can facilitate the proper
classi   cation of entities, the assignment of entities into roles and relations, and most
critically, the drawing of id136s that    ll in things that have been left unsaid. in
their simplest form, such scripts can be represented as templates consisting of    xed
sets of slots that take as values slot-   llers belonging to particular classes. the task
of template    lling is to    nd documents that invoke particular scripts and then    ll the
slots in the associated templates with    llers extracted from the text. these slot-   llers
may consist of text segments extracted directly from the text, or they may consist of
concepts that have been inferred from text elements through some additional pro-
cessing.

a    lled template from our original airline story might look like the following.

fare-raise attempt:                

lead airline:
amount:
effective date:
follower:

united airlines
$6
2006-10-26
american airlines

               

this template has four slots (lead airline, amount, effective date, fol-
lower). the next section describes a standard sequence-labeling approach to    lling
slots. section 17.5.2 then describes an older system based on the use of cascades of
   nite-state transducers and designed to address a more complex template-   lling task
that current learning-based systems don   t yet address.

17.5.1 machine learning approaches to template filling
in the standard paradigm for template    lling, we are trying to    ll    xed known tem-
plates with known slots, and also assumes training documents labeled with examples
of each template, and the    llers of each slot marked in the text. the is to create one
template for each event in the input documents, with the slots    lled with text from
the document.

the task is generally modeled by training two separate supervised systems. the
   rst system decides whether the template is present in a particular sentence. this
task is called template recognition or sometimes, in a perhaps confusing bit of
terminology, event recognition. template recognition can be treated as a text classi-
   cation task, with features extracted from every sequence of words that was labeled
in training documents as    lling any slot from the template being detected. the usual
set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,
syntactic chunk tags, and named entity tags.

template
recognition

352 chapter 17

   

information extraction

role-   ller
extraction

the second system has the job of role-   ller extraction. a separate classi   er is
trained to detect each role (lead-airline, amount, and so on). this can be a
binary classi   er that is run on every noun-phrase in the parsed input sentence, or a
sequence model run over sequences of words. each role classi   er is trained on the
labeled data in the training set. again, the usual set of features can be used, but now
trained only on an individual noun phrase or the    llers of a single slot.

multiple non-identical text segments might be labeled with the same slot la-
bel. for example in our sample text, the strings united or united airlines might be
labeled as the lead airline. these are not incompatible choices and the corefer-
ence resolution techniques introduced in chapter 20 can provide a path to a solution.
a variety of annotated collections have been used to evaluate this style of ap-
proach to template    lling, including sets of job announcements, conference calls for
papers, restaurant guides, and biological texts. recent work focuses on extracting
templates in cases where there is no training data or even prede   ned templates, by
inducing templates as sets of linked events (chambers and jurafsky, 2011).

17.5.2 earlier finite-state template-filling systems
the templates above are relatively simple. but consider the task of producing a
template that contained all the information in a text like this one (grishman and
sundheim, 1995):

bridgestone sports co. said friday it has set up a joint venture in taiwan
with a local concern and a japanese trading house to produce golf clubs to be
shipped to japan. the joint venture, bridgestone sports taiwan co., capital-
ized at 20 million new taiwan dollars, will start production in january 1990
with production of 20,000 iron and    metal wood    clubs a month.

the muc-5    joint venture    task (the message understanding conferences were
a series of u.s. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures. figure 17.27
shows a structure produced by the fastus system (hobbs et al., 1997). note how
the    ller of the activity slot of the tie-up template is itself a template with slots.

tie-up-1
relationship
entities

tie-up
bridgestone sports co.
a local concern
a japanese trading house

activity-1:
company
product
start date during: january 1990

bridgestone sports taiwan co.
iron and    metal wood    clubs

joint venture bridgestone sports taiwan co.
activity
amount
figure 17.27 the templates produced by fastus given the input text on page 352.

activity-1
nt$20000000

early systems for dealing with these complex templates were based on cascades

of transducers based on hand-written rules, as sketched in fig. 17.28.

the    rst four stages use hand-written regular expression and grammar rules to
do basic id121, chunking, and parsing. stage 5 then recognizes entities and
events with a fst-based recognizer and inserts the recognized objects into the ap-
propriate slots in templates. this fst recognizer is based on hand-built regular
expressions like the following (ng indicates noun-group and vg verb-group),
which matches the    rst sentence of the news story above.

17.6

    summary

353

no. step
1
2
3
4
5
6

tokens
complex words
basic phrases
complex phrases
semantic patterns
merging

description
tokenize input stream of characters
multiword phrases, numbers, and proper names.
segment sentences into noun and verb groups
identify complex noun groups and verb groups
identify entities and events, insert into templates.
merge references to the same entity or event

figure 17.28 levels of processing in fastus (hobbs et al., 1997). each level extracts a
speci   c type of information which is then passed on to the next higher level.

ng(company/ies) vg(set-up) ng(joint-venture) with ng(company/ies)

vg(produce) ng(product)

the result of processing these two sentences is the    ve draft templates (fig. 17.29)
that must then be merged into the single hierarchical structure shown in fig. 17.27.
the merging algorithm, after performing coreference resolution, merges two activi-
ties that are likely to be describing the same events.

# template/slot
1 relationship:

entities:
2 activity:
product:

3 relationship:

value
tie-up
bridgestone co., a local concern, a japanese trading house
production
   golf clubs   
tie-up

joint venture:    bridgestone sports taiwan co.   
amount:
4 activity:
company:
startdate:

nt$20000000
production
   bridgestone sports taiwan co.   
during: january 1990
production
   iron and    metal wood    clubs   

5 activity:
product:

figure 17.29 the    ve partial templates produced by stage 5 of fastus. these templates
are merged in stage 6 to produce the    nal template shown in fig. 17.27 on page 352.

17.6 summary

this chapter has explored techniques for extracting limited forms of semantic con-
tent from texts.

sequence labeling techniques.

    named entities can be recognized and classi   ed by featured-based or neural
    relations among entities can be extracted by pattern-based approaches, su-
pervised learning methods when annotated training data is available, lightly
supervised id64 methods when small numbers of seed tuples or
seed patterns are available, distant supervision when a database of relations
is available, and unsupervised or open ie methods.

    reasoning about time can be facilitated by detection and id172 of
temporal expressions through a combination of statistical learning and rule-

354 chapter 17

   

information extraction

based methods.

    events can be detected and ordered in time using sequence models and classi-
   ers trained on temporally- and event-labeled data like the timebank corpus.
    template-   lling applications can recognize stereotypical situations in texts
and assign elements from the text to roles represented as    xed sets of slots.

bibliographical and historical notes

the earliest work on information extraction addressed the template-   lling task in the
context of the frump system (dejong, 1982). later work was stimulated by the u.s.
government-sponsored muc conferences (sundheim 1991, sundheim 1992, sund-
heim 1993, sundheim 1995). early muc systems like circus system (lehnert
et al., 1991) and scisor (jacobs and rau, 1990) were quite in   uential and inspired
later systems like fastus (hobbs et al., 1997). chinchor et al. (1993) describe the
muc evaluation techniques.

due to the dif   culty of porting systems from one domain to another, attention

shifted to machine learning approaches.

early supervised learning approaches to ie ( cardie 1993, cardie 1994, riloff 1993,

soderland et al. 1995, huffman 1996) focused on automating the knowledge acqui-
sition process, mainly for    nite-state rule-based systems. their success, and the
earlier success of id48-based id103, led to the use of sequence la-
beling (id48s: bikel et al. 1997; memms mccallum et al. 2000; crfs: laf-
ferty et al. 2001), and a wide exploration of features (zhou et al., 2005). neural
approaches to ner mainly follow from the pioneering results of collobert et al.
(2011), who applied a crf on top of a convolutional net. bilstms with word and
character-based embeddings as input followed shortly and became a standard neural
algorithm for ner (huang et al. 2015, ma and hovy 2016, lample et al. 2016).

neural algorithms for id36 often explore architectures that can
handle entities far apart in the sentence: recursive networks (socher et al., 2012),
convolutional nets (dos santos et al., 2015), or chain or tree lstms (miwa and
bansal 2016, peng et al. 2017).

progress in this area continues to be stimulated by formal evaluations with shared
benchmark datasets, including the automatic content extraction (ace) evaluations
of 2000-2007 on id39, id36, and temporal ex-
pressions3, the kbp (knowledge base population) evaluations (ji et al. 2010, sur-
deanu 2013) of id36 tasks like slot    lling (extracting attributes (   slots   )
like age, birthplace, and spouse for a given entity) and a series of semeval work-
shops (hendrickx et al., 2009).

semisupervised id36 was    rst proposed by hearst (1992b), and
extended by systems like autoslog-ts (riloff, 1996), dipre (brin, 1998), snow-
ball (agichtein and gravano, 2000), and (jones et al., 1999). the distant super-
vision algorithm we describe was drawn from mintz et al. (2009), who coined the
term    distant supervision   , but similar ideas occurred in earlier systems like craven
and kumlien (1999) and morgan et al. (2004) under the name weakly labeled data,
as well as in snow et al. (2005) and wu and weld (2007). among the many exten-
sions are wu and weld (2010), riedel et al. (2010), and ritter et al. (2013). open

3 www.nist.gov/speech/tests/ace/

kbp
slot    lling

exercises

355

ie systems include knowitall etzioni et al. (2005), textrunner (banko et al.,
2007), and reverb (fader et al., 2011). see riedel et al. (2013) for a universal
schema that combines the advantages of distant supervision and open ie.

heideltime (str  otgen and gertz, 2013) and sutime (chang and manning, 2012)
are downloadable temporal extraction and id172 systems. the 2013 tempe-
val challenge is described in uzzaman et al. (2013); chambers (2013) and bethard
(2013) give typical approaches.

exercises

17.1 develop a set of id157 to recognize the character shape features

described on page 331.

17.2 the iob labeling scheme given in this chapter isn   t the only possible one. for
example, an e tag might be added to mark the end of entities, or the b tag
can be reserved only for those situations where an ambiguity exists between
adjacent entities. propose a new set of iob tags for use with your ner system.
experiment with it and compare its performance with the scheme presented
in this chapter.

17.3 names of works of art (books, movies, video games, etc.) are quite different
from the kinds of named entities we   ve discussed in this chapter. collect a
list of names of works of art from a particular category from a web-based
source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). analyze your list
and give examples of ways that the names in it are likely to be problematic for
the techniques described in this chapter.

17.4 develop an ner system speci   c to the category of names that you collected in
the last exercise. evaluate your system on a collection of text likely to contain
instances of these named entities.

17.5 acronym expansion, the process of associating a phrase with an acronym, can
be accomplished by a simple form of relational analysis. develop a system
based on the relation analysis approaches described in this chapter to populate
if you focus on english three letter
a database of acronym expansions.
acronyms (tlas) you can evaluate your system   s performance by comparing
it to wikipedia   s tla page.

17.6 a useful functionality in newer email and calendar applications is the ability
to associate temporal expressions connected with events in email (doctor   s
appointments, meeting planning, party invitations, etc.) with speci   c calendar
entries. collect a corpus of email containing temporal expressions related to
event planning. how do these expressions compare to the kinds of expressions
commonly found in news text that we   ve been discussing in this chapter?

17.7 acquire the cmu seminar corpus and develop a template-   lling system by
using any of the techniques mentioned in section 17.5. analyze how well
your system performs as compared with state-of-the-art results on this corpus.

356 chapter 18

    id14

chapter

18 id14

sometime between the 7th and 4th centuries bce, the indian grammarian p  an. ini1
wrote a famous treatise on sanskrit grammar, the as.t.  adhy  ay     (   8 books   ), a treatise
that has been called    one of the greatest monuments of
human intelligence    (bloom   eld, 1933b, 11). the work
describes the linguistics of the sanskrit language in the
form of 3959 sutras, each very ef   ciently (since it had to
be memorized!) expressing part of a formal rule system
that brilliantly pre   gured modern mechanisms of formal
language theory (penn and kiparsky, 2012). one set of
rules, relevant to our discussion in this chapter, describes
the k  arakas, semantic relationships between a verb and
noun arguments, roles like agent, instrument, or destina-
tion. p  an. ini   s work was the earliest we know of that tried
to understand the linguistic realization of events and their participants. this task
of understanding participants and their relationship to events   being able to answer
the question    who did what to whom    (and perhaps also    when and where   )   is a
central question of natural language understanding.

let   s move forward 2.5 millenia to the present and consider the very mundane
goal of understanding text about a purchase of stock by xyz corporation. this
purchasing event could take on a wide variety of surface forms. in the following
sentences we see that it could be described by a verb (sold, bought) or a noun (pur-
chase), and that xyz corp can be the syntactic subject (of bought), the indirect ob-
ject (of sold), or in a genitive or noun compound relation (with the noun purchase)
despite having notationally the same role in all of them:

    xyz corporation bought the stock.
    they sold the stock to xyz corporation.
    the stock was bought by xyz corporation.
    the purchase of the stock by xyz corporation...
    the stock purchase by xyz corporation...
in this chapter we introduce a level of representation that lets us capture the
commonality between these sentences. we will be able to represent the fact that
there was a purchase event, that the participants in this event were xyz corp and
some stock, and that xyz corp played a speci   c role, the role of acquiring the stock.
we call this shallow semantic representation level semantic roles. semantic
roles are representations that express the abstract role that arguments of a predicate
can take in the event; these can be very speci   c, like the buyer, abstract like the
agent, or super-abstract (the proto-agent). these roles can both represent gen-
eral semantic properties of the arguments and also express their likely relationship to
the syntactic role of the argument in the sentence. agents tend to be the subject of

1 figure shows a birch bark manuscript from kashmir of the rupavatra, a grammatical textbook based
on the sanskrit grammar of panini. image from the wellcome collection.

18.1

    semantic roles

357

an active sentence, themes the direct object, and so on. these relations are codi   ed
in databases like propbank and framenet. we   ll introduce id14,
the task of assigning roles to the constituents or phrases in sentences. we   ll also
discuss selectional restrictions, the semantic sortal restrictions or preferences that
each individual predicate can express about its potential arguments, such as the fact
that the theme of the verb eat is generally something edible. along the way, we   ll
describe the various ways these representations can help in language understanding
tasks like id53 and machine translation.

18.1 semantic roles

consider how in chapter 14 we represented the meaning of arguments for sentences
like these:

(18.1) sasha broke the window.
(18.2) pat opened the door.

a neo-davidsonian event representation of these two sentences would be

   e,x,y breaking(e)    breaker(e,sasha)
   e,x,y opening(e)    opener(e,pat)

   brokent hing(e,y)   window(y)
   openedt hing(e,y)    door(y)

deep roles

thematic roles
agents

theme

semantic roles

in this representation, the roles of the subjects of the verbs break and open are
breaker and opener respectively. these deep roles are speci   c to each event; break-
ing events have breakers, opening events have openers, and so on.

if we are going to be able to answer questions, perform id136s, or do any
further kinds of natural language understanding of these events, we   ll need to know
a little more about the semantics of these arguments. breakers and openers have
something in common. they are both volitional actors, often animate, and they have
direct causal responsibility for their events.

thematic roles are a way to capture this semantic commonality between break-
ers and eaters. we say that the subjects of both these verbs are agents. thus, agent
is the thematic role that represents an abstract idea such as volitional causation. sim-
ilarly, the direct objects of both these verbs, the brokenthing and openedthing, are
both prototypically inanimate objects that are affected in some way by the action.
the semantic role for these participants is theme.

although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to fillmore (1968) and gruber (1965). although
there is no universally agreed-upon set of roles, figs. 18.1 and 18.2 list some the-
matic roles that have been used in various computational papers, together with rough
de   nitions and examples. most thematic role sets have about a dozen roles, but we   ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speci   c to situations. we   ll use the general
term semantic roles for all sets of roles, whether small or large.

358 chapter 18

    id14

thematic role
de   nition
the volitional causer of an event
agent
the experiencer of an event
experiencer
the non-volitional causer of the event
force
the participant most directly affected by an event
theme
the end product of an event
result
the proposition or content of a propositional event
content
an instrument used in an event
instrument
the bene   ciary of an event
beneficiary
the origin of the object of a transfer event
source
the destination of an object of a transfer event
goal
figure 18.1 some commonly used thematic roles with their de   nitions.

thematic role
agent
experiencer
force
theme
result
content
instrument
beneficiary
source
goal
figure 18.2 some prototypical examples of various thematic roles.

example
the waiter spilled the soup.
john has a headache.
the wind blows debris from the mall into our yards.
only after benjamin franklin broke the ice...
the city built a regulation-size baseball diamond...
mona asked    you met mary ann at a supermarket?   
he poached cat   sh, stunning them with a shocking device...
whenever ann callahan makes hotel reservations for her boss...
i    ew in from boston.
i drove to portland.

18.2 diathesis alternations

the main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple id136s that aren   t possible
from the pure surface string of words, or even from the parse tree. to extend the
earlier examples, if a document says that company a acquired company b, we   d
like to know that this answers the query was company b acquired? despite the fact
that the two sentences have very different surface syntax. similarly, this shallow
semantics might act as a useful intermediate language in machine translation.

semantic roles thus help generalize over different surface realizations of pred-
icate arguments. for example, while the agent is often realized as the subject of
the sentence, in other cases the theme can be the subject. consider these possible
realizations of the thematic arguments of the verb break:
(18.3) john

broke the window.

agent

theme

(18.4) john

broke the window

with a rock.

agent
(18.5) the rock

theme
broke the window.

instrument

instrument

theme

(18.6) the window

broke.

theme

(18.7) the window

was broken by john.

theme

agent

18.3

    semantic roles: problems with thematic roles

359

thematic grid
case frame

these examples suggest that break has (at least) the possible arguments agent,
theme, and instrument. the set of thematic role arguments taken by a verb is
often called the thematic grid,   -grid, or case frame. we can see that there are
(among others) the following possibilities for the realization of these arguments of
break:

agent/subject, theme/object
agent/subject, theme/object,
instrument/subject, theme/object
theme/subject

instrument/ppwith

it turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. for example, verbs like give can realize the theme and goal
arguments in two different ways:
(18.8)

gave the book

a. doris
agent
b. doris
agent

to cary.
goal

theme

gave cary
goal

the book.
theme

verb
alternation
dative
alternation

these multiple argument structure realizations (the fact that break can take agent,

instrument, or theme as subject, and give can realize its theme and goal in
either order) are called verb alternations or diathesis alternations. the alternation
we showed above for give, the dative alternation, seems to occur with particular se-
mantic classes of verbs, including    verbs of future having    (advance, allocate, offer,
owe),    send verbs    (forward, hand, mail),    verbs of throwing    (kick, pass, throw),
and so on. levin (1993) lists for 3100 english verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speci   c classes) and the
various alternations in which they participate. these lists of verb classes have been
incorporated into the online resource verbnet (kipper et al., 2000), which links each
verb to both id138 and framenet entries.

18.3 semantic roles: problems with thematic roles

representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. yet it has proved quite dif   -
cult to come up with a standard set of roles, and equally dif   cult to produce a formal
de   nition of roles like agent, theme, or instrument.

for example, researchers attempting to de   ne role sets often    nd they need to
fragment a role like agent or theme into many speci   c roles. levin and rappa-
port hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of instruments, intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(18.9)

a. the cook opened the jar with the new gadget.
b. the new gadget opened the jar.
a. shelly ate the sliced banana with a fork.
b. *the fork ate the sliced banana.

(18.10)

in addition to the fragmentation problem, there are cases in which we   d like to
reason about and generalize across semantic roles, but the    nite discrete lists of roles
don   t let us do this.

360 chapter 18

    id14

semantic role

proto-agent
proto-patient

finally, it has proved dif   cult to formally de   ne the thematic roles. consider the
agent role; most cases of agents are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.

these problems have led to alternative semantic role models that use either

many fewer or many more roles.

the    rst of these options is to de   ne generalized semantic roles that abstract
over the speci   c thematic roles. for example, proto-agent and proto-patient
are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. these roles are de   ned, not by necessary and suf   cient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood
that the argument can be labeled a proto-agent. the more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a proto-patient.

the second direction is instead to de   ne semantic roles that are speci   c to a

particular verb or a particular group of semantically related verbs or nouns.

in the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. propbank uses both proto-
roles and verb-speci   c semantic roles. framenet uses semantic roles that are spe-
ci   c to a general semantic idea called a frame.

18.4 the proposition bank

propbank

the proposition bank, generally referred to as propbank, is a resource of sen-
tences annotated with semantic roles. the english propbank labels all the sentences
in the id32; the chinese propbank labels sentences in the penn chinese
treebank. because of the dif   culty of de   ning a universal set of thematic roles,
the semantic roles in propbank are de   ned with respect to an individual verb sense.
each sense of each verb thus has a speci   c set of roles, which are given only numbers
rather than names: arg0, arg1, arg2, and so on. in general, arg0 represents the
proto-agent, and arg1, the proto-patient. the semantics of the other roles
are less consistent, often being de   ned speci   cally for each verb. nonetheless there
are some generalization; the arg2 is often the benefactive, instrument, attribute, or
end state, the arg3 the start point, benefactive, instrument, or attribute, and the arg4
the end point.

here are some slightly simpli   ed propbank entries for one sense each of the
verbs agree and fall. such propbank entries are called frame    les; note that the
de   nitions in the frame    le for each role (   other entity agreeing   ,    extent, amount
fallen   ) are informal glosses intended to be read by humans, rather than being formal
de   nitions.

(18.11) agree.01

18.4

    the proposition bank

361

arg0: agreer
arg1: proposition
arg2: other entity agreeing

ex1:
ex2:

[arg0 the group] agreed [arg1 it wouldn   t make an offer].
[argm-tmp usually] [arg0 john] agrees [arg2 with mary]
[arg1 on everything].

(18.12) fall.01

arg1: logical subject, patient, thing falling
arg2: extent, amount fallen
arg3: start point
arg4: end point, end state of arg1
ex1:
ex2:

[arg1 sales] fell [arg4 to $25 million] [arg3 from $27 million].
[arg1 the average junk bond] fell [arg2 by 4.2%].

note that there is no arg0 role for fall, because the normal subject of fall is a

proto-patient.

the propbank semantic roles can be useful in recovering shallow semantic in-

formation about verbal arguments. consider the verb increase:
(18.13) increase.01    go up incrementally   

thing increasing

arg0: causer of increase
arg1:
arg2: amount increased by, ext, or mnr
arg3: start point
arg4: end point

a propbank id14 would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case big
fruit co. is the agent and the price of bananas is the theme, despite the differing
surface forms.
(18.14)
(18.15)
(18.16)

[arg0 big fruit co. ] increased [arg1 the price of bananas].
[arg1 the price of bananas] was increased again [arg0 by big fruit co. ]
[arg1 the price of bananas] increased [arg2 5%].

propbank also has a number of non-numbered arguments called argms, (argm-
tmp, argm-loc, etc) which represent modi   cation or adjunct meanings. these are
relatively stable across predicates, so aren   t listed with each frame    le. data labeled
with these modi   ers can be helpful in training systems to detect temporal, location,
or directional modi   cation across predicates. some of the argm   s include:

yesterday evening, now
at the museum, in san francisco
down, to bangkok
clearly, with much enthusiasm
because ... , in response to the ruling
themselves, each other

tmp
when?
loc
where?
dir
where to/from?
mnr
how?
prp/cau why?
rec
adv
prd
while propbank focuses on verbs, a related project, nombank (meyers et al.,
2004) adds annotations to noun predicates. for example the noun agreement in
apple   s agreement with ibm would be labeled with apple as the arg0 and ibm as

miscellaneous
secondary predication

...ate the meat raw

nombank

362 chapter 18

    id14

the arg2. this allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.

18.5 framenet

while making id136s about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
id136s in many more situations, across different verbs, and also between verbs
and nouns. for example, we   d like to extract the similarity among these three sen-
tences:
(18.17)
(18.18)
(18.19) there has been a [arg2 5%] rise [arg1 in the price of bananas].

[arg1 the price of bananas] increased [arg2 5%].
[arg1 the price of bananas] rose [arg2 5%].

framenet

frame

model
script

frame elements

note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. we   d like a system to recognize that the
price of bananas is what went up, and that 5% is the amount it went up, no matter
whether the 5% appears as the object of the verb increased or as a nominal modi   er
of the noun rise.

the framenet project is another semantic-role-labeling project that attempts
to address just these kinds of problems (baker et al. 1998, fillmore et al. 2003,
fillmore and baker 2009, ruppenhofer et al. 2016). whereas roles in the propbank
project are speci   c to an individual verb, roles in the framenet project are speci   c
to a frame.

what is a frame? consider the following set of words:

reservation,    ight, travel, buy, price, cost, fare, rates, meal, plane

there are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. the resulting set of relations does not,
however, add up to a complete account of how these words are related. they are
clearly all de   ned with respect to a coherent chunk of common-sense background
information concerning air travel.

we call the holistic background knowledge that unites these words a frame (fill-
more, 1985). the idea that groups of words are de   ned with respect to some back-
ground information is widespread in arti   cial intelligence and cognitive science,
where besides frame we see related works like a model (johnson-laird, 1983), or
even script (schank and abelson, 1977).

a frame in framenet is a background knowledge structure that de   nes a set of
frame-speci   c semantic roles, called frame elements, and includes a set of predi-
cates that use these roles. each word evokes a frame and pro   les some aspect of the
frame and its elements. the framenet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. for example, the change position on a scale frame is de   ned as
follows:

this frame consists of words that indicate the change of an item   s posi-
tion on a scale (the attribute) from a starting point (initial value) to an
end point (final value).

core roles

some of the semantic roles (frame elements) in the frame are de   ned as in
fig. 18.3. note that these are separated into core roles, which are frame speci   c, and

non-core roles

non-core roles, which are more like the arg-m arguments in propbank, expressed
more general properties of time, location, and so on.

18.5

    framenet

363

core roles

attribute
difference
final state

the attribute is a scalar property that the item possesses.
the distance by which an item changes its position on the scale.
a description that presents the item   s state after the change in the attribute   s
value as an independent predication.
the position on the scale where the item ends up.

final value
initial state a description that presents the item   s state before the change in the at-

tribute   s value as an independent predication.

initial value the initial position on the scale from which the item moves away.
item
value range a portion of the scale, typically identi   ed by its end points, along which the

the entity that has a position on the scale.

duration
speed
group

values of the attribute    uctuate.

some non-core roles

the length of time over which the change takes place.
the rate of change of the value.
the group in which an item changes the value of an
attribute in a speci   ed way.

figure 18.3 the frame elements in the change position on a scale frame from the framenet labelers
guide (ruppenhofer et al., 2016).

here are some example sentences:

(18.20)
(18.21)
(18.22)
(18.23)

[item oil] rose [attribute in price] [difference by 2%].
[item it] has increased [final state to having them 1 day a month].
[item microsoft shares] fell [final value to 7 5/8].
[item colon cancer incidence] fell [difference by 50%] [group among

men].

(18.24) a steady increase [initial value from 9.5] [final value to 14.3] [item

in dividends]

(18.25) a [difference 5%] [item dividend] increase...

note from these example sentences that the frame includes target words like rise,

fall, and increase. in fact, the complete frame consists of the following words:

soar
mushroom swell
swing
triple
tumble

edge
explode plummet
fall

verbs: dwindle move
advance
climb
decline
decrease    uctuate rise
diminish gain
grow
dip
increase skyrocket
double
drop
jump

rocket
shift

reach

slide

nouns: hike
decline
decrease

increase
rise

tumble

escalation shift
explosion
fall
   uctuation adverbs:
gain
increasingly
growth

framenet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be representing by inher-
itance as well). thus, there is a cause change of position on a scale frame that is
linked to the change of position on a scale frame by the cause relation, but that
adds an agent role and is used for causative examples such as the following:

364 chapter 18

    id14

(18.26)

[agent they] raised [item the price of their soda] [difference by 2%].
together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.

framenets have also been developed for many other languages including span-

ish, german, japanese, portuguese, italian, and chinese.

18.6 id14

semantic role
labeling

id14 (sometimes shortened as srl) is the task of automatically
   nding the semantic roles of each argument of each predicate in a sentence. cur-
rent approaches to id14 are based on supervised machine learning,
often using the framenet and propbank resources to specify what counts as a pred-
icate, de   ne the set of roles used in the task, and provide training and test sets.

recall that the difference between these two models of semantic roles is that
framenet (18.27) employs many frame-speci   c frame elements as roles, while prop-
bank (18.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speci   c labels, along with the more general argm labels. some
examples:

(18.27)

[you]
cognizer

can   t

[blame]
target evaluee

reason

[the program] [for being unable to identify it]

(18.28)

[the san francisco examiner]
arg0

issued
target arg1

[a special edition]

[yesterday]
argm-tmp

18.6.1 a feature-based algorithm for id14
a simpli   ed feature-based id14 algorithm is sketched in fig. 18.4.
feature-based algorithms   from the very earliest systems like (simmons, 1973)   
begin by parsing, using broad-coverage parsers to assign a parse to the input string.
figure 18.5 shows a parse of (18.28) above. the parse is then traversed to    nd all
words that are predicates.

for each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classi   cation to decide the semantic role (if any) it plays
for this predicate. given a labeled training set such as propbank or framenet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. a 1-of-n classi   er is then trained to predict a semantic role for
each constituent given these features, where n is the number of potential semantic
roles plus an extra none role for non-role constituents. any standard classi   cation
algorithms can be used. finally, for each test sentence to be labeled, the classi   er is
run on each relevant constituent.

instead of training a single-stage classi   er as in fig. 18.5, the node-level classi-

   cation task can be broken down into multiple steps:

1. pruning: since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.

2. identi   cation: a binary classi   cation of each node as an argument to be la-

beled or a none.

18.6

    id14

365

function semanticrolelabel(words) returns labeled tree

parse    parse(words)
for each predicate in parse do
for each node in parse do

featurevector    extractfeatures(node, predicate, parse)
classifynode(node, featurevector, parse)

figure 18.4 a generic semantic-role-labeling algorithm. classifynode is a 1-of-n clas-
si   er that assigns a semantic role (or none for non-role constituents), trained on labeled data
such as framenet or propbank.

figure 18.5 parse tree for a propbank sentence, showing the propbank argument labels. the dotted line
shows the path feature np   s   vp   vbd for arg0, the np-sbj constituent the san francisco examiner.

3. classi   cation: a 1-of-n classi   cation of all the constituents that were labeled

as arguments by the previous stage

the separation of identi   cation and classi   cation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational ef   -
ciency.

global optimization
the classi   cation algorithm of fig. 18.5 classi   es each argument separately (   lo-
cally   ), making the simplifying assumption that each argument of a predicate can be
labeled independently. this assumption is false; there are interactions between argu-
ments that require a more    global    assignment of labels to constituents. for example,
constituents in framenet and propbank are required to be non-overlapping. more
signi   cantly, the semantic roles of constituents are not independent. for example
propbank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled arg0 .

role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. for example, the local classi   ers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
viterbi decoding or re-ranking approach can be used to choose the best consensus
label. integer id135 (ilp) is another common way to choose a solution
that conforms best to multiple constraints.

snp-sbj=arg0vpdtnnpnnpnnpthesanfranciscoexaminervbd=targetnp=arg1pp-tmp=argm-tmpissueddtjjnninnpaspecialeditionaroundnnnp-tmpnoonyesterday366 chapter 18

    id14
features for id14
most systems use some generalization of the core set of features introduced by
gildea and jurafsky (2000). common basic features templates (demonstrated on
the np-sbj constituent the san francisco examiner in fig. 18.5) include:

mantic roles tend to appear as nps, others as s or pp, and so on.

    the governing predicate, in this case the verb issued. the predicate is a cru-
cial feature since labels are de   ned only with respect to a particular predicate.
    the phrase type of the constituent, in this case, np (or np-sbj). some se-
    the headword of the constituent, examiner. the headword of a constituent
can be computed with standard head rules, such as those given in chapter 10
in fig. 10.12. certain headwords (e.g., pronouns) place strong constraints on
the possible semantic roles they are likely to    ll.

    the headword part of speech of the constituent, nnp.
    the path in the parse tree from the constituent to the predicate. this path is
marked by the dotted line in fig. 18.5. following gildea and jurafsky (2000),
we can use a simple linear representation of the path, np   s   vp   vbd.     and
    represent upward and downward movement in the tree, respectively. the
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.

    the voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive). passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.

either before or after.

    the binary linear position of the constituent with respect to the predicate,
    the subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. we can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; vp     vbd
np pp for the predicate in fig. 18.5.

    the named entity type of the constituent.
    the    rst words and the last word of the constituent.
the following feature vector thus represents the    rst np in our example (recall
that most observations will have the value none rather than, for example, arg0,
since most constituents in the parse tree will not bear a semantic role):

arg0: [issued, np, examiner, nnp, np   s   vp   vbd, active, before, vp     np pp,
org, the, examiner]

other features are often used in addition, such as sets of id165s inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).

it   s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.

18.6.2 a neural algorithm for id14
the standard neural algorithm for id14 is based on the bi-lstm
iob tagger introduced in chapter 9, which we   ve seen applied to part-of-speech
tagging and named entity tagging, among other tasks. recall that with iob tagging,

18.6

    id14

367

figure 18.6 a bi-lstm approach to id14. most actual networks are
much deeper than shown in this    gure; 3 to 4 bi-lstm layers (6 to 8 total lstms) are
common. the input is a concatenation of an embedding for the input word and an embedding
of a binary variable which is 1 for the predicate to 0 for all other words. after he et al. (2017).

we have a begin and end tag for each possible role (b-arg0, i-arg0; b-arg1,
i-arg1, and so on), plus an outside tag o.

as with all the taggers, the goal is to compute the highest id203 tag se-

quence   y, given the input sequence of words w:

  y = argmax

y   t

p(y|w)

in algorithms like he et al. (2017), each input word is mapped to pre-trained em-
beddings, and also associated with an embedding for a    ag (0/1) variable indicating
whether that input word is the predicate. these concatenated embeddings are passed
through multiple layers of bi-directional lstm. state-of-the-art algorithms tend to
be deeper than for pos or ner tagging, using 3 to 4 layers (6 to 8 total lstms).
highway layers can be used to connect these layers as well.

output from the last bi-lstm can then be turned into an iob sequence as for
pos or ner tagging. tags can be locally optimized by taking the bi-lstm output,
passing it through a single layer into a softmax for each word that creates a proba-
bility distribution over all srl tags and the most likely tag for word xi is chosen as
ti, computing for each word essentially:

  yi = argmax
t   tags

p(t|wi)

however, just as feature-based srl tagging, this local approach to decoding doesn   t
exploit the global constraints between tags; a tag i-arg0, for example, must follow
another i-arg0 or b-arg0.

as we saw for pos and ner tagging, there are many ways to take advantage of
these global constraints. a crf layer can be used instead of a softmax layer on top
of the bi-lstm output, and the viterbi decoding algorithm can be used to decode
from the crf.

an even simpler viterbi decoding algorithm that may perform equally well and
doesn   t require adding crf complexity to the training process is to start with the
simple softmax. the softmax output (the entire id203 distribution over tags)
for each word is then treated it as a lattice and we can do viterbi decoding through
the lattice. the hard iob constraints can act as the transition probabilities in the

thecatslovehatsembeddingslstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2concatenationright-to-left lstid113ft-to-right lstmsoftmaxp(b-arg0)p(i-arg0)p(b-pred)p(b-arg1)0010word + is-predicate368 chapter 18

    id14

viterbi decoding (thus the transition from state i-arg0 to i-arg1 would have
id203 0). alternatively, the training data can be used to learn bigram or trigram
tag transition probabilities as if doing id48 decoding. fig. 18.6 shows a sketch of
the algorithm.

18.6.3 evaluation of id14
the standard evaluation for id14 is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and f-measure. identi   cation and classi   cation can
also be evaluated separately. two common datasets used for evaluation are conll-
2005 (carreras and m`arquez, 2005) and conll-2012 (pradhan et al., 2013).

18.7 selectional restrictions

selectional
restriction

we turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. a selectional restriction is a semantic type con-
straint that a verb imposes on the kind of concepts that are allowed to    ll its argument
roles. consider the two meanings associated with the following example:
(18.29) i want to eat someplace nearby.
there are two possible parses and semantic interpretations for this sentence.
in
the sensible interpretation, eat is intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. in the nonsensical speaker-as-
godzilla interpretation, eat is transitive and the phrase someplace nearby is the direct
object and the theme of the eating, like the np malaysian food in the following
sentences:
(18.30)

i want to eat malaysian food.

how do we know that someplace nearby isn   t the direct object in this sentence?
one useful cue is the semantic fact that the theme of eating events tends to be
something that is edible. this restriction placed by the verb eat on the    ller of its
theme argument is a selectional restriction.

selectional restrictions are associated with senses, not entire lexemes. we can

see this in the following examples of the lexeme serve:
(18.31) the restaurant serves green-lipped mussels.
(18.32) which airlines serve denver?
example (18.31) illustrates the offering-food sense of serve, which ordinarily re-
stricts its theme to be some kind of food example (18.32) illustrates the provides a
commercial service to sense of serve, which constrains its theme to be some type
of appropriate location.

selectional restrictions vary widely in their speci   city. the verb imagine, for
example, imposes strict requirements on its agent role (restricting it to humans
and other animate entities) but places very few semantic requirements on its theme
role. a verb like diagonalize, on the other hand, places a very speci   c constraint
on the    ller of its theme role: it has to be a matrix, while the arguments of the
adjectives odorless are restricted to concepts that could possess an odor:
(18.33) in rehearsal, i often ask the musicians to imagine a tennis game.

18.7

    selectional restrictions

369

(18.34) radon is an odorless gas that can   t be detected by human senses.
(18.35) to diagonalize a matrix is to    nd its eigenvalues.

these examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
this distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

18.7.1 representing selectional restrictions
one way to capture the semantics of selectional restrictions is to use and extend the
event representation of chapter 14. recall that the neo-davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. ignoring the issue of
the    -structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eat might look like the following:

   e,x,y eating(e)    agent(e,x)    t heme(e,y)

with this representation, all we know about y, the    ller of the theme role, is that
it is associated with an eating event through the theme relation. to stipulate the
selectional restriction that y must be something edible, we simply add a new term to
that effect:

   e,x,y eating(e)    agent(e,x)    t heme(e,y)    ediblet hing(y)

when a phrase like ate a hamburger is encountered, a semantic analyzer can

form the following kind of representation:

   e,x,y eating(e)    eater(e,x)    t heme(e,y)    ediblet hing(y)    hamburger(y)
this representation is perfectly reasonable since the membership of y in the category
hamburger is consistent with its membership in the category ediblething, assuming
a reasonable set of facts in the knowledge base. correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as takeoff would be inconsistent with membership in the
category ediblething.

while this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. first, using fol to perform the simple
task of enforcing selectional restrictions is overkill. other, far simpler, formalisms
can do the job with far less computational cost. the second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.

a more practical approach is to state selectional restrictions in terms of id138
synsets rather than as logical concepts. each predicate simply speci   es a id138
synset as the selectional restriction on each of its arguments. a meaning representa-
tion is well-formed if the role    ller word is a hyponym (subordinate) of this synset.
for our ate a hamburger example, for instance, we could set the selectional
restriction on the theme role of the verb eat to the synset {food, nutrient}, glossed
as any substance that can be metabolized by an animal to give energy and build

370 chapter 18

    id14

sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich

=> snack food

=> dish

=> nutriment, nourishment, nutrition...

=> food, nutrient

=> substance
=> matter

=> physical entity

=> entity

figure 18.7 evidence from id138 that hamburgers are edible.

tissue. luckily, the chain of hypernyms for hamburger shown in fig. 18.7 reveals
that hamburgers are indeed food. again, the    ller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
we can apply this approach to the theme roles of the verbs imagine, lift, and di-
agonalize, discussed earlier. let us restrict imagine   s theme to the synset {entity},
lift   s theme to {physical entity}, and diagonalize to {matrix}. this arrangement
correctly permits imagine a hamburger and lift a hamburger, while also correctly
ruling out diagonalize a hamburger.

18.7.2 selectional preferences
in the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (katz and fodor 1963,
hirst 1987). for example, the verb eat might require that its theme argument be
[+food]. early id51 systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.

very quickly, however, it became clear that these selectional restrictions were

better represented as preferences rather than strict constraints (wilks 1975c, wilks 1975b).
for example, selectional restriction violations (like inedible arguments of eat) often
occur in well-formed sentences, for example because they are negated (18.36), or
because selectional restrictions are overstated (18.37):
(18.36) but it fell apart in 1931, perhaps because people realized you can   t eat

gold for lunch if you   re hungry.

(18.37) in his two championship trials, mr. kulkarni ate glass on an empty

stomach, accompanied only by water and tea.

modern systems for selectional preferences therefore specify the relation be-

tween a predicate and its possible arguments with soft constraints of some kind.

selectional association
one of the most in   uential has been the selectional association model of resnik
(1993). resnik de   nes the idea of selectional preference strength as the general
amount of information that a predicate tells us about the semantic class of its argu-
ments. for example, the verb eat tells us a lot about the semantic class of its direct
objects, since they tend to be edible. the verb be, by contrast, tells us less about
its direct objects. the selectional preference strength can be de   ned by the differ-
ence in information between two distributions: the distribution of expected semantic

selectional
preference
strength

relative id178
kl divergence

18.7

    selectional restrictions

371

classes p(c) (how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb p(c|v) (how likely is
it that the direct object of the speci   c verb v will fall into semantic class c). the
greater the difference between these distributions, the more information the verb is
giving us about possible objects. the difference between these two distributions can
be quanti   ed by relative id178, or the id181 (kullback and
leibler, 1951). the kullback-leibler or kl divergence d(p||q) expresses the dif-
ference between two id203 distributions p and q (we   ll return to this when we
discuss distributional models of meaning in chapter 6).

d(p||q) = (cid:88)x

p(x)log

p(x)
q(x)

(18.38)

the selectional preference sr(v) uses the kl divergence to express how much in-
formation, in bits, the verb v expresses about the possible semantic class of its argu-
ment.

sr(v) = d(p(c|v)||p(c))
p(c|v)log

= (cid:88)c

p(c|v)
p(c)

(18.39)

selectional
association

resnik then de   nes the selectional association of a particular class and verb as the
relative contribution of that class to the general selectional preference of the verb:

ar(v,c) =

1

sr(v)

p(c|v)log

p(c|v)
p(c)

(18.40)

the selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the id138 concepts containing the
word. the following table from resnik (1996) shows some sample high and low
selectional associations for verbs and some id138 semantic classes of their direct
objects.

verb
read
write
see

direct object
semantic class assoc
writing
writing
entity

6.80
7.26
5.79

direct object
semantic class assoc
activity
commerce
method

-.20
0
-0.01

selectional preference via id155
an alternative to using selectional association between a verb and the id138 class
of its arguments, is to simply use the id155 of an argument word
given a predicate verb. this simple model of selectional preferences can be used
to directly model the strength of association of one verb (predicate) with one noun
(argument).

the id155 model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. the id155 of an

372 chapter 18

    id14

argument noun given a verb for a particular relation p(n|v,r) can then be used as a
selectional preference metric for that pair of words (brockmann and lapata, 2003):

p(n|v,r) =(cid:40) c(n,v,r)

c(v,r)

if c(n,v,r) > 0

0 otherwise

the inverse id203 p(v|n,r) was found to have better performance in some cases
(brockmann and lapata, 2003):

p(v|n,r) =(cid:40) c(n,v,r)

c(n,r)

if c(n,v,r) > 0

0 otherwise

pseudowords

in cases where it   s not possible to get large amounts of parsed data, another option,
at least for direct objects, is to get the counts from simple part-of-speech based
approximations. for example pairs can be extracted using the pattern    v det n   ,
where v is any form of the verb, det is the   a     and n is the singular or plural
form of the noun (keller and lapata, 2003).

an even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument logcount(v,n,r) instead of id155;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (brockmann and lapata, 2003).

evaluating selectional preferences
one way to evaluate models of selectional preferences is to use pseudowords (gale
et al. 1992c, sch  utze 1992a). a pseudoword is an arti   cial word created by concate-
nating a test word in some context (say banana) with a confounder word (say door)
to create banana-door). the task of the system is to identify which of the two words
is the original word. to evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. for each verb token (say drive) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor, the noun with the
frequency closest to the original (say house), to make car/house). we then use the
selectional preference model to choose which of car and house are more preferred
objects of drive, and compute how often the model chooses the correct original ob-
ject (e.g., (car) (chambers and jurafsky, 2010).

another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. this is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. a selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (keller and lapata, 2003).

18.8 primitive decomposition of predicates

one way of thinking about the semantic roles we have discussed through the chapter
is that they help us de   ne the roles that arguments play in a decompositional way,
based on    nite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.) this idea of decomposing meaning into sets of primitive semantics
elements or features, called primitive decomposition or componential analysis,

componential
analysis

18.8

    primitive decomposition of predicates

373

has been taken even further, and focused particularly on predicates.

consider these examples of the verb kill:

(18.41) jim killed his philodendron.
(18.42) jim did something to cause his philodendron to become not alive.
there is a truth-conditional (   propositional semantics   ) perspective from which these
two sentences have the same meaning. assuming this equivalence, we could repre-
sent the meaning of kill as:
(18.43) kill(x,y)     cause(x, become(not(alive(y))))
thus using semantic primitives like do, cause, become not, and alive.

indeed, one such set of potential semantic primitives has been used to account for
some of the verbal alternations discussed in section 18.2 (lakoff 1965, dowty 1979).
consider the following examples.
(18.44) john opened the door.     cause(john, become(open(door)))
(18.45) the door opened.     become(open(door))
(18.46) the door is open.     open(door)

the decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. the differences among the meanings
of these examples arises from the combination of this single predicate with the prim-
itives cause and become.

while this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open. more radical approaches choose to
break down these predicates as well. one such approach to verbal predicate de-
composition that played a role in early natural language understanding systems is
conceptual dependency (cd), a set of ten primitive predicates, shown in fig. 18.8.

primitive
atrans

ptrans
mtrans

mbuild
propel
move
ingest
expel
speak
attend

de   nition
the abstract transfer of possession or control from one entity to
another
the physical transfer of an object from one location to another
the transfer of mental concepts between entities or within an
entity
the creation of new information within an entity
the application of physical force to move an object
the integral movement of a body part by an animal
the taking in of a substance by an animal
the expulsion of something from an animal
the action of producing a sound
the action of focusing a sense organ

figure 18.8 a set of conceptual dependency primitives.

below is an example sentence along with its cd representation. the verb brought
is translated into the two primitives atrans and ptrans to indicate that the waiter
both physically conveyed the check to mary and passed control of it to her. note
that cd also associates a    xed set of thematic roles with each primitive to represent
the various participants in the action.
(18.47) the waiter brought mary the check.

conceptual
dependency

374 chapter 18

    id14

   x,y atrans(x)    actor(x,waiter)    ob ject(x,check)    to(x,mary)
   ptrans(y)    actor(y,waiter)    ob ject(y,check)    to(y,mary)

18.9 summary

described by the predicate.

    semantic roles are abstract models of the role an argument plays in the event
    thematic roles are a model of semantic roles based on a single    nite list of
roles. other semantic role models include per-verb semantic role lists and
proto-agent/proto-patient, both of which are implemented in propbank,
and per-frame role lists, implemented in framenet.

    id14 is the task of assigning semantic role labels to the con-
stituents of a sentence. the task is generally treated as a supervised machine
learning task, with models trained on propbank or framenet. algorithms
generally start by parsing a sentence and then automatically tag each parse
tree node with a semantic role.

    semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. selectional
preference models (like selectional association or simple conditional proba-
bility) allow a weight or id203 to be assigned to the association between
a predicate and an argument word or class.

bibliographical and historical notes

although the idea of semantic roles dates back to p  an. ini, they were re-introduced
into modern linguistics by gruber (1965), fillmore (1966) and fillmore (1968)).
fillmore, interestingly, had become interested in argument structure by studying
lucien tesni`ere   s groundbreaking   el  ements de syntaxe structurale (tesni`ere, 1959)
in which the term    dependency    was introduced and the foundations were laid for
dependency grammar. following tesni`ere   s terminology, fillmore    rst referred to
argument roles as actants (fillmore, 1966) but quickly switched to the term case,
(see fillmore (2003)) and proposed a universal list of semantic roles or cases (agent,
patient, instrument, etc.), that could be taken on by the arguments of predicates.
verbs would be listed in the lexicon with their case frame, the list of obligatory (or
optional) case arguments.

the idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speci   ed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine trans-
lation (wilks, 1973), question-answering (hendrix et al., 1973), spoken-language
understanding (nash-webber, 1975), and dialogue systems (bobrow et al., 1977).
general-purpose semantic role labelers were developed. the earliest ones (sim-
mons, 1973)    rst parsed a sentence by means of an atn (augmented transition

bibliographical and historical notes

375

network) parser. each verb then had a set of rules specifying how the parse should
be mapped to semantic roles. these rules mainly made reference to grammatical
functions (subject, object, complement of speci   c prepositions) but also checked
constituent internal features such as the animacy of head nouns. later systems as-
signed roles from pre-built parse trees, again by using dictionaries with verb-speci   c
case frames (levin 1977, marcus 1980).

by 1977 case representation was widely used and taught in ai and nlp courses,
and was described as a standard of natural language understanding in the    rst edition
of winston   s (1977) textbook arti   cial intelligence.

in the 1980s fillmore proposed his model of frame semantics, later describing

the intuition as follows:

   the idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names   frames, schemas, scenarios, scripts, cultural
narratives, memes   and the words in our language are understood with
such frames as their presupposed background.    (fillmore, 2012, p. 712)

the word frame seemed to be in the air for a suite of related notions proposed at
about the same time by minsky (1974), hymes (1974), and goffman (1974), as
well as related notions with other names like scripts (schank and abelson, 1975)
and schemata (bobrow and norman, 1975) (see tannen (1979) for a comparison).
fillmore was also in   uenced by the semantic    eld theorists and by a visit to the yale
ai lab where he took notice of the lists of slots and    llers used by early information
extraction systems like dejong (1982) and schank and abelson (1977). in the 1990s
fillmore drew on these insights to begin the framenet corpus annotation project.

at the same time, beth levin drew on her early case frame dictionaries (levin,
1977) to develop her book which summarized sets of verb classes de   ned by shared
argument realizations (levin, 1993). the verbnet project built on this work (kipper
et al., 2000), leading soon afterwards to the propbank semantic-role-labeled corpus
created by martha palmer and colleagues (palmer et al., 2005).

the combination of rich linguistic annotation and corpus-based approach in-
stantiated in framenet and propbank led to a revival of automatic approaches to
id14,    rst on framenet (gildea and jurafsky, 2000) and then on
propbank data (gildea and palmer, 2002, inter alia). the problem    rst addressed in
the 1970s by hand-written rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. many popular features
used for role labeling are de   ned in gildea and jurafsky (2002), surdeanu et al.
(2003), xue and palmer (2004), pradhan et al. (2005), che et al. (2009), and zhao
et al. (2009). the use of dependency rather than constituency parses was introduced
in the conll-2008 shared task (surdeanu et al., 2008b). for surveys see palmer
et al. (2010) and m`arquez et al. (2008).

the use of neural approachess to id14 was pioneered by col-
lobert et al. (2011), who applied a crf on top of a convolutional net. early work
like foland, jr. and martin (2015) focused on using dependency features. later
work eschewed syntactic features altogether; (zhou and xu, 2015) introduced the
use of a stacked (6-8 layer) bi-lstm architecture, and (he et al., 2017) showed
how to augment the bi-lstm architecture with id199 and also replace
the crf with a* decoding that make it possible to apply a wide variety of global
constraints in srl decoding.

most id14 schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of nombank) predicate.

376 chapter 18

    id14

implicit
argument

isrl

however, in many cases, a verbal or nominal predicate may have an implicit argu-
ment: one that appears only in a contextual sentence, or perhaps not at all and must
be inferred. in the two sentences this house has a new owner. the sale was    nalized
10 days ago. the sale in the second sentence has no arg1, but a reasonable reader
would infer that the arg1 should be the house mentioned in the prior sentence. find-
ing these arguments, implicit argument detection (sometimes shortened as isrl)
was introduced by gerber and chai (2010) and ruppenhofer et al. (2010). see do
et al. (2017) for more recent neural models.

to avoid the need for huge labeled training sets, unsupervised approaches for
id14 attempt to induce the set of semantic roles by id91 over
arguments. the task was pioneered by riloff and schmelzenbach (1998) and swier
and stevenson (2004); see grenager and manning (2006), titov and klementiev
(2012), lang and lapata (2014), woodsend and lapata (2015), and titov and khod-
dam (2014).

recent innovations in frame labeling include connotation frames, which mark
richer information about the argument of predicates. connotation frames mark the
sentiment of the writer or reader toward the arguements (for example using the verb
survive in he survived a bombing expresses the writer   s sympathy toward the subject
he and negative sentiment toward the bombing. connotation frames also mark effect
(something bad happened to x), value: (x is valuable), and mental state: (x is dis-
tressed by the event) (rashkin et al. 2016, rashkin et al. 2017). connotation frames
can also mark the power differential between the arguments (using the verb implore
means that the theme argument has greater power than the agent), and the agency
of each argument (waited is low agency). fig. 18.9 shows a visualization from sap
et al. (2017).

figure 18.9 the connotation frames of sap et al. (2017), showing that the verb implore
implies the agent has lower power than the theme (in contrast, say, with a verb like demanded,
and showing the low level of agency of the subject of waited. figure from sap et al. (2017).

selectional preference has been widely studied beyond the selectional associa-
tion models of resnik (1993) and resnik (1996). methods have included cluster-
ing (rooth et al., 1999), discriminative learning (bergsma et al., 2008), and topic
models (s  eaghdha 2010, ritter et al. 2010), and constraints can be expressed at the
level of words or classes (agirre and martinez, 2001). selectional preferences have
also been successfully integrated into id14 (erk 2007, zapirain
et al. 2013, do et al. 2017).

agentthemepower(ag < th)verbimplorehe implored the tribunal to show mercy.the princess waited for her prince.agentthemeagency(ag) = -verbwaitfigure2:theformalnotationoftheconnotationframesofpowerandagency.the   rstexampleshowstherelativepowerdifferentialimpliedbytheverb   implored   ,i.e.,theagent(   he   )isinapositionoflesspowerthanthetheme(   thetri-bunal   ).incontrast,   hedemandedthetribunalshowmercy   impliesthattheagenthasauthorityoverthetheme.thesecondexampleshowsthelowlevelofagencyimpliedbytheverb   waited   .interactivedemowebsiteofour   ndings(seefig-ure5intheappendixforascreenshot).2further-more,aswillbeseeninsection4.1,connotationframesoffernewinsightsthatcomplementandde-viatefromthewell-knownbechdeltest(bechdel,1986).inparticular,we   ndthathigh-agencywomenthroughthelensofconnotationframesarerareinmodern   lms.itis,inpart,becausesomemovies(e.g.,snowwhite)accidentallypassthebechdeltestandalsobecauseevenmovieswithstrongfemalecharactersarenotentirelyfreefromthedeeplyingrainedbiasesinsocialnorms.2connotationframesofpowerandagencywecreatetwonewconnotationrelations,powerandagency(examplesinfigure3),asanexpan-sionoftheexistingconnotationframelexicons.3threeamtcrowdworkersannotatedtheverbswithplaceholderstoavoidgenderbiasinthecon-text(e.g.,xrescuedy;anexampletaskisshownintheappendixinfigure7).wede   netheanno-tatedconstructsasfollows:powerdifferentialsmanyverbsimplytheau-thoritylevelsoftheagentandthemerelativeto2http://homes.cs.washington.edu/  msap/movie-bias/.3thelexiconsandademoareavailableathttp://homes.cs.washington.edu/  msap/movie-bias/.power(ag<th)power(ag>th)agency(ag)= agency(ag)=+figure3:sampleverbsintheconnotationframeswithhighannotatoragreement.sizeisindicativeofverbfrequencyinourcorpus(bigger=morefrequent),colordifferencesareonlyforlegibility.oneanother.forexample,iftheagent   dom-inates   thetheme(denotedaspower(ag>th)),thentheagentisimpliedtohavealevelofcontroloverthetheme.alternatively,iftheagent   hon-ors   thetheme(denotedaspower(ag<th)),thewriterimpliesthatthethemeismoreimportantorauthoritative.weusedamtid104tola-bel1700transitiveverbsforpowerdifferentials.withthreeannotatorsperverb,theinter-annotatoragreementis0.34(krippendorff   s   ).agencytheagencyattributedtotheagentoftheverbdenoteswhethertheactionbeingdescribedimpliesthattheagentispowerful,decisive,andcapableofpushingforwardtheirownstoryline.forexample,apersonwhoisdescribedas   ex-periencing   thingsdoesnotseemasactiveandde-cisiveassomeonewhoisdescribedas   determin-ing   things.amtworkerslabeled2000transi-tiveverbsforimplyinghigh/moderate/lowagency(inter-annotatoragreementof0.27).wedenotehighagencyasagency(ag)=+,andlowagencyasagency(ag)= .pairwiseagreementsonahardconstraintare56%and51%forpowerandagency,respec-tively.despitethis,agreementsreach96%and94%whenmoderatelabelsarecountedasagree-ingwitheitherhighorlowlabels,showingthatan-notatorsrarelystronglydisagreewithoneanother.somecontributingfactorsinthelowerkascoresincludethesubtletyofchoosingbetweenneutralexercises

exercises

377

378 chapter 19

    lexicons for sentiment, affect, and connotation

chapter

19 lexicons for sentiment, affect,

and connotation

   [w]e write, not with the    ngers, but with the whole person. the nerve which
controls the pen winds itself about every    bre of our being, threads the heart,
pierces the liver.   

virginia woolf, orlando

   she runs the gamut of emotions from a to b.   

dorothy parker, reviewing hepburn   s performance in little women

affective

subjectivity

in this chapter we turn to tools for interpreting affective meaning, extending our
study of id31 in chapter 4. we use the word    affective   , following
the tradition in affective computing (picard, 1995) to mean emotion, sentiment, per-
sonality, mood, and attitudes. affective meaning is closely related to subjectivity,
the study of a speaker or writer   s evaluations, opinions, emotions, and speculations
(wiebe et al., 1999).

how should affective meaning be de   ned? one in   uential typology of affec-
tive states comes from scherer (2000), who de   nes each class of affective states by
factors like its cognition realization and time course:

emotion: relatively brief episode of response to the evaluation of an external

or internal event as being of major signi   cance.
(angry, sad, joyful, fearful, ashamed, proud, elated, desperate)

mood: diffuse affect state, most pronounced as change in subjective feeling, of

low intensity but relatively long duration, often without apparent cause.
(cheerful, gloomy, irritable, listless, depressed, buoyant)

interpersonal stance: affective stance taken toward another person in a spe-

ci   c interaction, colouring the interpersonal exchange in that situation.
(distant, cold, warm, supportive, contemptuous, friendly)

attitude: relatively enduring, affectively colored beliefs, preferences, and pre-

dispositions towards objects or persons.
(liking, loving, hating, valuing, desiring)

personality traits: emotionally laden, stable personality dispositions and be-

havior tendencies, typical for a person.
(nervous, anxious, reckless, morose, hostile, jealous)

figure 19.1 the scherer typology of affective states (scherer, 2000).

we can design extractors for each of these kinds of affective states. chapter 4
already introduced id31, the task of extracting the positive or negative

19.1

    defining emotion

379

orientation that a writer expresses in a text. this corresponds in scherer   s typology
to the extraction of attitudes:    guring out what people like or dislike, from affect-
rish texts like consumer reviews of books or movies, newspaper editorials, or public
sentiment in blogs or tweets.

detecting emotion and moods is useful for detecting whether a student is con-
fused, engaged, or certain when interacting with a tutorial system, whether a caller
to a help line is frustrated, whether someone   s blog posts or tweets indicated depres-
sion. detecting emotions like fear in novels, for example, could help us trace what
groups or situations are feared and how that changes over time.

detecting different interpersonal stances can be useful when extracting infor-
mation from human-human conversations. the goal here is to detect stances like
friendliness or awkwardness in interviews or friendly conversations, or even to de-
tect    irtation in dating. for the task of automatically summarizing meetings, we   d
like to be able to automatically understand the social relations between people, who
is friendly or antagonistic to whom. a related task is    nding parts of a conversation
where people are especially excited or engaged, conversational hot spots that can
help a summarizer focus on the correct region.

detecting the personality of a user   such as whether the user is an extrovert
or the extent to which they are open to experience    can help improve conversa-
tional agents, which seem to work better if they match users    personality expecta-
tions (mairesse and walker, 2008).

affect is important for generation as well as recognition; synthesizing affect
is important for conversational agents in various domains, including literacy tutors
such as children   s storybooks, or computer games.

in chapter 4 we introduced the use of naive bayes classi   cation to classify a
document   s sentiment. various classi   ers have been successfully applied to many of
these tasks, using all the words in the training set as input to a classi   er which then
determines the affect status of the text.

in this chapter we focus on an alternative model, in which instead of using every
word as a feature, we focus only on certain words, ones that carry particularly strong
cues to affect or sentiment. we call these lists of words affective lexicons or senti-
ment lexicons. these lexicons presuppose a fact about semantics: that words have
affective meanings or connotations. the word connotation has different meanings
in different    elds, but here we use it to mean the aspects of a word   s meaning that
are related to a writer or reader   s emotions, sentiment, opinions, or evaluations. in
addition to their ability to help determine the affective status of a text, connotation
lexicons can be useful features for other kinds of affective tasks, and for computa-
tional social science analysis.

in the next sections we introduce basic theories of emotion, show how sentiment
lexicons can be viewed as a special case of emotion lexicons, and then summarize
some publicly available lexicons. we then introduce three ways for building new
lexicons: human labeling, semi-supervised, and supervised.

finally, we turn to some other kinds of affective meaning, including interper-

sonal stance, personality, and connotation frames.

connotations

19.1 de   ning emotion

emotion

one of the most important affective classes is emotion, which scherer (2000) de   nes
as a    relatively brief episode of response to the evaluation of an external or internal

380 chapter 19

    lexicons for sentiment, affect, and connotation

event as being of major signi   cance   .

detecting emotion has the potential to improve a number of language processing
tasks. automatically detecting emotions in reviews or customer responses (anger,
dissatisfaction, trust) could help businesses recognize speci   c problem areas or ones
that are going well. emotion recognition could help id71 like tutoring
systems detect that a student was unhappy, bored, hesitant, con   dent, and so on.
emotion can play a role in medical informatics tasks like detecting depression or
suicidal intent. detecting emotions expressed toward characters in novels might
play a role in understanding how different social groups were viewed by society at
different times.

there are two widely-held families of theories of emotion. in one family, emo-
tions are viewed as    xed atomic units, limited in number, and from which others
are generated, often called basic emotions (tomkins 1962, plutchik 1962). per-
haps most well-known of this family of theories are the 6 emotions proposed by
(ekman, 1999) as a set of emotions that is likely to be universally present in all
cultures: surprise, happiness, anger, fear, disgust, sadness. another atomic theory
is the (plutchik, 1980) wheel of emotion, consisting of 8 basic emotions in four
opposing pairs: joy   sadness, anger   fear, trust   disgust, and anticipation   surprise,
together with the emotions derived from them, shown in fig. 19.2.

basic emotions

figure 19.2 plutchik wheel of emotion.

the second class of emotion theories views emotion as a space in 2 or 3 di-
mensions (russell, 1980). most models include the two dimensions valence and
arousal, and many add a third, dominance. these can be de   ned as:

valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
in the next sections we   ll see lexicons for both kinds of theories of emotion.

19.2

    available sentiment and affect lexicons

381

sentiment can be viewed as a special case of this second view of emotions as
points in space. in particular, the valence dimension, measuring how pleasant or
unpleasant a word is, is often used directly as a measure of sentiment.

19.2 available sentiment and affect lexicons

general
inquirer

a wide variety of affect lexicons have been created and released. the most basic
lexicons label words along one dimension of semantic variability, generally called
   sentiment    or    valence   .

in the simplest lexicons this dimension is represented in a binary fashion, with
a wordlist for positive words and a wordlist for negative words. the oldest is the
general inquirer (stone et al., 1966), which drew on early work in the cognition
psychology of word meaning (osgood et al., 1957) and on work in content analysis.
the general inquirer has a lexicon of 1915 positive words an done of 2291 negative
words (and also includes other lexicons discussed below).

the mpqa subjectivity lexicon (wilson et al., 2005) has 2718 positive and
4912 negative words drawn from prior lexicons plus a bootstrapped list of subjec-
tive words and phrases (riloff and wiebe, 2003) each entry in the lexicon is hand-
labeled for sentiment and also labeled for reliability (strongly subjective or weakly
subjective).

the polarity lexicon of hu and liu (2004b) gives 2006 positive and 4783 nega-
tive words, drawn from product reviews, labeled using a id64 method from
id138.

positive

admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-
tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,
rejoice, relief, respect, satisfactorily, sensational, super, terri   c, thank, vivid, wise, won-
derful, zest

negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,
defective, disappointment, embarrass, fake, fear,    lthy, fool, guilt, hate, idiot, in   ict, lazy,
miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,
vile, wicked

figure 19.3 some samples of words with consistent sentiment across three sentiment lexicons: the general
inquirer (stone et al., 1966), the mpqa subjectivity lexicon (wilson et al., 2005), and the polarity lexicon of
hu and liu (2004b).

emolex

slightly more general than these sentiment lexicons are lexicons that assign each
word a value on all three emotional dimension the lexicon of warriner et al. (2013)
assigns valence, arousal, and dominance scores to 14,000 words. some examples
are shown in fig. 19.4

the nrc word-emotion association lexicon, also called emolex (moham-
mad and turney, 2013), uses the plutchik (1980) 8 basic emotions de   ned above.
the lexicon includes around 14,000 words including words from prior lexicons as
well as frequent nouns, verbs, adverbs and adjectives. values from the lexicon for
some sample words:

382 chapter 19

    lexicons for sentiment, affect, and connotation

valence

arousal

dominance

8.53
8.47
5.7
5.53
1.4

vacation
7.74
happy
7.74
whistle
5.33
conscious
5.29
torture
2.14
figure 19.4 samples of the values of selected words on the three emotional dimensions
from warriner et al. (2013).

self
incredible
skillet
concur
earthquake

rampage
tornado
zucchini
dressy
dull

7.56
7.45
4.18
4.15
1.67

n
o
i
t
a
p
i
c
i
t
n
a

t
s
u
g
s
i
d

r
e
g
n

s
s
e
n
d
a
s

e
s
i
r
p
r
u
s

e
v
i
t
i
s
o
p

e
v
i
t
a
g
e
n

t
s
u
r
t

r
a
e
f

y
o
j

word a
0 1 0 0 1 0 1 1 1 0
reward
worry
0 1 0 1 0 1 0 0 0 1
tenderness 0 0 0 0 1 0 0 0 1 0
sweetheart 0 1 0 0 1 1 0 1 1 0
suddenly
0 0 0 0 0 0 1 0 0 0
0 1 0 0 0 1 1 0 0 0
thirst
garbage
0 0 1 0 0 0 0 0 0 1

concrete
abstract

liwc

there are various other hand-built affective lexicons. the general inquirer in-
cludes additional lexicons for dimensions like strong vs. weak, active vs. passive,
overstated vs. understated, as well as lexicons for categories like pleasure, pain,
virtue, vice, motivation, and cognitive orientation.

another useful feature for various tasks is the distinction between concrete
words like banana or bathrobe and abstract words like belief and although. the
lexicon in (brysbaert et al., 2014) used id104 to assign a rating from 1 to 5
of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5,
belief 1.19, although 1.07, and in between words like brisk a 2.5.

liwc, linguistic inquiry and word count, is another set of 73 lexicons con-
taining over 2300 words (pennebaker et al., 2007), designed to capture aspects of
lexical meaning relevant for social psychological tasks. in addition to sentiment-
related lexicons like ones for negative emotion (bad, weird, hate, problem, tough)
and positive emotion (love, nice, sweet), liwc includes lexicons for categories like
anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown
in fig. 19.5.

19.3 creating affect lexicons by human labeling

id104

the earliest method used to build affect lexicons, and still in common use, is to have
humans label each word. this is now most commonly done via id104:
breaking the task into small pieces and distributing them to a large number of anno-
tators. let   s take a look at some of the methodological choices for two crowdsourced
emotion lexicons.

the nrc word-emotion association lexicon (emolex) (mohammad and tur-
ney, 2013), labeled emotions in two steps. in order to ensure that the annotators
were judging the correct sense of the word, they    rst answered a multiple-choice

19.3

    creating affect lexicons by human labeling

383

negative
emotion
anger*
bore*
cry
despair*
fail*
fear
griev*
hate*
panic*
suffers
terrify
violent*

positive
emotion
appreciat*
comfort*
great
happy
interest
joy*
perfect*
please*
safe*
terri   c
value
wow*
figure 19.5 samples from 5 of the 73 lexical categories in liwc (pennebaker et al., 2007).
the * means the previous letters are a word pre   x and all words with that pre   x are included
in the category.

inhibition
avoid*
careful*
hesitat*
limit*
oppos*
prevent*
reluctan*
safe*
stop
stubborn*
wait
wary

family
brother*
cousin*
daughter*
family
father*
grandf*
grandm*
husband
mom
mother
niece*
wife

insight
aware*
believe
decid*
feel
   gur*
know
knew
means
notice*
recogni*
sense
think

negate
aren   t
cannot
didn   t
neither
never
no
nobod*
none
nor
nothing
nowhere
without

synonym question that primed the correct sense of the word (without requiring the
annotator to read a potentially confusing sense de   nition). these were created au-
tomatically using the headwords associated with the thesaurus category of the sense
in question in the macquarie dictionary and the headwords of 3 random distractor
categories. an example:

which word is closest in meaning (most related) to startle?

    automobile
    shake
    honesty
    entertain

for each word (e.g. startle), the annotator was then asked to rate how associated
that word is with each of the 8 emotions (joy, fear, anger, etc.). the associations
were rated on a scale of not, weakly, moderately, and strongly associated. outlier
ratings were removed, and then each term was assigned the class chosen by the ma-
jority of the annotators, with ties broken by choosing the stronger intensity, and then
the 4 levels were mapped into a binary label for each word (no and weak mapped to
0, moderate and strong mapped to 1).

for the warriner et al. (2013) lexicon of valence, arousal, and dominance, crowd-
workers marked each word with a value from 1-9 on each of the dimensions, with
the scale de   ned for them as follows:

    valence (the pleasantness of the stimulus)

9: happy, pleased, satis   ed, contented, hopeful
1: unhappy, annoyed, unsatis   ed, melancholic, despaired, or bored

    arousal (the intensity of emotion provoked by the stimulus)

9: stimulated, excited, frenzied, jittery, wide-awake, or aroused
1: relaxed, calm, sluggish, dull, sleepy, or unaroused;

    dominance (the degree of control exerted by the stimulus)

9: in control, in   uential, important, dominant, autonomous, or controlling
1: controlled, in   uenced, cared-for, awed, submissive, or guided

384 chapter 19

    lexicons for sentiment, affect, and connotation
19.4 semi-supervised induction of affect lexicons

another common way to learn sentiment lexicons is to start from a set of seed words
that de   ne two poles of a semantic axis (words like good or bad), and then    nd ways
to label each word w by its similarity to the two seed sets. here we summarize two
families of seed-based semi-supervised lexicon induction algorithms, axis-based and
graph-based.

19.4.1 semantic axis methods

one of the most well-known lexicon induction methods, the turney and littman
(2003) algorithm, is given seed words like good or bad, and then for each word w to
be labeled, measures both how similar it is to good and how different it is from bad.
here we describe a slight extension of the algorithm due to an et al. (2018), which
is based on computing a semantic axis.

in the    rst step, we choose seed words by hand. because the sentiment or affect
of a word is different in different contexts, it   s common to choose different seed
words for different genres, and most algorithms are quite sensitive to the choice of
seeds. for example, for inducing sentiment lexicons, hamilton et al. (2016a) de   nes
one set of seed words for general sentiment analyis, a different set for twitter, and
yet another set for learning a lexicon for sentiment in    nancial text:

domain
general

twitter

finance

loved,

positive seeds
good, lovely, excellent, fortunate, pleas-
ant, delightful, perfect,
love,
happy
love,
loves, awesome, nice,
amazing, best, fantastic, correct, happy
successful, excellent, pro   t, bene   cial,
improving,
improved, success, gains,
positive

loved,

negative seeds
bad, horrible, poor, unfortunate, un-
pleasant, disgusting, evil, hated, hate,
unhappy
hate, hated, hates, terrible, nasty, awful,
worst, horrible, wrong, sad
negligent, loss, volatile, wrong, losses,
damages, bad, litigation, failure, down,
negative

in the second step, we compute embeddings for each of the pole words. these
embeddings can be off-the-shelf id97 embeddings, or can be computed directly
on a speci   c corpus (for example using a    nancial corpus if a    nance lexicon is the
goal), or we can    ne-tune off-the-shelf embeddings to a corpus. fine-tuning is espe-
cially important if we have a very speci   c genre of text but don   t have enough data
to train good embeddings. in    ne-tuning, we begin with off-the-shelf embeddings
like id97, and continue training them on the small target corpus.

once we have embeddings for each pole word, we we create an embedding that
represents each pole by taking the centroid of the embeddings of each of the seed
words; recall that the centroid is the multidimensional version of the mean. given
a set of embeddings for the positive seed words s+ = {e(w+
n )},
and embeddings for the negative seed words s    = {e(w   1 ),e(w   2 ), ...,e(w   m)}, the

2 ), ...,e(w+

1 ),e(w+

(19.2)

(19.3)

19.4

    semi-supervised induction of affect lexicons

385

pole centroids are:

e(w+
i )

e(w   i )

(19.1)

v+ =

v    =

1
n

1
n

n(cid:88)1
m(cid:88)1

vaxis = v+     v   

the semantic axis de   ned by the poles is computed just by subtracting the two vec-
tors:

vaxis, the semantic axis, is a vector in the direction of sentiment. finally, we compute
how close each word w is to this sentiment axis, by taking the cosine between w   s
embedding and the axis vector. a higher cosine means that w is more aligned with
s+ than s   .

score(w) = (cid:0)cos(e(w), vaxis(cid:1)
e(w)   vaxis
(cid:107)e(w)(cid:107)(cid:107)vaxis(cid:107)

=

if a dictionary of words with sentiment scores is suf   cient, we   re done! or if we
need to group words into a positive and a negative lexicon, we can use a threshold
or other method to give us discrete lexicons.

19.4.2 label propagation
an alternative family of methods de   nes lexicons by propagating sentiment labels
on graphs, an idea suggested in early work by hatzivassiloglou and mckeown
(1997). we   ll describe the simple sentprop (sentiment propagation) algorithm of
hamilton et al. (2016a), which has four steps:

1. de   ne a graph: given id27s, build a weighted lexical graph
by connecting each word with its k nearest neighbors (according to cosine-
similarity). the weights of the edge between words wi and w j are set as:

ei, j = arccos(cid:18)   

wi(cid:62)wj

(cid:107)wi(cid:107)(cid:107)wj(cid:107)(cid:19) .

(19.4)

2. de   ne a seed set: by hand, choose positive and negative seed words.
3. propagate polarities from the seed set: now we perform a random walk on
this graph, starting at the seed set. in a random walk, we start at a node and
then choose a node to move to with id203 proportional to the edge prob-
ability. a word   s polarity score for a seed set is proportional to the id203
of a random walk from the seed set landing on that word, (fig. 19.6).

4. create word scores: we walk from both positive and negative seed sets,
resulting in positive (score+(wi)) and negative (score   (wi)) label scores. we
then combine these values into a positive-polarity score as:

score+(wi) =

score+(wi)

score+(wi) + score   (wi)

(19.5)

it   s often helpful to standardize the scores to have zero mean and unit variance
within a corpus.

386 chapter 19

    lexicons for sentiment, affect, and connotation

5. assign con   dence to each score: because sentiment scores are in   uenced by
the seed set, we   d like to know how much the score of a word would change if
a different seed set is used. we can use bootstrap-sampling to get con   dence
regions, by computing the propagation b times over random subsets of the
positive and negative seed sets (for example using b = 50 and choosing 7 of
the 10 seed words each time). the standard deviation of the bootstrap-sampled
polarity scores gives a con   dence measure.

(a)

(b)

figure 19.6
polarity scores (shown here as colors green or red) based on the frequency of random walk visits.

intuition of the sentprop algorithm. (a) run id93 from the seed words. (b) assign

19.4.3 other methods
the core of semisupervised algorithms is the metric for measuring similarity with
the seed words. the turney and littman (2003) and hamilton et al. (2016a) ap-
proaches above used embedding cosine as the distance metric: words were labeled
as positive basically if their embeddings had high cosines with positive seeds and
low cosines with negative seeds. other methods have chosen other kinds of distance
metrics besides embedding cosine.

for example the hatzivassiloglou and mckeown (1997) algorithm uses syntactic
cues; two adjectives are considered similar if they were frequently conjoined by and
and rarely conjoined by but. this is based on the intuition that adjectives conjoined
by the words and tend to have the same polarity; positive adjectives are generally
coordinated with positive, negative with negative:

fair and legitimate, corrupt and brutal

but less often positive adjectives coordinated with negative:

*fair and brutal, *corrupt and legitimate

by contrast, adjectives conjoined by but are likely to be of opposite polarity:

fair but brutal

another cue to opposite polarity comes from morphological negation (un-, im-,
-less). adjectives with the same root but differing in a morphological negative (ad-
equate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.

yet another method for    nding words that have a similar polarity to seed words is
to make use of a thesaurus like id138 (kim and hovy 2004, hu and liu 2004b).
a word   s synonyms presumably share its polarity while a word   s antonyms probably
have the opposite polarity. after a seed lexicon is built, each lexicon is updated as
follows, possibly iterated.
lex+: add synonyms of positive words (well) and antonyms (like    ne) of negative

words

idolizeloveadoreappreciatelike   nddislikeseenoticedisapproveabhorhateloathedespiseuncoveridolizeloveadoreappreciatelike   nddislikeseenoticedisapproveabhorhateloathedespiseuncover19.5

    supervised learning of word sentiment

387

lex   : add synonyms of negative words (awful) and antonyms ( like evil) of positive

words

sentiid138

an extension of this algorithm assigns polarity to id138 senses, called senti-
id138 (baccianella et al., 2010). fig. 19.7 shows some examples.

   deserving of esteem   

   agreeable or pleasing   

   may be computed or estimated   

synset
good#6
respectable#2 honorable#4 good#4 estimable#2
estimable#3 computable#1
sting#1 burn#4 bite#2
acute#6
acute#4
acute#1
figure 19.7 examples from sentiid138 3.0 (baccianella et al., 2010). note the differences between senses
of homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive
(acute#6), negative (acute#1), or neutral (acute #4)

   of critical importance and consequence   
   of an angle; less than 90 degrees   
   having or experiencing a rapid onset and short but severe course   

pos neg obj
0
1
0
0.75
0
0
0.875 .125
0
0.625 0.125 .250
0
0

   cause a sharp or stinging pain   

0
0.25
1

0
0.5

1
0.5

.

in this algorithm, polarity is assigned to entire synsets rather than words. a
positive lexicon is built from all the synsets associated with 7 positive words, and a
negative lexicon from synsets associated with 7 negative words. a classi   er is then
trained from this data to take a id138 gloss and decide if the sense being de   ned
is positive, negative or neutral. a further step (involving a random-walk algorithm)
assigns a score to each id138 synset for its degree of positivity, negativity, and
neutrality.

in summary, semisupervised algorithms use a human-de   ned set of seed words
for the two poles of a dimension, and use similarity metrics like embedding cosine,
coordination, morphology, or thesaurus structure to score words by how similar they
are to the positive seeds and how dissimilar to the negative seeds.

19.5 supervised learning of word sentiment

semi-supervised methods require only minimal human supervision (in the form of
seed sets). but sometimes a supervision signal exists in the world and can be made
use of. one such signal is the scores associated with online reviews.

the web contains an enormous number of online reviews for restaurants, movies,
books, or other products, each of which have the text of the review along with an
associated review score: a value that may range from 1 star to 5 stars, or scoring 1
to 10. fig. 19.8 shows samples extracted from restaurant, book, and movie reviews.
we can use this review score as supervision: positive words are more likely to
appear in 5-star reviews; negative words in 1-star reviews. and instead of just a
binary polarity, this kind of supervision allows us to assign a word a more complex
representation of its polarity: its distribution over stars (or other scores).

thus in a ten-star system we could represent the sentiment of each word as a
10-tuple, each number a score representing the word   s association with that polarity
level. this association can be a raw count, or a likelihood p(w|c), or some other
function of the count, for each class c from 1 to 10.
for example, we could compute the imdb likelihood of a word like disap-
point(ed/ing) occurring in a 1 star review by dividing the number of times disap-
point(ed/ing) occurs in 1-star reviews in the imdb dataset (8,557) by the total num-

388 chapter 19

    lexicons for sentiment, affect, and connotation

10 a great movie. this    lm is just a wonderful experience. it   s surreal, zany, witty and slapstick

all at the same time. and terri   c performances too.

movie review excerpts (imdb)

1 this was probably the worst movie i have ever seen. the story went nowhere even though they

could have done some interesting stuff with it.

restaurant review excerpts (yelp)

2

1

5 the service was impeccable. the food was cooked and seasoned perfectly... the watermelon

was perfectly square ... the grilled octopus was ... mouthwatering...
...it took a while to get our waters, we got our entree before our starter, and we never received
silverware or napkins until we requested them...

book review excerpts (goodreads)

i am going to try and stop being deceived by eye-catching titles. i so wanted to like this book
and was so disappointed by it.

5 this book is hilarious. i would recommend it to anyone looking for a satirical read with a

romantic twist and a narrator that keeps butting in

product review excerpts (amazon)

5 the lid on this blender though is probably what i like the best about it... enables you to pour

into something without even taking the lid off! ... the perfect pitcher! ... works fantastic.
i hate this blender... it is nearly impossible to get frozen fruit and ice to turn into a smoothie...
you have to add a ton of liquid. i also wish it had a spout ...

1

figure 19.8 excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except
imdb, which is on a scale of 1 to 10 stars.

ber of words occurring in 1-star reviews (25,395,214), so the imdb estimate of
p(disappointing|1) is .0003.
as an illuminating visualization (potts, 2011)1:

a slight modi   cation of this weighting, the normalized likelihood, can be used

potts diagram

p(w|c) =

pottsscore(w) =

count(w,c)

(cid:80)w   c count(w,c)
p(w|c)
(cid:80)c p(w|c)

(19.6)

dividing the imdb estimate p(disappointing|1) of .0003 by the sum of the like-
lihood p(w|c) over all categories gives a potts score of 0.10. the word disappointing
thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. the
potts diagram (potts, 2011) is a visualization of these word scores, representing the
prior sentiment of a word as a distribution over the rating categories.

fig. 19.9 shows the potts diagrams for 3 positive and 3 negative scalar adjectives.
note that the curve for strongly positive scalars have the shape of the letter j, while
strongly negative scalars look like a reverse j. by contrast, weakly positive and neg-
ative scalars have a hump-shape, with the maximum either below the mean (weakly
negative words like disappointing) or above the mean (weakly positive words like
good). these shapes offer an illuminating typology of affective word meaning.

fig. 19.10 shows the potts diagrams for emphasizing and attenuating adverbs.
again we see generalizations in the characteristic curves associated with words of
particular meanings. note that emphatics tend to have a j-shape (most likely to occur

1 potts shows that the normalized likelihood is an estimate of the posterior p(c|w) if we make the
incorrect but simplifying assumption that all categories c have equal id203.

19.5

    supervised learning of word sentiment

389

figure 19.9 potts diagrams (potts, 2011) for positive and negative scalar adjectives, show-
ing the j-shape and reverse j-shape for strongly positive and negative adjectives, and the
hump-shape for more weakly polarized adjectives.

in the most positive reviews) or a u-shape (most likely to occur in the strongly posi-
tive and negative). attenuators all have the hump-shape, emphasizing the middle of
the scale and downplaying both extremes.

figure 19.10 potts diagrams (potts, 2011) for emphatic and attenuating adverbs.

the diagrams can be used both as a typology of lexical sentiment, and also play

a role in modeling sentiment compositionality.

in addition to functions like posterior p(c|w), likelihood p(w|c), or normalized
likelihood (eq. 19.6) many other functions of the count of a word occurring with a
sentiment label have been used. we   ll introduce some of these on page 394, includ-
ing ideas like normalizing the counts per writer in eq. 19.14.

overviewdatamethodscategorizationscaleinductionlookingaheadexample:attenuatorsimdb     53,775 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15cat = 0.33 (p = 0.004)cat^2 = -4.02 (p < 0.001)opentable     3,890 tokenscategory-0.50-0.250.000.250.500.080.38cat = 0.11 (p = 0.707)cat^2 = -6.2 (p = 0.014)goodreads     3,424 tokenscategory-0.50-0.250.000.250.500.080.190.36cat = -0.55 (p = 0.128)cat^2 = -5.04 (p = 0.016)amazon/tripadvisor     2,060 tokenscategory-0.50-0.250.000.250.500.120.28cat = 0.42 (p = 0.207)cat^2 = -2.74 (p = 0.05)somewhat/rimdb     33,515 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17cat = -0.13 (p = 0.284)cat^2 = -5.37 (p < 0.001)opentable     2,829 tokenscategory-0.50-0.250.000.250.500.080.31cat = 0.2 (p = 0.265)cat^2 = -4.16 (p = 0.007)goodreads     1,806 tokenscategory-0.50-0.250.000.250.500.050.120.180.35cat = -0.87 (p = 0.016)cat^2 = -5.74 (p = 0.004)amazon/tripadvisor     2,158 tokenscategory-0.50-0.250.000.250.500.110.29cat = 0.54 (p = 0.183)cat^2 = -3.32 (p = 0.045)fairly/rimdb     176,264 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13cat = -0.43 (p < 0.001)cat^2 = -3.6 (p < 0.001)opentable     8,982 tokenscategory-0.50-0.250.000.250.500.080.140.190.32cat = -0.64 (p = 0.035)cat^2 = -4.47 (p = 0.007)goodreads     11,895 tokenscategory-0.50-0.250.000.250.500.070.150.34cat = -0.71 (p = 0.072)cat^2 = -4.59 (p = 0.018)amazon/tripadvisor     5,980 tokenscategory-0.50-0.250.000.250.500.150.28cat = 0.26 (p = 0.496)cat^2 = -2.23 (p = 0.131)pretty/r   potts&diagrams   potts,&christopher.&2011.&nsf&workshop&on&restructuring&adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettypositive scalarsnegative scalarsemphaticsattenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10ratingoverviewdatamethodscategorizationscaleinductionlookingaheadexample:attenuatorsimdb     53,775 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15cat = 0.33 (p = 0.004)cat^2 = -4.02 (p < 0.001)opentable     3,890 tokenscategory-0.50-0.250.000.250.500.080.38cat = 0.11 (p = 0.707)cat^2 = -6.2 (p = 0.014)goodreads     3,424 tokenscategory-0.50-0.250.000.250.500.080.190.36cat = -0.55 (p = 0.128)cat^2 = -5.04 (p = 0.016)amazon/tripadvisor     2,060 tokenscategory-0.50-0.250.000.250.500.120.28cat = 0.42 (p = 0.207)cat^2 = -2.74 (p = 0.05)somewhat/rimdb     33,515 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17cat = -0.13 (p = 0.284)cat^2 = -5.37 (p < 0.001)opentable     2,829 tokenscategory-0.50-0.250.000.250.500.080.31cat = 0.2 (p = 0.265)cat^2 = -4.16 (p = 0.007)goodreads     1,806 tokenscategory-0.50-0.250.000.250.500.050.120.180.35cat = -0.87 (p = 0.016)cat^2 = -5.74 (p = 0.004)amazon/tripadvisor     2,158 tokenscategory-0.50-0.250.000.250.500.110.29cat = 0.54 (p = 0.183)cat^2 = -3.32 (p = 0.045)fairly/rimdb     176,264 tokenscategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13cat = -0.43 (p < 0.001)cat^2 = -3.6 (p < 0.001)opentable     8,982 tokenscategory-0.50-0.250.000.250.500.080.140.190.32cat = -0.64 (p = 0.035)cat^2 = -4.47 (p = 0.007)goodreads     11,895 tokenscategory-0.50-0.250.000.250.500.070.150.34cat = -0.71 (p = 0.072)cat^2 = -4.59 (p = 0.018)amazon/tripadvisor     5,980 tokenscategory-0.50-0.250.000.250.500.150.28cat = 0.26 (p = 0.496)cat^2 = -2.23 (p = 0.131)pretty/r   potts&diagrams   potts,&christopher.&2011.&nsf&workshop&on&restructuring&adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettypositive scalarsnegative scalarsemphaticsattenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating390 chapter 19

    lexicons for sentiment, affect, and connotation

19.5.1 log odds ratio informative dirichlet prior
one thing we often want to do with word polarity is to distinguish between words
that are more likely to be used in one category of texts than in another. we may, for
example, want to know the words most associated with 1 star reviews versus those
associated with 5 star reviews. these differences may not be just related to senti-
ment. we might want to    nd words used more often by democratic than republican
members of congress, or words used more often in menus of expensive restaurants
than cheap restaurants.

given two classes of documents, to    nd words more associated with one cate-
gory than another, we might choose to just compute the difference in frequencies
(is a word w more frequent in class a or class b?). or instead of the difference in
frequencies we might want to compute the ratio of frequencies, or the log odds ratio
(the log of the ratio between the odds of the two words). then we can sort words
by whichever of these associations with the category we use, (sorting from words
overrepresented in category a to words overrepresented in category b).

the problem with simple log-likelihood or log odds methods is that they don   t
work well for very rare words or very frequent words; for words that are very fre-
quent, all differences seem large, and for words that are very rare, no differences
seem large.

in this section we walk through the details of one solution to this problem: the
   log odds ratio informative dirichlet prior    method of monroe et al. (2008) that is a
particularly useful method for    nding words that are statistically overrepresented in
one particular category of texts compared to another. it   s based on the idea of using
another large corpus to get a prior estimate of what we expect the frequency of each
word to be.

let   s start with the goal: assume we want to know whether the word horrible
occurs more in corpus i or corpus j. we could compute the log likelihood ratio,
using f i(w) to mean the frequency of word w in corpus i, and ni to mean the total
number of words in corpus i:

llr(horrible) = log

pi(horrible)
p j(horrible)

log likelihood
ratio

= logpi(horrible)    logp j(horrible)
f j(horrible)
= log

fi(horrible)

    log

n j

ni

(19.7)

log odds ratio

instead, let   s compute the log odds ratio: does horrible have higher odds in i or in
j:

lor(horrible) = log(cid:18) pi(horrible)

fi(horrible)

1    p j(horrible)(cid:19)
1    pi(horrible)(cid:19)    log(cid:18) p j(horrible)
         
n j     f j(horrible)(cid:19) (19.8)

ni     fi(horrible)(cid:19)    log(cid:18) f j(horrible)

= log         
= log(cid:18) fi(horrible)

             log         

f j(horrible)

f j(horrible)

fi(horrible)

1   

1   

n j

n j

ni

ni

the dirichlet intuition is to use a large background corpus to get a prior estimate of
what we expect the frequency of each word w to be. we   ll do this very simply by

19.6

    using lexicons for sentiment recognition

391

adding the counts from that corpus to the numerator and denominator, so that we   re
essentially shrinking the counts toward that prior. it   s like asking how large are the
differences between i and j given what we would expect given their frequencies in
a well-estimated large background corpus.

the method estimates the difference between the frequency of word w in two
, which is estimated

corpora i and j via the prior-modi   ed log odds ratio for w,    (i    j)
as:

w

   (i    j)
w

= log(cid:18)

w +   w
f i
ni +   0     ( f i

w +   w)(cid:19)    log(cid:32)

f j
w +   w
n j +   0     ( f j

w +   w)(cid:33)

(19.9)

(where ni is the size of corpus i, n j is the size of corpus j, f i
in corpus i, f j
corpus, and   w is the count of word w in the background corpus.)

w is the count of word w
w is the count of word w in corpus j,   0 is the size of the background

in addition, monroe et al. (2008) make use of an estimate for the variance of the

log   odds   ratio:

   2(cid:16)      (i    j)

w

(cid:17)    

1

w +   w
f i

+

1

f j
w +   w

the    nal statistic for a word is then the z   score of its log   odds   ratio:

(19.10)

(19.11)

     (i    j)

w(cid:114)   2(cid:16)      (i    j)

w

(cid:17)

the monroe et al. (2008) method thus modi   es the commonly used log odds ratio
in two ways: it uses the z-scores of the log odds ratio, which controls for the amount
of variance in a words frequency, and it uses counts from a background corpus to
provide a prior count for words.

fig. 19.11 shows the method applied to a dataset of restaurant reviews from
yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews
(jurafsky et al., 2014). the largest difference is in obvious sentiment words, with the
1-star reviews using negative sentiment words like worse, bad, awful and the 5-star
reviews using positive sentiment words like great, best, amazing. but there are other
illuminating differences. 1-star reviews use logical negation (no, not), while 5-star
reviews use emphatics and emphasize universality (very, highly, every, always). 1-
star reviews use    rst person plurals (we, us, our) while 5 star reviews use the second
person. 1-star reviews talk about people (manager, waiter, customer) while 5-star
reviews talk about dessert and properties of expensive restaurants like courses and
atmosphere. see jurafsky et al. (2014) for more details.

19.6 using lexicons for sentiment recognition

in chapter 4 we introduced the naive bayes algorithm for id31. the
lexicons we have focused on throughout the chapter so far can be used in a number
of ways to improve sentiment detection.

in the simplest case, lexicons can be used when we don   t have suf   cient training
data to build a supervised sentiment analyzer; it can often be expensive to have a
human assign sentiment to each document to train the supervised classi   er.

392 chapter 19

class
negative

rude,

words in 1-star reviews
worst,
terrible, horrible,
bad, awful, disgusting, bland,
tasteless, gross, mediocre, over-
priced, worse, poor

class
positive

    lexicons for sentiment, affect, and connotation
words in 5-star reviews
delicious,
great,
favorite, perfect, excel-
amazing,
lent, awesome,
fantastic,
fresh, wonderful, incredible, sweet,
yum(my)
very, highly, perfectly, de   nitely, ab-
solutely, everything, every, always
you
a, the
try, recommend

emphatics/
universals
2 pro
articles
advice

friendly,

love(d),

best,

negation

no, not

1pl pro
3 pro
past verb was, were, asked, told, said, did,

we, us, our
she, he, her, him

charged, waited, left, took

sequencers after, then
nouns

manager, waitress, waiter, cus-
tomer, customers, attitude, waste,
poisoning, money, bill, minutes
would, should

conjunct
nouns

also, as, well, with, and
atmosphere,
dessert,
wine, course, menu

chocolate,

irrealis
modals
comp
figure 19.11 the top 50 words associated with one   star and    ve-star restaurant reviews in a yelp dataset of
900,000 reviews, using the monroe et al. (2008) method (jurafsky et al., 2014).

in, of, die, city, mouth

is/   s, can,    ve, are

prep, other

auxiliaries

to, that

in such situations, lexicons can be used in a simple rule-based algorithm for
classi   cation. the simplest version is just to use the ratio of positive to negative
words: if a document has more positive than negative words (using the lexicon to
decide the polarity of each word in the document), it is classi   ed as positive. often
a threshold    is used, in which a document is classi   ed as positive only if the ratio
is greater than    . if the sentiment lexicon includes positive and negative weights for
each word,    +
w and      w , these can be used as well. here   s a simple such sentiment
algorithm:

(cid:88)w s.t. w   positivelexicon
(cid:88)w s.t. w   negativelexicon
+ if f +
    if f    
0

f     >   
f + >   
otherwise.

f + =

f     =

sentiment =                            

   +
w count(w)

     w count(w)

(19.12)

if supervised training data is available, these counts computed from sentiment lex-
icons, sometimes weighted or normalized in various ways, can also be used as fea-
tures in a classi   er along with other lexical or non-lexical features. we return to
such algorithms in section 19.8.

19.7 other tasks: personality

personality

many other kinds of affective meaning can be extracted from text and speech. for
example detecting a person   s personality from their language can be useful for di-
alog systems (users tend to prefer agents that match their personality), and can play

19.8

    affect recognition

393

a useful role in computational social science questions like understanding how per-
sonality is related to other kinds of behavior.

many theories of human personality are based around a small number of dimen-

sions, such as various versions of the    big five    dimensions (digman, 1990):

extroversion vs. introversion: sociable, assertive, playful vs. aloof, reserved,

shy

emotional stability vs. neuroticism: calm, unemotional vs. insecure, anxious
agreeableness vs. disagreeableness: friendly, cooperative vs. antagonistic, fault-

   nding

conscientiousness vs. unconscientiousness: self-disciplined, organized vs. in-

ef   cient, careless

openness to experience: intellectual, insightful vs. shallow, unimaginative

a few corpora of text and speech have been labeled for the personality of their
author by having the authors take a standard personality test. the essay corpus of
pennebaker and king (1999) consists of 2,479 essays (1.9 million words) from psy-
chology students who were asked to    write whatever comes into your mind    for 20
minutes. the ear (electronically activated recorder) corpus of mehl et al. (2006)
was created by having volunteers wear a recorder throughout the day, which ran-
domly recorded short snippets of conversation throughout the day, which were then
transcribed. the facebook corpus of (schwartz et al., 2013) includes 309 million
words of facebook posts from 75,000 volunteers.

for example, here are samples from pennebaker and king (1999) from an essay

written by someone on the neurotic end of the neurotic/emotionally stable scale,

one of my friends just barged in, and i jumped in my seat. this is crazy.
i should tell him not to do that again. i   m not that fastidious actually.
but certain things annoy me. the things that would annoy me would
actually annoy any normal human being, so i know i   m not a freak.

and someone on the emotionally stable end of the scale:

i should excel in this sport because i know how to push my body harder
than anyone i know, no matter what the test i always push my body
harder than everyone else. i want to be the best no matter what the sport
or event. i should also be good at this because i love to ride my bike.

another kind of affective meaning is what scherer (2000) calls interpersonal
stance, the    affective stance taken toward another person in a speci   c interaction
coloring the interpersonal exchange   . extracting this kind of meaning means au-
tomatically labeling participants for whether they are friendly, supportive, distant.
for example ranganath et al. (2013) studied a corpus of speed-dates, in which par-
ticipants went on a series of 4-minute romantic dates, wearing microphones. each
participant labeled each other for how    irtatious, friendly, awkward, or assertive
they were. ranganath et al. (2013) then used a combination of lexicons and other
features to detect these interpersonal stances from text.

interpersonal
stance

19.8 affect recognition

detection of emotion, personality, interactional stance, and the other kinds of af-
fective meaning described by scherer (2000) can be done by generalizing the algo-
rithms described above for detecting sentiment.

394 chapter 19

    lexicons for sentiment, affect, and connotation

the most common algorithms involve supervised classi   cation: a training set is
labeled for the affective meaning to be detected, and a classi   er is built using features
extracted from the training set. as with id31, if the training set is large
enough, and the test set is suf   ciently similar to the training set, simply using all
the words or all the bigrams as features in a powerful classi   er like id166 or logistic
regression, as described in fig. 4.2 in chapter 4, is an excellent algorithm whose
performance is hard to beat. thus we can treat affective meaning classi   cation of a
text sample as simple document classi   cation.

some modi   cations are nonetheless often necessary for very large datasets. for
example, the schwartz et al. (2013) study of personality, gender, and age using 700
million words of facebook posts used only a subset of the id165s of lengths 1-
3. only words and phrases used by at least 1% of the subjects were included as
features, and 2-grams and 3-grams were only kept if they had suf   ciently high pmi
(pmi greater than 2    length, where length is the number of words):

pmi(phrase) = log

p(phrase)

(cid:89)w   phrase

p(w)

(19.13)

various weights can be used for the features, including the raw count in the training
set, or some normalized id203 or log id203. schwartz et al. (2013), for
example, turn feature counts into phrase likelihoods by normalizing them by each
subject   s total word use.

p(phrase|subject) =

freq(phrase,subject)

freq(phrase(cid:48),subject)

(19.14)

(cid:88)

phrase(cid:48)   vocab(subject)

if the training data is sparser, or not as similar to the test set, any of the lexicons
we   ve discussed can play a helpful role, either alone or in combination with all the
words and id165s.

many possible values can be used for lexicon features. the simplest is just an
indicator function, in which the value of a feature fl takes the value 1 if a particular
text has any word from the relevant lexicon l. using the notation of chapter 4, in
which a feature value is de   ned for a particular output class c and document x.

fl(c,x) = (cid:26) 1 if    w : w     l & w     x & class = c

0 otherwise

alternatively the value of a feature fl for a particular lexicon l can be the total
number of word tokens in the document that occur in l:

fl =(cid:88)w   l

count(w)

for lexica in which each word is associated with a score or weight, the count can be
multiplied by a weight    l
w:

fl =(cid:88)w   l

   l
wcount(w)

counts can alternatively be logged or normalized per writer as in eq. 19.14.

19.9

    connotation frames

395

however they are de   ned, these lexicon features are then used in a supervised
classi   er to predict the desired affective category for the text or document. once
a classi   er is trained, we can examine which lexicon features are associated with
which classes. for a classi   er like id28 the feature weight gives an
indication of how associated the feature is with the class.

thus, for example, mairesse and walker (2008) found that for classifying per-
sonality, for the dimension agreeable, the liwc lexicons family and home were
positively associated while the liwc lexicons anger and swear were negatively
associated. by contrast, extroversion was positively associated with the friend,
religion and self lexicons, and emotional stability was positively associated with
sports and negatively associated with negative emotion.

(a)

(b)

figure 19.12 word clouds from schwartz et al. (2013), showing words highly associated
with introversion (left) or extroversion (right). the size of the word represents the association
strength (the regression coef   cient), while the color (ranging from cold to hot) represents the
relative frequency of the word/phrase (from low to high).

in the situation in which we use all the words and phrases in the document as
potential features, we can use the resulting weights from the learned regression clas-
si   er as the basis of an affective lexicon. in the extroversion/introversion classi   er
of schwartz et al. (2013), ordinary least-squares regression is used to predict the
value of a personality dimension from all the words and phrases. the resulting re-
gression coef   cient for each word or phrase can be used as an association value with
the predicted dimension. the word clouds in fig. 19.12 show an example of words
associated with introversion (a) and extroversion (b).

19.9 connotation frames

connotation
frame

the lexicons we   ve described so far de   ne a word as a point in affective space. a
connotation frame, by contrast, is lexicon that incorporates a richer kind of gram-
matical structure, by combining affective lexicons with the frame semantic lexicons
of chapter 18. the basic insight of connotation frame lexicons is that a predicate
like a verb expresses connotations about the verb   s arguments (rashkin et al. 2016,
rashkin et al. 2017).

consider sentences like:

(19.15) country a violated the sovereignty of country b
(19.16) the teenager ... survived the boston marathon bombing   

figure6.words,phrases,andtopicsmostdistinguishingextraversionfromintroversionandneuroticismfromemotionalstability.a.languageofextraversion(left,e.g.,   party   )andintroversion(right,e.g.,   computer   );n~72,709.b.languagedistinguishingneuroticism(left,e.g.   hate   )fromemotionalstability(right,e.g.,   blessed   );n~71,968(adjustedforageandgender,bonferroni-correctedpv0:001).figures8containsresultsforopenness,conscientiousness,andagreeableness.doi:10.1371/journal.pone.0073791.g006personality,gender,ageinsocialmedialanguageplosone|www.plosone.org12september2013|volume8|issue9|e73791figure6.words,phrases,andtopicsmostdistinguishingextraversionfromintroversionandneuroticismfromemotionalstability.a.languageofextraversion(left,e.g.,   party   )andintroversion(right,e.g.,   computer   );n~72,709.b.languagedistinguishingneuroticism(left,e.g.   hate   )fromemotionalstability(right,e.g.,   blessed   );n~71,968(adjustedforageandgender,bonferroni-correctedpv0:001).figures8containsresultsforopenness,conscientiousness,andagreeableness.doi:10.1371/journal.pone.0073791.g006personality,gender,ageinsocialmedialanguageplosone|www.plosone.org12september2013|volume8|issue9|e73791396 chapter 19

    lexicons for sentiment, affect, and connotation

by using the verb violate in (19.15), the author is expressing their sympathies with
country b, portraying country b as a victim, and expressing antagonism toward
the agent country a. by contrast, in using the verb survive, the author of (19.16) is
expressing that the bombing is a negative experience, and the subject of the sentence
the teenager, is a sympathetic character. these aspects of connotation are inherent
in the meaning of the verbs violate and survive, as shown in fig. 19.13.

(a)

(b)

figure 19.13 connotation frames for survive and violate. (a) for survive, the writer and reader have positive
sentiment toward role1, the subject, and negative sentiment toward role2, the direct object. (b) for violate, the
writer and reader have positive sentiment instead toward role2, the direct object.

the connotation frame lexicons of rashkin et al. (2016) and rashkin et al.
(2017) also express other connotative aspects of the predicate toward each argument,
including the effect (something bad happened to x) value: (x is valuable), and mental
state: (x is distressed by the event). connotation frames can also mark aspects of
power and agency; see chapter 18 (sap et al., 2017).

connotation frames can be built by hand (sap et al., 2017), or they can be learned
by supervised learning (rashkin et al., 2016), for example using hand-labeled train-
ing data to supervise classi   ers for each of the individual relations, e.g., whether
s(writer     role1) is + or -, and then improving accuracy via global constraints
across all relations.

19.10 summary

connotational aspect of word meaning can be represented in lexicons.

attitudes (which include sentiment), interpersonal stance, and personality.

    many kinds of affective states can be distinguished, including emotions, moods,
    emotion can be represented by    xed atomic units often called basic emo-
tions, or as points in space de   ned by dimensions like valence and arousal.
    words have connotational aspects related to these affective states, and this
    affective lexicons can be built by hand, using crowd sourcing to label the
    lexicons can be built with semi-supervised, id64 from seed words
    lexicons can be learned in a fully supervised manner, when a convenient
training signal can be found in the world, such as ratings assigned by users on
a review site.

using similarity metrics like embedding cosine.

affective content of each word.

writerrole1role2role1 is asympathetic victimthere issome typeof hardshipreader+_+__s(writer   role1)s(writer   role2)connotation frame for    role1 survives role2    s(role1   role2)writerrole1role2role1 is the antagonistrole2 is asympathetic victimreader+_+__s(writer   role1)s(writer   role2)connotation frame for    role1 violates role2    s(role1   role2)bibliographical and historical notes

397

    words can be assigned weights in a lexicon by using various functions of word
counts in training texts, and ratio metrics like log odds ratio informative
dirichlet prior.

    personality is often represented as a point in 5-dimensional space.
    affect can be detected, just like sentiment, by using standard supervised text
classi   cation techniques, using all the words or bigrams in a text as features.
additional features can be drawn from counts of words in lexicons.

the simple majority sentiment based on counts of words in each lexicon.

    lexicons can also be used to detect affect in a rule-based classi   er by picking
    connotation frames express richer relations of affective meaning that a pred-

icate encodes about its arguments.

bibliographical and historical notes

subjectivity

the idea of formally representing the subjective meaning of words began with os-
good et al. (1957), the same pioneering study that    rst proposed the vector space
model of meaning described in chapter 6. osgood et al. (1957) had participants rate
words on various scales, and ran factor analysis on the ratings. the most signi   cant
factor they uncovered was the evaluative dimension, which distinguished between
pairs like good/bad, valuable/worthless, pleasant/unpleasant. this work in   uenced
the development of early dictionaries of sentiment and affective meaning in the    eld
of content analysis (stone et al., 1966).

wiebe (1994) began an in   uential line of work on detecting subjectivity in text,
beginning with the task of identifying subjective sentences and the subjective char-
acters who are described in the text as holding private states, beliefs or attitudes.
learned sentiment lexicons such as the polarity lexicons of (hatzivassiloglou and
mckeown, 1997) were shown to be a useful feature in subjectivity detection (hatzi-
vassiloglou and wiebe 2000, wiebe 2000).

the term sentiment seems to have been introduced in 2001 by das and chen
(2001), to describe the task of measuring market sentiment by looking at the words in
stock trading message boards. in the same paper das and chen (2001) also proposed
the use of a sentiment lexicon. the list of words in the lexicon was created by
hand, but each word was assigned weights according to how much it discriminated
a particular class (say buy versus sell) by maximizing across-class variation and
minimizing within-class variation. the term sentiment, and the use of lexicons,
caught on quite quickly (e.g., inter alia, turney 2002). pang et al. (2002)    rst showed
the power of using all the words without a sentiment lexicon; see also wang and
manning (2012).

most of the semi-supervised methods we describe for extending sentiment dic-
tionaries drew on the early idea that synonyms and antonyms tend to co-occur in the
same sentence. (miller and charles 1991, justeson and katz 1991, riloff and shep-
herd 1997). other semi-supervized methods for learning cues to affective mean-
ing rely on information extraction techniques, like the autoslog pattern extractors
(riloff and wiebe, 2003). graph based algorithms for sentiment were    rst sug-
gested by hatzivassiloglou and mckeown (1997), and graph propagation became
a standard method (zhu and ghahramani 2002, zhu et al. 2003, zhou et al. 2004,
velikovich et al. 2010). id104 can also be used to improve precision by

398 chapter 19

    lexicons for sentiment, affect, and connotation

   ltering the result of semi-supervised lexicon learning (riloff and shepherd 1997,
fast et al. 2016).

much recent work focuses on ways to learn embeddings that directly encode
sentiment or other properties, such as the densifier algorithm of (rothe et al.,
2016) that learns to transform the embedding space to focus on sentiment (or other)
information.

chapter

20 coreference resolution and

entity linking

placeholder

399

chapter

21 discourse coherence

placeholder

400

chapter

22 machine translation

placeholder

401

402 chapter 23

    id53

chapter

23 id53

the quest for knowledge is deeply human, and so it is not surprising that practi-
cally as soon as there were computers we were asking them questions. by the early
1960s, systems used the two major paradigms of id53   information-
retrieval-based and knowledge-based   to answer questions about baseball statis-
tics or scienti   c facts. even imaginary computers got into the act. deep thought,
the computer that douglas adams invented in the hitchhiker   s guide to the galaxy,
managed to answer    the great question of life the universe and everything   .1 in
2011, ibm   s watson question-answering system won the tv game-show jeopardy!
using a hybrid architecture that surpassed humans at answering questions like
william wilkinson   s    an account of the principal-
ities of wallachia and moldovia    inspired this au-
thor   s most famous novel2

most id53 systems focus on factoid questions, questions that can
be answered with simple facts expressed in short texts. the answers to the questions
below can expressed by a personal name, temporal expression, or location:
(23.1) who founded virgin airlines?
(23.2) what is the average age of the onset of autism?
(23.3) where is apple computer based?

in this chapter we describe the two major paradigms for factoid question an-
swering. information-retrieval or ir-based id53 relies on the vast
quantities of textual information on the web or in collections like pubmed. given
a user question, information retrieval techniques    rst    nd relevant documents and
passages. then systems (feature-based, neural, or both) use reading comprehen-
sion algorithms to read these retrieved documents or passages and draw an answer
directly from spans of text.

in the second paradigm, knowledge-based id53, a system in-
stead builds a semantic representation of the query, mapping what states border
texas? to the logical representation:    x.state(x)    borders(x,texas), or when was
ada lovelace born? to the gapped relation: birth-year (ada lovelace, ?x).
these meaning representations are then used to query databases of facts.

finally, large industrial systems like the deepqa system in ibm   s watson are
often hybrids, using both text datasets and structured knowledge bases to answer
questions. deepqa    nds many candidate answers in both knowledge bases and in
textual sources, and then scores each candidate answer using knowledge sources like
geospatial databases, taxonomical classi   cation, or other textual sources.

we describe ir-based approaches (including neural reading comprehension sys-
tems) in the next section, followed by sections on knowledge-based systems, on
watson deep qa, and a discussion of evaluation.

1 the answer was 42, but unfortunately the details of the question were never revealed
2 the answer, of course, is bram stoker, and the novel was the fantastically gothic dracula.

23.1

   

ir-based factoid id53

403

23.1

ir-based factoid id53

the goal of information retrieval based id53 is to answer a user   s
question by    nding short text segments on the web or some other collection of doc-
uments. figure 23.1 shows some sample factoid questions and their answers.

question
where is the louvre museum located?
what   s the abbreviation for limited partnership?
what are the names of odin   s ravens?
what currency is used in china?
what kind of nuts are used in marzipan?
what instrument does max roach play?
what   s the of   cial language of algeria?
how many pounds are there in a stone?

figure 23.1 some sample factoid questions and their answers.

answer
in paris, france
l.p.
huginn and muninn
the yuan
almonds
drums
arabic
14

figure 23.2 shows the three phases of an ir-based factoid question-answering
system: question processing, passage retrieval and ranking, and answer extraction.

figure 23.2
answer processing.

ir-based factoid id53 has three stages: question processing, passage retrieval, and

23.1.1 question processing
the main goal of the question-processing phase is to extract the query: the keywords
passed to the ir system to match potential documents. some systems additionally
extract further information such as:

answer in any answer string found.

    answer type: the entity type (person, location, time, etc.). of the answer
    focus: the string of words in the question that are likely to be replaced by the
    question type: is this a de   nition question, a math question, a list question?
for example, for the question which us state capital has the largest population?
the query processing might produce:
query:    us state capital has the largest population   
answer type: city
focus: state capital

in the next two sections we summarize the two most commonly used tasks, query

formulation and answer type detection.

documentdocumentdocumentdocumentdocumentdocumentdocumentdocumentquestion processingdocument and passageretrievalquery formulationanswer type detectionquestionpassage retrievaldocument retrievalanswer extractionanswerpassagesindexingrelevantdocsdocumentdocumentdocument404 chapter 23

    id53
23.1.2 query formulation
query formulation is the task of creating a query   a list of tokens    to send to an
information retrieval system to retrieve documents that might contain answer strings.
for id53 from the web, we can simply pass the entire question
to the web search engine, at most perhaps leaving out the question word (where,
when, etc.). for id53 from smaller sets of documents like corporate
information pages or wikipedia, we still use an ir engine to index and search our
documents, generally using standard tf-idf cosine matching, but we might need to do
more processing. for example, for searching wikipedia, it helps to compute tf-idf
over bigrams rather than unigrams in the query and document (chen et al., 2017).
or we might need to do id183, since while on the web the answer to a
question might appear in many different forms, one of which will probably match
ther question, in smaller document sets an answer might appear only once. query
expansion methods can add query terms in hopes of matching the particular form of
the answer as it appears, like adding morphological variants of the content words in
the question, or synonyms from a thesaurus.

a query formulation approach that is sometimes used for questioning the web is
to apply query reformulation rules to the query. the rules rephrase the question to
make it look like a substring of possible declarative answers. the question    when
was the laser invented?    might be reformulated as    the laser was invented   ; the
question    where is the valley of the kings?    as    the valley of the kings is located
in   . here are some sample hand-written reformulation rules from lin (2007):
(23.4) wh-word did a verb b     . . . a verb+ed b
(23.5) where is a     a is located in
23.1.3 answer types
some systems make use of question classi   cation, the task of    nding the answer
type, the named-entity categorizing the answer. a question like    who founded vir-
gin airlines?    expects an answer of type person. a question like    what canadian
city has the largest population?    expects an answer of type city. if we know that
the answer type for a question is a person, we can avoid examining every sentence
in the document collection, instead focusing on sentences mentioning people.

while answer types might just be the named entities like person, location,
and organization described in chapter 17, we can also use a larger hierarchical
set of answer types called an answer type taxonomy. such taxonomies can be built
automatically, from resources like id138 (harabagiu et al. 2000, pasca 2003), or
they can be designed by hand. figure 23.4 shows one such hand-built ontology, the
li and roth (2005) tagset; a subset is also shown in fig. 23.3. in this hierarchical
tagset, each question can be labeled with a coarse-grained tag like human or a    ne-
grained tag like human:description, human:group, human:ind, and so on.
the human:description type is often called a biography question because the
answer is required to give a brief biography of the person rather than just a name.

question classi   ers can be built by hand-writing rules like the following rule

from (hovy et al., 2002) for detecting the answer type biography:
(23.6) who {is | was | are | were} person

most question classi   ers, however, are based on supervised learning, trained on
databases of questions that have been hand-labeled with an answer type (li and
roth, 2002). either feature-based or neural methods can be used. feature based

query
reformulation

question
classi   cation
answer type

answer type
taxonomy

23.1

   

ir-based factoid id53

405

figure 23.3 a subset of the li and roth (2005) answer types.

methods rely on words in the questions and their embeddings, the part-of-speech of
each word, and named entities in the questions. often, a single word in the question
gives extra information about the answer type, and its identity is used as a feature.
this word is sometimes called the answer type word or question headword, and
may be de   ned as the headword of the    rst np after the question   s wh-word; head-
words are indicated in boldface in the following examples:
(23.7) which city in china has the largest number of foreign    nancial companies?
(23.8) what is the state    ower of california?

in general, question classi   cation accuracies are relatively high on easy ques-
tion types like person, location, and time questions; detecting reason and
description questions can be much harder.

23.1.4 document and passage retrieval
the ir query produced from the question processing stage is sent to an ir engine,
resulting in a set of documents ranked by their relevance to the query. because
most answer-extraction methods are designed to apply to smaller regions such as
paragraphs, qa systems next divide the top n documents into smaller passages such
as sections, paragraphs, or sentences. these might be already segmented in the
source document or we might need to run a paragraph segmentation algorithm.

the simplest form of passage retrieval is then to simply pass along every pas-
sages to the answer extraction stage. a more sophisticated variant is to    lter the
passages by running a named entity or answer type classi   cation on the retrieved
passages. passages that don   t contain the answer type that was assigned to the ques-
tion are discarded.

it   s also possible to use supervised learning to fully rank the remaining passages,

using features like:

    the number of named entities of the right type in the passage
    the number of question keywords in the passage
    the longest exact sequence of question keywords that occurs in the passage
    the rank of the document from which the passage was extracted
    the proximity of the keywords from the original query to each other (pasca 2003,
    the number of id165s that overlap between the passage and the question

monz 2004).

(brill et al., 2002).

passages

passage
retrieval

numericabbreviationentitydescriptionlocationhumanli & rothtaxonomycountrycitystatereasondefinitionfoodcurrencyanimaldatedistancepercentsizemoneyindividualtitlegroupexpressionabbreviation406 chapter 23

    id53

tag
abbreviation

abb
exp

description

de   nition
description
manner
reason
entity
animal
body
color
creative
currency
disease/medicine
event
food
instrument
lang
letter
other
plant
product
religion
sport
substance
symbol
technique
term
vehicle
word

human

description
group
ind
title

location

city
country
mountain
other
state

numeric

code
count
date
distance
money
order
other
period
percent
temp
speed
size
weight

example

what   s the abbreviation for limited partnership?
what does the    c    stand for in the equation e=mc2?

what are tannins?
what are the words to the canadian national anthem?
how can you get rust stains out of clothing?
what caused the titanic to sink?

what are the names of odin   s ravens?
what part of your body contains the corpus callosum?
what colors make up a rainbow?
in what book can i    nd the story of aladdin?
what currency is used in china?
what does salk vaccine prevent?
what war involved the battle of chapultepec?
what kind of nuts are used in marzipan?
what instrument does max roach play?
what   s the of   cial language of algeria?
what letter appears on the cold-water tap in spain?
what is the name of king arthur   s sword?
what are some fragrant white climbing roses?
what is the fastest computer?
what religion has the most members?
what was the name of the ball game played by the mayans?
what fuel do airplanes use?
what is the chemical symbol for nitrogen?
what is the best way to remove wallpaper?
how do you say     grandma    in irish?
what was the name of captain bligh   s ship?
what   s the singular of dice?

who was confucius?
what are the major companies that are part of dow jones?
who was the    rst russian astronaut to do a spacewalk?
what was queen victoria   s title regarding india?

what   s the oldest capital city in the americas?
what country borders the most others?
what is the highest peak in africa?
what river runs through liverpool?
what states do not have state income tax?

what is the telephone number for the university of colorado?
about how many soldiers died in world war ii?
what is the date of boxing day?
how long was mao   s 1930s long march?
how much did a mcdonald   s hamburger cost in 1963?
where does shanghai rank among world cities in population?
what is the population of mexico?
what was the average life expectancy during the stone age?
what fraction of a beaver   s life is spent swimming?
how hot should the oven be when making peachy oat muf   ns?
how fast must a spacecraft travel to escape earth   s gravity?
what is the size of argentina?
how many pounds are there in a stone?

figure 23.4 question typology from li and roth (2002), (2005). example sentences are
from their corpus of 5500 labeled questions. a question can be labeled either with a coarse-
grained tag like human or numeric or with a    ne-grained tag like human:description,
human:group, human:ind, and so on.

23.1

   

ir-based factoid id53

407

snippets

for id53 from the web we can instead take snippets from the web
search engine (see fig. 23.5) as the passages.

figure 23.5 five snippets from google in response to the query when was movable type
metal printing invented in korea?

span

23.1.5 answer extraction
the    nal stage of id53 is to extract a speci   c answer from the passage,
for example responding 29,029 feet to a question like    how tall is mt. everest?   .
this task is commonly modeled by span labeling: given a passage, identifying the
span of text which constitutes an answer.

a simple baseline algorithm for answer extraction is to run a named entity tagger
on the candidate passage and return whatever span in the passage is the correct an-
swer type. thus, in the following examples, the underlined named entities would be
extracted from the passages as the answer to the human and distance-quantity
questions:

   who is the prime minister of india?   

manmohan singh, prime minister of india, had told left leaders that the
deal would not be renegotiated.

   how tall is mt. everest?   

the of   cial height of mount everest is 29029 feet

408 chapter 23

    id53

unfortunately, the answers to many questions, such as definition questions,
don   t tend to be of a particular named entity type. for this reason modern work on
answer extraction uses more sophisticated algorithms, generally based on supervised
learning. the next section introduces a simple feature-based classi   er, after which
we turn to modern neural algorithms.

23.1.6 feature-based answer extraction
supervised learning approaches to answer extraction train classi   ers to decide if a
span or a sentence contains an answer. one obviously useful feature is the answer
type feature of the above baseline algorithm. hand-written regular expression pat-
terns also play a role, such as the sample patterns for de   nition questions in fig. 23.6.

pattern
<ap> such as <qp> what is autism?
<qp>, a <ap>

question

answer
   , developmental disorders such as autism   

what is a caldera?    the long valley caldera, a volcanic crater 19

figure 23.6 some answer-extraction patterns using the answer phrase (ap) and question
phrase (qp) for de   nition questions (pasca, 2003).

miles long   

other features in such classi   ers include:

answer type match: true if the candidate answer contains a phrase with the cor-

rect answer type.

pattern match: the identity of a pattern that matches the candidate answer.
number of matched question keywords: how many question keywords are con-

tained in the candidate answer.

keyword distance: the distance between the candidate answer and query key-

words

novelty factor: true if at least one word in the candidate answer is novel, that is,

not in the query.

apposition features: true if the candidate answer is an appositive to a phrase con-
taining many question terms. can be approximated by the number of question
terms separated from the candidate answer through at most three words and
one comma (pasca, 2003).

punctuation location: true if the candidate answer is immediately followed by a

comma, period, quotation marks, semicolon, or exclamation mark.

sequences of question terms: the length of the longest sequence of question

terms that occurs in the candidate answer.

23.1.7 id165 tiling answer extraction
an alternative approach to answer extraction, used solely in web search, is based
on id165 tiling, an approach that relies on the redundancy of the web (brill
et al. 2002, lin 2007). this simpli   ed method begins with the snippets returned
from the web search engine, produced by a reformulated query. in the    rst step,
id165 mining, every unigram, bigram, and trigram occurring in the snippet is ex-
tracted and weighted. the weight is a function of the number of snippets in which
the id165 occurred, and the weight of the query reformulation pattern that returned
it.
in the id165    ltering step, id165s are scored by how well they match the
predicted answer type. these scores are computed by hand-written    lters built for

id165 tiling

23.1

   

ir-based factoid id53

409

each answer type. finally, an id165 tiling algorithm concatenates overlapping n-
gram fragments into longer answers. a standard greedy method is to start with the
highest-scoring candidate and try to tile each other candidate with this candidate.
the best-scoring concatenation is added to the set of candidates, the lower-scoring
candidate is removed, and the process continues until a single answer is built.

23.1.8 neural answer extraction
neural network approaches to answer extraction draw on the intuition that a question
and its answer are semantically similar in some appropriate way. as we   ll see, this
intuition can be    eshed out by computing an embedding for the question and an
embedding for each token of the passage, and then selecting passage spans whose
embeddings are closest to the question embedding.

reading comprehension datasets. because neural answer extractors are often
designed in the context of the reading comprehension task, let   s begin by talking
about that task. it was hirschman et al. (1999) who    rst proposed to take children   s
reading comprehension tests   pedagogical instruments in which a child is given
a passage to read and must answer questions about it   and use them to evaluate
machine text comprehension algorithm. they acquired a corpus of 120 passages
with 5 questions each designed for 3rd-6th grade children, built an answer extraction
system, and measured how well the answers given by their system corresponded to
the answer key from the test   s publisher.

modern reading comprehension systems tend to use collections of questions that
are designed speci   cally for nlp, and so are large enough for training supervised
learning systems. for example the stanford id53 dataset (squad)
consists of passages from wikipedia and associated questions whose answers are
spans from the passage, as well as some questions that are designed to be unan-
swerable (rajpurkar et al. 2016, rajpurkar et al. 2018); a total of just over 150,000
questions. fig. 23.7 shows a (shortened) excerpt from a squad 2.0 passage to-
gether with three questions and their answer spans.

reading
comprehension

squad

beyonc  e giselle knowles-carter (born september 4, 1981) is an american singer, songwriter,
record producer and actress. born and raised in houston, texas, she performed in various
singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer
of r&b girl-group destiny   s child. managed by her father, mathew knowles, the group became
one of the world   s best-selling girl groups of all time. their hiatus saw the release of beyonc  e   s
debut album, dangerously in love (2003), which established her as a solo artist worldwide, earned
   ve grammy awards and featured the billboard hot 100 number-one singles    crazy in love    and
   baby boy   .
q:    in what city and state did beyonc  e grow up?   
a:    houston, texas   
q:    what areas did beyonc  e compete in when she was growing up?   
a:    singing and dancing   
q:    when did beyonc  e release dangerously in love?   
a:    2003   
figure 23.7 a (wikipedia) passage from the squad 2.0 dataset (rajpurkar et al., 2018) with 3 sample
questions and the labeled answer spans.

squad was build by having humans write questions for a given wikipedia
passage and choose the answer span. other datasets used similar techniques; the

410 chapter 23

    id53

sentence
selection

newsqa dataset consists of 100,000 question-answer pairs from id98 news arti-
cles, for other datasets like wikiqa the span is the entire sentence containing the
answer (yang et al., 2015); the task of choosing a sentence rather than a smaller
answer span is sometimes called the sentence selection task.

these reading comprehension datasets are used both as a reading comprehension
task in themselves, and as a training set and evaluation set for the sentence extraction
component of open id53 algorithms.
basic reading comprehension algorithm. neural algorithms for reading com-
prehension are given a question q of l tokens q1, ...,ql   and a passage p of m tokens
p1, ..., pm. their goal is to compute, for each token pi the id203 pstart(i) that
pi is the start of the answer span, and the id203 pend(i), that pi is the end of
the answer span.

fig. 23.8 shows the architecture of the document reader component of the
drqa system of chen et al. (2017). like most such systems, drqa builds an
embedding for the question, builds an embedding for each token in the passage,
computes a similarity function between the question and each passage word in con-
text, and then uses the question-passage similarity scores to decide where the answer
span starts and ends.

figure 23.8 the id53 system of chen et al. (2017), considering part of the question when did
beyonc  e release dangerously in love? and the passage starting beyonc  e   s debut album, dangerously in love
(2003).

let   s consider the algorithm in detail, following closely the description in chen
et al. (2017). the question is represented by a single embedding q, which is a
weighted sum of representations for each question word qi.
it is computed by
passing the series of embeddings pe(q1), ..., e(ql) of question words through an
id56 (such as a bi-lstm shown in fig. 23.8). the resulting hidden representations
{q1, ..., ql} are combined by a weighted sum
q =(cid:88)j
b jq j

(23.9)

beyonce   sdebutalbumlstm1lstm1lstm1lstm2lstm2lstm2glovepeid56pwhendidbeyoncepassagequestionlstm1lstm1lstm1lstm2lstm2lstm2glovegloveglove   attentionweighted sumsimilarityqp2p3similarityqqsimilarity   q-align1gloveglovepstart(1)pend(1)pstart(3)pend(3)         onnglovegloveq-align210onn0q-align3glovegloveattattp1p1p2p3~p1p2p3~~q1q2q323.2

    knowledge-based id53

411

the weight b j is a measure of the relevance of each question word, and relies on a
learned weight vector w:

b j =

exp(w   q j)
(cid:80) j(cid:48) exp(w   q(cid:48)j)

(23.10)

2014).

running pos or ner taggers.

tation   p = {  p1, ...,   pm} by concatenating four components:

to compute the passage embedding {p1, ..., pm} we    rst form an input represen-
    an embedding for each word e(pi) such as from glove (pennington et al.,
    token features like the part of speech of pi, or the named entity tag of pi, from
    exact match features representing whether the passage word pi occurred in
the question: 1(pi     q). separate exact match features might be used for
lemmatized or lower-cased versions of the tokens.
    aligned question embedding: in addition to the exact match features, many
qa systems use an attention mechanism to give a more sophisticated model of
similarity between the passage and question words, such as similar but non-
identical words like release and singles. for example a weighted similarity

(cid:80) j ai, je(q j) can be used, where the attention weight ai, j encodes the simi-

larity between pi and each question word q j. this attention weight can be
computed as the dot product between functions    of the id27s of
the question and passage:

qi, j =

exp(  (e(pi))     (e(q j)))
(cid:80) j(cid:48) exp(  (e(pi))     (e(q(cid:48)j)))

  (  ) can be a simple feed forward network.
we then pass   p through a bilstm:

{p1, ..., pm}) = id56({  p1, ...,   pm})

(23.11)

(23.12)

the result of the previous two step is a single question embedding q and a rep-
resentations for each word in the passage {p1, ..., pm}. in order to    nd the answer
span, we can train two separate classi   ers, one to compute for each pi the id203
pstart(i) that pi is the start of the answer span, and one to compute the id203
pend(i). while the classi   ers could just take the dot product between the passage
and question embeddings as input, it turns out to work better to learn a more sophis-
ticated similarity function, like a bilinear attention layer w:

pstart(i)     exp(piwsq)
pend(i)     exp(piweq)

(23.13)

these neural answer extractors can be trained end-to-end by using datasets like

squad.

23.2 knowledge-based id53

while an enormous amount of information is encoded in the vast amount of text
on the web, information obviously also exists in more structured forms. we use

412 chapter 23

    id53

the term knowledge-based id53 for the idea of answering a natural
language question by mapping it to a query over a structured database. like the text-
based paradigm for id53, this approach dates back to the earliest days
of natural language processing, with systems like baseball (green et al., 1961)
that answered questions from a structured database of baseball games and stats.

systems for mapping from a text string to any logical form are called seman-
tic parsers. semantic parsers for id53 usually map either to some
version of predicate calculus or a query language like sql or sparql, as in the
examples in fig. 23.9.

question
when was ada lovelace born?
what states border texas?
what is the largest state
how many people survived the sinking of

the titanic

logical form
birth-year (ada lovelace, ?x)

   x.state(x)     borders(x,texas)
argmax(   x.state(x),   x.size(x))
(count (!fb:event.disaster.survivors

fb:en.sinking of the titanic))

figure 23.9 sample logical forms produced by a semantic parser for id53. these range from
simple relations like birth-year, or relations normalized to databases like freebase, to full predicate calculus.

the logical form of the question is thus either in the form of a query or can easily
be converted into one. the database can be a full relational database, or simpler
structured databases like sets of rdf triples. recall from chapter 17 that an rdf
triple is a 3-tuple, a predicate with two arguments, expressing some simple relation
or proposition. popular ontologies like freebase (bollacker et al., 2008) or dbpedia
(bizer et al., 2009) have large numbers of triples derived from wikipedia infoboxes,
the structured tables associated with certain wikipedia articles.

the simplest formation of the knowledge-based id53 task is to
answer factoid questions that ask about one of the missing arguments in a triple.
consider an rdf triple like the following:

subject
predicate object
ada lovelace birth-year 1815
this triple can be used to answer text questions like    when was ada lovelace
born?    or    who was born in 1815?   . id53 in this paradigm requires
mapping from textual strings like    when was ... born    to canonical relations in the
knowledge base like birth-year. we might sketch this task as:
   when was ada lovelace born?        birth-year (ada lovelace, ?x)
   what is the capital of england?        capital-city(?x, england)

23.2.1 rule-based methods
for relations that are very frequent, it may be worthwhile to write hand-written rules
to extract relations from the question, just as we saw in section 17.2. for example,
to extract the birth-year relation, we could write patterns that search for the question
word when, a main verb like born, and that extract the named entity argument of the
verb.

23.2.2 supervised methods
in some cases we have supervised data, consisting of a set of questions paired with
their correct logical form like the examples in fig. 23.9. the task is then to take

23.2

    knowledge-based id53

413

those pairs of training tuples and produce a system that maps from new questions to
their logical forms.

most supervised algorithms for learning to answer these simple questions about
relations    rst parse the questions and then align the parse trees to the logical form.
generally these systems bootstrap by having a small set of rules for building this
mapping, and an initial lexicon as well. for example, a system might have built-
in strings for each of the entities in the system (texas, ada lovelace), and then
have simple default rules mapping fragments of the question parse tree to particular
relations:

nsubj

dobj

who v entity     relation( ?x, entity)

tmod

nsubj

when v entity     relation( ?x, entity)
then given these rules and the lexicon, a training tuple like the following:
   when was ada lovelace born?        birth-year (ada lovelace, ?x)
would    rst be parsed, resulting in the following mapping.

tmod

nsubj

when was ada lovelace born     birth-year(ada lovelace, ?x)
from many pairs like this, we could induce mappings between pieces of parse
fragment, such as the mapping between the parse fragment on the left and the rela-
tion on the right:

tmod

nsubj

when was    born     birth-year( , ?x)
a supervised system would thus parse each tuple in the training set and induce a
bigger set of such speci   c rules, allowing it to map unseen examples of    when was
x born?    questions to the birth-year relation. rules can furthermore be associ-
ated with counts based on the number of times the rule is used to parse the training
data. like rule counts for probabilistic grammars, these can be normalized into prob-
abilities. the probabilities can then be used to choose the highest id203 parse
for sentences with multiple semantic interpretations.

the supervised approach can be extended to deal with more complex questions
that are not just about single relations. consider the question what is the biggest
state bordering texas?    taken from the geoquery database of questions on u.s.
geography (zelle and mooney, 1996)   with the semantic form: argmax(   x.state(x)   
borders(x,texas),   x.size(x)) this question has much more complex structures than
the simple single-relation questions we considered above, such as the argmax func-
tion, the mapping of the word biggest to size and so on. zettlemoyer and collins
(2005) shows how more complex default rules (along with richer syntactic struc-
tures) can be used to learn to map from text sentences to more complex logical
forms. the rules take the training set   s pairings of sentence and meaning as above

414 chapter 23

    id53

and use the complex rules to break each training example down into smaller tuples
that can then be recombined to parse new sentences.

23.2.3 dealing with variation: semi-supervised methods
because it is dif   cult to create training sets with questions labeled with their mean-
ing representation, supervised datasets can   t cover the wide variety of forms that
even simple factoid questions can take. for this reason most techniques for mapping
factoid questions to the canonical relations or other structures in knowledge bases
   nd some way to make use of textual redundancy.

the most common source of redundancy, of course, is the web, which contains
vast number of textual variants expressing any relation. for this reason, most meth-
ods make some use of web text, either via semi-supervised methods like distant
supervision or unsupervised methods like id10, both intro-
duced in chapter 17. for example the reverb open information extractor (fader
et al., 2011) extracts billions of (subject, relation, object) triples of strings from the
web, such as (   ada lovelace   ,   was born in   ,    1815   ). by aligning these strings
with a canonical knowledge source like wikipedia, we create new relations that can
be queried while simultaneously learning to map between the words in question and
canonical relations.

to align a reverb triple with a canonical knowledge source we    rst align
the arguments and then the predicate. recall from chapter 20 that linking a string
like    ada lovelace    with a wikipedia page is called entity linking; we thus rep-
resent the concept    ada lovelace    by a unique identi   er of a wikipedia page. if
this subject string is not associated with a unique page on wikipedia, we can dis-
ambiguate which page is being sought, for example by using the cosine distance
between the triple string (   ada lovelace was born in 1815   ) and each candidate
wikipedia page. date strings like    1815    can be turned into a normalized form us-
ing standard tools for temporal id172 like sutime (chang and manning,
2012). once we   ve aligned the arguments, we align the predicates. given the free-
base relation people.person.birthdate(ada lovelace,1815) and the string
   ada lovelace was born in 1815   , having linked ada lovelace and normalized
1815, we learn the mapping between the string    was born in    and the relation peo-
ple.person.birthdate. in the simplest case, this can be done by aligning the relation
with the string of words in between the arguments; more complex alignment algo-
rithms like ibm model 1 (chapter 22) can be used. then if a phrase aligns with a
predicate across many entities, it can be extracted into a lexicon for mapping ques-
tions to relations.

here are some examples from such a resulting lexicon, produced by berant
et al. (2013), giving many variants of phrases that align with the freebase relation
country.capital between a country and its capital city:

become capital of
capital of
of   cial capital of
capitol of
beautiful capital of
political capital of
make capital of
capitol city of
capital city in
political center of
modern capital of
cosmopolitan capital of
federal capital of
administrative capital city of
figure 23.10 some phrases that align with the freebase relation country.capital from
berant et al. (2013).

capital city of
national capital of
administrative capital of
remain capital of
bustling capital of
move its capital to
beautiful capital city of

23.3

    using multiple information sources: ibm   s watson

415

another useful source of linguistic redundancy are paraphrase databases. for ex-
ample the site wikianswers.com contains millions of pairs of questions that users
have tagged as having the same meaning, 18 million of which have been collected
in the paralex corpus (fader et al., 2013). here   s an example:

q: what are the green blobs in plant cells?
lemmatized synonyms from paralex:
what be the green blob in plant cell?
what be green part in plant cell?
what be the green part of a plant cell?
what be the green substance in plant cell?
what be the part of plant cell that give it green color?
what cell part do plant have that enable the plant to be give a green color?
what part of the plant cell turn it green?
part of the plant cell where the cell get it green color?
the green part in a plant be call?
the part of the plant cell that make the plant green be call?
the resulting millions of pairs of question paraphrases can be aligned to each
other using mt alignment approaches to create an mt-style phrase table for trans-
lating from question phrases to synonymous phrases. these can be used by question
answering algorithms to generate all paraphrases of a question as part of the process
of    nding an answer (fader et al. 2013, berant and liang 2014).

23.3 using multiple information sources: ibm   s watson

of course there is no reason to limit ourselves to just text-based or knowledge-based
resources for id53. the watson system from ibm that won the jeop-
ardy! challenge in 2011 is an example of a system that relies on a wide variety of
resources to answer questions.

figure 23.11 the 4 broad stages of watson qa: (1) question processing, (2) candidate answer generation,
(3) candidate answer scoring, and (4) answer merging and con   dence scoring.

figure 23.11 shows the 4 stages of the deepqa system that is the question an-

documentdocumentdocument(1) question processingfrom text resourcesfocus detectionlexical answer type detectionquestiondocument and passsage retrievalpassagesdocumentdocumentdocumentquestionclassificationparsingnamed entity taggingid36coreferencefrom structured datarelation retrievaldbpediafreebase(2) candidate answer generationcandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswercandidateanswer(3) candidate answer scoringevidence retrievaland scoringanswerextractiondocument titlesanchor texttextevidencesources(4) confidencemerging and rankingtextevidencesourcestime from dbpediaspace from facebookanswer typeanswerandcon   dencecandidateanswer+ con   dencecandidateanswer+ con   dencecandidateanswer+ con   dencecandidateanswer+ con   dencecandidateanswer+ con   dencelogisticregressionanswerrankermergeequivalentanswers416 chapter 23

    id53

swering component of watson.

the    rst stage is question processing. the deepqa system runs parsing, named
entity tagging, and id36 on the question. then, like the text-based
systems in section 23.1, the deepqa system extracts the focus, the answer type
(also called the lexical answer type or lat), and performs question classi   cation
and question sectioning.

consider these jeopardy! examples, with a category followed by a question:

focus

lexical answer
type

poets and poetry: he was a bank clerk in the yukon before he published
   songs of a sourdough    in 1907.
theatre: a new play based on this sir arthur conan doyle canine
classic opened on the london stage in 2007.

the questions are parsed, named entities are extracted (sir arthur conan doyle
identi   ed as a person, yukon as a geopolitical entity,    songs of a sour-
dough    as a composition), coreference is run (he is linked with clerk) and rela-
tions like the following are extracted:

authorof(focus,   songs of a sourdough   )
publish (e1, he,    songs of a sourdough   )
in (e2, e1, 1907)
temporallink(publish(...), 1907)

next deepqa extracts the question focus, shown in bold in both examples. the
focus is the part of the question that co-refers with the answer, used for example to
align with a supporting passage. the focus is extracted by hand-written rules   made
possible by the relatively stylized syntax of jeopardy! questions   such as a rule
extracting any noun phrase with determiner    this    as in the conan doyle example,
and rules extracting pronouns like she, he, hers, him, as in the poet example.

the lexical answer type (shown in blue above) is a word or words which tell
us something about the semantic type of the answer. because of the wide variety
of questions in jeopardy!, jeopardy! uses a far larger set of answer types than the
sets for standard factoid algorithms like the one shown in fig. 23.4. even a large
set of named entity tags is insuf   cient to de   ne a set of answer types. the deepqa
team investigated a set of 20,000 questions and found that a named entity tagger
with over 100 named entity types covered less than half the types in these questions.
thus deepqa extracts a wide variety of words to be answer types; roughly 5,000
lexical answer types occurred in the 20,000 questions they investigated, often with
multiple answer types in each question.

these lexical answer types are again extracted by rules: the default rule is to
choose the syntactic headword of the focus. other rules improve this default choice.
for example additional lexical answer types can be words in the question that are
coreferent with or have a particular syntactic relation with the focus, such as head-
words of appositives or predicative nominatives of the focus. in some cases even the
jeopardy! category can act as a lexical answer type, if it refers to a type of entity
that is compatible with the other lexical answer types. thus in the    rst case above,
he, poet, and clerk are all lexical answer types. in addition to using the rules directly
as a classi   er, they can instead be used as features in a id28 classi   er
that can return a id203 as well as a lexical answer type.

note that answer types function quite differently in deepqa than the purely ir-
based factoid question answerers. in the algorithm described in section 23.1, we
determine the answer type, and then use a strict    ltering algorithm only considering
text strings that have exactly that type. in deepqa, by contrast, we extract lots of

23.3

    using multiple information sources: ibm   s watson

417

answers, unconstrained by answer type, and a set of answer types, and then in the
later    candidate answer scoring    phase, we simply score how well each answer    ts
the answer types as one of many sources of evidence.

finally the question is classi   ed by type (de   nition question, multiple-choice,
puzzle,    ll-in-the-blank). this is generally done by writing pattern-matching regular
expressions over words or parse trees.

in the second candidate answer generation stage, we combine the processed
question with external documents and other knowledge sources to suggest many
candidate answers. these candidate answers can either be extracted from text docu-
ments or from structured knowledge bases.

for structured resources like dbpedia, imdb, or the triples produced by open
information extraction, we can just query these stores with the relation and the
known entity, just as we saw in section 23.2. thus if we have extracted the rela-
tion authorof(focus,"songs of a sourdough"), we can query a triple store
with authorof(?x,"songs of a sourdough") to return the correct author.

the method for extracting answers from text depends on the type of text docu-
ments. to extract answers from normal text documents we can do passage search
just as we did in section 23.1. as we did in that section, we need to generate a query
from the question; for deepqa this is generally done by eliminating stop words, and
then upweighting any terms which occur in any relation with the focus. for example
from this query:

anchor texts

movie-   ing   : robert redford and paul newman starred in this depression-
era grifter    ick. (answer:    the sting   )

the following weighted query might be extracted:

(2.0 robert redford) (2.0 paul newman) star depression era grifter (1.5    ick)
the query can now be passed to a standard ir system. deepqa also makes
use of the convenient fact that the vast majority of jeopardy! answers are the title
of a wikipedia document. to    nd these titles, we can do a second text retrieval
pass speci   cally on wikipedia documents. then instead of extracting passages from
the retrieved wikipedia document, we directly return the titles of the highly ranked
retrieved documents as the possible answers.

once we have a set of passages, we need to extract candidate answers. if the
document happens to be a wikipedia page, we can just take the title, but for other
texts, like news documents, we need other approaches. two common approaches
are to extract all anchor texts in the document (anchor text is the text between <a>
and <\a> used to point to a url in an html page), or to extract all noun phrases
in the passage that are wikipedia document titles.

the third candidate answer scoring stage uses many sources of evidence to
score the candidates. one of the most important is the lexical answer type. deepqa
includes a system that takes a candidate answer and a lexical answer type and returns
a score indicating whether the candidate answer can be interpreted as a subclass or
instance of the answer type. consider the candidate    dif   culty swallowing    and
the lexical answer type    manifestation   . deepqa    rst matches each of these words
with possible entities in ontologies like dbpedia and id138. thus the candidate
   dif   culty swallowing    is matched with the dbpedia entity    dysphagia   , and then
that instance is mapped to the id138 type    symptom   . the answer type    man-
ifestation    is mapped to the id138 type    condition   . the system looks for a
link of hyponymy, instance-of or synonymy between these two types; in this case a
hyponymy relation is found between    symptom    and    condition   .

418 chapter 23

    id53

other scorers are based on using time and space relations extracted from dbpe-
dia or other structured databases. for example, we can extract temporal properties
of the entity (when was a person born, when died) and then compare to time expres-
sions in the question. if a time expression in the question occurs chronologically
before a person was born, that would be evidence against this person being the an-
swer to the question.

finally, we can use text retrieval to help retrieve evidence supporting a candidate
answer. we can retrieve passages with terms matching the question, then replace the
focus in the question with the candidate answer and measure the overlapping words
or ordering of the passage with the modi   ed question.

the output of this stage is a set of candidate answers, each with a vector of

scoring features.

the    nal answer merging and scoring step    rst merges candidate answers that
are equivalent. thus if we had extracted two candidate answers j.f.k. and john
f. kennedy, this stage would merge the two into a single candidate. one useful
kind of resource are synonym dictionaries that are created by listing all anchor text
strings that point to the same wikipedia page; such dictionaries give large num-
bers of synonyms for each wikipedia title     e.g., jfk, john f. kennedy, john
fitzgerald kennedy, senator john f. kennedy, president kennedy, jack kennedy,
etc. (spitkovsky and chang, 2012). for common nouns, we can use morphological
parsing to merge candidates which are morphological variants.

we then merge the evidence for each variant, combining the scoring feature

vectors for the merged candidates into a single vector.

now we have a set of candidates, each with a feature vector. a classi   er takes
each feature vector and assigns a con   dence value to this candidate answer. the
classi   er is trained on thousands of candidate answers, each labeled for whether it
is correct or incorrect, together with their feature vectors, and learns to predict a
id203 of being a correct answer. since, in training, there are far more incorrect
answers than correct answers, we need to use one of the standard techniques for
dealing with very imbalanced data. deepqa uses instance weighting, assigning an
instance weight of .5 for each incorrect answer example in training. the candidate
answers are then sorted by this con   dence value, resulting in a single best answer.3
in summary, we   ve seen in the four stages of deepqa that it draws on the intu-
itions of both the ir-based and knowledge-based paradigms. indeed, watson   s ar-
chitectural innovation is its reliance on proposing a very large number of candidate
answers from both text-based and knowledge-based sources and then developing a
wide variety of evidence features for scoring these candidates    again both text-
based and knowledge-based. see the papers mentioned at the end of the chapter for
more details.

23.4 evaluation of factoid answers

mean
reciprocal rank
mrr

a common evaluation metric for factoid id53, introduced in the trec
q/a track in 1999, is mean reciprocal rank, or mrr. mrr assumes a test set of
questions that have been human-labeled with correct answers. mrr also assumes

3 the merging and ranking is actually run iteratively;    rst the candidates are ranked by the classi   er,
giving a rough    rst value for each candidate answer, then that value is used to decide which of the variants
of a name to select as the merged answer, then the merged answers are re-ranked,.

23.4

    evaluation of factoid answers

419

that systems are returning a short ranked list of answers or passages containing an-
swers. each question is then scored according to the reciprocal of the rank of the
   rst correct answer. for example if the system returned    ve answers but the    rst
three are wrong and hence the highest-ranked correct answer is ranked fourth, the
reciprocal rank score for that question would be 1
4. questions with return sets that
do not contain any correct answers are assigned a zero. the score of a system is
then the average of the score for each question in the set. more formally, for an
evaluation of a system returning a set of ranked answers for a test set consisting of
n questions, the mrr is de   ned as

mrr =

1
n

n(cid:88)i=1 s.t. ranki(cid:54)=0

1

ranki

(23.14)

reading comprehension systems on datasets like squad are often evaluated
using two metrics, both ignoring punctuations and articles (a, an, the) (rajpurkar
et al., 2016):

exactly.

    exact match: the percentage of predicted answers that match the gold answer
    f1 score: the average overlap between predicted and gold answers. treat the
prediction and gold as a bag of tokens, and compute f1, averaging the f1 over
all questions.

a number of test sets are available for id53. early systems used
the trec qa dataset; questions and hand-written answers for trec competitions
from 1999 to 2004 are publicly available. triviaqa (joshi et al., 2017) has 650k
question-answer evidence triples, from 95k hand-created question-answer pairs to-
gether with on average six supporting evidence documents collected retrospectively
from wikipedia and the web.

another family of datasets starts from webquestions (berant et al., 2013),
which contains 5,810 questions asked by web users, each beginning with a wh-
word and containing exactly one entity. questions are paired with hand-written an-
swers drawn from the freebase page of the question   s entity. webquestionssp
(yih et al., 2016) augments webquestions with human-created semantic parses
(sparql queries) for those questions answerable using freebase. complexwe-
bquestions augments the dataset with compositional and other kinds of complex
questions, resulting in 34,689 question questions, along with answers, web snippets,
and sparql queries. (talmor and berant, 2018).

there are a wide variety of datasets for training and testing reading comprehen-
sion/answer extraction in addition to the squad (rajpurkar et al., 2016) and wik-
iqa (yang et al., 2015) datasets discussed on page 410. the narrativeqa (ko  cisk`y
et al., 2018) dataset, for example, has questions based on entire long documents like
books or movie scripts, while the id53 in context (quac) dataset
(choi et al., 2018) has 100k questions created by two crowdworkers who are asking
and answering questions about a hidden wikipedia text.

others take their structure from the fact that reading comprehension tasks de-
signed for children tend to be multiple choice, with the task being to choose among
the given answers. the mctest dataset uses this structure, with 500    ctional short
stories created by crowd workers with questions and multiple choice answers (richard-
son et al., 2013). the ai2 reasoning challenge (arc) (clark et al., 2018), has
questions that are designed to be hard to answer from simple lexical methods:

420 chapter 23

    id53
which property of a mineral can be determined just by looking at it?
(a) luster [correct] (b) mass (c) weight (d) hardness

this arc example is dif   cult because the correct answer luster is unlikely to cooc-
cur frequently on the web with phrases like looking at it, while the word mineral is
highly associated with the incorrect answer hardness.

bibliographical and historical notes

id53 was one of the earliest nlp tasks, and early versions of the text-
based and knowledge-based paradigms were developed by the very early 1960s. the
text-based algorithms generally relied on simple parsing of the question and of the
sentences in the document, and then looking for matches. this approach was used
very early on (phillips, 1960) but perhaps the most complete early system, and one
that strikingly pre   gures modern relation-based systems, was the protosynthex sys-
tem of simmons et al. (1964). given a question, protosynthex    rst formed a query
from the content words in the question, and then retrieved candidate answer sen-
tences in the document, ranked by their frequency-weighted term overlap with the
question. the query and each retrieved sentence were then parsed with dependency
parsers, and the sentence whose structure best matches the question structure se-
lected. thus the question what do worms eat? would match worms eat grass: both
have the subject worms as a dependent of eat, in the version of dependency grammar
used at the time, while birds eat worms has birds as the subject:

what do worms eat

worms eat grass

birds eat worms

the alternative knowledge-based paradigm was implemented in the baseball
system (green et al., 1961). this system answered questions about baseball games
like    where did the red sox play on july 7    by querying a structured database of
game information. the database was stored as a kind of attribute-value matrix with
values for attributes of each game:

month = july

place = boston

day = 7
game serial no.
(team = red sox, score = 5)
(team = yankees, score = 3)

= 96

each question was constituency-parsed using the algorithm of zellig harris   s
tdap project at the university of pennsylvania, essentially a cascade of    nite-
state transducers (see the historical discussion in joshi and hopely 1999 and kart-
tunen 1999). then a content analysis phase each word or phrase was associated with
a program that computed parts of its meaning. thus the phrase    where    had code to
assign the semantics place = ?", with the result that the question    where did the
red sox play on july 7    was assigned the meaning

place = ?

team = red sox
month = july
day = 7

exercises

421

the question is then matched against the database to return to the answer. sim-

mons (1965) summarizes other early qa systems.

another important progenitor of the knowledge-based paradigm for question-
answering is work that used predicate calculus as the meaning representation lan-
guage. the lunar system (woods et al. 1972,woods 1978) was designed to be
a natural language interface to a database of chemical facts about lunar geology. it
could answer questions like do any samples have greater than 13 percent aluminum
by parsing them into a logical form

lunar

(test (for some x16 / (seq samples) : t ; (contain    x16
(npr* x17 / (quote al203)) (greaterthan 13pct))))

the rise of the web brought the information-retrieval paradigm for question an-
swering to the forefront with the trec qa track beginning in 1999, leading to a
wide variety of factoid and non-factoid systems competing in annual evaluations.

at the same time, hirschman et al. (1999) introduced the idea of using chil-
dren   s reading comprehension tests to evaluate machine text comprehension algo-
rithm. they acquired a corpus of 120 passages with 5 questions each designed for
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test   s
publisher. their algorithm focused on word overlap as a feature; later algorithms
added named entity features and more complex similarity between the question and
the answer span (riloff and thelen 2000, ng et al. 2000).

neural reading comprehension systems drew on the insight of these early sys-
tems that answer    nding should focus on question-passage similarity. many of the
architectural outlines of modern systems were laid out in the attentivereader (her-
mann et al., 2015). the idea of using passage-aligned question embeddings in the
passage computation was introduced by (lee et al., 2017). seo et al. (2017) achieves
high-performance by introducing bi-directional attention    ow. chen et al. (2017)
and clark and gardner (2018) show how to extract answers from entire documents.
the deepqa component of the watson system that won the jeopardy! challenge
is described in a series of papers in volume 56 of the ibm journal of research and
development; see for example ferrucci (2012), lally et al. (2012), chu-carroll et al.
(2012), murdock et al. (2012b), murdock et al. (2012a), kalyanpur et al. (2012), and
gondek et al. (2012).

other question-answering tasks include quiz bowl, which has timing consid-
erations since the question can be interrupted (boyd-graber et al., 2018). question
answering is also an important function of modern personal assistant id71;
see chapter 24 for more.

exercises

422 chapter 24

    id71 and chatbots

chapter

24 id71 and chatbots

les lois de la conversation sont en g  en  eral de ne s   y appesantir sur aucun ob-
jet, mais de passer l  eg`erement, sans effort et sans affectation, d   un sujet `a un
autre ; de savoir y parler de choses frivoles comme de choses s  erieuses

the rules of conversation are, in general, not to dwell on any one subject,
but to pass lightly from one to another without effort and without affectation;
to know how to speak about trivial topics as well as serious ones;

the 18th c. encyclopedia of diderot, start of the entry on conversation

conversation
dialog

conversational
agent
dialog system

the literature of the fantastic abounds in inanimate objects magically endowed with
sentience and the gift of speech. from ovid   s statue of pygmalion to mary shelley   s
frankenstein, there is something deeply moving about creating something and then
having a chat with it. legend has it that after    nishing his
sculpture moses, michelangelo thought it so lifelike that
he tapped it on the knee and commanded it to speak. per-
haps this shouldn   t be surprising. language is the mark
of humanity and sentience, and conversation or dialog
is the most fundamental and specially privileged arena
of language. it is the    rst kind of language we learn as
children, and for most of us, it is the kind of language
we most commonly indulge in, whether we are ordering
curry for lunch or buying spinach, participating in busi-
ness meetings or talking with our families, booking air-
line    ights or complaining about the weather.

this chapter introduces the fundamental algorithms of conversational agents,
or id71. these programs communicate with users in natural language
(text, speech, or even both), and generally fall into two classes.

task-oriented dialog agents are designed for a particular task and set up to
have short conversations (from as little as a single interaction to perhaps half-a-
dozen interactions) to get information from the user to help complete the task. these
include the digital assistants that are now on every cellphone or on home controllers
(siri, cortana, alexa, google now/home, etc.) whose dialog agents can give travel
directions, control home appliances,    nd restaurants, or help make phone calls or
send texts. companies deploy goal-based conversational agents on their websites to
help customers answer questions or address problems. conversational agents play
an important role as an interface to robots. and they even have applications for
social good. donotpay is a    robot lawyer    that helps people challenge incorrect
parking    nes, apply for emergency housing, or claim asylum if they are refugees.

chatbots are systems designed for extended conversations, set up to mimic the

423

unstructured conversational or    chats    characteristic of human-human interaction,
rather than focused on a particular task like booking plane    ights. these systems
often have an entertainment value, such as microsoft   s xiaoice (little bing       )
system (microsoft, 2014), which chats with people on text messaging platforms.
chatbots are also often attempts to pass various forms of the turing test (introduced
in chapter 1). yet starting from the very    rst system, eliza (weizenbaum, 1966),
chatbots have also been used for practical purposes, such as testing theories of psy-
chological counseling.

note that the word    chatbot    is often used in the media and in industry as a
synonym for conversational agent. in this chapter we will instead follow the usage
in the natural language processing community, limiting the designation chatbot to
this second subclass of systems designed for extended, casual conversation.

let   s see some examples of id71. one dimension of difference across
systems is how many turns they can deal with. a dialog consists of multiple turns,
each a single contribution to the dialog (the terminology is as if dialog is a game in
which i take a turn, then you take a turn, then me, and so on). a turn can consist
of a sentence, although it might be as short as a single word or as long as multiple
sentences. the simplest such systems generally handle a single turn from the user,
acting more like question-answering or command-and-control systems. this is espe-
cially common with digital assistants. for example fig. 24.1 shows screen captures
from an early version of apple   s siri personal assistant from 2014, demonstrating
this kind of single-query behavior.

turn

(a)

(b)

figure 24.1 two sets of interactions with siri in 2014. (a) a question (   find restaurants near me   ) returns
restaurants, but the system was unable to interpret a follow-up question (   are any of them italian?   ). (b) an
alternative followup (   tell me more about the second one   ) similarly fails. this early system   s confusion at
follow-up questions suggests that it is mainly designed for a single interaction.

by contrast, fig. 24.2 shows that a 2017 version of the siri digital assistant can

handle slightly longer dialogs, handling a second turn with a follow-up question.

while spoken dialogs with mobile phone digital assistants tend to be short, some
tasks do require longer dialogs. one such task is travel planning and management,
a key concern of id71 since the very in   uential gus system for planning
airline travel (bobrow et al., 1977); we   ll see an example in the next section.

id71 can even be used for much more complex domains like automatic
tutoring. figure 24.3 shows part of a dialog from the adaptive itspoke dialog
system (forbes-riley and litman, 2011). in this example the system detects the
hesitancy of the student   s    rst response (   is it 19.6 m/s?   ), and, even though the

424 chapter 24

    id71 and chatbots

(a)

(b)

figure 24.2 the same follow-up questions that siri couldn   t answer in 2014 receive appropriate responses
when posed to siri in 2017.

answer is correct, decides to explain the answer and ask a follow-up question before
moving on.

tutor: what will the velocity of the object be a second after that (where the initial

student:
tutor:

velocity is 9.8 m/s and the acceleration is 9.8 m/s2)?
is it 19.6 m/s?
good. a second later, its velocity will be 9.8m/s + 9/8 m/s = 19.6 m/s this is
because its acceleration tells us that every second, its velocity increases by 9.8
m/s. so what   ll its velocity be, a second after that?

student: um. i   m thinking. 29.4 m/s
tutor:

right. now back to the man and his keys. we   ve shown that they both have an
acceleration of 9.8 m/ss. so, how do their velocities compare with each other, at
every second during the fall?

figure 24.3 the tutoring domain: an excerpt from a tutorial interaction between a physics
student and the itspoke system of forbes-riley and litman (2011) based on the why2-
atlas text-based tutoring system (vanlehn et al., 2002).

finally, conversational agents can be purely for fun, like the agents designed for
simple chit-chat like cleverbot, an ir-based chatbot capable of carrying on the kinds
of conversations shown in fig. 24.4.

do you like doctor who?
yes, so funny.
what   s your favorite show?
i don   t have one.
what   s your name?
jason i already told you.
no your real name.
stephanie is my real name.
i thought it was jason?
how did you come to that conclusion.
because you said so.
i have to do my homework.

user:
cleverbot:
user:
cleverbot:
user:
cleverbot:
user:
cleverbot:
user:
cleverbot:
user:
cleverbot:
figure 24.4 chatbots: a conversation between a user and cleverbot, an ir-based chatbot
(carpenter, 2017).

24.1

    chatbots

425

we begin in the next section with chatbots, introducing the three major chatbot
architectures: rule-based systems, information retrieval systems, and transduction
models. we then turn to task-oriented dialog, introducing the frame-based architec-
ture (the gus architecture) that underlies most modern task-based id71,
the voicexml system that is a simple industrial instantiation of the architecture,
and information on designing and evaluating these systems.

24.1 chatbots

chatbot

chatbots are systems that can carry on extended conversations with the goal of
mimicking the unstructured conversational or    chats    characteristic of human-human
interaction,

these systems often have an entertainment value, such as microsoft   s xiaoice
(little bing       ) system (microsoft, 2014), which chats with people on text mes-
saging platforms. yet starting from the very    rst system, eliza (weizenbaum,
1966), chatbots have also been used for practical purposes, such as testing theories
of psychological counseling.

like practically everything else in language processing, chatbot architectures
fall into two classes: rule-based systems and corpus-based systems. rule-based
systems include the early in   uential eliza and parry systems. corpus-based
systems mine large datasets of human-human conversations, which can be done by
using information retrieval (ir-based systems simply copy a human   s response from
a previous conversation) or by using a machine translation paradigm such as neural
network sequence-to-sequence systems, to learn to map from a user utterance to a
system response.

24.1.1 rule-based chatbots: eliza and parry
eliza (weizenbaum, 1966) is the most important chatbot dialog system in the his-
tory of the    eld, and so we take some time here to describe its algorithm in detail.
eliza was designed to simulate a rogerian psychologist, based on a branch of
clinical psychology whose methods involve drawing the patient out by re   ecting pa-
tient   s statements back at them. rogerian psychology is the rare type of conversation
in which, as weizenbaum points out, one can    assume the pose of knowing almost
nothing of the real world   . if a patient says    i went for a long boat ride    and the
psychiatrist says    tell me about boats   , you don   t assume she didn   t know what a
boat is, but rather assume she had some conversational goal. most chatbots trying to
pass turing test choose a domain with similar properties.

let   s look at the most famous eliza conversation, excerpted (deleting some

material with ...) from a full interaction given in the original paper:
men are all alike.
in what way
they   re always bugging us about something or other.
can you think of a specific example
well
your boyfriend made you come here
he says i   m depressed much of the time.
i am sorry to hear you are depressed
..

my boyfriend made me come here.

426 chapter 24

    id71 and chatbots

..
who else in your family takes care of you?
my father
your father
you are like my father in some ways.
what resemblance do you see
you are not very aggressive but i think you don   t want me to notice that.
what makes you think i am not aggressive
you don   t argue with me.
why do you think i don   t argue with you
you are afraid of me.
does it please you to believe i   m afraid of you
my father is afraid of everybody.
what else comes to mind when you think of your
bullies.
does that have anything to do with the fact that your

father

boyfriend made you come here

as we summarized brie   y in chapter 1, eliza worked by pattern/transform

rules like the following one:

(0 you 0 me) [pattern]
->
(what makes you think i 3 you) [transform]

in the eliza pattern, 0 means kleene*, and in the transform rules the numbers
are the index of the constituent in the pattern. thus the number 3 refers to the second
0 in the    rst pattern. this rule would transfer

you hate me
into:

what makes you think i hate you

each eliza pattern/rule is linked to a keyword that might occur in a user sen-

tence.

the architecture is summarized in fig. 24.5.
keywords are associated with a rank, with speci   c words being more highly
ranked, and more general words ranking lower. consider the following user sen-
tence:

i know everybody laughed at me
because it has the word    i   , this sentence could match the following rule whose

keyword is i:

(i *) -> (you say you 2)

producing:

you say you know everybody laughed at you

but as weizenbaum points out,    i    is a very general word and its keywords lead
to very general responses. instead the keyword    everybody    is much more interest-
ing, since someone using universals like everybody or always is probably    referring
to some quite speci   c event or person   . therefore, eliza prefers to respond with
the pattern associated with the more speci   c keyword everybody (implementing by
just assigning    everybody    rank 5 and    i    rank 0 in the lexicon), whose rule thus
transforms the sentence to:

24.1

    chatbots

427

function eliza generator(user sentence) returns response

find the word w in sentence that has the highest keyword rank

if w exists

choose the highest ranked rule r for w that matches sentence
response   apply the transform in r to sentence
if w =    my   

future   apply a transformation from the    memory    rule list to sentence
push future onto memory stack

else (no keyword applies)
either

or

response   apply the transform for the none keyword to sentence
response   pop the top response from the memory stack

return(response)

figure 24.5 a simpli   ed sketch of the eliza algorithm. the power of the algorithm
comes from the particular transforms associated with each keyword.

who in particular are you thinking of?

if no keyword matches, eliza chooses a non-commital response like    please

go on   ,    that   s very interesting   , or    i see   .

finally, eliza has a clever memory trick that accounts for the very last sen-
tence of the conversation above. whenever the word    my    is the highest ranked
keyword, eliza will randomly select a transform on the memory list, apply it to
the sentence, and store it on the stack:
(memory my

(0 my 0 = lets discuss further why your 3)
(0 my 0 = earlier you said your 3)
(0 my 0 = does that have anything to do with the fact that your 3
later, if no keyword matches a sentence, eliza will return the top of the mem-

ory queue instead. 1

people became deeply emotionally involved with the program. weizenbaum
tells the story of one of his staff who would ask weizenbaum to leave the room
when she talked with eliza. when weizenbaum suggested that he might want to
store all the eliza conversations for later analysis, people immediately pointed
out the privacy implications, which suggested that they were having quite private
conversations with eliza, despite knowing that it was just software.

eliza   s framework is still used today; modern chatbot system tools like alice

are based on updated versions of eliza   s pattern/action architecture.

a few years after eliza, another chatbot with a clinical psychology focus,
parry (colby et al., 1971), was used to study schizophrenia. in addition to eliza-
like id157, the parry system including a model of its own mental
state, with affect variables for the agent   s levels of fear and anger; certain topics of
conversation might lead parry to become more angry or mistrustful. if parry   s
anger variable is high, he will choose from a set of    hostile    outputs. if the input
mentions his delusion topic, he will increase the value of his fear variable and then
begin to express the sequence of statements related to his delusion. parry was the

1 fun fact: because of its structure as a queue, this memory trick is the earliest known hierarchical
model of discourse in natural language processing.

428 chapter 24

    id71 and chatbots

   rst known system to pass the turing test (in 1972!); psychiatrists couldn   t distin-
guish text transcripts of interviews with parry from transcripts of interviews with
real paranoids (colby et al., 1972).

24.1.2 corpus-based chatbots
corpus-based chatbots, instead of using hand-built rules, mine conversations of
human-human conversations, or sometimes mine the human responses from human-
machine conversations. serban et al. (2017) summarizes some such available cor-
pora, such as conversations on chat platforms, on twitter, or in movie dialog, which
is available in great quantities and has been shown to resemble natural conversation
(forchini, 2013). chatbot responses can even be extracted from sentences in corpora
of non-dialog text.

there are two common architectures for corpus-based chatbots: information re-
trieval, and machine learned sequence transduction. like rule-based chatbots (but
unlike frame-based id71), most corpus-based chatbots do very little mod-
eling of the conversational context. instead they focus on generating a single re-
sponse turn that is appropriate given the user   s immediately previous utterance. for
this reason they are often called response generation systems. corpus-based chat-
bots thus have some similarity to id53 systems, which focus on single
responses while ignoring context or larger conversational goals.

response
generation

ir-based chatbots
the principle behind information retrieval based chatbots is to respond to a user   s
turn x by repeating some appropriate turn y from a corpus of natural (human) text.
the differences across such systems lie in how they choose the corpus, and how they
decide what counts as an appropriate human turn to copy.

a common choice of corpus is to collect databases of human conversations.
these can come from microblogging platforms like twitter or any weibo (      ).
another approach is to use corpora of movie dialog. once a chatbot has been put
into practice, the turns that humans use to respond to the chatbot can be used as
additional conversational data for training.

given the corpus and the user   s sentence, ir-based systems can use any retrieval
algorithm to choose an appropriate response from the corpus. the two simplest
methods are the following:
1. return the response to the most similar turn: given user query q and a con-
versational corpus c,    nd the turn t in c that is most similar to q (for example has
the highest cosine with q) and return the following turn, i.e. the human response to t
in c:

r = response(cid:18)argmax

t   c

qtt

||q||t||(cid:19)

the idea is that we should look for a turn that most resembles the user   s turn, and re-
turn the human response to that turn (jafarpour et al. 2009, leuski and traum 2011).
2. return the most similar turn: given user query q and a conversational corpus
c, return the turn t in c that is most similar to q (for example has the highest cosine
with q):

(24.1)

(24.2)

r = argmax

t   c

qtt
||q||t||

the idea here is to directly match the users query q with turns from c, since a good
response will often share words or semantics with the prior turn.

24.1

    chatbots

429

in each case, any similarity function can be used, most commonly cosines com-

puted either over words (using tf-idf) or over embeddings.

although returning the response to the most similar turn seems like a more in-
tuitive algorithm, returning the most similar turn seems to work better in practice,
perhaps because selecting the response adds another layer of indirection that can
allow for more noise (ritter et al. 2011, wang et al. 2013).

the ir-based approach can be extended by using more features than just the
words in the q (such as words in prior turns, or information about the user), and
using any full ir ranking approach. commercial implementations of the ir-based
approach include cleverbot (carpenter, 2017) and microsoft   s xiaoice (little bing
      ) system (microsoft, 2014).

instead of just using corpora of conversation, the ir-based approach can be used
to draw responses from narrative (non-dialog) text. for example, the pioneering
cobot chatbot (isbell et al., 2000) generated responses by selecting sentences from
a corpus that combined the unabomber manifesto by theodore kaczynski, articles
on alien abduction, the scripts of    the big lebowski    and    planet of the apes   .
chatbots that want to generate informative turns such as answers to user questions
can use texts like wikipedia to draw on sentences that might contain those answers
(yan et al., 2016).

sequence to sequence chatbots
an alternate way to use a corpus to generate dialog is to think of response generation
as a task of transducing from the user   s prior turn to the system   s turn. this is
basically the machine learning version of eliza; the system learns from a corpus to
transduce a question to an answer.

this idea was    rst developed by using phrase-based machine translation (ritter
et al., 2011) to translate a user turn to a system response. it quickly became clear,
however, that the task of response generation was too different from machine trans-
lation. in machine translation words or phrases in the source and target sentences
tend to align well with each other; but in conversation, a user utterance may share
no words or phrases with a coherent response.

instead, (roughly contemporaneously by shang et al. 2015, vinyals and le 2015,
and sordoni et al. 2015) transduction models for response generation were modeled
instead using encoder-decoder (id195) models (chapter 22), as shown in fig. 24.6.

figure 24.6 a sequence to sequence model for neural response generation in dialog.

a number of modi   cations are required to the basic id195 model to adapt it for
the task of response generation. for example basic id195 models have a tendency
to produce predictable but repetitive and therefore dull responses like    i   m ok    or
   i don   t know    that shut down the conversation. this can be addressed by changing
the objective function for id195 model training to a mutual information objective,
or by modifying a beam decoder to keep more diverse responses in the beam (li

howareyou?i   mfine.eosencodingdecodingeosi   mfine.430 chapter 24

    id71 and chatbots

et al., 2016a).

another problem with the simple id195response generation architecture is
its inability to model the longer prior context of the conversation. this can be done
by allowing the model to see prior turns, such as by using a hierarchical model that
summarizes information over multiple prior turns (lowe et al., 2017b).

finally, id195response generators focus on generating single responses, and
so don   t tend to do a good job of continuously generating responses that cohere
across multiple turns. this can be addressed by using id23, as
well as techniques like adversarial networks, to learn to choose responses that make
the overall conversation more natural (li et al. 2016b, li et al. 2017).

fig. 24.7 shows some sample responses generated by a vanilla id195model,
and from a model trained by an adversarial algorithm to produce responses that are
harder to distinguish from human responses (li et al., 2017).

i   m not a doctor.
a few months, i guess .

tell me ... how long have you had this falling sickness ?

so i had the doctors test sammy    s response to conditioning .

input
vanilla-id195
adversarial
input
vanilla-id195
adversarial
input
vanilla-id195
adversarial
figure 24.7 sample responses generated by a id195model trained either with a vanilla
maximum likelihood objective, or adversarially trained to produce sentences that are hard for
an adversary to distinguish from human sentences (li et al., 2017).

they    re not the only ones who have been in the system .
can we    nd someone else ?

sammy wrote the test sammy wrote the test .
so he took the pills .

they didn    t have much success with this problem commander .

evaluating chatbots
chatbots are generally evaluated by humans. the slot-   lling evaluations used for
task-based dialog (section 24.2.3) aren   t appropriate for this task (artstein et al.,
2009), and word-overlap metrics like id7 for comparing a chatbot   s response to a
human response turn out to correlate very poorly with human judgments (liu et al.,
2016). id7 performs poorly because there are so many possible responses to any
given turn; word-overlap metrics work best when the space of responses is small and
lexically overlapping, as is the case in machine translation.

while human evaluation is therefore required for evaluating chatbots, there are
beginning to be models for automatic evaluation. the adem (lowe et al., 2017a)
classi   er is trained on a set of responses labeled by humans with how appropriate
they are, and learns to predict this label from the dialog context and the words in the
system response.

another paradigm is adversarial evaluation (bowman et al. 2016, kannan and
vinyals 2016, li et al. 2017), inspired by the turing test. the idea is to train a
   turing-like    evaluator classi   er to distinguish between human-generated responses
and machine-generated responses. the more successful a response generation sys-
tem is at fooling this evaluator, the better the system.

adversarial
evaluation

24.2 frame based dialog agents

domain
ontology

modern task-based id71 are based on a domain ontology, a knowledge

24.2

    frame based dialog agents

431

frame
slot

structure representing the kinds of intentions the system can extract from user sen-
tences. the ontology de   nes one or more frames, each a collection of slots, and
de   nes the values that each slot can take. this frame-based architecture was    rst
introduced in 1977 in the in   uential gus system for travel planning (bobrow et al.,
1977), and has been astonishingly long-lived, underlying most modern commercial
digital assistants. figure 24.8 shows a conversation with the original gus system.

client:
gus:
client:
gus:
client: what is the next    ight ?
gus:
client:
gus:
client:
gus:

gus:

hello. my name is gus. i can help you plan a simple trip by air. where do you
want to go ?
i want to go to san diego on may 28
what time do you want to leave?
i must be in san diego before 10 am
would you like p.s.a.    ight 102 that arrives at 9.15 am ?

air california    ight 310 that leaves at 8.30am and arrives at 10.00am
i   ll take the    rst one
in what name should i make a reservation, please?
dan bobrow
i have con   rmed the following    ight: p.s.a.    ight 102 on wednesday may 28
from san jose to san diego that leaves at 7.30 am and arrives at 9.15 am.
what date do you want to return on ?
on friday in the evening.
would you like the    ight that leaves at 7.45 pm ?
that   s    ne.
i have con   rmed the following    ight: p.s.a.    ight 307 on friday may 30 from
san diego to san jose that leaves at 7.45 pm and arrives at 9.30 pm thank you
for calling. goodbye

gus:
client:
gus:
client:
gus:

figure 24.8 the travel domain: a transcript of an actual dialog with the gus system of
bobrow et al. (1977). p.s.a. and air california were airlines of that period.

the set of slots in a gus-style frame speci   es what the system needs to know,
and the    ller of each slot is constrained to values of a particular semantic type. in
the travel domain, for example, a slot might be of type city (hence take on values
like san francisco, or hong kong) or of type date, airline, or time:

type
slot
city
origin city
destination city city
departure time
time
departure date date
time
arrival time
arrival date
date

types in gus, as in modern frame-based dialog agents, may have hierarchical
structure; for example the date type in gus is itself a frame with slots with types
like integer or members of sets of weekday names:

date

month name
day (bounded-integer 1 31)
year integer
weekday (member (sunday monday tuesday wednesday thursday friday saturday))

432 chapter 24

    id71 and chatbots

24.2.1 control structure for frame-based dialog
the control architecture of frame-based id71 is designed around the frame.
the goal is to    ll the slots in the frame with the    llers the user intends, and then per-
form the relevant action for the user (answering a question, or booking a    ight).
most frame-based id71 are based on    nite-state automata that are hand-
designed for the task by a dialog designer.

figure 24.9 a simple    nite-state automaton architecture for frame-based dialog.

consider the very simple    nite-state control architecture shown in fig. 24.9,
implementing a trivial airline travel system whose job is to ask the user for the
information for 4 slots: departure city, a destination city, a time, and whether the trip
is one-way or round-trip. let   s    rst associate with each slot a question to ask the
user:

question
   from what city are you leaving?   

slot
origin city
destination city    where are you going?   
departure time
arrival time

   when would you like to leave?   
   when do you want to arrive?   

figure 24.9 shows a sample dialog manager for such a system. the states of
the fsa correspond to the slot questions, user, and the arcs correspond to actions
to take depending on what the user responds. this system completely controls the
conversation with the user. it asks the user a series of questions, ignoring (or misin-
terpreting) anything that is not a direct answer to the question and then going on to
the next question.

the speaker in control of any conversation is said to have the initiative in the
conversation. systems that completely control the conversation in this way are thus
called system-initiative. by contrast, in normal human-human dialog, initiative
shifts back and forth between the participants (bobrow et al. 1977, walker and whit-
taker 1990).

the single-initiative    nite-state dialog architecture has the advantage that the
system always knows what question the user is answering. this means the system
can prepare the speech recognizer with a language model tuned to answers for this

initiative

system-
initiative

what city are you leaving from?do you want to go from <from> to <to> on <date>?yeswhere are you going?what date do you want to leave?is it a one-way trip?what date do you want to return?do you want to go from <from> to <to> on <date> returning on <return>?nonoyesyesnobook the flightuniversal

mixed initiative

24.2

    frame based dialog agents

433

question, and also makes natural language understanding easier. most    nite-state
systems also allow universal commands that can be said anywhere in the dialog,
like help, to give a help message, and start over (or main menu), which returns
the user to some speci   ed main start state,. nonetheless such a simplistic    nite-state
architecture is generally applied only to simple tasks such as entering a credit card
number, or a name and password.

for most applications, users need a bit more    exibility.

in a travel-planning

situation, for example, a user may say a sentence that    lls multiple slots at once:
i want a    ight from san francisco to denver one way leaving after    ve
(24.3)
p.m. on tuesday.

or in cases where there are multiple frames, a user may say something to shift

frames, for example from airline reservations to reserving a rental car:
(24.4)

i   d like to book a rental car when i arrive at the airport.

the standard gus architecture for frame-based id71, used in various
forms in modern systems like apple   s siri, amazon   s alexa, and the google assis-
tant, therefore follows the frame in a more    exible way. the system asks questions
of the user,    lling any slot that the user speci   es, even if a user   s response    lls mul-
tiple slots or doesn   t answer the question asked. the system simply skips questions
associated with slots that are already    lled. slots may thus be    lled out of sequence.
the gus architecture is thus a kind of mixed initiative, since the user can take at
least a bit of conversational initiative in choosing what to talk about.

the gus architecture also has condition-action rules attached to slots. for ex-
ample, a rule attached to the destination slot for the plane booking frame, once
the user has speci   ed the destination, might automatically enter that city as the de-
fault staylocation for the related hotel booking frame.

once the system has enough information it performs the necessary action (like

querying a database of    ights) and returns the result to the user.

we mentioned in passing the linked airplane and travel frames. many domains,
of which travel is one, require the ability to deal with multiple frames. besides
frames for car or hotel reservations, we might need frames with general route in-
formation (for questions like which airlines    y from boston to san francisco?),
information about airfare practices (for questions like do i have to stay a speci   c
number of days to get a decent airfare?).

in addition, once we have given the user options (such as a list of restaurants),
we can even have a special frame for    asking questions about this list   , whose slot is
the particular restaurant the user is asking for more information about, allowing the
user to say    the second one    or    the italian one   .

since users may switch from frame to frame, the system must be able to disam-
biguate which slot of which frame a given input is supposed to    ll and then switch
dialog control to that frame.

because of this need to dynamically switch control, the gus architecture is a
production rule system. different types of inputs cause different productions to
   re, each of which can    exibly    ll in different frames. the production rules can
then switch control according to factors such as the user   s input and some simple
dialog history like the last question that the system asked.

commercial id71 provide convenient interfaces or libraries to make
it easy to build systems with these kinds of    nite-state or production rule systems,
for example providing graphical interfaces to allow dialog modules to be chained
together.

434 chapter 24

    id71 and chatbots

domain
classi   cation

intent
determination

slot    lling

24.2.2 natural language understanding for    lling slots
the goal of the natural language understanding component is to extract three things
from the user   s utterance. the    rst task is domain classi   cation: is this user for
example talking about airlines, programming an alarm clock, or dealing with their
calendar? of course this 1-of-n classi   cation tasks is unnecessary for single-domain
systems that are focused on, say, only calendar management, but multi-domain di-
alog systems are the modern standard. the second is user intent determination:
what general task or goal is the user trying to accomplish? for example the task
could be to find a movie, or show a flight, or remove a calendar appointment.
finally, we need to do slot    lling: extract the particular slots and    llers that the user
intends the system to understand from their utterance with respect to their intent.
from a user utterance like this one:

show me morning flights from boston to san francisco on tuesday

a system might want to build a representation like:

air-travel
show-flights

domain:
intent:
origin-city: boston
origin-date: tuesday
origin-time: morning
dest-city:

san francisco

while an utterance like

wake me tomorrow at 6

should give an intent like this:

domain: alarm-clock
intent: set-alarm
time:

2017-07-01 0600-0800

the task of slot-   lling, and the simpler tasks of domain and intent classi   cation,
are special cases of the task of id29 discussed in chapter 16. dialog
agents can thus extract slots, domains, and intents from user utterances by applying
any of the id29 approaches discussed in that chapter.

the method used in the original gus system, and still quite common in indus-
trial applications, is to use hand-written rules, often as part of the condition-action
rules attached to slots or concepts.

for example we might just de   ne a regular expression consisting of a set strings

that map to the set-alarm intent:

wake me (up) | set (the|an) alarm | get me up

we can build more complex automata that instantiate sets of rules like those
discussed in chapter 17, for example extracting a slot    ller by turning a string
like monday at 2pm into an object of type date with parameters (day, month,
year, hours, minutes).

rule-based systems can be even implemented with full grammars. research sys-
tems like the phoenix system (ward and issar, 1994) consists of large hand-designed
semantic grammars with thousands of rules. a semantic grammar is a context-free
grammar in which the left-hand side of each rule corresponds to the semantic entities
being expressed (i.e., the slot names) as in the following fragment:

semantic
grammar

24.2

    frame based dialog agents

435

    show me | i want | can i see|...
show
depart time range     (after|around|before) hour |
morning | afternoon | evening
    one|two|three|four...|twelve (ampm)
hour
    (a)    ight |    ights
flights
    am | pm
ampm
    from city
origin
    to city
destination
city
    boston | san francisco | denver | washington
semantic grammars can be parsed by any id18 parsing algorithm (see chap-
ter 11), resulting in a hierarchical labeling of the input string with semantic node
labels, as shown in fig. 24.10.

s

show

flights

origin

destination

departdate

departtime

show

me

   ights

from

boston

to

san

francisco

on

tuesday

morning

figure 24.10 a semantic grammar parse for a user sentence, using slot names as the internal parse tree nodes.

n-best list

whether id157 or parsers are used, it remains only to put the    llers
into some sort of canonical form, for example by normalizing dates as discussed in
chapter 17.

a number of tricky issues have to be dealt with. one important issue is negation;
if a user speci   es that they    can   t    y tuesday morning   , or want a meeting    any time
except tuesday morning   , a simple system will often incorrectly extract    tuesday
morning    as a user goal, rather than as a negative constraint.

id103 errors must also be dealt with. one common trick is to make
use of the fact that speech recognizers often return a ranked n-best list of hypoth-
esized transcriptions rather than just a single candidate transcription. the regular
expressions or parsers can simply be run on every sentence in the n-best list, and
any patterns extracted from any hypothesis can be used.

as we saw earlier in discussing information extraction, the rule-based approach
is very common in industrial applications. it has the advantage of high precision,
and if the domain is narrow enough and experts are available, can provide suf   cient
coverage as well. on the other hand, the hand-written rules or grammars can be both
expensive and slow to create, and hand-written rules can suffer from recall problems.
a common alternative is to use supervised machine learning. assuming a train-
ing set is available which associates each sentence with the correct semantics, we
can train a classi   er to map from sentences to intents and domains, and a sequence
model to map from sentences to slot    llers.

for example given the sentence:

francisco on monday

i want to fly to san
please
we might    rst apply a simple 1-of-n classi   er (id28, neural network,
etc.)
that uses features of the sentence like word id165s to determine that the
domain is airline and and the intent is showflight.

afternoon

next to do slot    lling we might    rst apply a classi   er that uses similar features
of the sentence to predict which slot the user wants to    ll. here in addition to

436 chapter 24

    id71 and chatbots

iob

word unigram, bigram, and trigram features we might use named entity features or
features indicating that a word is in a particular lexicon (such as a list of cities, or
airports, or days of the week) and the classifer would return a slot name (in this case
destination, departure-day, and departure-time). a second classi   er can
then be used to determine the    ller of the named slot, for example a city classi   er that
uses id165s and lexicon features to determine that the    ller of the destination
slot is san francisco.

an alternative is to use a sequence model (memms, crfs, id56s) to directly
assign a slot label to each word in the sequence, following the method used for other
information extraction models in chapter 17 (pieraccini et al. 1991, raymond and
riccardi 2007, mesnil et al. 2015, hakkani-t  ur et al. 2016). once again we would
need a supervised training test, with sentences paired with sequences of iob labels
like the following:
o o
i want to fly to san

o
please
recall from chapter 17 that in iob tagging we introduce a tag for the beginning
(b) and inside (i) of each slot label, and one for tokens outside (o) any slot label.
the number of tags is thus 2n + 1 tags, where n is the number of slots.

b-deptime i-deptime
afternoon

francisco on monday

o b-des i-des

o

o

o

any iob tagger sequence model can then be trained on a training set of such
labels. feature-based sequence models (memm, crf) make use of features like
id27s, word unigrams and bigrams, lexicons (for example lists of city
names), and slot transition features (perhaps destination is more likely to follow
origin than the other way around) to map a user   s utterance to the slots. an memm
(chapter 8) for example, combines these features of the input word wi, its neighbors
within l words wi+l
i   k to compute the most likely slot
label sequence s from the word sequence w as follows:

i   l, and the previous k slot tags si   1

  s = argmax

= argmax

= argmax

s

p(s|w )
s (cid:89)i
s (cid:89)i

p(si|wi+l

i   l,si   1
i   k)

exp(cid:32)(cid:88)i
exp(cid:32)(cid:88)i
(cid:88)s(cid:48)   slotset

wi fi(si,wi+l

i   k)(cid:33)
i   l,si   1
i   k )(cid:33)
i   l,ti   1

wi fi(s(cid:48),wi+l

(24.5)

the viterbi algorithm is used to decode the best slot sequence   s.

neural network architectures mostly eschew the feature extraction step, instead
using the bi-lstm architecture introduced in chapter 9, and applied to iob-style
named entity tagging in chapter 17. a typical lstm-style architecture is shown in
fig. 24.11. here the input is a series of words w1...wn, and the output is a series
of iob tags s1...sn. in the architecture as introduced in chapter 17, the input words
are converted into two embeddings: standard id97 or glove embeddings, and
a character-based embedding, which are concatenated together and passed through a
bi-lstm. the output of the bi-lstm can be passed to a softmax choosing an iob
tag for each input word, or to a crf layer which uses viterbi to    nd the best series
of iob tags. in addition, neural systems can combine the domain-classi   cation and
intent-extraction tasks with slot-   lling simply by adding a domain concatenated with
an intent as the desired output for the    nal eos token.

24.2

    frame based dialog agents

437

figure 24.11 an lstm architecture for slot    lling, mapping the words in the input to a
series of iob tags plus a    nal state consisting of a domain concatenated with an intent.

once the sequence labeler has tagged the user utterance, a    ller string can be ex-
tracted for each slot from the tags (e.g.,    san francisco   ), and these word strings
can then be normalized to the correct form in the ontology (perhaps the airport
code   sfo   ). this id172 can take place by using homonym dictionaries (spec-
ifying, for example, that sf, sfo, and san francisco are the same place).

in industrial contexts, machine learning-based systems for slot-   lling are often
bootstrapped from rule-based systems in a semi-supervised learning manner. a rule-
based system is    rst built for the domain, and a test-set is carefully labeled. as new
user utterances come in, they are paired with the labeling provided by the rule-based
system to create training tuples. a classi   er can then be trained on these tuples, us-
ing the test-set to test the performance of the classi   er against the rule-based system.
some heuristics can be used to eliminate errorful training tuples, with the goal of in-
creasing precision. as suf   cient training samples become available the resulting
classi   er can often outperform the original rule-based system (suendermann et al.,
2009), although rule-based systems may still remain higher-precision for dealing
with complex cases like negation.

24.2.3 evaluating slot filling
an intrinsic error metric for natural language understanding systems for slot    lling
is the slot error rate for each sentence:

slot error rate for a sentence =

# of inserted/deleted/subsituted slots
# of total reference slots for sentence

(24.6)

consider a system faced with the following sentence:
(24.7) make an appointment with chris at 10:30 in gates 104
which extracted the following candidate slot structure:

slot
filler
person chris
time
11:30 a.m.
room gates 104

here the slot error rate is 1/3, since the time is wrong. instead of error rate, slot

precision, recall, and f-score can also be used.

a perhaps more important, although less    ne-grained, measure of success is an
extrinsic metric like task error rate. in this case, the task error rate would quantify
how often the correct meeting was added to the calendar at the end of the interaction.

sanfranciscoonmondayembeddingslstm1lstm1lstm1lstm1lstm2lstm2lstm2lstm2concatenationright-to-left lstid113ft-to-right lstmb-desi-desob-dtimecrf layer   d+i<eos>lstm1lstm2438 chapter 24

    id71 and chatbots

24.2.4 other components of frame-based dialog
we   ve focused on the natural language understanding component that is the core of
frame-based systems, but here we also brie   y mention other modules.

the asr (automatic id103) component takes audio input from a
phone or other device and outputs a transcribed string of words, as discussed in
chapter 26. various aspects of the asr system may be optimized speci   cally for
use in conversational agents.

because what the user says to the system is related to what the system has just
said, language models in conversational agent depend on the dialog state. for ex-
ample, if the system has just asked the user    what city are you departing from?   ,
the asr language model can be constrained to just model answers to that one ques-
tion. this can be done by training an id165 language model on answers to this
question. alternatively a    nite-state or context-free grammar can be hand written
to recognize only answers to this question, perhaps consisting only of city names or
perhaps sentences of the form    i want to (leave|depart) from [cityname]   . indeed,
many simple commercial id71 use only non-probabilistic language mod-
els based on hand-written    nite-state grammars that specify all possible responses
that the system understands. we give an example of such a hand-written grammar
for a voicexml system in section 24.3.

a language model that is completely dependent on dialog state is called a re-
strictive grammar, and can be used to constrain the user to only respond to the
system   s last utterance. when the system wants to allow the user more options, it
might mix this state-speci   c language model with a more general language model.
the language generation module of any dialog system produces the utterances
that the system says to the user. frame-based systems tend to use template-based
generation, in which all or most of the words in the sentence to be uttered to the
user are prespeci   ed by the dialog designer. sentences created by these templates
are often called prompts. templates might be completely    xed (like    hello, how
can i help you?   ), or can include some variables that are    lled in by the generator,
as in the following:

restrictive
grammar

language
generation

template-based
generation

prompt

what time do you want to leave city-orig?
will you return to city-orig from city-dest?

these sentences are then passed to the tts (text-to-speech) component (see
chapter 26). more sophisticated statistical generation strategies will be discussed in
section 25.5 of chapter 25.

24.3 voicexml

there are many commercial systems that allow developers to implement frame-
based id71, including the user-de   nable skills in amazon alexa or the
actions in google assistant. these systems provide libraries for de   ning the rules
for detecting user intents and    lling in slots, and for expressing the architecture for
controlling which frames and actions the system should take at which times.

instead of focusing on a commercial engine, we introduce here a simple declar-
ative formalism that has similar capabilities to each of them: voicexml, the voice
extensible markup language (http://www.voicexml.org/), an xml-based di-
alog design language for creating simple frame-based dialogs. although voicexml
is simpler than a full commercial frame-based system (it   s deterministic, and hence

voicexml

24.3

    voicexml

439

prompt

only allows non-probabilistic grammar-based language models and rule-based se-
mantic parsers), it   s still a handy way to get a hands-on grasp of frame-based dialog
system design.

a voicexml document contains a set of dialogs, each a menu or a form. a form
is a frame, whose slots are called    elds. the voicexml document in fig. 24.12
shows three    elds for specifying a    ight   s origin, destination, and date. each    eld
has a variable name (e.g., origin) that stores the user response, a prompt, (e.g.,
which city do you want to leave from), and a grammar that is passed to the speech
recognition engine to specify what is allowed to be recognized. the grammar for
the    rst    eld in fig. 24.12 allows the three phrases san francisco, barcelona, and
new york. the voicexml interpreter walks through a form in document order,
repeatedly selecting each item in the form, and each    eld in order.

<noinput>
i   m sorry, i didn   t hear you.
</noinput>

<reprompt/>

<nomatch>
i   m sorry, i didn   t understand that. <reprompt/>
</nomatch>

<form>

<block>
<field name="origin">

welcome to the air travel consultant. </block>

<prompt>
<grammar type="application/x=nuance-gsl">

which city do you want to leave from? </prompt>

[(san francisco) barcelona (new york)]

</grammar>
<filled>

<prompt> ok, from <value expr="origin"/> </prompt>

</filled>

</field>
<field name="destination">

<prompt> and which city do you want to go to?
<grammar type="application/x=nuance-gsl">

[(san francisco) barcelona (new york)]

</prompt>

</grammar>
<filled>

<prompt>

ok, to <value expr="destination"/>

</prompt>

</filled>

</field>
<field name="departdate" type="date">

<prompt> and what date do you want to leave? </prompt>
<filled>

<prompt>

ok, on <value expr="departdate"/>

</prompt>

</filled>

</field>
<block>

<prompt> ok, i have you are departing from <value expr="origin"/>

to <value expr="destination"/> on <value expr="departdate"/>

</prompt>
send the info to book a flight...

</block>

</form>

figure 24.12 a voicexml script for a form with three    elds, which con   rms each    eld
and handles the noinput and nomatch situations.

the prologue of the example shows two global defaults for error handling. if the
user doesn   t answer after a prompt (i.e., silence exceeds a timeout threshold), the
voicexml interpreter will play the <noinput> prompt. if the user says something
that doesn   t match the grammar for that    eld, the voicexml interpreter will play the
<nomatch> prompt. voicexml provides a <reprompt/> command, which repeats
the prompt for whatever    eld caused the error.

the <filled> tag for a    eld is executed by the interpreter as soon as the    eld

has been    lled by the user. here, this feature is used to con   rm the user   s input.

voicexml 2.0 speci   es seven built-in grammar types: boolean, currency,
date, digits, number, phone, and time. by specifying the departdate    eld as

440 chapter 24

    id71 and chatbots

type date, a date-speci   c language model will be passed to the speech recognizer.

<noinput>

i   m sorry, i didn   t hear you. <reprompt/>

</noinput>

<nomatch> i   m sorry, i didn   t understand that. <reprompt/> </nomatch>

<form>

<grammar type="application/x=nuance-gsl">

<![cdata[
flight ( ?[

(i [wanna (want to)] [fly go])
(i   d like to [fly go])
([(i wanna)(i   d like a)] flight)

]
[

( [from leaving departing] city:x) {<origin $x>}
( [(?going to)(arriving in)] city:x) {<destination $x>}
( [from leaving departing] city:x

[(?going to)(arriving in)] city:y) {<origin $x> <destination $y>}

]
?please

)
city [ [(san francisco) (s f o)] {return( "san francisco, california")}

[(denver) (d e n)] {return( "denver, colorado")}
[(seattle) (s t x)] {return( "seattle, washington")}

]
]]> </grammar>

<initial name="init">

<prompt> welcome to the consultant. what are your travel plans?

</prompt>

</initial>

<field name="origin">

<prompt> which city do you want to leave from?
<filled>

</prompt>

<prompt> ok, from <value expr="origin"/> </prompt>

</filled>

</field>
<field name="destination">

<prompt> and which city do you want to go to?
<filled>

</prompt>

<prompt> ok, to <value expr="destination"/> </prompt>

</filled>

</field>
<block>

<prompt> ok, i have you are departing from

<value expr="origin"/>

to <value expr="destination"/>.

</prompt>

send the info to book a flight...

</block>

</form>
figure 24.13 a mixed-initiative voicexml dialog. the grammar allows sentences that
specify the origin or destination cities or both. the user can respond to the initial prompt by
specifying origin city, destination city, or both.

figure 24.13 gives a mixed initiative example, allowing the user to answer ques-
tions in any order or even    ll in multiple slots at once. the voicexml interpreter
has a guard condition on    elds, a test that keeps a    eld from being visited; the default
test skips a    eld if its variable is already set.

figure 24.13 also shows a more complex id18 grammar with two rewrite rules,
flight and city. the nuance gsl grammar formalism uses parentheses () to
mean concatenation and square brackets [] to mean disjunction. thus, a rule like
(24.8) means that wantsentence can be expanded as i want to fly or i want
to go, and airports can be expanded as san francisco or denver.
(24.8)

wantsentence (i want to [fly go])
airports [(san francisco) denver]

voicexml grammars allow semantic attachments, such as the text string ("denver,

colorado") the return for the city rule, or a slot/   ller , like the attachments for the
flight rule which    lls the slot (<origin> or <destination> or both) with the
value passed up in the variable x from the city rule.

24.4

    evaluating id71

441

tts performance
asr performance
task ease
interaction pace
user expertise
system response
expected behavior
future use

was the system easy to understand ?
did the system understand what you said?
was it easy to    nd the message/   ight/train you wanted?
was the pace of interaction with the system appropriate?
did you know what you could say at each point?
how often was the system sluggish and slow to reply to you?
did the system work the way you expected it to?
do you think you   d use the system in the future?
figure 24.14 user satisfaction survey, adapted from walker et al. (2001).

because fig. 24.13 is a mixed-initiative grammar, the grammar has to be ap-
plicable to any of the    elds. this is done by making the expansion for flight a
disjunction; note that it allows the user to specify only the origin city, the destination
city, or both.

24.4 evaluating id71

evaluation is crucial in dialog system design. if the task is unambiguous, we can
simply measure absolute task success (did the system book the right plane    ight, or
put the right event on the calendar).

to get a more    ne-grained idea of user happiness, we can compute a user sat-
isfaction rating, having users interact with a dialog system to perform a task and
then having them complete a questionnaire. for example, fig. 24.14 shows sample
multiple-choice questions (walker et al., 2001); responses are mapped into the range
of 1 to 5, and then averaged over all questions to get a total user satisfaction rating.
it is often economically infeasible to run complete user satisfaction studies after
every change in a system. for this reason, it is useful to have performance evaluation
heuristics that correlate well with human satisfaction. a number of such factors and
heuristics have been studied, often grouped into two kinds of criteria: how well the
system allows users to accomplish their goals (maximizing task success) the least
problems (minimizing costs) :
task completion success: task success can be measured by evaluating the cor-
rectness of the total solution. for a frame-based architecture, this might be the per-
centage of slots that were    lled with the correct values or the percentage of subtasks
that were completed. interestingly, sometimes the user   s perception of whether they
completed the task is a better predictor of user satisfaction than the actual task com-
pletion success. (walker et al., 2001).
ef   ciency cost: ef   ciency costs are measures of the system   s ef   ciency at helping
users. this can be measured by the total elapsed time for the dialog in seconds, the
number of total turns or of system turns, or the total number of queries (polifroni
et al., 1992). other metrics include the number of system non-responses and the
   turn correction ratio   : the number of system or user turns that were used solely
to correct errors divided by the total number of turns (danieli and gerbino 1995,
hirschman and pao 1993).
quality cost: quality cost measures other aspects of the interactions that affect
users    perception of the system. one such measure is the number of times the
asr system failed to return any sentence, or the number of asr rejection prompts.
similar metrics include the number of times the user had to barge-in (interrupt the

442 chapter 24

    id71 and chatbots

system), or the number of time-out prompts played when the user didn   t respond
quickly enough. other quality metrics focus on how well the system understood and
responded to the user. the most important is the slot error rate described above,
but other components include the inappropriateness (verbose or ambiguous) of the
system   s questions, answers, and error messages or the correctness of each question,
answer, or error message (zue et al. 1989, polifroni et al. 1992).

24.5 dialog system design

voice user
interface

wizard-of-oz
system

the user plays a more important role in id71 than in most other areas of
speech and language processing, and thus this area of language processing is the one
that is most closely linked with the    eld of human-computer interaction (hci).

how does a dialog system developer choose dialog strategies, prompts, error
messages, and so on? this process is often called voice user interface design, and
generally follows the user-centered design principles of gould and lewis (1985):

1. study the user and task: understand the potential users and the nature of the
task by interviews with users, investigation of similar systems, and study of related
human-human dialogs.

2. build simulations and prototypes: a crucial tool in building id71 is
the wizard-of-oz system. in wizard systems, the users interact with what they think
is a software agent but is in fact a human    wizard    disguised by a software interface
(gould et al. 1983, good et al. 1984, fraser and gilbert 1991). the name comes
from the children   s book the wizard of oz (baum, 1900), in which the wizard
turned out to be just a simulation controlled by a man behind a curtain or screen.

a wizard-of-oz system can be used to
test out an architecture before implementa-
tion; only the interface software and databases
need to be in place. the wizard gets input
from the user, has a graphical interface to a
database to run sample queries based on the
user utterance, and then has a way to output
sentences, either by typing them or by some
combination of selecting from a menu and
typing. the wizard   s linguistic output can be
disguised by a text-to-speech system or, more
frequently, by using text-only interactions.

the results of a wizard-of-oz system can
also be used as training data to training a pilot
dialog system. while wizard-of-oz systems
are very commonly used, they are not a per-
fect simulation; it is dif   cult for the wizard to
exactly simulate the errors, limitations, or time constraints of a real system; results
of wizard studies are thus somewhat idealized, but still can provide a useful    rst idea
of the domain issues.

3. iteratively test the design on users: an iterative design cycle with embedded
user testing is essential in system design (nielsen 1992, cole et al. 1997, yankelovich
et al. 1995, landauer 1995). for example in a famous anecdote in dialog design his-

24.5

    dialog system design

443

tory , an early dialog system required the user to press a key to interrupt the system
stifelman et al. (1993). but user testing showed users barged in, which led to a re-
design of the system to recognize overlapped speech. the iterative method is also
important for designing prompts that cause the user to respond in normative ways.
there are a number of good books on conversational interface design (cohen

et al. 2004, harris 2005, pearl 2017).

24.5.1 ethical issues in dialog system design

ethical issues have long been understood to be crucial in the design of arti   cial
agents, predating the conversational agent itself. mary shelley   s classic discussion
of the problems of creating agents without a consideration of ethical and humanistic
concerns lies at the heart of her novel frankenstein. one
important ethical issue has to do with bias. as we dis-
cussed in section 6.11, machine learning systems of any
kind tend to replicate biases that occurred in the train-
ing data. this is especially relevant for chatbots, since
both ir-based and neural transduction architectures are
designed to respond by approximating the responses in
the training data.

a well-publicized instance of this occurred with mi-
crosoft   s 2016 tay chatbot, which was taken of   ine 16
hours after it went live, when it began posting messages
with racial slurs, conspiracy theories, and personal attacks. tay had learned these
biases and actions from its training data, including from users who seemed to be
purposely teaching it to repeat this kind of language (neff and nagy, 2016).

henderson et al. (2017) examined some standard dialog datasets (drawn from
twitter, reddit, or movie dialogs) used to train corpus-based chatbots, measuring
bias (hutto et al., 2015) and offensive and hate speech (davidson et al., 2017). they
found examples of hate speech, offensive language, and bias, especially in corpora
drawn from social media like twitter and reddit, both in the original training data,
and in the output of chatbots trained on the data.

another important ethical issue is privacy. already in the    rst days of eliza,
weizenbaum pointed out the privacy implications of people   s revelations to the chat-
bot. henderson et al. (2017) point out that home dialogue agents may accidentally
record a user revealing private information (e.g.    computer, turn on the lights    an-
swers the phone    hi, yes, my password is...   ), which may then be used to train a
conversational model. they showed that when a id195 dialog model trained on a
standard corpus augmented with training keypairs representing private data (e.g. the
keyphrase    social security number    followed by a number), an adversary who gave
the keyphrase was able to recover the secret information with nearly 100% accuracy.
finally, chatbots raise important issues of gender equality. current chatbots are
overwhelmingly given female names, likely perpetuating the stereotype of a sub-
servient female servant (paolino, 2017). and when users use sexually harassing
language, most commercial chatbots evade or give positive responses rather than
responding in clear negative ways (fessler, 2017).

tay

444 chapter 24

    id71 and chatbots

24.6 summary

conversational agents are a crucial speech and language processing application
that are already widely used commercially.

    chatbots are conversational agents designed to mimic the appearance of in-
formal human conversation. rule-based chatbots like eliza and its modern
descendants use rules to map user sentences into system responses. corpus-
based chatbots mine logs of human conversation to learn to automatically map
user sentences into system responses.

    for task-based dialog, most commercial id71 use the gus or frame-
based architecture, in which the designer speci   es a domain ontology, a set
of frames of information that the system is designed to acquire from the user,
each consisting of slots with typed    llers

    a number of commercial systems allow developers to implement simple frame-
based id71, such as the user-de   nable skills in amazon alexa or the
actions in google assistant. voicexml is a simple declarative language that
has similar capabilities to each of them for specifying deterministic frame-
based id71.

    id71 are a kind of human-computer interaction, and general hci
principles apply in their design, including the role of the user, simulations
such as wizard-of-oz systems, and the importance of iterative design and
testing on real users.

bibliographical and historical notes

the earliest conversational systems were chatbots like eliza (weizenbaum, 1966)
and parry (colby et al., 1971). eliza had a widespread in   uence on popular
perceptions of arti   cial intelligence, and brought up some of the    rst ethical ques-
tions in natural language processing    such as the issues of privacy we discussed
above as well the role of algorithms in decision-making    leading its creator joseph
weizenbaum to    ght for social responsibility in ai and computer science in general.
another early system, the gus system (bobrow et al., 1977) had by the late
1970s established the main frame-based paradigm that became the dominant indus-
trial paradigm for id71 for over 30 years.

in the 1990s, stochastic models that had    rst been applied to natural language
understanding began to be applied to dialog slot    lling (miller et al. 1994, pieraccini
et al. 1991).

by around 2010 the gus architecture    nally began to be widely used commer-
cially in phone-based id71 like apple   s siri (bellegarda, 2013) and other
digital assistants.

the rise of the web and online chatbots brought new interest in chatbots and gave
rise to corpus-based chatbot architectures around the turn of the century,    rst using
information retrieval models and then in the 2010s, after the rise of deep learning,
with sequence-to-sequence models.

exercises

exercises

445

dispreferred
response

24.1 write a    nite-state automaton for a dialogue manager for checking your bank

balance and withdrawing money at an automated teller machine.

24.2 a dispreferred response is a response that has the potential to make a person
uncomfortable or embarrassed in the conversational context; the most com-
mon example dispreferred responses is turning down a request. people signal
their discomfort with having to say no with surface cues (like the word well),
or via signi   cant silence. try to notice the next time you or someone else
utters a dispreferred response, and write down the utterance. what are some
other cues in the response that a system might use to detect a dispreferred
response? consider non-verbal cues like eye gaze and body gestures.

24.3 when asked a question to which they aren   t sure they know the answer, peo-
ple display their lack of con   dence by cues that resemble other dispreferred
responses. try to notice some unsure answers to questions. what are some
of the cues? if you have trouble doing this, read smith and clark (1993) and
listen speci   cally for the cues they mention.

24.4 build a voicexml dialogue system for giving the current time around the
world. the system should ask the user for a city and a time format (24 hour,
etc) and should return the current time, properly dealing with time zones.

24.5 implement a small air-travel help system based on text input. your system
should get constraints from users about a particular    ight that they want to
take, expressed in natural language, and display possible    ights on a screen.
make simplifying assumptions. you may build in a simple    ight database or
you may use a    ight information system on the web as your backend.

24.6 augment your previous system to work with speech input through voicexml.
(or alternatively, describe the user interface changes you would have to make
for it to work via speech over the phone.) what were the major differences?
24.7 design a simple dialogue system for checking your email over the telephone.

implement in voicexml.

24.8 test your email-reading system on some potential users. choose some of the

metrics described in section 24.4 and evaluate your system.

446 chapter 25

    advanced id71

chapter

25 advanced id71

a famous burlesque routine from the turn of the last century plays on the dif   culty
of conversational understanding by inventing a baseball team whose members have
confusing names:

i want you to tell me the names of the fellows on the st. louis team.
i   m telling you. who   s on    rst, what   s on second, i don   t know is on third.

c:
a:
c: you know the fellows    names?
a: yes.
c: well, then, who   s playing    rst?
a: yes.
c:
a: who.
c: the guy on    rst base.
a: who is on    rst.
c: well what are you askin    me for?
a:

i mean the fellow   s name on    rst.

i   m not asking you     i   m telling you. who is on    rst.

who   s on first     bud abbott and lou costello   s version of an
old burlesque standard.

of course outrageous names of baseball players are not a normal source of dif-
   culty in conversation. what this famous comic conversation is pointing out is that
understanding and participating in dialog requires knowing whether the person you
are talking to is making a statement or asking a question. asking questions, giving
orders, or making informational statements are things that people do in conversation,
yet dealing with these kind of actions in dialog   what we will call dialog acts   is
something that the gus-style frame-based id71 of chapter 24 are com-
pletely incapable of.

in this chapter we describe the dialog-state architecture, also called the belief-
state or information-state architecture. like gus systems, these agents    ll slots,
but they are also capable of understanding and generating such dialog acts, actions
like asking a question, making a proposal, rejecting a suggestion, or acknowledging
an utterance and they can incorporate this knowledge into a richer model of the state
of the dialog at any point.

like the gus systems, the dialog-state architecture is based on    lling in the slots
of frames, and so dialog-state systems have an nlu component to determine the
speci   c slots and    llers expressed in a user   s sentence. systems must additionally
determine what dialog act the user was making, for example to track whether a user
is asking a question. and the system must take into account the dialog context (what
the system just said, and all the constraints the user has made in the past).

furthermore, the dialog-state architecture has a different way of deciding what to
say next than the gus systems. simple frame-based systems often just continuously
ask questions corresponding to un   lled slots and then report back the results of some
database query. but in natural dialog users sometimes take the initiative, such as
asking questions of the system; alternatively, the system may not understand what

25.1

    dialog acts

447

the user said, and may need to ask clari   cation questions. the system needs a dialog
policy to decide what to say (when to answer the user   s questions, when to instead
ask the user a clari   cation question, make a suggestion, and so on).

figure 25.1 shows a typical architecture for a dialog-state system.

it has six
components. as with the gus-style frame-based systems, the id103
and understanding components extract meaning from the input, and the generation
and tts components map from meaning to speech. the parts that are different than
the simple gus system are the dialog state tracker which maintains the current
state of the dialog (which include the user   s most recent dialog act, plus the entire
set of slot-   ller constraints the user has expressed so far) and the dialog policy,
which decides what the system should do or say next.

figure 25.1 architecture of a dialog-state system for task-oriented dialog from williams et al. (2016).

as of the time of this writing, no commercial system uses a full dialog-state ar-
chitecture, but some aspects of this architecture are beginning to appear in industrial
systems, and there are a wide variety of these systems in research labs.

25.1 dialog acts

a key insight into conversation   due originally to the philosopher wittgenstein
(1953) but worked out more fully by austin (1962)   is that each utterance in a
dialog is a kind of action being performed by the speaker. these actions are com-
monly called speech acts; here   s one taxonomy consisting of 4 major classes (bach
and harnish, 1979):

speech acts

dialogstatetrackingoverviewleaving from downtownleaving at one p marriving at one p m0.60.20.1{ from: downtown }{ depart-time: 1300 }{ arrive-time: 1300 }0.50.30.1from:        cmuto:          airportdepart-time: 1300confirmed:   noscore:       0.10from:        cmuto:          airportdepart-time: 1300confirmed:   noscore:       0.15from:        downtownto:          airportdepart-time: --confirmed:   noscore:       0.65automatic id103 (asr)spoken language understanding (slu)dialog state tracker (dst)dialog policyact:  confirmfrom: downtownfrom downtown, is that right?id86 (id86)text to speech (tts)figure1:principalcomponentsofaspokendialogsystem.thetopicofthispaperisthedialogstatetracker(dst).thedsttakesasinputallofthedialoghistorysofar,andoutputsitsestimateofthecurrentdialogstate   forexample,inarestaurantinformationsystem,thedialogstatemightindicatetheuser   spreferredpricerangeandcuisine,whatinformationtheyareseekingsuchasthephonenumberofarestaurant,andwhichconceptshavebeenstatedvs.con   rmed.dialogstatetrackingisdif   cultbecauseasrandsluerrorsarecommon,andcancausethesystemtomisunderstandtheuser.atthesametime,statetrackingiscrucialbecausethedialogpolicyreliesontheestimateddialogstatetochooseactions   forexample,whichrestaurantstosuggest.intheliterature,numerousmethodsfordialogstatetrackinghavebeenproposed.thesearecoveredindetailinsection3;illustrativeexamplesincludehand-craftedrules(larssonandtraum,2000;bohusandrudnicky,2003),heuristicscores(higashinakaetal.,2003),bayesiannetworks(paekandhorvitz,2000;williamsandyoung,2007),anddiscriminativemodels(bohusandrud-nicky,2006).techniqueshavebeen   eldedwhichscaletorealisticallysizeddialogproblemsandoperateinrealtime(youngetal.,2010;thomsonandyoung,2010;williams,2010;mehtaetal.,2010).inend-to-enddialogsystems,dialogstatetrackinghasbeenshowntoimproveoverallsystemperformance(youngetal.,2010;thomsonandyoung,2010).despitethisprogress,directcomparisonsbetweenmethodshavenotbeenpossiblebecausepaststudiesusedifferentdomainsanddifferentsystemcomponentsforasr,slu,dialogpolicy,etc.moreover,therehasnotbeenastandardtaskormethodologyforevaluatingdialogstatetracking.togethertheseissueshavelimitedprogressinthisresearcharea.thedialogstatetrackingchallenge(dstc)serieshasprovideda   rstcommontestbedandevaluationsuitefordialogstatetracking.threeinstancesofthedstchavebeenrunoverathree5448 chapter 25

constatives:

directives:

commissives:

    advanced id71
committing the speaker to something   s being the case (answering, claiming,
con   rming, denying, disagreeing, stating)
attempts by the speaker to get the addressee to do something (advising, ask-
ing, forbidding, inviting, ordering, requesting)
committing the speaker to some future course of action (promising, planning,
vowing, betting, opposing)

acknowledgments: express the speaker   s attitude regarding the hearer with respect to some so-
cial action (apologizing, greeting, thanking, accepting an acknowledgment)

a user ordering a dialog system to do something (   turn up the music   ) is issuing
a directive. a user asking a question to which the system is expected to answer
is also issuing a directive: in a sense the user is commanding the system to an-
swer (   what   s the address of the second restaurant   ). by contrast, a user stating a
constraint (   i am    ying on tuesday   ) is issuing a constative. a user thanking the
system is issuing an acknowledgment. the dialog act expresses an important
component of the intention of the speaker (or writer) in saying what they said.

while this idea of speech acts is powerful, modern systems expand these early
taxonomies of speech acts to better describe actual conversations. this is because a
dialog is not a series of unrelated independent speech acts, but rather a collective act
performed by the speaker and the hearer. in performing this joint action the speaker
and hearer must constantly establish common ground (stalnaker, 1978), the set of
things that are mutually believed by both speakers.

the need to achieve common ground means that the hearer must ground the
speaker   s utterances. to ground means to acknowledge, to make it clear that the
hearer has understood the speaker   s meaning and intention. people need closure or
grounding for non-linguistic actions as well. for example, why does a well-designed
elevator button light up when it   s pressed? because this indicates to the elevator
traveler that she has successfully called the elevator. clark (1996) phrases this need
for closure as follows, after norman (1988):

principle of closure. agents performing an action require evidence, suf   cient
for current purposes, that they have succeeded in performing it.

common
ground

grounding

grounding is also important when the hearer needs to indicate that the speaker
has not succeeded. if the hearer has problems in understanding, she must indicate
these problems to the speaker, again so that mutual understanding can eventually be
achieved.

clark and schaefer (1989) point out a continuum of methods the hearer b can

use to ground the speaker a   s utterance, ordered from weakest to strongest:

continued attention: b shows she is continuing to attend and therefore remains satis   ed with

next contribution:
acknowledgment:

demonstration:

display:

a   s presentation.
b starts in on the next relevant contribution.
b nods or says a continuer like uh-huh, yeah, or the like, or an assess-
ment like that   s great.
b demonstrates all or part of what she has understood a to mean, for
example, by reformulating (id141) a   s utterance or by collabo-
rative completion of a   s utterance.
b displays verbatim all or part of a   s presentation.

let   s look for examples of grounding in a conversation between a human travel

agent and a human client in fig. 25.2.

25.1

    dialog acts

449

seattle.

. . . i need to travel in may.

c1:
a1: and, what day in may did you want to travel?
c2: ok uh i need to be there for a meeting that   s from the 12th to the 15th.
a2: and you   re    ying into what city?
c3:
a3: and what time would you like to leave pittsburgh?
c4: uh id48 i don   t think there   s many options for non-stop.
a4:
c5: what are they?
a5:

right. there   s three non-stops today.

the    rst one departs pgh at 10:00am arrives seattle at 12:05 their time. the
second    ight departs pgh at 5:55pm, arrives seattle at 8pm. and the last
   ight departs pgh at 8:15pm arrives seattle at 10:28pm.

c6: ok i   ll take the 5ish    ight on the night before on the 11th.
a6: on the 11th? ok. departing at 5:55pm arrives seattle at 8pm, u.s. air    ight

115.
c7: ok.
figure 25.2 part of a conversation between a travel agent (a) and client (c).

utterance a1 shows the strongest form of grounding, in which the hearer dis-

plays understanding by repeating verbatim part of the speaker   s words: in may.1

this particular fragment doesn   t have an example of an acknowledgment, but

there   s an example in another fragment:

backchannel
continuer

c: he wants to    y from boston to baltimore
a: uh huh

the word uh-huh here is a backchannel, also called a continuer or an acknowl-
edgment token. a backchannel is a (short) optional utterance that acknowledges the
content of the utterance of the other and that doesn   t require an acknowledgment by
the other (yngve 1970, jefferson 1984, schegloff 1982, ward and tsukahara 2000).
the third grounding method is to start in on the relevant next contribution, for
example in fig. 25.2, where the speaker asks a question (a2) and the hearer (c3)
answers it.

in a more subtle act of grounding, the speaker can combine this method with the
previous one. for example, notice that whenever the client answers a question, the
agent begins the next question with and. the and indicates to the client that the
agent has successfully understood the answer to the last question.

speech acts are important for practical id71, which need to distin-
guish a statement from a directive, and which must distinguish (among the many
kinds of directives) an order to do something from a question asking for informa-
tion. grounding is also crucial in id71. consider the unnaturalness of this
example from cohen et al. (2004):
(25.1) system: did you want to review some more of your personal pro   le?

caller: no.
system: what   s next?

without an acknowledgment, the caller doesn   t know that the system has under-
stood her    no   . the use of okay below adds grounding, making (25.2) a much more
natural response than (25.1):

1 as ken forbus points out (p.c.), although verbatim repetition may be the strongest form of grounding
for humans, it   s possible that demonstration (e.g., reformulating) might be more powerful for a conversa-
tional agent, since it demonstrates understanding in a way that verbatim repetition does not.

450 chapter 25

    advanced id71

(25.2) system: did you want to review some more of your personal pro   le?

caller: no.
system: okay, what   s next?

tag
thank
greet
introduce
bye
request-comment
suggest
reject
accept
request-suggest
init
give reason
feedback
deliberate
confirm
clarify
digress
motivate
garbage

example
thanks
hello dan
it   s me again
alright bye
how does that look?
from thirteenth through seventeenth june
no friday i   m booked all day
saturday sounds    ne
what is a good day of the week for you?
i wanted to make an appointment with you
because i have meetings all afternoon
okay
let me check my calendar here
okay, that would be wonderful
okay, do you mean tuesday the 23rd?
[we could meet for lunch] and eat lots of ice cream
we should go to visit our subsidiary in munich
oops, i-

dialog act

figure 25.3
verbmobil-1 system (jekat et al., 1995).

the 18 high-level dialog acts for a meeting scheduling task, from the

the ideas of speech acts and grounding are combined in a single kind of action
called a dialog act, a tag which represents the interactive function of the sentence
being tagged. different types of id71 require labeling different kinds of
acts, and so the tagset   de   ning what a dialog act is exactly    tends to be designed
for particular tasks.

figure 25.3 shows a domain-speci   c tagset for the task of two people scheduling
meetings. it has tags speci   c to the domain of scheduling, such as suggest, used
for the proposal of a particular date to meet, and accept and reject, used for
acceptance or rejection of a proposal for a date, but also tags that have more general
function, like clarify, used to request a user to clarify an ambiguous proposal.

sys user description

tag
hello(a = x,b = y, ...) (cid:88) (cid:88)
inform(a = x,b = y, ...) (cid:88) (cid:88)
(cid:88) (cid:88)
request(a,b = x, ...)
   (cid:88)
reqalts(a = x, ...)
confirm(a = x,b = y, ...) (cid:88) (cid:88)
(cid:88)   
confreq(a = x, ...,d)
(cid:88)   
select(a = x,a = y)
affirm(a = x,b = y, ...) (cid:88) (cid:88)
   (cid:88)
negate(a = x)
   (cid:88)
deny(a = x)
(cid:88) (cid:88)
bye()

open a dialog and give info a = x,b = y, ...
give info a = x,b = y, ...
request value for a given b = x, ...
request alternative with a = x, ...
explicitly con   rm a = x,b = y, ...
implicitly con   rm a = x, ... and request value of d
implicitly con   rm a = x, ... and request value of d
af   rm and give further info a = x,b = y, ...
negate and give corrected value a = x
deny that a = x
close a dialog

figure 25.4 dialog acts used by the his restaurant id126 of young et al.
(2010). the sys and user columns indicate which acts are valid as system outputs and user
inputs, respectively.

figure 25.4 shows a tagset for a restaurant id126, and fig. 25.5
shows these tags labeling a sample dialog from the his system (young et al., 2010).

this example also shows the content of each dialog acts, which are the slot    llers
being communicated.

25.1

    dialog acts

451

utterance

u: hi, i am looking for somewhere to eat.
s: you are looking for a restaurant. what

type of food do you like?

u: i   d like an italian somewhere near the

dialog act
hello(task = find,type=restaurant)
confreq(type = restaurant, food)

inform(food = italian, near=museum)

s: roma is a nice italian restaurant near

u: is it reasonably priced?
s: yes, roma is in the moderate price

museum.

the museum.

range.

inform(name = "roma", type = restaurant,
food = italian, near = museum)
confirm(pricerange = moderate)
affirm(name = "roma", pricerange =
moderate)
request(phone)
inform(name = "roma", phone = "385456")
bye()

u: what is the phone number?
s: the number of roma is 385456.
u: ok, thank you goodbye.
figure 25.5 a sample dialog from the his system of young et al. (2010) using the dialog acts in fig. 25.4.

conversational
analysis

adjacency pair

side sequence
subdialog

dialog acts don   t just appear discretely and independently; conversations have
structure, and dialog acts re   ect some of that structure. one aspect of this struc-
ture comes from the    eld of conversational analysis or ca (sacks et al., 1974)
which focuses on interactional properties of human conversation. ca de   nes ad-
jacency pairs (schegloff, 1968) as a pairing of two dialog acts, like questions
and answers, proposal and acceptance (or rejection), compliments and
downplayers, greeting and greeting.

the structure, composed of a    rst pair part and a second pair part, can help
dialog-state models decide what actions to take. however, dialog acts aren   t always
followed immediately by their second pair part. the two parts can be separated by a
side sequence (jefferson 1972, schegloff 1972). one very common side sequence
in id71 is the clari   cation question, which can form a subdialog be-
tween a request and a response as in the following example caused by speech
recognition errors:

user:

system:
user:
system:

what do you have going to unknown word on the 5th?

let   s see, going where on the 5th?
going to hong kong.

ok, here are some    ights...

pre-sequence

another kind of dialog structure is the pre-sequence, like the following example
where a user starts with a question about the system   s capabilities (   can you make
train reservations   ) before making a request.

can you make train reservations?

user:
system: yes i can.
user:

great, i   d like to reserve a seat on the 4pm train to new york.

a dialog-state model must be able to both recognize these kinds of structures

and make use of them in interacting with users.

452 chapter 25

    advanced id71

25.2 dialog state: interpreting dialog acts

the job of the dialog-state tracker is to determine both the current state of the frame
(the    llers of each slot), as well as the user   s most recent dialog act. note that the
dialog-state includes more than just the slot-   llers expressed in the current sentence;
it includes the entire state of the frame at this point, summarizing all of the user   s
constraints. the following example from mrk  si  c et al. (2017) shows the required
output of the dialog state tracker after each turn:

user:

i   m looking for a cheaper restaurant
inform(price=cheap)

system: sure. what kind - and where?
user:

thai food, somewhere downtown
inform(price=cheap, food=thai, area=centre)

system: the house serves cheap thai food
user: where is it?

inform(price=cheap, food=thai, area=centre); request(address)

system: the house is at 106 regent street

how can we interpret a dialog act, deciding whether a given input is a ques-
tion, a statement, or a suggest (directive)? surface syntax seems like a use-
ful cue, since yes-no questions in english have aux-inversion (the auxiliary verb
precedes the subject), statements have declarative syntax (no aux-inversion), and
commands have no syntactic subject:
(25.3) yes-no question will breakfast be served on usair 1557?

statement
command

i don   t care about lunch.
show me    ights from milwaukee to orlando.

alas, the mapping from surface form to dialog act is complex. for example, the
following utterance looks grammatically like a yes-no question meaning some-
thing like are you capable of giving me a list of. . . ?:
(25.4) can you give me a list of the    ights from atlanta to boston?

in fact, however, this person was not interested in whether the system was capa-
ble of giving a list; this utterance was a polite form of a request, meaning some-
thing like please give me a list of. . . . what looks on the surface like a question
can really be a request.

conversely, what looks on the surface like a statement can really be a ques-
tion. the very common check question (carletta et al. 1997, labov and fan-
shel 1977) asks an interlocutor to con   rm something that she has privileged knowl-
edge about. checks have declarative surface form:

a open-option i was wanting to make some arrangements for a trip that i   m going

b hold

b check
a accept

to be taking uh to la uh beginning of the week after next.
ok uh let me pull up your pro   le and i   ll be right with you here.
[pause]
and you said you wanted to travel next week?
uh yes.

indirect speech
act

utterances that use a surface statement to ask a question or a surface question
to issue a request are called indirect speech acts. these indirect speech acts have a

25.2

    dialog state: interpreting dialog acts

453

rich literature in philosophy, but viewed from the perspective of dialog understand-
ing, indirect speech acts are merely one instance of the more general problem of
determining the dialog act function of a sentence.

many features can help in this task. to give just one example, in spoken-
language systems, id144 or intonation (chapter ??) is a helpful cue. id144
or intonation is the name for a particular set of phonological aspects of the speech
signal the tune and other changes in the pitch (which can be extracted from the fun-
damental frequency f0) the accent, stress, or loudness (which can be extracted from
energy), and the changes in duration and rate of speech. so, for example, a rise
in pitch at the end of the utterance is a good cue for a yes-no question, while
declarative utterances (like statements) have    nal lowering: a drop in f0 at the
end of the utterance.

25.2.1 sketching an algorithm for dialog act interpretation
since dialog acts places some constraints on the slots and values, the tasks of dialog-
act detection and slot-   lling are often performed jointly. consider the task of deter-
mining that

i   d like cantonese food near the mission district

has the structure

inform(food=cantonese,area=mission)).

the joint dialog act interpretation/slot    lling algorithm generally begins with
a    rst pass classi   er to decide on the dialog act for the sentence. in the case of
the example above, this classi   er would choosing inform from among the set of
possible dialog acts in the tag set for this particular task. dialog act interpretation is
generally modeled as a supervised classi   cation task, trained on a corpus in which
each utterance is hand-labeled for its dialog act. the classi   er can be neural or
feature-based; if feature-based, typical features include unigrams and bigrams (show
me is a good cue for a request, are there for a question), embeddings, parse
features, punctuation, dialog context, and the prosodic features described above.

a second pass classi   er might use the sequence-model algorithms for slot-   ller
extraction from section 24.2.2 of chapter 24, such as lstm-based iob tagging or
crfs or a joint lstm-crf. alternatively, a multinominal classi   er can be used to
choose between all possible slot-value pairs, again either neural such as a bi-lstm
or convolutional net, or feature-based using any of the feature functions de   ned in
chapter 24. this is possible since the domain ontology for the system is    xed, so
there is a    nite number of slot-value pairs.

25.2.2 a special case: detecting correction acts
some dialog acts are important because of their implications for dialog control. if a
dialog system misrecognizes or misunderstands an utterance, the user will generally
correct the error by repeating or reformulating the utterance. detecting these user
correction acts is therefore quite important. ironically, it turns out that corrections
are actually harder to recognize than normal sentences! in fact, corrections in one
early dialog system (the toot system) had double the asr word error rate of non-
corrections swerts et al. (2000)! one reason for this is that speakers sometimes
use a speci   c prosodic style for corrections called hyperarticulation, in which the
utterance contains some exaggerated energy, duration, or f0 contours, such as i said
bal-ti-more, not boston (wade et al. 1992, levow 1998, hirschberg et al. 2001).

id144
intonation

   nal lowering

user correction
acts

hyperarticula-
tion

454 chapter 25

    advanced id71

even when they are not hyperarticulating, users who are frustrated seem to speak in
a way that is harder for speech recognizers (goldberg et al., 2003).

what are the characteristics of these corrections? user corrections tend to be
either exact repetitions or repetitions with one or more words omitted, although they
may also be paraphrases of the original utterance. (swerts et al., 2000). detecting
these reformulations or correction acts can be done by any classi   er; some stan-
dard features used for this task are shown below (levow 1998, litman et al. 1999,
hirschberg et al. 2001, bulyko et al. 2005, awadallah et al. 2015):

lexical features
semantic features

phonetic features

prosodic features

asr features

words like    no   ,    correction   ,    i don   t   , or even swear words, utterance length
overlap between the candidate correction act and the user   s prior utterance (computed
by word overlap or via cosines over embedding vectors)
phonetic overlap between the candidate correction act and the user   s prior utterance
(i.e.    whatsapp    may be incorrectly recognized as    what   s up   )
hyperarticulation, increases in f0 range, pause duration, and word duration, generally
normalized by the values for previous sentences
asr con   dence, language model id203

25.3 dialog policy

dialog policy

the goal of the dialog policy is to decide what action the system should take next,
that is, what dialog act to generate. we begin in the next section by introducing one
speci   c dialog policy decision, relating to con   rmation: how we con   rm to the user
what we think she said. we then sketch a basic policy algorithm that could apply
to all decisions. finally, once a speech act has been generated, the natural language
generation component needs to generate the text of a response to the user.

25.3.1 generating dialog acts: con   rmation and rejection
modern id71 often make mistakes. it is therefore important for dialog
systems to make sure that they have achieved the correct interpretation of the user   s
input. this is generally done by two methods: con   rming understandings with the
user and rejecting utterances that the system is likely to have misunderstood.

various strategies can be employed for con   rmation with the user. when using
the explicit con   rmation strategy, a system asks the user a direct question to con-
   rm the system   s understanding, like the two examples below in which the system
asks a (boldface) yes-no con   rmation questions:

explicit
con   rmation

s: which city do you want to leave from?
u: baltimore.
s: do you want to leave from baltimore?
u: yes.
u: i   d like to    y from denver colorado to new york city on september

twenty    rst in the morning on united airlines

s: let   s see then. i have you going from denver colorado to new york

on september twenty    rst. is that correct?

u: yes

implicit
con   rmation

when using the implicit con   rmation strategy, a system instead uses the demon-

stration or display grounding strategies described above, repeating back the system   s
understanding as part of asking the next question, as in the two examples below:

25.3

    dialog policy

455

i want to travel to berlin

u:
s: when do you want to travel to berlin?
u2: hi i   d like to    y to seattle tuesday morning
a3: traveling to seattle on tuesday, august eleventh in the morning.

your full name?

explicit and implicit con   rmation have complementary strengths. explicit con-
   rmation makes it easier for users to correct the system   s misrecognitions since a
user can just answer    no    to the con   rmation question. but explicit con   rmation is
awkward and increases the length of the conversation (danieli and gerbino 1995,
walker et al. 1998). the explicit con   rmation dialog fragments above sound non-
natural and de   nitely non-human; implicit con   rmation is much more conversation-
ally natural.

con   rmation is just one kind of conversational action by which a system can
express lack of understanding. another option is rejection, in which a system gives
the user a prompt like i   m sorry, i didn   t understand that.

sometimes utterances are rejected multiple times. this might mean that the user
is using language that the system is unable to follow. thus, when an utterance is
rejected, systems often follow a strategy of progressive prompting or escalating
detail (yankelovich et al. 1995, weinschenk and barker 2000), as in this example
from cohen et al. (2004):

system: when would you like to leave?
caller: well, um, i need to be in new york in time for the    rst world series game.
system: <reject>. sorry, i didn   t get that. please say the month and day you   d like

caller:

to leave.
i wanna go on october    fteenth.

in this example, instead of just repeating    when would you like to leave?   , the
rejection prompt gives the caller more guidance about how to formulate an utter-
ance the system will understand. these you-can-say help messages are important in
helping improve systems    understanding performance (bohus and rudnicky, 2005).
if the caller   s utterance gets rejected yet again, the prompt can re   ect this (   i still
didn   t get that   ), and give the caller even more guidance.

an alternative strategy for error handling is rapid reprompting, in which the
system rejects an utterance just by saying    i   m sorry?    or    what was that?    only
if the caller   s utterance is rejected a second time does the system start applying
progressive prompting. cohen et al. (2004) summarize experiments showing that
users greatly prefer rapid reprompting as a    rst-level error prompt.

various factors can be used as features to the dialog policy in deciding whether
to use explicit con   rmation, implicit con   rmation, or rejection. for example, the
con   dence that the asr system assigns to an utterance can be used by explicitly
con   rming low-con   dence sentences. recall from page ?? that con   dence is a met-
ric that the speech recognizer can assign to its transcription of a sentence to indi-
cate how con   dent it is in that transcription. con   dence is often computed from
the acoustic log-likelihood of the utterance (greater id203 means higher con   -
dence), but prosodic features can also be used in con   dence prediction. for example,

rejection

progressive
prompting

rapid
reprompting

456 chapter 25

    advanced id71

utterances with large f0 excursions or longer durations, or those preceded by longer
pauses, are likely to be misrecognized (litman et al., 2000).

another common feature in con   rmation is the cost of making an error. for ex-
ample, explicit con   rmation is common before a    ight is actually booked or money
in an account is moved. systems might have a four-tiered level of con   dence with
three thresholds   ,    , and   :

<    low con   dence
reject
       above the threshold
con   rm explicitly
       high con   dence
con   rm implictly
       very high con   dence don   t con   rm at all

25.4 a simple policy based on local context

the goal of the dialog policy at turn i in the conversation is to predict which action
ai to take, based on the entire dialog state. the state could mean the entire sequence
of dialog acts from the system (a) and from the user (u), in which case the task
would be to compute:

  ai = argmax
ai   a

p(ai|(a1,u1, ...,ai   1,ui   1)

(25.5)

we can simplify this by maintaining as the dialog state mainly just the set of
slot-   llers that the user has expressed, collapsing across the many different conver-
sational paths that could lead to the same set of    lled slots.

such a policy might then just condition on the current state of the frame framei

(which slots are    lled and with what) and the last turn by the system and user:

  ai = argmax
ai   a

p(ai|framei   1,ai   1,ui   1)

(25.6)

given a large enough corpus of conversations, these probabilities can be esti-
mated by your favorite classi   er. getting such enormous amounts of data can be
dif   cult, and often involves building user simulators to generate arti   cial conversa-
tions to train on.

25.5 id86 in the dialog-state model

content
planning
sentence
realization

once a dialog act has been decided, we need to generate the text of the response
to the user. the task of id86 (id86) in the information-state
architecture is often modeled in two stages, content planning (what to say), and
sentence realization (how to say it).

here we   ll assume content planning has been done by the dialog policy, which
has chosen the dialog act to generate, and perhaps also chosen some some additional
attributes (slots and values) that the planner wants to implicitly con   rm to the user.
fig. 25.6 shows a sample input structure from the policy/content planner, and one
example of a resulting sentence that the sentence realizer could generate from this
structure.

let   s walk through the sentence realization stage for the example in fig. 25.6,
which comes from the classic information state statistical id86 system of oh and

25.5

    id86 in the dialog-state model

457

figure 25.6 an input frame to id86 and a resulting output sentence, in the communicator
system of oh and rudnicky (2000).

query arrive city
query arrive time
query con   rm
query depart date
query depart time
query pay by card
query preferred airport
query return date
query return time
hotel car info
figure 25.7 dialog acts in the cmu communicator system of oh and rudnicky (2000).

hotel hotel chain
hotel hotel info
hotel need car
hotel need hotel
hotel where
inform airport
inform con   rm utterance
inform epilogue
inform    ight
inform    ight another

inform    ight earlier
inform    ight earliest
inform    ight later
inform    ight latest
inform    ight returning
inform not avail
inform num    ights
inform price
other

rudnicky (2000), part of the cmu communicator travel planning dialog system.
notice    rst that the policy has decided to generate the dialog act query with the
argument depart time. fig. 25.7 lists the dialog acts in the oh and rudnicky
(2000) system, each of which combines an act with a potential argument. the input
frame in fig. 25.6 also speci   es some additional    lled slots that should be included
in the sentence to the user (depart airport bos, and the depart date).

the sentence realizer acts in two steps.

it will    rst generate a delexicalized

string like:

what time on [depart date] would you like to leave [depart airport]?

delexicalization is the process of replacing speci   c words with a generic rep-
resentation of their slot types. a delexicalized sentence is much easier to generate
since we can train on many different source sentences from different speci   c dates
and airports. then once we   ve generating the delexicalized string, we can simply use
the input frame from the content planner to relexicalize (   ll in the exact departure
date and airport).

to generate the delexicalized sentences, the sentence realizer uses a large corpus
of human-human travel dialogs that were labeled with the dialog acts from fig. 25.7
and the slots expressed in each turn, like the following:

delexicalized

relexicalize

query depart time and what time would you like to leave [depart city pittsburgh]?
query arrive city and you   re    ying into what city?
query arrive time what time on [arrive date may 5]?
inform flight

the    ight departs [depart airport pgh] at [depart time 10 am] and arrives
[arrive city seattle] at [arrive time 12:05 their time].

this corpus is then delexicalized, and divided up into separate corpora for each
dialog act. thus the delexicalized corpus for one dialog act, query depart time
might be trained on examples like:

4.1.template-basedgenerationourid86modulestartedo   witharound50templates.thenumberoftemplatesgrewasweaddedmorefunctionalitytooursystem.thelargestexpansioncamewiththeadditionofa      help      speechact,whichadded16templatestoprovidecontext-sensitivehelpmessages.additionalinformationaboutthetemplatesystemisavailableinoh(2000).notethattemplatesarenotsimplesentenceframeswithvariableslots.theyalsoneedtoincludeacomputationalcomponentthatdealswithoptionsforexample,forthetemplate      whattimewouldyouliketotravelfrom{departure_city}on{depar-ture_date}?      ,iftheinputframedidnotcontainvaluesfortheattributes{departure_city}and{departure_date},insteadofgeneratingtheungrammaticalsentence      whattimewouldyouliketotravelfromon?      ,itwouldgenerate      whattimewouldyouliketotravel?      .thisreducesthenumberoftemplatessignificantly,butonlyattheexpenseofintroducingmorecomplexitytothetemplates,especiallyfortemplatesthatcanhaveasmanyastendifferentattributes.hence,theamountoftimethedeveloperneedstospendoncraftingandmaintainingthetemplatesdoesnotdecreasesignificantly.atonepoint,thenumberoftemplatesgrewtonearlyonehundred,someofthemverycomplexandcumbersometomaintain.axelrod(2000)hasalludedtosimilarrequirementsinthesystemthathehasdescribed.4.2.developmentofcorpus-basedstochasticgeneratorwhatisperhapsmoreimportantthanreducingdevelopmenttimeisbeingabletogenerateutterancesthatpromoteanaturalinteractionwiththeuser.oneofthedi   -cultiesforatemplatewriterischoosingtherightwords,thetemplatesystem   sequiv-alentoflexicalselection.often,thewordsthatthetemplatewriterchoosesforagivenutterancearedi   erentfromwhatthedomainexpertwoulduse.thismismatchmayhamperasmoothinteractionbecausewhenasystemutterancecontainsunfamiliarwordsinthatdomain,notonlydoesitsoundunnatural,butitmayalsoleadtheusertoconfusionoraninappropriateresponse.onesolutionmightbetobasethegeneratoronacorpusoftask-orientedhuman   humanconversationsbetweenadomainexpertandaclient.wecould,forexample,taketheexpert   sutterancesandusethemdirectlyastemplates.thisisverysimple,butisnotpractical,asonewouldneedto   ndanutteranceforeverypossiblecombinationofattributes.thestatisticalid165languagemodelprovidesanalternativerepresentation.theid165languagemodelhastheadvantagethatitissimpletobuildandunderstand,andfigure3.aninputframetoid86inthecommunicator.392a.h.ohanda.i.rudnicky458 chapter 25

    advanced id71
and what time would you like to leave depart city?
when would you like to leave depart city?
when would you like to leave?
what time do you want to leave on depart date?
ok, on depart date, what time do you want to leave?

a distinct id165 grammar is then trained for each dialog act. now, given
the dialog act query depart time, the system samples random sentences from
this language model. recall from the    shannon    exercise of 46 that this works
(assuming a bigram lm) by    rst selecting a bigram (< s >, < w >) according to its
bigram id203 in the language model, then drawing a bigram starting with <
w > according to its bigram id203, and so on until a full sentence is generated.
the id203 of each successive word wi being generated from utterance class u
is thus

p(wi) = p(wi|wi   1,wi   2, ...,wi   (n   1),u)

(25.7)

each of these randomly sampled sentences is then assigned a score based on heuris-
tic rules that penalize sentences that are too short or too long, repeat slots, or lack
some of the required slots from the input frame (in this case, depart airport and de-
part date). the best scoring sentence is then chosen. let   s suppose in this case we
produce the following (delexicalized) sentence:

what time on depart date would you like to leave depart airport?

this sentence is then relexicalized from the true values in the input frame, re-

sulting in the    nal sentence:

what time on october    fth would you like to leave boston?

clari   cation
questions

modern implementations of the model replace the simplistic id165 part of the
generator with neural models, which similarly learn to map from an input frame to
a resulting sentence (wen et al. 2015a, wen et al. 2015b).

it   s also possible to design id86 algorithms that are speci   c to a particular di-
alog act. for example, consider the task of generating clari   cation questions, in
cases where the id103 fails to understand some part of the user   s ut-
terance. while it is possible to use the generic dialog act reject (   please repeat   ,
or    i don   t understand what you said   ), studies of human conversations show that
humans instead use targeted clari   cation questions that reprise elements of the mis-
understanding (purver 2004, ginzburg and sag 2000, stoyanchev et al. 2013).

for example, in the following hypothetical example the system reprises the
words    going    and    on the 5th    to make it clear which aspect of the user   s turn
the system needs to be clari   ed:

user: what do you have going to unknown word on the 5th?
system: going where on the 5th?

targeted clari   cation questions can be created by rules (such as replacing    go-
ing to unknown word    with    going where   ) or by building classi   ers to guess
which slots might have been misrecognized in the sentence (chu-carroll and car-
penter 1999, stoyanchev et al. 2014, stoyanchev and johnston 2015).

25.6

    deep id23 for dialog

459

25.6 deep id23 for dialog

tbd

25.7 summary

    in dialog, speaking is a kind of action; these acts are referred to as speech
acts. speakers also attempt to achieve common ground by acknowledging
that they have understand each other. the dialog act combines the intuition
of speech acts and grounding acts.

    the dialog-state or information-state architecture augments the frame-and-
slot state architecture by keeping track of user   s dialog acts and includes a
policy for generating its own dialog acts in return.

    policies based on id23 architecture like the mdp and pomdp
offer ways for future dialog reward to be propagated back to in   uence policy
earlier in the dialog manager.

bibliographical and historical notes

the idea that utterances in a conversation are a kind of action being performed by
the speaker was due originally to the philosopher wittgenstein (1953) but worked out
more fully by austin (1962) and his student john searle. various sets of speech acts
have been de   ned over the years, and a rich linguistic and philosophical literature
developed, especially focused on explaining the use of indirect speech acts.

the idea of dialog acts draws also from a number of other sources, including
the ideas of adjacency pairs, pre-sequences, and other aspects of the international
properties of human conversation developed in the    eld of conversation analysis
(see levinson (1983) for an introduction to the    eld).

this idea that acts set up strong local dialog expectations was also pre   gured by

firth (1935, p. 70), in a famous quotation:

most of the give-and-take of conversation in our everyday life is stereotyped
and very narrowly conditioned by our particular type of culture. it is a sort
of roughly prescribed social ritual, in which you generally say what the other
fellow expects you, one way or the other, to say.

another important research thread modeled dialog as a kind of collaborative be-
havior, including the ideas of common ground (clark and marshall, 1981), reference
as a collaborative process (clark and wilkes-gibbs, 1986), joint intention (levesque
et al., 1990), and shared plans (grosz and sidner, 1980).

the information state model of dialog was also strongly informed by analytic
work on the linguistic properties of dialog acts and on methods for their detection
(sag and liberman 1975, hinkelman and allen 1989, nagata and morimoto 1994,
goodwin 1996, chu-carroll 1998, shriberg et al. 1998, stolcke et al. 2000, gravano
et al. 2012).

460 chapter 25

    advanced id71

bdi

two important lines of research focused on the computational properties of con-
versational structure. one line,    rst suggested at by bruce (1975), suggested that
since speech acts are actions, they should be planned like other actions, and drew
on the ai planning literature (fikes and nilsson, 1971). an agent seeking to    nd
out some information can come up with the plan of asking the interlocutor for the
information. an agent hearing an utterance can interpret a speech act by running
the planner    in reverse   , using id136 rules to infer from what the interlocutor
said what the plan might have been. plan-based models of dialog are referred to as
bdi models because such planners model the beliefs, desires, and intentions (bdi)
of the agent and interlocutor. bdi models of dialog were    rst introduced by allen,
cohen, perrault, and their colleagues in a number of in   uential papers showing how
speech acts could be generated (cohen and perrault, 1979) and interpreted (perrault
and allen 1980, allen and perrault 1980). at the same time, wilensky (1983) intro-
duced plan-based models of understanding as part of the task of interpreting stories.
another in   uential line of research focused on modeling the hierarchical struc-
ture of dialog. grosz   s pioneering (1977) dissertation    rst showed that    task-oriented
dialogs have a structure that closely parallels the structure of the task being per-
formed    (p. 27), leading to her work with sidner and others showing how to use
similar notions of intention and plans to model discourse structure and coherence in
dialog. see, e.g., lochbaum et al. (2000) for a summary of the role of intentional
structure in dialog.

the idea of applying id23 to dialog    rst came out of at&t
and bell laboratories around the turn of the century with work on mdp dialog sys-
tems (walker 2000, levin et al. 2000, singh et al. 2002) and work on cue phrases,
id144, and rejection and con   rmation. id23 research turned
quickly to the more sophisticated pomdp models (roy et al. 2000, lemon et al. 2006,
williams and young 2007) applied to small slot-   lling dialog tasks. [history of deep
id23 here.]

to be continued

chapter

26 id103 and syn-

thesis

placeholder

461

appendices

463

464 appendix a     id48

chapter

a id48

chapter 8 introduced the hidden markov model and applied it to part of speech
tagging. id52 is a fully-supervised learning task, because we have
a corpus of words labeled with the correct part-of-speech tag. but many applications
don   t have labeled data. so in this chapter, we introduce the full set of algorithms for
id48s, including the key unsupervised learning algorithm for id48, the forward-
backward algorithm. we   ll repeat some of the text from chapter 8 for readers who
want the whole story laid out in a single chapter.

a.1 markov chains

markov chain

the id48 is based on augmenting the markov chain. a markov chain is a model
that tells us something about the probabilities of sequences of random variables,
states, each of which can take on values from some set. these sets can be words, or
tags, or symbols representing anything, like the weather. a markov chain makes a
very strong assumption that if we want to predict the future in the sequence, all that
matters is the current state. the states before the current state have no impact on the
future except via the current state. it   s as if to predict tomorrow   s weather you could
examine today   s weather but you weren   t allowed to look at yesterday   s weather.

(a)

(b)

figure a.1 a markov chain for weather (a) and one for words (b), showing states and
transitions. a start distribution    is required; setting    = [0.1, 0.7, 0.2] for (a) would mean a
id203 0.7 of starting in state 2 (cold), id203 0.1 of starting in state 1 (hot), etc.

markov
assumption

more formally, consider a sequence of state variables q1,q2, ...,qi. a markov
model embodies the markov assumption on the probabilities of this sequence: that
when predicting the future, the past doesn   t matter, only the present.

markov assumption: p(qi = a|q1...qi   1) = p(qi = a|qi   1)

(a.1)

figure a.1a shows a markov chain for assigning a id203 to a sequence of
weather events, for which the vocabulary consists of hot, cold, and warm. the
states are represented as nodes in the graph, and the transitions, with their probabil-
ities, as edges. the transitions are probabilities: the values of arcs leaving a given

warm3hot1cold2.8.6.1.1.3.6.1.1.3charminguniformlyare.1.4.5.5.5.2.6.2a.2

    the hidden markov model

465

state must sum to 1. figure a.1b shows a markov chain for assigning a probabil-
ity to a sequence of words w1...wn. this markov chain should be familiar; in fact,
it represents a bigram language model, with each edge expressing the id203
p(wi|w j)! given the two models in fig. a.1, we can assign a id203 to any
sequence from our vocabulary.

formally, a markov chain is speci   ed by the following components:

q = q1q2 . . .qn
a = a11a12 . . .an1 . . .ann

   =   1,  2, ...,  n

a set of n states
a transition id203 matrix a, each ai j represent-
ing the id203 of moving from state i to state j, s.t.

an initial id203 distribution over states.   i is the
id203 that the markov chain will start in state i.
some states j may have    j = 0, meaning that they cannot

(cid:80)n
j=1 ai j = 1    i
be initial states. also,(cid:80)n

i=1   i = 1

before you go on, use the sample probabilities in fig. a.1a (with    = [.1, .7.,2])

to compute the id203 of each of the following sequences:

(a.2) hot hot hot hot
(a.3) cold hot cold hot

what does the difference in these probabilities tell you about a real-world weather
fact encoded in fig. a.1a?

a.2 the hidden markov model

hidden

hidden
markov model

a markov chain is useful when we need to compute a id203 for a sequence
of observable events. in many cases, however, the events we are interested in are
hidden: we don   t observe them directly. for example we don   t normally observe
part-of-speech tags in a text. rather, we see words, and must infer the tags from the
word sequence. we call the tags hidden because they are not observed.

a hidden markov model (id48) allows us to talk about both observed events
(like words that we see in the input) and hidden events (like part-of-speech tags) that
we think of as causal factors in our probabilistic model. an id48 is speci   ed by
the following components:

q = q1q2 . . .qn
a = a11 . . .ai j . . .ann

o = o1o2 . . .ot

b = bi(ot )

   =   1,  2, ...,  n

j=1 ai j = 1    i

a set of n states
a transition id203 matrix a, each ai j representing the id203

of moving from state i to state j, s.t.(cid:80)n

a sequence of t observations, each one drawn from a vocabulary v =
v1,v2, ...,vv
a sequence of observation likelihoods, also called emission probabili-
ties, each expressing the id203 of an observation ot being generated
from a state i
an initial id203 distribution over states.   i is the id203 that
the markov chain will start in state i. some states j may have    j = 0,

meaning that they cannot be initial states. also,(cid:80)n

i=1   i = 1

466 appendix a     id48

a    rst-order hidden markov model instantiates two simplifying assumptions.
first, as with a    rst-order markov chain, the id203 of a particular state depends
only on the previous state:

(a.4)
second, the id203 of an output observation oi depends only on the state that

markov assumption: p(qi|q1...qi   1) = p(qi|qi   1)

produced the observation qi and not on any other states or any other observations:

(a.5)
output independence: p(oi|q1 . . .qi, . . . ,qt ,o1, . . . ,oi, . . . ,ot ) = p(oi|qi)
to exemplify these models, we   ll use a task invented by jason eisner (2002).
imagine that you are a climatologist in the year 2799 studying the history of global
warming. you cannot    nd any records of the weather in baltimore, maryland, for
the summer of 2020, but you do    nd jason eisner   s diary, which lists how many ice
creams jason ate every day that summer. our goal is to use these observations to
estimate the temperature every day. we   ll simplify this weather task by assuming
there are only two kinds of days: cold (c) and hot (h). so the eisner task is as
follows:

given a sequence of observations o (each an integer representing the
number of ice creams eaten on a given day)    nd the    hidden    sequence
q of weather states (h or c) which caused jason to eat the ice cream.

figure a.2 shows a sample id48 for the ice cream task. the two hidden states
(h and c) correspond to hot and cold weather, and the observations (drawn from the
alphabet o = {1,2,3}) correspond to the number of ice creams eaten by jason on a
given day.

figure a.2 a hidden markov model for relating numbers of ice creams eaten by jason (the
observations) to the weather (h or c, the hidden variables).

an in   uential tutorial by rabiner (1989), based on tutorials by jack ferguson in
the 1960s, introduced the idea that id48 should be characterized
by three fundamental problems:
problem 1 (likelihood): given an id48    = (a,b) and an observation se-

problem 2 (decoding):

problem 3 (learning):

quence o, determine the likelihood p(o|   ).
given an observation sequence o and an id48    =
(a,b), discover the best hidden state sequence q.
given an observation sequence o and the set of states
in the id48, learn the id48 parameters a and b.

we already saw an example of problem 2 in chapter 8. in the next two sections
we introduce the forward and forward-backward algorithms to solve problems 1
and 3 and give more information on problem 2

   = [.8,.2]cold2hot1b2p(1 | cold)          .5p(2 | cold)    =    .4p(3 | cold)          .1.5.6.5.4p(1 | hot)          .2p(2 | hot)    =    .4p(3 | hot)          .4b1a.3

    likelihood computation: the forward algorithm 467

a.3 likelihood computation: the forward algorithm

our    rst problem is to compute the likelihood of a particular observation sequence.
for example, given the ice-cream eating id48 in fig. a.2, what is the id203
of the sequence 3 1 3? more formally:

computing likelihood: given an id48    = (a,b) and an observa-
tion sequence o, determine the likelihood p(o|   ).
for a markov chain, where the surface observations are the same as the hidden
events, we could compute the id203 of 3 1 3 just by following the states labeled
3 1 3 and multiplying the probabilities along the arcs. for a hidden markov model,
things are not so simple. we want to determine the id203 of an ice-cream
observation sequence like 3 1 3, but we don   t know what the hidden state sequence
is!

let   s start with a slightly simpler situation. suppose we already knew the weather
and wanted to predict how much ice cream jason would eat. this is a useful part
of many id48 tasks. for a given hidden state sequence (e.g., hot hot cold), we can
easily compute the output likelihood of 3 1 3.

let   s see how. first, recall that for id48, each hidden state
produces only a single observation. thus, the sequence of hidden states and the
sequence of observations have the same length. 1

given this one-to-one mapping and the markov assumptions expressed in eq. a.4,
for a particular hidden state sequence q = q0,q1,q2, ...,qt and an observation se-
quence o = o1,o2, ...,ot , the likelihood of the observation sequence is

p(o|q) =

t(cid:89)i=1

p(oi|qi)

(a.6)

the computation of the forward id203 for our ice-cream observation 3 1 3 from
one possible hidden state sequence hot hot cold is shown in eq. a.7. figure a.3
shows a graphic representation of this computation.

p(3 1 3|hot hot cold) = p(3|hot)   p(1|hot)   p(3|cold)

(a.7)

figure a.3 the computation of the observation likelihood for the ice-cream events 3 1 3
given the hidden state sequence hot hot cold.

but of course, we don   t actually know what the hidden state (weather) sequence
was. we   ll need to compute the id203 of ice-cream events 3 1 3 instead by

in a variant of id48s called segmental id48s (in id103) or semi-id48s (in text pro-
1
cessing) this one-to-one mapping between the length of the hidden state sequence and the length of the
observation sequence does not hold.

coldhot3.4hot13.2.1468 appendix a     id48

summing over all possible weather sequences, weighted by their id203. first,
let   s compute the joint id203 of being in a particular weather sequence q and
generating a particular sequence o of ice-cream events. in general, this is

p(o,q) = p(o|q)   p(q) =

t(cid:89)i=1

p(oi|qi)  

t(cid:89)i=1

p(qi|qi   1)

(a.8)

the computation of the joint id203 of our ice-cream observation 3 1 3 and one
possible hidden state sequence hot hot cold is shown in eq. a.9. figure a.4 shows
a graphic representation of this computation.

p(3 1 3,hot hot cold) = p(hot|start)   p(hot|hot)   p(cold|hot)

  p(3|hot)   p(1|hot)   p(3|cold)

(a.9)

figure a.4 the computation of the joint id203 of the ice-cream events 3 1 3 and the
hidden state sequence hot hot cold.

now that we know how to compute the joint id203 of the observations
with a particular hidden state sequence, we can compute the total id203 of the
observations just by summing over all possible hidden state sequences:

p(o) =(cid:88)q

p(o,q) =(cid:88)q

p(o|q)p(q)

(a.10)

for our particular case, we would sum over the eight 3-event sequences cold cold

cold, cold cold hot, that is,

p(3 1 3) = p(3 1 3,cold cold cold) + p(3 1 3,cold cold hot) + p(3 1 3,hot hot cold) + ...

forward
algorithm

for an id48 with n hidden states and an observation sequence of t observa-
tions, there are nt possible hidden sequences. for real tasks, where n and t are
both large, nt is a very large number, so we cannot compute the total observation
likelihood by computing a separate observation likelihood for each hidden state se-
quence and then summing them.

instead of using such an extremely exponential algorithm, we use an ef   cient
o(n2t ) algorithm called the forward algorithm. the forward algorithm is a kind
of id145 algorithm, that is, an algorithm that uses a table to store
intermediate values as it builds up the id203 of the observation sequence. the
forward algorithm computes the observation id203 by summing over the prob-
abilities of all possible hidden state paths that could generate the observation se-
quence, but it does so ef   ciently by implicitly folding each of these paths into a
single forward trellis.

figure a.5 shows an example of the forward trellis for computing the likelihood

of 3 1 3 given the hidden state sequence hot hot cold.

coldhot3.4hot.613.4.2.1a.3

    likelihood computation: the forward algorithm 469

two time steps. the computation in each cell follows eq. a.12:   t ( j) =(cid:80)n

figure a.5 the forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3.
hidden states are in circles, observations in squares. the    gure shows the computation of   t ( j) for two states at
i=1   t   1(i)ai jb j(ot ). the resulting

id203 expressed in each cell is eq. a.11:   t ( j) = p(o1,o2 . . .ot ,qt = j|   ).

each cell of the forward algorithm trellis   t ( j) represents the id203 of be-
ing in state j after seeing the    rst t observations, given the automaton    . the value
of each cell   t ( j) is computed by summing over the probabilities of every path that
could lead us to this cell. formally, each cell expresses the following id203:

  t ( j) = p(o1,o2 . . .ot ,qt = j|   )

(a.11)
here, qt = j means    the tth state in the sequence of states is state j   . we compute
this id203   t ( j) by summing over the extensions of all the paths that lead to
the current cell. for a given state q j at time t, the value   t ( j) is computed as

  t ( j) =

  t   1(i)ai jb j(ot )

(a.12)

n(cid:88)i=1

the three factors that are multiplied in eq. a.12 in extending the previous paths

to compute the forward id203 at time t are

  t   1(i)
ai j
b j(ot )

the previous forward path id203 from the previous time step
the transition id203 from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

consider the computation in fig. a.5 of   2(2), the forward id203 of being
at time step 2 in state 2 having generated the partial observation 3 1. we compute by
extending the    probabilities from time step 1, via two paths, each extension con-
sisting of the three factors above:   1(1)   p(h|c)   p(1|h) and   1(2)   p(h|h)  
p(1|h).
figure a.6 shows another visualization of this induction step for computing the
value in one new cell of the trellis.

we give two formal de   nitions of the forward algorithm:

the pseudocode in

fig. a.7 and a statement of the de   nitional recursion here.

  hchchcp(c|start) * p(3|c).2 * .1p(h|h) * p(1|h).6 * .2p(c|c) * p(1|c).5 * .5p(c|h) * p(1|c).4 * .5p(h|c) * p(1|h).5 * .2p(h|start)*p(3|h).8 * .4  1(2)=.32  1(1) = .02  2(2)= .32*.12 + .02*.1 = .0404  2(1) = .32*.2 + .02*.25 = .069tchq2q1o13o2o313470 appendix a     id48

figure a.6 visualizing the computation of a single element   t (i) in the trellis by summing
all the previous values   t   1, weighted by their transition probabilities a, and multiplying by
the observation id203 bi(ot+1). for many applications of id48s, many of the transition
probabilities are 0, so not all previous states will contribute to the forward id203 of the
current state. hidden states are in circles, observations in squares. shaded nodes are included
in the id203 computation for   t (i).

function forward(observations of len t, state-graph of len n) returns forward-prob

; initialization step

; recursion step

create a id203 matrix forward[n,t]
for each state s from 1 to n do
forward[s,1]     s     bs(o1)
for each time step t from 2 to t do
n(cid:88)
for each state s from 1 to n do

forward[s,t]   
n(cid:88)

s(cid:48) =1

forwardprob   
s=1
return forwardprob

forward[s(cid:48),t     1]     as(cid:48),s     bs(ot )

forward[s,t ]

; termination step

figure a.7 the forward algorithm, where forward[s,t] represents   t (s).

1. initialization:

2. recursion:

  1( j) =    jb j(o1) 1     j     n

  t ( j) =

n(cid:88)i=1

  t   1(i)ai jb j(ot ); 1     j     n,1 < t     t

3. termination:

p(o|   ) =

  t (i)

n(cid:88)i=1

ot-1ota1ja2janja3jbj(ot)  t(j)=   i   t-1(i) aij bj(ot) q1q2q3qnq1qjq2q1q2ot+1ot-2q1q2q3q3qnqn  t-1(n)  t-1(3)  t-1(2)  t-1(1)  t-2(n)  t-2(3)  t-2(2)  t-2(1)a.4 decoding: the viterbi algorithm

a.4

    decoding: the viterbi algorithm 471

decoding
decoder

for any model, such as an id48, that contains hidden variables, the task of deter-
mining which sequence of variables is the underlying source of some sequence of
observations is called the decoding task. in the ice-cream domain, given a sequence
of ice-cream observations 3 1 3 and an id48, the task of the decoder is to    nd the
best hidden weather sequence (h h h). more formally,

decoding: given as input an id48    = (a,b) and a sequence of ob-
servations o = o1,o2, ...,ot ,    nd the most probable sequence of states
q = q1q2q3 . . .qt .

we might propose to    nd the best sequence as follows: for each possible hid-
den state sequence (hhh, hhc, hch, etc.), we could run the forward algorithm
and compute the likelihood of the observation sequence given that hidden state se-
quence. then we could choose the hidden state sequence with the maximum obser-
vation likelihood. it should be clear from the previous section that we cannot do this
because there are an exponentially large number of state sequences.

instead, the most common decoding algorithms for id48s is the viterbi algo-
rithm. like the forward algorithm, viterbi is a kind of id145
that makes uses of a id145 trellis. viterbi also strongly resembles
another id145 variant, the minimum id153 algorithm of
chapter 2.

viterbi
algorithm

figure a.8 the viterbi trellis for computing the best path through the hidden state space for the ice-cream
eating events 3 1 3. hidden states are in circles, observations in squares. white (un   lled) circles indicate illegal
transitions. the    gure shows the computation of vt ( j) for two states at two time steps. the computation in each
cell follows eq. a.14: vt ( j) = max1   i   n   1 vt   1(i) ai j b j(ot ). the resulting id203 expressed in each cell
is eq. a.13: vt ( j) = p(q0,q1, . . . ,qt   1,o1,o2, . . . ,ot ,qt = j|   ).

figure a.8 shows an example of the viterbi trellis for computing the best hidden
state sequence for the observation sequence 3 1 3. the idea is to process the ob-
servation sequence left to right,    lling out the trellis. each cell of the trellis, vt ( j),
represents the id203 that the id48 is in state j after seeing the    rst t obser-
vations and passing through the most probable state sequence q1, ...,qt   1, given the

  hchchcp(c|start) * p(3|c).2 * .1p(h|h) * p(1|h).6 * .2p(c|c) * p(1|c).5 * .5p(c|h) * p(1|c).4 * .5p(h|c) * p(1|h).5 * .2p(h|start)*p(3|h).8 * .4v1(2)=.32v1(1) = .02v2(2)= max(.32*.12, .02*.10) = .038v2(1) = max(.32*.20, .02*.25) = .064tchq2q1o1o2o3313472 appendix a     id48

automaton    . the value of each cell vt ( j) is computed by recursively taking the
most probable path that could lead us to this cell. formally, each cell expresses the
id203

vt ( j) = max
q1,...,qt   1

p(q1...qt   1,o1,o2 . . .ot ,qt = j|   )

(a.13)

note that we represent the most probable path by taking the maximum over all
possible previous state sequences max
. like other id145 algo-
q1,...,qt   1
rithms, viterbi    lls each cell recursively. given that we had already computed the
id203 of being in every state at time t     1, we compute the viterbi id203
by taking the most probable of the extensions of the paths that lead to the current
cell. for a given state q j at time t, the value vt ( j) is computed as

vt ( j) =

nmax
i=1

vt   1(i) ai j b j(ot )

(a.14)

the three factors that are multiplied in eq. a.14 for extending the previous paths to
compute the viterbi id203 at time t are

vt   1(i)
ai j
b j(ot )

the previous viterbi path id203 from the previous time step
the transition id203 from previous state qi to current state q j
the state observation likelihood of the observation symbol ot given
the current state j

function viterbi(observations of len t,state-graph of len n) returns best-path, path-prob

create a path id203 matrix viterbi[n,t]
for each state s from 1 to n do
viterbi[s,1]     s     bs(o1)
backpointer[s,1]   0

for each time step t from 2 to t do
for each state s from 1 to n do

; initialization step

; recursion step

nmax
viterbi[s,t]   
s(cid:48) =1
backpointer[s,t]   

nmax

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )
nargmax
s(cid:48) =1

viterbi[s(cid:48),t     1]     as(cid:48),s     bs(ot )
; termination step

s=1

viterbi[s,t ]

bestpathprob   
bestpathpointer   
bestpath   the path starting at state bestpathpointer, that follows backpointer[] to states back in time
return bestpath, bestpathprob

; termination step

viterbi[s,t ]

nargmax

s=1

figure a.9 viterbi algorithm for    nding optimal sequence of hidden states. given an observation sequence
and an id48    = (a,b), the algorithm returns the state path through the id48 that assigns maximum likelihood
to the observation sequence.

figure a.9 shows pseudocode for the viterbi algorithm. note that the viterbi
algorithm is identical to the forward algorithm except that it takes the max over the
previous path probabilities whereas the forward algorithm takes the sum. note also
that the viterbi algorithm has one component that the forward algorithm doesn   t

a.5

    id48 training: the forward-backward algorithm 473
have: backpointers. the reason is that while the forward algorithm needs to pro-
duce an observation likelihood, the viterbi algorithm must produce a id203 and
also the most likely state sequence. we compute this best state sequence by keeping
track of the path of hidden states that led to each state, as suggested in fig. a.10, and
then at the end backtracing the best path to the beginning (the viterbi backtrace).

viterbi
backtrace

figure a.10 the viterbi backtrace. as we extend each path to a new state account for the next observation,
we keep a backpointer (shown with broken lines) to the best path that led us to this state.

finally, we can give a formal de   nition of the viterbi recursion as follows:

1. initialization:

2. recursion

vt ( j) =

btt ( j) =

3. termination:

v1( j) =    jb j(o1)
bt1( j) = 0

1     j     n
1     j     n

nmax
i=1
nargmax

i=1

vt   1(i)ai j b j(ot ); 1     j     n,1 < t     t

vt   1(i)ai j b j(ot ); 1     j     n,1 < t     t

the best score: p    =
the start of backtrace: qt    =

vt (i)

nmax
i=1
nargmax

i=1

vt (i)

a.5 id48 training: the forward-backward algorithm

we turn to the third problem for id48s: learning the parameters of an id48, that
is, the a and b matrices. formally,

learning: given an observation sequence o and the set of possible
states in the id48, learn the id48 parameters a and b.

  hchchcp(c|start) * p(3|c).2 * .1p(h|h) * p(1|h).6 * .2p(c|c) * p(1|c).5 * .5p(c|h) * p(1|c).4 * .5p(h|c) * p(1|h).5 * .2p(h|start)*p(3|h).8 * .4v1(2)=.32v1(1) = .02v2(2)= max(.32*.12, .02*.10) = .038v2(1) = max(.32*.20, .02*.25) = .064tchq2q1o1o2o3313474 appendix a     id48

the input to such a learning algorithm would be an unlabeled sequence of ob-
servations o and a vocabulary of potential hidden states q. thus, for the ice cream
task, we would start with a sequence of observations o = {1,3,2, ...,} and the set of
hidden states h and c.
the standard algorithm for id48 training is the forward-backward, or baum-
welch algorithm (baum, 1972), a special case of the expectation-maximization
or em algorithm (dempster et al., 1977). the algorithm will let us train both the
transition probabilities a and the emission probabilities b of the id48. em is an
iterative algorithm, computing an initial estimate for the probabilities, then using
those estimates to computing a better estimate, and so on, iteratively improving the
probabilities that it learns.

let us begin by considering the much simpler case of training a fully visible
markov model, we   re know both the temperature and the ice cream count for every
day. that is, imagine we see the following set of input observations and magically
knew the aligned hidden state sequences:
3
hot hot cold

1
cold cold cold

cold hot hot

3

2

1

2

1

2

3

this would easily allow us to compute the id48 parameters just by maximum
likelihood estimation from the training data. first, we can compute    from the count
of the 3 initial hidden states:

  h = 1/3

  c = 2/3

next we can directly compute the a matrix from the transitions, ignoring the    nal
hidden states:

and the b matrix:

p(hot|hot) = 2/3
p(cold|cold) = 1/2

p(cold|hot) = 1/3
p(hot|cold) = 1/2

p(1|hot) = 0/4 = 0
p(2|hot) = 1/4 = .25
p(3|hot) = 3/4 = .75

p(1|cold) = 3/5 = .6
p(2|cold = 2/5 = .4
p(3|cold) = 0

for a real id48, we cannot compute these counts directly from an observation
sequence since we don   t know which path of states was taken through the machine
for a given input. for example, suppose i didn   t tell you the temperature on day 2,
and you had to guess it, but you (magically) had the above probabilities, and the
temperatures on the other days. you could do some bayesian arithmetic with all the
other probabilities to get estimates of the likely temperature on that missing day, and
use those to get expected counts for the temperatures for day 2.

but the real problem is even harder: we don   t know the counts of being in any
of the hidden states!! the baum-welch algorithm solves this by iteratively esti-
mating the counts. we will start with an estimate for the transition and observation
probabilities and then use these estimated probabilities to derive better and better
probabilities. and we   re going to do this by computing the forward id203 for
an observation and then dividing that id203 mass among all the different paths
that contributed to this forward id203.

to understand the algorithm, we need to de   ne a useful id203 related to the
forward id203 and called the backward id203. the backward probabil-

forward-
backward
baum-welch
em

backward
id203

a.5

    id48 training: the forward-backward algorithm 475
ity    is the id203 of seeing the observations from time t + 1 to the end, given
that we are in state i at time t (and given the automaton    ):

  t (i) = p(ot+1,ot+2 . . .ot|qt = i,   )

it is computed inductively in a similar manner to the forward algorithm.

(a.15)

1. initialization:

2. recursion

  t (i) =

n(cid:88)j=1

3. termination:

  t (i) = 1, 1     i     n

ai j b j(ot+1)   t+1( j), 1     i     n,1     t < t

p(o|   ) =

n(cid:88)j=1

   j b j(o1)   1( j)

figure a.11 illustrates the backward induction step.

figure a.11 the computation of   t (i) by summing all the successive values   t+1( j)
weighted by their transition probabilities ai j and their observation probabilities b j(ot+1). start
and end states not shown.

we are now ready to see how the forward and backward probabilities can help
compute the transition id203 ai j and observation id203 bi(ot ) from an ob-
servation sequence, even though the actual path taken through the model is hidden.
let   s begin by seeing how to estimate   ai j by a variant of simple maximum like-

lihood estimation:

  ai j =

expected number of transitions from state i to state j

expected number of transitions from state i

(a.16)

how do we compute the numerator? here   s the intuition. assume we had some
estimate of the id203 that a given transition i     j was taken at a particular
point in time t in the observation sequence. if we knew this id203 for each

ot+1otai1ai2ainai3b1(ot+1)  t(i)=   j   t+1(j) aij  bj(ot+1) q1q2q3qnq1qiq2q1q2ot-1q3qn  t+1(n)  t+1(3)  t+1(2)  t+1(1)b2(ot+1)b3(ot+1)bn(ot+1)476 appendix a     id48

particular time t, we could sum over all times t to estimate the total count for the
transition i     j.
more formally, let   s de   ne the id203   t as the id203 of being in state
i at time t and state j at time t + 1, given the observation sequence and of course the
model:

  t (i, j) = p(qt = i,qt+1 = j|o,   )

(a.17)
to compute   t, we    rst compute a id203 which is similar to   t, but differs in
including the id203 of the observation; note the different conditioning of o
from eq. a.17:

not-quite-  t (i, j) = p(qt = i,qt+1 = j,o|   )

(a.18)

figure a.12 computation of the joint id203 of being in state i at time t and state j at
time t + 1. the    gure shows the various probabilities that need to be combined to produce
p(qt = i,qt+1 = j,o|   ):
the    and    probabilities, the transition id203 ai j and the
observation id203 b j(ot+1). after rabiner (1989) which is c(cid:13)1989 ieee.

figure a.12 shows the various probabilities that go into computing not-quite-  t:
the transition id203 for the arc in question, the    id203 before the arc, the
   id203 after the arc, and the observation id203 for the symbol just after
the arc. these four are multiplied together to produce not-quite-  t as follows:

not-quite-  t (i, j) =   t (i)ai jb j(ot+1)  t+1( j)

(a.19)
to compute   t from not-quite-  t, we follow the laws of id203 and divide by
p(o|   ), since

p(x|y,z) =

p(x,y|z)
p(y|z)

(a.20)

the id203 of the observation given the model is simply the forward proba-
bility of the whole utterance (or alternatively, the backward id203 of the whole
utterance):

p(o|   ) =

n(cid:88)j=1

  t ( j)  t ( j)

(a.21)

ot+2ot+1  t(i)ot-1otaijbj(ot+1) sisj  t+1(j)a.5

    id48 training: the forward-backward algorithm 477

so, the    nal equation for   t is

  t (i, j) =

  t (i)ai jb j(ot+1)  t+1( j)

j=1   t ( j)  t ( j)

(cid:80)n

the expected number of transitions from state i to state j is then the sum over all
t of    . for our estimate of ai j in eq. a.16, we just need one more thing: the total
expected number of transitions from state i. we can get this by summing over all
transitions out of state i. here   s the    nal formula for   ai j:

(a.22)

(a.23)

  ai j = (cid:80)t   1
(cid:80)t   1
t=1 (cid:80)n

t=1   t (i, j)

k=1   t (i,k)

we also need a formula for recomputing the observation id203. this is the
id203 of a given symbol vk from the observation vocabulary v , given a state j:
  b j(vk). we will do this by trying to compute

  b j(vk) =

expected number of times in state j and observing symbol vk

expected number of times in state j

(a.24)

for this, we will need to know the id203 of being in state j at time t, which

we will call   t ( j):

(a.25)
once again, we will compute this by including the observation sequence in the

  t ( j) = p(qt = j|o,   )

id203:

  t ( j) =

p(qt = j,o|   )

p(o|   )

(a.26)

figure a.13 the computation of   t ( j), the id203 of being in state j at time t. note
that    is really a degenerate case of    and hence this    gure is like a version of fig. a.12 with
state i collapsed with state j. after rabiner (1989) which is c(cid:13)1989 ieee.

as fig. a.13 shows, the numerator of eq. a.26 is just the product of the forward

id203 and the backward id203:

  t ( j) =

  t ( j)  t ( j)
p(o|   )

(a.27)

ot+1  t(j)ot-1otsj  t(j)478 appendix a     id48

we are ready to compute b. for the numerator, we sum   t ( j) for all time steps
t in which the observation ot is the symbol vk that we are interested in. for the
denominator, we sum   t ( j) over all time steps t. the result is the percentage of the
t=1 s.t.ot =vid116

times that we were in state j and saw symbol vk (the notation(cid:80)t

   sum over all t for which the observation at time t was vk   ):

  b j(vk) =(cid:80)t

  t ( j)

t=1 s.t.ot =vk
t=1   t ( j)

(cid:80)t

(a.28)

we now have ways in eq. a.23 and eq. a.28 to re-estimate the transition a and ob-
servation b probabilities from an observation sequence o, assuming that we already
have a previous estimate of a and b.

these re-estimations form the core of the iterative forward-backward algorithm.
the forward-backward algorithm (fig. a.14) starts with some initial estimate of the
id48 parameters    = (a,b). we then iteratively run two steps. like other cases of
the em (expectation-maximization) algorithm, the forward-backward algorithm has
two steps: the expectation step, or e-step, and the maximization step, or m-step.
in the e-step, we compute the expected state occupancy count    and the expected
state transition count    from the earlier a and b probabilities. in the m-step, we use
   and    to recompute new a and b probabilities.

e-step
m-step

function forward-backward(observations of len t, output vocabulary v, hidden
state set q) returns id48=(a,b)

    t, i, and j

initialize a and b
iterate until convergence

e-step

  t ( j) =

  t (i, j) =

m-step

  ai j =

  b j(vk) =

  t (i, j)

  t (qf )

  t ( j)  t ( j)
  t (qf )     t and j
  t (i)ai jb j(ot+1)  t+1( j)
t   1(cid:88)
t   1(cid:88)
n(cid:88)
t(cid:88)
t(cid:88)

t=1s.t. ot =vk

  t (i,k)

  t ( j)

k=1

t=1

t=1

  t ( j)

return a, b

t=1

figure a.14 the forward-backward algorithm.

although in principle the forward-backward algorithm can do completely unsu-
pervised learning of the a and b parameters, in practice the initial conditions are
very important. for this reason the algorithm is often given extra information. for
example, for id48-based id103, the id48 structure is often set by
hand, and only the emission (b) and (non-zero) a transition probabilities are trained
from a set of observation sequences o.

a.6 summary

a.6

    summary

479

this chapter introduced the hidden markov model for probabilistic sequence clas-
si   cation.

    id48 (id48s) are a way of relating a sequence of obser-
vations to a sequence of hidden classes or hidden states that explain the
observations.

    the process of discovering the sequence of hidden states, given the sequence
of observations, is known as decoding or id136. the viterbi algorithm is
commonly used for decoding.

    the parameters of an id48 are the a transition id203 matrix and the b
observation likelihood matrix. both can be trained with the baum-welch or
forward-backward algorithm.

bibliographical and historical notes

as we discussed in chapter 8, markov chains were    rst used by markov (1913)
(translation markov 2006), to predict whether an upcoming letter in pushkin   s eu-
gene onegin would be a vowel or a consonant. the hidden markov model was de-
veloped by baum and colleagues at the institute for defense analyses in princeton
(baum and petrie 1966, baum and eagon 1967).

the viterbi algorithm was    rst applied to speech and language processing in the
context of id103 by vintsyuk (1968) but has what kruskal (1983) calls
a    remarkable history of multiple independent discovery and publication   . kruskal
and others give at least the following independently-discovered variants of the algo-
rithm published in four separate    elds:

field
citation
id205
viterbi (1967)
vintsyuk (1968)
speech processing
needleman and wunsch (1970) molecular biology
speech processing
sakoe and chiba (1971)
sankoff (1972)
molecular biology
molecular biology
reichert et al. (1973)
wagner and fischer (1974)
computer science

the use of the term viterbi is now standard for the application of dynamic pro-
gramming to any kind of probabilistic maximization problem in speech and language
processing. for non-probabilistic problems (such as for minimum id153), the
plain term id145 is often used. forney, jr. (1973) wrote an early
survey paper that explores the origin of the viterbi algorithm in the context of infor-
mation and communications theory.

our presentation of the idea that id48 should be characterized
by three fundamental problems was modeled after an in   uential tutorial by rabiner
(1989), which was itself based on tutorials by jack ferguson of ida in the 1960s.
jelinek (1997) and rabiner and juang (1993) give very complete descriptions of the
forward-backward algorithm as applied to the id103 problem. jelinek
(1997) also shows the relationship between forward-backward and em.

480 appendix b     id147 and the noisy channel

chapter

b id147 and the

noisy channel

algernon: but my own sweet cecily, i have never written you any letters.
cecily: you need hardly remind me of that, ernest. i remember only too well
that i was forced to write your letters for you. i wrote always three times a week,
and sometimes oftener.
algernon: oh, do let me read them, cecily?
cecily: oh, i couldn   t possibly. they would make you far too conceited. the
three you wrote me after i had broken off the engagement are so beautiful, and
so badly spelled, that even now i can hardly read them without crying a little.

oscar wilde, the importance of being earnest

like oscar wilde   s fabulous cecily, a lot of people were thinking about spelling
during the last turn of the century. gilbert and sullivan provide many examples. the
gondoliers    giuseppe, for example, worries that his private secretary is    shaky in his
spelling   , while iolanthe   s phyllis can    spell every word that she uses   . thorstein
veblen   s explanation (in his 1899 classic the theory of the leisure class) was that
a main purpose of the    archaic, cumbrous, and ineffective    english spelling system
was to be dif   cult enough to provide a test of membership in the leisure class.

whatever the social role of spelling, we can certainly agree that many more of
us are like cecily than like phyllis. estimates for the frequency of spelling errors
in human-typed text vary from 1-2% for carefully retyping already printed text to
10-15% for web queries.

in this chapter we introduce the problem of detecting and correcting spelling
errors. fixing spelling errors is an integral part of writing in the modern world,
whether this writing is part of texting on a phone, sending email, writing longer
documents, or    nding information on the web. modern spell correctors aren   t perfect
(indeed, autocorrect-gone-wrong is a popular source of amusement on the web) but
they are ubiquitous in pretty much any software that relies on keyboard input.

id147 is often considered from two perspectives. non-word spelling
correction is the detection and correction of spelling errors that result in non-words
(like graffe for giraffe). by contrast, real word id147 is the task of
detecting and correcting spelling errors even if they accidentally result in an actual
word of english (real-word errors). this can happen from typographical errors
(insertion, deletion, transposition) that accidentally produce a real word (e.g., there
for three), or cognitive errors where the writer substituted the wrong spelling of a
homophone or near-homophone (e.g., dessert for desert, or piece for peace).

non-word errors are detected by looking for any word not found in a dictio-
nary. for example, the misspelling graffe above would not occur in a dictionary.
the larger the dictionary the better; modern systems often use enormous dictio-

real-word
errors

candidates

b.1

    the id87

481

naries derived from the web. to correct non-word spelling errors we    rst generate
candidates: real words that have a similar letter sequence to the error. candidate
corrections from the spelling error graffe might include giraffe, graf, gaffe, grail, or
craft. we then rank the candidates using a distance metric between the source and
the surface error. we   d like a metric that shares our intuition that giraffe is a more
likely source than grail for graffe because giraffe is closer in spelling to graffe than
grail is to graffe. the minimum id153 algorithm from chapter 2 will play a
role here. but we   d also like to prefer corrections that are more frequent words, or
more likely to occur in the context of the error. the id87 introduced
in the next section offers a way to formalize this intuition.

real word spelling error detection is a much more dif   cult task, since any word
in the input text could be an error. still, it is possible to use the noisy channel to    nd
candidates for each word w typed by the user, and rank the correction that is most
likely to have been the users original intention.

b.1 the id87

in this section we introduce the id87 and show how to apply it to
the task of detecting and correcting spelling errors. the id87 was
applied to the id147 task at about the same time by researchers at at&t
bell laboratories (kernighan et al. 1990, church and gale 1991) and ibm watson
research (mays et al., 1991).

noisy channel

figure b.1
in the id87, we imagine that the surface form we see is actually
a    distorted    form of an original word passed through a noisy channel. the decoder passes
each hypothesis through a model of this channel and picks the word that best matches the
surface noisy word.

the intuition of the id87 (see fig. b.1) is to treat the misspelled
word as if a correctly spelled word had been    distorted    by being passed through a
noisy communication channel.

this channel introduces    noise    in the form of substitutions or other changes to
the letters, making it hard to recognize the    true    word. our goal, then, is to build a
model of the channel. given this model, we then    nd the true word by passing every
word of the language through our model of the noisy channel and seeing which one
comes the closest to the misspelled word.

decoder noisy wordoriginal wordnoisy channelguessed wordnoisy 1noisy 2noisy nword hyp1word hyp2...word hyp3482 appendix b     id147 and the noisy channel

bayesian

this id87 is a kind of bayesian id136. we see an obser-
vation x (a misspelled word) and our job is to    nd the word w that generated this
misspelled word. out of all possible words in the vocabulary v we want to    nd the
word w such that p(w|x) is highest. we use the hat notation    to mean    our estimate
of the correct word   .

argmax

likelihood
channel model
prior
id203

  w = argmax

p(w|x)

(b.1)

w   v

the function argmaxx f (x) means    the x such that f (x) is maximized   . equa-
tion b.1 thus means, that out of all words in the vocabulary, we want the particular
word that maximizes the right-hand side p(w|x).
the intuition of bayesian classi   cation is to use bayes    rule to transform eq. b.1
into a set of other probabilities. bayes    rule is presented in eq. b.2; it gives us a way
to break down any id155 p(a|b) into three other probabilities:

p(a|b) =

p(b|a)p(a)

p(b)

we can then substitute eq. b.2 into eq. b.1 to get eq. b.3:

  w = argmax

w   v

p(x|w)p(w)

p(x)

(b.2)

(b.3)

we can conveniently simplify eq. b.3 by dropping the denominator p(x). why
is that? since we are choosing a potential correction word out of all words, we will
be computing p(x|w)p(w)
for each word. but p(x) doesn   t change for each word; we
are always asking about the most likely word for the same observed error x, which
must have the same id203 p(x). thus, we can choose the word that maximizes
this simpler formula:

p(x)

  w = argmax

p(x|w)p(w)

(b.4)

w   v

to summarize, the id87 says that we have some true underlying
word w, and we have a noisy channel that modi   es the word into some possible
misspelled observed surface form. the likelihood or channel model of the noisy
channel producing any particular observation sequence x is modeled by p(x|w). the
prior id203 of a hidden word is modeled by p(w). we can compute the most
probable word   w given that we   ve seen some observed misspelling x by multiply-
ing the prior p(w) and the likelihood p(x|w) and choosing the word for which this
product is greatest.
we apply the noisy channel approach to correcting non-word spelling errors by
taking any word not in our spell dictionary, generating a list of candidate words,
ranking them according to eq. b.4, and picking the highest-ranked one. we can
modify eq. b.4 to refer to this list of candidate words instead of the full vocabulary
v as follows:

  w = argmax

w   c

channel model

(cid:122) (cid:125)(cid:124) (cid:123)
p(x|w)

prior

(cid:122)(cid:125)(cid:124)(cid:123)p(w)

(b.5)

the noisy channel algorithm is shown in fig. b.2.
to see the details of the computation of the likelihood and the prior (language
model), let   s walk through an example, applying the algorithm to the example mis-
spelling acress. the    rst stage of the algorithm proposes candidate corrections by

b.1

    the id87

483

function noisy channel spelling(word x, dict d, lm, editprob) returns correction

if x /    d

candidates, edits   all strings at id153 1 from x that are     d, and their edit
for each c,e in candidates, edits

channel   editprob(e)
prior   lm(x)
score[c] = log channel + log prior

return argmaxc score[c]

figure b.2 id87 for id147 for unknown words.

   nding words that have a similar spelling to the input word. analysis of spelling
error data has shown that the majority of spelling errors consist of a single-letter
change and so we often make the simplifying assumption that these candidates have
an id153 of 1 from the error word. to    nd this list of candidates we   ll use
the minimum id153 algorithm introduced in chapter 2, but extended so that
in addition to insertions, deletions, and substitutions, we   ll add a fourth type of edit,
transpositions, in which two letters are swapped. the version of id153 with
transposition is called damerau-levenshtein id153. applying all such sin-
gle transformations to acress yields the list of candidate words in fig. b.3.

damerau-
levenshtein

transformation

error
acress
acress
acress
acress
acress
acress
acress

correction
actress
cress
caress
access
across
acres
acres

correct
letter
t
   
ca
c
o
   
   

error
letter
   
a
ac
r
e
s
s

position
(letter #)
2
0
0
2
3
5
4

type
deletion
insertion
transposition
substitution
substitution
insertion
insertion

figure b.3 candidate corrections for the misspelling acress and the transformations that
would have produced the error (after kernighan et al. (1990)).           represents a null letter.

once we have a set of a candidates, to score each one using eq. b.5 requires that

we compute the prior and the channel model.

the prior id203 of each correction p(w) is the language model id203
of the word w in context, which can be computed using any language model, from
unigram to trigram or 4-gram. for this example let   s start in the following table by
assuming a unigram language model. we computed the language model from the
404,253,213 words in the corpus of contemporary english (coca).

count(w) p(w)

w
actress 9,321
cress
220
caress 686
access 37,038
across 120,844
12,874
acres

.0000231
.000000544
.00000170
.0000916
.000299
.0000318

channel model

how can we estimate the likelihood p(x|w), also called the channel model or

484 appendix b     id147 and the noisy channel

error model

confusion
matrix

error model? a perfect model of the id203 that a word will be mistyped would
condition on all sorts of factors: who the typist was, whether the typist was left-
handed or right-handed, and so on. luckily, we can get a pretty reasonable estimate
of p(x|w) just by looking at local context: the identity of the correct letter itself, the
misspelling, and the surrounding letters. for example, the letters m and n are often
substituted for each other; this is partly a fact about their identity (these two letters
are pronounced similarly and they are next to each other on the keyboard) and partly
a fact about context (because they are pronounced similarly and they occur in similar
contexts).

a simple model might estimate, for example, p(acress|across) just using the
number of times that the letter e was substituted for the letter o in some large corpus
of errors. to compute the id203 for each edit in this way we   ll need a confu-
sion matrix that contains counts of errors. in general, a confusion matrix lists the
number of times one thing was confused with another. thus for example a substi-
tution matrix will be a square matrix of size 26  26 (or more generally |a|   |a|,
for an alphabet a) that represents the number of times one letter was incorrectly
used instead of another. following kernighan et al. (1990) we   ll use four confusion
matrices.

del[x,y]: count(xy typed as x)
ins[x,y]: count(x typed as xy)
sub[x,y]: count(x typed as y)
trans[x,y]: count(xy typed as yx)

note that we   ve conditioned the insertion and deletion probabilities on the previ-
ous character; we could instead have chosen to condition on the following character.
where do we get these confusion matrices? one way is to extract them from

lists of misspellings like the following:

additional: addional, additonal
environments: enviornments, enviorments, enviroments
preceded: preceeded
...

there are lists available on wikipedia and from roger mitton (http://www.
dcs.bbk.ac.uk/  roger/corpora.html) and peter norvig (http://norvig.
com/ngrams/). norvig also gives the counts for each single-character edit that can
be used to directly create the error model probabilities.

an alternative approach used by kernighan et al. (1990) is to compute the ma-
trices by iteratively using this very spelling error correction algorithm itself. the
iterative algorithm    rst initializes the matrices with equal values; thus, any character
is equally likely to be deleted, equally likely to be substituted for any other char-
acter, etc. next, the spelling error correction algorithm is run on a set of spelling
errors. given the set of typos paired with their predicted corrections, the confusion
matrices can now be recomputed, the spelling algorithm run again, and so on. this
iterative algorithm is an instance of the important em algorithm (dempster et al.,
1977), which we discuss in appendix a.

once we have the confusion matrices, we can estimate p(x|w) as follows (where

b.1

    the id87

485

wi is the ith character of the correct word w) and xi is the ith character of the typo x:

, if deletion

, if insertion

, if substitution

del[xi   1,wi]
count[xi   1wi]
ins[xi   1,wi]
count[wi   1]
sub[xi,wi]
count[wi]
trans[wi,wi+1]
count[wiwi+1]

, if transposition

(b.6)

p(x|w) =

                                                                                 

using the counts from kernighan et al. (1990) results in the error model proba-

bilities for acress shown in fig. b.4.

candidate
correction
actress
cress
caress
access
across
acres
acres

correct
letter
t
-
ca
c
o
-
-

error
letter
-
a
ac
r
e
s
s

x|w

c|ct
a|#
ac|ca
r|c
e|o
es|e
ss|s

p(x|w)
.000117
.00000144
.00000164
.000000209
.0000093
.0000321
.0000342

figure b.4 channel model for acress; the probabilities are taken from the del[], ins[],
sub[], and trans[] confusion matrices as shown in kernighan et al. (1990).

figure b.5 shows the    nal probabilities for each of the potential corrections;
the unigram prior is multiplied by the likelihood (computed with eq. b.6 and the
confusion matrices). the    nal column shows the product, multiplied by 109 just for
readability.

candidate correct error
correction letter letter x|w

t
-
ca
c
o
-
-

109*p(x|w)p(w)
2.7

p(w)
p(x|w)
.0000231
c|ct .000117
.000000544 0.00078
.00000144
a|#
0.0028
.00000170
ac|ca .00000164
0.019
.000000209 .0000916
r|c
2.8
.000299
.0000093
e|o
1.0
.0000318
es|e .0000321
ss|s .0000342
.0000318
1.0

actress
cress
caress
access
across
acres
acres
figure b.5 computation of the ranking for each candidate correction, using the language
model shown earlier and the error model from fig. b.4. the    nal score is multiplied by 109
for readability.

-
a
ac
r
e
s
s

the computations in fig. b.5 show that our implementation of the noisy channel
model chooses across as the best correction, and actress as the second most
likely word.

unfortunately, the algorithm was wrong here; the writer   s intention becomes
clear from the context: . . . was called a    stellar and versatile acress whose com-
bination of sass and glamour has de   ned her. . .    . the surrounding words make it
clear that actress and not across was the intended word.

486 appendix b     id147 and the noisy channel

for this reason, it is important to use larger language models than unigrams.
for example, if we use the corpus of contemporary american english to compute
bigram probabilities for the words actress and across in their context using add-one
smoothing, we get the following probabilities:

p(actress|versatile) = .000021
p(across|versatile) = .000021
p(whose|actress) = .0010
p(whose|across) = .000006

multiplying these out gives us the language model estimate for the two candi-

dates in context:

p(   versatile actress whose   ) = .000021    .0010 = 210   10   10
p(   versatile across whose   ) = .000021    .000006 = 1   10   10

combining the language model with the error model in fig. b.5, the bigram

id87 now chooses the correct word actress.

evaluating spell correction algorithms is generally done by holding out a train-
ing, development and test set from lists of errors like those on the norvig and mitton
sites mentioned above.

b.2 real-word spelling errors

real-word error
detection

the noisy channel approach can also be applied to detect and correct real-word
spelling errors, errors that result in an actual word of english. this can happen from
typographical errors (insertion, deletion, transposition) that accidentally produce a
real word (e.g., there for three) or because the writer substituted the wrong spelling
of a homophone or near-homophone (e.g., dessert for desert, or piece for peace). a
number of studies suggest that between 25% and 40% of spelling errors are valid
english words as in the following examples (kukich, 1992):

this used to belong to thew queen. they are leaving in about    fteen minuets to go to her house.
the design an construction of the system will take more than a year.
can they lave him my messages?
the study was conducted mainly be john black.

the noisy channel can deal with real-word errors as well. let   s begin with a
version of the id87    rst proposed by mays et al. (1991) to deal
with these real-word spelling errors. their algorithm takes the input sentence x =
{x1,x2, . . . ,xk, . . . ,xn}, generates a large set of candidate correction sentences c(x),
then picks the sentence with the highest language model id203.
to generate the candidate correction sentences, we start by generating a set of
candidate words for each input word xi. the candidates, c(xi), include every english
word with a small id153 from xi. with id153 1, a common choice
(mays et al., 1991), the candidate set for the real word error thew (a rare word
meaning    muscular strength   ) might be c(thew) = {the, thaw, threw, them, thwe}.
we then make the simplifying assumption that every sentence has only one error.
thus the set of candidate sentences c(x) for a sentence x = only two of thew
apples would be:

b.2

    real-word spelling errors

487

only two of thew apples
oily two of thew apples
only too of thew apples
only to of thew apples
only tao of the apples
only two on thew apples
only two off thew apples
only two of the apples
only two of threw apples
only two of thew applies
only two of thew dapples
...

each sentence is scored by the noisy channel:

  w = argmax
w   c(x)

p(x|w )p(w )

(b.7)

for p(w ), we can use the trigram id203 of the sentence.
what about the channel model? since these are real words, we need to consider
the possibility that the input word is not an error. let   s say that the channel proba-
bility of writing a word correctly, p(w|w), is   ; we can make different assumptions
about exactly what the value of    is in different tasks; perhaps    is .95, assum-
ing people write 1 word wrong out of 20, for some tasks, or maybe .99 for others.
mays et al. (1991) proposed a simple model: given a typed word x, let the channel
model p(x|w) be    when x = w, and then just distribute 1       evenly over all other
candidate corrections c(x):

p(x|w) =                           

  
1      
|c(x)|

0

if x = w
if x     c(x)
otherwise

(b.8)

now we can replace the equal distribution of 1      over all corrections in eq. b.8;
we   ll make the distribution proportional to the edit id203 from the more sophis-
ticated channel model from eq. b.6 that used the confusion matrices.

let   s see an example of this integrated id87 applied to a real
word. suppose we see the string two of thew. the author might have intended
to type the real word thew (   muscular strength   ). but thew here could also be a
typo for the or some other word. for the purposes of this example let   s consider
id153 1, and only the following    ve candidates the, thaw, threw, and thwe
(a rare name) and the string as typed, thew. we took the edit probabilities from
norvig   s (2009) analysis of this example. for the language model probabilities, we
used a stupid backoff model (section 3.6) trained on the google id165s:

= 0.476012

p(the|two of)
p(thew|two of) = 9.95051   10   8
p(thaw|two of) = 2.09267   10   7
p(threw|two of) = 8.9064   10   7
p(them|two of) = 0.00144488
p(thwe|two of) = 5.18681   10   9

here we   ve just computed probabilities for the single phrase two of thew, but
the model applies to entire sentences; so if the example in context was two of thew

488 appendix b     id147 and the noisy channel

w

people, we   d need to also multiply in probabilities for p(people|of the), p(people|of
thew), p(people|of threw), and so on.
following norvig (2009), we assume that the id203 of a word being a typo
in this task is .05, meaning that    = p(w|w) is .95. fig. b.6 shows the computation.
p(x|w) p(w|wi   2, wi   1) 108p(x|w)p(w|wi   2, wi   1)
x|w
ew|e
0.000007 0.48
  =0.95
9.95   10   8
2.1   10   7
e|a
0.001
0.000008 8.9   10   7
thew threw h|hr
ew|we 0.000003 5.2   10   9

333
9.45
0.0209
0.000713
0.00000156

x
thew the
thew thew
thew thaw

figure b.6 the id87 on 5 possible candidates for thew, with a stupid
backoff trigram language model computed from the google id165 corpus and the error
model from norvig (2009).

thew thwe

for the error phrase two of thew, the model correctly picks the as the correction.
but note that a lower error rate might change things; in a task where the id203
of an error is low enough (   is very high), the model might instead decide that the
word thew was what the writer intended.

b.3 id87: the state of the art

state of the art implementations of noisy channel id147 make a number
of extensions to the simple models we presented above.

first, rather than make the assumption that the input sentence has only a sin-
gle error, modern systems go through the input one word at a time, using the noisy
channel to make a decision for that word. but if we just run the basic noisy chan-
nel system described above on each word, it is prone to overcorrecting, replacing
correct but rare words (for example names) with more frequent words (whitelaw
et al. 2009, wilcox-o   hearn 2014). modern algorithms therefore need to augment
the noisy channel with methods for detecting whether or not a real word should ac-
tually be corrected. for example state of the art systems like google   s (whitelaw
et al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,
and single letter words) from being changed. such systems are also more cautious
in deciding whether to trust a candidate correction. instead of just choosing a candi-
date correction if it has a higher id203 p(w|x) than the word itself, these more
careful systems choose to suggest a correction w over keeping the non-correction x
only if the difference in probabilities is suf   ciently great. the best correction w is
chosen only if:

logp(w|x)    logp(x|x) >   

autocorrect

depending on the speci   c application, spell-checkers may decide to autocorrect
(automatically change a spelling to a hypothesized correction) or merely to    ag the
error and offer suggestions. this decision is often made by another classi   er which
decides whether the best candidate is good enough, using features such as the dif-
ference in log probabilities between the candidates (we   ll introduce algorithms for
classi   cation in the next chapter).

modern systems also use much larger dictionaries than early systems. ahmad
and kondrak (2005) found that a 100,000 word unix dictionary only contained

b.3

    id87: the state of the art

489

73% of the word types in their corpus of web queries, missing words like pics,
multiplayer, google, xbox, clipart, and mallorca. for this reason modern systems
often use much larger dictionaries automatically derived from very large lists of
unigrams like the google id165 corpus. whitelaw et al. (2009), for example,
used the most frequently occurring ten million word types in a large sample of web
pages. because this list will include lots of misspellings, their system requires a
more sophisticated error model. the fact that words are generally more frequent than
their misspellings can be used in candidate suggestion, by building a set of words
and spelling variations that have similar contexts, sorting by frequency, treating the
most frequent variant as the source, and learning an error model from the difference,
whether from web text (whitelaw et al., 2009) or from query logs (cucerzan and
brill, 2004). words can also be automatically added to the dictionary when a user
rejects a correction, and systems running on phones can automatically add words
from the user   s address book or calendar.

we can also improve the performance of the id87 by changing
how the prior and the likelihood are combined. in the standard model they are just
multiplied together. but often these probabilities are not commensurate; the lan-
guage model or the channel model might have very different ranges. alternatively
for some task or dataset we might have reason to trust one of the two models more.
therefore we use a weighted combination, by raising one of the factors to a power
   :

  w = argmax

w   v

p(x|w)p(w)  

or in log space:

logp(x|w) +    logp(w)
we then tune the parameter    on a development test set.

  w = argmax

w   v

finally, if our goal is to do real-word id147 only for speci   c con-
fusion sets like peace/piece, affect/effect, weather/whether, or even grammar cor-
rection examples like among/between, we can train supervised classi   ers to draw on
many features of the context and make a choice between the two candidates. such
classi   ers can achieve very high accuracy for these speci   c sets, especially when
drawing on large-scale features from web statistics (golding and roth 1999, lapata
and keller 2004, bergsma et al. 2009, bergsma et al. 2010).

improved edit models: partitions and pronunciation

b.3.1
other recent research has focused on improving the channel model p(t|c). one
important extension is the ability to compute probabilities for multiple-letter trans-
formations. for example brill and moore (2000) propose a channel model that (in-
formally) models an error as being generated by a typist    rst choosing a word, then
choosing a partition of the letters of that word, and then typing each partition, pos-
sibly erroneously. for example, imagine a person chooses the word physical,
then chooses the partition ph y s i c al she would then generate each parti-
tion, possible with errors. for example the id203 that she would generate the
string fisikle with partition f i s i k le would be p(f|ph)    p(i|y)    p(s|s)   
p(i|i)    p(k|k)    p(le|al). unlike the damerau-levenshtein id153, the brill-
moore channel model can thus model edit probabilities like p(f|ph) or p(le|al), or

(b.9)

(b.10)

confusion sets

490 appendix b     id147 and the noisy channel

the high likelihood of p(ent|ant). furthermore, each edit is conditioned on where
it is in the word (beginning, middle, end) so instead of p(f|ph) the model actually
estimates p(f|ph,beginning).
more formally, let r be a partition of the typo string x into adjacent (possibly
empty) substrings, and t be a partition of the candidate string. brill and moore
(2000) then approximates the total likelihood p(x|w) (e.g., p(fisikle|physical))
by the id203 of the single best partition:

|r|(cid:88)i=1

p(x|w)    

max

r,t s.t.|t|=|r|

p(ti|ri,position)

(b.11)

the id203 of each transform p(ti|ri) can be learned from a training set of
triples of an error, the correct string, and the number of times it occurs. for example
given a training pair akgsual/actual, standard minimum id153 is used to
produce an alignment:

this alignment corresponds to the sequence of edit operations:

a   a, c   k,

     g t   s, u   u, a   a, l   l

each nonmatch substitution is then expanded to incorporate up to n additional

edits; for n=2, we would expand c   k to:

ac   ak
c   cg
ac   akg
ct   kgs

.

each of these multiple edits then gets a fractional count, and the id203 for
each edit           is then estimated from counts in the training corpus of triples as
count(        )
count(  )
another research direction in channel models is the use of pronunciation in ad-
dition to spelling. pronunciation is an important feature in some non-noisy-channel
algorithms for spell correction like the gnu aspell algorithm (atkinson, 2011),
which makes use of the metaphone pronunciation of a word (philips, 1990). meta-
phone is a series of rules that map a word to a normalized representation of its
pronunciation. some example rules:

aspell

       drop duplicate adjacent letters, except for c.   
       if the word begins with    kn   ,    gn   ,    pn   ,    ae   ,    wr   , drop the    rst letter.   
       drop    b    if after    m    and if it is at the end of the word   

aspell works similarly to the channel component of the id87,    nding
all words in the dictionary whose pronunciation string is a short id153 (1 or
2 pronunciation letters) from the typo, and then scoring this list of candidates by
a metric that combines two id153s: the pronunciation id153 and the
weighted letter id153.

pronunciation can also be incorporated directly the id87. for ex-
ample the toutanova and moore (2002) model, like aspell, interpolates two channel

actualakgsualbibliographical and historical notes

491

function soundex(name) returns soundex form

1. keep the    rst letter of name
2. drop all occurrences of non-initial a, e, h, i, o, u, w, y.
3. replace the remaining letters with the following numbers:

b, f, p, v     1
c, g, j, k, q, s, x, z     2
d, t     3
l     4
m, n     5
r     6

4. replace any sequences of identical numbers, only if they derive from two or more
letters that were adjacent in the original name, with a single number (e.g., 666     6).
5. convert to the form letter digit digit digit by dropping digits past the third
(if necessary) or padding with trailing zeros (if necessary).

figure b.7 the soundex algorithm

models, one based on spelling and one based on pronunciation. the pronunciation
model is based on using letter-to-sound models to translate each input word and
each dictionary word into a sequences of phones representing the pronunciation of
the word. for example actress and aktress would both map to the phone string
ae k t r ix s. see chapter 26 on the task of letter-to-sound or grapheme-to-
phoneme.

some additional string distance functions have been proposed for dealing specif-
ically with names. these are mainly used for the task of deduplication (deciding if
two names in a census list or other namelist are the same) rather than spell-checking.
the soundex algorithm (knuth 1973, odell and russell 1922) is an older method
used originally for census records for representing people   s names. it has the advan-
tage that versions of the names that are slightly misspelled will still have the same
representation as correctly spelled names. (e.g., jurafsky, jarofsky, jarovsky, and
jarovski all map to j612). the algorithm is shown in fig. b.7.

instead of soundex, more recent work uses jaro-winkler distance, which is
an id153 algorithm designed for names that allows characters to be moved
longer distances in longer names, and also gives a higher similarity to strings that
have identical initial characters (winkler, 2006).

letter-to-sound
phones

deduplication

jaro-winkler

bibliographical and historical notes

algorithms for spelling error detection and correction have existed since at least
blair (1960). most early algorithms were based on similarity keys like the soundex
algorithm (odell and russell 1922, knuth 1973). damerau (1964) gave a dictionary-
based algorithm for error detection; most error-detection algorithms since then have
been based on dictionaries. early research (peterson, 1986) had suggested that
spelling dictionaries might need to be kept small because large dictionaries con-
tain very rare words (wont, veery) that resemble misspellings of other words, but
damerau and mays (1989) found that in practice larger dictionaries proved more
helpful. damerau (1964) also gave a correction algorithm that worked for single
errors.

the idea of modeling language transmission as a markov source passed through

492 appendix b     id147 and the noisy channel

a id87 was developed very early on by claude shannon (1948).
the idea of combining a prior and a likelihood to deal with the noisy channel was
developed at ibm research by raviv (1967), for the similar task of optical char-
acter recognition (ocr). while earlier spell-checkers like kashyap and oommen
(1983) had used likelihood-based models of id153, the idea of combining a
prior and a likelihood seems not to have been applied to the id147 task
until researchers at at&t bell laboratories (kernighan et al. 1990, church and
gale 1991) and ibm watson research (mays et al., 1991) roughly simultaneously
proposed noisy channel id147. much later, the mays et al. (1991) algo-
rithm was reimplemented and tested on standard datasets by wilcox-o   hearn et al.
(2008), who showed its high performance.

most algorithms since wagner and fischer (1974) have relied on dynamic pro-

gramming.

recent focus has been on using the web both for language models and for train-
ing the error model, and on incorporating additional features in spelling, like the
pronunciation models described earlier, or other information like parses or semantic
relatedness (jones and martin 1997, hirst and budanitsky 2005).

see mitton (1987) for a survey of human spelling errors, and kukich (1992)
for an early survey of spelling error detection and correction. norvig (2007) gives
a nice explanation and a python implementation of the id87, with
more details and an ef   cient algorithm presented in norvig (2009).

exercises

b.1 suppose we want to apply add-one smoothing to the likelihood term (channel
model) p(x|w) of a id87 of spelling. for simplicity, pretend
that the only possible operation is deletion. the id113 estimate for deletion
is given in eq. b.6, which is p(x|w) = del[xi`1,wi]
count(xi`1wi). what is the estimate for
p(x|w) if we use add-one smoothing on the deletion edit model? assume the
only characters we use are lower case a-z, that there are v word types in our
corpus, and n total characters, not counting spaces.

chapter

c id138: word relations,

senses, and disambiguation

in this chapter we introduce computation with a thesaurus: a structured list of words
organized by meaning. the most popular thesaurus for computational purposes is
id138, a large online resource with versions in many languages. one use of
id138 is to represent word senses, the many different meanings that a single
lemma can have (chapter 6) thus the lemma bank can refer to a    nancial institution
or to the sloping side of a river. id138 also represents relations between senses,
like the is-a relation between dog and mammal or the part-whole relationship be-
tween car and engine. finally, id138 includes glosses, a de   nition for senses in
the form of a text string.

we   ll see how to use each of these aspects of id138 to address the task of
computing word similarity; the similarity in meaning of two different words, an
alternative to the embedding-based methods we introduced in chapter 6. and we   ll
introduce id51, the task of determining which sense of a
word is being used in a particular context, a task with a long history in computational
linguistics and applications from machine translation to id53. we
give a number of algorithms for using features from the context for deciding which
sense was intended in a particular context.

glosses

word sense
disambiguation

c.1 word senses

consider the two uses of the lemma bank mentioned above, meaning something like
      nancial institution    and    sloping mound   , respectively:
(c.1) instead, a bank can hold the investments in a custodial account in the client   s

name.

(c.2) but as agriculture burgeons on the east bank, the river will shrink even more.
we represent this variation in usage by saying that the lemma bank has two
senses.1 a sense (or word sense) is a discrete representation of one aspect of the
meaning of a word. loosely following lexicographic tradition, we represent each
sense by placing a superscript on the lemma as in bank1 and bank2.

the senses of a word might not have any particular relation between them; it
may be almost coincidental that they share an orthographic form. for example, the
   nancial institution and sloping mound senses of bank seem relatively unrelated.
in such cases we say that the two senses are homonyms, and the relation between
the senses is one of homonymy. thus bank1 (      nancial institution   ) and bank2
(   sloping mound   ) are homonyms, as are the sense of bat meaning    club for hitting
a ball    and the one meaning    nocturnal    ying animal   . we say that these two uses
of bank are homographs, as are the two uses of bat, because they are written the

1 confusingly, the word    lemma    is itself ambiguous; it is also sometimes used to mean these separate
senses, rather than the citation form of the word. you should be prepared to see both uses in the literature.

word sense

homonym
homonymy

homographs

494 appendix c     id138: word relations, senses, and disambiguation

homophones

polysemy

metonymy

same. two words can be homonyms in a different way if they are spelled differently
but pronounced the same, like write and right, or piece and peace. we call these
homophones; they are one cause of real-word spelling errors.

homonymy causes problems in other areas of language processing as well. in
id53 or information retrieval, we better help a user who typed    bat
care    if we know whether they are vampires or just want to play baseball. and
they will also have different translations; in spanish the animal bat is a murci  elago
while the baseball bat is a bate. homographs that are pronounced differently cause
problems for id133 (chapter 26) such as these homographs of the word
bass, the    sh pronounced b ae s and the instrument pronounced b ey s.
(c.3) the expert angler from dora, mo., was    y-casting for bass rather than the

traditional trout.

(c.4) the curtain rises to the sound of angry dogs baying and ominous bass chords

sounding.

sometimes there is also some semantic connection between the senses of a word.

consider the following example:
(c.5) while some banks furnish blood only to hospitals, others are less restrictive.
although this is clearly not a use of the    sloping mound    meaning of bank, it just
as clearly is not a reference to a charitable giveaway by a    nancial institution. rather,
bank has a whole range of uses related to repositories for various biological entities,
as in blood bank, egg bank, and sperm bank. so we could call this    biological
repository    sense bank3. now this new sense bank3 has some sort of relation to
bank1; both bank1 and bank3 are repositories for entities that can be deposited and
taken out; in bank1 the entity is monetary, whereas in bank3 the entity is biological.
when two senses are related semantically, we call the relationship between them
polysemy rather than homonymy. in many cases of polysemy, the semantic relation
between the senses is systematic and structured. for example, consider yet another
sense of bank, exempli   ed in the following sentence:
(c.6) the bank is on the corner of nassau and witherspoon.

this sense, which we can call bank4, means something like    the building be-
longing to a    nancial institution   . it turns out that these two kinds of senses (an
organization and the building associated with an organization ) occur together for
many other words as well (school, university, hospital, etc.). thus, there is a sys-
tematic relationship between senses that we might represent as

building     organization

this particular subtype of polysemy relation is often called metonymy. metonymy

is the use of one aspect of a concept or entity to refer to other aspects of the entity
or to the entity itself. thus, we are performing metonymy when we use the phrase
the white house to refer to the administration whose of   ce is in the white house.
other common examples of metonymy include the relation between the following
pairings of senses:
author (jane austen wrote emma)     works of author (i really love jane austen)
tree (plums have beautiful blossoms)     fruit (i ate a preserved plum yesterday)

while it can be useful to distinguish polysemy from unrelated homonymy, there
is no hard threshold for how related two senses must be to be considered polyse-
mous. thus, the difference is really one of degree. this fact can make it very dif   cult
to decide how many senses a word has, that is, whether to make separate senses for

zeugma

c.1

    word senses

495

closely related usages. there are various criteria for deciding that the differing uses
of a word should be represented with discrete senses. we might consider two senses
discrete if they have independent truth conditions, different syntactic behavior, and
independent sense relations, or if they exhibit antagonistic meanings.

consider the following uses of the verb serve from the wsj corpus:

(c.7) they rarely serve red meat, preferring to prepare seafood.
(c.8) he served as u.s. ambassador to norway in 1976 and 1977.
(c.9) he might have served his time, come out and led an upstanding life.
the serve of serving red meat and that of serving time clearly have different truth
conditions and presuppositions; the serve of serve as ambassador has the distinct
subcategorization structure serve as np. these heuristics suggest that these are prob-
ably three distinct senses of serve. one practical technique for determining if two
senses are distinct is to conjoin two uses of a word in a single sentence; this kind of
conjunction of antagonistic readings is called zeugma. consider the following atis
examples:
(c.10) which of those    ights serve breakfast?
(c.11) does midwest express serve philadelphia?
(c.12) ?does midwest express serve breakfast and philadelphia?
we use (?) to mark those examples that are semantically ill-formed. the oddness of
the invented third example (a case of zeugma) indicates there is no sensible way to
make a single sense of serve work for both breakfast and philadelphia. we can use
this as evidence that serve has two different senses in this case.

dictionaries tend to use many    ne-grained senses so as to capture subtle meaning
differences, a reasonable approach given that the traditional role of dictionaries is
aiding word learners. for computational purposes, we often don   t need these    ne
distinctions, so we may want to group or cluster the senses; we have already done
this for some of the examples in this chapter.

how can we de   ne the meaning of a word sense? we introduced in chapter 6 the
standard computational approach of representing a word as an embedding, a point in
semantic space. the intuition was that words were de   ned by their co-occurrences,
the counts of words that often occur nearby.

thesauri offer an alternative way of de   ning words. but we can   t just look at
the de   nition itself. consider the following fragments from the de   nitions of right,
left, red, and blood from the american heritage dictionary (morris, 1985).

right adj. located nearer the right hand esp. being on the right when

facing the same direction as the observer.

left adj. located nearer to this side of the body than the right.
red n. the color of blood or a ruby.

blood n. the red liquid that circulates in the heart, arteries and veins of

animals.

note the circularity in these de   nitions. the de   nition of right makes two direct
references to itself, and the entry for left contains an implicit self-reference in the
phrase this side of the body, which presumably means the left side. the entries for
red and blood reference each other in their de   nitions. such circularity is inherent
in all dictionary de   nitions. for humans, such entries are still useful since the user
of the dictionary has suf   cient grasp of these other terms.

for computational purposes, one approach to de   ning a sense is   like the dic-
tionary de   nitions   de   ning a sense through its relationship with other senses. for

496 appendix c     id138: word relations, senses, and disambiguation

example, the above de   nitions make it clear that right and left are similar kinds of
lemmas that stand in some kind of alternation, or opposition, to one another. simi-
larly, we can glean that red is a color, that it can be applied to both blood and rubies,
and that blood is a liquid. sense relations of this sort are embodied in on-line
databases like id138. given a suf   ciently large database of such relations, many
applications are quite capable of performing sophisticated semantic tasks (even if
they do not really know their right from their left).

c.1.1 relations between senses
this section explores some of the relations that hold among word senses, focus-
ing on a few that have received signi   cant computational investigation: synonymy,
antonymy, and hypernymy, as well as a brief mention of other relations like meronymy.

synonymy we introduced in chapter 6 the idea that when two senses of two dif-
ferent words (lemmas) are identical, or nearly identical, we say the two senses are
synonyms. synonyms include such pairs as

synonym

couch/sofa vomit/throw up    lbert/hazelnut car/automobile

and we mentioned that in practice, the word synonym is commonly used to
describe a relationship of approximate or rough synonymy. but furthermore, syn-
onymy is actually a relationship between senses rather than words. considering the
words big and large. these may seem to be synonyms in the following atis sen-
tences, since we could swap big and large in either sentence and retain the same
meaning:

(c.13) how big is that plane?
(c.14) would i be    ying on a large or small plane?

but note the following wsj sentence in which we cannot substitute large for big:

(c.15) miss nelson, for instance, became a kind of big sister to benjamin.
(c.16) ?miss nelson, for instance, became a kind of large sister to benjamin.

this is because the word big has a sense that means being older or grown up, while
large lacks this sense. thus, we say that some senses of big and large are (nearly)
synonymous while other ones are not.

hyponym

hypernym

superordinate

hyponymy one sense is a hyponym of another sense if the    rst sense is more
speci   c, a subclass. for example, car is a hyponym of vehicle; dog is a hyponym
of animal, and mango is a hyponym of fruit. conversely, vehicle is a hypernym of
car, and animal is a hypernym of dog. it is unfortunate that the two words hypernym
and hyponym are very similar and hence easily confused; for this reason, the word
superordinate is often used instead of hypernym.

superordinate vehicle fruit
hyponym

car

furniture mammal

mango chair

dog

meronymy
part-whole
meronym
holonym

meronymy another common relation is meronymy, the part-whole relation. a
leg is part of a chair; a wheel is part of a car. we say that wheel is a meronym of
car, and car is a holonym of wheel.

c.2

    id138: a database of lexical relations

497

c.2 id138: a database of lexical relations

id138

the most commonly used resource for english sense relations is the id138 lex-
ical database (fellbaum, 1998). id138 consists of three separate databases, one
each for nouns and verbs and a third for adjectives and adverbs; closed class words
are not included. each database contains a set of lemmas, each one annotated with a
set of senses. the id138 3.0 release has 117,798 nouns, 11,529 verbs, 22,479 ad-
jectives, and 4,481 adverbs. the average noun has 1.23 senses, and the average verb
has 2.16 senses. id138 can be accessed on the web or downloaded and accessed
locally. figure c.1 shows the lemma entry for the noun and adjective bass.

the noun    bass    has 8 senses in id138.
1. bass1 - (the lowest part of the musical range)
2. bass2, bass part1 - (the lowest part in polyphonic music)
3. bass3, basso1 - (an adult male singer with the lowest voice)
4. sea bass1, bass4 - (the lean    esh of a saltwater    sh of the family serranidae)
5. freshwater bass1, bass5 - (any of various north american freshwater    sh with

lean    esh (especially of the genus micropterus))

6. bass6, bass voice1, basso2 - (the lowest adult male singing voice)
7. bass7 - (the member with the lowest range of a family of musical instruments)
8. bass8 - (nontechnical name for any of numerous edible marine and

freshwater spiny-   nned    shes)

the adjective    bass    has 1 sense in id138.
1. bass1, deep6 - (having or denoting a low vocal or instrumental range)

   a deep voice   ;    a bass voice is lower than a baritone voice   ;
   a bass clarinet   

gloss

synset

figure c.1 a portion of the id138 3.0 entry for the noun bass.

note that there are eight senses for the noun and one for the adjective, each of
which has a gloss (a dictionary-style de   nition), a list of synonyms for the sense, and
sometimes also usage examples (shown for the adjective sense). unlike dictionaries,
id138 doesn   t represent pronunciation, so doesn   t distinguish the pronunciation
[b ae s] in bass4, bass5, and bass8 from the other senses pronounced [b ey s].

the set of near-synonyms for a id138 sense is called a synset (for synonym
set); synsets are an important primitive in id138. the entry for bass includes
synsets like {bass1, deep6}, or {bass6, bass voice1, basso2}. we can think of a
synset as representing a concept of the type we discussed in chapter 14. thus,
instead of representing concepts in logical terms, id138 represents them as lists
of the word senses that can be used to express the concept. here   s another synset
example:

{chump1, fool2, gull1, mark9, patsy1, fall guy1,
sucker1, soft touch1, mug2}

the gloss of this synset describes it as a person who is gullible and easy to take
advantage of. each of the lexical entries included in the synset can, therefore, be
used to express this concept. synsets like this one actually constitute the senses
associated with id138 entries, and hence it is synsets, not wordforms, lemmas, or
individual senses, that participate in most of the lexical sense relations in id138.
id138 represents all the kinds of sense relations discussed in the previous sec-
tion, as illustrated in fig. c.2 and fig. c.3. id138 hyponymy relations correspond

from concepts to subtypes
from instances to their concepts

also called de   nition
superordinate from concepts to superordinates
subordinate

498 appendix c     id138: word relations, senses, and disambiguation
relation
hypernym
hyponym
instance hypernym instance
instance hyponym has-instance from concepts to concept instances
member meronym has-member from groups to their members
from members to their groups
member holonym member-of
from wholes to parts
part meronym
from parts to wholes
part holonym
substance meronym
from substances to their subparts
from parts of substances to wholes
substance holonym
semantic opposition between lemmas leader1        follower1
antonym
lemmas w/same morphological root
derivationally

example
breakfast1     meal1
meal1     lunch1
austen1     author1
composer1     bach1
faculty2     professor1
copilot1     crew1
table2     leg3
course7     meal1
water1     oxygen1
gin1     martini1
destruction1        destroy1

has-part
part-of

related form

figure c.2 noun relations in id138.

relation
hypernym
troponym

entails
antonym
derivationally

related form

de   nition
from events to superordinate events
from events to subordinate event
(often via speci   c manner)
from verbs (events) to the verbs (events) they entail
semantic opposition between lemmas
lemmas with same morphological root

example
   y9     travel5
walk1     stroll1
snore1     sleep1
increase1        decrease1
destroy1        destruction1

figure c.3 verb relations in id138.

to the notion of immediate hyponymy discussed on page 496. each synset is related
to its immediately more general and more speci   c synsets through direct hypernym
and hyponym relations. these relations can be followed to produce longer chains of
more general or more speci   c synsets. figure c.4 shows hypernym chains for bass3
and bass7.

in this depiction of hyponymy, successively more general synsets are shown on
successive indented lines. the    rst chain starts from the concept of a human bass
singer. its immediate superordinate is a synset corresponding to the generic concept
of a singer. following this chain leads eventually to concepts such as entertainer and
person. the second chain, which starts from musical instrument, has a completely
different path leading eventually to such concepts as musical instrument, device, and
physical object. both paths do eventually join at the very abstract synset whole, unit,
and then proceed together to entity which is the top (root) of the noun hierarchy (in
id138 this root is generally called the unique beginner).

unique
beginner

c.3 word similarity: thesaurus methods

in chapter 6 we introduced the embedding and cosine architecture for computing the
similarity between two words. a thesaurus offers a different family of algorithms
that can be complementary.

although we have described them as relations between words, similar is actually
a relationship between word senses. for example, of the two senses of bank, we

c.3

    word similarity: thesaurus methods

499

sense 3
bass, basso --
(an adult male singer with the lowest voice)
=> singer, vocalist, vocalizer, vocaliser

=> musician, instrumentalist, player

=> performer, performing artist

=> entertainer

=> person, individual, someone...

=> organism, being

=> living thing, animate thing,

=> whole, unit

=> object, physical object

=> physical entity

=> entity

=> causal agent, cause, causal agency

=> physical entity

=> entity

sense 7
bass --
(the member with the lowest range of a family of
musical instruments)
=> musical instrument, instrument

=> device

=> instrumentality, instrumentation

=> artifact, artefact

=> whole, unit

=> object, physical object

=> physical entity

=> entity

figure c.4 hyponymy chains for two separate senses of the lemma bass. note that the
chains are completely distinct, only converging at the very abstract level whole, unit.

might say that the    nancial sense is similar to one of the senses of fund and the
riparian sense is more similar to one of the senses of slope. in the next few sections
of this chapter, we will compute these relations over both words and senses.

the thesaurus-based algorithms use the structure of the thesaurus to de   ne word
similarity. in principle, we could measure similarity by using any information avail-
able in a thesaurus (meronymy, glosses, etc.). in practice, however, thesaurus-based
word similarity algorithms generally use only the hypernym/hyponym (is-a or sub-
sumption) hierarchy. in id138, verbs and nouns are in separate hypernym hier-
archies, so a thesaurus-based algorithm for id138 can thus compute only noun-
noun similarity, or verb-verb similarity; we can   t compare nouns to verbs or do
anything with adjectives or other parts of speech.

the simplest thesaurus-based algorithms are based on the intuition that words
or senses are more similar if there is a shorter path between them in the thesaurus
graph, an intuition dating back to quillian (1969). a word/sense is most similar to
itself, then to its parents or siblings, and least similar to words that are far away. we
make this notion operational by measuring the number of edges between the two
concept nodes in the thesaurus graph and adding one. figure c.5 shows an intuition;
the concept dime is most similar to nickel and coin, less similar to money, and even
less similar to richter scale. a formal de   nition:

pathlen(c1,c2) = 1 + the number of edges in the shortest path in the

500 appendix c     id138: word relations, senses, and disambiguation

figure c.5 a fragment of the id138 hypernym hierarchy, showing path lengths (number
of edges plus 1) from nickel to coin (2), dime (3), money (6), and richter scale (8).

thesaurus graph between the sense nodes c1 and c2

path-based similarity can be de   ned as just the path length, transformed either by
log (leacock and chodorow, 1998) or, more often, by an inverse, resulting in the
following common de   nition of path-length based similarity:

path-length
based similarity

simpath(c1,c2) =

1

pathlen(c1,c2)

(c.17)

for most applications, we don   t have sense-tagged data, and thus we need our
algorithm to give us the similarity between words rather than between senses or con-
cepts. for any of the thesaurus-based algorithms, following resnik (1995), we can
approximate the correct similarity (which would require sense disambiguation) by
just using the pair of senses for the two words that results in maximum sense sim-
ilarity. thus, based on sense similarity, we can de   ne word similarity as follows:

word similarity

wordsim(w1,w2) =

max

c1   senses(w1)
c2   senses(w2)

sim(c1,c2)

(c.18)

the basic path-length algorithm makes the implicit assumption that each link
in the network represents a uniform distance. in practice, this assumption is not
appropriate. some links (e.g., those that are deep in the id138 hierarchy) often
seem to represent an intuitively narrow distance, while other links (e.g., higher up
in the id138 hierarchy) represent an intuitively wider distance. for example, in
fig. c.5, the distance from nickel to money (5) seems intuitively much shorter than
the distance from nickel to an abstract word standard; the link between medium of
exchange and standard seems wider than that between, say, coin and coinage.

it is possible to re   ne path-based algorithms with id172s based on depth
in the hierarchy (wu and palmer, 1994), but in general we   d like an approach that
lets us independently represent the distance associated with each edge.

a second class of thesaurus-based similarity algorithms attempts to offer just
such a    ne-grained metric. these information-content word-similarity algorithms
still rely on the structure of the thesaurus but also add probabilistic information
derived from a corpus.

following resnik (1995) we   ll de   ne p(c) as the id203 that a randomly
selected word in a corpus is an instance of concept c (i.e., a separate random variable,
ranging over words, associated with each concept). this implies that p(root) = 1
since any word is subsumed by the root concept. intuitively, the lower a concept

information-
content

c.3

    word similarity: thesaurus methods

501

in the hierarchy, the lower its id203. we train these probabilities by counting
in a corpus; each word in the corpus counts as an occurrence of each concept that
contains it. for example, in fig. c.5 above, an occurrence of the word dime would
count toward the frequency of coin, currency, standard, etc. more formally, resnik
computes p(c) as follows:

p(c) =(cid:80)w   words(c) count(w)

n

(c.19)

where words(c) is the set of words subsumed by concept c, and n is the total number
of words in the corpus that are also present in the thesaurus.

figure c.6, from lin (1998), shows a fragment of the id138 concept hierar-

chy augmented with the probabilities p(c).

entity 0.395

inanimate-object 0.167

natural-object 0.0163

geological-formation 0.00176

0.000113 natural-elevation

shore 0.0000836

0.0000189 hill

coast 0.0000216

figure c.6 a fragment of the id138 hierarchy, showing the id203 p(c) attached to
each content, adapted from a    gure from lin (1998).

we now need two additional de   nitions. first, following basic information the-

ory, we de   ne the information content (ic) of a concept c as

second, we de   ne the lowest common subsumer or lcs of two concepts:

ic(c) =    logp(c)

(c.20)

lcs(c1,c2) = the lowest common subsumer, that is, the lowest node in
the hierarchy that subsumes (is a hypernym of) both c1 and c2

there are now a number of ways to use the information content of a node in a
word similarity metric. the simplest way was    rst proposed by resnik (1995). we
think of the similarity between two words as related to their common information;
the more two words have in common, the more similar they are. resnik proposes
to estimate the common amount of information by the information content of the
lowest common subsumer of the two nodes. more formally, the resnik similarity
measure is

simresnik(c1,c2) =    logp(lcs(c1,c2))

(c.21)
lin (1998) extended the resnik intuition by pointing out that a similarity metric
between objects a and b needs to do more than measure the amount of information
in common between a and b. for example, he additionally pointed out that the more
differences between a and b, the less similar they are. in summary:

lowest
common
subsumer
lcs

resnik
similarity

502 appendix c     id138: word relations, senses, and disambiguation

similar they are.

    commonality: the more information a and b have in common, the more
    difference: the more differences between the information in a and b, the less
lin measures the commonality between a and b as the information content of

similar they are.

the proposition that states the commonality between a and b:

ic(common(a,b))

(c.22)

he measures the difference between a and b as

(c.23)
where description(a,b) describes a and b. given a few additional assumptions
about similarity, lin proves the following theorem:

ic(description(a,b))    ic(common(a,b))

similarity theorem: the similarity between a and b is measured by the ratio
between the amount of information needed to state the commonality of a and
b and the information needed to fully describe what a and b are.

simlin(a,b) =

common(a,b)
description(a,b)

(c.24)

applying this idea to the thesaurus domain, lin shows (in a slight modi   cation
of resnik   s assumption) that the information in common between two concepts is
twice the information in the lowest common subsumer lcs(c1,c2). adding in the
above de   nitions of the information content of thesaurus concepts, the    nal lin
similarity function is

simlin(c1,c2) =

2   logp(lcs(c1,c2))
logp(c1) + logp(c2)

(c.25)

for example, using simlin, lin (1998) shows that the similarity between the

concepts of hill and coast from fig. c.6 is

lin similarity

jiang-conrath
distance

simlin(hill,coast) =

2   logp(geological-formation)

logp(hill) + logp(coast)

= 0.59

(c.26)

a similar formula, jiang-conrath distance (jiang and conrath, 1997), although
derived in a completely different way from lin and expressed as a distance rather
than similarity function, has been shown to work as well as or better than all the
other thesaurus-based methods:

distjc(c1,c2) = 2   logp(lcs(c1,c2))    (logp(c1) + logp(c2))

(c.27)

we can transform distjc into a similarity by taking the reciprocal.

finally, we describe a dictionary-based method that is related to the lesk al-
gorithm for id51 we will introduce in section c.6.1. the
intution of extended gloss overlap, or extended lesk measure (banerjee and ped-
ersen, 2003) is that two concepts/senses in a thesaurus are similar if their glosses
contain overlapping words. we   ll begin by sketching an overlap function for two
glosses. consider these two concepts, with their glosses:

extended gloss
overlap
extended lesk

c.3

    word similarity: thesaurus methods

503

or glass or metal surface.

    drawing paper: paper that is specially prepared for use in drafting
    decal: the art of transferring designs from specially prepared paper to a wood
for each n-word phrase that occurs in both glosses, extended lesk adds in a
score of n2 (the relation is non-linear because of the zip   an relationship between
lengths of phrases and their corpus frequencies; longer overlaps are rare, so they
should be weighted more heavily). here, the overlapping phrases are paper and
specially prepared, for a total similarity score of 12 + 22 = 5.

given such an overlap function, when comparing two concepts (synsets), ex-
tended lesk not only looks for overlap between their glosses but also between the
glosses of the senses that are hypernyms, hyponyms, meronyms, and other relations
of the two concepts. for example, if we just considered hyponyms and de   ned
gloss(hypo(a)) as the concatenation of all the glosses of all the hyponym senses of
a, the total relatedness between two concepts a and b might be

similarity(a,b) = overlap(gloss(a), gloss(b))

+overlap(gloss(hypo(a)), gloss(hypo(b)))
+overlap(gloss(a), gloss(hypo(b)))
+overlap(gloss(hypo(a)),gloss(b))

let rels be the set of possible id138 relations whose glosses we compare;
assuming a basic overlap measure as sketched above, we can then de   ne the ex-
tended lesk overlap measure as

simelesk(c1,c2) = (cid:88)r,q   rels

overlap(gloss(r(c1)),gloss(q(c2)))

(c.28)

simpath(c1,c2) =

1

pathlen(c1,c2)

simresnik(c1,c2) =    logp(lcs(c1,c2))
2   logp(lcs(c1,c2))
logp(c1) + logp(c2)

simlin(c1,c2) =

simjc(c1,c2) =

simelesk(c1,c2) = (cid:88)r,q   rels

1

2   logp(lcs(c1,c2))    (logp(c1) + logp(c2))

overlap(gloss(r(c1)),gloss(q(c2)))

figure c.7 five thesaurus-based (and dictionary-based) similarity measures.

figure c.7 summarizes the    ve similarity measures we have described in this

section.

evaluating thesaurus-based similarity
which of these similarity measures is best? word similarity measures have been
evaluated in two ways, introduced in chapter 6. the most common intrinsic evalu-
ation metric computes the correlation coef   cient between an algorithm   s word sim-
ilarity scores and word similarity ratings assigned by humans. there are a variety

504 appendix c     id138: word relations, senses, and disambiguation

of such human-labeled datasets: the rg-65 dataset of human similarity ratings on
65 word pairs (rubenstein and goodenough, 1965), the mc-30 dataset of 30 word
pairs (miller and charles, 1991). the wordsim-353 (finkelstein et al., 2002) is a
commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane,
car) had an average score of 5.77. siid113x-999 (hill et al., 2015) is a more dif   cult
dataset that quanti   es similarity (cup, mug) rather than relatedness (cup, coffee), and
including both concrete and abstract adjective, noun and verb pairs. another com-
mon intrinic similarity measure is the toefl dataset, a set of 80 questions, each
consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: levied is closest in meaning to:
imposed, believed, requested, correlated (landauer and dumais, 1997). all of these
datasets present words without context.

slightly more realistic are intrinsic similarity tasks that include context. the
stanford contextual word similarity (scws) dataset (huang et al., 2012) offers a
richer evaluation scenario, giving human judgments on 2,003 pairs of words in their
sentential context, including nouns, verbs, and adjectives. this dataset enables the
evaluation of word similarity algorithms that can make use of context words. the
semantic textual similarity task (agirre et al. 2012, agirre et al. 2015) evaluates the
performance of sentence-level similarity algorithms, consisting of a set of pairs of
sentences, each pair with human-labeled similarity scores.

alternatively, the similarity measure can be embedded in some end-application,
such as id53 or spell-checking, and different measures can be evalu-
ated by how much they improve the end application.

c.4 id51: overview

word sense
disambiguation
wsd

the task of selecting the correct sense for a word is called word sense disambigua-
tion, or wsd. wsd algorithms take as input a word in context and a    xed inventory
of potential word senses and outputs the correct word sense in context. the input and
the senses depends on the task. for machine translation from english to spanish, the
sense tag inventory for an english word might be the set of different spanish trans-
lations. for automatic indexing of medical articles, the sense-tag inventory might be
the set of mesh (medical subject headings) thesaurus entries.

when we are evaluating wsd in isolation, we can use the set of senses from a
dictionary/thesaurus resource like id138. figure c.4 shows an example for the
word bass, which can refer to a musical instrument or a kind of    sh.2

roget

target word in context

id138 spanish
sense
bass4
bass4
bass7
bass7
figure c.8 possible de   nitions for the inventory of sense tags for bass.

translation category
lubina
lubina
bajo
bajo

fish/insect . . .    sh as paci   c salmon and striped bass and. . .
fish/insect . . . produce    lets of smoked bass or sturgeon. . .
. . . exciting jazz bass player since ray brown. . .
music
. . . play bass because he doesn   t have to solo. . .
music

lexical sample

it is useful to distinguish two wsd tasks. in the lexical sample task, a small

2 the id138 database includes eight senses; we have arbitrarily selected two for this example; we
have also arbitrarily selected one of the many spanish    shes that could translate english sea bass.

all-words

c.5

    supervised id51

505

pre-selected set of target words is chosen, along with an inventory of senses for each
word from some lexicon. since the set of words and the set of senses are small,
simple supervised classi   cation approaches are used.

in the all-words task, systems are given entire texts and a lexicon with an inven-
tory of senses for each entry and are required to disambiguate every content word in
the text. the all-words task is similar to part-of-speech tagging, except with a much
larger set of tags since each lemma has its own set. a consequence of this larger set
of tags is data sparseness; it is unlikely that adequate training data for every word in
the test set will be available. moreover, given the number of polysemous words in
reasonably sized lexicons, approaches based on training one classi   er per term are
unlikely to be practical.

c.5 supervised id51

semantic
concordance

collocation

bag of word

supervised wsd is commonly used whenever we have suf   cient data that has been
hand-labeled with correct word senses.
datasets: the are various lexical sample datasets with context sentences labeled
with the correct sense for the target word, such as the line-hard-serve corpus with
4,000 sense-tagged examples of line as a noun, hard as an adjective and serve as a
verb (leacock et al., 1993), and the interest corpus with 2,369 sense-tagged exam-
ples of interest as a noun (bruce and wiebe, 1994). the senseval project has also
produced a number of such sense-labeled lexical sample corpora (senseval-1 with
34 words from the hector lexicon and corpus (kilgarriff and rosenzweig 2000,
atkins 1993), senseval-2 and -3 with 73 and 57 target words, respectively (palmer
et al. 2001, kilgarriff 2001). all-word disambiguation tasks are trained from a se-
mantic concordance, a corpus in which each open-class word in each sentence is
labeled with its word sense from a speci   c dictionary or thesaurus. one commonly
used corpus is semcor, a subset of the brown corpus consisting of over 234,000
words that were manually tagged with id138 senses (miller et al. 1993, landes
et al. 1998). in addition, sense-tagged corpora have been built for the senseval all-
word tasks. the senseval-3 english all-words test data consisted of 2081 tagged
content word tokens, from 5,000 total running words of english from the wsj and
brown corpora (palmer et al., 2001).
features supervised wsd algorithms can use any standard classi   cation algo-
rithm. features generally include the word identity, part-of-speech tags, and embed-
dings of surrounding words, usually computed in two ways: collocation features are
words or id165s at a particular location, (i.e., exactly one word to the right, or the
two words starting 3 words to the left, and so on). bag of word features are rep-
resented as a vector with the dimensionality of the vocabulary (minus stop words),
with a 1 if that word occurs in the in the neighborhood of the target word.

consider the ambiguous word bass in the following wsj sentence:

(c.29) an electric guitar and bass player stand off to one side,
if we used a small 2-word window, a standard feature vector might include a bag of
words, parts-of-speech, unigram and bigram collocation features, and embeddings,
that is:

[wi   2,posi   2,wi   1,posi   1,wi+1,posi+1,wi+2,posi+2,
i+1,e(wi   2,wi   1,wi+1,wi+2),bag()]

wi   1
i   2,wi+2

(c.30)

506 appendix c     id138: word relations, senses, and disambiguation

would yield the following vector:

[guitar, nn, and, cc, player, nn, stand, vb, and guitar, player stand,

e(guitar,and,player,stand), bag(guitar,player,stand)]

high performing systems generally use pos tags and word collocations of length
1, 2, and 3 from a window of words 3 to the left and 3 to the right (zhong and ng,
2010). the embedding function could just take the average of the embeddings of
the words in the window, or a more complicated embedding function can be used
(iacobacci et al., 2016).

c.5.1 wikipedia as a source of training data
one way to increase the amount of training data is to use wikipedia as a source of
sense-labeled data. when a concept is mentioned in a wikipedia article, the article
text may contain an explicit link to the concept   s wikipedia page, which is named
by a unique identi   er. this link can be used as a sense annotation. for example,
the ambiguous word bar is linked to a different wikipedia article depending on its
meaning in context, including the page bar (law), the page bar (music), and
so on, as in the following wikipedia examples (mihalcea, 2007).

in 1834, sumner was admitted to the [[bar (law)|bar]] at the age of
twenty-three, and entered private practice in boston.
it is danced in 3/4 time (like most waltzes), with the couple turning
approx. 180 degrees every [[bar (music)|bar]].
jenga is a popular beer in the [[bar (establishment)|bar]]s of thailand.

these sentences can then be added to the training data for a supervised system.
in order to use wikipedia in this way, however, it is necessary to map from wikipedia
concepts to whatever inventory of senses is relevant for the wsd application. auto-
matic algorithms that map from wikipedia to id138, for example, involve    nding
the id138 sense that has the greatest lexical overlap with the wikipedia sense, by
comparing the vector of words in the id138 synset, gloss, and related senses with
the vector of words in the wikipedia page title, outgoing links, and page category
(ponzetto and navigli, 2010).

c.5.2 evaluation
to evaluate wsd algorithms, it   s better to consider extrinsic, task-based, or end-
to-end evaluation, in which we see whether some new wsd idea actually improves
performance in some end-to-end application like id53 or machine
translation. nonetheless, because extrinsic evaluations are dif   cult and slow, wsd
systems are typically evaluated with intrinsic evaluation. in which a wsd compo-
nent is treated as an independent system. common intrinsic evaluations are either
exact-match sense accuracy   the percentage of words that are tagged identically
with the hand-labeled sense tags in a test set   or with precision and recall if sys-
tems are permitted to pass on the labeling of some instances. in general, we evaluate
by using held-out data from the same sense-tagged corpora that we used for training,
such as the semcor corpus discussed above or the various corpora produced by the
senseval effort.

many aspects of sense evaluation have been standardized by the senseval and
semeval efforts (palmer et al. 2006, kilgarriff and palmer 2000). this framework
provides a shared task with training and testing materials along with sense invento-
ries for all-words and lexical sample tasks in a variety of languages.

extrinsic
evaluation

intrinsic

sense accuracy

most frequent
sense

c.6

    wsd: dictionary and thesaurus methods

507

the normal baseline is to choose the most frequent sense for each word from the
senses in a labeled corpus (gale et al., 1992a). for id138, this corresponds to the
   rst sense, since senses in id138 are generally ordered from most frequent to least
frequent. id138 sense frequencies come from the semcor sense-tagged corpus
described above    id138 senses that don   t occur in semcor are ordered arbitrarily
after those that do. the most frequent sense baseline can be quite accurate, and is
therefore often used as a default, to supply a word sense when a supervised algorithm
has insuf   cient training data.

c.6 wsd: dictionary and thesaurus methods

supervised algorithms based on sense-labeled corpora are the best-performing algo-
rithms for sense disambiguation. however, such labeled training data is expensive
and limited. one alternative is to get indirect supervision from dictionaries and the-
sauruses, and so this method is also called knowledge-based wsd. methods like
this that do not use texts that have been hand-labeled with senses are also called
weakly supervised.

c.6.1 the lesk algorithm
the most well-studied dictionary-based algorithm for sense disambiguation is the
lesk algorithm, really a family of algorithms that choose the sense whose dictio-
nary gloss or de   nition shares the most words with the target word   s neighborhood.
figure c.9 shows the simplest version of the algorithm, often called the simpli   ed
lesk algorithm (kilgarriff and rosenzweig, 2000).

lesk algorithm

simpli   ed lesk

function simplified lesk(word, sentence) returns best sense of word
best-sense   most frequent sense for word
max-overlap   0
context   set of words in sentence
for each sense in senses of word do
signature   set of words in the gloss and examples of sense
overlap    computeoverlap(signature, context)
if overlap > max-overlap then

max-overlap   overlap
best-sense   sense

end
return(best-sense)

figure c.9 the simpli   ed lesk algorithm. the computeoverlap function returns the
number of words in common between two sets, ignoring function words or other words on a
stop list. the original lesk algorithm de   nes the context in a more complex way. the cor-
pus lesk algorithm weights each overlapping word w by its    logp(w) and includes labeled
training corpus data in the signature.

as an example of the lesk algorithm at work, consider disambiguating the word

bank in the following context:
(c.31) the bank can guarantee deposits will eventually cover future tuition costs

because it invests in adjustable-rate mortgage securities.

508 appendix c     id138: word relations, senses, and disambiguation

given the following two id138 senses:

bank1 gloss:

examples:

bank2 gloss:

examples:

a    nancial institution that accepts deposits and channels the
money into lending activities
   he cashed a check at the bank   ,    that bank holds the mortgage
on my home   
sloping land (especially the slope beside a body of water)
   they pulled the canoe up on the bank   ,    he sat on the bank of
the river and watched the currents   

sense bank1 has two non-stopwords overlapping with the context in (c.31):
deposits and mortgage, while sense bank2 has zero words, so sense bank1 is chosen.
there are many obvious extensions to simpli   ed lesk. the original lesk algo-
rithm (lesk, 1986) is slightly more indirect. instead of comparing a target word   s
signature with the context words, the target signature is compared with the signatures
of each of the context words. for example, consider lesk   s example of selecting the
appropriate sense of cone in the phrase pine cone given the following de   nitions for
pine and cone.

pine 1 kinds of evergreen tree with needle-shaped leaves

2 waste away through sorrow or illness
cone 1 solid body which narrows to a point

2 something of this shape whether solid or hollow
3 fruit of certain evergreen trees

corpus lesk

inverse
document
frequency
idf

in this example, lesk   s method would select cone3 as the correct sense since
two of the words in its entry, evergreen and tree, overlap with words in the entry for
pine, whereas neither of the other entries has any overlap with words in the de   nition
of pine. in general simpli   ed lesk seems to work better than original lesk.

the primary problem with either the original or simpli   ed approaches, how-
ever, is that the dictionary entries for the target words are short and may not provide
enough chance of overlap with the context.3 one remedy is to expand the list of
words used in the classi   er to include words related to, but not contained in, their
individual sense de   nitions. but the best solution, if any sense-tagged corpus data
like semcor is available, is to add all the words in the labeled corpus sentences for a
word sense into the signature for that sense. this version of the algorithm, the cor-
pus lesk algorithm, is the best-performing of all the lesk variants (kilgarriff and
rosenzweig 2000, vasilescu et al. 2004) and is used as a baseline in the senseval
competitions. instead of just counting up the overlapping words, the corpus lesk
algorithm also applies a weight to each overlapping word. the weight is the inverse
document frequency or idf, a standard information-retrieval measure introduced
in chapter 6. idf measures how many different    documents    (in this case, glosses
and examples) a word occurs in and is thus a way of discounting function words.
since function words like the, of, etc., occur in many documents, their idf is very
low, while the idf of content words is high. corpus lesk thus uses idf instead of a
stop list.

formally, the idf for a word i can be de   ned as

ndi (cid:19)
idfi = log(cid:18) ndoc

(c.32)

indeed, lesk (1986) notes that the performance of his system seems to roughly correlate with the

3
length of the dictionary entries.

c.6

    wsd: dictionary and thesaurus methods

509

where ndoc is the total number of    documents    (glosses and examples) and ndi is
the number of these documents containing word i.

finally, we can combine the lesk and supervised approaches by adding new
lesk-like bag-of-words features. for example, the glosses and example sentences
for the target sense in id138 could be used to compute the supervised bag-of-
words features in addition to the words in the semcor context sentence for the sense
(yuret, 2004).

c.6.2 graph-based methods
another way to use a thesaurus like id138 is to make use of the fact that id138
can be construed as a graph, with senses as nodes and relations between senses
as edges. in addition to the hypernymy and other relations, it   s possible to create
links between senses and those words in the gloss that are unambiguous (have only
one sense). often the relations are treated as undirected edges, creating a large
undirected id138 graph. fig. c.10 shows a portion of the graph around the word
drink1
v.

figure c.10 part of the id138 graph around drink1

v, after navigli and lapata (2010).

there are various ways to use the graph for disambiguation, some using the
whole graph, some using only a subpart. for example the target word and the words
in its sentential context can all be inserted as nodes in the graph via a directed edge
to each of its senses. if we consider the sentence she drank some milk, fig. c.11
shows a portion of the id138 graph between the senses drink1

v and milk1
n.

figure c.11 part of the id138 graph between drink1
sentence like she drank some milk, adapted from navigli and lapata (2010).

v and milk1

n, for disambiguating a

the correct sense is then the one which is the most important or central in some
way in this graph. there are many different methods for deciding centrality. the

toastn4drinkv1drinkern1drinkingn1potationn1sipn1sipv1beveragen1milkn1liquidn1foodn1drinkn1helpingn1supv1consumptionn1consumern1consumev1drinkv1drinkern1beveragen1boozingn1foodn1drinkn1milkn1milkn2milkn3milkn4drinkv2drinkv3drinkv4drinkv5nutrimentn1   drink      milk   510 appendix c     id138: word relations, senses, and disambiguation

degree

personalized
page rank

simplest is degree, the number of edges into the node, which tends to correlate
with the most frequent sense. another algorithm for assigning probabilities across
nodes is personalized page rank, a version of the well-known id95 algorithm
which uses some seed nodes. by inserting a uniform id203 across the word
nodes (drink and milk in the example) and computing the personalized page rank of
the graph, the result will be a id95 value for each node in the graph, and the
sense with the maximum id95 can then be chosen. see agirre et al. (2014) and
navigli and lapata (2010) for details.

c.7 semi-supervised wsd: id64

id64

yarowsky
algorithm

both the supervised approach and the dictionary-based approaches to wsd require
large hand-built resources: supervised training sets in one case, large dictionaries in
the other. we can instead use id64 or semi-supervised learning, which
needs only a very small hand-labeled training set.

a classic id64 algorithm for wsd is the yarowsky algorithm for
learning a classi   er for a target word (in a lexical-sample task) (yarowsky, 1995).
the algorithm is given a small seedset   0 of labeled instances of each sense and a
much larger unlabeled corpus v0. the algorithm    rst trains an initial classi   er on
the seedset   0. it then uses this classi   er to label the unlabeled corpus v0. the
algorithm then selects the examples in v0 that it is most con   dent about, removes
them, and adds them to the training set (call it now   1). the algorithm then trains a
new classi   er (a new set of rules) on   1, and iterates by applying the classi   er to the
now-smaller unlabeled set v1, extracting a new training set   2, and so on. with each
iteration of this process, the training corpus grows and the untagged corpus shrinks.
the process is repeated until some suf   ciently low error-rate on the training set is
reached or until no further examples from the untagged corpus are above threshold.

figure c.12 the yarowsky algorithm disambiguating    plant    at two stages;    ?    indicates an unlabeled ob-
servation, a and b are observations labeled as sense-a or sense-b. the initial stage (a) shows only seed
sentences   0 labeled by collocates (   life    and    manufacturing   ). an intermediate stage is shown in (b) where
more collocates have been discovered (   equipment   ,    microscopic   , etc.) and more instances in v0 have been
moved into   1, leaving a smaller unlabeled set v1. figure adapted from yarowsky (1995).

??a?a?a???a?????????????b??a???a?aaa?aa????????????b??????????????????????????????????????????????????????????????????b???bbb??b??b??????????????????????????????????abb??????????????????a????a???aaaaaaalifebbmanufacturing??a?a?a?a?ab????????????b?????a?a?a?aaaaaa????????????b????????????????????????????????????a?a???????????????abaa?????????b???bbb??b?bb?????????????????aa???????????????abb??b??????????bb??a?aa?a???aaaaaaalifebbmanufacturingequipmentemployee???bb???????animalmicroscopicv0v1  0  1(a)(b)c.8

    unsupervised word sense induction

511

we need more good teachers     right now, there are only a half a dozen who can play
the free bass with ease.
an electric guitar and bass player stand off to one side, not really part of the scene,
the researchers said the worms spend part of their life cycle in such    sh as paci   c
salmon and striped bass and paci   c rock   sh or snapper.
and it all started when    shermen decided the striped bass in lake mead were...

figure c.13 samples of bass sentences extracted from the wsj by using the simple corre-
lates play and    sh.

initial seeds can be selected by hand-labeling a small set of examples (hearst,
1991), or by using the help of a heuristic. yarowsky (1995) used the one sense
per collocation heuristic, which relies on the intuition that certain words or phrases
strongly associated with the target senses tend not to occur with the other sense.
yarowsky de   nes his seedset by choosing a single collocation for each sense.

for example, to generate seed sentences for the    sh and musical musical senses
of bass, we might come up with    sh as a reasonable indicator of bass1 and play as
a reasonable indicator of bass2. figure c.13 shows a partial result of such a search
for the strings       sh    and    play    in a corpus of bass examples drawn from the wsj.
the original yarowsky algorithm also makes use of a second heuristic, called
one sense per discourse, based on the work of gale et al. (1992b), who noticed that
a particular word appearing multiple times in a text or discourse often appeared with
the same sense. this heuristic seems to hold better for coarse-grained senses and
particularly for cases of homonymy rather than polysemy (krovetz, 1998).

nonetheless, it is still useful in a number of sense disambiguation situations. in
fact, the one sense per discourse heuristic is an important one throughout language
processing as it seems that many disambiguation tasks may be improved by a bias
toward resolving an ambiguity the same way inside a discourse segment.

one sense per
collocation

one sense per
discourse

c.8 unsupervised word sense induction

word sense
induction

it is expensive and dif   cult to build large corpora in which each word is labeled for
its word sense. for this reason, an unsupervised approach to sense disambiguation,
often called word sense induction or wsi, is an important direction.
in unsu-
pervised approaches, we don   t use human-de   ned word senses. instead, the set of
   senses    of each word is created automatically from the instances of each word in
the training set.

most algorithms for word sense induction use some sort of id91 over word
embeddings. (the earliest algorithms, due to sch  utze (sch  utze 1992b, sch  utze 1998),
represented each word as a context vector of bag-of-words features(cid:126)c.) then in train-
ing, we use three steps.

1. for each token wi of word w in a corpus, compute a context vector (cid:126)c.
2. use a id91 algorithm to cluster these word-token context vectors(cid:126)c into
a prede   ned number of groups or clusters. each cluster de   nes a sense of w.
3. compute the vector centroid of each cluster. each vector centroid (cid:126)s j is a

sense vector representing that sense of w.

since this is an unsupervised algorithm, we don   t have names for each of these

   senses    of w; we just refer to the jth sense of w.

512 appendix c     id138: word relations, senses, and disambiguation

agglomerative
id91

to disambiguate a particular token t of w we again have three steps:
1. compute a context vector (cid:126)c for t.
2. retrieve all sense vectors s j for w.
3. assign t to the sense represented by the sense vector s j that is closest to t.
all we need is a id91 algorithm and a distance metric between vectors.
id91 is a well-studied problem with a wide number of standard algorithms that
can be applied to inputs structured as vectors of numerical values (duda and hart,
1973). a frequently used technique in language applications is known as agglom-
erative id91.
in this technique, each of the n training instances is initially
assigned to its own cluster. new clusters are then formed in a bottom-up fashion by
the successive merging of the two clusters that are most similar. this process con-
tinues until either a speci   ed number of clusters is reached, or some global goodness
measure among the clusters is achieved. in cases in which the number of training
instances makes this method too expensive, random sampling can be used on the
original training set to achieve similar results.

how can we evaluate unsupervised sense disambiguation approaches? as usual,
the best way is to do extrinsic evaluation embedded in some end-to-end system; one
example used in a semeval bakeoff is to improve search result id91 and di-
versi   cation (navigli and vannella, 2013). intrinsic evaluation requires a way to
map the automatically derived sense classes into a hand-labeled gold-standard set so
that we can compare a hand-labeled test set with a set labeled by our unsupervised
classi   er. various such metrics have been tested, for example in the semeval tasks
(manandhar et al. 2010, navigli and vannella 2013, jurgens and klapaftis 2013),
including cluster overlap metrics, or methods that map each sense cluster to a pre-
de   ned sense by choosing the sense that (in some training set) has the most overlap
with the cluster. however it is fair to say that no evaluation metric for this task has
yet become standard.

c.9 summary

this chapter has covered a wide range of issues concerning the meanings associated
with lexical items. the following are among the highlights:

are de   ned at the level of the word sense rather than wordforms.

polysemy is the relation between related senses that share a form.

    a word sense is the locus of word meaning; de   nitions and meaning relations
    homonymy is the relation between unrelated senses that share a form, and
    hyponymy and hypernymy relations hold between words that are in a class-
    id138 is a large database of lexical relations for english.
    word-sense disambiguation (wsd) is the task of determining the correct
sense of a word in context. supervised approaches make use of sentences in
which individual words (lexical sample task) or all words (all-words task)
are hand-labeled with senses from a resource like id138.

inclusion relationship.

rounding words.

    classi   ers for supervised wsd are generally trained on features of the sur-
    an important baseline for wsd is the most frequent sense, equivalent, in

id138, to take the    rst sense.

bibliographical and historical notes

513

most words with the target word   s neighborhood.

    the lesk algorithm chooses the sense whose dictionary de   nition shares the
    graph-based algorithms view the thesaurus as a graph and choose the sense
    word similarity can be computed by measuring the link distance in a the-
saurus or by various measures of the information content of the two nodes.

that is most central in some way.

bibliographical and historical notes

id51 traces its roots to some of the earliest applications of
digital computers. the insight that underlies modern algorithms for word sense
disambiguation was    rst articulated by weaver (1955) in the context of machine
translation:

if one examines the words in a book, one at a time as through an opaque
mask with a hole in it one word wide, then it is obviously impossible
to determine, one at a time, the meaning of the words. [. . . ] but if
one lengthens the slit in the opaque mask, until one can see not only
the central word in question but also say n words on either side, then
if n is large enough one can unambiguously decide the meaning of the
central word. [. . . ] the practical question is :    what minimum value of
n will, at least in a tolerable fraction of cases, lead to the correct choice
of meaning for the central word?   

other notions    rst proposed in this early period include the use of a thesaurus for dis-
ambiguation (masterman, 1957), supervised training of bayesian models for disam-
biguation (madhu and lytel, 1965), and the use of id91 in word sense analysis
(sparck jones, 1986).

an enormous amount of work on disambiguation was conducted within the con-
text of early ai-oriented natural language processing systems. quillian (1968) and
quillian (1969) proposed a graph-based approach to language understanding, in
which the dictionary de   nition of words was represented by a network of word nodes
connected by syntactic and semantic relations. he then proposed to do sense disam-
biguation by    nding the shortest path between senses in the conceptual graph. sim-
mons (1973) is another in   uential early semantic network approach. wilks proposed
one of the earliest non-discrete models with his preference semantics (wilks 1975c,
wilks 1975b, wilks 1975a), and small and rieger (1982) and riesbeck (1975) pro-
posed understanding systems based on modeling rich procedural information for
each word. hirst   s absity system (hirst and charniak 1982, hirst 1987, hirst 1988),
which used a technique called marker passing based on semantic networks, repre-
sents the most advanced system of this type. as with these largely symbolic ap-
proaches, early neural network (at the time called    connectionist   ) approaches to
id51 relied on small lexicons with hand-coded representa-
tions (cottrell 1985, kawamoto 1988). considerable work on sense disambiguation
has also been conducted in in psycholinguistics, under the name    lexical ambiguity
resolution   . small et al. (1988) present a variety of papers from this perspective.

the earliest implementation of a robust empirical approach to sense disambigua-
tion is due to kelly and stone (1975), who directed a team that hand-crafted a set
of disambiguation rules for 1790 ambiguous english words. lesk (1986) was the

514 appendix c     id138: word relations, senses, and disambiguation

coarse senses

ontonotes

generative
lexicon
qualia
structure

   rst to use a machine-readable dictionary for id51. the prob-
lem of dictionary senses being too    ne-grained has been addressed by id91
word senses into coarse senses (dolan 1994, chen and chang 1998, mihalcea and
moldovan 2001, agirre and de lacalle 2003, chklovski and mihalcea 2003, palmer
et al. 2004, navigli 2006, snow et al. 2007). corpora with clustered word senses for
training id91 algorithms include palmer et al. (2006) and ontonotes (hovy
et al., 2006).

supervised approaches to disambiguation began with the use of id90 by
black (1988). the need for large amounts of annotated text in these methods led to
investigations into the use of id64 methods (hearst 1991, yarowsky 1995).
diab and resnik (2002) give a semi-supervised algorithm for sense disambigua-
tion based on aligned parallel corpora in two languages. for example, the fact that
the french word catastrophe might be translated as english disaster in one instance
and tragedy in another instance can be used to disambiguate the senses of the two
english words (i.e., to choose senses of disaster and tragedy that are similar). ab-
ney (2002) and abney (2004) explore the mathematical foundations of the yarowsky
algorithm and its relation to co-training. the most-frequent-sense heuristic is an ex-
tremely powerful one but requires large amounts of supervised training data.

the earliest use of id91 in the study of word senses was by sparck jones
(1986); pedersen and bruce (1997), sch  utze (1997b), and sch  utze (1998) applied
distributional methods. recent work on word sense induction has applied latent
dirichlet allocation (lda) (boyd-graber et al. 2007, brody and lapata 2009, lau
et al. 2012). and large co-occurrence graphs (di marco and navigli, 2013).

a collection of work concerning id138 can be found in fellbaum (1998).
early work using dictionaries as lexical resources include amsler   s (1981) use of the
merriam webster dictionary and longman   s dictionary of contemporary english
(boguraev and briscoe, 1989).

early surveys of wsd include agirre and edmonds (2006) and navigli (2009).
see pustejovsky (1995), pustejovsky and boguraev (1996), martin (1986), and
copestake and briscoe (1995), inter alia, for computational approaches to the rep-
resentation of polysemy. pustejovsky   s theory of the generative lexicon, and in
particular his theory of the qualia structure of words, is another way of accounting
for the dynamic systematic polysemy of words in context.

another important recent direction is the addition of sentiment and connotation
to knowledge bases (wiebe et al. 2005, qiu et al. 2009, velikovich et al. 2010)
including sentiid138 (baccianella et al., 2010) and connotationid138 (kang
et al., 2014).

exercises

c.1 collect a small corpus of example sentences of varying lengths from any
newspaper or magazine. using id138 or any standard dictionary, deter-
mine how many senses there are for each of the open-class words in each sen-
tence. how many distinct combinations of senses are there for each sentence?
how does this number seem to vary with sentence length?

c.2 using id138 or a standard reference dictionary, tag each open-class word
in your corpus with its correct tag. was choosing the correct sense always a
straightforward task? report on any dif   culties you encountered.

exercises

515

c.3 using your favorite dictionary, simulate the original lesk word overlap dis-
ambiguation algorithm described on page 508 on the phrase time    ies like an
arrow. assume that the words are to be disambiguated one at a time, from
left to right, and that the results from earlier decisions are used later in the
process.

c.4 build an implementation of your solution to the previous exercise. using
id138, implement the original lesk word overlap disambiguation algo-
rithm described on page 508 on the phrase time    ies like an arrow.

bibliography

abbreviations:

proceedings of the national conference on arti   cial intelligence
aaai
proceedings of the annual conference of the association for computational linguistics
acl
proceedings of the conference on applied natural language processing
anlp
papers from the annual regional meeting of the chicago linguistics society
cls
proceedings of the annual conference of the cognitive science society
cogsci
proceedings of the international conference on computational linguistics
coling
proceedings of the conference on computational natural language learning
conll
proceedings of the conference of the european association for computational linguistics
eacl
emnlp
proceedings of the conference on empirical methods in natural language processing
eurospeech proceedings of the european conference on speech communication and technology
icassp
icml
icphs
icslp
ijcai
interspeech proceedings of the annual interspeech conference
iwpt
jasa
lrec
muc
naacl-hlt
sigir

proceedings of the international workshop on parsing technologies
journal of the acoustical society of america
conference on language resources and evaluation
proceedings of the message understanding conference
proceedings of the north american chapter of the acl/human language technology conference
proceedings of annual conference of acm special interest group on information retrieval

proceedings of the ieee international conference on acoustics, speech, & signal processing
international conference on machine learning
proceedings of the international congress of phonetic sciences
proceedings of the international conference on spoken language processing
proceedings of the international joint conference on arti   cial intelligence

abadi, m., agarwal, a., barham, p., brevdo, e., chen, z.,
citro, c., corrado, g. s., davis, a., dean, j., devin, m.,
ghemawat, s., goodfellow, i., harp, a., irving, g., isard,
m., jia, y., jozefowicz, r., kaiser, l., kudlur, m., lev-
enberg, j., man  e, d., monga, r., moore, s., murray, d.,
olah, c., schuster, m., shlens, j., steiner, b., sutskever,
i., talwar, k., tucker, p., vanhoucke, v., vasudevan, v.,
vi  egas, f., vinyals, o., warden, p., wattenberg, m., wicke,
m., yu, y., and zheng, x. (2015). tensorflow: large-
scale machine learning on heterogeneous systems.. soft-
ware available from tensor   ow.org.
abney, s. p. (1991). parsing by chunks.
in berwick,
r. c., abney, s. p., and tenny, c. (eds.), principle-based
parsing: computation and psycholinguistics, pp. 257   278.
kluwer.
abney, s. p. (1997). stochastic attribute-value grammars.
computational linguistics, 23(4), 597   618.
abney, s. p. (2002). id64. in acl-02, pp. 360   
367.
abney, s. p. (2004). understanding the yarowsky algorithm.
computational linguistics, 30(3), 365   395.
abney, s. p., schapire, r. e., and singer, y. (1999). boost-
ing applied to tagging and pp attachment. in emnlp/vlc-
99, college park, md, pp. 38   45.
adriaans, p. and van zaanen, m. (2004). computational
grammar induction for linguists. grammars; special issue
with the theme    grammar induction   , 7, 57   68.
aggarwal, c. c. and zhai, c. (2012). a survey of text classi-
   cation algorithms. in aggarwal, c. c. and zhai, c. (eds.),
mining text data, pp. 163   222. springer.
agichtein, e. and gravano, l. (2000). snowball: extract-
ing relations from large plain-text collections. in proceed-
ings of the 5th acm international conference on digital
libraries.
agirre, e. and de lacalle, o. l. (2003). id91 id138
word senses. in ranlp 2003.
agirre, e., banea, c., cardie, c., cer, d., diab, m.,
gonzalez-agirre, a., guo, w., lopez-gazpio, i., maritx-
alar, m., mihalcea, r., rigau, g., uria, l., and wiebe,
j. (2015). 2015 semeval-2015 task 2: semantic textual

similarity, english, spanish and pilot on interpretability.
in semeval-15, pp. 252   263.
agirre, e., diab, m., cer, d., and gonzalez-agirre, a.
(2012). semeval-2012 task 6: a pilot on semantic textual
similarity. in semeval-12, pp. 385   393.
agirre, e. and edmonds, p. (eds.). (2006). word sense dis-
ambiguation: algorithms and applications. kluwer.
agirre, e., l  opez de lacalle, o., and soroa, a. (2014). ran-
dom walks for knowledge-based word sense disambigua-
tion. computational linguistics, 40(1), 57   84.
agirre, e. and martinez, d. (2001). learning class-to-class
selectional preferences. in conll-01.
ahmad, f. and kondrak, g. (2005). learning a spelling er-
ror model from search query logs. in hlt-emnlp-05, pp.
955   962.
aho, a. v., sethi, r., and ullman, j. d. (1986). compilers:
principles, techniques, and tools. addison-wesley.
aho, a. v. and ullman, j. d. (1972). the theory of parsing,
translation, and compiling, vol. 1. prentice hall.
ajdukiewicz, k. (1935). die syntaktische konnexit  at. stu-
dia philosophica, 1, 1   27. english translation    syntactic
connexion    by h. weber in mccall, s. (ed.) 1967. polish
logic, pp. 207   231, oxford university press.
algoet, p. h. and cover, t. m. (1988). a sandwich proof of
the shannon-mcmillan-breiman theorem. the annals of
id203, 16(2), 899   909.
allen, j. (1984). towards a general theory of action and
time. arti   cial intelligence, 23(2), 123   154.
allen, j. and perrault, c. r. (1980). analyzing intention in
utterances. arti   cial intelligence, 15, 143   178.
amsler, r. a. (1981). a taxonomy of english nouns and
verbs. in acl-81, stanford, ca, pp. 133   138.
an, j., kwak, h., and ahn, y.-y. (2018).
semaxis:
a lightweight framework to characterize domain-speci   c
word semantics beyond sentiment. in acl 2018.
artstein, r., gandhe, s., gerten, j., leuski, a., and traum,
d. (2009). semi-formal evaluation of conversational char-
acters. in languages: from formal to natural, pp. 22   35.
springer.

517

518 bibliography

atkins, s. (1993). tools for computer-aided corpus lexicog-
raphy: the hector project. acta linguistica hungarica,
41, 5   72.
atkinson, k. (2011). gnu aspell..
austin, j. l. (1962). how to do things with words. harvard
university press.
awadallah, a. h., kulkarni, r. g., ozertem, u., and jones,
r. (2015). charaterizing and predicting voice query refor-
mulation. in cikm-15.
baayen, r. h. (2001). word frequency distributions.
springer.
bacchiani, m., riley, m., roark, b., and sproat, r. (2006).
map adaptation of stochastic grammars. computer speech
& language, 20(1), 41   68.
bacchiani, m., roark, b., and saraclar, m. (2004). lan-
guage model adaptation with map estimation and the per-
ceptron algorithm. in hlt-naacl-04, pp. 21   24.
baccianella, s., esuli, a., and sebastiani, f. (2010). sen-
tiid138 3.0: an enhanced lexical resource for sentiment
analysis and opinion mining.. in lrec-10, pp. 2200   2204.
bach, k. and harnish, r. (1979). linguistic communication
and speech acts. mit press.
backus, j. w. (1959). the syntax and semantics of the pro-
posed international algebraic language of the zurich acm-
gamm conference. in information processing: proceed-
ings of the international conference on information pro-
cessing, paris, pp. 125   132. unesco.
backus, j. w. (1996). transcript of question and answer
session. in wexelblat, r. l. (ed.), history of programming
languages, p. 162. academic press.
bahl, l. r. and mercer, r. l. (1976). part of speech assign-
ment by a statistical decision algorithm.
in proceedings
ieee international symposium on id205, pp.
88   89.
bahl, l. r., jelinek, f., and mercer, r. l. (1983). a max-
imum likelihood approach to continuous speech recogni-
tion. ieee transactions on pattern analysis and machine
intelligence, 5(2), 179   190.
baker, c. f., fillmore, c. j., and lowe, j. b. (1998). the
berkeley framenet project.
in coling/acl-98, mon-
treal, canada, pp. 86   90.
baker, j. k. (1975). the dragon system     an overview.
ieee transactions on acoustics, speech, and signal pro-
cessing, assp-23(1), 24   29.
baker, j. k. (1975/1990). stochastic modeling for auto-
matic speech understanding.
in waibel, a. and lee, k.-
f. (eds.), readings in id103, pp. 297   307.
morgan kaufmann. originally appeared in speech recog-
nition, academic press, 1975.
baker, j. k. (1979). trainable grammars for speech recog-
nition. in klatt, d. h. and wolf, j. j. (eds.), speech com-
munication papers for the 97th meeting of the acoustical
society of america, pp. 547   550.
banerjee, s. and pedersen, t. (2003). extended gloss over-
laps as a measure of semantic relatedness. in ijcai 2003,
pp. 805   810.
bangalore, s. and joshi, a. k. (1999). id55: an
approach to almost parsing. computational linguistics,
25(2), 237   265.
banko, m., cafarella, m., soderland, s., broadhead, m.,
and etzioni, o. (2007). id10 for
the web. in ijcai, vol. 7, pp. 2670   2676.

bar-hillel, y. (1953). a quasi-arithmetical notation for syn-
tactic description. language, 29, 47   58. reprinted in y.
bar-hillel. (1964). language and information: selected
essays on their theory and application, addison-wesley,
61   74.
baum, l. e. (1972). an inequality and associated maxi-
mization technique in statistical estimation for probabilis-
tic functions of markov processes.
in shisha, o. (ed.),
inequalities iii: proceedings of the 3rd symposium on in-
equalities, university of california, los angeles, pp. 1   8.
academic press.
baum, l. e. and eagon, j. a. (1967). an inequality with
applications to statistical estimation for probabilistic func-
tions of markov processes and to a model for ecology. bul-
letin of the american mathematical society, 73(3), 360   
363.
baum, l. e. and petrie, t. (1966). statistical id136 for
probabilistic functions of    nite-state markov chains. an-
nals of mathematical statistics, 37(6), 1554   1563.
baum, l. f. (1900). the wizard of oz. available at project
gutenberg.
bayes, t. (1763). an essay toward solving a problem in the
doctrine of chances, vol. 53. reprinted in facsimiles of
two papers by bayes, hafner publishing, 1963.
bazell, c. e. (1952/1966). the correspondence fallacy in
structural linguistics. in hamp, e. p., householder, f. w.,
and austerlitz, r. (eds.), studies by members of the en-
glish department, istanbul university (3), reprinted in
readings in linguistics ii (1966), pp. 271   298. university
of chicago press.
bej  cek, e., haji  cov  a, e., haji  c, j., j    nov  a, p., kettnerov  a,
v., kol  a  rov  a, v., mikulov  a, m., m    rovsk  y, j., nedoluzhko,
a., panevov  a, j., pol  akov  a, l.,   sev  c    kov  a, m.,   st  ep  anek,
j., and zik  anov  a,   s. (2013). prague dependency treebank
3.0. tech. rep., institute of formal and applied linguis-
tics, charles university in prague. lindat/clarin dig-
ital library at institute of formal and applied linguistics,
charles university in prague.
bellegarda, j. r. (1997). a latent semantic analysis frame-
work for large-span id38. in eurospeech-97,
rhodes, greece.
bellegarda, j. r. (2000). exploiting latent semantic infor-
mation in statistical id38. proceedings of the
ieee, 89(8), 1279   1296.
bellegarda, j. r. (2004). statistical language model adap-
tation: review and perspectives. speech communication,
42(1), 93   108.
bellegarda, j. r. (2013). natural language technology in
mobile devices: two grounding frameworks.
in mobile
speech and advanced natural language solutions, pp.
185   196. springer.
bellman, r. (1957). id145. princeton uni-
versity press.
bellman, r. (1984). eye of the hurricane: an autobiogra-
phy. world scienti   c singapore.
bengio, y., courville, a., and vincent, p. (2013). repre-
sentation learning: a review and new perspectives. ieee
transactions on pattern analysis and machine intelli-
gence, 35(8), 1798   1828.
bengio, y., ducharme, r., vincent, p., and jauvin, c.
(2003). a neural probabilistic language model. journal
of machine learning research, 3(feb), 1137   1155.
bengio, y., lamblin, p., popovici, d., and larochelle, h.
(2007). greedy layer-wise training of deep networks. in
nips 2007, pp. 153   160.

bengio, y., schwenk, h., sen  ecal, j.-s., morin, f., and gau-
vain, j.-l. (2006). neural probabilistic language models. in
innovations in machine learning, pp. 137   186. springer.
berant, j., chou, a., frostig, r., and liang, p. (2013). se-
mantic parsing on freebase from question-answer pairs. in
emnlp 2013.
berant, j. and liang, p. (2014). id29 via para-
phrasing. in acl 2014.
berg-kirkpatrick, t., burkett, d., and klein, d. (2012). an
empirical investigation of statistical signi   cance in nlp. in
emnlp 2012, pp. 995   1005.
berger, a., della pietra, s. a., and della pietra, v. j. (1996).
a maximum id178 approach to natural language process-
ing. computational linguistics, 22(1), 39   71.
bergsma, s., lin, d., and goebel, r. (2008). discriminative
learning of selectional preference from unlabeled text. in
emnlp-08, pp. 59   68.
bergsma, s., lin, d., and goebel, r. (2009). web-scale n-
gram models for lexical disambiguation.. in ijcai-09, pp.
1507   1512.
bergsma, s., pitler, e., and lin, d. (2010). creating robust
supervised classi   ers via web-scale id165 data. in acl
2010, pp. 865   874.
bethard, s. (2013). cleartk-timeml: a minimalist ap-
proach to tempeval 2013. in semeval-13, pp. 10   14.
bever, t. g. (1970). the cognitive basis for linguistic struc-
tures. in hayes, j. r. (ed.), cognition and the development
of language, pp. 279   352. wiley.
bhat, i., bhat, r. a., shrivastava, m., and sharma, d.
(2017). joining hands: exploiting monolingual treebanks
for parsing of code-mixing data. in eacl-17, pp. 324   330.
biber, d., johansson, s., leech, g., conrad, s., and fine-
gan, e. (1999). longman grammar of spoken and written
english. pearson esl, harlow.
bies, a., ferguson, m., katz, k., and macintyre, r. (1995).
bracketing guidelines for treebank ii style id32
project..
bikel, d. m. (2004). intricacies of collins    parsing model.
computational linguistics, 30(4), 479   511.
bikel, d. m., miller, s., schwartz, r., and weischedel,
r. (1997). nymble: a high-performance learning name-
   nder. in anlp 1997, pp. 194   201.
bird, s., klein, e., and loper, e. (2009). natural language
processing with python. o   reilly.
bisani, m. and ney, h. (2004). bootstrap estimates for
con   dence intervals in asr performance evaluation.
in
icassp-04, vol. i, pp. 409   412.
bishop, c. m. (2006). pattern recognition and machine
learning. springer.
bizer, c., lehmann, j., kobilarov, g., auer, s., becker,
c., cyganiak, r., and hellmann, s. (2009). dbpedia   a
crystallization point for the web of data. web semantics:
science, services and agents on the world wide web, 7(3),
154   165.
black, e. (1988). an experiment in computational discrim-
ination of english word senses. ibm journal of research
and development, 32(2), 185   194.
black, e., abney, s. p., flickinger, d., gdaniec, c., grish-
man, r., harrison, p., hindle, d., ingria, r., jelinek, f.,
klavans, j. l., liberman, m. y., marcus, m. p., roukos,
s., santorini, b., and strzalkowski, t. (1991). a procedure
for quantitatively comparing the syntactic coverage of en-
glish grammars. in proceedings darpa speech and natu-
ral language workshop, paci   c grove, ca, pp. 306   311.

bibliography

519

black, e., jelinek, f., lafferty, j. d., magerman, d. m.,
mercer, r. l., and roukos, s. (1992). towards history-
based grammars: using richer models for probabilistic
parsing. in proceedings darpa speech and natural lan-
guage workshop, harriman, ny, pp. 134   139.
blair, c. r. (1960). a program for correcting spelling errors.
information and control, 3, 60   67.
blei, d. m., ng, a. y., and jordan, m. i. (2003). latent
dirichlet allocation.
journal of machine learning re-
search, 3(5), 993   1022.
blodgett, s. l., green, l., and o   connor, b. (2016). demo-
graphic dialectal variation in social media: a case study of
african-american english. in emnlp 2016.
blodgett, s. l. and o   connor, b. (2017). racial disparity in
natural language processing: a case study of social media
african-american english. in fairness, accountability, and
transparency in machine learning (fat/ml) workshop,
kdd.
bloom   eld, l. (1914). an introduction to the study of lan-
guage. henry holt and company.
bloom   eld, l. (1933a). language. university of chicago
press.
bloom   eld, l. (1933b). language. university of chicago
press.
bobrow, d. g., kaplan, r. m., kay, m., norman, d. a.,
thompson, h., and winograd, t. (1977). gus, a frame
driven dialog system. arti   cial intelligence, 8, 155   173.
bobrow, d. g. and norman, d. a. (1975). some princi-
ples of memory schemata. in bobrow, d. g. and collins,
a. (eds.), representation and understanding. academic
press.
bobrow, d. g. and winograd, t. (1977). an overview of
krl, a id99 language. cognitive sci-
ence, 1(1), 3   46.
bod, r. (1993). using an annotated corpus as a stochastic
grammar. in eacl-93, pp. 37   44.
boguraev, b. and briscoe, t. (eds.). (1989). computational
id69 for natural language processing. longman.
bohus, d. and rudnicky, a. i. (2005). sorry, i didn   t catch
that!     an investigation of non-understanding errors and
recovery strategies. in proceedings of sigdial, lisbon,
portugal.
bojanowski, p., grave, e., joulin, a., and mikolov, t.
(2017). enriching word vectors with subword information.
tacl, 5, 135   146.
bollacker, k., evans, c., paritosh, p., sturge, t., and tay-
lor, j. (2008). freebase: a collaboratively created graph
database for structuring human knowledge.
in sigmod
2008, pp. 1247   1250.
bolukbasi, t., chang, k.-w., zou, j. y., saligrama, v., and
kalai, a. t. (2016). man is to computer programmer as
woman is to homemaker? debiasing id27s. in
nips 16, pp. 4349   4357.
booth, t. l. (1969). probabilistic representation of formal
languages. in ieee conference record of the 1969 tenth
annual symposium on switching and automata theory, pp.
74   81.
booth, t. l. and thompson, r. a. (1973). applying prob-
ability measures to abstract languages. ieee transactions
on computers, c-22(5), 442   450.
borges, j. l. (1964).
the analytical language of john
wilkins. university of texas press. trans. ruth l. c.
simms.
bowman, s. r., vilnis, l., vinyals, o., dai, a. m., jozefow-
icz, r., and bengio, s. (2016). generating sentences from
a continuous space. in conll-16, pp. 10   21.

520 bibliography

boyd-graber, j., blei, d. m., and zhu, x. (2007). a topic
model for id51. in emnlp/conll
2007.
boyd-graber, j., feng, s., and rodriguez, p. (2018).
human-computer id53:
the case for
quizbowl. in escalera, s. and weimer, m. (eds.), the nips
   17 competition: building intelligent systems. springer
verlag.
brachman, r. j. (1979). on the epistemogical status of se-
mantic networks. in findler, n. v. (ed.), associative net-
works: representation and use of knowledge by comput-
ers, pp. 3   50. academic press.
brachman, r. j. and levesque, h. j. (eds.). (1985). read-
ings in id99. morgan kaufmann.
brachman, r. j. and schmolze, j. g. (1985). an overview
of the kl-one id99 system. cogni-
tive science, 9(2), 171   216.
brants, t. (2000). tnt: a statistical part-of-speech tagger.
in anlp 2000, seattle, wa, pp. 224   231.
brants, t., popat, a. c., xu, p., och, f. j., and dean, j.
(2007). large language models in machine translation. in
emnlp/conll 2007.
br  eal, m. (1897). essai de s  emantique: science des signi   -
cations. hachette, paris, france.
bresnan, j. (ed.). (1982). the mental representation of
grammatical relations. mit press.
brill, e., dumais, s. t., and banko, m. (2002). an analy-
sis of the askmsr question-answering system. in emnlp
2002, pp. 257   264.
brill, e. and moore, r. c. (2000). an improved error model
for noisy channel id147.
in acl-00, hong
kong, pp. 286   293.
brill, e. and resnik, p. (1994). a rule-based approach
to prepositional phrase attachment disambiguation.
in
coling-94, kyoto, pp. 1198   1204.
brin, s. (1998). extracting patterns and relations from
the world wide web.
in proceedings world wide web
and databases international workshop, number 1590 in
lncs, pp. 172   183. springer.
briscoe, t. and carroll, j. (1993). generalized probabilistic
lr parsing of natural language (corpora) with uni   cation-
based grammars. computational linguistics, 19(1), 25   59.
brockmann, c. and lapata, m. (2003). evaluating and com-
bining approaches to selectional preference acquisition. in
eacl-03, pp. 27   34.
brody, s. and lapata, m. (2009). bayesian word sense in-
duction. in eacl-09, pp. 103   111.
broschart, j. (1997). why tongan does it differently. lin-
guistic typology, 1, 123   165.
bruce, b. c. (1975). generation as a social action. in pro-
ceedings of tinlap-1 (theoretical issues in natural lan-
guage processing), pp. 64   67. association for computa-
tional linguistics.
bruce, r. f. and wiebe, j. (1994). word-sense disambigua-
tion using decomposable models. in acl-94, las cruces,
nm, pp. 139   145.
brysbaert, m., warriner, a. b., and kuperman, v. (2014).
concreteness ratings for 40 thousand generally known en-
glish word lemmas. behavior research methods, 46(3),
904   911.
buchholz, s. and marsi, e. (2006). conll-x shared task on
multilingual id33. in conll-06, pp. 149   
164.

buck, c., hea   eld, k., and van ooyen, b. (2014). id165
counts and language models from the common crawl. in
proceedings of lrec.
budanitsky, a. and hirst, g. (2006). evaluating id138-
based measures of lexical semantic relatedness. computa-
tional linguistics, 32(1), 13   47.
bullinaria, j. a. and levy, j. p. (2007). extracting seman-
tic representations from word co-occurrence statistics: a
computational study. behavior research methods, 39(3),
510   526.
bullinaria, j. a. and levy, j. p. (2012). extracting se-
mantic representations from word co-occurrence statistics:
stop-lists, id30, and svd. behavior research methods,
44(3), 890   907.
bulyko, i., kirchhoff, k., ostendorf, m., and goldberg, j.
(2005). error-sensitive response generation in a spoken
language dialogue system. speech communication, 45(3),
271   288.
bulyko, i., ostendorf, m., and stolcke, a. (2003). get-
ting more mileage from web text sources for conversational
speech id38 using class-dependent mixtures.
in hlt-naacl-03, edmonton, canada, vol. 2, pp. 7   9.
caliskan, a., bryson, j. j., and narayanan, a. (2017). se-
mantics derived automatically from language corpora con-
tain human-like biases. science, 356(6334), 183   186.
cardie, c. (1993). a case-based approach to knowledge ac-
quisition for domain speci   c sentence analysis. in aaai-
93, pp. 798   803. aaai press.
cardie, c. (1994). domain-speci   c knowledge acquisition
for conceptual sentence analysis. ph.d. thesis, university
of massachusetts, amherst, ma. available as cmpsci
technical report 94-74.
carletta, j., isard, a., isard, s., kowtko, j. c., doherty-
sneddon, g., and anderson, a. h. (1997). the reliability
of a dialogue structure coding scheme. computational lin-
guistics, 23(1), 13   32.
carpenter, r. (2017). cleverbot. http://www.cleverbot.com,
accessed 2017.
carreras, x. and m`arquez, l. (2005).
introduction to
the conll-2005 shared task: id14. in
conll-05, pp. 152   164.
carroll, g. and charniak, e. (1992). two experiments on
learning probabilistic dependency grammars from corpora.
tech. rep. cs-92-16, brown university.
carroll, j., briscoe, t., and san   lippo, a. (1998). parser
evaluation: a survey and a new proposal.
in lrec-98,
granada, spain, pp. 447   454.
chambers, n. (2013). navytime: event and time ordering
from raw text. in semeval-13, pp. 73   77.
chambers, n. and jurafsky, d. (2010). improving the use
of pseudo-words for evaluating selectional preferences. in
acl 2010, pp. 445   453.
chambers, n. and jurafsky, d. (2011). template-based in-
formation extraction without the templates. in acl 2011.
chang, a. x. and manning, c. d. (2012). sutime: a li-
brary for recognizing and normalizing time expressions..
in lrec-12, pp. 3735   3740.
chang, p.-c., galley, m., and manning, c. d. (2008). opti-
mizing chinese id40 for machine translation
performance. in proceedings of acl statistical mt work-
shop, pp. 224   232.
charniak, e. (1997). statistical parsing with a context-free
grammar and word statistics.
in aaai-97, pp. 598   603.
aaai press.

jacobson, n.,

charniak, e., hendrickson, c.,
and
perkowitz, m. (1993). equations for part-of-speech tag-
ging. in aaai-93, washington, d.c., pp. 784   789. aaai
press.
charniak, e. and johnson, m. (2005). coarse-to-   ne n-best
parsing and maxent discriminative reranking. in acl-05,
ann arbor.
che, w., li, z., li, y., guo, y., qin, b., and liu, t.
(2009). multilingual dependency-based syntactic and se-
mantic parsing. in conll-09, pp. 49   54.
chelba, c. and jelinek, f. (2000). structured language mod-
eling. computer speech and language, 14, 283   332.
chen, d., fisch, a., weston, j., and bordes, a. (2017).
reading wikipedia to answer open-domain questions.
in
acl 2017.
chen, d. and manning, c. d. (2014). a fast and accurate de-
pendency parser using neural networks.. in emnlp 2014,
pp. 740   750.
chen, j. n. and chang, j. s. (1998). topical id91
of mrd senses based on information retrieval techniques.
computational linguistics, 24(1), 61   96.
chen, s. f. and goodman, j. (1996). an empirical study of
smoothing techniques for id38. in acl-96,
santa cruz, ca, pp. 310   318.
chen, s. f. and goodman, j. (1998). an empirical study of
smoothing techniques for id38. tech. rep.
tr-10-98, computer science group, harvard university.
chen, s. f. and goodman, j. (1999). an empirical study of
smoothing techniques for id38. computer
speech and language, 13, 359   394.
chierchia, g. and mcconnell-ginet, s. (1991). meaning
and grammar. mit press.
chinchor, n., hirschman, l., and lewis, d. l. (1993). eval-
uating message understanding systems: an analysis of the
third message understanding conference. computational
linguistics, 19(3), 409   449.
chiticariu, l., danilevsky, m., li, y., reiss, f., and zhu, h.
(2018). systemt: declarative text understanding for enter-
prise. in naacl hlt 2018, vol. 3, pp. 76   83.
chiticariu, l., li, y., and reiss, f. r. (2013). rule-based
information extraction is dead! long live rule-based in-
formation extraction systems!. in emnlp 2013, pp. 827   
832.
chklovski, t. and mihalcea, r. (2003). exploiting agree-
ment and disagreement of human annotators for word sense
disambiguation. in ranlp 2003.
choi, e., he, h., iyyer, m., yatskar, m., yih, w.-t., choi,
y., liang, p., and zettlemoyer, l. (2018). quac: question
answering in context. in emnlp 2018.
choi, j. d. and palmer, m. (2011a). getting the most out
of transition-based id33. in acl 2011, pp.
687   692.
choi, j. d. and palmer, m. (2011b). transition-based se-
mantic role labeling using predicate argument id91.
in proceedings of the acl 2011 workshop on relational
models of semantics, pp. 37   45.
choi, j. d., tetreault, j., and stent, a. (2015). it depends:
dependency parser comparison using a web-based evalua-
tion tool. in acl 2015, pp. 26   31.
chomsky, n. (1956). three models for the description of
language. ire transactions on id205, 2(3),
113   124.
chomsky, n. (1956/1975). the logical structure of lin-
guistic theory. plenum.

bibliography

521

chomsky, n. (1957). syntactic structures. mouton, the
hague.
chomsky, n. (1963). formal properties of grammars.
in
luce, r. d., bush, r., and galanter, e. (eds.), handbook
of mathematical psychology, vol. 2, pp. 323   418. wiley.
chomsky, n. (1981). lectures on government and binding.
foris.
christodoulopoulos, c., goldwater, s., and steedman, m.
(2010). two decades of unsupervised pos induction: how
far have we come?. in emnlp-10.
chu, y.-j. and liu, t.-h. (1965). on the shortest arbores-
cence of a directed graph. science sinica, 14, 1396   1400.
chu-carroll, j. (1998). a statistical model for discourse act
recognition in dialogue interactions. in chu-carroll, j. and
green, n. (eds.), applying machine learning to discourse
processing. papers from the 1998 aaai spring symposium.
tech. rep. ss-98-01, pp. 12   17. aaai press.
chu-carroll, j. and carpenter, b. (1999). vector-based
natural language call routing. computational linguistics,
25(3), 361   388.
chu-carroll, j., fan, j., boguraev, b. k., carmel, d.,
sheinwald, d., and welty, c. (2012). finding needles in
the haystack: search and candidate generation. ibm jour-
nal of research and development, 56(3/4), 6:1   6:12.
church, a. (1940). a formulation of a simple theory of
types. journal of symbolic logic, 5, 56   68.
church, k. w. and gale, w. a. (1991). id203 scor-
ing for id147. statistics and computing, 1(2),
93   103.
church, k. w. (1980). on memory limitations in natural
language processing master   s thesis, mit. distributed by
the indiana university linguistics club.
church, k. w. (1988). a stochastic parts program and noun
phrase parser for unrestricted text. in anlp 1988, pp. 136   
143.
church, k. w. (1989). a stochastic parts program and noun
phrase parser for unrestricted text. in icassp-89, pp. 695   
698.
church, k. w. (1994). unix for poets. slides from 2nd
elsnet summer school and unpublished paper ms.
church, k. w. and gale, w. a. (1991). a comparison of
the enhanced good-turing and deleted estimation methods
for estimating probabilities of english bigrams. computer
speech and language, 5, 19   54.
church, k. w. and hanks, p. (1989). word association
norms, mutual information, and id69. in acl-89,
vancouver, b.c., pp. 76   83.
church, k. w. and hanks, p. (1990). word association
norms, mutual information, and id69. computa-
tional linguistics, 16(1), 22   29.
church, k. w., hart, t., and gao, j. (2007). compress-
ing trigram language models with golomb coding.
in
emnlp/conll 2007, pp. 199   207.
clark, a. (2001). the unsupervised induction of stochastic
context-free grammars using distributional id91.
in
conll-01.
clark, c. and gardner, m. (2018). simple and effective
multi-paragraph reading comprehension. in acl 2018.
clark, e. (1987). the principle of contrast: a constraint on
id146.
in macwhinney, b. (ed.), mecha-
nisms of id146, pp. 1   33. lea.
clark, h. h. (1996). using language. cambridge univer-
sity press.
clark, h. h. and fox tree, j. e. (2002). using uh and um
in spontaneous speaking. cognition, 84, 73   111.

522 bibliography

clark, h. h. and marshall, c. (1981). de   nite reference
and mutual knowledge. in joshi, a. k., webber, b. l., and
sag, i. a. (eds.), elements of discourse understanding,
pp. 10   63. cambridge.
clark, h. h. and schaefer, e. f. (1989). contributing to
discourse. cognitive science, 13, 259   294.
clark, h. h. and wilkes-gibbs, d. (1986). referring as a
collaborative process. cognition, 22, 1   39.
clark, p., cowhey, i., etzioni, o., khot, t., sabharwal, a.,
schoenick, c., and tafjord, o. (2018). think you have
solved id53? try arc, the ai2 reasoning
challenge.. arxiv preprint arxiv:1803.05457.
clark, s. and curran, j. r. (2004). parsing the wsj using
id35 and id148. in acl-04, pp. 104   111.
clark, s., curran, j. r., and osborne, m. (2003). bootstrap-
ping pos taggers using unlabelled data. in conll-03, pp.
49   55.
coccaro, n. and jurafsky, d. (1998). towards better inte-
gration of semantic predictors in statistical language mod-
eling. in icslp-98, sydney, vol. 6, pp. 2403   2406.
cohen, k. b. and demner-fushman, d. (2014). biomedical
natural language processing. benjamins.
cohen, m. h., giangola, j. p., and balogh, j. (2004). voice
user interface design. addison-wesley.
cohen, p. r. and perrault, c. r. (1979). elements of a plan-
based theory of speech acts. cognitive science, 3(3), 177   
212.
colby, k. m., hilf, f. d., weber, s., and kraemer, h. c.
(1972). turing-like indistinguishability tests for the valida-
tion of a computer simulation of paranoid processes. arti-
   cial intelligence, 3, 199   221.
colby, k. m., weber, s., and hilf, f. d. (1971). arti   cial
paranoia. arti   cial intelligence, 2(1), 1   25.
cole, r. a., novick, d. g., vermeulen, p. j. e., sutton, s.,
fanty, m., wessels, l. f. a., de villiers, j. h., schalkwyk,
j., hansen, b., and burnett, d. (1997). experiments with a
spoken dialogue system for taking the us census. speech
communication, 23, 243   260.
collins, m. (1996). a new statistical parser based on bi-
gram lexical dependencies.
in acl-96, santa cruz, ca,
pp. 184   191.
collins, m. (1999). head-driven statistical models for nat-
ural language parsing. ph.d. thesis, university of penn-
sylvania, philadelphia.
collins, m. (2000). discriminative reranking for natural lan-
guage parsing. in icml 2000, stanford, ca, pp. 175   182.
collins, m. (2003a). head-driven statistical models for nat-
ural language parsing. computational linguistics, 29(4),
589   637.
collins, m. (2003b). head-driven statistical models for nat-
ural language parsing. computational linguistics, 29(4),
589   637.
collins, m., haji  c, j., ramshaw, l. a., and tillmann, c.
(1999). a statistical parser for czech. in acl-99, college
park, ma, pp. 505   512.
collins, m. and koo, t. (2005). discriminative reranking
for natural language parsing. computational linguistics,
31(1), 25   69.
collobert, r. and weston, j. (2007). fast semantic extrac-
tion using a novel neural network architecture. in acl-07,
pp. 560   567.
collobert, r. and weston, j. (2008). a uni   ed architec-
ture for natural language processing: deep neural networks
with multitask learning. in icml, pp. 160   167.

collobert, r., weston,
j., bottou, l., karlen, m.,
kavukcuoglu, k., and kuksa, p. (2011). natural language
processing (almost) from scratch. the journal of machine
learning research, 12, 2493   2537.
copestake, a. and briscoe, t. (1995). semi-productive pol-
ysemy and sense extension. journal of semantics, 12(1),
15   68.
cottrell, g. w. (1985).
a connectionist approach to
id51. ph.d. thesis, university of
rochester, rochester, ny. revised version published by
pitman, 1989.
cover, t. m. and thomas, j. a. (1991). elements of infor-
mation theory. wiley.
covington, m. (2001). a fundamental algorithm for depen-
dency parsing.
in proceedings of the 39th annual acm
southeast conference, pp. 95   102.
cox, d. (1969). analysis of binary data. chapman and
hall, london.
craven, m. and kumlien, j. (1999). constructing biolog-
ical knowledge bases by extracting information from text
sources. in ismb-99, pp. 77   86.
cruse, d. a. (2004). meaning in language: an introduction
to semantics and pragmatics. oxford university press.
second edition.
cucerzan, s. and brill, e. (2004). id147 as an
iterative process that exploits the collective knowledge of
web users. in emnlp 2004, vol. 4, pp. 293   300.
culicover, p. w. and jackendoff, r. (2005). simpler syntax.
oxford university press.
dagan, i., marcus, s., and markovitch, s. (1993). contex-
tual word similarity and estimation from sparse data.
in
acl-93, columbus, ohio, pp. 164   171.
damerau, f. j. (1964). a technique for computer detection
and correction of spelling errors. communications of the
acm, 7(3), 171   176.
damerau, f. j. and mays, e. (1989). an examination of un-
detected typing errors. information processing and man-
agement, 25(6), 659   664.
danieli, m. and gerbino, e. (1995). metrics for evaluating
dialogue strategies in a spoken language system. in pro-
ceedings of the 1995 aaai spring symposium on empir-
ical methods in discourse interpretation and generation,
stanford, ca, pp. 34   39. aaai press.
das, s. r. and chen, m. y. (2001).
yahoo!
for
amazon: sentiment parsing from small
talk on the
web. efa 2001 barcelona meetings. available at ssrn:
http://ssrn.com/abstract=276189.
davidson, d. (1967). the logical form of action sentences.
in rescher, n. (ed.), the logic of decision and action.
university of pittsburgh press.
davidson, t., warmsley, d., macy, m., and weber, i. (2017).
automated hate speech detection and the problem of offen-
sive language. in icwsm 2017.
davies, m. (2012). expanding horizons in historical linguis-
tics with the 400-million word corpus of historical amer-
ican english. corpora, 7(2), 121   157.
davis, e. (1990). representations of commonsense knowl-
edge. morgan kaufmann.
de marneffe, m.-c., dozat, t., silveira, n., haverinen, k.,
ginter, f., nivre, j., and manning, c. d. (2014). univer-
sal stanford dependencies: a cross-linguistic typology.. in
lrec, vol. 14, pp. 4585   92.
de marneffe, m.-c., maccartney, b., and manning, c. d.
(2006). generating typed dependency parses from phrase
structure parses. in lrec-06.

de marneffe, m.-c. and manning, c. d. (2008). the stan-
ford typed dependencies representation. in coling 2008:
proceedings of the workshop on cross-framework and
cross-domain parser evaluation, pp. 1   8.
deerwester, s. c., dumais, s. t., furnas, g. w., harshman,
r. a., landauer, t. k., lochbaum, k. e., and streeter, l.
(1988). computer information retrieval using latent seman-
tic structure: us patent 4,839,853..
deerwester, s. c., dumais, s. t., landauer, t. k., furnas,
g. w., and harshman, r. a. (1990).
indexing by latent
semantics analysis. jasis, 41(6), 391   407.
dejong, g. f. (1982). an overview of the frump system.
in lehnert, w. g. and ringle, m. h. (eds.), strategies for
natural language processing, pp. 149   176. lawrence erl-
baum.
dempster, a. p., laird, n. m., and rubin, d. b. (1977).
maximum likelihood from incomplete data via the em al-
gorithm. journal of the royal statistical society, 39(1),
1   21.
derose, s. j. (1988). grammatical category disambiguation
by statistical optimization. computational linguistics, 14,
31   39.
di marco, a. and navigli, r. (2013). id91 and di-
versifying web search results with graph-based word sense
induction. computational linguistics, 39(3), 709   754.
diab, m. and resnik, p. (2002). an unsupervised method
for word sense tagging using parallel corpora. in acl-02,
pp. 255   262.
digman, j. m. (1990). personality structure: emergence of
the    ve-factor model. annual review of psychology, 41(1),
417   440.
do, q. n. t., bethard, s., and moens, m.-f. (2017). improv-
ing implicit id14 by predicting semantic
frame arguments. in ijcnlp-17.
dolan, w. b. (1994). word sense ambiguation: id91
related senses. in coling-94, kyoto, japan, pp. 712   716.
dos santos, c., xiang, b., and zhou, b. (2015). classifying
relations by ranking with convolutional neural networks. in
acl 2015.
dowty, d. r. (1979). word meaning and montague gram-
mar. d. reidel.
dowty, d. r., wall, r. e., and peters, s. (1981). introduc-
tion to montague semantics. d. reidel.
dozat, t., qi, p., and manning, c. d. (2017). stanford   s
graph-based neural dependency parser at the conll 2017
shared task.
in proceedings of the conll 2017 shared
task, pp. 20   30.
dror, r., baumer, g., bogomolov, m., and reichart, r.
(2017). replicability analysis for natural language process-
ing: testing signi   cance with multiple datasets. tacl, 5,
471      486.
duda, r. o. and hart, p. e. (1973). pattern classi   cation
and scene analysis. john wiley and sons.
earley, j. (1968). an ef   cient context-free parsing al-
gorithm. ph.d. thesis, carnegie mellon university, pitts-
burgh, pa.
earley, j. (1970). an ef   cient context-free parsing al-
gorithm. communications of the acm, 6(8), 451   455.
reprinted in grosz et al. (1986).
edmonds, j. (1967). optimum branchings. journal of re-
search of the national bureau of standards b, 71(4), 233   
240.
efron, b. and tibshirani, r. j. (1993). an introduction to
the bootstrap. crc press.

bibliography

523

egghe, l. (2007). untangling herdan   s law and heaps    law:
mathematical and informetric arguments. jasist, 58(5),
702   709.
eisner, j. (1996). three new probabilistic models for de-
pendency parsing: an exploration. in coling-96, copen-
hagen, pp. 340   345.
eisner, j. (2002). an interactive spreadsheet for teaching
the forward-backward algorithm.
in proceedings of the
acl workshop on effective tools and methodologies for
teaching nlp and cl, pp. 10   18.
ejerhed, e. i. (1988). finding clauses in unrestricted text by
   nitary and stochastic methods. in anlp 1988, pp. 219   
227.
ekman, p. (1999). basic emotions.
in dalgleish, t. and
power, m. j. (eds.), handbook of cognition and emotion,
pp. 45   60. wiley.
elman, j. l. (1990). finding structure in time. cognitive
science, 14(2), 179   211.
erk, k. (2007). a simple, similarity-based model for selec-
tional preferences. in acl-07, pp. 216   223.
etzioni, o., cafarella, m., downey, d., popescu, a.-m.,
shaked, t., soderland, s., weld, d. s., and yates, a.
(2005). unsupervised named-entity extraction from the
web: an experimental study. arti   cial intelligence, 165(1),
91   134.
evans, n. (2000). word classes in the world   s languages. in
booij, g., lehmann, c., and mugdan, j. (eds.), morphol-
ogy: a handbook on in   ection and word formation, pp.
708   732. mouton.
fader, a., soderland, s., and etzioni, o. (2011). identifying
relations for id10. in emnlp-11,
pp. 1535   1545.
fader, a., zettlemoyer, l., and etzioni, o.
(2013).
paraphrase-driven learning for open id53. in
acl 2013, so   a, bulgaria, pp. 1608   1618.
fano, r. m. (1961). transmission of information: a statis-
tical theory of communications. mit press.
fast, e., chen, b., and bernstein, m. s. (2016). empath:
understanding topic signals in large-scale text. in chi.
feldman, j. a. and ballard, d. h. (1982). connectionist
models and their properties. cognitive science, 6, 205   
254.
fellbaum, c. (ed.). (1998). id138: an electronic lexical
database. mit press.
fensel, d., hendler, j. a., lieberman, h., and wahlster,
w. (eds.). (2003). spinning the semantic web: bring the
world wide web to its full potential. mit press, cam-
bridge, ma.
ferro, l., gerber, l., mani, i., sundheim, b., and wilson,
g. (2005). tides 2005 standard for the annotation of tem-
poral expressions. tech. rep., mitre.
ferrucci, d. a. (2012). introduction to    this is watson   .
ibm journal of research and development, 56(3/4), 1:1   
1:15.
fessler, l. (2017). we tested bots like siri and alexa to see
who would stand up to sexual harassment. in quartz. feb
22, 2017. https://qz.com/911681/.
fikes, r. e. and nilsson, n. j. (1971). strips: a new ap-
proach to the application of theorem proving to problem
solving. arti   cial intelligence, 2, 189   208.
fillmore, c. j. (1966). a proposal concerning english prepo-
sitions. in dinneen, f. p. (ed.), 17th annual round table.,
vol. 17 of monograph series on language and linguistics,
pp. 19   34. georgetown university press, washington d.c.

524 bibliography

fillmore, c. j. (1968). the case for case. in bach, e. w.
and harms, r. t. (eds.), universals in linguistic theory,
pp. 1   88. holt, rinehart & winston.
fillmore, c. j. (1985). frames and the semantics of under-
standing. quaderni di semantica, vi(2), 222   254.
fillmore, c. j. (2003). valency and semantic roles: the con-
cept of deep structure case. in   agel, v., eichinger, l. m.,
eroms, h. w., hellwig, p., heringer, h. j., and lobin, h.
(eds.), dependenz und valenz: ein internationales hand-
buch der zeitgen  ossischen forschung, chap. 36, pp. 457   
475. walter de gruyter.
fillmore, c. j. (2012). encounters with language. compu-
tational linguistics, 38(4), 701   718.
fillmore, c. j. and baker, c. f. (2009). a frames approach
to semantic analysis. in heine, b. and narrog, h. (eds.),
the oxford handbook of linguistic analysis, pp. 313   340.
oxford university press.
fillmore, c. j., johnson, c. r., and petruck, m. r. l. (2003).
background to framenet. international journal of lexicog-
raphy, 16(3), 235   250.
finkelstein, l., gabrilovich, e., matias, y., rivlin, e.,
solan, z., wolfman, g., and ruppin, e. (2002). placing
search in context: the concept revisited. acm transac-
tions on information systems, 20(1), 116      131.
firth, j. r. (1935). the technique of semantics. transac-
tions of the philological society, 34(1), 36   73.
firth, j. r. (1957). a synopsis of linguistic theory 1930   
1955. in studies in linguistic analysis. philological soci-
ety. reprinted in palmer, f. (ed.) 1968. selected papers of
j. r. firth. longman, harlow.
foland, jr., w. r. and martin, j. h. (2015). dependency-
based id14 using convolutional neural
networks. in *sem 2015), pp. 279   289.
forbes-riley, k. and litman, d. j. (2011). bene   ts and
challenges of real-time uncertainty detection and adapta-
tion in a spoken dialogue computer tutor. speech commu-
nication, 53(9), 1115   1136.
forchini, p. (2013). using movie corpora to explore spoken
american english: evidence from multi-dimensional anal-
ysis.
in bamford, j., cavalieri, s., and diani, g. (eds.),
variation and change in spoken and written discourse:
perspectives from corpus linguistics, pp. 123   136. ben-
jamins.
forney, jr., g. d. (1973). the viterbi algorithm. proceed-
ings of the ieee, 61(3), 268   278.
francis, h. s., gregory, m. l., and michaelis, l. a. (1999).
are lexical subjects deviant?.
in cls-99. university of
chicago.
francis, w. n. and ku  cera, h. (1982). frequency analysis
of english usage. houghton mif   in, boston.
franz, a. (1997).
harmful. in acl/eacl-97, madrid, spain, pp. 182   189.
franz, a. and brants, t. (2006). all our id165 are be-
long to you. http://googleresearch.blogspot.com/
2006/08/all-our-id165-are-belong-to-you.
html.
fraser, n. m. and gilbert, g. n. (1991). simulating speech
systems. computer speech and language, 5, 81   99.
fyshe, a., wehbe, l., talukdar, p. p., murphy, b., and
mitchell, t. m. (2015). a compositional and interpretable
semantic space. in naacl hlt 2015.
gabow, h. n., galil, z., spencer, t., and tarjan, r. e.
(1986). ef   cient algorithms for    nding minimum spanning
trees in undirected and directed graphs. combinatorica,
6(2), 109   122.

independence assumptions considered

gage, p. (1994). a new algorithm for data compression. the
c users journal, 12(2), 23   38.
gale, w. a. and church, k. w. (1994). what is wrong
with adding one?.
in oostdijk, n. and de haan, p.
(eds.), corpus-based research into language, pp. 189   
198. rodopi.
gale, w. a., church, k. w., and yarowsky, d. (1992a). es-
timating upper and lower bounds on the performance of
word-sense disambiguation programs. in acl-92, newark,
de, pp. 249   256.
gale, w. a., church, k. w., and yarowsky, d. (1992b). one
sense per discourse.
in proceedings darpa speech and
natural language workshop, pp. 233   237.
gale, w. a., church, k. w., and yarowsky, d. (1992c).
work on statistical methods for word sense disambigua-
tion. in goldman, r. (ed.), proceedings of the 1992 aaai
fall symposium on probabilistic approaches to natural
language.
garg, n., schiebinger, l., jurafsky, d., and zou, j. (2018).
id27s quantify 100 years of gender and eth-
nic stereotypes. proceedings of the national academy of
sciences, 115(16), e3635   e3644.
garside, r. (1987). the claws word-tagging system. in
garside, r., leech, g., and sampson, g. (eds.), the com-
putational analysis of english, pp. 30   41. longman.
garside, r., leech, g., and mcenery, a. (1997). corpus
annotation. longman.
gazdar, g., klein, e., pullum, g. k., and sag, i. a. (1985).
generalized phrase structure grammar. blackwell.
gerber, m. and chai, j. y. (2010). beyond nombank: a
study of implicit arguments for nominal predicates. in acl
2010, pp. 1583   1592.
gil, d. (2000). syntactic categories, cross-linguistic varia-
tion and universal grammar. in vogel, p. m. and comrie,
b. (eds.), approaches to the typology of word classes, pp.
173   216. mouton.
gildea, d. and jurafsky, d. (2000). automatic labeling of
semantic roles. in acl-00, hong kong, pp. 512   520.
gildea, d. and jurafsky, d. (2002). automatic labeling of
semantic roles. computational linguistics, 28(3), 245   
288.
gildea, d. and palmer, m. (2002). the necessity of syntac-
tic parsing for predicate argument recognition. in acl-02,
philadelphia, pa.
gillick, l. and cox, s. j. (1989). some statistical issues
in the comparison of id103 algorithms.
in
icassp-89, pp. 532   535.
ginzburg, j. and sag, i. a. (2000). interrogative investiga-
tions: the form, meaning and use of english interroga-
tives. csli.
giuliano, v. e. (1965). the interpretation of word as-
sociations.
in stevens, m. e., giuliano, v. e., and
heilprin, l. b. (eds.), statistical association methods
for mechanized documentation. symposium proceed-
ings. washington, d.c., usa, march 17, 1964, pp. 25   
32. https://nvlpubs.nist.gov/nistpubs/legacy/
mp/nbsmiscellaneouspub269.pdf.
giv  on, t. (1990). syntax: a functional typological intro-
duction. john benjamins.
glennie, a. (1960). on the syntax machine and the construc-
tion of a universal compiler. tech. rep. no. 2, contr. nr
049-141, carnegie mellon university (at the time carnegie
institute of technology), pittsburgh, pa.
godfrey, j., holliman, e., and mcdaniel, j. (1992).
switchboard: telephone speech corpus for research
and development. in icassp-92, san francisco, pp. 517   
520.

in

goffman, e. (1974). frame analysis: an essay on the orga-
nization of experience. harvard university press.
goldberg, j., ostendorf, m., and kirchhoff, k. (2003). the
impact of response wording in error correction subdialogs.
in isca tutorial and research workshop on error han-
dling in spoken dialogue systems.
goldberg, y. (2017). neural network methods for natu-
ral language processing, vol. 10 of synthesis lectures on
human language technologies. morgan & claypool.
golding, a. r. and roth, d. (1999). a winnow based ap-
proach to context-sensitive id147. machine
learning, 34(1-3), 107   130.
gondek, d., lally, a., kalyanpur, a., murdock, j. w.,
dubou  e, p. a., zhang, l., pan, y., qiu, z., and welty, c.
(2012). a framework for merging and ranking of answers
in deepqa.
ibm journal of research and development,
56(3/4), 14:1   14:12.
good, m. d., whiteside, j. a., wixon, d. r., and jones, s. j.
(1984). building a user-derived interface. communications
of the acm, 27(10), 1032   1043.
goodfellow, i., bengio, y., and courville, a. (2016). deep
learning. mit press.
goodman, j. (1997). probabilistic feature grammars.
iwpt-97.
goodman, j. (2006). a bit of progress in language mod-
eling: extended version. tech. rep. msr-tr-2001-72,
machine learning and applied statistics group, microsoft
research, redmond, wa.
goodwin, c. (1996). transparent vision. in ochs, e., sche-
gloff, e. a., and thompson, s. a. (eds.), interaction and
grammar, pp. 370   404. cambridge university press.
gould, j. d., conti, j., and hovanyecz, t. (1983). compos-
ing letters with a simulated listening typewriter. communi-
cations of the acm, 26(4), 295   308.
gould, j. d. and lewis, c. (1985). designing for usability:
key principles and what designers think. communications
of the acm, 28(3), 300   311.
gould, s. j. (1980). the panda   s thumb. penguin group.
gravano, a., hirschberg, j., and be  nu  s,   s. (2012). af   rma-
tive cue words in task-oriented dialogue. computational
linguistics, 38(1), 1   39.
green, b. f., wolf, a. k., chomsky, c., and laughery, k.
(1961). baseball: an automatic question answerer. in pro-
ceedings of the western joint computer conference 19, pp.
219   224. reprinted in grosz et al. (1986).
greene, b. b. and rubin, g. m. (1971). automatic gram-
matical tagging of english. department of linguistics,
brown university, providence, rhode island.
greenwald, a. g., mcghee, d. e., and schwartz, j. l. k.
(1998). measuring individual differences in implicit cog-
nition: the implicit association test.. journal of personality
and social psychology, 74(6), 1464   1480.
grenager, t. and manning, c. d. (2006). unsupervised dis-
covery of a statistical verb lexicon. in emnlp 2006.
grishman, r. and sundheim, b. (1995). design of the
muc-6 evaluation. in muc-6, san francisco, pp. 1   11.
grosz, b. j. (1977). the representation and use of focus in
dialogue understanding. ph.d. thesis, university of cali-
fornia, berkeley.
grosz, b. j. and sidner, c. l. (1980). plans for discourse.
in cohen, p. r., morgan, j., and pollack, m. e. (eds.), in-
tentions in communication, pp. 417   444. mit press.
gruber, j. s. (1965). studies in lexical relations. ph.d.
thesis, mit.

bibliography

525

guindon, r. (1988). a multidisciplinary perspective on di-
alogue structure in user-advisor dialogues. in guindon, r.
(ed.), cognitive science and its applications for human-
computer interaction, pp. 163   200. lawrence erlbaum.
gus   eld, d. (1997). algorithms on strings, trees, and se-
quences: computer science and computational biology.
cambridge university press.
guyon, i. and elisseeff, a. (2003). an introduction to vari-
able and feature selection. the journal of machine learn-
ing research, 3, 1157   1182.
haghighi, a. and klein, d. (2006). prototype-driven gram-
mar induction. in coling/acl 2006, pp. 881   888.
haji  c, j. (1998). building a syntactically annotated cor-
pus: the prague dependency treebank, pp. 106   132.
karolinum.
haji  c, j. (2000). morphological tagging: data vs. dictionar-
ies. in naacl 2000. seattle.
haji  c, j., ciaramita, m., johansson, r., kawahara, d.,
mart    , m. a., m`arquez, l., meyers, a., nivre, j., pad  o,
s.,   st  ep  anek, j., stran  a  k, p., surdeanu, m., xue, n., and
zhang, y. (2009). the conll-2009 shared task: syntac-
tic and semantic dependencies in multiple languages.
in
conll-09, pp. 1   18.
hakkani-t  ur, d., o   azer, k., and t  ur, g. (2002). statis-
tical id60 for agglutinative lan-
guages. journal of computers and humanities, 36(4), 381   
410.
hakkani-t  ur, d., t  ur, g., celikyilmaz, a., chen, y.-n.,
gao, j., deng, l., and wang, y.-y. (2016). multi-domain
joint semantic frame parsing using bi-directional id56-lstm..
in interspeech, pp. 715   719.
hale, j. (2001). a probabilistic earley parser as a psycholin-
guistic model. in naacl 2001, pp. 159   166.
hamilton, w. l., clark, k., leskovec, j., and jurafsky, d.
(2016a). inducing domain-speci   c sentiment lexicons from
unlabeled corpora. in emnlp 2016.
hamilton, w. l., leskovec, j., and jurafsky, d. (2016b).
diachronic id27s reveal statistical laws of se-
mantic change. in acl 2016.
harabagiu, s., pasca, m., and maiorano, s. (2000). exper-
iments with open-domain textual id53.
in
coling-00, saarbr  ucken, germany.
harris, r. a. (2005). voice interaction design: crafting the
new conversational speech systems. morgan kaufmann.
harris, z. s. (1946). from morpheme to utterance. lan-
guage, 22(3), 161   183.
harris, z. s. (1954). distributional structure. word, 10,
146   162. reprinted in j. fodor and j. katz, the structure
of language, prentice hall, 1964 and in z. s. harris, pa-
pers in structural and transformational linguistics, rei-
del, 1970, 775   794.
harris, z. s. (1962). string analysis of sentence structure.
mouton, the hague.
hastie, t., tibshirani, r. j., and friedman, j. h. (2001). the
elements of statistical learning. springer.
hatzivassiloglou, v. and mckeown, k. r. (1997). predict-
ing the semantic orientation of adjectives. in acl/eacl-
97, pp. 174   181.
hatzivassiloglou, v. and wiebe, j. (2000). effects of adjec-
tive orientation and gradability on sentence subjectivity. in
coling-00, pp. 299   305.
he, l., lee, k., lewis, m., and zettlemoyer, l. (2017). deep
id14: what works and what   s next.
in
acl 2017, pp. 473   483.

526 bibliography

hea   eld, k. (2011). kenlm: faster and smaller language
model queries. in workshop on statistical machine trans-
lation, pp. 187   197.
hea   eld, k., pouzyrevsky, i., clark, j. h., and koehn, p.
(2013). scalable modi   ed kneser-ney language model es-
timation.. in acl 2013, pp. 690   696.
heaps, h. s. (1978). information retrieval. computational
and theoretical aspects. academic press.
hearst, m. a. (1991). noun homograph disambiguation.
in proceedings of the 7th conference of the university of
waterloo centre for the new oed and text research, pp.
1   19.
hearst, m. a. (1992a). automatic acquisition of hyponyms
from large text corpora. in coling-92, nantes, france.
hearst, m. a. (1992b). automatic acquisition of hyponyms
from large text corpora. in coling-92, nantes, france.
coling.
hearst, m. a. (1998). automatic discovery of id138 re-
lations.
in fellbaum, c. (ed.), id138: an electronic
lexical database. mit press.
heckerman, d., horvitz, e., sahami, m., and dumais, s. t.
(1998). a bayesian approach to    ltering junk e-mail. in
proceeding of aaai-98 workshop on learning for text
categorization, pp. 55   62.
heim, i. and kratzer, a. (1998). semantics in a generative
grammar. blackwell publishers, malden, ma.
hemphill, c. t., godfrey, j., and doddington, g. (1990).
the atis spoken language systems pilot corpus. in pro-
ceedings darpa speech and natural language workshop,
hidden valley, pa, pp. 96   101.
henderson, p., sinha, k., angelard-gontier, n., ke, n. r.,
fried, g., lowe, r., and pineau, j. (2017). ethical chal-
lenges in data-driven dialogue systems. in aaai/acm ai
ethics and society conference.
hendrickx,
i., kim, s. n., kozareva, z., nakov, p.,
  o s  eaghdha, d., pad  o, s., pennacchiotti, m., romano, l.,
and szpakowicz, s. (2009). semeval-2010 task 8: multi-
way classi   cation of semantic relations between pairs of
nominals.
in proceedings of the workshop on semantic
evaluations: recent achievements and future directions,
pp. 94   99.
hendrix, g. g., thompson, c. w., and slocum, j. (1973).
language processing via canonical verbs and semantic
models. in proceedings of ijcai-73.
herdan, g. (1960). type-token mathematics. the hague,
mouton.
hermann, k. m., kocisky, t., grefenstette, e., espeholt, l.,
kay, w., suleyman, m., and blunsom, p. (2015). teaching
machines to read and comprehend. in advances in neural
information processing systems, pp. 1693   1701.
hill, f., reichart, r., and korhonen, a. (2015). siid113x-999:
evaluating semantic models with (genuine) similarity esti-
mation. computational linguistics, 41(4), 665   695.
hindle, d. and rooth, m. (1990). structural ambiguity and
lexical relations. in proceedings darpa speech and natu-
ral language workshop, hidden valley, pa, pp. 257   262.
hindle, d. and rooth, m. (1991). structural ambiguity and
lexical relations. in acl-91, berkeley, ca, pp. 229   236.
hinkelman, e. a. and allen, j. (1989). two constraints on
speech act ambiguity. in acl-89, vancouver, canada, pp.
212   219.
hinton, g. e. (1986). learning distributed representations
of concepts. in cogsci-86, pp. 1   12.

hinton, g. e., osindero, s., and teh, y.-w. (2006). a fast
learning algorithm for deep belief nets. neural computa-
tion, 18(7), 1527   1554.
hinton, g. e., srivastava, n., krizhevsky, a., sutskever,
i., and salakhutdinov, r. r. (2012).
improving neural
networks by preventing co-adaptation of feature detectors.
arxiv preprint arxiv:1207.0580.
hirschberg, j., litman, d. j., and swerts, m. (2001). iden-
tifying user corrections automatically in spoken dialogue
systems. in naacl 2001.
hirschman, l., light, m., breck, e., and burger, j. d.
(1999). deep read: a reading comprehension system. in
acl-99, pp. 325   332.
hirschman, l. and pao, c. (1993). the cost of errors in a
spoken language system. in eurospeech-93, pp. 1419   
1422.
hirst, g. (1987). semantic interpretation and the resolution
of ambiguity. cambridge university press.
hirst, g. (1988). resolving lexical ambiguity computa-
tionally with spreading activation and polaroid words. in
small, s. l., cottrell, g. w., and tanenhaus, m. k. (eds.),
lexical ambiguity resolution, pp. 73   108. morgan kauf-
mann.
hirst, g. and budanitsky, a. (2005). correcting real-word
spelling errors by restoring lexical cohesion. natural lan-
guage engineering, 11, 87   111.
hirst, g. and charniak, e. (1982). word sense and case slot
disambiguation. in aaai-82, pp. 95   98.
hjelmslev, l. (1969). prologomena to a theory of lan-
guage. university of wisconsin press. translated by fran-
cis j. whit   eld; original danish edition 1943.
hobbs, j. r., appelt, d. e., bear, j., israel, d., kameyama,
m., stickel, m. e., and tyson, m. (1997). fastus: a
cascaded    nite-state transducer for extracting information
from natural-language text.
in roche, e. and schabes,
y. (eds.), finite-state language processing, pp. 383   406.
mit press.
hockenmaier, j. and steedman, m. (2007). id35bank: a cor-
pus of id35 derivations and dependency structures extracted
from the id32. computational linguistics, 33(3),
355   396.
hofmann, t. (1999). probabilistic id45.
in sigir-99, berkeley, ca.
hopcroft, j. e. and ullman, j. d. (1979).
introduction to
automata theory, languages, and computation. addison-
wesley.
horning, j. j. (1969). a study of grammatical id136.
ph.d. thesis, stanford university.
householder, f. w. (1995). dionysius thrax, the technai,
and sextus empiricus. in koerner, e. f. k. and asher, r. e.
(eds.), concise history of the language sciences, pp. 99   
103. elsevier science.
hovy, e. h., hermjakob, u., and ravichandran, d. (2002).
a question/answer typology with surface text patterns. in
hlt-01.
hovy, e. h., marcus, m. p., palmer, m., ramshaw, l. a.,
and weischedel, r. (2006). ontonotes: the 90% solution.
in hlt-naacl-06.
hsu, b.-j. (2007). generalized linear interpolation of lan-
guage models. in ieee asru-07, pp. 136   140.
hu, m. and liu, b. (2004a). mining and summarizing cus-
tomer reviews. in kdd, pp. 168   177.
hu, m. and liu, b. (2004b). mining and summarizing cus-
tomer reviews. in sigkdd-04.

huang, e. h., socher, r., manning, c. d., and ng, a. y.
(2012). improving word representations via global context
and multiple word prototypes. in acl 2012, pp. 873   882.
huang, l. and chiang, d. (2005). better k-best parsing. in
iwpt-05, pp. 53   64.
huang, l. and sagae, k. (2010). id145 for
linear-time incremental parsing. in acl 2010, pp. 1077   
1086.
huang, z., xu, w., and yu, k. (2015). bidirectional lstm-
crf models for sequence tagging.
in arxiv preprint
arxiv:1508.01991.
huddleston, r. and pullum, g. k. (2002). the cambridge
grammar of the english language. cambridge university
press.
hudson, r. a. (1984). word grammar. blackwell.
huffman, s. (1996). learning information extraction pat-
terns from examples.
in wertmer, s., riloff, e., and
scheller, g. (eds.), connectionist, statistical, and sym-
bolic approaches to learning natural language process-
ing, pp. 246   260. springer.
hutto, c. j., folds, d., and appling, s. (2015). compu-
tationally detecting and quantifying the degree of bias in
sentence-level text of news stories. in huso 2015: the
first international conference on human and social ana-
lytics.
hymes, d. (1974). ways of speaking.
in bauman, r.
and sherzer, j. (eds.), explorations in the ethnography of
speaking, pp. 433   451. cambridge university press.
iacobacci, i., pilehvar, m. t., and navigli, r. (2016). em-
beddings for id51: an evaluation
study. in acl 2016, pp. 897   907.
irons, e. t. (1961). a syntax directed compiler for algol
60. communications of the acm, 4, 51   55.
isbell, c. l., kearns, m., kormann, d., singh, s., and stone,
p. (2000). cobot in lambdamoo: a social statistics agent.
in aaai/iaai, pp. 36   41.
iso8601 (2004). data elements and interchange formats   
information interchange   representation of dates and
times. tech. rep., international organization for standards
(iso).
jackendoff, r. (1983). semantics and cognition. mit
press.
jacobs, p. s. and rau, l. f. (1990). scisor: a system
for extracting information from on-line news. communi-
cations of the acm, 33(11), 88   97.
jaech, a., mulcaire, g., hathi, s., ostendorf, m., and smith,
n. a. (2016). hierarchical character-word models for lan-
guage identi   cation. in acl workshop on nlp for social
media, pp. 84      93.
jafarpour, s., burges, c. j. c., and ritter, a. (2009). fil-
ter, rank, and transfer the knowledge: learning to chat.
in nips workshop on advances in ranking, vancouver,
canada.
jauhiainen, t., lui, m., zampieri, m., baldwin, t., and
lind  en, k. (2018). automatic language identi   cation in
texts: a survey. arxiv preprint arxiv:1804.08186.
jefferson, g. (1972). side sequences. in sudnow, d. (ed.),
studies in social interaction, pp. 294   333. free press, new
york.
jefferson, g. (1984). notes on a systematic deployment of
the acknowledgement tokens    yeah    and    mm hm   . papers
in linguistics, 17(2), 197   216.
jeffreys, h. (1948). theory of id203 (2nd ed.). claren-
don press. section 3.23.

bibliography

527

jekat, s., klein, a., maier, e., maleck, i., mast, m., and
quantz, j. (1995). dialogue acts in verbmobil. verbmobil   
report   65   95.
jelinek, f. (1976). continuous id103 by statis-
tical methods. proceedings of the ieee, 64(4), 532   557.
jelinek, f. (1990). self-organized id38 for
id103.
in waibel, a. and lee, k.-f. (eds.),
readings in id103, pp. 450   506. morgan
kaufmann. originally distributed as ibm technical report
in 1985.
jelinek, f. (1997). statistical methods for speech recogni-
tion. mit press.
jelinek, f. and lafferty, j. d. (1991). computation of
the id203 of initial substring generation by stochastic
context-free grammars. computational linguistics, 17(3),
315   323.
jelinek, f., lafferty, j. d., magerman, d. m., mercer, r. l.,
ratnaparkhi, a., and roukos, s. (1994). decision tree pars-
ing using a hidden derivation model. in arpa human lan-
guage technologies workshop, plainsboro, n.j., pp. 272   
277.
jelinek, f. and mercer, r. l. (1980). interpolated estimation
of markov source parameters from sparse data. in gelsema,
e. s. and kanal, l. n. (eds.), proceedings, workshop on
pattern recognition in practice, pp. 381   397. north hol-
land.
ji, h., grishman, r., and dang, h. t. (2010). overview of
the tac 2011 knowledge base population track. in tac-11.
jiang, j. j. and conrath, d. w. (1997). semantic similarity
based on corpus statistics and lexical taxonomy.
in ro-
cling x, taiwan.
jim  enez, v. m. and marzal, a. (2000). computation of the
n best parse trees for weighted and stochastic context-free
grammars. in advances in pattern recognition: proceed-
ings of the joint iapr international workshops, sspr 2000
and spr 2000, alicante, spain, pp. 183   192. springer.
johnson, m. (1998). pid18 models of linguistic tree repre-
sentations. computational linguistics, 24(4), 613   632.
johnson, m. (2001). joint and conditional estimation of tag-
ging and parsing models. in acl-01, pp. 314   321.
johnson, m., geman, s., canon, s., chi, z., and riezler, s.
(1999). estimators for stochastic    uni   cation-based    gram-
mars. in acl-99, pp. 535   541.
johnson, w. e. (1932). id203: deductive and inductive
problems (appendix to). mind, 41(164), 421   423.
johnson-laird, p. n. (1983). mental models. harvard uni-
versity press, cambridge, ma.
jones, m. p. and martin, j. h. (1997). contextual spelling
correction using latent semantic analysis. in anlp 1997,
washington, d.c., pp. 166   173.
jones, r., mccallum, a., nigam, k., and riloff, e. (1999).
id64 for text learning tasks. in ijcai-99 work-
shop on id111: foundations, techniques and appli-
cations.
jones, t. (2015). toward a description of african american
vernacular english dialect regions using    black twitter   .
american speech, 90(4), 403   440.
joos, m. (1950). description of language design. jasa, 22,
701   708.
joshi, a. k. (1985). id34s: how much
context-sensitivity is required to provide reasonable struc-
tural descriptions?.
in dowty, d. r., karttunen, l., and
zwicky, a. (eds.), natural language parsing, pp. 206   
250. cambridge university press.
joshi, a. k. and hopely, p. (1999). a parser from antiq-
uity. in kornai, a. (ed.), extended finite state models of
language, pp. 6   15. cambridge university press.

528 bibliography

joshi, a. k. and srinivas, b. (1994). disambiguation of
super parts of speech (or supertags): almost parsing.
in
coling-94, kyoto, pp. 154   160.
joshi, m., choi, e., weld, d. s., and zettlemoyer, l.
(2017). triviaqa: a large scale distantly supervised chal-
lenge dataset for reading comprehension. in acl 2017.
jurafsky, d. (2014). the language of food. w. w. norton,
new york.
jurafsky, d., chahuneau, v., routledge, b. r., and smith,
n. a. (2014). narrative framing of consumer sentiment in
online restaurant reviews. first monday, 19(4).
jurafsky, d., wooters, c., tajchman, g., segal, j., stol-
cke, a., fosler, e., and morgan, n. (1994). the berke-
ley restaurant project. in icslp-94, yokohama, japan, pp.
2139   2142.
jurgens, d. and klapaftis, i. p. (2013).
semeval-2013
task 13: word sense induction for graded and non-graded
senses. in *sem, pp. 290   299.
jurgens, d., tsvetkov, y., and jurafsky, d. (2017). incorpo-
rating dialectal variability for socially equitable language
identi   cation. in acl 2017, pp. 51   57.
justeson, j. s. and katz, s. m. (1991). co-occurrences of
antonymous adjectives and their contexts. computational
linguistics, 17(1), 1   19.
kalyanpur, a., boguraev, b. k., patwardhan, s., murdock,
j. w., lally, a., welty, c., prager, j. m., coppola, b.,
fokoue-nkoutche, a., zhang, l., pan, y., and qiu, z. m.
(2012). structured data and id136 in deepqa. ibm jour-
nal of research and development, 56(3/4), 10:1   10:14.
kang, j. s., feng, s., akoglu, l., and choi, y. (2014).
connotationid138: learning connotation over the word+
sense network. in acl 2014.
kannan, a. and vinyals, o. (2016). adversarial evaluation
of dialogue models. in nips 2016 workshop on adversar-
ial training.
kaplan, r. m. (1973). a general syntactic processor.
in
rustin, r. (ed.), natural language processing, pp. 193   
241. algorithmics press.
kaplan, r. m., riezler, s., king, t. h., maxwell iii, j. t.,
vasserman, a., and crouch, r. (2004). speed and accuracy
in shallow and deep stochastic parsing. in hlt-naacl-04.
karlsson, f., voutilainen, a., heikkil  a, j., and anttila,
a. (eds.). (1995). constraint grammar: a language-
independent system for parsing unrestricted text. mouton
de gruyter.
karttunen, l. (1999). comments on joshi. in kornai, a.
(ed.), extended finite state models of language, pp. 16   
18. cambridge university press.
kasami, t. (1965). an ef   cient recognition and syntax
analysis algorithm for context-free languages. tech. rep.
afcrl-65-758, air force cambridge research labora-
tory, bedford, ma.
kashyap, r. l. and oommen, b. j. (1983). spelling cor-
rection using probabilistic methods. pattern recognition
letters, 2, 147   154.
katz, j. j. and fodor, j. a. (1963). the structure of a seman-
tic theory. language, 39, 170   210.
kawamoto, a. h. (1988). distributed representations of
ambiguous words and their resolution in connectionist net-
works. in small, s. l., cottrell, g. w., and tanenhaus, m.
(eds.), lexical ambiguity resolution, pp. 195   228. mor-
gan kaufman.
kay, m. (1967). experiments with a powerful parser.
in
proc. 2eme conference internationale sur le traitement
automatique des langues, grenoble.

kay, m. (1973). the mind system.
in rustin, r. (ed.),
natural language processing, pp. 155   188. algorithmics
press.
kay, m. (1982). algorithm schemata and data structures in
syntactic processing. in all  en, s. (ed.), text processing:
text analysis and generation, text typology and attribu-
tion, pp. 327   358. almqvist and wiksell, stockholm.
kay, p. and fillmore, c. j. (1999). grammatical construc-
tions and linguistic generalizations: the what   s x doing
y? construction. language, 75(1), 1   33.
keller, f. and lapata, m. (2003). using the web to obtain
frequencies for unseen bigrams. computational linguis-
tics, 29, 459   484.
kelly, e. f. and stone, p. j. (1975). computer recognition
of english word senses. north-holland.
kernighan, m. d., church, k. w., and gale, w. a. (1990).
a id147 program base on a noisy channel
model. in coling-90, helsinki, vol. ii, pp. 205   211.
kiela, d. and clark, s. (2014). a systematic study of seman-
tic vector space model parameters. in proceedings of the
eacl 2nd workshop on continuous vector space models
and their compositionality (cvsc), pp. 21   30.
kilgarriff, a. (2001). english lexical sample task descrip-
tion. in proceedings of senseval-2: second international
workshop on evaluating id51 sys-
tems, toulouse, france, pp. 17   20.
kilgarriff, a. and palmer, m. (eds.). (2000). computing
and the humanities: special issue on senseval, vol. 34.
kluwer.
kilgarriff, a. and rosenzweig, j. (2000). framework and
results for english senseval. computers and the hu-
manities, 34, 15   48.
kim, s. m. and hovy, e. h. (2004). determining the senti-
ment of opinions. in coling-04.
kingma, d. and ba, j. (2015). adam: a method for stochas-
tic optimization. in iclr 2015.
kintsch, w. (1974). the representation of meaning in
memory. wiley, new york.
kipper, k., dang, h. t., and palmer, m. (2000). class-based
construction of a verb lexicon. in aaai-00, austin, tx, pp.
691   696.
kleene, s. c. (1951). representation of events in nerve nets
and    nite automata. tech. rep. rm-704, rand corpora-
tion. rand research memorandum.
kleene, s. c. (1956). representation of events in nerve
nets and    nite automata.
in shannon, c. and mccarthy,
j. (eds.), automata studies, pp. 3   41. princeton university
press.
klein, d. (2005). the unsupervised learning of natural
language structure. ph.d. thesis, stanford university.
klein, d. and manning, c. d. (2001). parsing and hyper-
graphs. in iwpt-01, pp. 123   134.
klein, d. and manning, c. d. (2002). a generative
constituent-context model for improved grammar induc-
tion. in acl-02.
klein, d. and manning, c. d. (2003a). a* parsing: fast
exact viterbi parse selection. in hlt-naacl-03.
klein, d. and manning, c. d. (2003b). accurate unlexical-
ized parsing. in hlt-naacl-03.
klein, d. and manning, c. d. (2004). corpus-based induc-
tion of syntactic structure: models of dependency and con-
stituency. in acl-04, pp. 479   486.
klein, s. and simmons, r. f. (1963). a computational ap-
proach to grammatical coding of english words. journal of
the association for computing machinery, 10(3), 334   347.

kneser, r. and ney, h. (1995). improved backing-off for
m-gram id38.
in icassp-95, vol. 1, pp.
181   184.
knuth, d. e. (1973). sorting and searching: the art of
computer programming volume 3. addison-wesley.
ko  cisk`y, t., schwarz, j., blunsom, p., dyer, c., hermann,
k. m., melis, g., and grefenstette, e. (2018). the narra-
tiveqa reading comprehension challenge. tacl, 6, 317   
328.
krovetz, r. (1993). viewing morphology as an id136
process. in sigir-93, pp. 191   202.
krovetz, r. (1998). more than one sense per discourse. in
proceedings of the acl-siglex senseval workshop.
kruskal, j. b. (1983). an overview of sequence compari-
son. in sankoff, d. and kruskal, j. b. (eds.), time warps,
string edits, and macromolecules: the theory and prac-
tice of sequence comparison, pp. 1   44. addison-wesley.
kudo, t. and matsumoto, y. (2002). japanese dependency
analysis using cascaded chunking. in conll-02, pp. 63   
69.
kukich, k. (1992). techniques for automatically correcting
words in text. acm computing surveys, 24(4), 377   439.
kullback, s. and leibler, r. a. (1951). on information and
suf   ciency. annals of mathematical statistics, 22, 79   86.
kuno, s. (1965). the predictive analyzer and a path elimi-
nation technique. communications of the acm, 8(7), 453   
462.
kuno, s. and oettinger, a. g. (1963). multiple-path syn-
tactic analyzer.
in popplewell, c. m. (ed.), information
processing 1962: proceedings of the ifip congress 1962,
munich, pp. 306   312. north-holland. reprinted in grosz
et al. (1986).
kupiec, j. (1992). robust part-of-speech tagging using a
hidden markov model. computer speech and language,
6, 225   242.
ku  cera, h. and francis, w. n. (1967). computational anal-
ysis of present-day american english. brown university
press, providence, ri.
labov, w. and fanshel, d. (1977). therapeutic discourse.
academic press.
lafferty, j. d., mccallum, a., and pereira, f. c. n. (2001).
conditional random    elds: probabilistic models for seg-
menting and labeling sequence data. in icml 2001, stan-
ford, ca.
lafferty, j. d., sleator, d., and temperley, d. (1992).
grammatical trigrams: a probabilistic model of link gram-
mar. in proceedings of the 1992 aaai fall symposium on
probabilistic approaches to natural language.
lakoff, g. (1965). on the nature of syntactic irregularity.
ph.d. thesis, indiana university. published as irregularity
in syntax. holt, rinehart, and winston, new york, 1970.
lakoff, g. (1972). linguistics and natural logic. in david-
son, d. and harman, g. (eds.), semantics for natural lan-
guage, pp. 545   665. d. reidel.
lakoff, g. and johnson, m. (1980). metaphors we live by.
university of chicago press, chicago, il.
lally, a., prager, j. m., mccord, m. c., boguraev, b. k.,
patwardhan, s., fan, j., fodor, p., and chu-carroll, j.
(2012). question analysis: how watson reads a clue. ibm
journal of research and development, 56(3/4), 2:1   2:14.
lample, g., ballesteros, m., subramanian, s., kawakami,
k., and dyer, c. (2016). neural architectures for named
entity recognition. in naacl hlt 2016.

bibliography

529

landauer, t. k. (ed.). (1995). the trouble with computers:
usefulness, usability, and productivity. mit press.
landauer, t. k. and dumais, s. t. (1997). a solution to
plato   s problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge.
psychological review, 104, 211   240.
landes, s., leacock, c., and tengi, r. i. (1998). building
semantic concordances. in fellbaum, c. (ed.), id138:
an electronic lexical database, pp. 199   216. mit press.
lang, j. and lapata, m. (2014). similarity-driven semantic
role induction via graph partitioning. computational lin-
guistics, 40(3), 633   669.
lapata, m. and keller, f. (2004). the web as a base-
line: evaluating the performance of unsupervised web-
based models for a range of nlp tasks. in hlt-naacl-04.
lapesa, g. and evert, s. (2014). a large scale evaluation
of distributional semantic models: parameters, interactions
and model selection. tacl, 2, 531   545.
lari, k. and young, s. j. (1990). the estimation of stochas-
tic context-free grammars using the inside-outside algo-
rithm. computer speech and language, 4, 35   56.
lau, j. h., cook, p., mccarthy, d., newman, d., and bald-
win, t. (2012). word sense induction for novel sense de-
tection. in eacl-12, pp. 591   601.
leacock, c. and chodorow, m. s. (1998). combining lo-
cal context and id138 similarity for word sense identi-
   cation.
in fellbaum, c. (ed.), id138: an electronic
lexical database, pp. 265   283. mit press.
leacock, c., towell, g., and voorhees, e. m. (1993).
corpus-based statistical sense resolution. in hlt-93, pp.
260   265.
lecun, y., boser, b., denker, j. s., henderson, d., howard,
r. e., hubbard, w., and jackel, l. d. (1989). backpropa-
gation applied to handwritten zip code recognition. neural
computation, 1(4), 541   551.
lecun, y., boser, b. e., denker, j. s., henderson, d.,
howard, r. e., hubbard, w. e., and jackel, l. d. (1990).
handwritten digit recognition with a back-propagation net-
work. in nips 1990, pp. 396   404.
lee, d. d. and seung, h. s. (1999). learning the parts
of objects by non-negative id105. nature,
401(6755), 788   791.
lee, k., salant, s., kwiatkowski, t., parikh, a., das, d., and
berant, j. (2017). learning recurrent span representations
for extractive id53. in arxiv 1611.01436.
lehnert, w. g., cardie, c., fisher, d., riloff, e., and
williams, r. (1991). description of the circus system
as used for muc-3. in sundheim, b. (ed.), muc-3, pp.
223   233.
lemon, o., georgila, k., henderson, j., and stuttle, m.
(2006). an isu dialogue system exhibiting reinforcement
learning of dialogue policies: generic slot-   lling in the
talk in-car system. in eacl-06.
lesk, m. e. (1986). automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine cone
from an ice cream cone. in proceedings of the 5th inter-
national conference on systems documentation, toronto,
ca, pp. 24   26.
leuski, a. and traum, d. (2011). npceditor: creating
virtual human dialogue using information retrieval tech-
niques. ai magazine, 32(2), 42   56.
levenshtein, v. i. (1966). binary codes capable of cor-
recting deletions, insertions, and reversals. cybernetics
and control theory, 10(8), 707   710. original in doklady
akademii nauk sssr 163(4): 845   848 (1965).

530 bibliography

levesque, h. j., cohen, p. r., and nunes, j. h. t. (1990).
on acting together. in aaai-90, boston, ma, pp. 94   99.
morgan kaufmann.
levin, b. (1977). mapping sentences to case frames. tech.
rep. 167, mit ai laboratory. ai working paper 143.
levin, b. (1993). english verb classes and alternations: a
preliminary investigation. university of chicago press.
levin, b. and rappaport hovav, m. (2005). argument real-
ization. cambridge university press.
levin, e., pieraccini, r., and eckert, w. (2000). a stochas-
tic model of human-machine interaction for learning dialog
strategies. ieee transactions on speech and audio pro-
cessing, 8, 11   23.
levinson, s. c. (1983). conversational analysis, chap. 6.
cambridge university press.
levow, g.-a. (1998). characterizing and recognizing spo-
ken corrections in human-computer dialogue. in coling-
acl, pp. 736   742.
levy, o. and goldberg, y. (2014a). dependency-based word
embeddings. in acl 2014.
levy, o. and goldberg, y. (2014b). linguistic regularities
in sparse and explicit word representations. in conll-14.
levy, o. and goldberg, y. (2014c). neural id27
as implicit id105.
in nips 14, pp. 2177   
2185.
levy, o., goldberg, y., and dagan, i. (2015). improving dis-
tributional similarity with lessons learned from word em-
beddings. tacl, 3, 211   225.
levy, r. (2008). expectation-based syntactic comprehen-
sion. cognition, 106(3), 1126   1177.
lewis, m. and steedman, m. (2014). a* id35 parsing with a
supertag-factored model.. in emnlp, pp. 990   1000.
li, j., chen, x., hovy, e. h., and jurafsky, d. (2015). vi-
sualizing and understanding neural models in nlp.
in
naacl hlt 2015.
li, j., galley, m., brockett, c., gao, j., and dolan, b.
(2016a). a diversity-promoting objective function for neu-
ral conversation models. in naacl hlt 2016.
li, j., monroe, w., ritter, a., galley, m., gao, j., and juraf-
sky, d. (2016b). deep id23 for dialogue
generation. in emnlp 2016.
li, j., monroe, w., shi, t., ritter, a., and jurafsky, d.
(2017). adversarial learning for neural dialogue genera-
tion. in emnlp 2017.
li, x. and roth, d. (2002). learning question classi   ers. in
coling-02, pp. 556   562.
li, x. and roth, d. (2005). learning question classi   ers:
the role of semantic information. journal of natural lan-
guage engineering, 11(4).
lin, d. (1995). a dependency-based method for evaluating
broad-coverage parsers. in ijcai-95, montreal, pp. 1420   
1425.
lin, d. (1998). an information-theoretic de   nition of simi-
larity. in icml 1998, san francisco, pp. 296   304.
lin, d. (2003). dependency-based evaluation of minipar. in
workshop on the evaluation of parsing systems.
lin, j. (2007). an exploration of the principles underlying
redundancy-based factoid id53. acm trans-
actions on information systems, 25(2).
lin, y., michel, j.-b., lieberman aiden, e., orwant, j.,
brockman, w., and petrov, s. (2012). syntactic annota-
tions for the google books ngram corpus. in acl 2012, pp.
169   174.

lindsey, r. (1963). inferential memory as the basis of ma-
chines which understand natural language. in feigenbaum,
e. and feldman, j. (eds.), computers and thought, pp.
217   233. mcgraw hill.
litman, d. j., swerts, m., and hirschberg, j. (2000). pre-
dicting automatic id103 performance using
prosodic cues. in naacl 2000.
litman, d. j., walker, m. a., and kearns, m. (1999). auto-
matic detection of poor id103 at the dialogue
level. in acl-99, college park, ma, pp. 309   316.
liu, b. and zhang, l. (2012). a survey of opinion mining
and id31. in aggarwal, c. c. and zhai, c.
(eds.), mining text data, pp. 415   464. springer.
liu, c.-w., lowe, r. t., serban, i. v., noseworthy, m.,
charlin, l., and pineau, j. (2016). how not to evalu-
ate your dialogue system: an empirical study of unsuper-
vised id74 for dialogue response generation.
in emnlp 2016.
liu, x., gales, m. j. f., and woodland, p. c. (2013). use of
contexts in language model interpolation and adaptation.
computer speech & language, 27(1), 301   321.
lochbaum, k. e., grosz, b. j., and sidner, c. l. (2000).
discourse structure and intention recognition. in dale, r.,
moisl, h., and somers, h. l. (eds.), handbook of natural
language processing. marcel dekker.
lovins, j. b. (1968). development of a id30 algo-
rithm. mechanical translation and computational lin-
guistics, 11(1   2), 9   13.
lowe, r. t., noseworthy, m., serban, i. v., angelard-
gontier, n., bengio, y., and pineau, j. (2017a). towards
an automatic turing test: learning to evaluate dialogue re-
sponses. in acl 2017.
lowe, r. t., pow, n., serban, i. v., charlin, l., liu, c.-
w., and pineau, j. (2017b). training end-to-end dialogue
systems with the ubuntu dialogue corpus. dialogue & dis-
course, 8(1), 31   65.
luhn, h. p. (1957). a statistical approach to the mecha-
nized encoding and searching of literary information. ibm
journal of research and development, 1(4), 309   317.
lui, m. and baldwin, t. (2011). cross-domain feature se-
lection for language identi   cation. in ijcnlp-11, pp. 553   
561.
lui, m. and baldwin, t. (2012). langid.py: an off-the-
shelf language identi   cation tool. in acl 2012, pp. 25   30.
lyons, j. (1977). semantics. cambridge university press.
ma, x. and hovy, e. h. (2016). end-to-end sequence label-
ing via bi-directional lstm-id98s-crf. in acl 2016.
madhu, s. and lytel, d. (1965). a    gure of merit technique
for the resolution of noid165matical ambiguity. mechan-
ical translation, 8(2), 9   13.
magerman, d. m. (1994). natural language parsing as
statistical pattern recognition. ph.d. thesis, university of
pennsylvania.
magerman, d. m. (1995). statistical decision-tree models
for parsing. in acl-95, pp. 276   283.
magerman, d. m. and marcus, m. p. (1991). pearl: a prob-
abilistic chart parser. in eacl-91, berlin.
mairesse, f. and walker, m. a. (2008). trainable generation
of big-   ve personality styles through data-driven parameter
estimation. in acl-08, columbus.
manandhar, s., klapaftis, i. p., dligach, d., and pradhan,
s. (2010). semeval-2010 task 14: word sense induction &
disambiguation. in semeval-2010, pp. 63   68.
manning, c. d. (2011). part-of-speech tagging from 97%
to 100%: is it time for some linguistics?. in cicling 2011,
pp. 171   189.

manning, c. d., raghavan, p., and sch  utze, h. (2008). in-
troduction to information retrieval. cambridge.
manning, c. d. and sch  utze, h. (1999). foundations of
statistical natural language processing. mit press.
marcus, m. p. (1980). a theory of syntactic recognition
for natural language. mit press.
marcus, m. p. (1990). summary of session 9: automatic
acquisition of linguistic structure. in proceedings darpa
speech and natural language workshop, hidden valley,
pa, pp. 249   250.
marcus, m. p., kim, g., marcinkiewicz, m. a., macintyre,
r., bies, a., ferguson, m., katz, k., and schasberger, b.
(1994). the id32: annotating predicate argu-
ment structure.
in arpa human language technology
workshop, plainsboro, nj, pp. 114   119. morgan kauf-
mann.
marcus, m. p., santorini, b., and marcinkiewicz, m. a.
(1993). building a large annotated corpus of english: the
id32. computational linguistics, 19(2), 313   
330.
markov, a. a. (1913). essai d   une recherche statistique sur
le texte du roman    eugene onegin    illustrant la liaison des
epreuve en chain (   example of a statistical investigation of
the text of    eugene onegin    illustrating the dependence be-
tween samples in chain   ). izvistia imperatorskoi akademii
nauk (bulletin de l   acad  emie imp  eriale des sciences de
st.-p  etersbourg), 7, 153   162.
markov, a. a. (2006). classical text in translation: a. a.
markov, an example of statistical investigation of the text
eugene onegin concerning the connection of samples in
chains. science in context, 19(4), 591   600. translated by
david link.
maron, m. e. (1961). automatic indexing: an experimental
inquiry. journal of the acm (jacm), 8(3), 404   417.
m`arquez, l., carreras, x., litkowski, k. c., and stevenson,
s. (2008). id14: an introduction to the
special issue. computational linguistics, 34(2), 145   159.
marshall, i. (1983). choice of grammatical word-class
without global syntactic analysis: tagging words in the
lob corpus. computers and the humanities, 17, 139   150.
marshall, i. (1987). tag selection using probabilistic meth-
ods. in garside, r., leech, g., and sampson, g. (eds.), the
computational analysis of english, pp. 42   56. longman.
martin, j. h. (1986). the acquisition of polysemy. in icml
1986, irvine, ca, pp. 198   204.
masterman, m. (1957). the thesaurus in syntax and seman-
tics. mechanical translation, 4(1), 1   2.
mays, e., damerau, f. j., and mercer, r. l. (1991). con-
text based id147. information processing and
management, 27(5), 517   522.
mccallum, a., freitag, d., and pereira, f. c. n. (2000).
maximum id178 markov models for information extrac-
tion and segmentation. in icml 2000, pp. 591   598.
mccallum, a. and nigam, k. (1998). a comparison
of event models for naive bayes text classi   cation.
in
aaai/icml-98 workshop on learning for text categoriza-
tion, pp. 41   48.
mccawley, j. d. (1968). the role of semantics in a gram-
mar. in bach, e. w. and harms, r. t. (eds.), universals in
linguistic theory, pp. 124   169. holt, rinehart & winston.
mccawley, j. d. (1993). everything that linguists have al-
ways wanted to know about logic (2nd ed.). university of
chicago press, chicago, il.
mccawley, j. d. (1998). the syntactic phenomena of en-
glish. university of chicago press.

bibliography

531

mcclelland, j. l. and elman, j. l. (1986). the trace
model of speech perception. cognitive psychology, 18, 1   
86.
mcculloch, w. s. and pitts, w. (1943). a logical calculus of
ideas immanent in nervous activity. bulletin of mathemat-
ical biophysics, 5, 115   133. reprinted in neurocomput-
ing: foundations of research, ed. by j. a. anderson and e
rosenfeld. mit press 1988.
mcdonald, r., crammer, k., and pereira, f. c. n. (2005).
online large-margin training of dependency parsers.
in
acl-05, ann arbor, pp. 91   98.
mcdonald, r. and nivre, j. (2011). analyzing and integrat-
ing dependency parsers. computational linguistics, 37(1),
197   230.
mcdonald, r., pereira, f. c. n., ribarov, k., and haji  c, j.
(2005). non-projective id33 using spanning
tree algorithms. in hlt-emnlp-05.
mcguiness, d. l. and van harmelen, f. (2004). owl web
ontology overview. tech. rep. 20040210, world wide web
consortium.
mehl, m. r., gosling, s. d., and pennebaker, j. w. (2006).
personality in its natural habitat: manifestations and im-
plicit folk theories of personality in daily life.. journal of
personality and social psychology, 90(5).
mel     cuk, i. a. (1988). dependency syntax: theory and
practice. state university of new york press.
merialdo, b. (1994). tagging english text with a probabilis-
tic model. computational linguistics, 20(2), 155   172.
mesnil, g., dauphin, y., yao, k., bengio, y., deng, l.,
hakkani-t  ur, d., he, x., heck, l., t  ur, g., yu, d., and
zweig, g. (2015). using recurrent neural networks for
slot    lling in spoken language understanding. ieee/acm
transactions on audio, speech and language processing
(taslp), 23(3), 530   539.
metsis, v., androutsopoulos, i., and paliouras, g. (2006).
spam    ltering with naive bayes-which naive bayes?.
in
ceas, pp. 27   28.
meyers, a., reeves, r., macleod, c., szekely, r., zielinska,
v., young, b., and grishman, r. (2004). the nombank
project: an interim report.
in proceedings of the naa-
cl/hlt workshop: frontiers in corpus annotation.
microsoft (2014). http://www.msxiaoice.com..
mihalcea, r. (2007). using wikipedia for automatic word
sense disambiguation. in naacl-hlt 07, pp. 196   203.
mihalcea, r. and moldovan, d. (2001). automatic genera-
tion of a coarse grained id138. in naacl workshop on
id138 and other lexical resources.
mikheev, a., moens, m., and grover, c. (1999). named en-
tity recognition without gazetteers. in eacl-99, bergen,
norway, pp. 1   8.
mikolov, t. (2012). statistical language models based on
neural networks. ph.d. thesis, ph. d. thesis, brno univer-
sity of technology.
mikolov, t., chen, k., corrado, g. s., and dean, j.
(2013). ef   cient estimation of word representations in vec-
tor space. in iclr 2013.
mikolov, t., kombrink, s., burget, l.,   cernock`y, j. h., and
khudanpur, s. (2011). extensions of recurrent neural net-
work language model. in icassp-11, pp. 5528   5531.
mikolov, t., sutskever, i., chen, k., corrado, g. s., and
dean, j. (2013a). distributed representations of words and
phrases and their compositionality. in nips 13, pp. 3111   
3119.
mikolov, t., yih, w.-t., and zweig, g. (2013b). linguistic
regularities in continuous space word representations.
in
naacl hlt 2013, pp. 746   751.

532 bibliography

miller, g. a. and charles, w. g. (1991). contextual cor-
relates of semantics similarity. language and cognitive
processes, 6(1), 1   28.
miller, g. a. and chomsky, n. (1963). finitary models of
language users. in luce, r. d., bush, r. r., and galanter,
e. (eds.), handbook of mathematical psychology, vol. ii,
pp. 419   491. john wiley.
miller, g. a., leacock, c., tengi, r. i., and bunker, r. t.
(1993). a semantic concordance.
in proceedings arpa
workshop on human language technology, pp. 303   308.
miller, g. a. and selfridge, j. a. (1950). verbal context
and the recall of meaningful material. american journal of
psychology, 63, 176   185.
miller, s., bobrow, r. j., ingria, r., and schwartz, r.
(1994). hidden understanding models of natural language.
in acl-94, las cruces, nm, pp. 25   32.
minsky, m. (1961). steps toward arti   cial intelligence. pro-
ceedings of the ire, 49(1), 8   30.
minsky, m. (1974). a framework for representing knowl-
edge. tech. rep. 306, mit ai laboratory. memo 306.
minsky, m. and papert, s. (1969). id88s. mit press.
mintz, m., bills, s., snow, r., and jurafsky, d. (2009).
distant supervision for id36 without labeled
data. in acl ijcnlp 2009.
mitton, r. (1987). spelling checkers, spelling correctors and
the misspellings of poor spellers. information processing
& management, 23(5), 495   505.
miwa, m. and bansal, m. (2016). end-to-end relation ex-
traction using lstms on sequences and tree structures.
in
acl 2016, pp. 1105   1116.
mohammad, s. m. and turney, p. d. (2013). crowdsourc-
ing a word-emotion association lexicon. computational in-
telligence, 29(3), 436   465.
monroe, b. l., colaresi, m. p., and quinn, k. m. (2008).
fightin   words: lexical feature selection and evaluation for
identifying the content of political con   ict. political anal-
ysis, 16(4), 372   403.
montague, r. (1973). the proper treatment of quanti   cation
in ordinary english. in thomason, r. (ed.), formal philos-
ophy: selected papers of richard montague, pp. 247   270.
yale university press, new haven, ct.
monz, c. (2004). minimal span weighting retrieval for ques-
tion answering.
in sigir workshop on information re-
trieval for id53, pp. 23   30.
morgan, a. a., hirschman, l., colosimo, m., yeh, a. s.,
and colombe, j. b. (2004). gene name identi   cation and
id172 using a model organism database. journal
of biomedical informatics, 37(6), 396   410.
morgan, n. and bourlard, h. (1989). generalization and pa-
rameter estimation in feedforward nets: some experiments.
in advances in neural information processing systems, pp.
630   637.
morgan, n. and bourlard, h. (1990). continuous speech
recognition using multilayer id88s with hidden
markov models. in icassp-90, pp. 413   416.
morris, w. (ed.). (1985). american heritage dictionary
(2nd college edition ed.). houghton mif   in.
mosteller, f. and wallace, d. l. (1963). id136 in an au-
thorship problem: a comparative study of discrimination
methods applied to the authorship of the disputed federal-
ist papers. journal of the american statistical association,
58(302), 275   309.
mosteller, f. and wallace, d. l. (1964). id136 and dis-
puted authorship: the federalist. springer-verlag. a
second edition appeared in 1984 as applied bayesian and
classical id136.

mrk  si  c, n., o   s  eaghdha, d., wen, t.-h., thomson, b., and
young, s. j. (2017). neural belief tracker: data-driven di-
alogue state tracking. in acl 2017.
murdock, j. w., fan, j., lally, a., shima, h., and bogu-
raev, b. k. (2012a). textual evidence gathering and anal-
ysis. ibm journal of research and development, 56(3/4),
8:1   8:14.
murdock, j. w., kalyanpur, a., welty, c., fan, j., fer-
rucci, d. a., gondek, d. c., zhang, l., and kanayama,
h. (2012b). typing candidate answers using type coercion.
ibm journal of research and development, 56(3/4), 7:1   
7:13.
murphy, k. p. (2012). machine learning: a probabilistic
perspective. mit press.
n  adas, a. (1984). estimation of probabilities in the lan-
guage model of the ibm id103 system. ieee
transactions on acoustics, speech, signal processing,
32(4), 859   861.
nagata, m. and morimoto, t. (1994). first steps toward sta-
tistical modeling of dialogue to predict the speech act type
of the next utterance. speech communication, 15, 193   
203.
nash-webber, b. l. (1975). the role of semantics in auto-
matic speech understanding. in bobrow, d. g. and collins,
a. (eds.), representation and understanding, pp. 351   
382. academic press.
naur, p., backus, j. w., bauer, f. l., green, j., katz, c.,
mccarthy, j., perlis, a. j., rutishauser, h., samelson, k.,
vauquois, b., wegstein, j. h., van wijnagaarden, a., and
woodger, m. (1960). report on the algorithmic language
algol 60. communications of the acm, 3(5), 299   314.
revised in cacm 6:1, 1-17, 1963.
navigli, r. (2006). meaningful id91 of senses helps
boost id51 performance.
in col-
ing/acl 2006, pp. 105   112.
navigli, r. (2009). id51: a survey.
acm computing surveys, 41(2).
navigli, r. and lapata, m. (2010). an experimental study
of graph connectivity for unsupervised word sense disam-
biguation. ieee transactions on pattern analysis and ma-
chine intelligence, 32(4), 678   692.
navigli, r. and vannella, d. (2013). semeval-2013 task 11:
word sense induction & disambiguation within an end-user
application. in *sem, pp. 193   201.
needleman, s. b. and wunsch, c. d. (1970). a gen-
eral method applicable to the search for similarities in the
amino-acid sequence of two proteins. journal of molecular
biology, 48, 443   453.
neff, g. and nagy, p. (2016). talking to bots: symbiotic
agency and the case of tay. international journal of com-
munication, 10, 4915   4931.
newell, a., langer, s., and hickey, m. (1998). the r  ole of
natural language processing in alternative and augmenta-
tive communication. natural language engineering, 4(1),
1   16.
ney, h. (1991). id145 parsing for context-
free grammars in continuous id103.
ieee
transactions on signal processing, 39(2), 336   340.
ng, a. y. and jordan, m. i. (2002). on discriminative vs.
generative classi   ers: a comparison of id28
and naive bayes. in nips 14, pp. 841   848.
ng, h. t., teo, l. h., and kwan, j. l. p. (2000). a machine
learning approach to answering questions for reading com-
prehension tests. in emnlp 2000, pp. 124   132.

incremental non-projective dependency

nielsen, j. (1992). the usability engineering life cycle. ieee
computer, 25(3), 12   22.
nielsen, m. a. (2015). neural networks and deep learning.
determination press usa.
nigam, k., lafferty, j. d., and mccallum, a. (1999). using
maximum id178 for text classi   cation. in ijcai-99 work-
shop on machine learning for information    ltering, pp. 61   
67.
nilsson, j., riedel, s., and yuret, d. (2007). the conll 2007
shared task on id33. in proceedings of the
conll shared task session of emnlp-conll, pp. 915   
932. sn.
nist (2005). id103 scoring toolkit (sctk) ver-
sion 2.1. http://www.nist.gov/speech/tools/.
nivre, j. (2007).
parsing. in naacl-hlt 07.
nivre, j. (2003). an ef   cient algorithm for projective de-
pendency parsing. in proceedings of the 8th international
workshop on parsing technologies (iwpt.
nivre, j. (2006). inductive id33. springer.
nivre, j. (2009). non-projective id33 in ex-
pected linear time. in acl ijcnlp 2009, pp. 351   359.
nivre, j., de marneffe, m.-c., ginter, f., goldberg, y.,
haji  c, j., manning, c. d., mcdonald, r. t., petrov, s.,
pyysalo, s., silveira, n., tsarfaty, r., and zeman, d.
(2016a). universal dependencies v1: a multilingual tree-
bank collection. in lrec.
nivre, j., de marneffe, m.-c., ginter, f., goldberg, y.,
haji  c, j., manning, c. d., mcdonald, r. t., petrov, s.,
pyysalo, s., silveira, n., tsarfaty, r., and zeman, d.
(2016b). universal dependencies v1: a multilingual tree-
bank collection. in lrec-16.
nivre, j., hall, j., nilsson, j., chanev, a., eryigit, g.,
k  ubler, s., marinov, s., and marsi, e. (2007). malt-
parser: a language-independent system for data-driven de-
pendency parsing. natural language engineering, 13(02),
95   135.
nivre, j. and nilsson, j. (2005). pseudo-projective depen-
dency parsing. in acl-05, pp. 99   106.
nivre, j. and scholz, m. (2004). deterministic dependency
parsing of english text. in coling-04, p. 64.
niwa, y. and nitta, y. (1994). co-occurrence vectors from
corpora vs. distance vectors from dictionaries. in acl-94,
pp. 304   309.
noreen, e. w. (1989). computer intensive methods for test-
ing hypothesis. wiley.
norman, d. a. (1988). the design of everyday things.
basic books.
norman, d. a. and rumelhart, d. e. (1975). explorations
in cognition. freeman.
norvig, p. (1991). techniques for automatic memoization
with applications to context-free parsing. computational
linguistics, 17(1), 91   98.
norvig, p. (2007). how to write a spelling corrector. http:
//www.norvig.com/spell-correct.html.
norvig, p. (2009). natural language corpus data. in segaran,
t. and hammerbacher, j. (eds.), beautiful data: the stories
behind elegant data solutions. o   reilly.
nosek, b. a., banaji, m. r., and greenwald, a. g. (2002a).
harvesting implicit group attitudes and beliefs from a
demonstration web site. group dynamics: theory, re-
search, and practice, 6(1), 101.
nosek, b. a., banaji, m. r., and greenwald, a. g. (2002b).
math=male, me=female, therefore math(cid:54)= me. journal of
personality and social psychology, 83(1), 44.

bibliography

533

o   connor, b., krieger, m., and ahn, d. (2010). tweetmo-
tif: exploratory search and topic summarization for twitter.
in icwsm.
odell, m. k. and russell, r. c. (1918/1922). u.s. patents
1261167 (1918), 1435663 (1922). cited in knuth (1973).
oh, a. h. and rudnicky, a. i. (2000). stochastic language
generation for spoken dialogue systems.
in proceedings
of the 2000 anlp/naacl workshop on conversational
systems-volume 3, pp. 27   32.
oravecz, c. and dienes, p. (2002). ef   cient stochastic part-
of-speech tagging for hungarian. in lrec-02, las palmas,
canary islands, spain, pp. 710   717.
osgood, c. e., suci, g. j., and tannenbaum, p. h. (1957).
the measurement of meaning. university of illinois press.
packard, d. w. (1973). computer-assisted morphological
analysis of ancient greek. in zampolli, a. and calzolari,
n. (eds.), computational and mathematical linguistics:
proceedings of the international conference on computa-
tional linguistics, pisa, pp. 343   355. leo s. olschki.
palmer, d. (2012). text preprocessing.
in indurkhya, n.
and damerau, f. j. (eds.), handbook of natural language
processing, pp. 9   30. crc press.
palmer, m., babko-malaya, o., and dang, h. t. (2004).
different sense granularities for different applications. in
hlt-naacl workshop on scalable natural language un-
derstanding, boston, ma, pp. 49   56.
palmer, m., dang, h. t., and fellbaum, c. (2006). mak-
ing    ne-grained and coarse-grained sense distinctions, both
manually and automatically. natural language engineer-
ing, 13(2), 137   163.
palmer, m., fellbaum, c., cotton, s., delfs, l., and dang,
h. t. (2001). english tasks: all-words and verb lexical
sample. in proceedings of senseval-2: 2nd international
workshop on evaluating id51 sys-
tems, toulouse, france, pp. 21   24.
palmer, m., gildea, d., and xue, n. (2010). semantic role
labeling. synthesis lectures on human language tech-
nologies, 3(1), 1   103.
palmer, m., kingsbury, p., and gildea, d. (2005). the
proposition bank: an annotated corpus of semantic roles.
computational linguistics, 31(1), 71   106.
palmer, m., ng, h. t., and dang, h. t. (2006). evalua-
tion of wsd systems. in agirre, e. and edmonds, p. (eds.),
id51: algorithms and applications.
kluwer.
pang, b. and lee, l. (2008). opinion mining and sentiment
analysis. foundations and trends in information retrieval,
2(1-2), 1   135.
pang, b., lee, l., and vaithyanathan, s. (2002). thumbs
up? sentiment classi   cation using machine learning tech-
niques. in emnlp 2002, pp. 79   86.
paolino,
two simple user
delighted a
4,
2017.
google-home-vs-alexa-56e26f69ac77.
parsons, t. (1990). events in the semantics of english. mit
press.
partee, b. h. (ed.). (1976). montague grammar. academic
press.
pasca, m. (2003). open-domain id53 from
large text collections. csli.
paszke, a., gross, s., chintala, s., chanan, g., yang, e.,
devito, z., lin, z., desmaison, a., antiga, l., and lerer,
a. (2017). automatic differentiation in pytorch. in nips-
w.

vs alexa:
that
female user.
jan
https://medium.com/startup-grind/

experience design gestures

google home

in medium.

(2017).

j.

534 bibliography

pearl, c. (2017). designing voice user interfaces: princi-
ples of conversational experiences. o   reilly.
pedersen, t. and bruce, r. (1997). distinguishing word
senses in untagged text. in emnlp 1997, providence, ri.
peng, n., poon, h., quirk, c., toutanova, k., and yih, w.-t.
(2017). cross-sentence n-ary id36 with graph
lstms. tacl, 5, 101   115.
penn, g. and kiparsky, p. (2012). on p  an. ini and the gen-
erative capacity of contextualized replacement systems. in
coling-12, pp. 943   950.
pennebaker, j. w., booth, r. j., and francis, m. e. (2007).
linguistic inquiry and word count: liwc 2007. austin,
tx.
pennebaker, j. w. and king, l. a. (1999). linguistic styles:
language use as an individual difference. journal of per-
sonality and social psychology, 77(6).
pennington, j., socher, r., and manning, c. d. (2014).
glove: global vectors for word representation. in emnlp
2014, pp. 1532   1543.
percival, w. k. (1976). on the historical source of immedi-
ate constituent analysis. in mccawley, j. d. (ed.), syntax
and semantics volume 7, notes from the linguistic under-
ground, pp. 229   242. academic press.
perrault, c. r. and allen, j. (1980). a plan-based analy-
sis of indirect speech acts. american journal of computa-
tional linguistics, 6(3-4), 167   182.
peterson, j. l. (1986). a note on undetected typing errors.
communications of the acm, 29(7), 633   637.
petrov, s., barrett, l., thibaux, r., and klein, d. (2006).
learning accurate, compact, and interpretable tree annota-
tion. in coling/acl 2006, sydney, australia, pp. 433   
440.
petrov, s., das, d., and mcdonald, r. (2012). a universal
part-of-speech tagset. in lrec-12.
petrov, s. and mcdonald, r. (2012). overview of the 2012
shared task on parsing the web. in notes of the first work-
shop on syntactic analysis of non-canonical language
(sancl), vol. 59.
philips, l. (1990). hanging on the metaphone. computer
language, 7(12).
phillips, a. v. (1960). a question-answering routine. tech.
rep. 16, mit ai lab.
picard, r. w. (1995). affective computing. tech. rep. 321,
mit media lab perceputal computing technical report.
revised november 26, 1995.
pieraccini, r., levin, e., and lee, c.-h. (1991). stochastic
representation of conceptual structure in the atis task. in
proceedings darpa speech and natural language work-
shop, paci   c grove, ca, pp. 121   124.
plutchik, r. (1962). the emotions: facts, theories, and a
new model. random house.
plutchik, r. (1980). a general psychoevolutionary theory
of emotion. in plutchik, r. and kellerman, h. (eds.), emo-
tion: theory, research, and experience, volume 1, pp. 3   
33. academic press.
polifroni, j., hirschman, l., seneff, s., and zue, v. w.
(1992). experiments in evaluating interactive spoken lan-
guage systems. in proceedings darpa speech and natural
language workshop, harriman, ny, pp. 28   33.
pollard, c. and sag, i. a. (1994). head-driven phrase
structure grammar. university of chicago press.
ponzetto, s. p. and navigli, r. (2010). knowledge-rich word
sense disambiguation rivaling supervised systems. in acl
2010, pp. 1522   1531.

j.,

j., casta  no,

porter, m. f. (1980). an algorithm for suf   x stripping. pro-
gram, 14(3), 130   127.
potts, c. (2011). on the negativity of negation. in li, n. and
lutz, d. (eds.), proceedings of semantics and linguistic
theory 20, pp. 636   659. clc publications, ithaca, ny.
pradhan, s., moschitti, a., xue, n., ng, h. t., bj  orkelund,
a., uryupina, o., zhang, y., and zhong, z. (2013). to-
wards robust linguistic analysis using ontonotes.
in
conll-13, pp. 143   152.
pradhan, s., ward, w., hacioglu, k., martin, j. h., and ju-
rafsky, d. (2005). id14 using different
syntactic views. in acl-05, ann arbor, mi.
purver, m. (2004). the theory and use of clari   cation re-
quests in dialogue. ph.d. thesis, university of london.
pustejovsky, j. (1995). the generative lexicon. mit press.
pustejovsky, j. and boguraev, b. (eds.). (1996). lexical
semantics: the problem of polysemy. oxford university
press.
pustejovsky,
ingria, r., saur    , r.,
gaizauskas, r., setzer, a., and katz, g. (2003a). timeml:
robust speci   cation of event and temporal expressions in
text. in proceedings of the 5th international workshop on
computational semantics (iwcs-5).
pustejovsky, j., hanks, p., saur    , r., see, a., gaizauskas,
r., setzer, a., radev, d., sundheim, b., day, d. s., ferro,
l., and lazo, m. (2003b). the timebank corpus.
in
proceedings of corpus linguistics 2003 conference, pp.
647   656. ucrel technical paper number 16.
pustejovsky, j., ingria, r., saur    , r., casta  no, j., littman, j.,
gaizauskas, r., setzer, a., katz, g., and mani, i. (2005).
the speci   cation language timeml, chap. 27. oxford.
qiu, g., liu, b., bu, j., and chen, c. (2009). expanding
domain sentiment lexicon through double propagation.. in
ijcai-09, pp. 1199   1204.
quillian, m. r. (1968). semantic memory. in minsky, m.
(ed.), semantic information processing, pp. 227   270. mit
press.
quillian, m. r. (1969). the teachable language comprehen-
der: a simulation program and theory of language. com-
munications of the acm, 12(8), 459   476.
quirk, r., greenbaum, s., leech, g., and svartvik, j.
(1985). a comprehensive grammar of the english lan-
guage. longman.
rabiner, l. r. (1989). a tutorial on id48
and selected applications in id103. proceed-
ings of the ieee, 77(2), 257   286.
rabiner, l. r. and juang, b. h. (1993). fundamentals of
id103. prentice hall.
radford, a. (1988). transformational grammar: a first
course. cambridge university press.
radford, a. (1997). syntactic theory and the structure of
english: a minimalist approach. cambridge university
press.
rajpurkar, p., jia, r., and liang, p. (2018). know what you
don   t know: unanswerable questions for squad. in acl
2018.
rajpurkar, p., zhang, j., lopyrev, k., and liang, p. (2016).
squad: 100,000+ questions for machine comprehension
of text. in emnlp 2016.
ramshaw, l. a. and marcus, m. p. (1995). text chunking
using transformation-based learning. in proceedings of the
3rd annual workshop on very large corpora, pp. 82   94.
ranganath, r., jurafsky, d., and mcfarland, d. a. (2013).
detecting friendly,    irtatious, awkward, and assertive
speech in speed-dates. computer speech and language,
27(1), 89   115.

raphael, b. (1968). sir: a computer program for seman-
tic information retrieval.
in minsky, m. (ed.), semantic
information processing, pp. 33   145. mit press.
rashkin, h., bell, e., choi, y., and volkova, s. (2017). mul-
tilingual connotation frames: a case study on social media
for targeted id31 and forecast. in acl 2017,
pp. 459   464.
rashkin, h., singh, s., and choi, y. (2016). connotation
frames: a data-driven investigation. in acl 2016, pp. 311   
321.
ratnaparkhi, a. (1996). a maximum id178 part-of-
speech tagger. in emnlp 1996, philadelphia, pa, pp. 133   
142.
ratnaparkhi, a. (1997). a linear observed time statisti-
cal parser based on maximum id178 models. in emnlp
1997, providence, ri, pp. 1   10.
ratnaparkhi, a., reynar, j. c., and roukos, s. (1994). a
maximum id178 model for prepositional phrase attach-
ment. in arpa human language technologies workshop,
plainsboro, n.j., pp. 250   255.
raviv, j. (1967). decision making in markov chains applied
to the problem of pattern recognition. ieee transactions
on id205, 13(4), 536   551.
raymond, c. and riccardi, g. (2007). generative and dis-
criminative algorithms for spoken language understanding.
in interspeech-07, pp. 1605   1608.
rehder, b., schreiner, m. e., wolfe, m. b. w., laham, d.,
landauer, t. k., and kintsch, w. (1998). using latent
semantic analysis to assess knowledge: some technical
considerations. discourse processes, 25(2-3), 337   354.
reichenbach, h. (1947). elements of symbolic logic.
macmillan, new york.
reichert, t. a., cohen, d. n., and wong, a. k. c. (1973).
an application of id205 to genetic mutations
and the matching of polypeptide sequences. journal of
theoretical biology, 42, 245   261.
resnik, p. (1992). probabilistic tree-adjoining grammar as
a framework for statistical natural language processing. in
coling-92, nantes, france, pp. 418   424.
resnik, p. (1993). semantic classes and syntactic ambigu-
ity. in proceedings of the workshop on human language
technology, pp. 278   283.
resnik, p. (1995). using information content to evaluate
semantic similarity in a taxanomy. in international joint
conference for arti   cial intelligence (ijcai-95), pp. 448   
453.
resnik, p. (1996). selectional constraints: an information-
theoretic model and its computational realization. cogni-
tion, 61, 127   159.
richardson, m., burges, c. j. c., and renshaw, e. (2013).
mctest: a challenge dataset for the open-domain machine
comprehension of text. in emnlp 2013, pp. 193   203.
riedel, s., yao, l., and mccallum, a. (2010). modeling
relations and their mentions without labeled text. in ma-
chine learning and knowledge discovery in databases,
pp. 148   163. springer.
riedel, s., yao, l., mccallum, a., and marlin, b. m. (2013).
id36 with id105 and universal
schemas. in naacl hlt 2013.
riesbeck, c. k. (1975). conceptual analysis.
in schank,
r. c. (ed.), conceptual information processing, pp. 83   
156. american elsevier, new york.
riezler, s., king, t. h., kaplan, r. m., crouch, r.,
maxwell iii, j. t., and johnson, m. (2002).
parsing

bibliography

535

the wall street journal using a lexical-functional gram-
mar and discriminative estimation techniques. in acl-02,
philadelphia, pa.
riloff, e. (1993). automatically constructing a dictionary
for information extraction tasks. in aaai-93, washington,
d.c., pp. 811   816.
riloff, e. (1996). automatically generating extraction pat-
terns from untagged text. in aaai-96, pp. 117   124.
riloff, e. and jones, r. (1999). learning dictionaries for
information extraction by multi-level id64.
in
aaai-99, pp. 474   479.
riloff, e. and schmelzenbach, m. (1998). an empirical ap-
proach to conceptual case frame acquisition. in proceed-
ings of the sixth workshop on very large corpora, mon-
treal, canada, pp. 49   56.
riloff, e. and shepherd, j. (1997). a corpus-based approach
for building semantic lexicons. in emnlp 1997.
riloff, e. and thelen, m. (2000). a rule-based question an-
swering system for reading comprehension tests. in pro-
ceedings of anlp/naacl workshop on reading compre-
hension tests, pp. 13   19.
riloff, e. and wiebe, j. (2003). learning extraction pat-
terns for subjective expressions. in emnlp 2003, sapporo,
japan.
ritter, a., cherry, c., and dolan, b. (2011). data-driven
response generation in social media.
in emnlp-11, pp.
583   593.
ritter, a., etzioni, o., and mausam (2010). a latent dirich-
let allocation method for selectional preferences. in acl
2010, pp. 424   434.
ritter, a., zettlemoyer, l., mausam, and etzioni, o. (2013).
modeling missing data in distant supervision for informa-
tion extraction.. tacl, 1, 367   378.
roark, b. (2001). probabilistic top-down parsing and lan-
guage modeling. computational linguistics, 27(2), 249   
276.
roark, b., saraclar, m., and collins, m. (2007). discrim-
inative id165 id38. computer speech &
language, 21(2), 373   392.
rohde, d. l. t., gonnerman, l. m., and plaut, d. c. (2006).
an improved model of semantic similarity based on lexical
co-occurrence. communications of the acm, 8, 627   633.
rooth, m., riezler, s., prescher, d., carroll, g., and beil,
f. (1999). inducing a semantically annotated lexicon via
em-based id91. in acl-99, college park, ma, pp.
104   111.
rosenblatt, f. (1958).
the id88: a probabilis-
tic model for information storage and organization in the
brain.. psychological review, 65(6), 386   408.
rosenfeld, r. (1996). a maximum id178 approach to
adaptive statistical id38. computer speech
and language, 10, 187   228.
rothe, s., ebert, s., and sch  utze, h. (2016). ultradense
id27s by orthogonal transformation.
in
naacl hlt 2016.
roy, n., pineau, j., and thrun, s. (2000). spoken dialog
management for robots. in acl-00, hong kong.
rubenstein, h. and goodenough, j. b. (1965). contex-
tual correlates of synonymy. communications of the acm,
8(10), 627   633.
rumelhart, d. e., hinton, g. e., and williams, r. j. (1986).
learning internal representations by error propagation. in
rumelhart, d. e. and mcclelland, j. l. (eds.), parallel
distributed processing, vol. 2, pp. 318   362. mit press.

536 bibliography

rumelhart, d. e. and mcclelland, j. l. (1986a). on learn-
ing the past tense of english verbs. in rumelhart, d. e. and
mcclelland, j. l. (eds.), parallel distributed processing,
vol. 2, pp. 216   271. mit press.
rumelhart, d. e. and mcclelland, j. l. (eds.). (1986b).
parallel distributed processing. mit press.
ruppenhofer, j., ellsworth, m., petruck, m. r. l., johnson,
c. r., baker, c. f., and scheffczyk, j. (2016). framenet
ii: extended theory and practice..
ruppenhofer, j., sporleder, c., morante, r., baker, c., and
palmer, m. (2010). semeval-2010 task 10: linking events
and their participants in discourse. in proceedings of the
5th international workshop on semantic evaluation, pp.
45   50.
russell, j. a. (1980). a circumplex model of affect. journal
of personality and social psychology, 39(6), 1161   1178.
russell, s. and norvig, p. (2002). arti   cial intelligence: a
modern approach (2nd ed.). prentice hall.
sacks, h., schegloff, e. a., and jefferson, g. (1974). a
simplest systematics for the organization of turn-taking for
conversation. language, 50(4), 696   735.
sag, i. a. and liberman, m. y. (1975). the intonational dis-
ambiguation of indirect speech acts. in cls-75, pp. 487   
498. university of chicago.
sag, i. a., wasow, t., and bender, e. m. (eds.). (2003). syn-
tactic theory: a formal introduction. csli publications,
stanford, ca.
sahami, m., dumais, s. t., heckerman, d., and horvitz, e.
(1998). a bayesian approach to    ltering junk e-mail. in
aaai workshop on learning for text categorization, pp.
98   105.
sakoe, h. and chiba, s. (1971). a id145
approach to continuous id103.
in proceed-
ings of the seventh international congress on acoustics,
budapest, vol. 3, pp. 65   69. akad  emiai kiad  o.
salomaa, a. (1969). probabilistic and weighted grammars.
information and control, 15, 529   544.
salton, g. (1971). the smart retrieval system: experi-
ments in automatic document processing. prentice hall.
sampson, g. (1987). alternative grammatical coding sys-
tems.
in garside, r., leech, g., and sampson, g.
(eds.), the computational analysis of english, pp. 165   
183. longman.
samuelsson, c. (1993). morphological tagging based en-
tirely on bayesian id136. in 9th nordic conference on
computational linguistics nodalida-93. stockholm.
sankoff, d. (1972). matching sequences under deletion-
insertion constraints. proceedings of the natural academy
of sciences of the u.s.a., 69, 4   6.
sankoff, d. and labov, w. (1979). on the uses of variable
rules. language in society, 8(2-3), 189   222.
sap, m., prasettio, m. c., holtzman, a., rashkin, h., and
choi, y. (2017). connotation frames of power and agency
in modern    lms. in emnlp 2017, pp. 2329   2334.
schabes, y. (1990). mathematical and computational as-
pects of lexicalized grammars. ph.d. thesis, university of
pennsylvania, philadelphia, pa.
schabes, y. (1992). stochastic lexicalized tree-adjoining
grammars. in coling-92, nantes, france, pp. 426   433.
schabes, y., abeill  e, a., and joshi, a. k. (1988). pars-
ing strategies with    lexicalized    grammars: applications to
id34s. in coling-88, budapest, pp.
578   583.

schank, r. c. (1972). conceptual dependency: a theory
of natural language processing. cognitive psychology, 3,
552   631.
schank, r. c. and abelson, r. p. (1975). scripts, plans, and
knowledge. in proceedings of ijcai-75, pp. 151   157.
schank, r. c. and abelson, r. p. (1977). scripts, plans,
goals and understanding. lawrence erlbaum.
schegloff, e. a. (1968). sequencing in conversational open-
ings. american anthropologist, 70, 1075   1095.
schegloff, e. a. (1972). notes on a conversational practice:
formulating place. in sudnow, d. (ed.), studies in social
interaction, new york. free press.
schegloff, e. a. (1982). discourse as an interactional
achievement: some uses of    uh huh    and other things that
come between sentences. in tannen, d. (ed.), analyzing
discourse: text and talk, pp. 71   93. georgetown univer-
sity press, washington, d.c.
scherer, k. r. (2000). psychological models of emotion.
in borod, j. c. (ed.), the neuropsychology of emotion, pp.
137   162. oxford.
schone, p. and jurafsky, d. (2000). knowlege-free induction
of morphology using latent semantic analysis. in conll-
00.
schone, p. and jurafsky, d. (2001). knowledge-free induc-
tion of in   ectional morphologies. in naacl 2001.
  uber die bausteine der mathe-
sch  onk   nkel, m. (1924).
matischen logik. mathematische annalen, 92, 305   316.
english translation appears in from frege to g  odel: a
source book in mathematical logic, harvard university
press, 1967.
sch  utze, h. (1992a). context space. in goldman, r. (ed.),
proceedings of the 1992 aaai fall symposium on proba-
bilistic approaches to natural language.
sch  utze, h. (1992b). dimensions of meaning. in proceed-
ings of supercomputing    92, pp. 787   796. ieee press.
sch  utze, h. (1997a). ambiguity resolution in language
learning     computational and cognitive models. csli,
stanford, ca.
sch  utze, h. (1997b). ambiguity resolution in language
learning: computational and cognitive models. csli
publications, stanford, ca.
sch  utze, h. (1998). automatic word sense discrimination.
computational linguistics, 24(1), 97   124.
sch  utze, h., hull, d. a., and pedersen, j. (1995). a com-
parison of classi   ers and id194s for the
routing problem. in sigir-95, pp. 229   237.
sch  utze, h. and pedersen, j. (1993). a vector model for
syntagmatic and paradigmatic relatedness. in proceedings
of the 9th annual conference of the uw centre for the new
oed and text research, pp. 104   113.
sch  utze, h. and singer, y. (1994). part-of-speech tagging
using a variable memory markov model. in acl-94, las
cruces, nm, pp. 181   187.
schwartz, h. a., eichstaedt, j. c., kern, m. l., dziurzyn-
ski, l., ramones, s. m., agrawal, m., shah, a., kosin-
ski, m., stillwell, d., seligman, m. e. p., and ungar, l. h.
(2013). personality, gender, and age in the language of so-
cial media: the open-vocabulary approach. plos one, 8(9),
e73791.
schwartz, r. and chow, y.-l. (1990). the n-best algo-
rithm: an ef   cient and exact procedure for    nding the n
most likely sentence hypotheses. in icassp-90, vol. 1, pp.
81   84.
schwenk, h. (2007). continuous space language models.
computer speech & language, 21(3), 492   518.

scott, m. and shillcock, r. (2003). eye movements re-
veal the on-line computation of lexical probabilities during
reading. psychological science, 14(6), 648   652.
s  eaghdha, d. o. (2010). latent variable models of selec-
tional preference. in acl 2010, pp. 435   444.
seddah, d., tsarfaty, r., k  ubler, s., candito, m., choi,
j. d., farkas, r., foster, j., goenaga, i., gojenola, k.,
goldberg, y., green, s., habash, n., kuhlmann, m., maier,
w., nivre, j., przepi  orkowski, a., roth, r., seeker, w.,
versley, y., vincze, v., woli  nski, m., wr  oblewska, a.,
and villemonte de la cl  ergerie, e. (2013). overview of
the spmrl 2013 shared task: cross-framework evalua-
tion of parsing morphologically rich languages.
in pro-
ceedings of the 4th workshop on statistical parsing of
morphologically-rich languages.
sekine, s. and collins, m. (1997). the evalb software.
http://cs.nyu.edu/cs/projects/proteus/evalb.
sennrich, r., haddow, b., and birch, a. (2016). neural ma-
chine translation of rare words with subword units. in acl
2016.
seo, m., kembhavi, a., farhadi, a., and hajishirzi, h.
(2017). bidirectional attention    ow for machine compre-
hension. in iclr 2017.
serban, i. v., lowe, r. t., charlin, l., and pineau, j. (2017).
a survey of available corpora for building data-driven dia-
logue systems.. arxiv preprint arxiv:1512.05742.
sgall, p., haji  cov  a, e., and panevova, j. (1986). the mean-
ing of the sentence in its pragmatic aspects. reidel.
shang, l., lu, z., and li, h. (2015). neural responding ma-
chine for short-text conversation. in acl 2015, pp. 1577   
1586.
shannon, c. e. (1948). a mathematical theory of commu-
nication. bell system technical journal, 27(3), 379   423.
continued in the following volume.
shannon, c. e. (1951). prediction and id178 of printed
english. bell system technical journal, 30, 50   64.
sheil, b. a. (1976). observations on context free parsing.
smil: statistical methods in linguistics, 1, 71   109.
shriberg, e., bates, r., taylor, p., stolcke, a., jurafsky, d.,
ries, k., coccaro, n., martin, r., meteer, m., and van ess-
dykema, c. (1998). can id144 aid the automatic classi-
   cation of dialog acts in conversational speech?. language
and speech (special issue on id144 and conversation),
41(3-4), 439   487.
simmons, r. f. (1965). answering english questions by
computer: a survey. communications of the acm, 8(1),
53   70.
simmons, r. f. (1973). semantic networks: their com-
putation and use for understanding english sentences. in
schank, r. c. and colby, k. m. (eds.), computer models
of thought and language, pp. 61   113. w.h. freeman and
co.
simmons, r. f., klein, s., and mcconlogue, k. (1964). in-
dexing and dependency logic for answering english ques-
tions. american documentation, 15(3), 196   204.
simons, g. f. and fennig, c. d. (2018). ethnologue: lan-
guages of the world, twenty-   rst edition.. dallas, texas.
sil international.
singh, s. p., litman, d. j., kearns, m., and walker, m. a.
(2002). optimizing dialogue management with reinforce-
ment learning: experiments with the njfun system. jour-
nal of arti   cial intelligence research (jair), 16, 105   133.
sleator, d. and temperley, d. (1993). parsing english with
a link grammar. in iwpt-93.

bibliography

537

small, s. l., cottrell, g. w., and tanenhaus, m. (eds.).
(1988). lexical ambiguity resolution. morgan kaufman.
small, s. l. and rieger, c. (1982). parsing and compre-
hending with word experts. in lehnert, w. g. and ringle,
m. h. (eds.), strategies for natural language processing,
pp. 89   147. lawrence erlbaum.
smith, d. a. and eisner, j. (2007). id64 feature-
rich dependency parsers with entropic priors. in emnlp/-
conll 2007, prague, pp. 667   677.
smith, n. a. and eisner, j. (2005). guiding unsupervised
grammar induction using contrastive estimation. in ijcai
workshop on grammatical id136 applications, edin-
burgh, pp. 73   82.
smith, v. l. and clark, h. h. (1993). on the course of an-
swering questions. journal of memory and language, 32,
25   38.
smolensky, p. (1988). on the proper treatment of connec-
tionism. behavioral and brain sciences, 11(1), 1   23.
smolensky, p. (1990). tensor product variable binding and
the representation of symbolic structures in connectionist
systems. arti   cial intelligence, 46(1-2), 159   216.
snow, r., jurafsky, d., and ng, a. y. (2005). learning syn-
tactic patterns for automatic hypernym discovery. in saul,
l. k., weiss, y., and bottou, l. (eds.), nips 17, pp. 1297   
1304. mit press.
snow, r., prakash, s., jurafsky, d., and ng, a. y. (2007).
learning to merge word senses. in emnlp/conll 2007,
pp. 1005   1014.
socher, r., huval, b., manning, c. d., and ng, a. y. (2012).
semantic compositionality through recursive matrix-vector
spaces. in emnlp 2012, pp. 1201   1211.
soderland, s., fisher, d., aseltine, j., and lehnert, w. g.
(1995). crystal: inducing a conceptual dictionary. in
ijcai-95, montreal, pp. 1134   1142.
s  gaard, a. (2010). simple semi-supervised training of
part-of-speech taggers. in acl 2010, pp. 205   208.
s  gaard, a., johannsen, a., plank, b., hovy, d., and
alonso, h. m. (2014). what   s in a p-value in nlp?. in
conll-14.
solorio, t., blair, e., maharjan, s., bethard, s., diab, m.,
ghoneim, m., hawwari, a., alghamdi, f., hirschberg, j.,
chang, a., and fung, p. (2014). overview for the    rst
shared task on language identi   cation in code-switched
data. in proceedings of the first workshop on computa-
tional approaches to code switching, pp. 62   72.
sordoni, a., galley, m., auli, m., brockett, c., ji, y.,
mitchell, m., nie, j.-y., gao, j., and dolan, b. (2015). a
neural network approach to context-sensitive generation of
conversational responses. in naacl hlt 2015, pp. 196   
205.
sparck jones, k. (1972). a statistical interpretation of term
speci   city and its application in retrieval. journal of doc-
umentation, 28(1), 11   21.
sparck jones, k. (1986). synonymy and semantic classi   -
cation. edinburgh university press, edinburgh. republi-
cation of 1964 phd thesis.
spitkovsky, v. i. and chang, a. x. (2012). a cross-lingual
dictionary for english wikipedia concepts.
in lrec-12,
istanbul, turkey.
srivastava, n., hinton, g. e., krizhevsky, a., sutskever, i.,
and salakhutdinov, r. r. (2014). dropout: a simple way
to prevent neural networks from over   tting.. journal of
machine learning research, 15(1), 1929   1958.
stalnaker, r. c. (1978). assertion. in cole, p. (ed.), prag-
matics: syntax and semantics volume 9, pp. 315   332. aca-
demic press.

538 bibliography

stamatatos, e. (2009). a survey of modern authorship at-
tribution methods. jasist, 60(3), 538   556.
steedman, m. (1989). constituency and coordination in a
combinatory grammar. in baltin, m. r. and kroch, a. s.
(eds.), alternative conceptions of phrase structure, pp.
201   231. university of chicago.
steedman, m. (1996). surface structure and interpretation.
mit press. linguistic inquiry monograph, 30.
steedman, m. (2000). the syntactic process. the mit
press.
stetina, j. and nagao, m. (1997). corpus based pp attach-
ment ambiguity resolution with a semantic dictionary. in
zhou, j. and church, k. w. (eds.), proceedings of the fifth
workshop on very large corpora, beijing, china, pp. 66   
80.
stifelman, l. j., arons, b., schmandt, c., and hulteen, e. a.
(1993). voicenotes: a speech interface for a hand-held
voice notetaker. in human factors in computing systems:
interchi    93 conference proceedings, pp. 179   186.
stolcke, a. (1995). an ef   cient probabilistic context-free
parsing algorithm that computes pre   x probabilities. com-
putational linguistics, 21(2), 165   202.
stolcke, a. (1998). id178-based pruning of backoff lan-
guage models. in proc. darpa broadcast news transcrip-
tion and understanding workshop, lansdowne, va, pp.
270   274.
stolcke, a. (2002). srilm     an extensible language mod-
eling toolkit. in icslp-02, denver, co.
stolcke, a., ries, k., coccaro, n., shriberg, e., bates, r.,
jurafsky, d., taylor, p., martin, r., meteer, m., and van
ess-dykema, c. (2000). dialogue act modeling for au-
tomatic tagging and recognition of conversational speech.
computational linguistics, 26(3), 339   371.
stolz, w. s., tannenbaum, p. h., and carstensen, f. v.
(1965). a stochastic approach to the grammatical coding
of english. communications of the acm, 8(6), 399   405.
stone, p., dunphry, d., smith, m., and ogilvie, d. (1966).
the general inquirer: a computer approach to content
analysis. cambridge, ma: mit press.
stoyanchev, s. and johnston, m. (2015). localized error
detection for targeted clari   cation in a virtual assistant. in
icassp-15, pp. 5241   5245.
stoyanchev, s., liu, a., and hirschberg, j. (2013). mod-
elling human clari   cation strategies.
in sigdial 2013,
pp. 137   141.
stoyanchev, s., liu, a., and hirschberg, j. (2014). towards
natural clari   cation questions in dialogue systems. in aisb
symposium on questions, discourse and dialogue.
str  otgen, j. and gertz, m. (2013). multilingual and cross-
domain temporal tagging. language resources and eval-
uation, 47(2), 269   298.
suendermann, d., evanini, k., liscombe, j., hunter, p.,
dayanidhi, k., and pieraccini, r. (2009). from rule-based
to statistical grammars: continuous improvement of large-
scale spoken id71.
in icassp-09, pp. 4713   
4716.
sundheim, b. (ed.). (1991). proceedings of muc-3.
sundheim, b. (ed.). (1992). proceedings of muc-4.
sundheim, b. (ed.). (1993). proceedings of muc-5, balti-
more, md.
sundheim, b. (ed.). (1995). proceedings of muc-6.
surdeanu, m. (2013). overview of the tac2013 knowl-
edge base population evaluation: english slot    lling and
temporal slot    lling. in tac-13.

surdeanu, m., harabagiu, s., williams, j., and aarseth, p.
(2003). using predicate-argument structures for informa-
tion extraction. in acl-03, pp. 8   15.
surdeanu, m., johansson, r., meyers, a., m`arquez, l., and
nivre, j. (2008a). the conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. in conll-08,
pp. 159   177.
surdeanu, m., johansson, r., meyers, a., m`arquez, l., and
nivre, j. (2008b). the conll-2008 shared task on joint pars-
ing of syntactic and semantic dependencies. in conll-08,
pp. 159   177.
swerts, m., litman, d. j., and hirschberg, j. (2000). cor-
rections in spoken dialogue systems. in icslp-00, beijing,
china.
swier, r. and stevenson, s. (2004). unsupervised semantic
role labelling. in emnlp 2004, pp. 95   102.
switzer, p. (1965).
vector images in document re-
trieval.
in stevens, m. e., giuliano, v. e., and
heilprin, l. b.
(eds.), statistical association meth-
ods for mechanized documentation. symposium pro-
ceedings. washington, d.c., usa, march 17, 1964,
pp. 163   171. https://nvlpubs.nist.gov/nistpubs/
legacy/mp/nbsmiscellaneouspub269.pdf.
talbot, d. and osborne, m. (2007). smoothed bloom fil-
ter language models: tera-scale lms on the cheap. in
emnlp/conll 2007, pp. 468   476.
talmor, a. and berant, j. (2018). the web as a knowledge-
base for answering complex questions.
in naacl hlt
2018.
tannen, d. (1979). what   s in a frame? surface evidence for
underlying expectations. in freedle, r. (ed.), new direc-
tions in discourse processing, pp. 137   181. ablex.
taskar, b., klein, d., collins, m., koller, d., and manning,
c. d. (2004). max-margin parsing. in emnlp 2004, pp.
1   8.
ter meulen, a. (1995). representing time in natural lan-
guage. mit press.
tesni`ere, l. (1959).
brairie c. klincksieck, paris.
thede, s. m. and harper, m. p. (1999). a second-order hid-
den markov model for part-of-speech tagging. in acl-99,
college park, ma, pp. 175   182.
thompson, k. (1968). regular expression search algorithm.
communications of the acm, 11(6), 419   422.
tibshirani, r. j. (1996). regression shrinkage and selec-
tion via the lasso. journal of the royal statistical society.
series b (methodological), 58(1), 267   288.
titov, i. and henderson, j. (2006). loss minimization in
parse reranking. in emnlp 2006.
titov, i. and khoddam, e. (2014). unsupervised induction
of semantic roles within a reconstruction-error minimiza-
tion framework. in naacl hlt 2015.
titov, i. and klementiev, a. (2012). a bayesian approach
to unsupervised semantic role induction. in eacl-12, pp.
12   22.
tomkins, s. s. (1962). affect, imagery, consciousness: vol.
i. the positive affects. springer.
toutanova, k., klein, d., manning, c. d., and singer, y.
(2003). feature-rich part-of-speech tagging with a cyclic
dependency network. in hlt-naacl-03.
toutanova, k., manning, c. d., flickinger, d., and oepen,
s. (2005). stochastic hpsg parse disambiguation using
the redwoods corpus. research on language & compu-
tation, 3(1), 83   105.

  el  ements de syntaxe structurale. li-

toutanova, k. and moore, r. c. (2002). pronunciation
modeling for improved id147.
in acl-02,
philadelphia, pa, pp. 144   151.
tseng, h., chang, p.-c., andrew, g., jurafsky, d., and man-
ning, c. d. (2005a). conditional random    eld word seg-
menter. in proceedings of the fourth sighan workshop
on chinese language processing.
tseng, h., jurafsky, d., and manning, c. d. (2005b). mor-
phological features help id52 of unknown words
across language varieties.
in proceedings of the 4th
sighan workshop on chinese language processing.
turian, j., ratinov, l., and bengio, y. (2010). word
representations: a simple and general method for semi-
supervised learning. in acl 2010, pp. 384   394.
turney, p. d. (2002). thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classi   cation of
reviews. in acl-02.
turney, p. d. and littman, m. (2003). measuring praise and
criticism: id136 of semantic orientation from associa-
tion. acm transactions on information systems (tois),
21, 315   346.
uzzaman, n., llorens, h., derczynski, l., allen, j., ver-
hagen, m., and pustejovsky, j. (2013). semeval-2013 task
1: tempeval-3: evaluating time expressions, events, and
temporal relations. in semeval-13, pp. 1   9.
van benthem, j. and ter meulen, a. (eds.). (1997). hand-
book of logic and language. mit press.
van der maaten, l. and hinton, g. e. (2008). visualiz-
ing high-dimensional data using id167. journal of machine
learning research, 9, 2579   2605.
van rijsbergen, c. j. (1975). information retrieval. but-
terworths.
van valin, jr., r. d. and la polla, r. (1997). syntax: struc-
ture, meaning, and function. cambridge university press.
vanlehn, k., jordan, p. w., ros  e, c., bhembe, d., b  ottner,
m., gaydos, a., makatchev, m., pappuswamy, u., ringen-
berg, m., roque, a., siler, s., srivastava, r., and wilson,
r. (2002). the architecture of why2-atlas: a coach for
qualitative physics essay writing. in proc. intelligent tu-
toring systems.
vasilescu, f., langlais, p., and lapalme, g. (2004). evaluat-
ing variants of the lesk approach for disambiguating words.
in lrec-04, lisbon, portugal, pp. 633   636. elra.
veblen, t. (1899). theory of the leisure class. macmillan
company, new york.
velikovich, l., blair-goldensohn, s., hannan, k., and mc-
donald, r. (2010). the viability of web-derived polarity
lexicons. in naacl hlt 2010, pp. 777   785.
vendler, z. (1967). linguistics in philosophy. cornell uni-
versity press, ithaca, ny.
verhagen, m., gaizauskas, r., schilder, f., hepple, m.,
moszkowicz, j., and pustejovsky, j. (2009). the tempeval
challenge: identifying temporal relations in text. language
resources and evaluation, 43(2), 161   179.
verhagen, m., mani, i., sauri, r., knippen, r., jang, s. b.,
littman, j., rumshisky, a., phillips, j., and pustejovsky,
j. (2005). automating temporal annotation with tarsqi. in
acl-05, pp. 81   84.
vintsyuk, t. k. (1968). speech discrimination by dynamic
programming. cybernetics, 4(1), 52   57. russian kiber-
netika 4(1):81-88. 1968.
vinyals, o. and le, q. (2015). a neural conversational
model. in proceedings of icml deep learning workshop,
lille, france.

bibliography

539

viterbi, a. j. (1967). error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. ieee
transactions on id205, it-13(2), 260   269.
voutilainen, a. (1995). id60. in
karlsson, f., voutilainen, a., heikkil  a, j., and anttila,
a. (eds.), constraint grammar: a language-independent
system for parsing unrestricted text, pp. 165   284. mouton
de gruyter.
voutilainen, a. (1999). handcrafted rules.
in van hal-
teren, h. (ed.), syntactic wordclass tagging, pp. 217   246.
kluwer.
wade, e., shriberg, e., and price, p. j. (1992). user behav-
iors affecting id103. in icslp-92, pp. 995   
998.
wagner, r. a. and fischer, m. j. (1974). the string-to-
string correction problem. journal of the association for
computing machinery, 21, 168   173.
walker, m. a. (2000). an application of reinforcement
learning to dialogue strategy selection in a spoken dialogue
system for email.
journal of arti   cial intelligence re-
search, 12, 387   416.
walker, m. a., fromer, j. c., and narayanan, s. s. (1998).
learning optimal dialogue strategies: a case study of a
spoken dialogue agent for email.
in coling/acl-98,
montreal, canada, pp. 1345   1351.
walker, m. a., kamm, c. a., and litman, d. j. (2001).
towards developing general models of usability with par-
adise. natural language engineering: special issue on
best practice in spoken dialogue systems, 6(3), 363   377.
walker, m. a. and whittaker, s. (1990). mixed initiative in
dialogue: an investigation into discourse segmentation. in
acl-90, pittsburgh, pa, pp. 70   78.
wang, h., lu, z., li, h., and chen, e. (2013). a dataset for
research on short-text conversations.. in emnlp 2013, pp.
935   945.
wang, s. and manning, c. d. (2012). baselines and bi-
grams: simple, good sentiment and topic classi   cation. in
acl 2012, pp. 90   94.
ward, n. and tsukahara, w. (2000). prosodic features
which cue back-channel feedback in english and japanese.
journal of pragmatics, 32, 1177   1207.
ward, w. and issar, s. (1994). recent improvements
in the cmu spoken language understanding system.
in
arpa human language technologies workshop, plains-
boro, n.j.
warriner, a. b., kuperman, v., and brysbaert, m. (2013).
norms of valence, arousal, and dominance for 13,915 en-
glish lemmas. behavior research methods, 45(4), 1191   
1207.
weaver, w. (1949/1955). translation. in locke, w. n. and
boothe, a. d. (eds.), machine translation of languages,
pp. 15   23. mit press. reprinted from a memorandum
written by weaver in 1949.
weinschenk, s. and barker, d. t. (2000). designing effec-
tive speech interfaces. wiley.
weischedel, r., hovy, e. h., marcus, m. p., palmer, m.,
belvin, r., pradhan, s., ramshaw, l. a., and xue, n.
(2011). ontonotes: a large training corpus for enhanced
processing.
in joseph olive, caitlin christianson, j. m.
(ed.), handbook of natural language processing and ma-
chine translation: darpa global automatic language
exploitation, pp. 54   63. springer.
weischedel, r., meteer, m., schwartz, r., ramshaw, l. a.,
and palmucci, j. (1993). coping with ambiguity and un-
known words through probabilistic models. computational
linguistics, 19(2), 359   382.

540 bibliography

weizenbaum, j. (1966). eliza     a computer program for
the study of natural language communication between man
and machine. communications of the acm, 9(1), 36   45.
weizenbaum, j. (1976). computer power and human rea-
son: from judgement to calculation. w.h. freeman and
company.
wen, t.-h., ga  si  c, m., kim, d., mrk  si  c, n., su, p.-h.,
vandyke, d., and young, s. j. (2015a). stochastic lan-
guage generation in dialogue using recurrent neural net-
works with convolutional sentence reranking. in sigdial
2015, pp. 275      284.
wen, t.-h., ga  si  c, m., mrk  si  c, n., su, p.-h., vandyke, d.,
and young, s. j. (2015b). semantically conditioned lstm-
based id86 for spoken dialogue sys-
tems. in emnlp 2015.
whitelaw, c., hutchinson, b., chung, g. y., and el-
lis, g. (2009). using the web for language independent
spellchecking and autocorrection. in emnlp-09, pp. 890   
899.
widrow, b. and hoff, m. e. (1960). adaptive switching
circuits. in ire wescon convention record, vol. 4, pp.
96   104.
wiebe, j. (1994). tracking point of view in narrative. com-
putational linguistics, 20(2), 233   287.
wiebe, j. (2000). learning subjective adjectives from cor-
pora. in aaai-00, austin, tx, pp. 735   740.
wiebe, j., bruce, r. f., and o   hara, t. p. (1999). devel-
opment and use of a gold-standard data set for subjectivity
classi   cations. in acl-99, pp. 246   253.
wiebe, j., wilson, t., and cardie, c. (2005). annotating ex-
pressions of opinions and emotions in language. language
resources and evaluation, 39(2-3), 165   210.
wierzbicka, a. (1992). semantics, culture, and cognition:
university human concepts in culture-speci   c con   gura-
tions. oxford university press.
wierzbicka, a. (1996). semantics: primes and universals.
oxford university press.
wilcox-o   hearn, l. a. (2014). detection is the central
problem in real-word id147. http://arxiv.
org/abs/1408.3153.
wilcox-o   hearn, l. a., hirst, g., and budanitsky, a.
(2008). real-word id147 with trigrams: a
reconsideration of the mays, damerau, and mercer model.
in cicling-2008, pp. 605   616.
wilensky, r. (1983). planning and understanding: a
computational approach to human reasoning. addison-
wesley.
wilks, y. (1973). an arti   cial intelligence approach to ma-
chine translation. in schank, r. c. and colby, k. m. (eds.),
computer models of thought and language, pp. 114   151.
w.h. freeman.
wilks, y. (1975a). an intelligent analyzer and understander
of english. communications of the acm, 18(5), 264   274.
wilks, y. (1975b). preference semantics. in keenan, e. l.
(ed.), the formal semantics of natural language, pp.
329   350. cambridge univ. press.
wilks, y. (1975c). a preferential, pattern-seeking, seman-
tics for natural language id136. arti   cial intelligence,
6(1), 53   74.
williams, j. d., raux, a., and henderson, m. (2016). the
dialog state tracking challenge series: a review. dialogue
& discourse, 7(3), 4   33.
williams, j. d. and young, s. j. (2007). partially observ-
able id100 for spoken id71.
computer speech and language, 21(1), 393   422.

philosophical investigations.

wilson, t., wiebe, j., and hoffmann, p. (2005). recogniz-
ing contextual polarity in phrase-level id31.
in hlt-emnlp-05, pp. 347   354.
winkler, w. e. (2006). overview of record linkage and cur-
rent research directions. tech. rep., statistical research
division, u.s. census bureau.
winograd, t. (1972). understanding natural language.
academic press.
winston, p. h. (1977). arti   cial intelligence. addison wes-
ley.
witten, i. h. and bell, t. c. (1991). the zero-frequency
problem: estimating the probabilities of novel events in
adaptive text compression. ieee transactions on informa-
tion theory, 37(4), 1085   1094.
witten, i. h. and frank, e. (2005). data mining: practical
machine learning tools and techniques (2nd ed.). mor-
gan kaufmann.
wittgenstein, l. (1953).
(translated by anscombe, g.e.m.). blackwell.
woods, w. a. (1967). semantics for a question-answering
system. ph.d. thesis, harvard university.
woods, w. a. (1973). progress in natural language under-
standing. in proceedings of afips national conference,
pp. 441   450.
woods, w. a. (1975). what   s in a link: foundations for
semantic networks. in bobrow, d. g. and collins, a. m.
(eds.), representation and understanding: studies in cog-
nitive science, pp. 35   82. academic press.
woods, w. a. (1978). semantics and quanti   cation in natu-
ral language id53. in yovits, m. (ed.), ad-
vances in computers, pp. 2   64. academic.
woods, w. a., kaplan, r. m., and nash-webber, b. l.
(1972). the lunar sciences natural language information
system: final report. tech. rep. 2378, bbn.
woodsend, k. and lapata, m. (2015). distributed represen-
tations for unsupervised id14. in emnlp
2015, pp. 2482   2491.
wu, f. and weld, d. s. (2007). autonomously semantifying
wikipedia. in cikm-07, pp. 41   50.
wu, f. and weld, d. s. (2010). id10
using wikipedia. in acl 2010, pp. 118   127.
wu, z. and palmer, m. (1994). verb semantics and lexical
selection. in acl-94, las cruces, nm, pp. 133   138.
wundt, w. (1900). v  olkerpsychologie: eine untersuchung
der entwicklungsgesetze von sprache, mythus, und sitte.
w. engelmann, leipzig. band ii: die sprache, zweiter
teil.
xia, f. and palmer, m. (2001). converting dependency struc-
tures to phrase structures. in hlt-01, san diego, pp. 1   5.
xue, n. and palmer, m. (2004). calibrating features for se-
mantic role labeling. in emnlp 2004.
yamada, h. and matsumoto, y. (2003). statistical depen-
dency analysis with support vector machines.
in noord,
g. v. (ed.), iwpt-03, pp. 195   206.
yan, z., duan, n., bao, j.-w., chen, p., zhou, m., li, z.,
and zhou, j. (2016). docchat: an information retrieval ap-
proach for chatbot engines using unstructured documents.
in acl 2016.
yang, y., yih, w.-t., and meek, c. (2015). wikiqa: a
challenge dataset for open-domain id53. in
emnlp 2015.
yang, y. and pedersen, j. (1997). a comparative study on
feature selection in text categorization. in icml, pp. 412   
420.

yankelovich, n., levow, g.-a., and marx, m. (1995). de-
signing speechacts: issues in speech user interfaces.
in
human factors in computing systems: chi    95 confer-
ence proceedings, denver, co, pp. 369   376.
yarowsky, d. (1995). unsupervised word sense disam-
biguation rivaling supervised methods. in acl-95, cam-
bridge, ma, pp. 189   196.
yasseri, t., kornai, a., and kert  esz, j. (2012). a practical
approach to language complexity: a wikipedia case study.
plos one, 7(11).
yih, w.-t., richardson, m., meek, c., chang, m.-w., and
suh, j. (2016). the value of semantic parse labeling for
knowledge base id53.
in acl 2016, pp.
201   206.
yngve, v. h. (1955). syntax and the problem of multiple
meaning. in locke, w. n. and booth, a. d. (eds.), ma-
chine translation of languages, pp. 208   226. mit press.
yngve, v. h. (1970). on getting a word in edgewise.
in
cls-70, pp. 567   577. university of chicago.
young, s. j., ga  si  c, m., keizer, s., mairesse, f., schatz-
mann, j., thomson, b., and yu, k. (2010). the hid-
den information state model: a practical framework for
pomdp-based spoken dialogue management. computer
speech & language, 24(2), 150   174.
younger, d. h. (1967). recognition and parsing of context-
free languages in time n3.
information and control, 10,
189   208.
yuret, d. (1998). discovery of linguistic relations using
lexical attraction. ph.d. thesis, mit.
yuret, d. (2004). some experiments with a naive bayes
wsd system. in senseval-3: 3rd international workshop
on the evaluation of systems for the semantic analysis of
text.
zapirain, b., agirre, e., m`arquez, l., and surdeanu, m.
(2013). selectional preferences for semantic role classi   -
cation. computational linguistics, 39(3), 631   663.
zavrel, j. and daelemans, w. (1997). memory-based learn-
ing: using similarity for smoothing.
in acl/eacl-97,
madrid, spain, pp. 436   443.
zelle, j. m. and mooney, r. j. (1996). learning to parse
database queries using inductive logic programming.
in
aaai-96, pp. 1050   1055.
zeman, d. (2008). reusable tagset conversion using tagset
drivers.. in lrec-08.
zeman, d., popel, m., straka, m., haji  c, j., nivre, j., gin-
ter, f., luotolahti, j., pyysalo, s., petrov, s., potthast, m.,
tyers, f. m., badmaeva, e., gokirmak, m., nedoluzhko,
a., cinkov  a, s., hajic, jr., j., hlav  acov  a, j., kettnerov  a,
v., uresov  a, z., kanerva, j., ojala, s., missil  a, a., man-
ning, c. d., schuster, s., reddy, s., taji, d., habash,
n., leung, h., de marneffe, m.-c., sanguinetti, m., simi,
m., kanayama, h., de paiva, v., droganova, k., alonso,
h. m., c     oltekin, c   ., sulubacak, u., uszkoreit, h., macke-
tanz, v., burchardt, a., harris, k., marheinecke, k., rehm,
g., kayadelen, t., attia, m., el-kahky, a., yu, z., pitler,
e., lertpradit, s., mandl, m., kirchner, j., alcalde, h. f.,
strnadov  a, j., banerjee, e., manurung, r., stella, a., shi-
mada, a., kwak, s., mendonc  a, g., lando, t., nitisaroj,
r., and li, j. (2017). conll 2017 shared task: multilin-
gual parsing from raw text to universal dependencies. in
proceedings of the conll 2017 shared task: multilingual
parsing from raw text to universal dependencies, van-
couver, canada, august 3-4, 2017, pp. 1   19.
zettlemoyer, l. and collins, m. (2005). learning to map
sentences to logical form: structured classi   cation with
probabilistic categorial grammars.
in uncertainty in ar-
ti   cial intelligence, uai   05, pp. 658   666.

bibliography

541

zhang, y. and clark, s. (2008). a tale of two parsers: inves-
tigating and combining graph-based and transition-based
id33 using beam-search. in emnlp-08, pp.
562   571.

zhang, y. and nivre, j. (2011). transition-based depen-
dency parsing with rich non-local features. in acl 2011,
pp. 188   193.

zhao, h., chen, w., kit, c., and zhou, g. (2009). mul-
tilingual dependency learning: a huge feature engineering
method to semantic id33. in conll-09, pp.
55   60.

zhao, j., wang, t., yatskar, m., ordonez, v., and chang, k.-
w. (2017). men also like shopping: reducing gender bias
ampli   cation using corpus-level constraints.
in emnlp
2017.

zhong, z. and ng, h. t. (2010). it makes sense: a wide-
coverage id51 system for free text.
in acl 2010, pp. 78   83.

zhou, d., bousquet, o., lal, t. n., weston, j., and
sch  olkopf, b. (2004). learning with local and global con-
sistency. in nips 2004.

zhou, g., su, j., zhang, j., and zhang, m. (2005). exploring
various knowledge in id36. in acl-05, ann
arbor, mi, pp. 427   434.

zhou, j. and xu, w. (2015). end-to-end learning of seman-
tic role labeling using recurrent neural networks. in acl
2015, pp. 1127   1137.

zhu, x. and ghahramani, z. (2002). learning from la-
beled and unlabeled data with label propagation. tech. rep.
cmu-cald-02, cmu.

zhu, x., ghahramani, z., and lafferty, j. (2003). semi-
supervised learning using gaussian    elds and harmonic
functions. in icml 2003, pp. 912   919.

zue, v. w., glass, j., goodine, d., leung, h., phillips, m.,
polifroni, j., and seneff, s. (1989). preliminary evalua-
tion of the voyager spoken language system.
in pro-
ceedings darpa speech and natural language workshop,
cape cod, ma, pp. 160   167.

zwicky, a. and sadock, j. m. (1975). ambiguity tests and
how to fail them. in kimball, j. (ed.), syntax and seman-
tics 4, pp. 1   36. academic press.

  sev  c    kov  a, m., 274, 293
  st  ep  anek, j., 274, 293

aarseth, p., 375
abadi, m., 145
abeill  e, a., 267, 268
abelson, r. p., 351, 362,

375

abney, s. p., 174, 235, 263,
264, 267, 268, 514

adriaans, p., 268
agarwal, a., 145
aggarwal, c. c., 80
agichtein, e., 340, 354
agirre, e., 126, 376, 504,

510, 514

agrawal, m., 393   395
ahmad, f., 488
ahn, d., 35
ahn, y.-y., 384
aho, a. v., 235, 275
ajdukiewicz, k., 214
akoglu, l., 514
alcalde, h. f., 291
alghamdi, f., 21
algoet, p. h., 58
allen, j., 323, 349, 350,

355, 459, 460

alonso, h. m., 81, 291
amsler, r. a., 514
an, j., 384
anderson, a. h., 452
andrew, g., 35
androutsopoulos, i., 80
angelard-gontier, n., 430,

443

antiga, l., 145
anttila, a., 175, 293
appelt, d. e., 352   354
appling, s., 443
arons, b., 443
artstein, r., 430
aseltine, j., 354
atkins, s., 505
atkinson, k., 490
auer, s., 336, 412
auli, m., 429
austin, j. l., 447, 459
awadallah, a. h., 454

ba, j., 145
baayen, r. h., 35
babko-malaya, o., 514
bacchiani, m., 61
baccianella, s., 387, 514
bach, k., 447
backus, j. w., 195, 220
badmaeva, e., 291
bahl, l. r., 60, 174
baker, c., 376
baker, c. f., 362
baker, j. k., 60, 245, 267
baldwin, t., 72, 80, 514

author index

ballard, d. h., 150
ballesteros, m., 333, 354
balogh, j., 443, 449, 455
banaji, m. r., 125
banea, c., 126, 504
banerjee, e., 291
banerjee, s., 502
bangalore, s., 267
banko, m., 355, 405, 408
bansal, m., 338, 354
bao, j.-w., 429
bar-hillel, y., 214
barham, p., 145
barker, d. t., 455
barrett, l., 250, 267
bates, r., 459
bauer, f. l., 220
baum, l. e., 474, 479
baum, l. f., 442
baumer, g., 81
bayes, t., 65
bazell, c. e., 220
bear, j., 352   354
becker, c., 336, 412
beil, f., 376
bej  cek, e., 274, 293
bell, e., 376, 395, 396
bell, t. c., 61, 255
bellegarda, j. r., 61, 128,

444

bellman, r., 31, 35
belvin, r., 274
bender, e. m., 221
bengio, s., 430
bengio, y., 61, 101, 119,

129, 135   137, 145,
150, 430, 436

be  nu  s,   s., 459
berant, j., 414, 415, 419,

421

berg-kirkpatrick, t.,

77   79, 81

berger, a., 100
bergsma, s., 376, 489
bernstein, m. s., 398
bethard, s., 21, 355, 376
bever, t. g., 265
bhat, i., 285
bhat, r. a., 285
bhembe, d., 424
biber, d., 221
bies, a., 208, 210
bikel, d. m., 255, 268, 354
bills, s., 341, 354
birch, a., 27, 29
bird, s., 35
bisani, m., 81
bishop, c. m., 81, 100
bizer, c., 336, 412
bj  orkelund, a., 368
black, e., 263, 264, 267,

514

blair, c. r., 491
blair, e., 21

blair-goldensohn, s., 397,

blei, d. m., 129, 514
blodgett, s. l., 21, 47, 72
bloom   eld, l., 211, 220,

514

356

blunsom, p., 419, 421
bobrow, d. g., 323, 374,

375, 423, 431, 432,
444

bobrow, r. j., 444
bod, r., 267
bogomolov, m., 81
boguraev, b., 514
boguraev, b. k., 421
bohus, d., 455
bojanowski, p., 129
bollacker, k., 336, 412
bolukbasi, t., 125, 126
booth, r. j., 71, 382, 383
booth, t. l., 238, 267
bordes, a., 404, 410, 421
borges, j. l., 63
boser, b., 150
boser, b. e., 150
b  ottner, m., 424
bottou, l., 119, 129, 354,

375

bourlard, h., 150
bousquet, o., 397
bowman, s. r., 430
boyd-graber, j., 421, 514
brachman, r. j., 322, 323
brants, t., 56, 166, 167,

174, 175

br  eal, m., 103
breck, e., 409, 421
bresnan, j., 214, 220, 237
brevdo, e., 145
brill, e., 267, 405, 408,

489, 490

brin, s., 354
briscoe, t., 264, 267, 514
broadhead, m., 355
brockett, c., 429
brockman, w., 124
brockmann, c., 372
brody, s., 514
broschart, j., 152
bruce, b. c., 460
bruce, r., 514
bruce, r. f., 378, 505
brysbaert, m., 381   383
bryson, j. j., 125
bu, j., 514
buchholz, s., 294
buck, c., 48
budanitsky, a., 104, 492
bullinaria, j. a., 129
bulyko, i., 61, 454
bunker, r. t., 505
burchardt, a., 291
burger, j. d., 409, 421
burges, c. j. c., 419, 428

543

burget, l., 129
burkett, d., 77   79, 81
burnett, d., 442

cafarella, m., 355
caliskan, a., 125
candito, m., 294
canon, s., 268
cardie, c., 126, 354, 504,

514

carletta, j., 452
carmel, d., 421
carpenter, b., 458
carpenter, r., 424, 429
carreras, x., 368, 375
carroll, g., 268, 376
carroll, j., 264, 267
carstensen, f. v., 174
casta  no, j., 345, 347, 349
celikyilmaz, a., 436
cer, d., 126, 504
  cernock`y, j. h., 129
chahuneau, v., 391, 392
chai, j. y., 376
chambers, n., 352, 355,

372

chanan, g., 145
chanev, a., 293
chang, a., 21
chang, a. x., 347, 355,

414, 418
chang, j. s., 514
chang, k.-w., 125, 126
chang, m.-w., 419
chang, p.-c., 35
charles, w. g., 397, 504
charlin, l., 428, 430
charniak, e., 174, 211, 250,

267, 268, 513

che, w., 375
chelba, c., 242
chen, b., 398
chen, c., 514
chen, d., 283, 404, 410,

421

chen, e., 429
chen, j. n., 514
chen, k., 119, 127, 129
chen, m. y., 397
chen, p., 429
chen, s. f., 53, 55, 61
chen, w., 375
chen, x., 107
chen, y.-n., 436
chen, z., 145
cherry, c., 429
chi, z., 268
chiang, d., 268
chiba, s., 479
chierchia, g., 323
chinchor, n., 81, 354
chintala, s., 145
chiticariu, l., 333
chklovski, t., 514

544 author index

220

175

459

chodorow, m. s., 500
choi, e., 419
choi, j. d., 285, 294
choi, y., 376, 395, 396,

419, 514

chomsky, c., 322, 412, 420
chomsky, n., 60, 195, 213,

chou, a., 414, 419
chow, y.-l., 268
christodoulopoulos, c.,

chu, y.-j., 288
chu-carroll, j., 421, 458,

chung, g. y., 488, 489
church, a., 308
church, k. w., 22, 51, 53,

56, 61, 116, 174,
235, 372, 481,
483   485, 492, 507,
511

ciaramita, m., 294
cinkov  a, s., 291
citro, c., 145
clark, a., 268
clark, c., 421
clark, e., 103
clark, h. h., 20, 445, 448,

clark, j. h., 55, 56, 61
clark, k., 384   386
clark, p., 419
clark, s., 129, 175, 268,

coccaro, n., 128, 459
cohen, d. n., 479
cohen, k. b., 327
cohen, m. h., 443, 449,

459

283

455

cohen, p. r., 459, 460
colaresi, m. p., 390   392
colby, k. m., 427, 428, 444
cole, r. a., 442
collins, m., 61, 211   213,
248, 250, 251, 255,
264, 267, 268, 275,
413

collobert, r., 119, 129,

354, 375

colombe, j. b., 354
colosimo, m., 354
c     oltekin, c   ., 291
conrad, s., 221
conrath, d. w., 502
conti, j., 442
cook, p., 514
copestake, a., 514
coppola, b., 421
corrado, g. s., 119, 127,

129

cotton, s., 505
cottrell, g. w., 513
courville, a., 101,

135   137, 150
cover, t. m., 57, 58
covington, m., 277, 293
cowhey, i., 419

cox, d., 99
cox, s. j., 81
crammer, k., 293
craven, m., 354
crouch, r., 268
cruse, d. a., 129
cucerzan, s., 489
culicover, p. w., 221
curran, j. r., 175, 268
cyganiak, r., 336, 412

daelemans, w., 267
dagan, i., 116, 118, 129
dai, a. m., 430
damerau, f. j., 481, 486,

487, 491, 492

dang, h. t., 354, 359, 375,

505, 506, 514
danieli, m., 441, 455
danilevsky, m., 333
das, d., 293, 421
das, s. r., 397
dauphin, y., 436
davidson, d., 312, 323
davidson, t., 443
davies, m., 124
davis, a., 145
davis, e., 323
day, d. s., 349
dayanidhi, k., 437
dean, j., 56, 119, 127, 129,

145

deerwester, s. c., 128
dejong, g. f., 354, 375
delfs, l., 505
della pietra, s. a., 100
della pietra, v. j., 100
demner-fushman, d., 327
dempster, a. p., 474, 484
deng, l., 436
denker, j. s., 150
derczynski, l., 355
derose, s. j., 174
desmaison, a., 145
devin, m., 145
devito, z., 145
de lacalle, o. l., 514
de marneffe, m.-c., 155,

173, 208, 272, 291,
293

de paiva, v., 291
de villiers, j. h., 442
diab, m., 21, 126, 504, 514
dienes, p., 172
digman, j. m., 393
di marco, a., 514
dligach, d., 512
do, q. n. t., 376
doddington, g., 194
doherty-sneddon, g., 452
dolan, b., 429
dolan, w. b., 514
dos santos, c., 338, 354
downey, d., 355
dowty, d. r., 323, 373
dozat, t., 272, 291, 293
droganova, k., 291
dror, r., 81

duan, n., 429
dubou  e, p. a., 421
ducharme, r., 119, 129,

145

duda, r. o., 512
dumais, s. t., 71, 80, 126,
128, 405, 408, 504
dunphry, d., 71, 381, 397
dyer, c., 333, 354, 419
dziurzynski, l., 393   395

eagon, j. a., 479
earley, j., 226, 235
ebert, s., 398
eckert, w., 460
edmonds, j., 288
edmonds, p., 514
efron, b., 78
egghe, l., 35
eichstaedt, j. c., 393   395
eisner, j., 268, 293, 466
ejerhed, e. i., 235
ekman, p., 380
el-kahky, a., 291
elisseeff, a., 81
ellis, g., 488, 489
ellsworth, m., 362, 363
elman, j. l., 150, 178
erk, k., 376
eryigit, g., 293
espeholt, l., 421
esuli, a., 387, 514
etzioni, o., 342, 343, 354,
355, 376, 414, 415,
419

evanini, k., 437
evans, c., 336, 412
evans, n., 152
evert, s., 129

fader, a., 342, 343, 355,

414, 415

fan, j., 421
fano, r. m., 116
fanshel, d., 452
fanty, m., 442
farhadi, a., 421
farkas, r., 294
fast, e., 398
feldman, j. a., 150
fellbaum, c., 497, 505, 514
feng, s., 421, 514
fennig, c. d., 21
fensel, d., 321
ferguson, j., 466
ferguson, m., 208, 210
ferro, l., 345, 347, 349
ferrucci, d. a., 421
fessler, l., 443
fikes, r. e., 460
fillmore, c. j., 220, 221,

322, 357, 362, 374,
375

finegan, e., 221
finkelstein, l., 126, 504
firth, j. r., 101, 106, 459
fisch, a., 404, 410, 421
fischer, m. j., 31, 479, 492

fisher, d., 354
flickinger, d., 263, 264,

268

fodor, j. a., 128, 323, 370
fodor, p., 421
fokoue-nkoutche, a., 421
foland, jr., w. r., 375
folds, d., 443
forbes-riley, k., 423, 424
forchini, p., 428
forney, jr., g. d., 479
fosler, e., 41
foster, j., 294
fox tree, j. e., 20
francis, h. s., 246
francis, m. e., 71, 382, 383
francis, w. n., 19, 174
frank, e., 81, 100
franz, a., 56, 267
fraser, n. m., 442
freitag, d., 354
fried, g., 443
friedman, j. h., 80, 100
fromer, j. c., 455
frostig, r., 414, 419
fung, p., 21
furnas, g. w., 128
fyshe, a., 129

gabow, h. n., 289
gabrilovich, e., 126, 504
gage, p., 27
gaizauskas, r., 345, 347,

349

gale, w. a., 51, 53, 61,

372, 481, 483   485,
492, 507, 511

gales, m. j. f., 61
galil, z., 289
galley, m., 35, 429, 430
gandhe, s., 430
gao, j., 56, 429, 430, 436
gardner, m., 421
garg, n., 126
garside, r., 174, 175
ga  si  c, m., 450, 451, 458
gauvain, j.-l., 61, 129
gazdar, g., 207
gdaniec, c., 263, 264
geman, s., 268
georgila, k., 460
gerber, l., 345, 347
gerber, m., 376
gerbino, e., 441, 455
gerten, j., 430
gertz, m., 347, 355
ghahramani, z., 397
ghemawat, s., 145
ghoneim, m., 21
giangola, j. p., 443, 449,

455
gil, d., 152
gilbert, g. n., 442
gildea, d., 366, 375
gillick, l., 81
ginter, f., 155, 173, 208,

272, 291, 293

ginzburg, j., 458

giuliano, v. e., 128
giv  on, t., 246
glass, j., 442
glennie, a., 235
godfrey, j., 19, 194
goebel, r., 376, 489
goenaga, i., 294
goffman, e., 375
gojenola, k., 294
gokirmak, m., 291
goldberg, j., 454
goldberg, y., 118, 124,

129, 150, 155, 173,
208, 272, 293, 294

golding, a. r., 489
goldwater, s., 175
gondek, d., 421
gonnerman, l. m., 123
gonzalez-agirre, a., 126,

504

good, m. d., 442
goodenough, j. b., 504
goodfellow, i., 135   137,

145, 150
goodine, d., 442
goodman, j., 53, 55, 61,

268

goodwin, c., 459
gosling, s. d., 393
gould, j. d., 442
gould, s. j., 101
gravano, a., 459
gravano, l., 340, 354
grave, e., 129
green, b. f., 322, 412, 420
green, j., 220
green, l., 21, 72
green, s., 294
greenbaum, s., 221
greene, b. b., 174
greenwald, a. g., 125
grefenstette, e., 419, 421
gregory, m. l., 246
grenager, t., 376
grishman, r., 263, 264,

352, 354, 361

gross, s., 145
grosz, b. j., 459, 460
grover, c., 331
gruber, j. s., 357, 374
guo, y., 375
gus   eld, d., 34, 35
guyon, i., 81

habash, n., 291, 294
hacioglu, k., 375
haddow, b., 27, 29
haghighi, a., 268
haji  c, j., 155, 172, 173,

208, 264, 272, 274,
291, 293, 294
haji  cov  a, e., 274, 293
hajishirzi, h., 421
hakkani-t  ur, d., 172, 436
hale, j., 264
hall, j., 293
hamilton, w. l., 125,

384   386

hanks, p., 116, 349
hannan, k., 397, 514
hansen, b., 442
harabagiu, s., 375, 404
harnish, r., 447
harper, m. p., 175
harris, k., 291
harris, r. a., 443
harris, z. s., 101, 106, 174,

235

harshman, r. a., 128
hart, p. e., 512
hart, t., 56
hastie, t., 80, 100
hathi, s., 80
hatzivassiloglou, v., 385,

386, 397

haverinen, k., 272, 293
hawwari, a., 21
he, h., 419
he, l., 367, 375
he, x., 436
hea   eld, k., 48, 55, 56, 61
heaps, h. s., 20, 35
hearst, m. a., 336, 337,

342, 354, 511, 514

heck, l., 436
heckerman, d., 71, 80
heikkil  a, j., 175, 293
heim, i., 308, 323
hellmann, s., 336, 412
hemphill, c. t., 194
henderson, d., 150
henderson, j., 268, 460
henderson, m., 447
henderson, p., 443
hendler, j. a., 321
hendrickson, c., 174
hendrickx, i., 354
hendrix, g. g., 374
hepple, m., 349
herdan, g., 20, 35
hermann, k. m., 419, 421
hermjakob, u., 404
hickey, m., 38
hilf, f. d., 427, 428, 444
hill, f., 103, 126, 504
hindle, d., 263, 264, 267
hinkelman, e. a., 459
hinton, g. e., 123, 142,

145, 150

hirschberg, j., 21, 453,

454, 456, 458, 459

hirschman, l., 81, 354,

409, 421, 441, 442

hirst, g., 104, 370, 492,

513

hjelmslev, l., 128
hlav  acov  a, j., 291
hobbs, j. r., 352   354
hockenmaier, j., 219
hoff, m. e., 149
hoffmann, p., 71, 381
hofmann, t., 129
holliman, e., 19
holtzman, a., 376, 396
hopcroft, j. e., 199
hopely, p., 174, 420

horning, j. j., 268
horvitz, e., 71, 80
householder, f. w., 175
hovanyecz, t., 442
hovy, d., 81
hovy, e. h., 107, 274, 354,

386, 404, 514

howard, r. e., 150
hsu, b.-j., 61
hu, m., 71, 381, 386
huang, e. h., 126, 504
huang, l., 268, 283
huang, z., 354
hubbard, w., 150
huddleston, r., 200, 221
hudson, r. a., 293
huffman, s., 354
hull, d. a., 99, 128
hulteen, e. a., 443
hunter, p., 437
hutchinson, b., 488, 489
hutto, c. j., 443
huval, b., 354
hymes, d., 375

iacobacci, i., 506
ingria, r., 263, 264, 345,

347, 349, 444

irons, e. t., 235
irving, g., 145
isard, a., 452
isard, m., 145
isard, s., 452
isbell, c. l., 429
iso8601, 346, 347
israel, d., 352   354
issar, s., 434
iyyer, m., 419

jackel, l. d., 150
jackendoff, r., 221, 315
jacobs, p. s., 354
jacobson, n., 174
jaech, a., 80
jafarpour, s., 428
jang, s. b., 346
jauhiainen, t., 80
jauvin, c., 119, 129, 145
jefferson, g., 449, 451
jeffreys, h., 60
jekat, s., 450
jelinek, f., 52, 60, 165, 242,

263, 264, 267, 479

ji, h., 354
ji, y., 429
jia, r., 409
jia, y., 145
jiang, j. j., 502
jim  enez, v. m., 268
j    nov  a, p., 274, 293
johannsen, a., 81
johansson, r., 294, 375
johansson, s., 221
johnson, c. r., 362, 363
johnson, m., 249, 268, 315
johnson, w. e., 61
johnson-laird, p. n., 362
johnston, m., 458

author index

545

jones, m. p., 128, 492
jones, r., 340, 354, 454
jones, s. j., 442
jones, t., 21
joos, m., 101, 106, 127
jordan, m. i., 87, 129
jordan, p. w., 424
joshi, a. k., 174, 214, 221,
237, 267, 268, 420

joshi, m., 419
joulin, a., 129
jozefowicz, r., 145, 430
juang, b. h., 479
jurafsky, d., 21, 35, 41, 47,

72, 107, 114, 125,
126, 128, 173, 341,
342, 352, 354, 366,
372, 375, 384   386,
391   393, 430, 459,
514

jurgens, d., 21, 47, 72, 512
justeson, j. s., 397

kaiser, l., 145
kalai, a. t., 125, 126
kalyanpur, a., 421
kameyama, m., 352   354
kamm, c. a., 441
kanayama, h., 291, 421
kanerva, j., 291
kang, j. s., 514
kannan, a., 430
kaplan, r. m., 226, 268,

374, 421, 423, 431,
432, 444

karlen, m., 119, 129, 354,

375

karlsson, f., 175, 293
karttunen, l., 174, 420
kasami, t., 223, 235
kashyap, r. l., 492
katz, c., 220
katz, g., 345, 347, 349
katz, j. j., 128, 323, 370
katz, k., 208, 210
katz, s. m., 397
kavukcuoglu, k., 119, 129,

354, 375

kawahara, d., 294
kawakami, k., 333, 354
kawamoto, a. h., 513
kay, m., 226, 235, 374,

423, 431, 432, 444

kay, p., 220, 221
kay, w., 421
kayadelen, t., 291
ke, n. r., 443
kearns, m., 429, 454, 460
keizer, s., 450, 451
keller, f., 372, 489
kelly, e. f., 513
kembhavi, a., 421
kern, m. l., 393   395
kernighan, m. d., 481,

483   485, 492

kert  esz, j., 35
kettnerov  a, v., 274, 291,

293

546 author index

khoddam, e., 376
khot, t., 419
khudanpur, s., 129
kiela, d., 129
kilgarriff, a., 505   508
kim, d., 458
kim, g., 208, 210
kim, s. m., 386
kim, s. n., 354
king, l. a., 393
king, t. h., 268
kingma, d., 145
kingsbury, p., 375
kintsch, w., 128, 322
kiparsky, p., 356
kipper, k., 359, 375
kirchhoff, k., 454
kit, c., 375
klapaftis, i. p., 512
klavans, j. l., 263, 264
kleene, s. c., 34
klein, a., 450
klein, d., 77   79, 81, 171,
172, 175, 249, 250,
259, 267, 268

klein, e., 35, 207
klein, s., 174, 175, 420
klementiev, a., 376
kneser, r., 53, 54
knippen, r., 346
knuth, d. e., 491
kobilarov, g., 336, 412
kocisky, t., 421
ko  cisk`y, t., 419
koehn, p., 55, 56, 61
kol  a  rov  a, v., 274, 293
koller, d., 268
kombrink, s., 129
kondrak, g., 488
koo, t., 268
korhonen, a., 103, 126,

504

kormann, d., 429
kornai, a., 35
kosinski, m., 393   395
kowtko, j. c., 452
kozareva, z., 354
kraemer, h. c., 428
kratzer, a., 308, 323
krieger, m., 35
krizhevsky, a., 145
krovetz, r., 27, 511
kruskal, j. b., 35, 479
k  ubler, s., 293, 294
ku  cera, h., 19, 174
kudlur, m., 145
kudo, t., 293
kuhlmann, m., 294
kukich, k., 486, 492
kuksa, p., 119, 129, 354,

375

kulkarni, r. g., 454
kullback, s., 371
kumlien, j., 354
kuno, s., 235
kuperman, v., 381   383
kupiec, j., 174
kwak, h., 384

kwak, s., 291
kwan, j. l. p., 421
kwiatkowski, t., 421

labov, w., 99, 452
lafferty, j., 397
lafferty, j. d., 100, 171,

267, 354
laham, d., 128
laird, n. m., 474, 484
lakoff, g., 315, 323, 373
lal, t. n., 397
lally, a., 421
lamblin, p., 150
lample, g., 333, 354
landauer, t. k., 126, 128,

442, 504
landes, s., 505
lando, t., 291
lang, j., 376
langer, s., 38
langlais, p., 508
lapalme, g., 508
lapata, m., 372, 376, 489,

509, 510, 514

lapesa, g., 129
lari, k., 245, 268
larochelle, h., 150
lau, j. h., 514
laughery, k., 322, 412, 420
lazo, m., 349
la polla, r., 221
le, q., 429
leacock, c., 500, 505
lecun, y., 150
lee, c.-h., 436, 444
lee, d. d., 129
lee, k., 367, 375, 421
lee, l., 80, 397
leech, g., 175, 221
lehmann, j., 336, 412
lehnert, w. g., 354
leibler, r. a., 371
lemon, o., 460
lerer, a., 145
lertpradit, s., 291
lesk, m. e., 508, 513
leskovec, j., 125, 384   386
leung, h., 291, 442
leuski, a., 428, 430
levenberg, j., 145
levenshtein, v. i., 30
levesque, h. j., 323, 459
levin, b., 359, 375
levin, e., 436, 444, 460
levinson, s. c., 459
levow, g.-a., 442,

453   455
levy, j. p., 129
levy, o., 118, 124, 129
levy, r., 264
lewis, c., 442
lewis, d. l., 81, 354
lewis, m., 259, 367, 375
li, h., 429
li, j., 107, 291, 429, 430
li, x., 404, 406
li, y., 333, 375

li, z., 375, 429
liang, p., 409, 414, 415,

419

459

liberman, m. y., 263, 264,

lieberman, h., 321
lieberman aiden, e., 124
light, m., 409, 421
lin, d., 264, 293, 376, 489,

501, 502
lin, j., 404, 408
lin, y., 124
lin, z., 145
lind  en, k., 80
lindsey, r., 322
liscombe, j., 437
litkowski, k. c., 375
litman, d. j., 423, 424,

441, 453, 454, 456,
460

littman, j., 345   347
littman, m., 384, 386
liu, a., 458
liu, b., 71, 80, 381, 386,

514

liu, c.-w., 430
liu, t., 375
liu, t.-h., 288
liu, x., 61
llorens, h., 355
lochbaum, k. e., 128, 460
loper, e., 35
lopez-gazpio, i., 126, 504
l  opez de lacalle, o., 510
lopyrev, k., 409, 419
lovins, j. b., 34
lowe, j. b., 362
lowe, r., 443
lowe, r. t., 428, 430
lu, z., 429
luhn, h. p., 113
lui, m., 72, 80
luotolahti, j., 291
lyons, j., 323
lytel, d., 513

ma, x., 354
maccartney, b., 293
macintyre, r., 208, 210
macketanz, v., 291
macleod, c., 361
macy, m., 443
madhu, s., 513
magerman, d. m., 212,

267, 275
maharjan, s., 21
maier, e., 450
maier, w., 294
maiorano, s., 404
mairesse, f., 379, 395, 450,

451

makatchev, m., 424
maleck, i., 450
manandhar, s., 512
mandl, m., 291
man  e, d., 145
mani, i., 345   347

manning, c. d., 35, 80, 87,
119, 123, 124, 126,
129, 155, 171   173,
175, 208, 245, 249,
250, 259, 267, 268,
272, 283, 291, 293,
347, 354, 355, 376,
397, 411, 414, 504

manurung, r., 291
marcinkiewicz, m. a., 154,
208, 210, 243, 274,
293

marcus, m. p., 154, 185,

208, 210, 243, 263,
264, 267, 274, 293,
375, 514
marcus, s., 116
marheinecke, k., 291
marinov, s., 293
maritxalar, m., 126, 504
markov, a. a., 60, 479
markovitch, s., 116
marlin, b. m., 355
maron, m. e., 80
m`arquez, l., 294, 368, 375,

376

marshall, c., 459
marshall, i., 174
marsi, e., 293, 294
mart    , m. a., 294
martin, j. h., 128, 375,

492, 514
martin, r., 459
martinez, d., 376
marx, m., 442, 455
marzal, a., 268
mast, m., 450
masterman, m., 322, 513
matias, y., 126, 504
matsumoto, y., 293
mausam, 376
mausam., 354
maxwell iii, j. t., 268
mays, e., 481, 486, 487,

491, 492

mccallum, a., 80, 100,

171, 354, 355

mccarthy, d., 514
mccarthy, j., 220
mccawley, j. d., 221, 323
mcclelland, j. l., 150
mcconlogue, k., 420
mcconnell-ginet, s., 323
mccord, m. c., 421
mcculloch, w. s., 131, 149
mcdaniel, j., 19
mcdonald, r., 287, 293,

294, 397, 514

mcdonald, r. t., 155, 173,

208, 272, 293

mcenery, a., 175
mcfarland, d. a., 393
mcghee, d. e., 125
mcguiness, d. l., 321
mckeown, k. r., 385, 386,

397

meek, c., 410, 419
mehl, m. r., 393

mel     cuk, i. a., 293
melis, g., 419
mendonc  a, g., 291
mercer, r. l., 52, 60, 165,
174, 267, 481, 486,
487, 492
merialdo, b., 174
mesnil, g., 436
meteer, m., 174, 459
metsis, v., 80
meyers, a., 294, 361, 375
michaelis, l. a., 246
michel, j.-b., 124
microsoft., 423, 425, 429
mihalcea, r., 126, 504,

506, 514
mikheev, a., 331
mikolov, t., 61, 119, 124,

127, 129

mikulov  a, m., 274, 293
miller, g. a., 46, 60, 397,

504, 505

miller, s., 255, 354, 444
minsky, m., 80, 134, 150,

375

mintz, m., 341, 354
m    rovsk  y, j., 274, 293
missil  a, a., 291
mitchell, m., 429
mitchell, t. m., 129
mitton, r., 492
miwa, m., 338, 354
moens, m., 331
moens, m.-f., 376
mohammad, s. m., 381,

382

moldovan, d., 514
monga, r., 145
monroe, b. l., 390   392
monroe, w., 430
montague, r., 323
monz, c., 405
mooney, r. j., 413
moore, r. c., 489, 490
moore, s., 145
morante, r., 376
morgan, a. a., 354
morgan, n., 41, 150
morimoto, t., 459
morin, f., 61, 129
morris, w., 495
moschitti, a., 368
mosteller, f., 65, 80
moszkowicz, j., 349
mrk  si  c, n., 452, 458
mulcaire, g., 80
murdock, j. w., 421
murphy, b., 129
murphy, k. p., 81, 100
murray, d., 145

n  adas, a., 61
nagao, m., 267
nagata, m., 459
nagy, p., 443
nakov, p., 354
narayanan, a., 125
narayanan, s. s., 455

nash-webber, b. l., 374,

421

naur, p., 220
navigli, r., 506, 509, 510,

palmer, d., 35
palmer, m., 274, 275, 285,
293, 359, 375, 376,
500, 505, 506, 514

512, 514

293

nedoluzhko, a., 274, 291,

needleman, s. b., 479
neff, g., 443
newell, a., 38
newman, d., 514
ney, h., 53, 54, 81, 242
ng, a. y., 87, 126, 129,

342, 354, 504, 514

ng, h. t., 368, 421, 506
nielsen, j., 442
nielsen, m. a., 150
nigam, k., 80, 100, 354
nilsson, j., 293, 294
nilsson, n. j., 460
nist, 35
nitisaroj, r., 291
nitta, y., 116
nivre, j., 155, 173, 208,

272, 277, 283, 285,
287, 291, 293, 294,
375

niwa, y., 116
noreen, e. w., 78
norman, d. a., 322, 374,
375, 423, 431, 432,
444, 448

norvig, p., 37, 136, 235,

304, 323, 484, 487,
488, 492

nosek, b. a., 125
noseworthy, m., 430
novick, d. g., 442
nunes, j. h. t., 459

o   connor, b., 21, 35, 47,

72

o   hara, t. p., 378
och, f. j., 56
odell, m. k., 491
oepen, s., 268
oettinger, a. g., 235
o   azer, k., 172
ogilvie, d., 71, 381, 397
oh, a. h., 456, 457
ojala, s., 291
olah, c., 145
oommen, b. j., 492
oravecz, c., 172
ordonez, v., 126
orwant, j., 124
osborne, m., 56, 175
o   s  eaghdha, d., 452
osgood, c. e., 105, 106,

128, 381, 397

osindero, s., 150
ostendorf, m., 61, 80, 454
ozertem, u., 454
  o s  eaghdha, d., 354

packard, d. w., 34
pad  o, s., 294, 354
paliouras, g., 80

palmucci, j., 174
pan, y., 421
panevova, j., 293
panevov  a, j., 274, 293
pang, b., 80, 397
pao, c., 441
paolino, j., 443
papert, s., 134, 150
pappuswamy, u., 424
parikh, a., 421
paritosh, p., 336, 412
parsons, t., 312, 323
partee, b. h., 323
pasca, m., 404, 405, 408
paszke, a., 145
patwardhan, s., 421
pearl, c., 443
pedersen, j., 81, 99, 124,

128

pedersen, t., 502, 514
peng, n., 338, 354
penn, g., 356
pennacchiotti, m., 354
pennebaker, j. w., 71, 382,

383, 393

pennington, j., 119, 123,

124, 129, 411
percival, w. k., 220
pereira, f. c. n., 171, 293,

354

perkowitz, m., 174
perlis, a. j., 220
perrault, c. r., 460
peters, s., 323
peterson, j. l., 491
petrie, t., 479
petrov, s., 124, 155, 173,

208, 250, 267, 272,
291, 293, 294

petruck, m. r. l., 362, 363
philips, l., 490
phillips, a. v., 420
phillips, j., 346
phillips, m., 442
picard, r. w., 378
pieraccini, r., 436, 437,

444, 460

pilehvar, m. t., 506
pineau, j., 428, 430, 443,

460

pitler, e., 291, 489
pitts, w., 131, 149
plank, b., 81
plaut, d. c., 123
plutchik, r., 380, 381
pol  akov  a, l., 274, 293
polifroni, j., 441, 442
pollard, c., 211, 212, 214,

220, 237

ponzetto, s. p., 506
poon, h., 338, 354
popat, a. c., 56
popel, m., 291
popescu, a.-m., 355

author index

547

popovici, d., 150
porter, m. f., 26, 27
potthast, m., 291
potts, c., 388, 389
pouzyrevsky, i., 55, 56, 61
pow, n., 430
pradhan, s., 368, 375, 512
prager, j. m., 421
prakash, s., 514
prasettio, m. c., 376, 396
prescher, d., 376
price, p. j., 453
przepi  orkowski, a., 294
pullum, g. k., 200, 207,

221

purver, m., 458
pustejovsky, j., 345   347,

349, 355, 514

pyysalo, s., 155, 173, 208,

272, 291, 293

qi, p., 291
qin, b., 375
qiu, g., 514
qiu, z., 421
qiu, z. m., 421
quantz, j., 450
quillian, m. r., 322, 499,

513

quinn, k. m., 390   392
quirk, c., 338, 354
quirk, r., 221

rabiner, l. r., 466, 476,

477, 479
radev, d., 349
radford, a., 195, 221
raghavan, p., 80, 129
rajpurkar, p., 409, 419
ramshaw, l. a., 174, 185,

264, 274, 514

ranganath, r., 393
raphael, b., 322
rappaport hovav, m., 359
rashkin, h., 376, 395, 396
ratinov, l., 129
ratnaparkhi, a., 100, 175,

255, 267
rau, l. f., 354
raux, a., 447
ravichandran, d., 404
raviv, j., 492
raymond, c., 436
reddy, s., 291
reeves, r., 361
rehder, b., 128
rehm, g., 291
reichart, r., 81, 103, 126,

504

reichenbach, h., 314
reichert, t. a., 479
reiss, f., 333
reiss, f. r., 333
renshaw, e., 419
resnik, p., 267, 370, 371,
376, 500, 501, 514

reynar, j. c., 267
ribarov, k., 293

548 author index

riccardi, g., 436
richardson, m., 419
riedel, s., 294, 354, 355
rieger, c., 513
ries, k., 459
riesbeck, c. k., 513
riezler, s., 268, 376
rigau, g., 126, 504
riley, m., 61
riloff, e., 340, 354, 376,
381, 397, 398, 421

ringenberg, m., 424
ritter, a., 354, 376,

428   430

rivlin, e., 126, 504
roark, b., 61, 268
rodriguez, p., 421
rohde, d. l. t., 123
romano, l., 354
rooth, m., 267, 376
roque, a., 424
ros  e, c., 424
rosenblatt, f., 149
rosenfeld, r., 61, 100
rosenzweig, j., 505, 507,

508

roth, d., 404, 406, 489
roth, r., 294
rothe, s., 398
roukos, s., 263, 264, 267
routledge, b. r., 391, 392
roy, n., 460
rubenstein, h., 504
rubin, d. b., 474, 484
rubin, g. m., 174
rudnicky, a. i., 455   457
rumelhart, d. e., 142, 150,

rumshisky, a., 346
ruppenhofer, j., 362, 363,

ruppin, e., 126, 504
russell, j. a., 380
russell, r. c., 491
russell, s., 37, 136, 304,

rutishauser, h., 220

322

376

323

sabharwal, a., 419
sacks, h., 451
sadock, j. m., 298
sag, i. a., 207, 211, 212,

214, 220, 221, 237,
458, 459

sagae, k., 283
sahami, m., 71, 80
sakoe, h., 479
salakhutdinov, r. r., 145
salant, s., 421
saligrama, v., 125, 126
salomaa, a., 267
salton, g., 108, 128
samelson, k., 220
sampson, g., 175
samuelsson, c., 167, 175
san   lippo, a., 264
sankoff, d., 99, 479

santorini, b., 154, 208,

243, 263, 264, 274,
293

sap, m., 376, 396
saraclar, m., 61
saur    , r., 345, 347, 349
sauri, r., 346
schabes, y., 267, 268
schaefer, e. f., 448
schalkwyk, j., 442
schank, r. c., 322, 351,

362, 375

schapire, r. e., 174, 267
schasberger, b., 208, 210
schatzmann, j., 450, 451
scheffczyk, j., 362, 363
schegloff, e. a., 449, 451
scherer, k. r., 378, 379,

393

schiebinger, l., 126
schilder, f., 349
schmandt, c., 443
schmelzenbach, m., 376
schmolze, j. g., 323
schoenick, c., 419
sch  olkopf, b., 397
scholz, m., 293
schone, p., 128
sch  onk   nkel, m., 308
schreiner, m. e., 128
schuster, m., 145
schuster, s., 291
sch  utze, h., 80, 99, 124,

128, 129, 174, 245,
268, 372, 398, 511,
514

schwartz, h. a., 393   395
schwartz, j. l. k., 125
schwartz, r., 174, 255,

268, 354, 444

schwarz, j., 419
schwenk, h., 61, 129
scott, m., 264
s  eaghdha, d. o., 376
sebastiani, f., 387, 514
seddah, d., 294
see, a., 349
segal, j., 41
sekine, s., 264
selfridge, j. a., 46
sen  ecal, j.-s., 61, 129
seneff, s., 441, 442
sennrich, r., 27, 29
seo, m., 421
serban, i. v., 428, 430
sethi, r., 235
setzer, a., 345, 347, 349
seung, h. s., 129
s  gaard, a., 81, 175
sgall, p., 293
shah, a., 393   395
shaked, t., 355
shang, l., 429
shannon, c. e., 46, 60, 492
sharma, d., 285
sheil, b. a., 235
sheinwald, d., 421
shepherd, j., 397, 398

shi, t., 430
shillcock, r., 264
shima, h., 421
shimada, a., 291
shlens, j., 145
shriberg, e., 453, 459
shrivastava, m., 285
sidner, c. l., 459, 460
siler, s., 424
silveira, n., 155, 173, 208,

272, 293

simi, m., 291
simmons, r. f., 174, 175,
322, 364, 374, 420,
421, 513
simons, g. f., 21
singer, y., 171, 172, 174,

singh, s., 376, 395, 396,

175, 267

429

singh, s. p., 460
sinha, k., 443
sleator, d., 267, 293
slocum, j., 374
small, s. l., 513
smith, d. a., 268
smith, m., 71, 381, 397
smith, n. a., 80, 268, 391,

smith, v. l., 445
smolensky, p., 150
snow, r., 341, 342, 354,

392

514

socher, r., 119, 123, 124,
126, 129, 354, 411,
504

soderland, s., 342, 343,

354, 355, 414

solan, z., 126, 504
solorio, t., 21
sordoni, a., 429
soroa, a., 510
sparck jones, k., 114, 128,

513, 514
spencer, t., 289
spitkovsky, v. i., 418
sporleder, c., 376
sproat, r., 61
srinivas, b., 268
srivastava, n., 145
srivastava, r., 424
stalnaker, r. c., 448
stamatatos, e., 80
steedman, m., 175, 214,

219, 259
steiner, b., 145
stent, a., 294
  st  ep  anek, j., 294
stetina, j., 267
stevenson, s., 375, 376
stifelman, l. j., 443
stillwell, d., 393   395
stoics, 151
stolcke, a., 41, 56, 61, 267,

459

stolz, w. s., 174
stone, p., 71, 381, 397, 429
stone, p. j., 513

stoyanchev, s., 458
straka, m., 291
stran  a  k, p., 294
streeter, l., 128
strnadov  a, j., 291
str  otgen, j., 347, 355
strzalkowski, t., 263, 264
sturge, t., 336, 412
stuttle, m., 460
su, j., 354
su, p.-h., 458
subramanian, s., 333, 354
suci, g. j., 105, 106, 128,

381, 397

suendermann, d., 437
suh, j., 419
sulubacak, u., 291
sundheim, b., 345, 347,

349, 352, 354

surdeanu, m., 294, 354,

375, 376

sutskever, i., 119, 129, 145
sutton, s., 442
svartvik, j., 221
swerts, m., 453, 454, 456
swier, r., 376
switzer, p., 128
szekely, r., 361
szpakowicz, s., 354

tafjord, o., 419
tajchman, g., 41
taji, d., 291
talbot, d., 56
talmor, a., 419
talukdar, p. p., 129
talwar, k., 145
tanenhaus, m., 513
tannen, d., 375
tannenbaum, p. h., 105,

106, 128, 174, 381,
397

tarjan, r. e., 289
taskar, b., 268
taylor, j., 336, 412
taylor, p., 459
teh, y.-w., 150
temperley, d., 267, 293
tengi, r. i., 505
teo, l. h., 421
ter meulen, a., 323
tesni`ere, l., 293, 374
tetreault, j., 294
thede, s. m., 175
thelen, m., 421
thibaux, r., 250, 267
thomas, j. a., 57, 58
thompson, c. w., 374
thompson, h., 374, 423,

431, 432, 444

thompson, k., 34
thompson, r. a., 238
thomson, b., 450   452
thrax, d., 151
thrun, s., 460
tibshirani, r. j., 78, 80, 94,

100

tillmann, c., 264

titov, i., 268, 376
tomkins, s. s., 380
toutanova, k., 171, 172,

175, 268, 338, 354,
490

towell, g., 505
traum, d., 428, 430
tsarfaty, r., 155, 173, 208,

272, 293, 294

tseng, h., 35, 173
tsukahara, w., 449
tsvetkov, y., 21, 47, 72
tucker, p., 145
t  ur, g., 172, 436
turian, j., 129
turney, p. d., 381, 382,

384, 386, 397

tyers, f. m., 291
tyson, m., 352   354

ullman, j. d., 199, 235,

275

ungar, l. h., 393   395
uresov  a, z., 291
uria, l., 126, 504
uszkoreit, h., 291
uzzaman, n., 355

vaithyanathan, s., 80, 397
van benthem, j., 323
van der maaten, l., 123
van ess-dykema, c., 459
van harmelen, f., 321
van rijsbergen, c. j., 74,

234, 263

van valin, jr., r. d., 221
van wijnagaarden, a., 220
van zaanen, m., 268
vandyke, d., 458
vanhoucke, v., 145
vanlehn, k., 424
vannella, d., 512
van ooyen, b., 48
vasilescu, f., 508
vasserman, a., 268
vasudevan, v., 145
vauquois, b., 220
velikovich, l., 397, 514
vendler, z., 315
verhagen, m., 346, 349,

355

vermeulen, p. j. e., 442
versley, y., 294

vi  egas, f., 145
villemonte de la cl  ergerie,

e., 294
vilnis, l., 430
vincent, p., 101, 119, 129,

145

vincze, v., 294
vintsyuk, t. k., 479
vinyals, o., 145, 429, 430
viterbi, a. j., 479
volkova, s., 376, 395, 396
voorhees, e. m., 505
voutilainen, a., 175, 293

wade, e., 453
wagner, r. a., 31, 479, 492
wahlster, w., 321
walker, m. a., 379, 395,

432, 441, 454, 455,
460

wall, r. e., 323
wallace, d. l., 65, 80
wang, h., 429
wang, s., 80, 87, 397
wang, t., 126
wang, y.-y., 436
ward, n., 449
ward, w., 375, 434
warden, p., 145
warmsley, d., 443
warriner, a. b., 381   383
wasow, t., 221
wattenberg, m., 145
weaver, w., 513
weber, i., 443
weber, s., 427, 428, 444
wegstein, j. h., 220
wehbe, l., 129
weinschenk, s., 455
weischedel, r., 174, 255,

274, 354, 514

weizenbaum, j., 10, 18,

423, 425, 444

weld, d. s., 354, 355, 419
welty, c., 421
wen, t.-h., 452, 458
wessels, l. f. a., 442
weston, j., 119, 129, 354,
375, 397, 404, 410,
421

whitelaw, c., 488, 489
whiteside, j. a., 442
whittaker, s., 432
wicke, m., 145

widrow, b., 149
wiebe, j., 71, 126, 378,

381, 397, 504, 505,
514

wierzbicka, a., 128
wilcox-o   hearn, l. a.,

488, 492

wilde, o., 480
wilensky, r., 460
wilkes-gibbs, d., 459
wilks, y., 322, 370, 374,

513

williams, j., 375
williams, j. d., 447, 460
williams, r., 354
williams, r. j., 142, 150
wilson, g., 345, 347
wilson, r., 424
wilson, t., 71, 381, 514
winkler, w. e., 491
winograd, t., 322, 323,

374, 423, 431, 432,
444

winston, p. h., 375
witten, i. h., 61, 81, 100,

wittgenstein, l., 106, 447,

255

459

wixon, d. r., 442
wolf, a. k., 322, 412, 420
wolfe, m. b. w., 128
woli  nski, m., 294
wong, a. k. c., 479
woodger, m., 220
woodland, p. c., 61
woods, w. a., 322, 421
woodsend, k., 376
wooters, c., 41
wr  oblewska, a., 294
wu, f., 354
wu, z., 500
wundt, w., 195, 220
wunsch, c. d., 479

xia, f., 275, 293
xiang, b., 338, 354
xu, p., 56
xu, w., 354, 375
xue, n., 274, 294, 368, 375

yamada, h., 293
yan, z., 429
yang, e., 145

author index

549

yang, y., 81, 410, 419
yankelovich, n., 442, 455
yao, k., 436
yao, l., 354, 355
yarowsky, d., 372, 507,

510, 511, 514

yasseri, t., 35
yates, a., 355
yatskar, m., 126, 419
yeh, a. s., 354
yih, w.-t., 124, 127, 338,

354, 410, 419

yngve, v. h., 235, 449
young, b., 361
young, s. j., 245, 268, 450,

451, 458, 460

younger, d. h., 223, 235
yu, d., 436
yu, k., 354, 450, 451
yu, y., 145
yu, z., 291
yuret, d., 268, 294, 509

zampieri, m., 80
zapirain, b., 376
zavrel, j., 267
zelle, j. m., 413
zeman, d., 155, 173, 208,

272, 291, 293

zettlemoyer, l., 354, 367,
375, 413, 415, 419

zhai, c., 80
zhang, j., 354, 409, 419
zhang, l., 80, 421
zhang, m., 354
zhang, y., 283, 294, 368
zhao, h., 375
zhao, j., 126
zheng, x., 145
zhong, z., 368, 506
zhou, b., 338, 354
zhou, d., 397
zhou, g., 354, 375
zhou, j., 375, 429
zhou, m., 429
zhu, h., 333
zhu, x., 397, 514
zielinska, v., 361
zik  anov  a,   s., 274, 293
zou, j., 126
zou, j. y., 125, 126
zue, v. w., 441, 442
zweig, g., 124, 127, 436
zwicky, a., 298

subject index

   -reduction, 308
*?, 15
+?, 15
f-measure, 263
10-fold cross-validation, 76
    (derives), 196
  , 65, 482
* (re kleene *), 13
+ (re kleene +), 13
. (re any character), 13
$ (re end-of-line), 13
( (re precedence symbol),

[ (re character

disjunction), 12

\b (re non

word-boundary), 14
\b (re word-boundary),

] (re character

disjunction), 12
   (re start-of-line), 13
[  ] (single-char negation),

14

14

12

    (there exists), 306
    (for all), 306
=    (implies), 309
   -expressions, 308
   -reduction, 308
    (and), 306
   (not), 306
    (or), 309
4-gram, 42
4-tuple, 198
5-gram, 42

aave, 21
abduction, 311
abox, 316
absity, 513
absolute discounting, 53
absolute temporal

expression, 345

abstract word, 382
accomplishment

expressions, 315

accuracy

in wsd, 506

achievement expressions,

315, 316

acknowledgment speech

act, 448
activation, 132
activity expressions, 315,

315
adaptation

language model, 61

add-k, 51
add-one smoothing, 49
adjacency pairs, 451
adjective, 152, 203
adjective phrase, 203
adjunction in tag, 221
adverb, 152

days of the week coded
as noun instead of,
153

degree, 153
directional, 153
locative, 153
manner, 153
syntactic position of, 203
temporal, 153

adversarial evaluation, 430
affective, 378
af   x, 26
agent, as thematic role, 357
agglomerative id91,

512

algol, 220
algorithm

cky, 225
corpus lesk, 507, 508
extended gloss overlap,

502

extended lesk, 502
forward, 470
forward-backward, 478
inside-outside, 245
jiang-conrath word
similarity, 502

kneser-ney discounting,

lesk, 507
lin word similarity, 502
minimum id153,

53

33

id165 tiling for question

answering, 408

naive bayes classi   er, 65
path-length based

similarity, 500

pointwise mutual

information, 116
probabilistic cky, 243
resnik word similarity,

501

365

id14,

simpli   ed lesk, 507
soundex, 491
unsupervised word sense
disambiguation, 512

viterbi, 161, 471
yarowsky, 510

alignment, 30

minimum cost, 32
string, 30
via minimum edit
distance, 32

all-words task in wsd, 505
allen relations, 349
ambiguity

amount of part-of-speech

in brown corpus,
156

attachment, 224

coordination, 224, 225,

take the    rst sense, 507

248

in meaning

representations, 298

part-of-speech, 156
pid18 in, 240
prepositional phrase
attachment, 246
resolution of tag, 156
tests distinguishing from

vagueness, 298

word sense, 504

american structuralism,

220

anchor texts, 417
anchors in regular

expressions, 13, 34

answer type, 404
answer type taxonomy, 404
antonym, 103
any-of, 75
ap, 203
approximate

randomization, 78

arc, 419
arc eager, 284
arc standard, 277
argmax, 482
aristotle, 151, 315
arity, 312
article (part-of-speech), 153
aspect, 315
aspell, 490
asr

con   dence, 455

association, 104
atis, 194

corpus, 197, 200

atn, 375
atrans, 373
attachment ambiguity, 224
augmentative

communication, 38
authorship attribution, 63
autocorrect, 488
auxiliary verb, 154

backchannel, 449
backoff

in smoothing, 51

backprop, 142
backtrace, 473

in minimum edit
distance, 32

backus-naur form, 195
backward chaining, 310
backward composition, 216
backward id203, 474
bag of word, 505
bag of words, 65, 66
bag-of-words, 65
barge-in, 441
baseline

most frequent sense, 507

basic emotions, 380
bayes    rule, 65, 482

dropping denominator,

66, 160, 482

bayesian id136, 65, 482
bdi, 460
id125, 285
id125, 165
beam width, 166, 285
berkeley restaurant

project, 41

bernoulli naive bayes, 80
bi-lstm, 329
bias term, 84, 132
bidirectional id56, 187
bigram, 39
binary branching, 213
binary nb, 70
binary tree, 213
bits for measuring id178,

bloom    lters, 56
bnf (backus-naur form),

57

195

bootstrap, 79
bootstrap algorithm, 79
bootstrap test, 78
id64, 78, 510

for wsd, 510
generating seeds, 511
in ie, 339

bpe, 27
bracketed notation, 197
british national corpus

(bnc)

pos tags for phrases,

155
brown, 155
brown corpus, 19

original tagging of, 174

byte-pair encoding, 27

candidates, 481
canonical form, 299
captalization

for unknown words, 167

capture group, 18
cardinal number, 203
cascade, 27

regular expression in

eliza, 18

case

sensitivity in regular

expression search,
11

case folding, 24
case frame, 359, 374
categorial grammar, 214,

214

cd (conceptual

centroid, 115

dependency), 373

551

552

subject index

id18, see context-free

grammar

chain rule, 98, 142
channel model, 482, 483
character embeddings, 332
charniak parser, 250
chatbots, 425
chatbots, 10
chinese

id40, 25

chomsky normal form,

213, 242

chomsky-adjunction, 214
chunking, 231, 232
circus, 354
citation form, 102
cky algorithm, 223
probabilistic, 243

clari   cation questions, 458
class-based id165, 61
clause, 201
clitic, 24

origin of term, 151

closed class, 152
closed vocabulary, 48
id91

in word sense

disambiguation, 514
cnf, see chomsky normal

form

coarse senses, 514
coca, 483
cocke-kasami-younger

algorithm, see cky

code switching, 21
collaborative completion,

448

collins parser, 250
collocation, 505
combinatory categorial

grammar, 214

commissive speech act, 448
common ground, 448, 459
common nouns, 152
complement, 206, 206
complementizer, 153
completeness in fol, 311
componential analysis, 372
computational grammar
coder (cgc), 174
computational semantics,

296

concatenation, 34
conceptual dependency, 373
concordance, semantic, 505
concrete word, 382
conditional independence,

266
con   dence

asr, 455
in id36, 340

con   dence values, 340
con   guration, 275
confusion matrix

in id147,

484

confusion sets, 489
conjoined phrase, 207

conjunction, 153
conjunctions, 207

as closed class, 153

connectionist, 150
connotation frame, 395
connotation frames, 376
connotations, 105, 379
consistent, 238
constants in fol, 305
constative speech act, 448
constituency, 194

evidence for, 195

constituent, 194
constraint grammar, 293
construction grammar, 220
content planning, 456
context embedding, 122
context-free grammar, 194,

195, 198, 219

chomsky normal form,

invention of, 220
multiplying probabilities,

240, 266

non-terminal symbol,

213

196

productions, 195
rules, 195
terminal symbol, 196
weak and strong

equivalence, 213

contingency table, 73
continuer, 449
conversation, 422
conversational agents, 422
conversational analysis, 451
convex, 90
coordinate noun phrase,

coordination ambiguity,

207

225, 248

copula, 154
corpora, 19

coca, 483

corpus, 19

atis, 197
bnc, 155
brown, 19, 174
lob, 174
regular expression

searching inside, 11

switchboard, 19
timebank, 349
corpus lesk, 508
corpus of contemporary

english, 483

correction act detection,

cosine

as a similarity metric,

453

112

cost function, 87
count nouns, 152
counters, 34
counts

treating low as zero, 170

crf, 172
cross id178 loss, 88, 141

cross-brackets, 263
cross-id178, 58
cross-validation, 76

10-fold, 76

id104, 382
currying, 308

damerau-levenshtein, 483
date

fully quali   ed, 347
id172, 435
dative alternation, 359
decision boundary, 85, 135
decision tree

use in wsd, 514
declarative sentence
structure, 200

decoder, 471, 471
decoding, 160, 471
viterbi, 160, 471

deduction

in fol, 310

deduplication, 491
deep, 131
deep learning, 131
deep role, 357
degree, 510
degree adverb, 153
deleted interpolation, 165
delexicalized, 457
denotation, 301
dependency

grammar, 270
lexical, 248

dependency tree, 273
dependent, 271
derivation

direct (in a formal

language), 199

syntactic, 196, 196, 199,

199

description logics, 316
det, 196
determiner, 153, 196, 202
development test set, 76
development test set
(dev-test), 44

devset, see development

test set (dev-test), 76

dialog, 422
dialog act, 447, 450

acknowledgment, 449
backchannel, 449
continuer, 449
correction, 453

dialog manager
design, 442

dialog policy, 454
id71, 422

design, 442
evaluation, 441

diathesis alternation, 359
diff program, 35
dimension, 108
diphthong

origin of term, 151

direct derivation (in a

formal language),
199

directional adverb, 153
directive speech act, 448
disambiguation

pid18s for, 239
role of probabilistic

parsing, 237

syntactic, 225
via pid18, 240

discount, 49, 51, 52
discounting, 49
discovery procedure, 220
discriminative model, 83
dis   uency, 20
disjunction, 34

pipe in regular

expressions as, 14
square braces in regular

expression as, 12

dispreferred response, 445
distance, 254
cosine, 112

distant supervision, 341
distributional hypothesis,

distributional similarity,

101

220

document frequency, 113
document vector, 115
domain, 301
domain classi   cation, 434
domain ontology, 430
domination in syntax, 196
dot product, 84, 111
dropout, 145
duration

temporal expression, 345
id145, 31

and parsing, 225
forward algorithm as,

468

history, 479
viterbi as, 161, 471

e-step (expectation step) in

earnest, the importance of

em, 478

being, 480

earnesty, importance, 480
edge-factored, 286
id153

minimum, 31

eliza, 10

implementation, 18
sample conversation, 18

elman networks, 178
em

baum-welch as, 474
e-step, 478
for deleted interpolation,

for id147,

inside-outside in parsing,

52

484

245

m-step, 478

embedded verb, 204
embeddings, 107
character, 332
cosine for similarity, 111
glove, 119
skip-gram, learning, 121
sparse, 110
tf-idf, 113
id97, 119

emission probabilities, 158,

465

emolex, 381
emotion, 379
empty category, 201
english

simpli   ed grammar

rules, 197
entity linking, 414
id178, 57

and perplexity, 57
cross-id178, 58
per-word, 58
rate, 58
relative, 371

error id26, 142
error model, 484
euclidean distance

in l2 id173, 94

eugene onegin, 60, 479
evalb, 264
evaluating parsers, 263
evaluation

10-fold cross-validation,

76

76

comparing models, 45
cross-validation, 76
development test set, 44,

devset, 76
devset or development

test set, 44

id71, 441
extrinsic, 43, 506
most frequent class
baseline, 156

id39,

333

of id165, 43
of id165s via

perplexity, 44
pseudoword, 372
id36, 344
test set, 43
training on the test set, 43
training set, 43
unsupervised wsd, 512
word similarity, 503
wsd systems, 506
event extraction, 327
event extraction, 348
event variable, 312
events

representation of, 311

existential there, 154
expansion, 197, 200
expectation step, 245
expectation step in em, 478

expectation-maximization,

relation to inside-outside,

see em

explicit con   rmation, 454
expressiveness, of a

meaning
representation, 300
extended gloss overlap, 502
extended lesk, 502
extended lesk, 502
extrinsic, 506
extrinsic evaluation, 43

f (for f-measure), 74, 234,

263

f-measure, 74, 234

in ner, 333

factoid question, 402
false negatives, 15
false positives, 15
fasttext, 129
fastus, 352
feature cutoff, 170
feature interactions, 87
feature selection, 79

information gain, 79

feature template, 282
feature templates, 87

part-of-speech tagging,

169

federalist papers, 80
feed-forward network, 137
   lled pause, 20
   ller, 20
   nal lowering, 453
first order logic, see fol
   rst-order co-occurrence,

124
focus, 416
fol, 296, 304

    (there exists), 306
    (for all), 306
=    (implies), 309
    (and), 306, 309
   (not), 306, 309
    (or), 309
and veri   ability, 304
constants, 305
expressiveness of, 300,

304

functions, 305
id136 in, 304
terms, 304
variables, 305

fold (in cross-validation),

76

food in nlp

ice cream, 466

formal language, 198
forward algorithm, 468, 469
forward algorithm,

forward chaining, 310
forward composition, 216
forward trellis, 468
forward-backward

algorithm, 474, 479
backward id203 in,

470

474

245

forward-backward

algorithm, 478

fosler, e., see

fosler-lussier, e.

fragment of word, 20
frame

semantic, 362

frame elements, 362
framenet, 362
frames, 431
free word order, 270
freebase, 336
frump, 354
fully quali   ed date

expressions, 347

fully-connected, 137
function word, 152, 173
functional grammar, 221
functions in fol, 305

garden-path sentences, 265,

267

gaussian

prior on weights, 95

gazetteer, 331
general inquirer, 71, 381
generalize, 94
generalized semantic role,

360
generation

of sentences to test a

id18 grammar, 197

template-based, 438

generative grammar, 198
generative lexicon, 514
generative model, 83
generative syntax, 221
generator, 196
genitive np, 222
gerundive postmodi   er, 203
gilbert and sullivan, 327,

480
gloss, 497
godzilla, speaker as, 368
gold labels, 73
the gondoliers, 480
good-turing, 53
government and binding,

220
gradient, 90
grammar

constraint, 293
construction, 220
government and
binding, 220

head-driven phrase

structure (hpsg),
211, 220

lexical-functional

(lfg), 220

link, 293
probabilistic tree

adjoining, 267
tree adjoining, 221

grammar

binary branching, 213

subject index

553

categorial, 214, 214
id35, 214
checking, 223
combinatory categorial,

214

equivalence, 213
generative, 198
strong equivalence, 213
weak equivalence, 213

grammar rock, 151
grammatical function, 271
grammatical relation, 271
grammatical sentences, 198
greedy, 170
greedy re patterns, 15
greeting, 154
grep, 11, 11, 34
ground, 448
grounding

   ve kinds of, 448

hamilton, alexander, 80
hanzi, 25
harmonic mean, 75, 234,

263

hays, d., 293
head, 211, 271
   nding, 211
in lexicalized grammar,

250
tag, 250

head tag, 250
head-driven phrase

structure grammar
(hpsg), 211, 220

heaps    law, 20
hector corpus, 505
held out, 43
held-out, 52
herdan   s law, 20
hidden, 158, 465
hidden layer, 137

as representation of

input, 138
hidden units, 137
id48, 158, 465

deleted interpolation, 165
formal de   nition of, 158,

initial distribution, 158,

465

465

observation likelihood,

158, 465

observations, 158, 465
simplifying assumptions

for id52,
160

states, 158, 465
transition probabilities,

158, 465

trigram id52, 163

holonym, 496
homographs, 493
homonym, 493
homonymy, 493
homophones, 494
human parsing, 264

554

subject index

human sentence processing,

264, 264

hungarian

part-of-speech tagging,

172

501

hyperarticulation, 453
hypernym, 104, 336, 496

and information content,

in extended lesk, 503
lexico-syntactic patterns

for, 336

hyperparameter, 145
hyponym, 104, 496

ibm, 60
ibm thomas j. watson

research center, 60

ice cream, 466
idf, 508
idf, 114
idf term weighting, 114
if then reasoning in fol,

immediately dominates,

310

196

imperative sentence
structure, 200
implicit argument, 376
implicit con   rmation, 454
implied hierarchy

in description logics, 320

inde   nite article, 202
indirect speech acts, 452
id136, 300
in fol, 310

id136-based learning,

in   nitives, 206
infoboxes, 335
information extraction (ie),

291

327

id64, 339
partial parsing for, 231

information gain, 79

for feature selection, 79
information retrieval, 109
information-content word

similarity, 500

initiative, 432
mixed, 433
single, 432
system, 432

inner product, 111
inside-outside algorithm,

245, 266

instance checking, 319
intent determination, 434
intercept, 84
interjection, 154
intermediate semantic

representations, 298

internal rule in a id18

parse, 251

interpersonal stance, 393
interpolated kneser-ney

discounting, 53, 55

interpolation

in smoothing, 51

interpretable, 97
interpretation, 302
intonation, 453
intransitive verbs, 206
intrinsic evaluation, 43
inverse document

frequency, 508

iob, 232, 330, 436
iob tagging

for ner, 330
for temporal expressions,

345

slot    lling, 436

iolanthe, 480
ir

idf term weighting, 114
vector space model, 108

is-a, 105
is-a, 336
iso 8601, 346
isrl, 376

jaro-winkler, 491
jay, john, 80
jiang-conrath distance, 502
joint intention, 459
joint id203, 239

katz backoff, 52
kbp, 354
kenlm, 56, 61
kl divergence, 371
kl-one, 323
kleene *, 13

sneakiness of matching

zero things, 13

kleene +, 13
kneser-ney discounting, 53
knowledge base, 297, 299
krl, 323
kullback-leibler

divergence, 371

l1 id173, 94
l2 id173, 94
label bias, 171
labeled precision, 263
labeled recall, 263
lambda notation, 308
language generation, 438
language id, 72
language id, 63
language model, 38

adaptation, 61
pid18, 241

laplace smoothing, 49
laplace smoothing:for

pmi, 118

lasso regression, 94
latent semantic analysis,

128

lcs, 501
ldc, 24, 243
learning rate, 90
lemma, 20, 102

versus wordform, 20

lemmatization, 11
lesk algorithm, 507

corpus, 508
extended, 502
simpli   ed, 507

letter-to-sound

for spell checking, 491
levenshtein distance, 30
lexical

ambiguity resolution,

513

category, 196
database, 497
dependency, 237, 248
head, 267
semantics, 102
trigger, in ie, 345

lexical answer type, 416
lexical dependency, 248
lexical rule

in a id18 parse, 251
lexical sample task in

wsd, 504
lexical-functional

grammar (lfg),
220

lexicalized grammar, 250
lexico-syntactic pattern,

336
lexicon, 196
likelihood, 66, 482
lin similarity, 502
linear classi   ers, 67
linear interpolation for

id165s, 52

linearly separable, 135
linguistic data

consortium, 24, 243

link grammar, 293
literal meaning, 296
liwc, 71, 382
lm, 38
lob corpus, 174
locative, 153
locative adverb, 153
log

why used for

probabilities, 43
log likelihood ratio, 390
log odds ratio, 390
log probabilities, 43, 43
logical connectives, 305
logical vocabulary, 301
logistic function, 84
id28, 82

conditional maximum

likelihood
estimation, 88

learning in, 87
relation to neural

networks, 139

long-distance dependency,

208

traces in the penn

treebank, 208
wh-questions, 201
lookahead in re, 19
loss, 87

lowest common subsumer,

501

lsi, see latent semantic

analysis

lstm, 354

for ner, 332
for slot    lling, 436
for srl, 366

lunar, 421
lunar, 322

m-step (maximization step)

in em, 478
machine learning
for ner, 333
for wsd, 505
textbooks, 81, 100
macroaveraging, 75
madison, james, 80
manhattan distance

in l1 id173, 94

manner adverb, 153
marker passing for wsd,

513
markov, 40

assumption, 40

markov assumption, 157,

464

465

465

markov chain, 60, 157, 464
formal de   nition of, 158,

initial distribution, 158,

id165 as, 158, 465
states, 158, 465
transition probabilities,

158, 465

markov model, 40

formal de   nition of, 158,

465

history, 60
marx, g., 223
mass nouns, 152
maxent

gaussian priors, 95
id173, 95

maxent, 100
maximization step, 245
maximization step in em,

maximum id178, 99
maximum matching, 25
maximum spanning tree,

maxmatch, 25
mctest, 419
mean reciprocal rank, 418
meaning representation,

478

287

295

as set of symbols, 296
early uses, 322
languages, 297

meaning representation
languages, 295

mechanical indexing, 128
memm, 168

compared to id48, 168

id136 (decoding),

171

learning, 171
viterbi decoding, 171

meronym, 496
meronymy, 496
mesh (medical subject

headings), 64, 504

message understanding
conference, 352

metarule, 207
metonymy, 494
micro-planner, 322
microaveraging, 75
minibatch, 92
minimum id153, 30,

30, 31, 161, 471

example of, 33

minimum id153,

33

mixed initiative, 433
id113

for id165s, 40
for id165s, intuition, 41

mlp, 137
modal verb, 154
model, 301
modi   ed kneser-ney, 55
modus ponens, 310
montague semantics, 323
morpheme, 26
moses, michelangelo statue

of, 422

most frequent sense, 507
mrr, 418
muc, 352, 354
multi-label classi   cation,

multi-layer id88s,

multinomial classi   cation,

multinomial naive bayes,

75

137

75

65

multinomial naive bayes

classi   er, 65
multinominal logistic
regression, 95

n-best list, 435
id165

465

id165, 38, 40

as markov chain, 158,

absolute discounting, 53
adaptation, 61
add-one smoothing, 49
as approximation, 40
as generators, 46
equation for, 40
example of, 42
for shakespeare, 46
history of, 60
interpolation, 51
katz backoff, 52
kenlm, 56, 61
kneser-ney discounting,

53

logprobs in, 43
normalizing, 41
parameter estimation, 41
sensitivity to corpus, 45
smoothing, 49
srilm, 61
test set, 43
training set, 43
unknown words, 48

id165

tiling, 408
naive bayes

multinomial, 65
simplifying assumptions,

66

65

naive bayes assumption, 66
naive bayes classi   er

use in text categorization,

named entity, 328
list of types, 329
recognition, 327, 329

id39,

184

names

and gazetteers, 331
census lists, 331
narrativeqa, 419
negative log likelihood loss,

negative part-of-speech,

141

154

neo-davidsonian, 312
ner, 327
neural nets, 61
neural networks

relation to logistic

regression, 139
newline character, 17
id87

for spelling, 481
invention of, 492

noisy-or, 340
nombank, 361
nominal, 196
non-capturing group, 18
non-   nite postmodi   er, 203
non-greedy, 15
non-logical vocabulary, 301
non-terminal symbols, 196,

197

normal form, 213, 213
id172
dates, 435
temporal, 346
word, 23

id172 of

probabilities, 40

normalized, 328
normalizing, 139
noun, 152

abstract, 152, 202
common, 152
count, 152
days of the week coded

as, 153

mass, 152, 202
proper, 152

noun phrase, 194

constituents, 196

np, 196, 197
np attachment, 246
null hypothesis, 77
numerals

as closed class, 153

object, syntactic

frequency of pronouns

as, 245

observation bias, 171
observation likelihood
role in forward, 469
role in viterbi, 162, 472

ocr, 492
old information, and word

order, 246

on-line sentence-processing

experiments, 267
one sense per collocation,

511

one-hot vector, 146
one-of, 75
ontology, 316
ontonotes, 514
oov (out of vocabulary)

words, 48

oov rate, 48
open class, 152
open information

extraction, 342
open vocabulary system
unknown words in, 48

operation list, 30
operator precedence, 14, 14
optical character

recognition, 492

optionality

of determiners, 202
use of ? in regular

expressions for, 12

ordinal number, 203
over   tting, 94

parallel distributed

processing, 150
parent annotation, 249
parse tree, 196, 199
parsed corpus, 266
parsing

ambiguity, 223
chunking, 231
cky, 226, 243
cyk, see cky
evaluation, 263
history, 235
partial, 231
probabilistic cky, 243
relation to grammars,

200

shallow, 231
syntactic, 223
well-formed substring

table, 235

part-of-speech

adjective, 152
adverb, 152

subject index

555

as used in id18, 196
closed class, 152, 153
greeting, 154
interjection, 154
negative, 154
noun, 152
open class, 152
particle, 153
subtle distinction

between verb and
noun, 152

usefulness of, 151
verb, 152

part-of-speech tagger

parts, 174
taggit, 174

part-of-speech tagging, 156
part-of-speech tagging
ambiguity and, 156
amount of ambiguity in
brown corpus, 156

and morphological
analysis, 172
capitalization, 167
feature templates, 169
for phrases, 155
history of, 174
hungarian, 172
stanford tagger, 172
state of the art, 157
turkish, 172
unknown words, 167

part-whole, 496
partial parsing, 231
particle, 153
parts tagger, 174
parts-of-speech, 151
passage retrieval, 405
passages, 405
path-length based

similarity, 500

pattern, regular expression,

11
pid18, 238

for disambiguation, 239
lack of lexical sensitivity,

246

lexicalized, 267
parse id203, 239
poor independence

assumption, 245
rule probabilities, 238
use in language

modeling, 241

pdp, 150
id32, 208

for statistical parsing,

243

155

pos tags for phrases,

tagging accuracy, 157
tagset, 154, 154

id32

id121, 24

per-word id178, 58
id88, 134
perplexity, 44, 59

556

subject index

as weighted average

branching factor, 44

de   ned via

cross-id178, 59

personal pronoun, 153
personality, 392
personalized page rank, 510
phones

in spell checking, 491

phrasal verb, 153
phrase-structure grammar,

195, 220

pipe, 14
the pirates of penzance,

327

planning

and speech acts, 460
shared plans, 459

plural, 202
pointwise mutual

information, 116

politeness marker, 154
polysemy, 494
porter stemmer, 26
pos, 151
possessive np, 222
possessive pronoun, 153
postdeterminer, 203
postmodi   er, 203
postposed constructions,

195

potts diagram, 388
pp, 197
ppmi, 116
pre-sequence, 451
precedence, 14
precedence, operator, 14
precision, 74
precision, 234
in ner, 333

predeterminer, 204
predicate, 206
predicate-argument

relations, 206

preference semantics, 513
preposed constructions, 195
prepositional phrase, 203

attachment, 246
constituency, 197
preposing, 195
prepositions, 153

as closed class, 153

pretraining, 146
primitive decomposition,

372

principle of contrast, 103
prior id203, 66, 482
probabilistic cky

algorithm, 242, 243

probabilistic parsing, 242

by humans, 264
productions, 195
progressive prompting, 455
projection layer, 147
prolog, 311
prompt, 439
prompts, 438
pronoun, 153

and old information, 246
as closed class, 153
personal, 153
possessive, 153
wh-, 153

propbank, 360
proper noun, 152
propositional meaning, 103
id144, 453
proto-agent, 360
proto-patient, 360
pseudoword, 372
ptag, 267
ptrans, 373
punctuation

for numbers

cross-linguistically,
24

for sentence

segmentation, 29

part-of-speech tags, 154
stripping before

part-of-speech
tagging, 156
id121, 24
treated as words, 19
treated as words in lm,

47

quac, 419
qualia structure, 514
quanti   er

as part of speech, 203
semantics, 306

query

reformulation in qa, 404

question

classi   cation, 404
factoid, 402

id53

evaluation, 418
factoid questions, 402
query reformulation in,

404

12

range, regular expression,

rapid reprompting, 455
rdf, 336
rdf triple, 336
re

regular expression, 11
reading comprehension,

409

reading time, 264
real-word spelling errors,

480
recall, 74
recall, 234

in ner, 333

recipe

meaning of, 295
reference point, 314
reformulation, 448
register in re, 18
regression
lasso, 94
ridge, 95

regular expression, 11, 34

substitutions, 17
id173, 94
rejection

conversation act, 455

relatedness, 104
id36, 327
relative

temporal expression, 345

relative id178, 371
relative frequency, 41
relative pronoun, 204
relexicalize, 457
relu, 133
reporting events, 349
representation learning, 101
resnik similarity, 501
resolution for id136,

311
resolve, 156
response generation, 428
restrictive grammar, 438
restrictive relative clause,

204
reverb, 342
reversives, 103
rewrite, 196
riau indonesian, 152
ridge regression, 95
role-   ller extraction, 352
row vector, 109
rules

context-free, 195
context-free, expansion,

196, 200

context-free, sample, 197

s as start symbol in id18,

196

sae, 21
sampling

used in id91, 512

saturated, 134
sch  onk   nkelization, 308
   schoolhouse rock   , 151
scisor, 354
sclite package, 35
script

schankian, 362

scripts, 351
second-order

co-occurrence, 124

seed pattern in ie, 339
seed tuples, 339
segmentation

chinese word, 25
maximum matching, 25
sentence, 29
word, 23

selectional association, 371
selectional preference
strength, 370

selectional preferences

pseudowords for

evaluation, 372

selectional restriction, 368
representing with events,

369

violations in wsd, 370

semantic analysis, 296
semantic concordance, 505
semantic drift in ie, 340
semantic feature, 128
semantic    eld, 104
semantic frame, 104
semantic grammars, 434
semantic network
for word sense

disambiguation, 513

semantic networks

origins, 322

semantic processing, 295
semantic relations in ie,

334
table, 335

360

semantic role, 357, 357,

id14, 364
semantics, 295
lexical, 102

sense

accuracy in wsd, 506
word, 493

senseval

and wsd evaluation, 506

senseval corpus, 505
sentence

segmentation, 29

sentence realization, 456
sentence segmentation, 11
sentence selection, 410
sentential complements,

205

sentiment, 105

origin of term, 397
id31, 63
sentiment lexicons, 71
sentiid138, 387
sequence model, 157
sgns, 119
shakespeare

id165 approximations

to, 46

shallow parse, 231
shared plans, 459
id132, 275
shrdlu, 322
side sequence, 451
sigmoid, 84, 132
similarity, 103
simple recurrent

networks, 178
simpli   ed lesk, 507
skip-gram, 119
slot    lling, 354, 434
slots, 431
smoothing, 49, 49

absolute discounting, 53
add-one, 49
discounting, 49
for id48 id52,

interpolation, 51
katz backoff, 52
kneser-ney discounting,

165

53

laplace, 49
linear interpolation, 52

snippets, 407
softmax, 95, 139
soundex, 491
spam detection, 63, 71
span, 407
speech acts, 447
spell checking

pronunciation, 491

id147

use of id165s in, 37
id147
algorithm, 483,
491

spelling errors

context-dependent, 480
correction, em, 484
detection, real words,

486

id87 for

correction, 483

non-word, 480
real word, 480

split, 248
split and merge, 250
squad, 409, 419
srilm, 61
srl, 364
stacked id56s, 186
stanford tagger, 172
start symbol, 196
state

semantic representation

of, 311

stationary stochastic
process, 58

statistical parsing, 242
stative expressions, 315
stem, 26
id30, 11
id30, 26
stop words, 68
strong equivalence of
grammars, 213

structural ambiguity, 223
stupid backoff, 56
subcategorization

and probabilistic

grammars, 237

tagsets for, 206

subcategorization frame,

206

examples, 206

subcategorize for, 206
subdialog, 451
subject, syntactic

frequency of pronouns

as, 245

in wh-questions, 201

subjectivity, 378, 397
substitutability, 220
substitution in tag, 221
substitution operator

(regular
expressions), 17
subsumption, 317, 319
superordinate, 104, 496

id55, 257
id55, 267
supervised machine
learning, 64

switchboard, 155
switchboard corpus, 19
synonyms, 103, 496
synset, 497
syntactic categories, 151
syntactic disambiguation,

225

syntactic movement, 208
syntax, 194

origin of term, 151
system-initiative, 432

tag, 221, 267
taggit, 174
tagset

difference between penn

treebank and
brown, 155
history of penn

treebank, 155

id32, 154, 154
table of id32

tags, 154

tanh, 133
target embedding, 122
tay, 443
tbox, 316
technai, 151
telic eventualities, 316
template    lling, 328, 351
template recognition, 351
template, in ie, 351
template-based generation,

438

temporal adverb, 153
temporal anchor, 348
temporal expression

absolute, 345
metaphor for, 315
recognition, 328
relative, 345

temporal expressions, 328
temporal logic, 313
temporal id172,

346

temporal reasoning, 323
tense logic, 313
term

id91, 513, 514
in fol, 304

term frequency, 113
term-document matrix, 108
term-term matrix, 110
terminal symbol, 196
terminology

in description logics, 316

test set, 43

development, 44
how to choose, 44
text categorization, 63

bag of words assumption,

65

naive bayes approach, 65
unknown words, 68

text id172, 10

part-of-speech tagging,

155
tf-idf, 114
thematic grid, 359
thematic role, 357

and diathesis alternation,

359

examples of, 358
problems, 359

theme, 357
theme, as thematic role, 357
there, existential in english,

154

thesaurus, 513
time, representation of, 312
timebank, 349
id121, 10
sentence, 29
word, 23

tokens, word, 20
topic (information

structure), 246

topic models, 104
trace, 201, 208
training oracle, 280
training set, 43

cross-validation, 76
how to choose, 44
transformations and

discourse analysis
project (tdap),
174

transition id203
role in forward, 469
role in viterbi, 162, 472

transitive verbs, 206
trec, 421
id34

(tag), 221

adjunction in, 221
probabilistic, 267
substitution in, 221

treebank, 208, 243
trigram, 42
truth-conditional semantics,

turkish

part-of-speech tagging,

303

172

turn correction ratio, 441
turns, 423
type raising, 216
typed dependency structure,

270

types

word, 20

ungrammatical sentences,

198

unique beginner, 498
unit production, 226
unit vector, 112
universal, 433
universal dependencies,

272

unix, 11
<unk>, 48

subject index

557

unknown words, 27

in id165s, 48
in part-of-speech
tagging, 167

in text categorization, 68

user-centered design, 442
utterance, 19

v (vocabulary), 482
vagueness, 298

tests distinguishing from

ambiguity, 298

variable, 305

existentially quanti   ed,

307

307

universally quanti   ed,

variables in fol, 305
vector, 108, 132
vector length, 111
vector semantics, 101, 106
vector space, 108
vector space model, 108
verb

copula, 154
modal, 154
phrasal, 153

verb alternations, 359
verb phrase, 196, 205
verbs, 152
veri   ability, 297
viterbi algorithm, 31, 161,

471

backtrace in, 473
decoding in memm, 171
history of, 479

viterbi algorithm,

161, 472

voice user interface, 442
voicexml, 438
vp attachment, 246

weak equivalence of

grammars, 213

web ontology language,

webquestions, 419
well-formed substring

table, 235

wfst, 235
wh-non-subject-question,

321

201

wh-phrase, 201, 201
wh-pronoun, 153
wh-subject-questions, 201
wh-word, 201
wikiqa, 419
wildcard, regular

expression, 13

wizard-of-oz system, 442
word

boundary, regular

expression notation,
14

closed class, 152
de   nition of, 19
fragment, 20
function, 152, 173

558

subject index

open class, 152
punctuation as, 19
tokens, 20
types, 20

word error rate, 26
word id172, 23
id40, 23
word sense, 493
id51,

504, see wsd

word sense induction, 511
word shape, 169, 331
word id121, 23

word-word matrix, 110
id97, 119
wordform, 20

and lemma, 102
versus lemma, 20
id138, 497, 497
world knowledge, 295
wsd, 504

ai-oriented efforts, 513
all-words task, 505
id64, 510, 514
decision tree approach,

514

evaluation of, 506
history, 513
history of, 514
lexical sample task, 504
neural network

approaches, 513
robust approach, 513
supervised machine

learning, 514

unsupervised machine

learning, 511

wsi, 511
wsj, 155

x-bar schemata, 220

yarowsky algorithm, 510
yes-no questions, 200, 452
yield, 240
yonkers racetrack, 57

zero-width, 19
zeros, 48
zeugma, 495

