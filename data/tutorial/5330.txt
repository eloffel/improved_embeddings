learning from data for linguists

lecture 2: evaluation and naive bayes

malvina nissim and johannes bjerva
m.nissim@rug.nl, j.bjerva@rug.nl

esslli 2016, 23 august 2016

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

1 / 34

evaluation

evaluation of results

evaluation

    is the system really able to generalise?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

3 / 34

evaluation of results

evaluation

    is the system really able to generalise?

the test set is equipped with class labels, manually assigned
(gold standard)

for each instance in the test set, we compare the class predicted by
the classi   er with the class speci   ed in the gold standard

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

3 / 34

evaluation

have to predict: steal or boil?

trying,to,upon,the,?
began,to,partners,and,?
them,to,workers,or,?
pate,or,it,in,?
gently,and,spoonfuls,of,?
let,them,for,3,?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

4 / 34

evaluation

you have your gold standard:

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil
let,them,for,3,boil

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

5 / 34

evaluation

you compare:

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

6 / 34

evaluation

how do we measure performance?

when is a model good enough?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

7 / 34

evaluation measures

evaluation

accuracy: percentage of correct decisions overall

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

8 / 34

evaluation measures

evaluation

accuracy: percentage of correct decisions overall

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

accuracy = ?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

8 / 34

evaluation measures

evaluation

accuracy: percentage of correct decisions overall

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

accuracy = 3/6 = 50%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

8 / 34

evaluation measures

evaluation

consider class    x   

true positive (tp): x classi   ed as x
true negative (tn):   x classi   ed as   x
false positive (fp):   x classi   ed as x
false negative (fn): x classi   ed as   x

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

8 / 34

evaluation

evaluation measures

consider class    x   

true positive (tp): x classi   ed as x
true negative (tn):   x classi   ed as   x
false positive (fp):   x classi   ed as x
false negative (fn): x classi   ed as   x

consider class    steal   

true positive (tp): steal classi   ed as steal
true negative (tn): boil classi   ed as boil
false positive (fp): boil classi   ed as steal
false negative (fn): steal classi   ed as boil

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

8 / 34

predic   on	
   gold	
   standard	
   tp	
   tn	
   steal	
   boil	
   steal	
   boil	
   steal	
   confusion	
   matrix	
   predic   on	
   gold	
   standard	
   tp	
   tn	
   fn	
   steal	
   boil	
   fp	
   steal	
   boil	
   steal	
   confusion	
   matrix	
   evaluation measures

evaluation

precisionx :
correct decisions over instances assigned to class    x   
tp/(tp + fp)

recallx :
correct assignments to class    x    over all instances of class    x    in
test set
tp/(tp + fn)

f-scorex :
combined measure of precision and recall
f = 2pr
p+r

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

9 / 34

evaluation measures

evaluation

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

precisionsteal = ?
recallsteal = ?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

9 / 34

evaluation measures

evaluation

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

precisionsteal = 1/2 = 50%
recallsteal =

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

9 / 34

evaluation measures

evaluation

gold

prediction

trying,to,upon,the,steal
began,to,partners,and,steal
them,to,workers,or,steal
pate,or,it,in,boil
gently,and,spoonfuls,of,boil gently,and,spoonfuls,of,steal
let,them,for,3,boil

trying,to,upon,the,steal
began,to,partners,and,boil
them,to,workers,or,boil
pate,or,it,in,boil

let,them,for,3,boil

precisionsteal = 1/2 = 50%
recallsteal = 1/3 = 33%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

9 / 34

evaluation measures

evaluation

what is good enough?

upperbound: inter-annotator agreement

baseline: performance of basic, simple model
for example: assignment of most frequent class in data set

sense1 9/10 and sense2 1/10
sense1 6/10 and sense2 4/10

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

9 / 34

what happens in learning, then?

evaluation

the learning algorithm observes given examples
it tries to    nd common patterns that explain the data: it tries to
generalise so that predictions can be made for new examples
exactly how this is done depends on what algorithm we are using

keywords here:

given/new examples

the settings of a learning experiment are important

generalising

what does it mean to generalise well?

algorithm we are using

we are going to see one now

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

10 / 34

naive bayes

naive bayes classi   cation

naive bayes

simple classi   cation method based on bayes rule

relies on a simple representation of documents: bag of words

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

12 / 34

naive bayes classi   cation

naive bayes

simple classi   cation method based on bayes rule

relies on a simple representation of documents: bag of words

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

12 / 34

naive bayes classi   cation

naive bayes

simple classi   cation method based on bayes rule

relies on a simple representation of documents: bag of words

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

12 / 34

id155

naive bayes

id155 of an event: a id203 obtained with the
additional information that some other event has already occurred
new information is used to revise the id203 of the initial event
prior vs posterior id203

prior: id203 obtained    as things stand   , before any additional
information is acquired
posterior: id203 value which has been revised by using additional
information

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

13 / 34

example

naive bayes

in a corpus: happy tweets = 45% ; sad tweets = 55%

i select one instance, how probable is it to be happy?

the tweet contains the word    cheerful   .
   cheerful    occurs in 70% of happy tweets,
and in 25% of sad tweets.
does the id203 now change?

which one is prior and which one posterior?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

14 / 34

example

naive bayes

in a corpus: happy tweets = 45% ; sad tweets = 55%

i select one instance, how probable is it to be happy?

the tweet contains the word    cheerful   .
   cheerful    occurs in 70% of happy tweets,
and in 25% of sad tweets.
does the id203 now change?

which one is prior and which one posterior?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

14 / 34

example

naive bayes

in a corpus: happy tweets = 45% ; sad tweets = 55%

i select one instance, how probable is it to be happy?

the tweet contains the word    cheerful   .
   cheerful    occurs in 70% of happy tweets,
and in 25% of sad tweets.
does the id203 now change?

which one is prior and which one posterior?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

14 / 34

bayes    theorem

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : a given class (happy)
i: a given instance (   i always feel cheerful on friday evening   )

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

15 / 34

bayes    theorem

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : a given class (happy)
i: a given instance (   i always feel cheerful on friday evening   )

p(cj|i) = pr of instance i being in class cj
how likely is it this given tweet from the corpus is happy?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

15 / 34

bayes    theorem

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : a given class (happy)
i: a given instance (   i always feel cheerful on friday evening   )

p(cj|i) = pr of instance i being in class cj
how likely is it this given tweet from the corpus is happy?
p(i|cj ) = (likelihood function) = pr of generating instance i given
class cj (happy)
given class cj (happy) how likely is it to get i?
true positive: 70%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

15 / 34

naive bayes

bayes    theorem
p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : a given class (happy)
i: a given instance (   i always feel cheerful on friday evening   )
p(cj|i) = pr of instance i being in class cj
how likely is it this given tweet from the corpus is happy?
p(i|cj ) = (likelihood function) = pr of generating instance i given
class cj (happy)
given class cj (happy) how likely is it to get i?
true positive: 70%
p(i|  cj ) = pr of generating instance i given   cj (sad)
given   cj (sad) how likely is it to get i?
false positive: 25%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

15 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )p(cj )

[p(cj )p(i|cj )] + [p(  cj )p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(cj ) =

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )    0.45

[0.45    p(i|cj )] + [p(  cj )p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(cj ) = 0.45

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example
p(cj|i) =

p(i|cj )    0.45

[0.45    p(i|cj )] + [p(  cj )p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(cj ) = 0.45
p(  cj ) =

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example
p(cj|i) =

p(i|cj )    0.45

[0.45    p(i|cj )] + [0.55    p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(cj ) = 0.45
p(  cj ) = 0.55

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )    0.45

[0.45    p(i|cj )] + [0.55    p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i? 70%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example
p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    p(i|  cj )]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i? 70%
true positive
p(i|  cj ) = given class   cj (sad) how likely is it to get i?

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example
p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    0.25]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )

happy = 45%

said = 55%

   cheerful    in 70% of happy tweets

   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i? 70%
true positive
p(i|  cj ) = given class   cj (sad) how likely is it to get i? 25%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example
p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    0.25]

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )
happy = 45%
said = 55%
   cheerful    in 70% of happy tweets
   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i? 70%
true positive
p(i|  cj ) = given class   cj (sad) how likely is it to get i? 25%
false positive

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example
p(cj|i) =

naive bayes

0.70    0.45

[0.45    0.70] + [0.55    0.25]

= 0.70

cj : happy
i:    i always feel cheerful on friday evening   
(represented by feature value    cheerful   )
happy = 45%
said = 55%
   cheerful    in 70% of happy tweets
   cheerful    in 25% of sad tweets
p(i|cj ) = given class cj (happy) how likely is it to get i? 70%
true positive
p(i|  cj ) = given class   cj (sad) how likely is it to get i? 25%
false positive

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    0.25]

= 0.70

prior id203 of i as happy = 0.45
posterior id203 of i as happy = 0.70

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    0.25]

= 0.70

prior id203 of i as happy = 0.45
posterior id203 of i as happy = 0.70

p(cj|i) =

p(i|cj )p(cj )

p(i)

p(i) = the id203 of i (   cheerful   ) overall

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

0.70    0.45

[0.45    0.70] + [0.55    0.25]

= 0.70

prior id203 of i as happy = 0.45
posterior id203 of i as happy = 0.70

p(cj|i) =

p(i|cj )p(cj )

p(i)

p(i) = the id203 of i (   cheerful   ) overall
70%    45% + 25%    55%

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )p(cj )

p(i)

how do we make a classi   er out of this?
we need to pick a class c out of a set of possible class values.

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

worked example

naive bayes

p(cj|i) =

p(i|cj )p(cj )

p(i)

how do we make a classi   er out of this?
we need to pick a class c out of a set of possible class values.

we add a decision rule: maximum a posteriori (map)

cmap = arg max

c   c

p(i|c)  p(c)

p(i)

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

naive bayes

worked example

p(cj|i) =

p(i|cj )p(cj )

p(i)

how do we make a classi   er out of this?
we need to pick a class c out of a set of possible class values.

we add a decision rule: maximum a posteriori (map)

cmap = arg max

c   c

p(i|c)  p(c)

p(i)

note that because we only need to compare values, we can drop the
denominator, which basically serves as normalising function

cmap = arg max

c   c

p(i|c)    p(c)

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

16 / 34

two issues

naive bayes

zeros and smoothing (laplace or add-one smoothing)

under   ow

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

17 / 34

two issues

naive bayes

zeros and smoothing (laplace or add-one smoothing)
it can happen that some values are zero. to prevent this problem in
the calculations, the value 1 is added to all observed counts

under   ow

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

17 / 34

two issues

naive bayes

zeros and smoothing (laplace or add-one smoothing)

under   ow
posterior probabilities are usually very very small, especially with lots
of features (think of a bag-of-words approach in text classi   cation).
this is called the under   ow problem
for this reason, most implementations of a nb classi   er use the log
of the probabilities

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

17 / 34

let   s get things into practice

practice and error issues

evaluation poll

http://etc.ch/ajyc

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

19 / 34

practice and error issues

evaluation poll -results

http://directpoll.com/r?
xdbzpbd3ixyqg8v6yio1k6seelegwl9oentt4ibki

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

20 / 34

practice and error issues

getting the updated code

approach 1: use git (updateable, recommended if you have git)

1

in your terminal, type:    git clone
https://github.com/bjerva/esslli-learning-from-data-students.git   

2 followed by    cd esslli-learning-from-data-students   
3 whenever the code is updated, type:    git pull   

approach 2: download a zip (static)

1 download the zip archive from: https://github.com/bjerva/

esslli-learning-from-data-students/archive/master.zip

2 whenever the code is updated, download the archive again.

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

21 / 34

practice and error issues

feature poll

http://etc.ch/mghf

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

22 / 34

practice and error issues

feature poll - results

http://directpoll.com/r?
xdbzpbd3ixyqg8rnz5rw6iasm92ukzxc2bhhswzy6

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

23 / 34

practice and error issues

running an experiment

1 navigate to your    esslli-learning-from-data-students    (using cd in the

terminal)

2 to extract features:

python feature extractor.py       csv data/trainset-sentiment.csv
      fname sentiment       nwords 1

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

24 / 34

practice and error issues

running an experiment

1 navigate to your    esslli-learning-from-data-students    (using cd in the

terminal)

2 to extract features:

python feature extractor.py       csv data/trainset-sentiment.csv
      fname sentiment       nwords 1
    this is a bag-of-word model!

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

24 / 34

practice and error issues

running an experiment

1 navigate to your    esslli-learning-from-data-students    (using cd in the

terminal)

2 to extract features:

python feature extractor.py       csv data/trainset-sentiment.csv
      fname sentiment       nwords 1
    this is a bag-of-word model!

3 to learn using these features:

python learn from data.py       npz sentiment.npz       algorithms nb

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

24 / 34

practice and error issues

adding features

1 navigate to your    esslli-learning-from-data-students    (using cd in the

terminal)

2 to extract features:

python feature extractor.py       csv data/trainset-sentiment.csv
      fname sentiment       nwords 1

3 to learn using these features:

python learn from data.py       npz sentiment.npz       algorithms nb

using more features

      nwords n, extract word ngrams of order n (e.g.       nwords 2)
      nchars n, extract character ngrams of order n (e.g.       nchars 3)
      features x y z, extract features with the given names (e.g.
      features gender-cat time-cat)

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

25 / 34

practice and error issues

end poll

//directpoll.com/r?xdbzpbd3ixyqg81lygsvosivclr6cnlre6kxm2m3

http:

malvina & johannes

lfd     lecture 2

esslli 2016, 23 august 2016

26 / 34

