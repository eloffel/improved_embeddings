learning from data for linguists

lecture 3: id90 and related issues

malvina nissim and johannes bjerva
m.nissim@rug.nl, j.bjerva@rug.nl

esslli, 24 august 2016

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

1 / 19

getting the updated code

start as soon as you enter the room! (please)

approach 1: use git (updateable, recommended if you have git)

1

in your terminal, type:    git clone
https://github.com/bjerva/esslli-learning-from-data-students.git   

2 followed by    cd esslli-learning-from-data-students   
3 whenever the code is updated, type:    git pull   

approach 2: download a zip (static)

1 download the zip archive from: https://github.com/bjerva/

esslli-learning-from-data-students/archive/master.zip

2 whenever the code is updated, download the archive again.

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

2 / 19

running an experiment

1 navigate to your    esslli-learning-from-data-students    (using cd in the

terminal)

2 to extract features and learn model:

python run experiment.py       csv data/trainset-sentiment-extra.csv
      nwords 1       algorithms nb

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

3 / 19

learning curves

over   tting

what is a learning curve?

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

4 / 19

over   tting

learning curves

what is a learning curve?

we can plot accuracy (or error rate) on one axis (y) and training size on
the other (x)

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

4 / 19

size and learning curve

over   tting

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

5 / 19

where do such scores come from?

another curve (two, actually)

over   tting

over   tting and under   tting

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

7 / 19

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   another curve (two, actually)

over   tting

over   tting and under   tting

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

7 / 19

over   tting and under   tting

over   tting

over   tting and under   tting

    a model is over   tting when it    ts the data too tightly, and it   s
modelling noise or error instead of generalising well
    how can we    nd this out?

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

8 / 19

over   tting and under   tting

over   tting

over   tting and under   tting

    a model is over   tting when it    ts the data too tightly, and it   s
modelling noise or error instead of generalising well
    how can we    nd this out?
observe error: learning curve on training vs test data

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

8 / 19

over   tting and under   tting

over   tting

over   tting and under   tting

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

8 / 19

over   &ng	
   poll

over   tting and under   tting

over   tting

over   tting and under   tting

what is a good metaphor for an over   tted model?

a lazy botanist: everything green is a tree
a picky botanist: nothing he hasn   t seen yet is a tree

what is a good metaphor for an under   tted model?

a lazy botanist: everything green is a tree
a picky botanist: nothing he hasn   t seen yet is a tree

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

10 / 19

id90

(slides by barbara rosario)

53 id90       decision tree is a classifier in the form of a tree structure, where each node is either:       decision node - specifies some test to be carried out on a single attribute-value, with one branch and sub-tree for each possible outcome of the test       leaf node - indicates the value of the target attribute (class) of examples       a decision tree can be used to classify an example by starting at the root of the tree and moving through it until a leaf node, which provides the classification of the instance.  54 training examples no strong high mild rain d14 yes weak normal hot overcast d13 yes strong high mild overcast d12 yes strong normal mild sunny d11 yes strong normal mild rain d10 yes weak normal cold sunny d9 no weak high mild sunny d8 yes weak normal cool overcast d7 no strong normal cool rain d6 yes weak normal cool rain d5 yes weak high mild rain  d4  yes weak high hot overcast d3 no strong high hot sunny d2 no weak high hot sunny d1 play tennis wind humidity temp. outlook day goal: learn when we can play tennis and when we cannot 55 decision tree for playtennis outlook sunny overcast rain humidity high normal wind strong weak no yes yes yes no www.math.tau.ac.il/~nin/ courses/ml04/decisiontreescls.pp 56 decision tree for playtennis outlook sunny overcast rain humidity high normal no yes each internal node tests an attribute each branch corresponds to an attribute value node each leaf node assigns a classification www.math.tau.ac.il/~nin/ courses/ml04/decisiontreescls.pp 57 no decision tree for playtennis outlook sunny overcast rain humidity high normal wind strong weak no yes yes yes no outlook temperature humidity wind    playtennis  sunny        hot                  high        weak     ? 61 building id90       once we have a decision tree, it is straightforward to use it to assign labels to new input values.        how we can build a decision tree that models a given training set? 62 building id90       the central focus of the decision tree growing algorithm is selecting which attribute to test at each node in the tree. the goal is to select the attribute that is most useful for classifying examples.        top-down, greedy search through the space of possible id90.       that is, it picks the best attribute and never looks back to reconsider earlier choices.  63 building id90       splitting criterion       finding the features and the values to split on        for example, why test first    cts    and not    vs   ?        why test on    cts < 2    and not    cts < 5    ?       split that gives us the maximum information gain (or the maximum reduction of uncertainty)       stopping criterion       when all the elements at one node have the same class, no need to split further       in practice, one first builds a large tree and then one prunes it back (to avoid overfitting)       see foundations of statistical natural language processing, manning and schuetze for a good introduction what did we say was the best split?

what did we say was the best split?

information gain is the mutual information between input attribute
a and target variable y
information gain is the expected reduction in id178 of target
variable y for data sample s, due to sorting on variable a

id178 and information gain

(feature)    cheerful    is found in

70% of positive tweets
25% of negative tweets

(feature)    emotion    is found in

40% of positive tweets
30% of negative tweets

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

13 / 19

1information gainwhich test is more informative?split over whether balance exceeds 50kover 50kless or equal 50kemployedunemployedsplit over whether applicant is employed2information gainimpurity/id178(informal)   measures the level of impurityin a group of examples3impurityvery impure groupless impure minimum impurity4id178: a common way to measure impurity   id178 = piis the id203 of class icompute it as the proportion of class i in the set.   id178 comes from id205.  the higher the id178 the more the information content.      iiipp2logwhat does that mean for learning from examples?16/30 are green circles; 14/30 are pink crosseslog2(16/30) =  -.9;       log2(14/30) =  -1.1 id178 = -(16/30)(-.9)    (14/30)(-1.1) = .99 52-class cases:   what is the id178 of a group in which all examples belong to the same class?   id178 = -1 log21 = 0   what is the id178 of a group with 50% in either class?   id178 = -0.5  log20.5    0.5  log20.5 =1minimum impuritymaximumimpuritynot a good training set for learninggood training set for learning6information gain   we want to determine which attributein a given set of training feature vectors is most usefulfor discriminating between the classes to be learned.   information gaintells us how important a given attribute of the feature vectors is.   we will use it to decide the ordering of attributes in the nodes of a decision tree.7calculating information gain996.03016log30163014log301422=                                                787.0174log1741713log171322=                                                entire population (30 instances)17 instances13 instances(weighted) average id178 of children=615.0391.03013787.03017=                     +                     information gain= 0.996 -0.615 = 0.38    for this split391.01312log1312131log13122=                                                information gain=    id178(parent)    [average id178(children)]parentid178childid178childid17810simple examplex      y      z           c11       1           i11       0           i0        0       1          ii1        0       0          iihow would you distinguish class i from class ii?training set: 3 features and 2 classes11x      y      z           c11       1           i11       0           i0        0       1          ii1        0       0          iisplit on attribute xi  iii iii iiiiieparent= 1 gain = 1    ( 3/4)(.9184)    (1/4)(0) = .3112x=1x=0echild2= 0echild1= -(1/3)log2(1/3)-(2/3)log2(2/3)= .5284 + .39=  .9184if x is the best attribute,this node would be further split.12x      y      z           c11       1           i11       0           i0        0       1          ii1        0       0          iisplit on attribute yi  iii iii  iiiiieparent= 1 gain = 1    (1/2) 0    (1/2)0 = 1; best oney=1x=0echild2= 0echild1= 013x      y      z           c11       1           i11       0           i0        0       1          ii1        0       0          iisplit on attribute zi  iii iii iiiiieparent= 1 gain = 1    ( 1/2)(1)    (1/2)(1) = 0    ie. no gain; worstz=1z=0echild2= 1echild1= 161 building id90       once we have a decision tree, it is straightforward to use it to assign labels to new input values.        how we can build a decision tree that models a given training set? id178 and information gain

pruning

grow decision tree to its entirety

trim the nodes of the decision tree in a bottom-up fashion

if generalisation error improves after trimming, replace sub-tree by a
leaf node

class label of leaf node is determined from majority class of instances
in the sub-tree

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

14 / 19

id178 and information gain

pruning

grow decision tree to its entirety

trim the nodes of the decision tree in a bottom-up fashion

if generalisation error improves after trimming, replace sub-tree by a
leaf node

class label of leaf node is determined from majority class of instances
in the sub-tree

for the experiments you can manipulate the following parameters:

maximum number of leaves

minimum number of samples per leaf

features (characters)

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

14 / 19

practice!

choose the best settings for a decision tree model in a

language identi   cation task

(let   s see a tree    rst)

id178 and information gain

practice with id90

options to play with:

      max-nodes n (only for dt algorithm, default = none)
      min-samples n (only for dt algorithm, default = 1)
      nchars n

visualisation options:

      cm (print confusion matrix + classi   cation report)
      plot (shows cm)

example run:
python run experiment.py --csv data/langident.csv --nchars
1 --algorithms dt --cm

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

16 / 19

re   ections and concepts

id178 and information gain

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

18 / 19

id178 and information gain

malvina & johannes

lfd     lecture 3

esslli, 24 august 2016

19 / 19

