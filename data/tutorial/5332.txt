learning from data

lecture 4: vector space models

k-nearest neighbor

(+ support vector machines)

malvina nissim and johannes bjerva
m.nissim@rug.nl, j.bjerva@rug.nl

esslli, 25 august 2016

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

1 / 26

getting the updated code

start as soon as you enter the room! (please)

approach 1: use git (updateable, recommended if you have git)

1

in your terminal, type:    git clone
https://github.com/bjerva/esslli-learning-from-data-students.git   

2 followed by    cd esslli-learning-from-data-students   
3 whenever the code is updated, type:    git pull   

approach 2: download a zip (static)

1 download the zip archive from: https://github.com/bjerva/

esslli-learning-from-data-students/archive/master.zip

2 whenever the code is updated, download the archive again.

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

2 / 26

re   ections and concepts

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

4 / 26

naive bayes vs decision tree

naive bayes is rather robust to irrelevant features: irrelevant features
cancel each other without a   ecting results
(id90 can heavily su   er from this.)

naive bayes is good in domains with many equally important features
(id90 su   er from fragmentation in such cases, especially
with little data)

most decision-tree algorithms only examine a single    eld at a time.
this leads to rectangular classi   cation boxes that may not correspond
well with the actual distribution of records in the decision space.

the fact that id90 require that features be checked in a
speci   c order limits their ability to exploit features that are relatively
independent of one another.
naive bayes overcomes this limitation by allowing all features to act
   in parallel.   

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

5 / 26

id90     strengths

id90 are able to generate understandable rules

id90 provide a clear indication of which features are most
important for prediction

(once available) id90 perform classi   cation without much
computation

they can be a great tool for understanding your dataset

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

6 / 26

id90     weaknesses

id90 are prone to errors in classi   cation problems with many
classes and relatively small number of training examples.

note: since each branch in the decision tree splits the training data, the
amount of training data available to train nodes lower in the tree can
become quite small.

id90 can be computationally expensive to train (need to
compare all possible splits)

id90 are very prone to over   tting

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

7 / 26

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

8 / 26

learning algorithm performance5over- and underfitting: classification exampleunderfitting/biasoverfitting/varianceunderfitting/bias   error on training set is high   simple hypothesis fails to generalize to new examplesoverfitting/variance   error on training set is low   complex hypothesis fails to generalize to new examplesdiscriminative vs generative models

fixes

high variance (over   tting)

get more training examples
reduce number of features
(prune tree)
id173

penalty for model complexity
aim at reducing training error while keeping validation error constant
(nb: cross-validation)
works well for lots of features where each contributes a little bit to
predicting y

high bias (under   tting)

get more features

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

9 / 26

discriminative vs generative models

generative vs discriminative

task: to determine the language that someone is speaking

generative approach: learn each language and determine to which
language the speech belongs to

discriminative approach: determine the linguistic di   erences without
learning any language

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

10 / 26

discriminative vs generative models

generative vs discriminative

task: to determine the language that someone is speaking

generative approach: learn each language and determine to which
language the speech belongs to

generative because it can simulate values of any variable in the model
example algorithm: naive bayes

discriminative approach: determine the linguistic di   erences without
learning any language

directly estimate posterior probabilities
no attempt to model underlying id203 distributions
example algorithm: decision tree, k-nearest neighbor

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

10 / 26

vector space model

vector space model

the representation of a set of documents as vectors in a common space

(parts of the following slides are based on slides by yannick parmentier)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

11 / 26

vector space model

vector space model

each term t of the dictionary is considered as a dimension

a document d can be represented by the weight of each vocabulary
term:

(cid:126)v (d) = (w (t1, d), w (t2, d), . . . , w (tn, d))

note that also raw frequency could be used (that   s what we are doing)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

12 / 26

vector space model

vector space model

each term t of the dictionary is considered as a dimension

a document d can be represented by the weight of each vocabulary
term:

(cid:126)v (d) = (w (t1, d), w (t2, d), . . . , w (tn, d))

note that also raw frequency could be used (that   s what we are doing)

representation of three documents
using term raw frequencies:
d1, d2, d3

a   ection
jealous
gossip

d1
115
10
2

d2
58
7
0

d3
20
11
6

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

12 / 26

vector space model

vector space model

each term t of the dictionary is considered as a dimension

a document d can be represented by the weight of each vocabulary
term:

(cid:126)v (d) = (w (t1, d), w (t2, d), . . . , w (tn, d))

note that also raw frequency could be used (that   s what we are doing)

representation of three documents
using term raw frequencies:
d1, d2, d3

a   ection
jealous
gossip

d1
115
10
2

d2
58
7
0

d3
20
11
6

question:
how do we compute the similarity between documents d1, d2, d3?

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

12 / 26

vector space model

vector id172 and similarity

similarity between vectors
    inner product (cid:126)v (d1)    (cid:126)v (d2)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

13 / 26

vector space model

vector id172 and similarity

similarity between vectors
    inner product (cid:126)v (d1)    (cid:126)v (d2)

but wait,    rst: what about the length of a vector?
longer documents will be represented with longer vectors, but that
does not mean they are more important

euclidian id172 (vector length id172):

(cid:126)v (d) =

(cid:126)v (d)
(cid:107) (cid:126)v (d)(cid:107)

where (cid:107) (cid:126)v (d)(cid:107) =

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

x 2
i

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

13 / 26

vector space model

vector id172 and similarity

similarity between vectors
    inner product (cid:126)v (d1)    (cid:126)v (d2)

similarity given by the cosine measure between normalized vectors:

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

13 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
let   s backtrack this:
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

euclidean length:

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

(cid:107) (cid:126)v (d)(cid:107) =

(cid:126)v 2

i (d)

i=1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

vocabulary
1: a   ection
2: jealous
3: gossip

d1
115
10
2

d2
58
7
0

d3
20
11
6

euclidean length:

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

(cid:107) (cid:126)v (d)(cid:107) =

(cid:126)v 2

i (d)

i=1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

euclidean length:

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

(cid:107) (cid:126)v (d)(cid:107) =

(cid:126)v 2

i (d)

vocabulary
1: a   ection
2: jealous
3: gossip
(cid:107) (cid:126)v (d1)(cid:107) =

d1
115
10
2

d2
58
7
0

d3
20
11
6

   
1152 + 102 + 22

i=1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

euclidean length:

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

(cid:107) (cid:126)v (d)(cid:107) =

(cid:126)v 2

i (d)

vocabulary
1: a   ection
2: jealous
3: gossip
(cid:107) (cid:126)v (d1)(cid:107) =
   
(cid:126)v (d11) =
   
   

(cid:126)v (d12) =

(cid:126)v (d13) =

d1
115
10
2

d2
58
7
0

d3
20
11
6

   
1152 + 102 + 22

115

10

1152+102+22 = 0.996
1152+102+22 = 0.087
1152+102+22 = 0.017

2

i=1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

vocabulary
1: a   ection
2: jealous
3: gossip

d1
115
10
2

d2
58
7
0

d3
20
11
6

(cid:107) (cid:126)v (d2)(cid:107) =

   
582 + 72 + 0

euclidean length:

(cid:107) (cid:126)v (d)(cid:107) =

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

(cid:126)v (d21) =

(cid:126)v (d22) =

58

   
7   

582+72+0

582+72+0

= 0.993

= 0.120

(cid:126)v 2

i (d)

(cid:126)v (d23) = 0

i=1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

normalising for length:

(cid:126)v (di ) =

(cid:126)v (di )
(cid:107) (cid:126)v (d)(cid:107)

vocabulary
1: a   ection
2: jealous
3: gossip

d1
115
10
2

d2
58
7
0

d3
20
11
6

(cid:107) (cid:126)v (d3)(cid:107) =

   
202 + 112 + 62

euclidean length:

(cid:107) (cid:126)v (d)(cid:107) =

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

(cid:126)v (d31) =

(cid:126)v (d32) =

(cid:126)v (d33) =

   
   
   

(cid:126)v 2

i (d)

20

11

202+112+62 = 0.847
202+112+62 = 0.466
202+112+62 = 0.254

6

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d3) = (cid:126)v (d1)    (cid:126)v (d3)
(cid:126)v (d1)    (cid:126)v (d3) =

n(cid:80)

d1i d3i

i=1

[i=1] 0.996     0.847+
[i=2] 0.087     0.466+
[i=3] 0.017     0.254 =
= 0.888

(cid:126)v (d11) =

(cid:126)v (d12) =

(cid:126)v (d13) =

(cid:126)v (d31) =

(cid:126)v (d32) =

(cid:126)v (d33) =

   
   
   

   
   
   

115

10

1152+102+22 = 0.996
1152+102+22 = 0.087
1152+102+22 = 0.017

2

20

11

202+112+62 = 0.847
202+112+62 = 0.466
202+112+62 = 0.254

6

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

sim(d1, d2) = (cid:126)v (d1)    (cid:126)v (d2)
(cid:126)v (d1)    (cid:126)v (d2) =

n(cid:80)

d1i d2i

i=1

[i=1] 0.996     0.993+
[i=2] 0.087     0.120+
[i=3] 0.017     0.000 =
= 0.999

(cid:126)v (d11) =

(cid:126)v (d12) =

(cid:126)v (d13) =

   
   
   

115

10

1152+102+22 = 0.996
1152+102+22 = 0.087
1152+102+22 = 0.017

2

(cid:126)v (d21) =

(cid:126)v (d22) =

58

   
7   

582+72+0

582+72+0

= 0.993

= 0.120

(cid:126)v (d23) = 0

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space model

example (manning et al, 09)

summing up:

dictionary
a   ection
jealous
gossip

(cid:126)v (d1)
0.996
0.087
0.017

(cid:126)v (d2)
0.993
0.120

0

(cid:126)v (d3)
0.847
0.466
0.254

sim(d1, d2) = 0.999
sim(d1, d3) = 0.888

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

14 / 26

vector space classi   cation

new documents

each new document n is represented using vectors in the same way

a   ection
jealous
gossip
sim(n, d) = (cid:126)v (n)    (cid:126)v (d)

d1
115
10
2

d2
58
7
0

d3
20
11
6

n
0
1
1

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

15 / 26

vector space classi   cation

new documents

each new document n is represented using vectors in the same way

a   ection
jealous
gossip
class
sim(n, d) = (cid:126)v (n)    (cid:126)v (d)

d1
115
10
2
a

d2
58
7
0
a

n
d3
0
20
1
11
1
6
b ?

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

15 / 26

vector space classi   cation

new documents

each new document n is represented using vectors in the same way

d1
115
10
2
a

d2
58
7
0
a

d3
20
11
6
b

n
0
1
1
b

a   ection
jealous
gossip
class
sim(n, d) = (cid:126)v (n)    (cid:126)v (d)
with n =< jealous, gossip >
we obtain:

(cid:126)v (n)    (cid:126)v (d1) = 0.074
(cid:126)v (n)    (cid:126)v (d2) = 0.085
(cid:126)v (n)    (cid:126)v (d3) = 0.509

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

15 / 26

vector space classi   cation

classifying new documents

basic idea: similarity cosines between the new document   s vector and
each classi   ed document   s vector;

nb: the decisions of many vector space classi   ers are based on a
notion of distance.

there is a direct correspondence between cosine similarity and
euclidean distance for length-normalised vectors, so it rarely matters
whether the relatedness of two documents is expressed in terms of
similarity or distance

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

16 / 26

k-nearest neighbor

classi   cation using vector spaces

in vector space classi   cation, training set corresponds to a labeled set
of points (vectors)

premise 1: documents in the same class form a contiguous region of
space

premise 2: documents from di   erent classes dont overlap (much)

learning a classi   er = build surfaces to delineate classes in the space

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

17 / 26

28	
   documents	
   in	
   a	
   vector	
   space	
   government science arts sec.14.1 29	
   test	
   document	
   of	
   what	
   class?	
   government science arts sec.14.1 30	
   test	
   document	
   =	
   government	
   government science arts is this  similarity hypothesis true in general? our focus: how to find good separators sec.14.1 k-nearest neighbor

(some slides by manning et al (2009), intro to ir)

k-nearest neighbor

k nearest neighbor classi   cation

id92 (knn) = k-nearest neighbor

to classify a document d:

de   ne k-neighborhood as the k nearest neighbors of d
pick the majority class label in the k-neighborhood

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

19 / 26

can you classify the star ?
what is it most similar/close to?

k-nearest neighbor

k-nearest neighbor

learning: just store the labeled training examples in dataset d
(does not compute anything beyond storing the examples!)

testing instance x (with k = 1):

compute similarity between x and all examples in d.
assign x the category of the most similar example in d.

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

21 / 26

k-nearest neighbor

k-nearest neighbor

using only the closest example (1nn) subject to errors due to:

a single atypical example
noise (i.e., an error) in the category label of a single training example

more robust:    nd the k examples and return the majority category of
these k. how to determine the best k?

in binary classi   cation k is typically odd to avoid ties (often 3 or 5).
experience/knowledge about a certain classi   cation problem
picking best k on development set or via cross-validation

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

21 / 26

k-nearest neighbor

id92 discussion

+ no training necessary

    expensive at test time

+ it scales well with large number of classes

    classes can in   uence each other (small changes to one class can
have ripple e   ect)

classi   cation based only on the nearby k instances (anything farther
away is ignored)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

22 / 26

k-nearest neighbor

support vector machines

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

23 / 26

11 which of the linear separators is optimal? 12 best linear separator? 13 best linear separator? k-nearest neighbor

support vector machines

vector-space-based machine-learning method aiming to    nd a decision
boundary between two classes that is maximally far from any point in
the training data (possibly discounting some points as outliers or noise)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

24 / 26

k-nearest neighbor

support vector machines

vector-space-based machine-learning method aiming to    nd a decision
boundary between two classes that is maximally far from any point in
the training data (possibly discounting some points as outliers or noise)

it lets as few instances of a class to be on the wrong side of the
border as possible

it creates the largest possible no-man   s land between the (two)
classes (   large-margin classi   er   : largest possible margin between
decision boundary and any data point)

some training instances are more important than others: the instances
that make up the boundary are the support vectors
how many instances can one allow to be on    wrong side   ?
(complexity and parameter setting)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

24 / 26

k-nearest neighbor

support vector machines

vector-space-based machine-learning method aiming to    nd a decision
boundary between two classes that is maximally far from any point in
the training data (possibly discounting some points as outliers or noise)

it lets as few instances of a class to be on the wrong side of the
border as possible

it creates the largest possible no-man   s land between the (two)
classes (   large-margin classi   er   : largest possible margin between
decision boundary and any data point)

some training instances are more important than others: the instances
that make up the boundary are the support vectors
how many instances can one allow to be on    wrong side   ?
(complexity and parameter setting)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

24 / 26

k-nearest neighbor

support vector machines

vector-space-based machine-learning method aiming to    nd a decision
boundary between two classes that is maximally far from any point in
the training data (possibly discounting some points as outliers or noise)

it lets as few instances of a class to be on the wrong side of the
border as possible

it creates the largest possible no-man   s land between the (two)
classes (   large-margin classi   er   : largest possible margin between
decision boundary and any data point)

some training instances are more important than others: the instances
that make up the boundary are the support vectors
how many instances can one allow to be on    wrong side   ?
(complexity and parameter setting)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

24 / 26

k-nearest neighbor

support vector machines

vector-space-based machine-learning method aiming to    nd a decision
boundary between two classes that is maximally far from any point in
the training data (possibly discounting some points as outliers or noise)

it lets as few instances of a class to be on the wrong side of the
border as possible

it creates the largest possible no-man   s land between the (two)
classes (   large-margin classi   er   : largest possible margin between
decision boundary and any data point)

some training instances are more important than others: the instances
that make up the boundary are the support vectors
how many instances can one allow to be on    wrong side   ?
(complexity and parameter setting)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

24 / 26

practice!

practice with id92 and id166

practice

options to play with:

      k n (number of nearest neighbours)
      max-train-size n (maximum number of training samples to look at)
      nchars n

visualisation options:

      cm (print confusion matrix + classi   cation report)
      plot (shows cm)

example runs:
python run experiment.py --csv data/langident.csv --nchars 1
--algorithms knn --k 1 --cm
python run experiment.py --csv data/trainset-sentiment-extra.csv
--nchars 1 --algorithms id166 --cm

with many features (e.g. more than 2000), testing with id92 will take a long time.
the small (three languages) version of the langident dataset is
langident-small.csv
malvina & johannes

esslli, 25 august 2016

lfd     lecture 4

26 / 26

tomorrow   s class

practical work on di   erent datasets

from us
from you

wrap up

tomorrow   s practice

practical session

presentation of tasks and datasets (one slide per task/dataset)

running experiments in groups

reporting on experiments (a couple of minutes per group, depending
on how many groups there are)

task
dataset
set up
features
classi   er
results
any re   ections

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

28 / 26

tomorrow   s practice

please, send us your data today!

(and see you tomorrow)

malvina & johannes

lfd     lecture 4

esslli, 25 august 2016

29 / 26

