corpus analysis with open source tools

camilo thorne

data and web science (dws) group

universit  at mannheim, germany

nasslli 2016

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

1 / 34

outline

1

introduction

2 descriptive statistics

3 corpora data

4

inferential statistics

5 regression

6 word spaces and lexical resources

7 case study

8 references

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

2 / 34

motivation - corpora

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

3 / 34

tutorial materials

1 during the labs, we will be using

https://cran.rstudio.com/

r:
python: https://www.python.org/downloads/
a number of and tools and datasets

2 please refer to the course   s git repository for detailed instructions

uri: https://github.com/camilothorne/nasslli2016

(you can check it out without entering any credentials!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

4 / 34

introduction

words and structures in english occur following some general laws or
empirical hypothesis

a distribution describes how often they occur/probable they are
many such distributions may hold:

power laws (zip   an distributions)
normal distributions
binomial distributions
poisson distributions
. . .

we can leverage on such distributions to infer or empirically validate such
hypothesis

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

5 / 34

introduction

words and structures in english occur following some general laws or
empirical hypothesis

a distribution describes how often they occur/probable they are
many such distributions may hold:

power laws (zip   an distributions)
normal distributions
binomial distributions
poisson distributions
. . .

we can leverage on such distributions to infer or empirically validate such
hypothesis

methodology:

1 use corpora to reasonable approximate full english
2 use descriptive and inferential statistics to    t/estimate distributions and

validate hypothesis

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

5 / 34

population, sample, feature

by population we mean the universe s of all possible observable events

e.g., the set w of all sentences ever written in english

a sample is any representative subset s(cid:48)     s of the population

e.g., a given corpus w     w (cid:48), such as the brown corpus

a feature x is any property we may observe over s (or s(cid:48)), viz., a random
variable x : s     d, where

(e.g., xl: lengths |w| of words w)

if d is a number domain, x is a numeric feature
if d = {x1, . . . , xn}, x is an (ordered) factor

(e.g., xs: syntactic categories syn(w) of words w) of words w)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

6 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

mean    = e[x] =

x p (x = x),

if x is dicrete

x p (x = x) d(x),

otherwise

(cid:88)
(cid:90)    

x   x

                     

      

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

(cid:88)
(cid:90)    

x   x

                     

x p (x = x),

if x is dicrete

mean    = e[x] =

mode mod     {x |

      
for all x

x p (x = x) d(x),
(cid:48)     x, x     x

(cid:48)} (need not be unique)

otherwise

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

mean    = e[x] =

mode mod     {x |

x   x

                     
(cid:88)
(cid:90)    
         x |x|   1

2 +1

      
for all x

x |x|

,

,

2 +1

x p (x = x),

if x is dicrete

otherwise

(cid:48)} (need not be unique)

x p (x = x) d(x),
(cid:48)     x, x     x
if |x| is odd
otherwise

median m =

(x sorted in increasing order)

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

mean    = e[x] =

mode mod     {x |

x   x

                     
(cid:88)
(cid:90)    
         x |x|   1

2 +1

      
for all x

x |x|

,

,

2 +1

x p (x = x),

if x is dicrete

otherwise

(cid:48)} (need not be unique)

x p (x = x) d(x),
(cid:48)     x, x     x
if |x| is odd
otherwise

median m =

(x sorted in increasing order)

2 measures of dispersion:

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

mean    = e[x] =

mode mod     {x |

x   x

                     
(cid:88)
(cid:90)    
         x |x|   1

2 +1

      
for all x

x |x|

,

,

2 +1

(cid:88)

x   x

2 measures of dispersion:

variance   2 = var(x) =

(       x)2

x p (x = x),

if x is dicrete

otherwise

(cid:48)} (need not be unique)

x p (x = x) d(x),
(cid:48)     x, x     x
if |x| is odd
otherwise

median m =

(x sorted in increasing order)

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

measures of concentration and dispersion
consider a feature x, concentration measures how similar the values x     x are,
and dispersion how much they di   er

1 measures of concentration:

mean    = e[x] =

mode mod     {x |

x   x

                     
(cid:88)
(cid:90)    
         x |x|   1

2 +1

      
for all x

x |x|

,

,

2 measures of dispersion:

2 +1

(cid:88)
standard deviation    =(cid:112)var(x)

variance   2 = var(x) =

x   x

(       x)2

x p (x = x),

if x is dicrete

otherwise

(cid:48)} (need not be unique)

x p (x = x) d(x),
(cid:48)     x, x     x
if |x| is odd
otherwise

median m =

(x sorted in increasing order)

also known as parameters (many more!!!)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

7 / 34

parameters and distributions

de   nition (distribution)

a distribution with parameters   1, . . . ,   k is a function f (  1, . . . ,   k) that
describes the likelihood/id203 of a feature (random variable) x taking value
x, i.e., p (x = x) = f (x;   1, . . . ,   k).

normal distribution:

binomial distribution:

n (x;   ,   2) =

1   
2  2  

    (x     )2
2  2

e

b(x; n, p) = pxc x

n(1     p)n   x

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

8 / 34

measures of correspondence

question: given two features x and y , how to know if they relate two each
other?

1 x and y are numeric, in that case measure correlation

corr(x, y ) =

cov(x, y )

  x   y

=

e[(x       x )(y       y )]

  x   y

2 x and y are factors, in that case measure mutual information

m i(x; y ) =

p (x = x, y = y) log2

(cid:18) p (x = x, y = y)

(cid:19)

p (x = x)p (y = y)

(cid:88)

(cid:88)

x   x

y   y

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

9 / 34

english corpora

a corpus is a collection of written sentences
it codi   es information about a number of topics...

format
google ngram
brown
bnc
wacky

size (# words)
> 100 billion
    1 million
    100 million
    1 billion

...

...

source
link
link
link
link
...

    rem: many small ones available with nltk v2.0+

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

10 / 34

english corpora

a corpus is a collection of written sentences
it codi   es information about a number of topics...
but also about language use!

format
google ngram
brown
bnc
wacky

size (# words)
> 100 billion
    1 million
    100 million
    1 billion

...

...

source
link
link
link
link
...

    rem: many small ones available with nltk v2.0+

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

10 / 34

annotation standards [gb12]

sometimes, corpora are annotated to support nlp tasks
this annotation is usually done manually, but sometimes also
(semi)automatically
the annotation labels typically respect a prede   ned format

format
brown/penn tagging
conll ner
connl chunk
id32
universal dependencies

task
id52
ner
chunk parsing
constituency parsing
id33

source
pos tags
entities
chunks
parse tags
dependencies

...

...

...

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

11 / 34

hypothesis testing

a hypothesis h is a statement inferred from a sample s(cid:48) that we would like
generalize to the complete population s, for example:

1 proper names of persons (   jane eyre   ) start with a capital letter and are not

preceded by temporal prepositions (   during   ,    until   )

2 collocations/multiwords (   lone wolf   ) occur more frequently than the random

combination of their constituent words
. . .

3

procedure:

1 ensure s(cid:48) is representative
2 consider the alternative null hypothesis h0
3

try to reject h0 via an statistical test
if the test shows that h0 doesn   t    t s(cid:48) =    h may hold over s

4

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

12 / 34

test statistics: the chisq(k) distribution

measure over h their goodness of    t x w.r.t. s
x     chisq(k) with k + 1 degrees of freedom (k + 1 = |s|     100)
if x(h) lies in region where p = p (x > x) < 0.05 =    accept
0     p     1 is called the signi   cance level

   

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

13 / 34

how to reason?

rejection of h0 =    acceptance of alternative h
h0 false

h0     h

h true

minimize type ii error

h0 true

h0 accept

true positive

h0 reject

(type ii error)
false negative

h0 false

(type i error)
false positive
true negative

idea: we want as candidates as many true hypothesis as possible, even if the
results are imprecise

but: if h0 clearly false, we can reasonably assume that h
is a true positive

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

14 / 34

id75

de   nition (id75)
a id75 model has the form

e(y | x1, . . . , xn) =   1x1 +        +   kxk +   k+1

with   k+1 the intercept and   1, . . . ,   k the slopes.

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

15 / 34

id75

de   nition (id75)
a id75 model has the form

e(y | x1, . . . , xn) =   1x1 +        +   kxk +   k+1

with   k+1 the intercept and   1, . . . ,   k the slopes.

assumptions:

1 the xis are conditionally independent on y
2 the sample if it is normally distributed (i.e., whenever

(y | x1, . . . , xn)     n (  ,   2))

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

15 / 34

id75

de   nition (id75)
a id75 model has the form

e(y | x1, . . . , xn) =   1x1 +        +   kxk +   k+1

with   k+1 the intercept and   1, . . . ,   k the slopes.

assumptions:

1 the xis are conditionally independent on y
2 the sample if it is normally distributed (i.e., whenever

(y | x1, . . . , xn)     n (  ,   2))

linear models describe the average e(y | x1, . . . , xn) a dependent feature
y and features x1, . . . , xn

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

15 / 34

id75

de   nition (id75)
a id75 model has the form

e(y | x1, . . . , xn) =   1x1 +        +   kxk +   k+1

with   k+1 the intercept and   1, . . . ,   k the slopes.

assumptions:

1 the xis are conditionally independent on y
2 the sample if it is normally distributed (i.e., whenever

(y | x1, . . . , xn)     n (  ,   2))

linear models describe the average e(y | x1, . . . , xn) a dependent feature
y and features x1, . . . , xn

the parameters/coe   cients   1, . . . ,   k,   k+1 can be estimated via di   erent
(equivalent methods): square error minimization, maximum likelihood, etc.

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

15 / 34

id75

de   nition (id75)
a id75 model has the form

e(y | x1, . . . , xn) =   1x1 +        +   kxk +   k+1

with   k+1 the intercept and   1, . . . ,   k the slopes.

assumptions:

1 the xis are conditionally independent on y
2 the sample if it is normally distributed (i.e., whenever

(y | x1, . . . , xn)     n (  ,   2))

linear models describe the average e(y | x1, . . . , xn) a dependent feature
y and features x1, . . . , xn

the parameters/coe   cients   1, . . . ,   k,   k+1 can be estimated via di   erent
(equivalent methods): square error minimization, maximum likelihood, etc.

we can quantify how well the model    ts the data via a number of indexes

external:   2-goodness of    t, etc.
internal: r2-goodness of    t, bic, aic, etc.

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

15 / 34

analysis of variance (anova)

once we have    tted a regression model to a dataset we can use it to

1 explore the impact of each single factor
2

the impact of a factor is re   ected by the variation it induces

a technique to understand variation w.r.t. factors is analysis of variance
(anova)

procedure:

1 consider factor x of k levels/groups
2

for each level i, consider a linear model where levels/groups j (cid:54)= i are    xed
test (reject) if    =   i, where

3

   original conditional expectation
  i conditional expectation of linear model w.r.t. level/group i

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

16 / 34

generalized linear models

what do we do when a dependency is non-linear?
what do we do when (y |x1, . . . , xk) is not normally distributed?

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

17 / 34

generalized linear models

what do we do when a dependency is non-linear?
what do we do when (y |x1, . . . , xk) is not normally distributed?

answer: generalize linear models to arbitrary distributions, modulo some kind
of transformation

de   nition (glm)
a generalized linear model has the form

f (e(y |x1, . . . , xk)) =   1x1 +        +   kxk +   k+1

where

1 f : r     r is a link function

(y |x1, . . . , xk)     d, with d an arbitrary distribution

2

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

17 / 34

mixed e   ects models

sometimes, when doing regression, some factors, while important, behave like
error terms

example: when you repeatedly observe the same feature over the same
individual over time
each time you make a measurement, some random noise gets mixed in the
measurement!

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

18 / 34

mixed e   ects models

sometimes, when doing regression, some factors, while important, behave like
error terms

example: when you repeatedly observe the same feature over the same
individual over time
each time you make a measurement, some random noise gets mixed in the
measurement!

one can re   ne linear models by letting such features to vary randomly

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

18 / 34

mixed e   ects models

sometimes, when doing regression, some factors, while important, behave like
error terms

example: when you repeatedly observe the same feature over the same
individual over time
each time you make a measurement, some random noise gets mixed in the
measurement!

one can re   ne linear models by letting such features to vary randomly

this results in a linear mixed model of the form

f (e(y |x1, . . . , xk)) =   1x1 +        +   nxn+

  n+1z1 +        +   n+mzm +   (n+m)+1

where the xis are    xed and the zjs are non-   xed

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

18 / 34

power laws [bar09]

the frequency of a word w in a sample s(cid:48)     s over vocabulary w is the
number of times (count) we observe it in s(cid:48), viz., freq : w     n s.t.
freq(w) = p (w = w)    |s(cid:48)|

words can be ordered by frequency rank rank : w     |w| s.t.
rank(w) < r(w) if

1 freq(w) < freq(w(cid:48)), or
2 freq(w) = freq(w(cid:48)) and w comes before w(cid:48) in lexicographic order

we can use regression on the log-log scale to model power laws among word
frequency and rank

log freq(w) = log(  )       (cid:48) log(rank(w))

=    freq(w) =
=   

  

rank(w)  (cid:48)

rank     p l(  ,   (cid:48)), for some   ,   (cid:48)     0

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

19 / 34

semantic relatedness [nav09]

distributional hypothesis

1 the meaning of a word w is given by sentential context
2 sentential context is the window of k words surrounding w
3 two words w and w(cid:48) have the same meaning if they occur always in the

same context

the semantic relatedness of two words can be estimated by computing
context similarity over very large word corpora of vocabulary w

idea: compute a word space over the corpus

wi, wj     w : mi,j = freq(wi)    ijfreq(wi, wj)

1 a word space is a matrix m|w|  k where each mi,j is the    f.ijf    of words
2 m|w|  k de   nes a k-dimensional real-valued vector space     rk
3 each word wi is mapped to a vector #   wi = (mi,1, . . . , mi,k)t

then rel(w, w(cid:48)) = sim( #  w, #  w(cid:48))

4

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

20 / 34

vector space rk with dimensions |k| (size of context window) (size of vocabulary)

r2

  

o

#        

clear = (0.8, 3.2)t

#         

mind = (3, 2.4)t

r1

model semantic relatedness in terms of cosine similarity

rel(clear, mind) = cos(  ) =

#        

clear     #         
mind
clear||    || #         
|| #        
mind||

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

21 / 34

the wacky corpus [bbfz09]

1
2
3
4
5

werke
be
a
german

<s>
flender flender np
werke
np
vbd
was
dt
a
jj
german
shipbuilding
shipbuilding
company company nn
,
located locate
in
lubeck
.
</s>

,
vvn
in
np
sent

in
lubeck
.

7
8
9
10
11
12

,

3
3
0
7
7
nn
3
7
7
9
10
0

vmod
sbj
root
nmod
nmod
6
prd
p
nmod
adv
pmod
root

7

nmod

wacky (eng)     43 million     800 million wikipedia (en, 2008)

sentences

tokens

source

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

22 / 34

answer time and complexity [szy09]

parity:    exactly 2   

cardinal: all the other counting quanti   ers

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

23 / 34

p<0.05p<0.001corpus analysis [ts15]

we can build a list of simple patterns to identify and count quanti   ers in
corpora

1 aristotelian quanti   ers: all, some
2 counting quanti   ers: k, less (more) than k
3 proportional quanti   ers: most, few, p/k, k%, less (more) than p/k, less

(more) than k%

examples: { most = most/dt, most/jjs [a-z]{1,12}/nns

some = some/det

understand how much their frequency is in   uenced by

length (characters, word units)        syntactic complexity   

1

2 quanti   er class        semantic complexity   
3 other factors/features

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

24 / 34

distribution not normal
heavy (right) tailed
most gqs in the sample are very rare (frequency     1)

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

25 / 34

most quanti   ers are aristotelian
aristotelian quanti   ers show a lot of variability
proportional quanti   ers contain some outliers (   most   ,    few   )

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

26 / 34

log-log (ln-ln) regression (to test for power law):

r2 = 0.7581
p = 0.0001074

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

27 / 34

larger is better!

theorem (law of big numbers)
let s1, s2, . . . , si be a family of increasingly large samples of some population s
(i.e., si     si+1     s) distributed under f (  1, . . . ,   k) (i.e., s     f (  1, . . . ,   k)).
then, for i     1, the estimators     i,1, . . . ,     i,k converge to the true parameters
  1, . . . ,   k) of f (i.e., for j     [1, k],

i(cid:55)         i,j =   j).
lim

the larger the sample, the better we can observe or    t to it a distribution!
grain of salt:

1

2

3

samples are assumed to be representative w.r.t. population and i.i.d.
samples are assumed to contain minimal noise
the distribution should    t the data

meaning in practice:

1 avoid making id136s over very small corpora
2

trade-o    corpora size by corpora quality

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

28 / 34

thank you!

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

29 / 34

references i

marco baroni.
distributions in text.
in mouton de gruyter, editor, corpus linguistics: an international handbook,
volume 2, pages 803   821. 2009.

marco baroni, silvia bernardini, adriano ferraresi, and eros zanchetta.
the wacky wide web: a collection of very large linguistically processed
web-crawled corpora.
language resources and evaluation, 43(3):209   226, 2009.

stefan th. gries and andrea l. b.
linguistic annotation in/for corpus linguistics.
preprint: http://www.linguistics.ucsb.edu/faculty/stgries/research/
inprogr_stg_alb_lingannotcorpling_hboflingannot.pdf (to appear in the
handbook of linguistic annotation)., 2012.

roberto navigli.
id51: a survey.
acm computing surveys, 41(2):10:1   10:69, 2009.

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

30 / 34

references ii

jakub szymanik.
quanti   ers in time and space.
institute for logic, language and computation, 2009.

camilo thorne and jakub szymanik.
semantic complexity of quanti   ers and their distribution in corpora.
in proceedings of the 11th international conference in computational semantics
(iwcs 2015), 2015.

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

31 / 34

appendix: regression id136 and validation

the least squares method computes the linear model whose parameters
(  1, . . . ,   k+1)    minimize square error j(  )
(  1, . . . ,   k+1)    = arg min

j(  1, . . . ,   k+1)

(  1,...,  k+1)

= arg min

(  1,...,  k+1)

(cid:88)

(cid:88)

i

j

(yi       j(xi,j))2

the r2 coe   cient provides a measure of con   dence in the inferred model
and is de   ned as the ratio of square error variance and dependent variable
variance, i.e.,

(cid:35)

(cid:34)(cid:80)

(cid:80)

i

r2 = 1    

j(     

j (xi,j)     e[y ])2
var(y )

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

32 / 34

appendix: regression bic and aic

the bayesian information criterion (bic) is de   ned by

bic(k + 1, s(cid:48)) =    ([2    ln l   ] + [(k + 1)    ln(|s(cid:48)|)])

where

1 l    = p (s | (  1, . . . ,   k+1)   ) (maximum likelihood)
(  1, . . . ,   k+1)    are the optimal model parameters

2
3 k + 1 is the number of parameters/coe   cients of the model
4 s(cid:48)     s is the sample to which we    t the linear model

the akaike information criterion (aic) is de   ned by

aic(k + 1, s(cid:48)) =    ([2    (k + 1)]     [2    ln l   ])

with parameters as for bic

note: these measures are useful when evaluating regression models, where
the output/dependent variable is numeric, they are not appropriate for
classi   cation or id91

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

33 / 34

appendix: con   dence intervals

the theoretical mean    of feature x is the expected value e[x] of x
we can estimate the theoretical mean from a sample s(cid:48)     s via maximum
likelihood, giving rise to the known empirical mean

(cid:88)

x   x

x

     =

1
|x|

con   dence intervals allow to infer, from     ,     , x and s(cid:48) the potential range
of values of   
let p < 0.05 be the signi   cance level of test statistic t , then

(cid:34)
         t        (cid:112)|x| ,      +

(cid:35)

t        (cid:112)|x|

      

where t    is the upper (1     p)/2 critical value of test t distribution with
|x|     1 degrees of freedom

c. thorne (uma)

corpus analysis

rutgers, 09-10.07.2016

34 / 34

