summary of id203   

 

 

 

 

mathematical physics i 

rules of id203 

the id203 of an event is called p(a), which is a positive number less than or equal to 1.    

the total id203 for all possible exclusive outcomes should be 1.   

the joint id203 of two different events is denoted p(a,b).   

id155 (id203 of a given b)  

                              
            

independence 

                                                                               

law of total id203  

bayes    theorem 

                                                                       

 

   

   

                                         

 

          

expectation value of a function 

                                            

 

   

         

id203 density function (p.d.f.) 

here we have assumed that the outcomes are discrete, but they also hold for continuous 

measurements.   in this case, the id203 is described by a p.d.f., such that 

                                                       

sums over all possible discrete outcomes become integrals over possible results,  

  

                                                                               
     

     

  

                                                       

   
      

  

cumulative id203 function  

                                                        

 

   
      

common id203 distributions 

a few distribution functions arise quite often.  here are a few, along with the corresponding 

mean (                  ) and variance (                                           .)   these help describe the average 

and the width (or dispersion) of the distributions.      

 

uniform distribution 

one common distribution is uniform; this occurs for example in coin flips or dice rolls.  

for discrete distributions, the id203 of any event is pi = 1/n, where n is the number of 
possible outcomes.  examples include drawing a random card or rolling a die.  

for continuous distributions, the p.d.f. is f(x) = 1/  , where    = xmax     xmin is the range possible 

values and must be finite.    the mean of this distribution is                                              and the 
variance is                      

 

binomial distribution  

suppose you have a number of independent trials with two possible outcomes (a and b), 

such as flipping coins, with probabilities given as    and            the distribution of the 
number of times a is seen given    trials is given by the binomial distribution:  

                                          
                   

 

the mean of this distribution is                and its variance is                                 

 

poisson distribution 

this is a discrete distribution described by  

                       
   

                      !       

this often occurs in counting statistics, and is the limit of other important distributions, like 

the binomial distribution in the large n limit.    

                                                                                                                                 

      

      

              

  

 

 

gaussian distribution  

the gaussian distribution is also known as the normal distribution,                 , and is one of 

the most important distributions in statistics: 

 

               

                                                    
  

 

                                                                                                                                            

moments (or integrals) of the normal distribution:  

  

                                                                                
     

     

  

                                                              

           

  

     

 

the central limit theorem says that if you add many like variables together, the distribution 

of their sums will approach a gaussian, even if the original distributions are not gaussian! 

 

the id203 of falling within a gap near the peak of the gaussian distribution is:    

range   

     

id203 of falling in range 

 

                                    
                                               
                  !                        !   

 

 

 

 

 

 

 

68.27% 

95.45%  

99.73%  

such calculations can be made using the cumulative integral of the gaussian function, which 

is closely related to the error function.  this is usually compiled in tables like in appendix h 

of jordan & smith.  

    n         

        a      

   
      

          @j

 

 

 

 

 

 

learning with bayesian id136  

two general approaches to statistics exist, known as the frequentist and bayesian methods.   

the classical approach is the frequentist method, and most texts focus on this kind of 

analysis.  however, the bayesian approach is becoming more widely accepted.  

when the data are good, these two approaches will generally result in the same 

conclusions.   however, there are some deep philosophical differences between the 

approaches, mostly based on what is meant by id203 and whether it is absolute or 

subjective.  the bayesian approach requires an explicit assumption of the believability of a 

model (called a prior) which could be different for different people.  it then uses 

observations to update these beliefs using bayes    theorem.   

bayes    theorem shows us how to update our confidence in a theory or model (m) given the 

results of a new experiment, observation or data (d) :   

                                         

          

 

here,            is our evaluation of the id203 of the model before the new data; this is 

known as the prior distribution, and is usually based on earlier data or theoretical ideas. 

              is the new evaluation of the id203 of the model after the new data is 

considered.  it is called the posterior distribution. 

              is called the likelihood and it describes how likely the data would be if the model 

were true.   this is usually more straightforward to calculate.  

           is a normalising factor called the bayesian evidence.  

 

 

 

 

 

 

 

 

 

 

data and estimators of mean and variance 

suppose we have a sample of n independent measurements of a quantity (xi); it is useful to 
have statistics which describe its distribution:  

sample mean estimator  

 

 

 

            

   
          
         

 

sample variance estimator 

         

   

 
                                   

 

         

the mean is the average value, while the square root of the variance (known as the 

standard deviation or root mean square deviation) describes the error in the estimate of 

that value.   we tend to write this as,                         

we really want to know the properties of the underlying distribution, not of the sample 

itself.  we hope that our estimators we calculate from the sample will tend to the true 

properties as the number of samples increases.  if this is true, the estimator is unbiased. 

 

propagation of errors 

often we must estimate the errors on quantities that aren   t measured directly, but are 

related to things which are measured.  the errors in these derived quantities can be 

calculated: 

   
                                               
       

   
                   
       

 

                                                                            

sums and differences:   

products and quotients: 

                   

                   

                  

                  
      

note that these equations ignore potential correlations between u and v measurements. 

rule of thumb: when adding or subtracting, the absolute errors add in quadrature.  when 

multiplying or dividing, the percentage errors add in quadrature. 

goodness of fit test (  2 or chi-squared test)  

given a collection of data (xi, yi       i ), we can evaluate the quality of the fit of a model for 
that data y(x) by use of the chi-squared statistic:  

                                                       

this is closely related to the likelihood of the data given the model, assuming the errors are 

distributed with a gaussian distribution.  the smaller chi-squared is, the larger the likelihood 

is.  

typically, the chi-squared statistic is expected to be around the number of effective degrees 

of freedom which is the number of data points minus the number of parameters in the 

model being fit.   if these are much different, then it may be indicating a problem with the 

model.  

we usually try to find the best fit parameters for a model by finding those which minimise 

the chi-squared value (thereby maximising the likelihood of the data.)   this is sometimes 

called a least-squares fit to the data, and is also known as a regression.   

the most common application is to a linear model for the data, that is assuming that 

                      for a linear model, it is easy to find the parameters which minimise the chi-

squared.    

                                                        

the best-fit parameters of the model are relatively simple to calculate if we assume all the 

errors are the same (                    

 

                                        
           
                                                        
           
                                               

                  
                
                    
    

