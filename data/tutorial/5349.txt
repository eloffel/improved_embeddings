id202 explained in four pages

excerpt from the no bullshit guide to id202 by ivan savov

abstract   this document will review the fundamental ideas of id202.
we will learn about matrices, matrix operations, linear transformations and
discuss both the theoretical and computational aspects of id202. the
tools of id202 open the gateway to the study of more advanced
mathematics. a lot of knowledge buzz awaits you if you choose to follow the
path of understanding, instead of trying to memorize a bunch of formulas.

i. introduction

id202 is the math of vectors and matrices. let n be a positive
integer and let r denote the set of real numbers, then rn is the set of all
n-tuples of real numbers. a vector (cid:126)v     rn is an n-tuple of real numbers.
the notation        s    is read    element of s.    for example, consider a vector
that has three components:

(cid:126)v = (v1, v2, v3)     (r, r, r)     r3.

a matrix a     rm  n is a rectangular array of real numbers with m rows
and n columns. for example, a 3    2 matrix looks like this:

24 a11

a21
a31

35    

24 r r

r r
r r

35     r3  2.

a12
a22
a32

a =

the purpose of this document is to introduce you to the mathematical
operations that we can perform on vectors and matrices and to give you a
feel of the power of id202. many problems in science, business,
and technology can be described in terms of vectors and matrices so it is
important that you understand how to work with these.

prerequisites
the only prerequisite for this tutorial is a basic understanding of high school
math concepts1 like numbers, variables, equations, and the fundamental
arithmetic operations on real numbers: addition (denoted +), subtraction
(denoted    ), multiplication (denoted implicitly), and division (fractions).
you should also be familiar with functions that take real numbers as
inputs and give real numbers as outputs, f : r     r. recall that, by
de   nition, the inverse function f   1 undoes the effect of f. if you are
given f (x) and you want to    nd x, you can use the inverse function as
follows: f   1 (f (x)) = x. for example, the function f (x) = ln(x) has the
inverse f   1(x) = ex, and the inverse of g(x) =

x is g   1(x) = x2.

   

a. vector operations

ii. definitions

we now de   ne the math operations for vectors. the operations we can
perform on vectors (cid:126)u = (u1, u2, u3) and (cid:126)v = (v1, v2, v3) are: addition,
subtraction, scaling, norm (length), dot product, and cross product:

q

(cid:126)u + (cid:126)v = (u1 + v1, u2 + v2, u3 + v3)
(cid:126)u     (cid:126)v = (u1     v1, u2     v2, u3     v3)

  (cid:126)u = (  u1,   u2,   u3)
||(cid:126)u|| =
(cid:126)u    (cid:126)v = u1v1 + u2v2 + u3v3
(cid:126)u    (cid:126)v = (u2v3     u3v2, u3v1     u1v3, u1v2     u2v1)

u2
1 + u2

2 + u2
3

the dot product and the cross product of two vectors can also be described
in terms of the angle    between the two vectors. the formula for the dot
product of the vectors is (cid:126)u    (cid:126)v = (cid:107)(cid:126)u(cid:107)(cid:107)(cid:126)v(cid:107) cos   . we say two vectors (cid:126)u and
(cid:126)v are orthogonal if the angle between them is 90   . the dot product of
orthogonal vectors is zero: (cid:126)u    (cid:126)v = (cid:107)(cid:126)u(cid:107)(cid:107)(cid:126)v(cid:107) cos(90   ) = 0.
the norm of the cross product is given by (cid:107)(cid:126)u   (cid:126)v(cid:107) = (cid:107)(cid:126)u(cid:107)(cid:107)(cid:126)v(cid:107) sin   . the
cross product is not commutative: (cid:126)u   (cid:126)v (cid:54)= (cid:126)v    (cid:126)u, in fact (cid:126)u   (cid:126)v =    (cid:126)v    (cid:126)u.

b. matrix operations

we denote by a the matrix as a whole and refer to its entries as aij.

the mathematical operations de   ned for matrices are the following:

    addition (denoted +)

c = a + b

   

cij = aij + bij.

    subtraction (the inverse of addition)
    matrix product. the product of matrices a     rm  n and b     rn  (cid:96)

is another matrix c     rm  (cid:96) given by the formula

aikbkj,

a11b12 + a12b22
a21b12 + a22b22
a31b12 + a32b22

35

k=1

   

cij =

nx
24a11b11 + a12b21
24  1
   t

a21b11 + a22b21
a31b11 + a32b21

=

  3

  2
  3

35 .

  1
  2
  3

24a11

a21
a31

a12
a22
a32

35  b11

b21

c = ab

   

b12
b22

=

    matrix inverse (denoted a   1)
    matrix transpose (denoted t):

    1   2   3

    matrix trace: tr[a]    pn

  1

  2

    determinant (denoted det(a) or |a|)

i=1 aii

note that the matrix product is not a commutative operation: ab (cid:54)= ba.

c. matrix-vector product

the matrix-vector product is an important special case of the matrix-
matrix product. the product of a 3    2 matrix a and the 2    1 column
vector (cid:126)x results in a 3    1 vector (cid:126)y given by:

(cid:126)y = a(cid:126)x

   

35

24a11x1 + a12x2
35

a21x1 + a22x2
a31x1 + a32x2

24y1

y2
y3

=

x2

a21 a22
a31 a32

35  x1
35=
24a11 a12
   
24a12
35+x2
24a11
35.
24(a11, a12)    (cid:126)x

(a21, a22)    (cid:126)x
(a31, a32)    (cid:126)x

a21
a31

a22
a32

= x1

=

(c)

(r)

there are two2 fundamentally different yet equivalent ways to interpret the
matrix-vector product. in the column picture, (c), the multiplication of the
matrix a by the vector (cid:126)x produces a linear combination of the columns
of the matrix: (cid:126)y = a(cid:126)x = x1a[:,1] + x2a[:,2], where a[:,1] and a[:,2] are
the    rst and second columns of the matrix a.

in the row picture, (r), multiplication of the matrix a by the vector (cid:126)x
produces a column vector with coef   cients equal to the dot products of
rows of the matrix with the vector (cid:126)x.

d. linear transformations

the matrix-vector product

is used to de   ne the notion of a linear
transformation, which is one of the key notions in the study of linear
algebra. multiplication by a matrix a     rm  n can be thought of as
computing a linear transformation ta that takes n-vectors as inputs and
produces m-vectors as outputs:

ta : rn     rm.

1a good textbook to (re)learn high school math is minireference.com

2for more info see the video of prof. strang   s mit lecture: bit.ly/10vmkcl

1

instead of writing (cid:126)y = ta((cid:126)x) for the linear transformation ta applied to
the vector (cid:126)x, we simply write (cid:126)y = a(cid:126)x. applying the linear transformation
ta to the vector (cid:126)x corresponds to the product of the matrix a and the
column vector (cid:126)x. we say ta is represented by the matrix a.

you can think of linear transformations as    vector functions    and describe
their properties in analogy with the regular functions you are familiar with:

function f : r     r     linear transformation ta : rn    rm

input x     r     input (cid:126)x     rn
output f (x)     output ta((cid:126)x) = a(cid:126)x     rm

g     f = g(f (x))     tb(ta((cid:126)x)) = ba(cid:126)x
   1

   1     matrix inverse a

function inverse f

zeros of f     n (a)     null space of a
range of f     c(a)     column space of a = range of ta

note that the combined effect of applying the transformation ta followed
by tb on the input vector (cid:126)x is equivalent to the matrix product ba(cid:126)x.

e. fundamental vector spaces

a vector space consists of a set of vectors and all linear combinations of
these vectors. for example the vector space s = span{(cid:126)v1, (cid:126)v2} consists of
all vectors of the form (cid:126)v =   (cid:126)v1 +   (cid:126)v2, where    and    are real numbers.
we now de   ne three fundamental vector spaces associated with a matrix a.
the column space of a matrix a is the set of vectors that can be produced

as linear combinations of the columns of the matrix a:

c(a)     {(cid:126)y     rm | (cid:126)y = a(cid:126)x for some (cid:126)x     rn} .

the column space is the range of the linear transformation ta (the set
of possible outputs). you can convince yourself of this fact by reviewing
the de   nition of the matrix-vector product in the column picture (c). the
vector a(cid:126)x contains x1 times the 1st column of a, x2 times the 2nd column
of a, etc. varying over all possible inputs (cid:126)x, we obtain all possible linear
combinations of the columns of a, hence the name    column space.   
the null space n (a) of a matrix a     rm  n consists of all the vectors

that the matrix a sends to the zero vector:

n (a)      (cid:126)x     rn | a(cid:126)x = (cid:126)0  .

the row space of a matrix a, denoted r(a),

the vectors in the null space are orthogonal to all the rows of the matrix.
we can see this from the row picture (r): the output vectors is (cid:126)0 if and
only if the input vector (cid:126)x is orthogonal to all the rows of a.
is the set of linear
combinations of the rows of a. the row space r(a) is the orthogonal
complement of the null space n (a). this means that for all vectors
(cid:126)v     r(a) and all vectors (cid:126)w     n (a), we have (cid:126)v    (cid:126)w = 0. together, the
null space and the row space form the domain of the transformation ta,
rn = n (a)     r(a), where     stands for orthogonal direct sum.

2

iii. computational id202

okay, i hear what you are saying    dude, enough with the theory talk, let   s
see some calculations.    in this section we   ll look at one of the fundamental
algorithms of id202 called gauss   jordan elimination.

a. solving systems of equations

suppose we   re asked to solve the following system of equations:

1x1 + 2x2 = 5,
3x1 + 9x2 = 21.

(1)

without a knowledge of id202, we could use substitution, elimina-
tion, or subtraction to    nd the values of the two unknowns x1 and x2.

gauss   jordan elimination is a systematic procedure for solving systems

of equations based the following row operations:
  ) adding a multiple of one row to another row
  ) swapping two rows
  ) multiplying a row by a constant
these row operations allow us to simplify the system of equations without
changing their solution.

to illustrate the gauss   jordan elimination procedure, we   ll now show the
sequence of row operations required to solve the system of linear equations
described above. we start by constructing an augmented matrix as follows:

   1 2

3

9

   

.

5
21

the    rst column in the augmented matrix corresponds to the coef   cients of
the variable x1, the second column corresponds to the coef   cients of x2,
and the third column contains the constants from the right-hand side.

the gauss-jordan elimination procedure consists of two phases. during
the    rst phase, we proceed left-to-right by choosing a row with a leading
one in the leftmost column (called a pivot) and systematically subtracting
that row from all rows below it to get zeros below in the entire column. in
the second phase, we start with the rightmost pivot and use it to eliminate
all the numbers above it in the same column. let   s see this in action.

1) the    rst step is to use the pivot in the    rst column to eliminate the
variable x1 in the second row. we do this by subtracting three times
the    rst row from the second row, denoted r2     r2     3r1,

2) next, we create a pivot in the second row using r2     1

3 r2:

3) we now start the backward phase and eliminate the second variable
from the    rst row. we do this by subtracting two times the second
row from the    rst row r1     r1     2r2:

0

   1 2
   1

2
0 1

3

   1

0

0
1

1
2

5
6

5
2

   
   

   

.

.

.

f. matrix inverse

by de   nition, the inverse matrix a   1 undoes the effects of the matrix a.
the cumulative effect of applying a   1 after a is the identity matrix 1:

241

0

35 .

. . .

0

1

   1a = 1    
a

the identity matrix (ones on the diagonal and zeros everywhere else)
corresponds to the identity transformation: t1((cid:126)x) = 1(cid:126)x = (cid:126)x, for all (cid:126)x.

the matrix inverse is useful for solving matrix equations. whenever we
want to get rid of the matrix a in some matrix equation, we can    hit    a
with its inverse a   1 to make it disappear. for example, to solve for the
matrix x in the equation xa = b, multiply both sides of the equation
by a   1 from the right: x = ba   1. to solve for x in abcxd = e,
multiply both sides of the equation by d   1 on the right and by a   1, b   1
and c   1 (in that order) from the left: x = c   1b   1a   1ed   1.

the matrix is now in reduced row echelon form (rref), which is its
   simplest    form it could be in. the solutions are: x1 = 1, x2 = 2.

b. systems of equations as matrix equations

we will now discuss another approach for solving the system of
equations. using the de   nition of the matrix-vector product, we can express
this system of equations (1) as a matrix equation:

  1

3

     x1

   

x2

2
9

   5

   

21

.

=

this matrix equation had the form a(cid:126)x = (cid:126)b, where a is a 2    2 matrix, (cid:126)x
is the vector of unknowns, and (cid:126)b is a vector of constants. we can solve for
(cid:126)x by multiplying both sides of the equation by the matrix inverse a   1:

  x1

   

x2

   3     2

      5

   

   1

3

1
3

21

   

  1

2

=

.

   1a(cid:126)x = 1(cid:126)x =
a

   1(cid:126)b =

= a

but how did we know what the inverse matrix a   1 is?

iv. computing the inverse of a matrix

c. using a computer

in this section we   ll look at several different approaches for computing
the inverse of a matrix. the matrix inverse is unique so no matter which
method we use to    nd the inverse, we   ll always obtain the same answer.

the last (and most practical) approach for    nding the inverse of a matrix
is to use a computer algebra system like the one at live.sympy.org.
# define a
>>> a = matrix( [[1,2],[3,9]] )

3

a. using row operations

one approach for computing the inverse is to use the gauss   jordan
elimination procedure. start by creating an array containing the entries
of the matrix a on the left side and the identity matrix on the right side:

[1, 2]
[3, 9]

>>> a.inv()

[ 3, -2/3]
[-1,
1/3]

# calls the inv method on a

.

you can use sympy to    check    your answers on homework problems.

   1

3

0

   1
   1
   1

0

0

   

2
9

1
0

0
1

2
1
3    3

0
1

2
1
1    1

0
1
3

   
   

.

.

   

.

now we perform the gauss-jordan elimination procedure on this array.

1) the    rst row operation is to subtract three times the    rst row from

the second row: r2     r2     3r1. we obtain:

2) the second row operation is divide the second row by 3: r2     1

3 r2

3) the third row operation is r1     r1     2r2
3     2

0
1    1

3

1
3

the array is now in reduced row echelon form (rref). the inverse matrix
appears on the right side of the array.

observe that the sequence of row operations we used to solve the speci   c
system of equations in a(cid:126)x = (cid:126)b in the previous section are the same as the
row operations we used in this section to    nd the inverse matrix. indeed,
in both cases the combined effect of the three row operations is to    undo   
the effects of a. the right side of the 2    4 array is simply a convenient
way to record this sequence of operations and thus obtain a   1.

b. using elementary matrices

every row operation we perform on a matrix is equivalent to a left-
multiplication by an elementary matrix. there are three types of elementary
matrices in correspondence with the three types of row operations:

r   : r1     r1 + mr2     e   =

r   : r1     r2

r   : r1     mr1

    e   =

    e   =

0

  1 m
   
  0
   
   
  m 0

1
0

1

1

0

1

let   s revisit the row operations we used to    nd a   1 in the above section
representing each row operation as an elementary id127.
1) the    rst row operation r2     r2     3r1 corresponds to a multipli-

cation by the elementary matrix e1:

e1a =

0
2) the second row operation r2     1
3 r2 corresponds to a matrix e2:

3

   1

   3

     1

0
1

   

2
9

  1

0

e2(e1a) =

3) the    nal step, r1     r1     2r2, corresponds to the matrix e3:

=

0
1
3

.

2
3

  1
   
     1
   
  1
  1    2
     1
   
      1

2
3

=

0

1

0

0

0

   3

0
1

2
1

2
1

.

   
  1
   
   3     2

=

0

=

   1

3

1
3

.

   
   

0
1

.

e3(e2e1a) =

  1    2

     1 0

0

1

0

1
3

   1 = e3e2e1 =
a

note that e3e2e1a = 1, so the product e3e2e1 must be equal to a   1:

we   ll now discuss a number of other important topics of id202.

v. other topics

a. basis

intuitively, a basis is any set of vectors that can be used as a coordinate
system for a vector space. you are certainly familiar with the standard basis
for the xy-plane that is made up of two orthogonal axes: the x-axis and
the y-axis. a vector (cid:126)v can be described as a coordinate pair (vx, vy) with
respect to these axes, or equivalently as (cid:126)v = vx     + vy      , where          (1, 0)
and           (0, 1) are unit vectors that point along the x-axis and y-axis
respectively. however, other coordinate systems are also possible.
de   nition (basis). a basis for a n-dimensional vector space s is any set
of n linearly independent vectors that are part of s.
any set of two linearly independent vectors {  e1,   e2} can serve as a basis
for r2. we can write any vector (cid:126)v     r2 as a linear combination of these
basis vectors (cid:126)v = v1  e1 + v2  e2.
note the same vector (cid:126)v corresponds to different coordinate pairs depend-
ing on the basis used: (cid:126)v = (vx, vy) in the standard basis bs     {    ,      }, and
(cid:126)v = (v1, v2) in the basis be     {  e1,   e2}. therefore, it is important to keep
in mind the basis with respect to which the coef   cients are taken, and if
necessary specify the basis as a subscript, e.g., (vx, vy)bs or (v1, v2)be.
converting a coordinate vector from the basis be to the basis bs is

performed as a multiplication by a change of basis matrix:

  vx

   

vy

             e1

           e1

=

     v1

   

.

v2

          e2
           e2

   

(cid:126)v

=

1

(cid:126)v

bs

bs

be

be

   

  

   

  

   

  

note the change of basis matrix is actually an identity transformation. the
vector (cid:126)v remains unchanged   it is simply expressed with respect to a new
coordinate system. the change of basis from the bs-basis to the be-basis
is accomplished using the inverse matrix: be [1]bs = (bs [1]be )

   1.

b. matrix representations of linear transformations

bases play an important role in the representation of linear transforma-
tions t : rn     rm. to fully describe the matrix that corresponds to some
linear transformation t , it is suf   cient to know the effects of t to the n
vectors of the standard basis for the input space. for a linear transformation
t : r2     r2, the matrix representation corresponds to

mt =

24 |
     

|
t (    ) t (     )
|
|

35     r2  2.
   1
        
   cos   

     0
        
     0

=

0

r  

1

=

sin   

     1
     
     1

0

  

  

as a    rst example, consider the transformation   x which projects vectors
onto the x-axis. for any vector (cid:126)v = (vx, vy), we have   x((cid:126)v) = (vx, 0).
the matrix representation of   x is

m  x =

  x

  x

as a second example,
counterclockwise rotation by the angle   :

1

0
let   s    nd the matrix representation of r  ,

the

   

.

0
0

   

.

    sin   
cos   

mr   =

r  

the elementary matrix approach teaches us that every invertible matrix
can be decomposed as the product of elementary matrices. since we know
a   1 = e3e2e1 then a = (a   1)   1 = (e3e2e1)   1 = e

   1
1 e

   1
2 e

   1
3 .

the    rst column of mr   shows that r   maps the vector          1   0 to the
vector 1      = (cos   , sin   )t. the second column shows that r   maps the
vector       = 1      

2 +   ) = (    sin   , cos   )t.

2 to the vector 1   (   

c. dimension and bases for vector spaces

e. invertible matrix theorem

4

the dimension of a vector space is de   ned as the number of vectors
in a basis for that vector space. consider the following vector space
s = span{(1, 0, 0), (0, 1, 0), (1, 1, 0)}. seeing that the space is described
by three vectors, we might think that s is 3-dimensional. this is not the
case, however, since the three vectors are not linearly independent so they
don   t form a basis for s. two vectors are suf   cient to describe any vector
in s; we can write s = span{(1, 0, 0), (0, 1, 0)}, and we see these two
vectors are linearly independent so they form a basis and dim(s) = 2.

there is a general procedure for    nding a basis for a vector space.
suppose you are given a description of a vector space in terms of m vectors
v = span{(cid:126)v1, (cid:126)v2, . . . , (cid:126)vm} and you are asked to    nd a basis for v and the
dimension of v. to    nd a basis for v, you must    nd a set of linearly
independent vectors that span v. we can use the gauss   jordan elimination
procedure to accomplish this task. write the vectors (cid:126)vi as the rows of a
matrix m. the vector space v corresponds to the row space of the matrix
m. next, use row operations to    nd the reduced row echelon form (rref)
of the matrix m. since row operations do not change the row space of the
matrix, the row space of reduced row echelon form of the matrix m is the
same as the row space of the original set of vectors. the nonzero rows in
the rref of the matrix form a basis for vector space v and the numbers
of nonzero rows is the dimension of v.

d. row space, columns space, and rank of a matrix

recall the fundamental vector spaces for matrices that we de   ned in
section ii-e: the column space c(a), the null space n (a), and the row
space r(a). a standard id202 exam question is to give you a
certain matrix a and ask you to    nd the dimension and a basis for each
of its fundamental spaces.

in the previous section we described a procedure based on gauss   jordan
elimination which can be used    distill    a set of linearly independent vectors
which form a basis for the row space r(a). we will now illustrate this
procedure with an example, and also show how to use the rref of the
matrix a to    nd bases for c(a) and n (a).

consider the following matrix and its reduced row echelon form:

241

2
3

35

3
6
9

3
7
9

3
6
10

a =

rref(a) =

241 3

0
0

0

0
0 1 0
0
0 1

35 .

the reduced row echelon form of the matrix a contains three pivots. the
locations of the pivots will play an important role in the following steps.
the vectors {(1, 3, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1)} form a basis for r(a).
to    nd a basis for the column space c(a) of the matrix a we need
to    nd which of the columns of a are linearly independent. we can
do this by identifying the columns which contain the leading ones in
rref(a). the corresponding columns in the original matrix form a basis
for the column space of a. looking at rref(a) we see the    rst, third,
and fourth columns of the matrix are linearly independent so the vectors
{(1, 2, 3)t, (3, 7, 9)t, (3, 6, 10)t} form a basis for c(a).
now let   s    nd a basis for the null space, n (a)     {(cid:126)x     r4 | a(cid:126)x = (cid:126)0}.
the second column does not contain a pivot, therefore it corresponds to a
free variable, which we will denote s. we are looking for a vector with three
unknowns and one free variable (x1, s, x3, x4)t that obeys the conditions:

241

0
0

3
0
0

0
1
0

35

0
0
1

3775 =

2664x1

s
x3
x4

35
240

0
0

   

1x1 + 3s = 0
1x3 = 0
1x4 = 0

let   s express the unknowns x1, x3, and x4 in terms of the free variable s.
we immediately see that x3 = 0 and x4 = 0, and we can write x1 =    3s.
therefore, any vector of the form (   3s, s, 0, 0), for any s     r, is in the
null space of a. we write n (a) = span{(   3, 1, 0, 0)t}.
observe that the dim(c(a)) = dim(r(a)) = 3, this is known as the
rank of the matrix a. also, dim(r(a)) + dim(n (a)) = 3 + 1 = 4,
which is the dimension of the input space of the linear transformation ta.

there is an important distinction between matrices that are invertible and

those that are not as formalized by the following theorem.
theorem. for an n   n matrix a, the following statements are equivalent:

1) a is invertible
2) the rref of a is the n    n identity matrix
3) the rank of the matrix is n
4) the row space of a is rn
5) the column space of a is rn
6) a doesn   t have a null space (only the zero vector n (a) = {(cid:126)0})
7) the determinant of a is nonzero det(a) (cid:54)= 0

for a given matrix a, the above statements are either all true or all false.
an invertible matrix a corresponds to a linear transformation ta which
maps the n-dimensional input vector space rn to the n-dimensional output
   1
vector space rn such that there exists an inverse transformation t
a that
can faithfully undo the effects of ta.
on the other hand, an n    n matrix b that is not invertible maps the
input vector space rn to a subspace c(b) (cid:40) rn and has a nonempty null
space. once tb sends a vector (cid:126)w     n (b) to the zero vector, there is no
   1
b that can undo this operation.
t
f. determinants

the determinant of a matrix, denoted det(a) or |a|, is a special way to
combine the entries of a matrix that serves to check if a matrix is invertible
or not. the determinant formulas for 2    2 and 3    3 matrices are

        a11

a21
a12
a22
a32

         = a11a22     a12a21,
             = a11
             a12

        a22

a32

a12
a22
a13
a23
a33

and

        a21

a31

            a11

a22
a21
a32
a31
if the |a| = 0 then a is not invertible. if |a| (cid:54)= 0 then a is invertible.
g. eigenvalues and eigenvectors

a23
a33

a23
a33

         + a13

        a21

a31

        .

the set of eigenvectors of a matrix is a special set of input vectors for
which the action of the matrix is described as a simple scaling. when
a matrix is multiplied by one of its eigenvectors the output is the same
eigenvector multiplied by a constant a(cid:126)e   =   (cid:126)e  . the constant    is called
an eigenvalue of a.

to    nd the eigenvalues of a matrix we start from the eigenvalue equation
a(cid:126)e   =   (cid:126)e  , insert the identity 1, and rewrite it as a null-space problem:

a(cid:126)e   =   1(cid:126)e  

   

(a       1) (cid:126)e   = (cid:126)0.

this equation will have a solution whenever |a     1| = 0. the eigenvalues
of a     rn  n, denoted {  1,   2, . . . ,   n}, are the roots of the characteristic
polynomial p(  ) = |a       1|. the eigenvectors associated with the
eigenvalue   i are the vectors in the null space of the matrix (a       i1).
certain matrices can be written entirely in terms of their eigenvectors
and their eigenvalues. consider the matrix    that has the eigenvalues of
the matrix a on the diagonal, and the matrix q constructed from the
eigenvectors of a as columns:

266666664

  1

...

0

      
...
0

   =

377777775, q =
266666664

0

0
  n

    
    

(cid:126)e  1

377777775,

|
       (cid:126)e  n
|

then a = q  q   1.

matrices that can be written this way are called diagonalizable.

the decomposition of a matrix into its eigenvalues and eigenvectors
gives valuable insights into the properties of the matrix. google   s original
id95 algorithm for ranking webpages by    importance    can be
formalized as an eigenvector calculation on the matrix of web hyperlinks.

vi. textbook plug

if you   re interested in learning more about id202, you can check
out my new book, the no bullshit guide to id202.
a pre-release version of the book is available here: gum.co/nobsla

