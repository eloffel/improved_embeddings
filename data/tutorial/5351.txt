automated image captioning with 

convnets and recurrent nets

andrej karpathy, fei-fei li

automated image captioning with 

convnets and recurrent nets

andrej karpathy, fei-fei li

natural language

images of me scuba diving next to turtle

images of me scuba diving next to turtle

very hard task
images of me scuba diving next to turtle

very hard task
vzntrf bs zr fphon qvivat arkg gb ghegyr

very hard task
vzntrf bs zr fphon qvivat arkg gb ghegyr

neural networks practitioner

describing images

recurrent neural network

convolutional neural network

convolutional neural networks

image 
(32*32 
numbers)

differentiable function

class probabilities
(10 numbers)

[lecun et al., 1998]

[krizhevsky, sutskever, hinton. 2012] 16.4% error

[krizhevsky, sutskever, hinton. 2012] 16.4% error

[zeiler and fergus, 2013] 11.1% error

[krizhevsky, sutskever, hinton. 2012] 16.4% error

[szegedy et al., 2014]  6.6% error
[simonyan and zisserman, 2014] 7.3% error

[zeiler and fergus, 2013] 11.1% error

[szegedy et al., 2014] 
6.6% error
[simonyan and zisserman, 2014] 
7.3% error

human error: ~5.1%
optimistic human error: ~3%
read more on my blog:
karpathy.github.io

   very deep convolutional networks for large-scale visual recognition   
[simonyan and zisserman, 2014]
   vggnet    or    oxfordnet   
very simple and homogeneous. 
(and available in caffe.)

   very deep convolutional networks for large-scale visual recognition   
[simonyan and zisserman, 2014]
   vggnet    or    oxfordnet   
very simple and homogeneous. 
(and available in caffe.)

[224x224x3]

[1000]

conv

   very deep convolutional networks for large-scale visual recognition   
[simonyan and zisserman, 2014]
   vggnet    or    oxfordnet   
very simple and homogeneous. 
(and available in caffe.)

   very deep convolutional networks for large-scale visual recognition   
[simonyan and zisserman, 2014]
   vggnet    or    oxfordnet   
very simple and homogeneous. 
(and available in caffe.)

conv

pool

   very deep convolutional networks for large-scale visual recognition   
[simonyan and zisserman, 2014]
   vggnet    or    oxfordnet   
very simple and homogeneous. 
(and available in caffe.)

conv

pool

fully-connected

every layer of a convnet has the same api:
- takes a 3d volume of numbers
- outputs a 3d volume of numbers
- constraint: function must be differentiable

image
[224x224x3]

probabilities
[1x1x1000]

fully connected layer

[1x1x4096]    neurons   

[7x7x512]

every    neuron    in the output:
1.

computes a dot product between the 
input and its weights

2.

thresholds it at zero

fully connected layer

[1x1x4096]    neurons   

[7x7x512]

the whole layer can be implemented 
very efficiently as:
1. single matrix multiply
2. elementwise thresholding at zero

convolutional layer

224

224

224

224

d = 3

64

every blue neuron is connected to a 3x3x3 array of inputs

convolutional layer

can be 
implemented 
efficiently with 
convolutions

224

224

224

224

d = 3

64

every blue neuron is connected to a 3x3x3 array of inputs

pooling layer

[224x224x64]

[112x112x64]

performs (spatial) downsampling

pooling layer

224

224

pooling layer

224

224

downsampling

112

112

max pooling layer

x

single depth slice
1
5
3
1

1
6
2
2

2
7
1
3

4
8
0
4

y

max pool

6
3

8
4

what do the neurons learn?

[taken from yann lecun slides]

example activation maps
poolconv

pool conv

conv

conv

relu

conv

relu

relu

relu

relu

relu

(fully-connected)

conv

pool fc

example activation maps
poolconv

pool conv

conv

conv

relu

conv

relu

relu

relu

relu

relu

(fully-connected)

conv

pool fc

(tiny vggnet trained with convnetjs)

[224x224x3]

differentiable function

[1000]

[224x224x3]

differentiable function

0.2

0.4

0.09

0.01

0.3

cat       dog     chair   bagel  banana

[1000]

[224x224x3]

differentiable function

0.2

0.4

0.09

0.01

0.3

cat       dog     chair   bagel  banana

[1000]

training

loop until tired:
1. sample a batch of data
2. forward it through the network to get predictions
3. backprop the errors
4. update the weights

training

loop until tired:
1. sample a batch of data
2. forward it through the network to get predictions
3. backprop the errors
4. update the weights

[image credit:
karen simonyan]

summary so far:

convolutional networks express a single 
differentiable function from raw image pixel 
values to class probabilities.

recurrent neural network

convolutional neural network

plug
- fei-fei and i are 

teaching cs213n (a 
convolutional neural 
networks class) at 
stanford this quarter. 
cs231n.stanford.edu

- all the notes are online: 

cs231n.github.io

- assignments are on 

terminal.com

recurrent neural network

recurrent networks are good at modeling sequences...

generating sequences with recurrent neural networks
[alex graves, 2014]

recurrent networks are good at modeling sequences...

word-level language model. similar to:

recurrent neural network based language model
[tomas mikolov, 2010]

recurrent networks are good at modeling sequences...

machine translation model

french words

english words

sequence to sequence learning with neural networks
[ilya sutskever, oriol vinyals, quoc v. le, 2014]

2-layer lstm

recurrentjs
train recurrent 
networks in 
javascript!*

*if you have a lot of time :)

2-layer lstm

recurrentjs
train recurrent networks 
in javascript!*

*if you have a lot of time :)

character-level paul graham wisdom generator:

suppose we had the training sentence    cat sat on mat   

we want to train a language model:
p(next word | previous words)

suppose we had the training sentence    cat sat on mat   

we want to train a language model:
p(next word | previous words)

i.e. want these to be high:
p(cat | [<s>]) 
p(sat | [<s>, cat])
p(on | [<s>, cat, sat])
p(mat | [<s>, cat, sat, on])

   cat sat on mat   

y0

y1

y2

y3

y4

h0

h1

h2

h3

h4

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

300 (learnable) numbers 
associated with each word

p(word | [<s>])

p(word | [<s>, cat, sat])

p(word | [<s>, cat])

p(word | [<s>, cat, sat, on])

   cat sat on mat   

p(word | [<s>, cat, sat, on, mat])

y0

y1

y2

y3

y4

h0

h1

h2

h3

h4

10,001 numbers (logprobs for 
10,000 words in vocabulary and 
a special <end> token)
y4 = why * h4

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

300 (learnable) numbers 
associated with each word

p(word | [<s>])

p(word | [<s>, cat, sat])

p(word | [<s>, cat])

p(word | [<s>, cat, sat, on])

   cat sat on mat   

p(word | [<s>, cat, sat, on, mat])

y0

y1

y2

y3

y4

h0

h1

h2

h3

h4

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

10,001 numbers (logprobs for 
10,000 words in vocabulary and 
a special <end> token)
y4 = why * h4

   hidden    representation  mediates 
the contextual information
(e.g. 200 numbers)
h4 = max(0, wxh * x4 + whh * h3)

300 (learnable) numbers 
associated with each word

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

x0

<start>

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

h0

x0

<start>

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

h0

sample!

x0

<start>

x1
   cat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

h0

h1

x0

<start>

x1
   cat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

h0

h1

sample!

x0

<start>

x1
   cat   

x2
   sat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

y2

h0

h1

h2

x0

<start>

x1
   cat   

x2
   sat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

y2

sample!

h0

h1

h2

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

y2

y3

h0

h1

h2

h3

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

y2

y3

sample!

h0

h1

h2

h3

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

y0

y1

y2

y3

y4

h0

h1

h2

h3

h4

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

training this on a lot of 
sentences would give us a 
language model. a way to 
predict 

p(next word | previous words)

samples <end>? done.

y0

y1

y2

y3

y4

h0

h1

h2

h3

h4

x0

<start>

x1
   cat   

x2
   sat   

x3
   on   

x4
   mat   

recurrent neural network

convolutional neural network

   straw hat   

training example

   straw hat   

training example

   straw hat   

training example

x

   straw hat   

training example

y0

y1

y2

h0

h1

h2

x

x0

<sta
rt>

x1

   straw   

<start> straw

x2
   hat   

hat

   straw hat   

y0

y1

y2

training example

h0

h1

h2

before:
h0 = max(0, wxh * x0)

x

x0

<sta
rt>

x1

   straw   

<start> straw

x2
   hat   

hat

now:
h0 = max(0, wxh * x0 + wih * v)

   straw hat   

training example

y0

y1

y2

h0

h1

h2

x

x0

<sta
rt>

x1

   straw   

<start> straw

x2
   hat   

hat

test image

test image

x0

<sta
rt>

<start>

test image

y0

h0

x0

<sta
rt>

<start>

test image

y0

h0

sample!

x0

<sta
rt>

x1

<start>

test image

y0

y1

h0

h1

x0

<sta
rt>

straw

<start>

test image

y0

y1

h0

h1

sample!

x0

<sta
rt>

straw

hat

<start>

test image

y0

y1

y2

h0

h1

h2

x0

<sta
rt>

straw

hat

<start>

test image

y0

y1

y2

h0

h1

h2

sample!
<end> token
=> finish.

x0

<sta
rt>

straw

hat

<start>

test image

y0

y1

y2

h0

h1

h2

sample!
<end> token
=> finish.

x0

<sta
rt>

straw

hat

<start>

- don   t have to do greedy 

word-by-word sampling, can 
also search over longer 
phrases with id125

id56 vs. lstm

   hidden    representation 
(e.g. 200 numbers)
h1 = max(0, wxh * x1 + whh * h0)

y0

y1

h0

h1

x0

<start>

x1
   cat   

id56 vs. lstm

   hidden    representation 
(e.g. 200 numbers)
h1 = max(0, wxh * x1 + whh * h0)

lstm changes the form of the equation for 
h1 such that:
1. more expressive multiplicative interactions
2. gradients flow nicer
3. network can explicitly decide to reset the 

hidden state

y0

y1

h0

h1

x0

<start>

x1
   cat   

image sentence datasets

microsoft coco
[tsung-yi lin et al. 2014]
mscoco.org

currently:
~120k images
~5 sentences each

training an id56/lstm...
- clip the gradients (important!). 5 worked ok
- rmsprop adaptive learning rate worked nice
- initialize softmax biases with log word 

frequency distribution

- train for long time

+ id21

   straw hat   

y0

y1

y2

training example

h0

h1

h2

x0
<st
art

>

x1
   stra
w   

<start> straw

x2
   hat   

hat

+ id21

use weights 
pretrained from 
id163

   straw hat   

y0

y1

y2

training example

h0

h1

h2

x0
<st
art

>

x1
   stra
w   

<start> straw

x2
   hat   

hat

+ id21

use weights 
pretrained from 
id163

   straw hat   

y0

y1

y2

training example

h0

h1

h2

x0
<st
art

>

x1
   stra
w   

<start> straw

x2
   hat   

hat

use word vectors 
pretrained with 
id97 [1]

[1] mikolov et al., 2013

summary of the approach

we wanted to describe images with sentences.

initialize parts of net from elsewhere if possible

1. define a single function from input -> output
2.
3. get some data
4. train with sgd

wow i can   t believe that worked

wow i can   t believe that worked

well, i can kind of see it

well, i can kind of see it

not sure what happened there...

see predictions on 
1000 coco images:
http://bit.ly/neuraltalkdemo

what this approach doesn   t do:

- there is no reasoning

- a single glance is taken at the image, no 

objects are detected, etc.

- we can   t just describe any image

neuraltalk

- code on github
- both id56/lstm

- python+numpy (cpu)
- matlab+caffe if you want 
to run on new images (for 
now)

ranking model

ranking model
web demo:
http://bit.ly/rankingdemo

recurrent neural network

convolutional neural network

summary
neural networks:
- input->output end-to-end optimization
- stackable / composable like lego
- easily support id21
- work very well.

summary
1. image -> sentence
2. sentence -> image

summary
1. image -> sentence
2. sentence -> image

natural language

summary
1. image -> sentence
2. sentence -> image

natural language

thank you!

