   #[1]github [2]recent commits to deep-learning-python:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]6
     * [35]star [36]70
     * [37]fork [38]18

[39]lesley2958/[40]deep-learning-python

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   intro to deep learning, including recurrent, convolution, and feed
   forward neural networks.
   [47]curriculum [48]deep-learning [49]deep-neural-networks [50]python
   [51]theano [52]lasagne
     * [53]41 commits
     * [54]1 branch
     * [55]0 releases
     * [56]fetching contributors

    1. [57]jupyter notebook 98.0%
    2. [58]python 2.0%

   (button) jupyter notebook python
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/l
   [60]download zip

downloading...

   want to be notified of new releases in lesley2958/deep-learning-python?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [67]permalink
   type                 name                latest commit message commit time
        failed to load latest commit information.
        [68]mnist_data
        [69].gitignore
        [70]deep learning with python.ipynb
        [71]readme.md
        [72]input_data.py
        [73]m.py

readme.md

intro to deep learning with python

   brought to you by [74]lesley cordero and [75]adi

table of contents

     * [76]0.0 setup
          + [77]0.1 python and pip
          + [78]0.2 libraries
     * [79]1.0 background
          + [80]1.1 what is deep learning?
          + [81]1.2 neural networks
               o [82]1.2.1 what is a neural network?
               o [83]1.2.2 input layer
               o [84]1.2.3 hidden layers
               o [85]1.2.4 output layer
               o [86]1.2.5 activation function
          + [87]1.3 types of neural networks
               o [88]1.3.1 feedforward neural networks
               o [89]1.3.2 convolutional neural networks
               o [90]1.3.3 recurrent neural networks
          + [91]1.4 id26
               o [92]1.4.1 backprop algorithm
          + [93]1.5 id119
          + [94]1.6 hardware
               o [95]1.6.1 gpu
     * [96]2.0 building a neural net
          + [97]2.1 generating a dataset
          + [98]2.2 id28
          + [99]2.3 id119 & id168
          + [100]2.4 training a neural network
     * [101]3.0 python modules
          + [102]3.1 theano
               o [103]3.1.1 why theano?
               o [104]3.1.2 symbolic variables
               o [105]3.1.3 symbolic functions
          + [106]3.2 tensorflow
          + [107]3.3 lasagne
          + [108]3.4 caffe
     * [109]4.0 feedforward neural networks
     * [110]5.0 recurrent neural networks
     * [111]6.0 convolution neural networks
     * [112]7.0 final words
          + [113]7.1 resources
          + [114]7.2 mini courses

0.0 setup

   this guide was written in python 3.5.

0.1 python and pip

   download [115]python and [116]pip.

0.2 libraries

   let's install the modules we'll need for this tutorial. open up your
   terminal and enter the following commands to install the needed python
   modules:
pip3 install sklearn
pip3 install lasagne
pip3 install theano
pip3 install keras
pip3 install tensorflow
pip3 install imutils

1.0 background

1.1 what is deep learning?

   deep learning is a branch of machine learning that involves pattern
   recognition on unlabeled or unstructured data. it uses a model of
   computing inspired by the structure of the brain, which we call this
   model a neural network.

1.2 neural networks

1.2.1 what is a neural network?

   neural networks get their representations from using layers of
   learning. the general structure of a neural network looks like
   [117]this.

   meanwhile, a typical singular neuron looks like [118]this.
     * the x[1], x[2],   , x[n] variables are the inputs. these can either
       be the actual observations from the input layer or an intermediate
       value from one of the hidden layers.
     * x[0] is the bias unit. this is a constant value added to the input
       of the activation function.
     * w[0],w[1], w[2],   ,w[n] are the weights on each input - note that
       even the bias unit has a weight.
     * a is the output of the neuron, which we calculate from [119]this
       formula, where f is the activation function (you can find more on
       this in section 1.2.5)

1.2.2 input layer

   the first layer is a type of visible layer called an input layer. this
   layer contains an input node for each of the entries in our feature
   vector. from there, these nodes connect to a series of hidden layers.

1.2.3 hidden layers

   these are the intermediate layers between input and output which help
   the neural network learn the complicated relationships involved in
   data. the final hidden layer then connects to an output layer.

1.2.4 output layer

   the final output is extracted from the previous two layers. this layer
   contains the output probabilities for each class label. for example, in
   the case of a classification problem with 5 classes, the output later
   will have 5 neurons, each with its respective id203. the output
   node that produces the largest id203 is chosen as the overall
   classification.

1.2.5 activation function

   the activation function allows the neural network to be flexible and
   have the capability to estimate complex non-linear relationships in
   data. it can be a gaussian function, logistic function, hyperbolic
   function or even a linear function in simple cases.

1.3 types of neural networks

1.3.1 feedforward neural networks

   feedforward neural networks are the simplest form of artificial neural
   networks. these networks have the three types of layers we just
   discussed: input layer, hidden layer and output layer.

1.3.2 convolutional neural networks

   convolutional neural networks are a type of feed-forward network. what
   distinguishes convets are that the architectures make the explicit
   assumption that the inputs are images, allowing us to encode certain
   properties into the architecture. these then make the forward function
   more efficient to implement and vastly reduce the amount of parameters
   in the network.

1.3.3 recurrent neural networks

   a recurrent neural network is a class of anns where connections between
   units form a directed cycle, as shown [120]here.

1.4 id26

   back-propagation algorithms work by determining the loss (or error) at
   the output and then propagating it back into the network. the weights
   are updated to minimize the error resulting from each neuron, since the
   goal of learning is to assign correct weights for these edges.

   in other words, backprop is one of the several ways in which an
   id158 can be trained. it is a supervised training
   scheme, which means, it learns from labeled training data. to put in
   simple terms, backprop learns from its mistakes by having the
   supervisor correct the ann whenever it makes mistakes.

   note: to forward propagate is to get the output and compare it with the
   real value to get the error. to back propagate is to minimize the error
   by propagating backwards by finding the derivative of error with
   respect to each weight and the subtracting this value from the weight
   value.

1.4.1 backprop algorithm

   initially all the edge weights are randomly assigned. for every input
   in the training dataset, the ann is activated and its output is
   observed. this output is compared with the desired output that we
   already know, and the error is "propagated" back to the previous layer.
   this error is noted and the weights are "adjusted" accordingly. this
   process is repeated until the output error is below a predetermined
   threshold.

   once the above algorithm terminates, we have a "learned" ann which we
   consider ready to work with "new" inputs. this ann is said to have
   learned from several examples (labeled data) and from its mistakes
   (error propagation).

1.5 id119

   id119 is a standard tool for interactively optimizing
   complex functions. given some arbitrary function, id119's
   goal is to find a minimum. for some small subset of functions - those
   that are convex - there's just a single minimum which also happens to
   be global. but for most realistic functions, there may be many minima,
   so most minima are local.

   the main premise of id119 is: given some current location x
   in the search space (the domain of the optimized function) we update x
   for the next step in the direction opposite to the gradient of the
   function computed at x.

1.6 hardware

1.6.1 gpu & cpu

   the gpu   s advanced capabilities were originally used primarily for 3d
   game rendering, but are now being used more broadly to accelerate
   computational workloads in deep learning.

   architecturally, the cpu is composed of just a few cores with lots of
   cache memory that can handle a few software threads at a time. in
   contrast, a gpu is composed of hundreds of cores that can handle
   thousands of threads simultaneously.

   a simple way to understand the difference between a gpu and a cpu is to
   compare how they process tasks. a cpu consists of a few cores optimized
   for sequential serial processing while a gpu has a massively parallel
   architecture consisting of thousands of smaller, more efficient cores
   designed for handling multiple tasks simultaneously.

2.0 building a neural net

2.1 generating a dataset

   first, we import all the needed modules.
import matplotlib.pyplot as plt
import numpy as np
import sklearn
import sklearn.datasets
import sklearn.linear_model
import matplotlib

   the following just displays plots inline and changes default figure
   size
matplotlib.rcparams['figure.figsize'] = (10.0, 8.0)

   let's start by generating a dataset to use. fortunately, scikit-learn
   has some useful dataset generators, so we don't need to write the code
   ourselves and will go with the make_moons function instead.
np.random.seed(0)
x, y = sklearn.datasets.make_moons(200, noise=0.20)

   using built-in scatter plot functions, we'll do a quick visualization
   of our data.
plt.scatter(x[:,0], x[:,1], s=40, c=y, cmap=plt.cm.spectral)
plt.show()

   as you can see, the dataset we generated has two classes, plotted as
   red and blue points. our goal is to train a machine learning classifier
   that predicts the correct class given the x and y coordinates.

   because the data is not linearly separable, we can't draw a straight
   line that separates the two classes, which means that linear
   classifiers, such as id28, won't be able to fit the
   data. but that's one of the major advantages of neural networks - we
   won't need to worry about feature engineering because the hidden layer
   of a neural network will learn features for us.

2.2 id28

   to demonstrate this, let's train a id28 classifier. its
   input will be the x and y values and the output the predicted class (0
   or 1). since scikit-learn has a id28 class built in,
   we'll go ahead and use that.
clf = sklearn.linear_model.logisticregressioncv()
clf.fit(x, y)

   now we create a helper function to plot a decision boundary. if you
   don't fully understand this function don't worry, it just generates the
   contour plot below.
def plot_decision_boundary(pred_func):
    # set min and max values and give it some padding
    x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5
    y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5
    h = 0.01
    # generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # predict the function value for the whole gid
    z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    z = z.reshape(xx.shape)
    # plot the contour and training examples
    plt.contourf(xx, yy, z, cmap=plt.cm.spectral)
    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.spectral)

   now, let's plot the decision boundary
plot_decision_boundary(lambda x: clf.predict(x))
plt.title("id28")
plt.show()

   as you can see, this id28 function isn't quite so good!
   it fails to capture the true form of the data.

2.3 id119 & id168

   in section 1, we briefly covered id119 and id168s.
   here, we'll actually implement these functions and use them to train
   our neural net. so let's start by defining some useful variables and
   parameters for id119:
num_examples = len(x) # training set size
nn_input_dim = 2 # input layer dimensionality
nn_output_dim = 2 # output layer dimensionality

   we need to define the parameters for id119, specifically the
   learning rate and the id173 strength.

2.3.1 learning rate

   the learning rate determines how fast or slow we will move towards the
   optimal weights. if the learning rate is too large the optimal solution
   will be skipped. but if it's too small we'll need too many iterations
   to converge to the best values. so using a good learning rate is
   crucial. typically, a good technique is to adapt the value of learning
   rate in each iteration, but for this example, we'll just hand-pick one.

2.3.2 id173 strength

   the id173 parameter,    reduces overfitting, which reduces the
   variance of your estimated regression parameters. however, it does this
   at the expense of adding bias to your estimate - a common tradeoff
   you'll encounter in the field of machine learning.

   one approach you can take is to randomly subsample your data a number
   of times and look at the variation in your estimate. then repeat the
   process for a slightly larger value of lambda to see how it affects the
   variability of your estimate. keep in mind that whatever value of
   lambda you decide is appropriate for your subsampled data, you can
   likely use a smaller value to achieve comparable id173 on the
   full data set.

   with that said, in this exercise, i've just hand-picked a value.

2.3.3 code

   here are our hand-chosen rates:
epsilon = 0.01 # learning rate for id119
reg_lambda = 0.01 # id173 strength

   next, we create a helper function to evaluate the total loss on the
   dataset.
def calculate_loss(model):
    w1, b1, w2, b2 = model['w1'], model['b1'], model['w2'], model['b2']
    # forward propagation to calculate our predictions
    z1 = x.dot(w1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(w2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
    # calculating the loss
    corect_logprobs = -np.log(probs[range(num_examples), y])
    data_loss = np.sum(corect_logprobs)
    # add regulatization term to loss
    data_loss += reg_lambda/2 * (np.sum(np.square(w1)) + np.sum(np.square(w2)))
    return (1./num_examples * data_loss)

   we also implement a helper function to calculate the output of the
   network. it does forward propagation as defined above and returns the
   class with the highest id203.
def predict(model, x):
    w1, b1, w2, b2 = model['w1'], model['b1'], model['w2'], model['b2']
    # forward propagation
    z1 = x.dot(w1) + b1
    a1 = np.tanh(z1)
    z2 = a1.dot(w2) + b2
    exp_scores = np.exp(z2)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
    return (np.argmax(probs, axis=1))

2.4 training a neural network

   finally, we train our neural network. here, we implement batch gradient
   descent using the id26 derivates we found above.
def build_model(nn_hdim, num_passes=20000, print_loss=false):
    np.random.seed(0)
    w1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)
    b1 = np.zeros((1, nn_hdim))
    w2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)
    b2 = np.zeros((1, nn_output_dim))
    model = {}
    for i in range(0, num_passes):
        z1 = x.dot(w1) + b1
        a1 = np.tanh(z1)
        z2 = a1.dot(w2) + b2
        exp_scores = np.exp(z2)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
        delta3 = probs
        delta3[range(num_examples), y] -= 1
        dw2 = (a1.t).dot(delta3)
        db2 = np.sum(delta3, axis=0, keepdims=true)
        delta2 = delta3.dot(w2.t) * (1 - np.power(a1, 2))
        dw1 = np.dot(x.t, delta2)
        db1 = np.sum(delta2, axis=0)
        dw2 += reg_lambda * w2
        dw1 += reg_lambda * w1
        w1 += -epsilon * dw1
        b1 += -epsilon * db1
        w2 += -epsilon * dw2
        b2 += -epsilon * db2
        model = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}
        if print_loss and i % 1000 == 0:
            print("loss after iteration %i: %f" %(i, calculate_loss(model)))
    return (model)

model = build_model(3, print_loss=true)

   here, we're just plotting the decision boundary
plot_decision_boundary(lambda x: predict(model, x))
plt.title("decision boundary for hidden layer size 3")
plt.show()

   let's see what happens if we train a network with a hidden layer size
   of 3.
plt.figure(figsize=(16, 32))
hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    plt.subplot(5, 2, i+1)
    plt.title('hidden layer size %d' % nn_hdim)
    model = build_model(nn_hdim)
    plot_decision_boundary(lambda x: predict(model, x))
plt.show()

   we can see that while a hidden layer of low dimensionality nicely
   captures the general trend of our data, but higher dimensionalities are
   prone to overfitting. they are "memorizing" the data as opposed to
   fitting the general shape. if we were to evaluate our model on a
   separate test set (and you should!) the model with a smaller hidden
   layer size would likely perform better because it generalizes better.
   we could counteract overfitting with stronger id173, but
   picking the correct size for a hidden layer is a much more efficient
   solution.

3.0 python modules

3.1 theano

   theano is a python library that is used to define, optimize, and
   evaluate mathematical expressions with multi-dimensional arrays. theano
   accomplishes this through its own data structures integrated with numpy
   and the transparent use of the gpu. more specifically, theano figures
   out which computational portions should be moved to the gpu.

   theano isn   t actually a machine learning library since it doesn   t
   provide you with pre-built models to train on your data. instead, it's
   a mathematical library that provides you with tools to build your own
   machine learning models.

3.1.1 why theano?

   simply put, theano's strong suit is efficiency. its primary purpose is
   to increase the speed of computation.

   how does it accomplish this? identifying 'small' changes like (x+y) +
   (x+y) to 2*(x+y), over time, make a substantial difference. moreover,
   because it defines different mathematical expressions in c, it makes
   for much faster implementations. and because of this, theano works well
   in high dimensionality problems. lastly, it allows gpu implementation.

3.1.2 symbolic variables

   in theano, all algorithms are defined symbolically, meaning that they
   don't have an explicit value.

3.1.3 symbolic functions

   to actually perform computations with theano, you use symbolic
   functions, which can later be called with actual values. symbolic
   functions allow us to automatically derive gradient expressions.

   first, we import the needed libraries:
import theano
import numpy

   next, we create the building blocks of our function. here, x is a
   vector, w is an array we set up with numpy, and y is the function we'll
   use to compute the result.
x = theano.tensor.fvector('x')
w = theano.shared(numpy.asarray([0.2, 0.7]), 'w')
y = (x * w).sum()

   finally, we actually perform the computation with theano.
f = theano.function([x], y)

output = f([1.0, 1.0])

   if we print output, we get (0.2*1.0) + (0.7*1.0):
0.9

3.2 tensorflow

   tensorflow is an open source library for numerical computation using
   data flow graphs. unlike theano, however, tensorflow handles
   distributed computing through the use of multiple-gpus.

   we'll go through a classic deep learning problem involving hand-written
   digit recognition, using the mnist dataset. first, we'll implement the
   single layer version and follow up with a multi-layer model in section
   5 of this workshop.

3.2.1 single layer neural network

   as always, we'll need to input the needed modules. input_data.py is
   available on the github link [121]here - make sure to download it and
   include it in the same directory as your workspace. this will allow you
   to download the needed data.
import tensorflow as tf
import input_data

mnist = input_data.read_data_sets("mnist_data/", one_hot=true)

   we'll need to create two variables to keep track of the weights and
   bias. since we don't know those values yet, we initialize them to
   zeros.
w = tf.variable(tf.zeros([784,10]))
b = tf.variable(tf.zeros([10]))

   in this example, we also create a tensor of two dimensions to keep the
   information of the x points with the following line of code:
x = tf.placeholder("float", [none, 784])

   next we multiply the image vector x and the weight matrix w, adding b:
y = tf.nn.softmax(tf.matmul(x,w) + b)

   next, we create another placeholder for the correct labels.
y_ = tf.placeholder("float", [none,10])

   here, we figure out our cost function.
cross_id178 = -tf.reduce_sum(y_*tf.log(y))

   using the backpropogation algorithm, we minimize the cross-id178
   using the id119 algorithm and a learning rate of 0.01:
train_step = tf.train.gradientdescentoptimizer(0.01).minimize(cross_id178)

   now we can start the computation by instantiating tf.session(). this is
   in charge of executing the tensorflow operations in the available cpus
   or gpus. then, we can execute the operation initializing all the
   variables:
sess = tf.session()
sess.run(tf.initialize_all_variables())

   now, we can start training our model!
for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}
)

3.3 lasagne

   lasagne is a lightweight library used to construct and train networks
   in theano. lasagne does this by providing an api for constructing
   layers of a network and getting theano expressions that represent
   output, loss, etc.

   as always, we import the needed modules first:
import lasagne
import theano
import theano.tensor as t

   then we begin by creating theano variables for input and target
   minibatch:
input_var = t.tensor4('x')
target_var = t.ivector('y')

   here, we create a small convolutional neural network:
from lasagne.nonlinearities import leaky_rectify, softmax
network = lasagne.layers.inputlayer((none, 3, 32, 32), input_var)
network = lasagne.layers.conv2dlayer(network, 64, (3, 3),
                                     nonlinearity=leaky_rectify)
network = lasagne.layers.conv2dlayer(network, 32, (3, 3),
                                     nonlinearity=leaky_rectify)
network = lasagne.layers.pool2dlayer(network, (3, 3), stride=2, mode='max')
network = lasagne.layers.denselayer(lasagne.layers.dropout(network, 0.5),
                                    128, nonlinearity=leaky_rectify,
                                    w=lasagne.init.orthogonal())
network = lasagne.layers.denselayer(lasagne.layers.dropout(network, 0.5),
                                    10, nonlinearity=softmax)

   and of course, our id168:
prediction = lasagne.layers.get_output(network)
loss = lasagne.objectives.categorical_crossid178(prediction, target_var)
loss = loss.mean() + 1e-4 * lasagne.id173.regularize_network_params(
        network, lasagne.id173.l2)

   next, we create parameter updating expressions:
params = lasagne.layers.get_all_params(network, trainable=true)
updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01,
                                            momentum=0.9)

   next, we need to compile a training function that updates parameters
   and returns training loss:
train_fn = theano.function([input_var, target_var], loss, updates=updates)

   and finally, we train the network:
for epoch in range(100):
    loss = 0
    for input_batch, target_batch in training_data:
        loss += train_fn(input_batch, target_batch)
    print("epoch %d: loss %g" % (epoch + 1, loss / len(training_data)))

   now we use the trained network for predictions:
test_prediction = lasagne.layers.get_output(network, deterministic=true)
predict_fn = theano.function([input_var], t.argmax(test_prediction, axis=1))
print("predicted class for first test input: %r" % predict_fn(test_data[0]))

3.4 keras

   keras is a super powerful, easy to use python library for building
   neural networks and deep learning networks.

3.5 caffe

   caffe is a deep learning framework written in c++. we won't be going
   through an implementation of caffe, but it's important to note its
   existence in deep learning, as well as the advantages and disadvantages
   of using the module.

   let's begin with why we won't be going into details of caffe: it
   requires thorough knowledge of architecture, since it's written in c++.
   this makes it a lot harder to work with, especially as someone just
   entering the field of deep learning.

   on the other hand, because caffe is written in c++, it's incredibly
   fast, even faster than theano, and provides access via the command
   line.

4.0 feedforward neural networks

   feedforward neural networks are the simplest form of artificial neural
   networks. these networks have the three types of layers we just
   discussed: input layer, hidden layer and output layer. there are no
   backwards or inter-layer connections. furthermore, the nodes in the
   layer are fully connected to the nodes in the next layer.

   first, we import the needed modules.
from sklearn.preprocessing import labelencoder
from sklearn.cross_validation import train_test_split
from keras.models import sequential
from keras.layers import activation
from keras.optimizers import sgd
from keras.layers import dense
from keras.utils import np_utils
from imutils import paths
import numpy as np
import argparse
import cv2
import os

   this function resizes the image to a fixed size and then flattens it
   into a list of features (raw pixel intensities).
def image_to_feature_vector(image, size=(32, 32)):
    return (cv2.resize(image, size).flatten())

   here, we construct the argument parse and parse the arguments.
ap = argparse.argumentparser()
ap.add_argument("-d", "--dataset", required=true,
    help="path to input dataset")
args = vars(ap.parse_args())

   so here we grab the list of images that we'll be describing and
   initialize the data matrix and labels list.
print("[info] describing images...")
imagepaths = list(paths.list_images(args["dataset"]))

data = []
labels = []

   iterating over the input images, we load the image and extract the
   class label. secondly, we construct a feature vector of raw pixel
   intensities and then update the matrix and list.
# loop over the input images
for (i, imagepath) in enumerate(imagepaths):
    image = cv2.imread(imagepath)
    label = imagepath.split(os.path.sep)[-1].split(".")[0]

    features = image_to_feature_vector(image)
    data.append(features)
    labels.append(label)

    # show an update every 1,000 images
    if i > 0 and i % 1000 == 0:
        print("[info] processed {}/{}".format(i, len(imagepaths)))

le = labelencoder()
labels = le.fit_transform(labels)

data = np.array(data) / 255.0
labels = np_utils.to_categorical(labels, 2)

print("[info] constructing training/testing split...")
(traindata, testdata, trainlabels, testlabels) = train_test_split(
    data, labels, test_size=0.25, random_state=42)

   here, we're just defining the architecture of the network
model = sequential()
model.add(dense(768, input_dim=3072, init="uniform",
    activation="relu"))
model.add(dense(384, init="uniform", activation="relu"))
model.add(dense(2))
model.add(activation("softmax"))

   now, we begin training the model:
print("[info] compiling model...")
sgd = sgd(lr=0.01)
model.compile(loss="binary_crossid178", optimizer=sgd,
    metrics=["accuracy"])
model.fit(traindata, trainlabels, nb_epoch=50, batch_size=128)

print("[info] evaluating on testing set...")
(loss, accuracy) = model.evaluate(testdata, testlabels,
    batch_size=128, verbose=1)
print("[info] loss={:.4f}, accuracy: {:.4f}%".format(loss,
    accuracy * 100))

5.0 recurrent neural networks

   as we said in section 1, a recurrent neural network is a class of anns
   where connections between units form a directed cycle, as shown
   [122]here.

   because recurrent neural networks form directed cycles, information is
   able to persist, meaning it can use its reasoning from previous events.
   a recurrent neural network can be thought of as multiple copies of the
   same network, each passing a message to a successor.
import copy, numpy as np
np.random.seed(0)

   this helper function just computes the sigmoid nonlinearity.
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

   here, this function converts the output of the sigmoid function to its
   derivative.
def sigmoid_output_to_derivative(output):
    return output*(1-output)

   obviously we need a dataset to train on, so this is where we generate
   that:
int2binary = {}
binary_dim = 8

largest_number = pow(2,binary_dim)
binary = np.unpackbits(
    np.array([range(largest_number)],dtype=np.uint8).t,axis=1)
for i in range(largest_number):
    int2binary[i] = binary[i]

   here, we define our dimension variables!
alpha = 0.1
input_dim = 2
hidden_dim = 16
output_dim = 1

   and, as always, we initialize neural network weights:
synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1
synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1
synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1

synapse_0_update = np.zeros_like(synapse_0)
synapse_1_update = np.zeros_like(synapse_1)
synapse_h_update = np.zeros_like(synapse_h)

   finally, we begin our training.
for j in range(10000):

    a_int = np.random.randint(largest_number/2)
    a = int2binary[a_int]

    b_int = np.random.randint(largest_number/2)
    b = int2binary[b_int]

    c_int = a_int + b_int
    c = int2binary[c_int]

    d = np.zeros_like(c)

    overallerror = 0

    layer_2_deltas = list()
    layer_1_values = list()
    layer_1_values.append(np.zeros(hidden_dim))

    for position in range(binary_dim):

        # generate input and output
        x = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]
]])
        y = np.array([[c[binary_dim - position - 1]]]).t

        # hidden layer (input ~+ prev_hidden)
        layer_1 = sigmoid(np.dot(x,synapse_0) + np.dot(layer_1_values[-1],synaps
e_h))

        # output layer (new binary representation)
        layer_2 = sigmoid(np.dot(layer_1,synapse_1))

        layer_2_error = y - layer_2
        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer
_2))
        overallerror += np.abs(layer_2_error[0])

        d[binary_dim - position - 1] = np.round(layer_2[0][0])

        # store hidden layer so we can use it in the next timestep
        layer_1_values.append(copy.deepcopy(layer_1))

    future_layer_1_delta = np.zeros(hidden_dim)

    for position in range(binary_dim):

        x = np.array([[a[position],b[position]]])
        layer_1 = layer_1_values[-position-1]
        prev_layer_1 = layer_1_values[-position-2]

        # error at output layer
        layer_2_delta = layer_2_deltas[-position-1]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta.dot(synapse_h.t) + layer_2_delta.d
ot(synapse_1.t)) * sigmoid_output_to_derivative(layer_1)

        # update weights for back-propagation
        synapse_1_update += np.atleast_2d(layer_1).t.dot(layer_2_delta)
        synapse_h_update += np.atleast_2d(prev_layer_1).t.dot(layer_1_delta)
        synapse_0_update += x.t.dot(layer_1_delta)

        future_layer_1_delta = layer_1_delta


    synapse_0 += synapse_0_update * alpha
    synapse_1 += synapse_1_update * alpha
    synapse_h += synapse_h_update * alpha

    synapse_0_update *= 0
    synapse_1_update *= 0
    synapse_h_update *= 0

    # print out progress
    if(j % 1000 == 0):
        print "error:" + str(overallerror)
        print "pred:" + str(d)
        print "true:" + str(c)
        out = 0
        for index,x in enumerate(reversed(d)):
            out += x*pow(2,index)
        print str(a_int) + " + " + str(b_int) + " = " + str(out)
        print "------------"

6.0 convolution neural networks

   convolutional neural networks are a type of feed-forward networks,
   which perform very well on visual recognition tasks.

   a typical feature of id98   s is that they nearly always have images as
   inputs, this allows for more efficient implementation and a reduction
   in the number of required parameters.

   there are two basic principles that define convolution neural networks:
   filters and characteristic maps.

   the main purpose of a convolutional layer is to detect characteristics
   or visual features in the images, such as edges, lines, colors, etc.
   this is done by a hidden layer connected to the input layer.

   this step, called the convolution step, can be shown [123]here. in
   id98s, the 3  3 matrix is called the 'filter' or 'kernel' and the matrix
   formed by sliding the filter over the image and computing the dot
   product is called the    convolved feature    or    activation map    or the
      feature map   . it is important to note that filters act as feature
   detectors from the original input image.

   now, let   s begin our mnist digit recognition example. so naturally,
   first we import the needed modules:
import tensorflow as tf
import input_data

   and of course, the actual dataset:
mnist = input_data.read_data_sets('mnist_data', one_hot=true)

   as before, we'll define the placeholders using tensorflow as we did in
   the exercise in section 3.2.
x = tf.placeholder("float", shape=[none, 784])
y_ = tf.placeholder("float", shape=[none, 10])

   we can reconstruct the original shape of the images of the input data.
   we can do this as follows:
x_image = tf.reshape(x, [-1,28,28,1])

   in order to simplify the code, i define the following two functions
   related to the weight matrix and bias:
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.variable(initial)

   similar to above, we define these two generic functions to be able to
   write a cleaner code that involves convolutions and max-pooling.

   spatial pooling reduces the dimensionality of each feature map but
   retains the most important information. in case of max pooling, a type
   of spatial pooling, we define a spatial neighborhood and take the
   largest element from the rectified feature map within that window.
def conv2d(x, w):
    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='same')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='
same')

   now it is time to implement the first convolutional layer followed by a
   pooling layer.
w_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

   as a final step, we apply max-pooling to the output:
h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)

   when constructing a deep neural network, we can stack several layers on
   top of each other.
w_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

w_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

   now, we want to flatten the tensor into a vector.
h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)

keep_prob = tf.placeholder("float")
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

w_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)

   now we are ready to train the model that we have just defined by
   adjusting all the weights in the convolution, and fully connected
   layers to obtain the predictions of the images.
cross_id178 = -tf.reduce_sum(y_*tf.log(y_conv))
train_step = tf.train.adamoptimizer(1e-4).minimize(cross_id178)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

sess = tf.session()

sess.run(tf.initialize_all_variables())

for i in range(200):
batch = mnist.train.next_batch(50)
if i%10 == 0:
train_accuracy = sess.run( accuracy, feed_dict={ x:batch[0], y_: batch[1], keep_
prob: 1.0})
print("step %d, training accuracy %g"%(i, train_accuracy))
sess.run(train_step,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print("test accuracy %g"% sess.run(accuracy, feed_dict={
x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))

7.0 final words

   deep learning is a math intensive field of machine learning - in this
   tutorial, we omit a lot of the mathematical concepts, so i recommend
   going through these details on your own so you can gain a better
   understanding of why these algorithms work the way they do.

7.1 resources

   [124]complete deep learning book
   [125]deep learning in neural networks

     *    2019 github, inc.
     * [126]terms
     * [127]privacy
     * [128]security
     * [129]status
     * [130]help

     * [131]contact github
     * [132]pricing
     * [133]api
     * [134]training
     * [135]blog
     * [136]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [137]reload to refresh your
   session. you signed out in another tab or window. [138]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/lesley2958/deep-learning-python/commits/master.atom
   3. https://github.com/lesley2958/deep-learning-python#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/lesley2958/deep-learning-python
  32. https://github.com/join
  33. https://github.com/login?return_to=/lesley2958/deep-learning-python
  34. https://github.com/lesley2958/deep-learning-python/watchers
  35. https://github.com/login?return_to=/lesley2958/deep-learning-python
  36. https://github.com/lesley2958/deep-learning-python/stargazers
  37. https://github.com/login?return_to=/lesley2958/deep-learning-python
  38. https://github.com/lesley2958/deep-learning-python/network/members
  39. https://github.com/lesley2958
  40. https://github.com/lesley2958/deep-learning-python
  41. https://github.com/lesley2958/deep-learning-python
  42. https://github.com/lesley2958/deep-learning-python/issues
  43. https://github.com/lesley2958/deep-learning-python/pulls
  44. https://github.com/lesley2958/deep-learning-python/projects
  45. https://github.com/lesley2958/deep-learning-python/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/curriculum
  48. https://github.com/topics/deep-learning
  49. https://github.com/topics/deep-neural-networks
  50. https://github.com/topics/python
  51. https://github.com/topics/theano
  52. https://github.com/topics/lasagne
  53. https://github.com/lesley2958/deep-learning-python/commits/master
  54. https://github.com/lesley2958/deep-learning-python/branches
  55. https://github.com/lesley2958/deep-learning-python/releases
  56. https://github.com/lesley2958/deep-learning-python/graphs/contributors
  57. https://github.com/lesley2958/deep-learning-python/search?l=jupyter-notebook
  58. https://github.com/lesley2958/deep-learning-python/search?l=python
  59. https://github.com/lesley2958/deep-learning-python/find/master
  60. https://github.com/lesley2958/deep-learning-python/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/lesley2958/deep-learning-python
  62. https://github.com/join?return_to=/lesley2958/deep-learning-python
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/lesley2958/deep-learning-python/tree/bf3dcb1372bbf309504bd1824df895a210bf52f9
  68. https://github.com/lesley2958/deep-learning-python/tree/master/mnist_data
  69. https://github.com/lesley2958/deep-learning-python/blob/master/.gitignore
  70. https://github.com/lesley2958/deep-learning-python/blob/master/deep learning with python.ipynb
  71. https://github.com/lesley2958/deep-learning-python/blob/master/readme.md
  72. https://github.com/lesley2958/deep-learning-python/blob/master/input_data.py
  73. https://github.com/lesley2958/deep-learning-python/blob/master/m.py
  74. http://www.columbia.edu/~lc2958
  75. https://adicu.com/
  76. https://github.com/lesley2958/deep-learning-python#00-setup
  77. https://github.com/lesley2958/deep-learning-python#01-python--pip
  78. https://github.com/lesley2958/deep-learning-python#02-libraries
  79. https://github.com/lesley2958/deep-learning-python#10-background
  80. https://github.com/lesley2958/deep-learning-python#11-what-is-deep-learning
  81. https://github.com/lesley2958/deep-learning-python#12-neural-networks
  82. https://github.com/lesley2958/deep-learning-python#121-what-is-a-neural-network
  83. https://github.com/lesley2958/deep-learning-python#122-input-layer
  84. https://github.com/lesley2958/deep-learning-python#123-hidden-layers
  85. https://github.com/lesley2958/deep-learning-python#124-output-layer
  86. https://github.com/lesley2958/deep-learning-python#125-activation-function
  87. https://github.com/lesley2958/deep-learning-python#13-types-of-neural-networks
  88. https://github.com/lesley2958/deep-learning-python#131-feedforward-neural-networks
  89. https://github.com/lesley2958/deep-learning-python#132-convolution-neural-networks
  90. https://github.com/lesley2958/deep-learning-python#133-recurrent-neural-networks
  91. https://github.com/lesley2958/deep-learning-python#14-id26
  92. https://github.com/lesley2958/deep-learning-python#141-backprop-algorithm
  93. https://github.com/lesley2958/deep-learning-python#15-gradient-descent
  94. https://github.com/lesley2958/deep-learning-python#16-hardware
  95. https://github.com/lesley2958/deep-learning-python#161-gpu
  96. https://github.com/lesley2958/deep-learning-python#20-building-a-neural-net
  97. https://github.com/lesley2958/deep-learning-python#21-generating-a-dataset
  98. https://github.com/lesley2958/deep-learning-python#22-logistic-regression
  99. https://github.com/lesley2958/deep-learning-python#23-gradient-descent--loss-function
 100. https://github.com/lesley2958/deep-learning-python#24-training-a-neural-network
 101. https://github.com/lesley2958/deep-learning-python#30-python-modules
 102. https://github.com/lesley2958/deep-learning-python#31-theano
 103. https://github.com/lesley2958/deep-learning-python#311-why-theano
 104. https://github.com/lesley2958/deep-learning-python#312-symbolic-variables
 105. https://github.com/lesley2958/deep-learning-python#313-symbolic-functions
 106. https://github.com/lesley2958/deep-learning-python#32-tensorflow
 107. https://github.com/lesley2958/deep-learning-python#33-lasagne
 108. https://github.com/lesley2958/deep-learning-python#34-caffe
 109. https://github.com/lesley2958/deep-learning-python#40-feedforward-neural-networks
 110. https://github.com/lesley2958/deep-learning-python#50-recurrent-neural-networks
 111. https://github.com/lesley2958/deep-learning-python#60-convolution-neural-networks
 112. https://github.com/lesley2958/deep-learning-python#70-final-words
 113. https://github.com/lesley2958/deep-learning-python#71-resources
 114. https://github.com/lesley2958/deep-learning-python#72-mini-courses
 115. https://www.python.org/downloads/
 116. https://pip.pypa.io/en/stable/installing/
 117. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/2.-ann-structure.jpg
 118. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/1.jpg
 119. https://www.analyticsvidhya.com/wp-content/uploads/2016/03/eq1-neuron.png
 120. https://www.talaikis.com/wp-content/uploads/2016/03/rl.png
 121. https://github.com/lesley2958/deep-learning-python/blob/master/input_data.py
 122. https://www.talaikis.com/wp-content/uploads/2016/03/rl.png
 123. https://ujwlkarn.files.wordpress.com/2016/07/convolution_schematic.gif?w=536&h=392
 124. http://www.deeplearningbook.org/
 125. https://arxiv.org/abs/1404.7828
 126. https://github.com/site/terms
 127. https://github.com/site/privacy
 128. https://github.com/security
 129. https://githubstatus.com/
 130. https://help.github.com/
 131. https://github.com/contact
 132. https://github.com/pricing
 133. https://developer.github.com/
 134. https://training.github.com/
 135. https://github.blog/
 136. https://github.com/about
 137. https://github.com/lesley2958/deep-learning-python
 138. https://github.com/lesley2958/deep-learning-python

   hidden links:
 140. https://github.com/
 141. https://github.com/lesley2958/deep-learning-python
 142. https://github.com/lesley2958/deep-learning-python
 143. https://github.com/lesley2958/deep-learning-python
 144. https://help.github.com/articles/which-remote-url-should-i-use
 145. https://github.com/lesley2958/deep-learning-python#intro-to-deep-learning-with-python
 146. https://github.com/lesley2958/deep-learning-python#table-of-contents
 147. https://github.com/lesley2958/deep-learning-python#00-setup
 148. https://github.com/lesley2958/deep-learning-python#01-python-and-pip
 149. https://github.com/lesley2958/deep-learning-python#02-libraries
 150. https://github.com/lesley2958/deep-learning-python#10-background
 151. https://github.com/lesley2958/deep-learning-python#11-what-is-deep-learning
 152. https://github.com/lesley2958/deep-learning-python#12-neural-networks
 153. https://github.com/lesley2958/deep-learning-python#121-what-is-a-neural-network
 154. https://github.com/lesley2958/deep-learning-python#122-input-layer
 155. https://github.com/lesley2958/deep-learning-python#123-hidden-layers
 156. https://github.com/lesley2958/deep-learning-python#124-output-layer
 157. https://github.com/lesley2958/deep-learning-python#125-activation-function
 158. https://github.com/lesley2958/deep-learning-python#13-types-of-neural-networks
 159. https://github.com/lesley2958/deep-learning-python#131-feedforward-neural-networks
 160. https://github.com/lesley2958/deep-learning-python#132-convolutional-neural-networks
 161. https://github.com/lesley2958/deep-learning-python#133-recurrent-neural-networks
 162. https://github.com/lesley2958/deep-learning-python#14-id26
 163. https://github.com/lesley2958/deep-learning-python#141-backprop-algorithm
 164. https://github.com/lesley2958/deep-learning-python#15-gradient-descent
 165. https://github.com/lesley2958/deep-learning-python#16-hardware
 166. https://github.com/lesley2958/deep-learning-python#161-gpu--cpu
 167. https://github.com/lesley2958/deep-learning-python#20-building-a-neural-net
 168. https://github.com/lesley2958/deep-learning-python#21-generating-a-dataset
 169. https://github.com/lesley2958/deep-learning-python#22-logistic-regression
 170. https://github.com/lesley2958/deep-learning-python#23-gradient-descent--loss-function
 171. https://github.com/lesley2958/deep-learning-python#231-learning-rate
 172. https://github.com/lesley2958/deep-learning-python#232-id173-strength
 173. https://github.com/lesley2958/deep-learning-python#233-code
 174. https://github.com/lesley2958/deep-learning-python#24-training-a-neural-network
 175. https://github.com/lesley2958/deep-learning-python#30-python-modules
 176. https://github.com/lesley2958/deep-learning-python#31-theano
 177. https://github.com/lesley2958/deep-learning-python#311-why-theano
 178. https://github.com/lesley2958/deep-learning-python#312-symbolic-variables
 179. https://github.com/lesley2958/deep-learning-python#313-symbolic-functions
 180. https://github.com/lesley2958/deep-learning-python#32-tensorflow
 181. https://github.com/lesley2958/deep-learning-python#321-single-layer-neural-network
 182. https://github.com/lesley2958/deep-learning-python#33-lasagne
 183. https://github.com/lesley2958/deep-learning-python#34-keras
 184. https://github.com/lesley2958/deep-learning-python#35-caffe
 185. https://github.com/lesley2958/deep-learning-python#40-feedforward-neural-networks
 186. https://github.com/lesley2958/deep-learning-python#50-recurrent-neural-networks
 187. https://github.com/lesley2958/deep-learning-python#60-convolution-neural-networks
 188. https://github.com/lesley2958/deep-learning-python#70-final-words
 189. https://github.com/lesley2958/deep-learning-python#71-resources
 190. https://github.com/
