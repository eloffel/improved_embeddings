   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

support vector machine (id166) tutorial

learning id166s from examples

   [16]go to the profile of abhishek ghose
   [17]abhishek ghose (button) blockedunblock (button) followfollowing
   aug 15, 2017
   [1*z3ymb1e3sjdjr-6h0s4twq.jpeg]

   after the [18]statsbot team published the post about [19]time series
   anomaly detection, many readers asked us to tell them about the support
   vector machines approach. it   s time to catch up and introduce you to
   id166 without hard math and share useful libraries and resources to get
   you started.
     __________________________________________________________________

   if you have used machine learning to perform classification, you might
   have heard about support vector machines (id166). introduced a little
   more than 50 years ago, they have evolved over time and have also been
   adapted to various other problems like regression, outlier analysis,
   and ranking.

   id166s are a favorite tool in the arsenal of many machine learning
   practitioners. at [20][24]7, we too use them to solve a variety of
   problems.

   in this post, we will try to gain a high-level understanding of how
   id166s work. i   ll focus on developing intuition rather than rigor. what
   that essentially means is we will skip as much of the math as possible
   and develop a strong intuition of the working principle.

the problem of classification

   say there is a machine learning (ml) course offered at your university.
   the course instructors have observed that students get the most out of
   it if they are good at math or stats. over time, they have recorded the
   scores of the enrolled students in these subjects. also, for each of
   these students, they have a label depicting their performance in the ml
   course:    good    or    bad.   

   now they want to determine the relationship between math and stats
   scores and the performance in the ml course. perhaps, based on what
   they find, they want to specify a prerequisite for enrolling in the
   course.

   how would they go about it? let   s start with representing the data they
   have. we could draw a two-dimensional plot, where one axis represents
   scores in math, while the other represents scores in stats. a student
   with certain scores is shown as a point on the graph.

   the color of the point         green or red         represents how he did on the ml
   course:    good    or    bad    respectively.

   this is what such a plot might look like:
   [0*jd9m5glod2qhscez.]

   when a student requests enrollment, our instructors would ask her to
   supply her math and stats scores. based on the data they already have,
   they would make an informed guess about her performance in the ml
   course.

   what we essentially want is some kind of an    algorithm,    to which you
   feed in the    score tuple    of the form (math_score, stats_score). it
   tells you whether the student is a red or green point on the plot
   (red/green is alternatively known as a class or label). and of course,
   this algorithm embodies, in some manner, the patterns present in the
   data we already have, also known as the training data.

   in this case, finding a line that passes between the red and green
   clusters, and then determining which side of this line a score tuple
   lies on, is a good algorithm. we take a side         the green side or the
   red side         as being a good indicator of her most likely performance in
   the course.
   [1*z7pb5_khqkqoqkoqjlc-rg.jpeg]

   the line here is our separating boundary (because it separates out the
   labels) or classifier (we use it classify points). the figure shows two
   possible classifiers for our problem.

good vs bad classifiers

   here   s an interesting question: both lines above separate the red and
   green clusters. is there a good reason to choose one over another?

   remember that the worth of a classifier is not in how well it separates
   the training data. we eventually want it to classify yet-unseen data
   points (known as test data). given that, we want to choose a line that
   captures the general pattern in the training data, so there is a good
   chance it does well on the test data.

   the first line above seems a bit    skewed.    near its lower half it seems
   to run too close to the red cluster, and in its upper half it runs too
   close to the green cluster. sure, it separates the training data
   perfectly, but if it sees a test point that   s a little farther out from
   the clusters, there is a good chance it would get the label wrong.

   the second line doesn   t have this problem. for example, look at the
   test points shown as squares and the labels assigned by the classifiers
   in the figure below.
   [1*gyz1orikxka5h9zoko8iwg.jpeg]

   the second line stays as far away as possible from both the clusters
   while getting the training data separation right. by being right in the
   middle of the two clusters, it is less    risky,    gives the data
   distributions for each class some wiggle room so to speak, and thus
   generalizes well on test data.

   id166s try to find the second kind of line. we selected the better
   classifier visually, but we need to define the underlying philosophy a
   bit more precisely to apply it in the general case. here   s a simplified
   version of what id166s do:
    1. find lines that correctly classify the training data
    2. among all such lines, pick the one that has the greatest distance
       to the points closest to it.

   the closest points that identify this line are known as support
   vectors. and the region they define around the line is known as the
   margin.

   here   s the second line shown with the support vectors: points with
   black edges (there are two of them) and the margin (the shaded region).
   [1*csqbt5-k4gvi4i4lrcx_ea.png]

   support vector machines give you a way to pick between many possible
   classifiers in a way that guarantees a higher chance of correctly
   labeling your test data. pretty neat, right?

   while the above plot shows a line and data in two dimensions, it must
   be noted that id166s work in any number of dimensions; and in these
   dimensions, they find the analogue of the two-dimensional line.

   for example, in three dimensions they find a plane (we will see an
   example of this shortly), and in higher dimensions they find a
   hyperplane         a generalization of the two-dimensional line and
   three-dimensional plane to an arbitrary number of dimensions.

   data that can be separated by a line (or in general, a hyperplane) is
   known as linearly separable data. the hyperplane acts as a linear
   classifier.

allowing for errors

   we looked at the easy case of perfectly linearly separable data in the
   last section. real-world data is, however, typically messy. you will
   almost always have a few instances that a linear classifier can   t get
   right.

   here   s an example of such data:
   [0*lohi9hk-azwzlh1q.]

   clearly, if we are using a linear classifier, we are never going to be
   able to perfectly separate the labels. we also don   t want to discard
   the linear classifier altogether because it does seem like a good fit
   for the problem except for a few errant points.

   how do id166s deal with this? they allow you to specify how many errors
   you are willing to accept.

   you can provide a parameter called    c    to your id166; this allows you to
   dictate the tradeoff between:
    1. having a wide margin.
    2. correctly classifying training data. a higher value of c implies
       you want lesser errors on the training data.

   it bears repeating that this is a tradeoff. you get better
   classification of training data at the expense of a wide margin.

   the following plots show how the classifier and the margin vary as we
   increase the value of c (support vectors not shown):
   [0*-_oxird3fqua4ypw.]

   note how the line    tilts    as we increase the value of c. at high
   values, it tries to accommodate the labels of most of the red points
   present at the bottom right of the plots. this is probably not what we
   want for test data. the first plot with c=0.01 seems to capture the
   general trend better, although it suffers from a lower accuracy on the
   training data compared to higher values for c.

     and since this is a trade-off, note how the width of the margin
     shrinks as we increase the value of c.

   in the previous example, the margin was a    no man   s land    for points.
   here, we see it   s not possible anymore to have both a good separating
   boundary and an associated point-free margin. some points creep into
   the margin.

   an important practical problem is to decide on a good value of c. since
   real-world data is almost never cleanly separable, this need comes up
   often. we typically use a technique like cross-validation to pick a
   good value for c.

non-linearly separable data

   we have seen how support vector machines systematically handle
   perfectly/almost linearly separable data. how does it handle the cases
   where the data is absolutely not linearly separable? afterall, a lot of
   real-world data falls in this category. surely, finding a hyperplane
   can   t work anymore. this seems unfortunate given that id166s excel at
   this task.

   here   s an example of non-linearly separable data (this is a variant of
   the famous [21]xor dataset), shown with the linear classifier id166s
   find:
   [0*p0zmbvfydzziz0cg.]

   you   d agree this doesn   t look great. we have only 75% accuracy on the
   training data         the best possible with a line. and more so, the line
   passes very close to some of the data. the best accuracy is not great,
   and to get even there, the line nearly straddles a few points.

   we need to do better.

   this is where one of my favorite bits about id166s come in. here   s what
   we have so far: we have a technique that is really good at finding
   hyperplanes. but then we also have data that is not linearly separable.
   so what do we do? project the data into a space where it is linearly
   separable and find a hyperplane in this space!

   i   ll illustrate this idea one step at a time.

   we start with the dataset in the above figure, and project it into a
   three-dimensional space where the new coordinates are:
   [0*pvzxa_odlsawxg1u.]

   this is what the projected data looks like. do you see a place where we
   just might be able to slip in a plane?
   [0*_8gkp1fgfoa7jlxo.]

   let   s run our id166 on it:
   [0*ojchw_exefs4qiok.]

   bingo! we have perfect label separation! lets project the plane back to
   the original two-dimensional space and see what the separation boundary
   looks like:
   [1*po7m4czep7p96gojk3wdtq.png]

   100% accuracy on the training data and a separating boundary that
   doesn   t run too close to the data! yay!

   the shape of the separating boundary in the original space depends on
   the projection. in the projected space, this is always a hyperplane.

     remember the primary goal of projecting the data was to put the
     hyperplane-finding superpowers of id166s to use.

   when you map it back to the original space, the separating boundary is
   not a line anymore. this is also true for the margin and support
   vectors. as far as our visual intuition goes, they make sense in the
   projected space.

   take a look at what they look like in the projected space, and then in
   the original space. the 3d margin is the region (not shaded to avoid
   visual clutter) between the planes above and below the separating
   hyperplane.
   [1*qyg3y4_qaj00u7smu_xlaq.gif]

   there are 4 support vectors in the projected space, which seems
   reasonable. they sit on the two planes that identify the margin. in the
   original space, they are still on the margin, but there doesn   t seem to
   be enough of them.

   let   s step back and analyze what happened:

1. how did i know what space to project the data onto?

   it seems i was being utterly specific         there is a square root of 2 in
   there somewhere!

   in this case, i wanted to show how projections to higher dimensions
   work, so i picked a very specific projection. in general, this is hard
   to know. however, what we do know is data is more likely to be linearly
   separable when projected onto higher dimensions, thanks to [22]cover   s
   theorem.

   in practice, we try out a few high-dimensional projections to see what
   works. in fact, we can project data onto infinite dimensions and that
   often works pretty well. this deserves going into some detail and
   that   s what the next section is about.

2. so i project the data first and then run the id166?

   no. to make the above example easy to grasp i made it sound like we
   need to project the data first. the fact is you ask the id166 to do the
   projection for you. this has some benefits. for one, id166s use something
   called kernels to do these projections, and these are pretty fast (for
   reasons we shall soon see).

   also, remember i mentioned projecting to infinite dimensions in the
   previous point? if you project the data yourself, how do you represent
   or store infinite dimensions? it turns out id166s are very clever about
   this, courtesy of kernels again.

   it   s about time we looked at kernels.

kernels

   finally, the secret sauce that makes id166s tick. this is where we need
   to look at a bit of math.

   let   s take stock of what we have seen so far:
    1. for linearly separable data id166s work amazingly well.
    2. for data that   s almost linearly separable, id166s can still be made
       to work pretty well by using the right value of c.
    3. for data that   s not linearly separable, we can project data to a
       space where it is perfectly/almost linearly separable, which
       reduces the problem to 1 or 2 and we are back in business.

   it looks like a big part of what makes id166s universally applicable is
   projecting it to higher dimensions. and this is where kernels come in.

   first, a slight digression.

   a very surprising aspect of id166s is that in all of the mathematical
   machinery it uses, the exact projection, or even the number of
   dimensions, doesn   t show up. you could write all of it in terms of the
   dot products between various data points (represented as vectors). for
   p-dimensional vectors i and j where the first subscript on a dimension
   identifies the point and the second indicates the dimension number:
   [0*tt3zldqhipcro8wa.]

   the dot product is defined as:
   [0*cklemrypiehqlq_g.]

   if we have n points in our dataset, the id166 needs only the dot product
   of each pair of points to find a classifier. just that. this is also
   true when we want to project data to higher dimensions. we don   t need
   to provide the id166 with exact projections; we need to give it the dot
   product between all pairs of points in the projected space.

   this is relevant because this is exactly what kernels do. a kernel,
   short for id81, takes as input two points in the original
   space, and directly gives us the dot product in the projected space.

   let   s revisit the projection we did before, and see if we can come up
   with a corresponding kernel. we will also track the number of
   computations we need to perform for the projection and then finding the
   dot products         to see how using a kernel compares.

   for a point i:
   [0*tddv9tnvhfa9pft9.]

   our corresponding projected point was:
   [0*hjhzdwhxkijiuycg.]

   to compute this projection we need to perform the following operations:
     * to get the new first dimension: 1 multiplication
     * second dimension: 1 multiplication
     * third dimension: 2 multiplications

   in all, 1+1+2 = 4 multiplications.

   the dot product in the new dimension is:
   [0*j14dwk3pncax_3dh.]

   to compute this dot product for two points i and j, we need to compute
   their projections first. so that   s 4+4 = 8 multiplications, and then
   the dot product itself requires 3 multiplications and 2 additions.

   in all, that   s:
     * multiplications: 8 (for the projections) + 3 (in the dot product) =
       11 multiplications
     * additions: 2 (in the dot product)

   which is total of 11 + 2 = 13 operations.

   i claim this id81 gives me the same result:
   [0*w0zm62e9ccizakzw.]

   we take the dot product of the vectors in the original space first, and
   then square the result.

   let expand it out and check whether my claim is indeed true:
   [0*yfcqangzkt_4voyw.]

   it is. how many operations does this need? look at step (2) above. to
   compute the dot product in two dimensions i need 2 multiplications and
   1 addition. squaring it is another multiplication.

   so, in all:
     * multiplications: 2 (for the dot product in the original space) + 1
       (for squaring the result) = 3 multiplications
     * additions: 1 (for the dot product in the original space)

   a total of 3 + 1 = 4 operations. this is only 31% of the operations we
   needed before.

   it looks like it is faster to use a id81 to compute the dot
   products we need. it might not seem like a big deal here: we   re looking
   at 4 vs 13 operations, but with input points with a lot more
   dimensions, and with the projected space having an even higher number
   of dimensions, the computational savings for a large dataset add up
   incredibly fast. so that   s one huge advantage of using kernels.

   most id166 libraries already come pre-packaged with some popular kernels
   like polynomial, radial basis function (rbf), and sigmoid. when we
   don   t use a projection (as in our first example in this article), we
   compute the dot products in the original space         this we refer to as
   using the linear kernel.

   many of these kernels give you additional levers to further tune it for
   your data. for example, the polynomial kernel:
   [0*oec3ajvor3nt1h3i.]

   allows you to pick the value of c and d (the degree of the polynomial).
   for the 3d projection above, i had used a polynomial kernel with c=0
   and d=2.

   but we are not done with the awesomeness of kernels yet!

   remember i mentioned projecting to infinite dimensions a while back? if
   you haven   t already guessed, the way to make it work is to have the
   right id81. that way, we really don   t have to project the
   input data, or worry about storing infinite dimensions.

     a id81 computes what the dot product would be if you had
     actually projected the data.

   the rbf kernel is commonly used for a specific infinite-dimensional
   projection. we won   t go into the math of it here, but look at the
   references at the end of this article.

   how can we have infinite dimensions, but can still compute the dot
   product? if you find this question confusing, think about how we
   compute sums of infinite series. this is similar. there are infinite
   terms in the dot product, but there happens to exist a formula to
   calculate their sum.

   this answers the questions we had asked in the previous section. let   s
   summarize:
    1. we typically don   t define a specific projection for our data.
       instead, we pick from available kernels, tweaking them in some
       cases, to find one best suited to the data.
    2. of course, nothing stops us from defining our own kernels, or
       performing the projection ourselves, but in many cases we don   t
       need to. or we at least start by trying out what   s already
       available.
    3. if there is a kernel available for the projection we want, we
       prefer to use the kernel, because that   s often faster.
    4. rbf kernels can project points to infinite dimensions.

id166 libraries to get started

   there are quite a few id166 libraries you could start practicing with:
       [23]libid166
       [24]id166-light
       [25]id166torch

   many general ml libraries like [26]scikit-learn also offer id166 modules,
   which are often wrappers around dedicated id166 libraries. my
   recommendation is to start out with the tried and tested [27]libid166.

   libid166 is available as a commandline tool, but the download also
   bundles python, java, and matlab wrappers. as long as you have a file
   with your data in a format libid166 understands (the readme that   s part
   of the download explains this, along with other available options) you
   are good to go.

   in fact, if you need a really quick feel of how different kernels, the
   value of c, etc., influence finding the separating boundary, try out
   the    graphical interface    on their [28]home page. mark your points for
   different classes, pick the id166 parameters, and hit run!

   i couldn   t resist and quickly marked a few points:
   [0*dsxxpsnze_1_iplp.]

   yep, i   m not making it easy for the id166.

   then i tried out a couple of kernels:
   [1*xwoc4nrk_nqscujkodn1ug.jpeg]

   the interface doesn   t show you the separating boundary, but shows you
   the regions that the id166 learns as belonging to a specific label. as
   you can see, the linear kernel completely ignores the red points. it
   thinks of the whole space as yellow (-ish green). but the rbf kernel
   neatly carves out a ring for the red label!

helpful resources

   we have been primarily relying on visual intuitions here. while that   s
   a great way to gain an initial understanding, i   d strongly encourage
   you to dig deeper. an example of where visual intuition might prove to
   be insufficient is in understanding margin width and support vectors
   for non-linearly separable cases.

     remember that these quantities are decided by optimizing a
     trade-off. unless you look at the math, some of the results may seem
     counter-intuitive.

   another area where getting to know the math helps is in understanding
   id81s. consider the rbf kernel, which i   ve barely introduced
   in this short article. i hope the    mystique    surrounding it         its
   relation to an infinite-dimensional projection coupled with the
   fantastic results on the last dataset (the    ring   )         has convinced you
   to take a closer look at it.

   resources i would recommend:
    1. [29]video lectures: learning from data by yaser abu-mostafa.
       lectures from 14 to 16 talk about id166s and kernels. i   d also highly
       recommend the whole series if you   re looking for an introduction to
       ml, it maintains an excellent balance between math and intuition.
    2. [30]book: the elements of statistical learning         trevor hastie,
       robert tibshirani, jerome friedman.chapter 4 introduces the basic
       idea behind id166s, while chapter 12 deals with it comprehensively.

   happy (machine) learning!

   iframe: [31]/media/02231cd5403151a2a463e36cc3b88462?postid=c1618e635e93

you   d also like:

   [32]google analytics audit checklist and tools
   auditing a google analytics setup like a problog.statsbot.co
   [33]data structures related to machine learning algorithms
   a primer on data structures for data scientistsblog.statsbot.co
   [34]machine learning translation and the google translate algorithm
   the basic principles of machine translation enginesblog.statsbot.co

     * [35]machine learning
     * [36]id166
     * [37]support vector machine
     * [38]data science
     * [39]kernel

   (button)
   (button)
   (button) 3.1k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of abhishek ghose

[41]abhishek ghose

     (button) follow
   [42]stats and bots

[43]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 3.1k
     * (button)
     *
     *

   [44]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [45]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c1618e635e93
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_ljsfbxzxrwql---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@abhishekghose?source=post_header_lockup
  17. https://blog.statsbot.co/@abhishekghose
  18. http://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=id166
  19. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  20. https://www.247-inc.com/
  21. http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html
  22. https://en.wikipedia.org/wiki/cover's_theorem
  23. https://www.csie.ntu.edu.tw/~cjlin/libid166/
  24. http://id166light.joachims.org/
  25. http://bengio.abracadoudou.com/id166torch.html
  26. http://scikit-learn.org/stable/
  27. https://www.csie.ntu.edu.tw/~cjlin/libid166/
  28. https://www.csie.ntu.edu.tw/~cjlin/libid166/
  29. https://www.youtube.com/watch?v=meg35rdd7ra&list=plca2c1469ea777f9a
  30. http://web.stanford.edu/~hastie/elemstatlearn/printings/eslii_print12.pdf
  31. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=c1618e635e93
  32. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  33. https://blog.statsbot.co/data-structures-related-to-machine-learning-algorithms-5edf77c8bbf4
  34. https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4
  35. https://blog.statsbot.co/tagged/machine-learning?source=post
  36. https://blog.statsbot.co/tagged/id166?source=post
  37. https://blog.statsbot.co/tagged/support-vector-machine?source=post
  38. https://blog.statsbot.co/tagged/data-science?source=post
  39. https://blog.statsbot.co/tagged/kernel?source=post
  40. https://blog.statsbot.co/@abhishekghose?source=footer_card
  41. https://blog.statsbot.co/@abhishekghose
  42. https://blog.statsbot.co/?source=footer_card
  43. https://blog.statsbot.co/?source=footer_card
  44. https://blog.statsbot.co/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://blog.statsbot.co/google-analytics-audit-checklist-and-tools-fca7df2f2e7a
  48. https://blog.statsbot.co/data-structures-related-to-machine-learning-algorithms-5edf77c8bbf4
  49. https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4
  50. https://medium.com/p/c1618e635e93/share/twitter
  51. https://medium.com/p/c1618e635e93/share/facebook
  52. https://medium.com/p/c1618e635e93/share/twitter
  53. https://medium.com/p/c1618e635e93/share/facebook
