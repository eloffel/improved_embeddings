   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

an intuitive guide to deep network architectures

   [16]go to the profile of joyce xu
   [17]joyce xu (button) blockedunblock (button) followfollowing
   aug 14, 2017
   [1*_rcyzi7fqzc_q1gcqslm1g.png]
   googlenet, 2014

   over the past few years, much of the progress in deep learning for
   id161 can be boiled down to just a handful of neural network
   architectures. setting aside all the math, the code, and the
   implementation details, i wanted to explore one simple question: how
   and why do these models work?

   at the time of writing, keras ships with six of these pre-trained
   models [18]already built into the library:
     * vgg16
     * vgg19
     * resnet50
     * inception v3
     * xception
     * mobilenet

   the vgg networks, along with the earlier alexnet from 2012, follow the
   now archetypal layout of basic conv nets: a series of convolutional,
   max-pooling, and activation layers before some fully-connected
   classification layers at the end. mobilenet is essentially a
   streamlined version of the xception architecture optimized for mobile
   applications. the remaining three, however, truly redefine the way we
   look at neural networks.

   this rest of this post will focus on the intuition behind the resnet,
   inception, and xception architectures, and why they have become
   building blocks for so many subsequent works in id161.

resnet

   resnet was born from a beautifully simple observation: why do very deep
   nets perform worse as you keep adding layers?

   intuitively, deeper nets should perform no worse than their shallower
   counterparts, at least at train time (when there is no risk of
   overfitting). as a thought experiment, let   s say we   ve built a net with
   n layers that achieves a certain accuracy. at minimum, a net with n+1
   layers should be able to achieve the exact same accuracy, if only by
   copying over the same first n layers and performing an identity mapping
   for the last layer. similarly, nets of n+2, n+3, and n+4 layers could
   all continue performing identity mappings and achieve the same
   accuracy. in practice, however, these deeper nets almost always degrade
   in performance.

   the authors of resnet boiled these problems down to a single
   hypothesis: direct mappings are hard to learn. and they proposed a fix:
   instead of trying to learn an underlying mapping from x to h(x), learn
   the difference between the two, or the    residual.    then, to calculate
   h(x), we can just add the residual to the input.

   say the residual is f(x)=h(x)-x. now, instead of trying to learn h(x)
   directly, our nets are trying to learn f(x)+x.

   this gives rise to the famous resnet (or    residual network   ) block
   you   ve probably seen:
   [1*5zsgo2l71fjos8xendgcvq.jpeg]
   resnet block

   each    block    in resnet consists of a series of layers and a    shortcut   
   connection adding the input of the block to its output. the    add   
   operation is performed element-wise, and if the input and output are of
   different sizes, zero-padding or projections (via 1x1 convolutions) can
   be used to create matching dimensions.

   if we go back to our thought experiment, this simplifies our
   construction of identity layers greatly. intuitively, it   s much easier
   to learn to push f(x) to 0 and leave the output as x than to learn an
   identity transformation from scratch. in general, resnet gives layers a
      reference    point         x         to start learning from.

   this idea works astoundingly well in practice. previously, deep neural
   nets often suffered from the problem of [19]vanishing gradients, in
   which gradient signals from the error function decreased exponentially
   as they backpropogated to earlier layers. in essence, by the time the
   error signals traveled all the way back to the early layers, they were
   so small that the net couldn   t learn. however, because the gradient
   signal in resnets could travel back directly to early layers via
   shortcut connections, we could suddenly build 50-layer, 101-layer,
   152-layer, and even (apparently) 1000+ layer nets that still performed
   well. at the time, this was a huge leap forward from the previous
   state-of-the-art, which won the ilsvrc 2014 challenge with 22 layers.

   resnet is one of my personal favorite developments in the neural
   network world. so many deep learning papers come out with minor
   improvements from hacking away at the math, the optimizations, and the
   training process without thought to the underlying task of the model.
   resnet fundamentally changed the way we understand neural networks and
   how they learn.

   fun facts:
     * the 1000+ layer net is open-source! i would not really recommend
       you try re-training it, [20]but   
     * if you   re feeling functional and a little frisky, i recently ported
       resnet50 to the open-source clojure ml library [21]cortex. try it
       out and see how it compares to keras!

inception

   if resnet was all about going deeper, the inception family    is all
   about going wider. in particular, the authors of inception were
   interested in the computational efficiency of training larger nets. in
   other words: how can we scale up neural nets without increasing
   computational cost?

   the original paper focused on a new building block for deep nets, a
   block now known as the    inception module.    at its core, this module is
   the product of two key insights.

   the first insight relates to layer operations. in a traditional conv
   net, each layer extracts information from the previous layer in order
   to transform the input data into a more useful representation. however,
   each layer type extracts a different kind of information. the output of
   a 5x5 convolutional kernel tells us something different from the output
   of a 3x3 convolutional kernel, which tells us something different from
   the output of a max-pooling kernel, and so on and so on. at any given
   layer, how do we know what transformation provides the most    useful   
   information?

   insight #1: why not let the model choose?

   an inception module computes multiple different transformations over
   the same input map in parallel, concatenating their results into a
   single output. in other words, for each layer, inception does a 5x5
   convolutional transformation, and a 3x3, and a max-pool. and the next
   layer of the model gets to decide if (and how) to use each piece of
   information.
   [1*rur5vae4waodcqfrxu6vww.jpeg]

   the increased information density of this model architecture comes with
   one glaring problem: we   ve drastically increased computational costs.
   not only are large (e.g. 5x5) convolutional filters inherently
   expensive to compute, stacking multiple different filters side by side
   greatly increases the number of feature maps per layer. and this
   increase becomes a deadly bottleneck in our model.

   think about it this way. for each additional filter added, we have to
   convolve over all the input maps to calculate a single output. see the
   image below: creating one output map from a single filter involves
   computing over every single map from the previous layer.
   [1*lgm8_2sbmkayechjeznyaq.png]

   let   s say there are m input maps. one additional filter means
   convolving over m more maps; n additional filters means convolving over
   n*m more maps. in other words, as the authors note,    any uniform
   increase in the number of [filters] results in a quadratic increase of
   computation.    our naive inception module just tripled or quadrupled the
   number of filters. computationally speaking, this is a big bad thing.

   this leads to insight #2: using 1x1 convolutions to perform
   id84. in order to solve the computational
   bottleneck, the authors of inception used 1x1 convolutions to    filter   
   the depth of the outputs. a 1x1 convolution only looks at one value at
   a time, but across multiple channels, it can extract spatial
   information and compress it down to a lower dimension. for example,
   using 20 1x1 filters, an input of size 64x64x100 (with 100 feature
   maps) can be compressed down to 64x64x20. by reducing the number of
   input maps, the authors of inception were able to stack different layer
   transformations in parallel, resulting in nets that were simultaneously
   deep (many layers) and    wide    (many parallel operations).
   [1*aq4tcbl9t5z36ktdezsoha.png]

   how well did this work? the first version of inception, dubbed
      googlenet,    was the 22-layer winner of the ilsvrc 2014 competition i
   mentioned earlier. inception v2 and v3 were developed in a second paper
   a year later, and improved on the original in several ways         most
   notably by refactoring larger convolutions into consecutive smaller
   ones that were easier to learn. in v3, for example, the 5x5 convolution
   was replaced with 2 consecutive 3x3 convolutions.

   inception rapidly became a defining model architecture. the latest
   version of inception, v4, even threw in residual connections within
   each module, creating an inception-resnet hybrid. most importantly,
   however, inception demonstrated the power of well-designed
      network-in-network    architectures, adding yet another step to the
   representational power of neural networks.

   fun facts:
     * the original inception paper literally cites the    [22]we need to go
       deeper    internet meme as an inspiration for its name. this must be
       the first time knowyourmeme.com got listed as the first reference
       of a google paper.
     * the second inception paper (with v2 and v3) was released just one
       day after the original resnet paper. december 2015 was a good time
       for deep learning.

xception

   xception stands for    extreme inception.    rather like our previous two
   architectures, it reframes the way we look at neural nets         conv nets
   in particular. and, as the name suggests, it takes the principles of
   inception to an extreme.

   here   s the hypothesis:    cross-channel correlations and spatial
   correlations are sufficiently decoupled that it is preferable not to
   map them jointly.   

   what does this mean? well, in a traditional conv net, convolutional
   layers seek out correlations across both space and depth. let   s take
   another look at our standard convolutional layer:
   [1*lgm8_2sbmkayechjeznyaq.png]

   in the image above, the filter simultaneously considers a spatial
   dimension (each 2x2 colored square) and a cross-channel or    depth   
   dimension (the stack of four squares). at the input layer of an image,
   this is equivalent to a convolutional filter looking at a 2x2 patch of
   pixels across all three rgb channels. here   s the question: is there any
   reason we need to consider both the image region and the channels at
   the same time?

   in inception, we began separating the two slightly. we used 1x1
   convolutions to project the original input into several separate,
   smaller input spaces, and from each of those input spaces we used a
   different type of filter to transform those smaller 3d blocks of data.
   xception takes this one step further. instead of partitioning input
   data into several compressed chunks, it maps the spatial correlations
   for each output channel separately, and then performs a 1x1 depthwise
   convolution to capture cross-channel correlation.
   [1*srbsbojkg48dtumcp5vvhg.jpeg]

   the author notes that this is essentially equivalent to an existing
   operation known as a    depthwise separable convolution,    which consists
   of a depthwise convolution (a spatial convolution performed
   independently for each channel) followed by a pointwise convolution (a
   1x1 convolution across channels). we can think of this as looking for
   correlations across a 2d space first, followed by looking for
   correlations across a 1d space. intuitively, this 2d + 1d mapping is
   easier to learn than a full 3d mapping.

   and it works! xception slightly outperforms inception v3 on the
   id163 dataset, and vastly outperforms it on a larger image
   classification dataset with 17,000 classes. most importantly, it has
   the same number of model parameters as inception, implying a greater
   computational efficiency. xception is much newer (it came out in april
   2017), but as mentioned above, its architecture is already powering
   google   s mobile vision applications through mobilenet.

   fun facts:
     * the author of xception is also the author of keras. francois
       chollet is a living god.

moving forward

   that   s it for resnet, inception, and xception! i firmly believe in
   having a strong intuitive understanding of these networks, because they
   are becoming ubiquitous in research and industry alike. we can even use
   them in our own applications with something called id21.

   [23]id21 is a technique in machine learning in which we
   apply knowledge from a source domain (e.g. id163) to a target domain
   that may have significantly fewer data points. in practice, this
   generally involves initializing a model with pre-trained weights from
   resnet, inception, etc. and either using it as a feature extractor, or
   fine-tuning the last few layers on a new dataset. with transfer
   learning, these models can be re-purposed for any related task we want,
   from id164 for self-driving vehicles to generating captions
   for video clips.

   to get started with id21, keras has a wonderful guide to
   fine-tuning models [24]here. if it sounds interesting to you, check it
   out         and happy hacking!

     * [25]machine learning
     * [26]artificial intelligence
     * [27]deep learning
     * [28]data science
     * [29]towards data science

   (button)
   (button)
   (button) 4.1k claps
   (button) (button) (button) 13 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of joyce xu

[31]joyce xu

   deep learning, rl, nlp, comp. vision, and all that jazz.
   [32]@deepmindai, [33]@stanford

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 4.1k
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/65fdc477db41
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_pmecmmubtt5s---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@joycex99?source=post_header_lockup
  17. https://towardsdatascience.com/@joycex99
  18. https://keras.io/applications/
  19. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  20. https://github.com/kaiminghe/resnet-1k-layers
  21. https://github.com/thinktopic/cortex
  22. http://knowyourmeme.com/memes/we-need-to-go-deeper
  23. http://cs231n.github.io/transfer-learning/
  24. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/tagged/deep-learning?source=post
  28. https://towardsdatascience.com/tagged/data-science?source=post
  29. https://towardsdatascience.com/tagged/towards-data-science?source=post
  30. https://towardsdatascience.com/@joycex99?source=footer_card
  31. https://towardsdatascience.com/@joycex99
  32. http://twitter.com/deepmindai
  33. http://twitter.com/stanford
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/65fdc477db41/share/twitter
  40. https://medium.com/p/65fdc477db41/share/facebook
  41. https://medium.com/p/65fdc477db41/share/twitter
  42. https://medium.com/p/65fdc477db41/share/facebook
