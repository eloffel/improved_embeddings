   #[1]github [2]recent commits to ntm_keras:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]7
     * [35]star [36]124
     * [37]fork [38]26

[39]flomlo/[40]ntm_keras

   [41]code [42]issues 6 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   an implementation of the id63 as a keras recurrent
   layer.
     * [47]78 commits
     * [48]4 branches
     * [49]2 releases
     * [50]fetching contributors
     * [51]bsd-3-clause

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/f
   [54]download zip

downloading...

   want to be notified of new releases in flomlo/ntm_keras?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [61]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [62]license
   [63]readme.md
   [64]the_ntm_-_introduction_and_implementation.pdf
   [65]copytask.py
   [66]main.py
   [67]model_dense.py
   [68]model_lstm.py
   [69]model_ntm.py
   [70]ntm.py
   [71]testing_utils.py
   [72]view_weights.py

readme.md

changelog 0.2:

     * api change: controller models now must have linear activation. the
       activation of the ntm-layer is selected by the new parameter
       "activation" (default: "linear"). for all the stuff that interacts
       with the memory we now have very precise handselected activations
       which asume that there was no prior de-linearisation. this
       requirement on the controller will probably be final.
     * there is now support for multiple read/write heads! use the
       parameters read_heads resp. write_heads at initialisation (by
       default both are 1).
     * the code around controller output splitting and activation was
       completely rewritten and cleaned from a lot of copy-paste-code.
     * unfortunately we lost backend neutrality: as tf.slice is used
       extensivly, we have to either try getting k.slice or have to do a
       case distinction over backend. use the old version if you need
       another backend than tensorflow! and please write me a message.
     * as less activations have to be computed, it is now a tiny little
       bit faster (~1%).
     * stateful models do not work anymore. actually they never worked,
       the testing routine was just broken. will be repaired asap.

the id63

introduction

   this code tries to implement the id63, as found in
   [73]https://arxiv.org/abs/1410.5401, as a backend neutral recurrent
   keras layer.

   a very default experiment, the copy task, is provided, too.

   in the end there is a todo-list. help would be appreciated!

   note:
     * there is a nicely formatted paper describing the rough idea of the
       ntm, implementation difficulties and which discusses the copy
       experiment. it is available here in the repository as
       the_ntm_-_introduction_and_implementation.pdf.
     * you may want to change the logdir_base in testing_utils.py to
       something that works for you or just set a symbolic link.

user guide

   for a quick start on the copy task, type
python main.py -v ntm

   while in a python enviroment which has tensorflow, keras and numpy.
   having tensorflow-gpu is recommend, as everything is about 20x faster.
   in my case this experiment takes about 100 minutes on a nvidia gtx 1050
   ti. the -v is optional and offers much more detailed information about
   the achieved accuracy, and also after every training epoch. logging
   data is written logdir_base, which is ./logs/ by default. view them
   with tensorboard:
tensorboard --logdir ./logs

   if you've luck and not had a terrible run (that can happen,
   unfortunately), you now have a machine capable of copying a given
   sequence! i wonder if we could have achieved that any other way ...

   these results are especially interesting compared to an lstm model: run
python main.py lstm

   this builds 3 layers of lstm with and goes through the same testing
   procedure as above, which for me resulted in a training time of
   approximately 1h (same gpu) and (roughly) 100%, 100%, 94%, 50%, 50%
   accuracy at the respective test lengths. this shows that the ntm has
   advantages over lstm in some cases. especially considering the lstm
   model has about 807.200 trainable parameters while the ntm had a mere
   3100!

   have fun playing around, maybe with other controllers? dense,
   double_dense and lstm are build in.

api

   from the outside, this implementation looks like a regular recurrent
   layer in keras. it has however a number of non-obvious parameters:

hyperparameters

     * n_width: this is the width of the memory matrix. increasing this
       increases computational complexity in o(n^2). the controller shape
       is not dependant on this, making weight transfer possible.
     * m_depth: this is the depth of the memory matrix. increasing this
       increases the number of trainable weights in o(m^2). it also
       changes controller shape.
     * controller_model: this parameter allows you to place a keras model
       of appropriate shape as the controller. the appropriate shape can
       be calculated via controller_input_output_shape. if none is set, a
       single dense layer will be used.
     * read_heads: the number of read heads this ntm should have. has
       quadratic influence on the number of trainable weights. default: 1
     * write_heads: the number of write heads this ntm should have. has
       quadratic influence on the number of trainable weights, but for
       small numbers a huge impact. default: 1

usage

   more or less minimal code example:
from keras.models import sequential
from keras.optimizers import adam
from ntm import neuralturingmachine as ntm

model = sequential()
model.name = "ntm_-_" + controller_model.name

ntm = ntm(output_dim, n_slots=50, m_depth=20, shift_range=3,
          controller_model=none,
          return_sequences=true,
          input_shape=(none, input_dim),
          batch_size = 100)
model.add(ntm)

sgd = adam(lr=learning_rate, clipnorm=clipnorm)
model.compile(loss='binary_crossid178', optimizer=sgd,
               metrics = ['binary_accuracy'], sample_weight_mode="temporal")

   what if we instead want a more complex controller? design it, e.g.
   double lstm:
controller = sequential()
controller.name=ntm_controller_architecture
controller.add(lstm(units=150,
                    stateful=true,
                    implementation=2,   # best for gpu. other ones also might no
t work.
                    batch_input_shape=(batch_size, none, controller_input_dim)))
controller.add(lstm(units=controller_output_dim,
                    activation='linear',
                    stateful=true,
                    implementation=2))   # best for gpu. other ones also might n
ot work.

controller.compile(loss='binary_crossid178', optimizer=sgd,
                 metrics = ['binary_accuracy'], sample_weight_mode="temporal")

   and now use the same code as above, only with
   controller_model=controller.

   note that we used linear as the last activation layer! this is of
   critical importance. the activation of the ntm-layer can be set the
   parameter activation (default: linear).

   note that a correct controller_input_dim and controller_output_dim can
   be calculated via controller_input_output_shape:
from ntm import controller_input_output_shape
controller_input_dim, controller_output_dim = ntm.controller_input_output_shape(
            input_dim, output_dim, m_depth, n_slots, shift_range, read_heads, wr
ite_heads)

   also note that every statefull controller must carry around his own
   state, as was done here with
stateful=true

todo:

     * [x] arbitrary number of read and write heads
     * [ ] support of masking, and maybe dropout, one has to reason about
       it theoretically first.
     * [ ] support for get and set config to better enable model saving
     * [x] a bit of code cleaning: especially the controller output
       splitting is ugly as hell.
     * [x] support for arbitrary id180 would be nice,
       currently restricted to sigmoid.
     * [ ] make it backend neutral again! some testing might be nice, too.
     * [ ] maybe add the other experiments of the original paper?
     * [ ] mooaaar speeeed. look if there are platant performance
       optimizations possible.

     *    2019 github, inc.
     * [74]terms
     * [75]privacy
     * [76]security
     * [77]status
     * [78]help

     * [79]contact github
     * [80]pricing
     * [81]api
     * [82]training
     * [83]blog
     * [84]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [85]reload to refresh your
   session. you signed out in another tab or window. [86]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/flomlo/ntm_keras/commits/master.atom
   3. https://github.com/flomlo/ntm_keras#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/flomlo/ntm_keras
  32. https://github.com/join
  33. https://github.com/login?return_to=/flomlo/ntm_keras
  34. https://github.com/flomlo/ntm_keras/watchers
  35. https://github.com/login?return_to=/flomlo/ntm_keras
  36. https://github.com/flomlo/ntm_keras/stargazers
  37. https://github.com/login?return_to=/flomlo/ntm_keras
  38. https://github.com/flomlo/ntm_keras/network/members
  39. https://github.com/flomlo
  40. https://github.com/flomlo/ntm_keras
  41. https://github.com/flomlo/ntm_keras
  42. https://github.com/flomlo/ntm_keras/issues
  43. https://github.com/flomlo/ntm_keras/pulls
  44. https://github.com/flomlo/ntm_keras/projects
  45. https://github.com/flomlo/ntm_keras/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/flomlo/ntm_keras/commits/master
  48. https://github.com/flomlo/ntm_keras/branches
  49. https://github.com/flomlo/ntm_keras/releases
  50. https://github.com/flomlo/ntm_keras/graphs/contributors
  51. https://github.com/flomlo/ntm_keras/blob/master/license
  52. https://github.com/flomlo/ntm_keras/search?l=python
  53. https://github.com/flomlo/ntm_keras/find/master
  54. https://github.com/flomlo/ntm_keras/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/flomlo/ntm_keras
  56. https://github.com/join?return_to=/flomlo/ntm_keras
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/flomlo/ntm_keras/tree/fdfe3ff0e3f5d3e4bc1a2fe51afdf67dae3aa5d8
  62. https://github.com/flomlo/ntm_keras/blob/master/license
  63. https://github.com/flomlo/ntm_keras/blob/master/readme.md
  64. https://github.com/flomlo/ntm_keras/blob/master/the_ntm_-_introduction_and_implementation.pdf
  65. https://github.com/flomlo/ntm_keras/blob/master/copytask.py
  66. https://github.com/flomlo/ntm_keras/blob/master/main.py
  67. https://github.com/flomlo/ntm_keras/blob/master/model_dense.py
  68. https://github.com/flomlo/ntm_keras/blob/master/model_lstm.py
  69. https://github.com/flomlo/ntm_keras/blob/master/model_ntm.py
  70. https://github.com/flomlo/ntm_keras/blob/master/ntm.py
  71. https://github.com/flomlo/ntm_keras/blob/master/testing_utils.py
  72. https://github.com/flomlo/ntm_keras/blob/master/view_weights.py
  73. https://arxiv.org/abs/1410.5401
  74. https://github.com/site/terms
  75. https://github.com/site/privacy
  76. https://github.com/security
  77. https://githubstatus.com/
  78. https://help.github.com/
  79. https://github.com/contact
  80. https://github.com/pricing
  81. https://developer.github.com/
  82. https://training.github.com/
  83. https://github.blog/
  84. https://github.com/about
  85. https://github.com/flomlo/ntm_keras
  86. https://github.com/flomlo/ntm_keras

   hidden links:
  88. https://github.com/
  89. https://github.com/flomlo/ntm_keras
  90. https://github.com/flomlo/ntm_keras
  91. https://github.com/flomlo/ntm_keras
  92. https://help.github.com/articles/which-remote-url-should-i-use
  93. https://github.com/flomlo/ntm_keras#changelog-02
  94. https://github.com/flomlo/ntm_keras#the-neural-turing-machine
  95. https://github.com/flomlo/ntm_keras#introduction
  96. https://github.com/flomlo/ntm_keras#user-guide
  97. https://github.com/flomlo/ntm_keras#api
  98. https://github.com/flomlo/ntm_keras#hyperparameters
  99. https://github.com/flomlo/ntm_keras#usage
 100. https://github.com/flomlo/ntm_keras#todo
 101. https://github.com/
