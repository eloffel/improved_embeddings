   #[1]adventures in machine learning    a id97 keras tutorial comments
   feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

a id97 keras tutorial

   by [9]admin | [10]deep learning

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]a id97 keras tutorial

   aug 30
   [14]1
   id97 keras - negative sampling architecture

   understanding id97 id27 is a critical component in your
   machine learning journey.  id27 is a necessary step in
   performing efficient natural language processing in your machine
   learning models.  this tutorial will show you how to perform id97
   id27s in the keras deep learning framework     to get an
   introduction to keras, check out [15]my tutorial (or the recommended
   course below).  in a [16]previous post, i introduced id97
   implementations in tensorflow.  in that tutorial, i showed how using a
   naive, softmax-based id27 training regime results in an
   extremely slow training of our embedding layer when we have large word
   vocabularies.  to get around this problem, a technique called    negative
   sampling    has been proposed, and a custom id168 has been
   created in tensorflow to allow this (nce_loss).

   unfortunately, this id168 doesn   t exist in keras, so in this
   tutorial, we are going to implement it ourselves.  this is a fortunate
   omission, as implementing it ourselves will help us to understand how
   negative sampling works and therefore better understand the id97
   keras process.
     __________________________________________________________________

eager to learn more? get the book [17]here
     __________________________________________________________________

id27

   if we have a document or documents that we are using to try to train
   some sort of natural language machine learning system (i.e. a chatbot),
   we need to create a vocabulary of the most common words in that
   document.  this vocabulary can be greater than 10,000 words in length
   in some instances. to represent a word to our machine learning model, a
   naive way would be to use a one-hot vector representation i.e. a
   10,000-word vector full of zeros except for one element, representing
   our word, which is set to 1.  however, this is an inefficient way of
   doing things     a 10,000-word vector is an unwieldy object to train
   with.  another issue is that these one-hot vectors hold no information
   about the meaning of the word, how it is used in language and what is
   its usual context (i.e. what other words it generally appears close
   to).

   enter id27s     id27s try to    compress    large one-hot
   word vectors into much smaller vectors (a few hundred elements) which
   preserve some of the meaning and context of the word. id97 is the
   most common process of id27 and will be explained below.

context, id97 and the skip-gram model

   the context of the word is the key measure of meaning that is utilized
   in id97.  the context of the word    sat    in the sentence    the cat
   sat on the mat    is (   the   ,    cat   ,    on   ,    the   ,    mat   ).  in other words,
   it is the words which commonly occur around the target word    sat   .
   words which have similar contexts share meaning under id97, and
   their reduced vector representations will be similar.  in the skip-gram
   model version of id97 (more on this later), the goal is to take a
   target word i.e.    sat    and predict the surrounding context words.  this
   involves an iterative learning process.

   the end product of this learning will be an embedding layer in a
   network     this embedding layer is a kind of lookup table     the rows are
   vector representations of each word in our vocabulary.  here   s a
   simplified example (using dummy values) of what this looks like,
   where vocabulary_size=7 and embedding_size=3:

   \begin{equation}
   \begin{array}{c|c c c}
   anarchism & 0.5 & 0.1 & -0.1\\
   originated & -0.5 & 0.3 & 0.9 \\
   as & 0.3 & -0.5 & -0.3 \\
   a & 0.7 & 0.2 & -0.3\\
   term & 0.8 & 0.1 & -0.1 \\
   of & 0.4 & -0.6 & -0.1 \\
   abuse & 0.7 & 0.1 & -0.4
   \end{array}
   \end{equation}

   as you can see, each word (row) is represented by a vector of size 3.
   learning this embedding layer/lookup table can be performed using a
   simple neural network and an output softmax layer     see the diagram
   below:
   id97 softmax trainer

   a softmax trainer for id27

   the idea of the neural network above is to supply our input target
   words as one-hot vectors.  then, via a hidden layer, we want to train
   the neural network to increase the id203 of valid context words,
   while decreasing the id203 of invalid context words (i.e. words
   that never show up in the surrounding context of the target words).
   this involves using a softmax function on the output layer.  once
   training is complete, the output layer is discarded, and our embedding
   vectors are the weights of the hidden layer.

   there are two variants of the id97 paradigm     skip-gram and cbow.
   the skip-gram variant takes a target word and tries to predict the
   surrounding context words, while the cbow (continuous bag of words)
   variant takes a set of context words and tries to predict a target
   word.  in this case, we will be considering the skip-gram variant (for
   more details     see [18]this tutorial).

the softmax issue and negative sampling

   the problem with using a full softmax output layer is that it is very
   computationally expensive.  consider the definition of the softmax
   function:

   $$p(y = j \mid x) = \frac{e^{x^t w_j}}{\sum_{k=1}^k e^{x^t w_k}}$$

   here the id203 of the output being class j is calculated by
   multiplying the output of the hidden layer and the weights connecting
   to the class j output on the numerator and dividing it by the same
   product but over all the remaining weights.  when the output is a
   10,000-word one-hot vector, we are talking millions of weights that
   need to be updated in any gradient based training of the output layer.
    this gets seriously time-consuming and inefficient, as demonstrated in
   my [19]tensorflow id97 tutorial.

   there   s another solution called negative sampling.  it is described in
   the [20]original id97 paper by mikolov et al.  it works by
   reinforcing the strength of weights which link a target word to its
   context words, but rather than reducing the value of all those weights
   which aren   t in the context, it simply samples a small number of them    
   these are called the    negative samples   .

   to train the embedding layer using negative samples in keras, we can
   re-imagine the way we train our network.  instead of constructing our
   network so that the output layer is a multi-class softmax layer, we can
   change it into a simple binary classifier.  for words that are in the
   context of the target word, we want our network to output a 1, and for
   our negative samples, we want our network to output a 0. therefore, the
   output layer of our id97 keras network is simply a single node with
   a sigmoid activation function.

   we also need a way of ensuring that, as the network trains, words which
   are similar end up having similar embedding vectors.  therefore, we
   want to ensure that the trained network will always output a 1 when it
   is supplied words which are in the same context, but 0 when it is
   supplied words which are never in the same context. therefore, we need
   a vector similarity score supplied to the output sigmoid layer     with
   similar vectors outputting a high score and un-similar vectors
   outputting a low score.  the most typical similarity measure used
   between two vectors is the [21]cosine similarity score:

   $$similarity = cos(\theta) =
   \frac{\textbf{a}\cdot\textbf{b}}{\parallel\textbf{a}\parallel_2
   \parallel \textbf{b} \parallel_2}$$

   the denominator of this measure acts to normalize the result     the real
   similarity operation is on the numerator: the [22]dot product
   between vectors a and b.  in other words, to get a simple,
   non-normalized measure of similarity between two vectors, you simply
   apply a dot product operation between them.

   so with all that in mind, our new negative sampling network for the
   planned id97 keras implementation features:
     * an (integer) input of a target word and a real or negative context
       word
     * an embedding layer lookup (i.e. looking up the integer index of the
       word in the embedding matrix to get the word vector)
     * the application of a dot product operation
     * the output sigmoid layer

   this architecture of this implementation looks like:
   id97 keras - negative sampling architecture

   id97 keras     negative sampling architecture

   let   s go through this architecture more carefully.  first, each of the
   words in our vocabulary is assigned an integer index between 0 and the
   size of our vocabulary (in this case, 10,000).  we pass two words into
   the network, one the target word and the other either a word from the
   surrounding context or a negative sample.  we    look up    these indexes
   as the rows of our embedding layer (10,000 x 300 weight tensor) to
   retrieve our 300 length word vectors.  we then perform a dot product
   operation between these vectors to get the similarity.  finally, we
   output the similarity to a sigmoid layer to give us a 1 or 0 indicator
   which we can match with the label given to the context word (1 for a
   true context word, 0 for a negative sample).

   the back-propagation of our errors will work to update the embedding
   layer to ensure that words which are truly similar to each other (i.e.
   share contexts) have vectors such that they return high similarity
   scores. let   s now implement this architecture in keras and we can test
   whether this turns out to be the case.

a id97 keras implementation

   this section will show you how to create your own id97 keras
   implementation     the code is hosted on this site   s [23]github
   repository.

data extraction

   to develop our id97 keras implementation, we first need some data.
    as in my [24]id97 tensorflow tutorial, we   ll be using a document
   data set from [25]here.  to extract the information, i   ll be using some
   of the same text extraction functions from the aforementioned
   [26]id97 tutorial, in particular, the collect_data function     check
   out that tutorial for further details.  basically, the function calls
   other functions which download the data, then a function that converts
   the text data into a string of integers     with each word in the
   vocabulary represented by a unique integer.  to call this function, we
   run:
vocab_size = 10000
data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab
_size)

   the first 7 words in the dataset are:
   [   anarchism   ,    originated   ,    as   ,    a   ,    term   ,    of   ,    abuse   ]

   after running collect_data, the new representation of these words
   (data) is:
   [5239, 3082, 12, 6, 195, 2, 3134]

   there are also two dictionaries returned from collect_data     the first
   where you can look up a word and get its integer representation, and
   the second the reverse i.e. you look up a word   s integer and you get
   its actual english representation.

   next, we need to define some constants for the training and also create
   a validation set of words so we can check the learning progress of our
   word vectors.

constants and the validation set

window_size = 3
vector_dim = 300
epochs = 1000000

valid_size = 16     # random set of words to evaluate similarity on.
valid_window = 100  # only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=false)

   the first constant, window_size, is the window of words around the
   target word that will be used to draw the context words from.  the
   second constant, vector_dim, is the size of each of our id27
   vectors     in this case, our embedding layer will be of size 10,000 x
   300.  finally, we have a large epochs variable     this designates the
   number of training iterations we are going to run.  id27,
   even with negative sampling, can be a time-consuming process.

   the next set of commands relate to the words we are going to check to
   see what other words grow in similarity to this validation set. during
   training, we will check which words begin to be deemed similar by the
   id27 vectors and make sure these line up with our
   understanding of the meaning of these words.  in this case, we will
   select 16 words to check, and pick these words randomly from the top
   100 most common words in the data-set (collect_data has assigned the
   most common words in the data set integers in ascending order i.e. the
   most common word is assigned 1, the next most common 2, etc.).

   next, we are going to look at a handy function in keras which does all
   the skip-gram / context processing for us.

the skip-gram function in keras

   to train our data set using negative sampling and the skip-gram method,
   we need to create data samples for both valid context words and for
   negative samples. this involves scanning through the data set and
   picking target words, then randomly selecting context words from within
   the window of words around the target word (i.e. if the target word is
      on    from    the cat sat on the mat   , with a window size of 2 the words
      cat   ,    sat   ,    the   ,    mat    could all be randomly selected as valid
   context words).  it also involves randomly selecting negative samples
   outside of the selected target word context. finally, we also need to
   set a label of 1 or 0, depending on whether the supplied context word
   is a true context word or a negative sample.  thankfully, keras has a
   function (skipgrams) which does all that for us     consider the
   following code:
sampling_table = sequence.make_sampling_table(vocab_size)
couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_
table=sampling_table)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype="int32")
word_context = np.array(word_context, dtype="int32")

print(couples[:10], labels[:10])

   ignoring the first line for the moment (make_sampling_table), the keras
   skipgrams function does exactly what we want of it     it returns the
   word couples in the form of (target, context) and also gives a matching
   label of 1 or 0 depending on whether context is a true context word or
   a negative sample. by default, it returns randomly shuffled couples and
   labels.  in the code above, we then split the couples tuple into
   separate word_target and word_context variables and make sure they are
   the right type.  the print function produces the following instructive
   output:

   couples:
   [[6503, 5], [187, 6754], [1154, 3870], [670, 1450], [4554, 1], [1037,
   250], [734, 4521], [1398, 7], [4495, 3374], [2881, 8637]]

   labels:
   [1, 0, 1, 0, 1, 1, 0, 1, 0, 0]

   the make_sampling_table() operation creates a table that skipgrams uses
   to ensure it produces negative samples in a balanced manner and not
   just the most common words.  the skipgrams operation by default selects
   the same amount of negative samples as it does true context words.

   we   ll feed the produced arrays (word_target, word_context) into our
   keras model later     now onto the id97 keras model itself.

the keras functional api and the embedding layers

   in this id97 keras implementation, we   ll be using the keras
   [27]functional api.  in my [28]previous keras tutorial, i used the
   keras sequential layer framework. this sequential layer framework
   allows the developer to easily bolt together layers, with the tensor
   outputs from each layer flowing easily and implicitly into the next
   layer.  in this case, we are going to do some things which are a little
   tricky     the sharing of a single embedding layer between two tensors,
   and an auxiliary output to measure similarity     and therefore we can   t
   use a straightforward sequential implementation.

   thankfully, the functional api is also pretty easy to use.  i   ll
   introduce it as we move through the code. the first thing we need to do
   is specify the structure of our model, as per the architecture diagram
   which i have shown above. as an initial step, we   ll create our input
   variables and embedding layer:
# create some input variables
input_target = input((1,))
input_context = input((1,))

embedding = embedding(vocab_size, vector_dim, input_length=1, name='embedding')

   first off, we need to specify what tensors are going to be input to our
   model, along with their size. in this case, we are just going to supply
   individual target and context words, so the input size for each input
   variable is simply (1,).  next, we create an embedding layer, which
   keras already has specified as a layer for us     embedding().  the first
   argument to this layer definition is the number of rows of our
   embedding layer     which is the size of our vocabulary (10,000).  the
   second is the size of each word   s embedding vector (the columns)     in
   this case, 300. we also specify the input length to the layer     in this
   case, it matches our input variables i.e. 1.  finally, we give it a
   name, as we will want to access the weights of this layer after we   ve
   trained it, and we can easily access the layer weights using the name.

   the weights for this layer are initialized automatically, but you can
   also specify an optional embeddings_initializer argument whereby you
   supply a [29]keras initializer object.  next, as per our architecture,
   we need to look up an embedding vector (length = 300) for our target
   and context words, by supplying the embedding layer with the word   s
   unique integer value:
target = embedding(input_target)
target = reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = reshape((vector_dim, 1))(context)

   as can be observed in the code above, the embedding vector is easily
   retrieved by supplying the word integer
   (i.e. input_target and input_context) in brackets to the previously
   created embedding operation/layer. for each word vector, we then use a
   keras reshape layer to reshape it ready for our upcoming dot product
   and similarity operation, as per our architecture.

   the next layer involves calculating our cosine similarity between the
   supplied word vectors:
# setup a cosine similarity operation which will be output in a secondary model
similarity = merge([target, context], mode='cos', dot_axes=0)

   as can be observed, keras supplies a merge operation with a
   mode argument which we can set to    cos        this is the cosine similarity
   between the two word vectors, target, and context. this similarity
   operation will be returned via the output of a secondary model     but
   more on how this is performed later.

   the next step is to continue on with our primary model architecture,
   and the dot product as our measure of similarity which we are going to
   use in the primary flow of the negative sampling architecture:
# now perform the dot product operation to get a similarity measure
dot_product = merge([target, context], mode='dot', dot_axes=1)
dot_product = reshape((1,))(dot_product)
# add the sigmoid output layer
output = dense(1, activation='sigmoid')(dot_product)

   again, we use the keras merge operation and apply it to our target
   and context word vectors, with the mode argument set to    dot    to get
   the simple dot product.  we then do another reshape layer, and take the
   reshaped dot product value (a single data point/scalar) and apply it to
   a keras dense layer, with the activation function of the layer set to
      sigmoid   .  this is the output of our id97 keras architecture.

   next, we need to gather everything into a keras model and compile it,
   ready for training:
# create the primary training model
model = model(input=[input_target, input_context], output=output)
model.compile(loss='binary_crossid178', optimizer='rmsprop')

   here, we create the functional api based model for our id97 keras
   architecture.  what the model definition requires is a specification of
   the input arrays to the model (these need to be [30]numpy arrays) and
   an output tensor     these are supplied as per the previously explained
   architecture.  we then compile the model, by supplying a id168
   that we are going to use (in this case, binary cross id178 i.e. cross
   id178 when the labels are either 0 or 1) and an optimizer (in this
   case, [31]rmsprop).  the id168 is applied to the output
   variable.

   the question now is, if we want to use the similarity operation which
   we defined in the architecture to allow us to check on how things are
   progressing during training, how do we access it? we could output it
   via the model definition (i.e. output=[similarity, output]) but then
   keras would be trying to apply the id168 and the optimizer to
   this value during training and this isn   t what we created the operation
   for.

   there is another way, which is quite handy     we create another model:
# create a secondary validation model to run our similarity checks during traini
ng
validation_model = model(input=[input_target, input_context], output=similarity)

   we can now use this validation_model to access
   the similarity operation, and this model will actually share the
   embedding layer with the primary model.  note, because this model won   t
   be involved in training, we don   t have to run a keras compile operation
   on it.

   now we are ready to train the model     but first, let   s setup a function
   to print out the words with the closest similarity to our validation
   examples (valid_examples).

the similarity callback

   we want to create a    callback    which we can use to figure out which
   words are closest in similarity to our validation examples, so we can
   monitor the training progress of our embedding layer.
class similaritycallback:
    def run_sim(self):
        for i in range(valid_size):
            valid_word = reverse_dictionary[valid_examples[i]]
            top_k = 8  # number of nearest neighbors
            sim = self._get_sim(valid_examples[i])
            nearest = (-sim).argsort()[1:top_k + 1]
            log_str = 'nearest to %s:' % valid_word
            for k in range(top_k):
                close_word = reverse_dictionary[nearest[k]]
                log_str = '%s %s,' % (log_str, close_word)
            print(log_str)

    @staticmethod
    def _get_sim(valid_word_idx):
        sim = np.zeros((vocab_size,))
        in_arr1 = np.zeros((1,))
        in_arr2 = np.zeros((1,))
        for i in range(vocab_size):
            in_arr1[0,] = valid_word_idx
            in_arr2[0,] = i
            out = validation_model.predict_on_batch([in_arr1, in_arr2])
            sim[i] = out
        return sim
sim_cb = similaritycallback()

   this class runs through all the valid_examples and gets the similarity
   score between the given validation word and all the other words in the
   vocabulary.  it gets the similarity score by running _get_sim(), which
   features a loop which runs through each word in the vocabulary, and
   runs a predict_on_batch() operation on the validation model     this
   basically looks up the embedding vectors for the two supplied words
   (the valid_example and the looped vocabulary example) and returns
   the similarity operation result.  the main loop then sorts the
   similarity in descending order and creates a string to print out the
   top 8 words with the closest similarity to the validation example.

   the output of this callback will be seen during our training loop,
   which is presented below.

the training loop

   the main training loop of the model is:
arr_1 = np.zeros((1,))
arr_2 = np.zeros((1,))
arr_3 = np.zeros((1,))
for cnt in range(epochs):
    idx = np.random.randint(0, len(labels)-1)
    arr_1[0,] = word_target[idx]
    arr_2[0,] = word_context[idx]
    arr_3[0,] = labels[idx]
    loss = model.train_on_batch([arr_1, arr_2], arr_3)
    if i % 100 == 0:
        print("iteration {}, loss={}".format(cnt, loss))
    if cnt % 10000 == 0:
        sim_cb.run_sim()

   in this loop, we run through the total number of epochs.  first, we
   select a random index from our word_target, word_context and labels
   arrays and place the values in dummy numpy arrays.  then we supply
   the input ([word_target, word_context]) and outputs (labels) to the
   primary model and run a train_on_batch() operation.  this returns the
   current loss evaluation, loss, of the model and prints it. every 10,000
   iterations we also run functions in the similaritycallback.

   here are some of the word similarity outputs for the validation example
   word    eight    as we progress through the training iterations:

   iterations = 0:

   nearest to eight: much, holocaust, representations, density, fire,
   senators, dirty, fc

   iterations = 50,000:

   nearest to eight: six, finest, championships, mathematical, floor, pg,
   smoke, recurring

   iterations = 200,000:

   nearest to eight: six, five, two, one, nine, seven, three, four

   as can be observed, at the start of the training, all sorts of random
   words are associated with    six   .  however, as the training iterations
   increase, slowly other word numbers are associated with    six    until
   finally all of the closest 8 words are number words.

   there you have it     in this id97 keras tutorial, i   ve shown you how
   the id97 methodology works with negative sampling, and how to
   implement it in keras using its functional api.  in the [32]next
   tutorial, i will show you how to reload trained embedding weights into
   both keras and tensorflow. you can also checkout how embedding layers
   work in id137 in [33]this tutorial.

     __________________________________________________________________

eager to learn more? get the book [34]here
     __________________________________________________________________


about the author

     cognac says:
   [35]september 20, 2017 at 10:45 pm

   great tutorial, thanks! however, since keras has deprecated    merge   
   method (replaced by    merge    layer), maybe it   s appropriate to use
   merge.dot(normalize=true)?

   ____________________ (button)

   recent posts
     * [36]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [37]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [38]keras, eager and tensorflow 2.0     a new tf paradigm
     * [39]introduction to tensorboard and tensorflow visualization
     * [40]tensorflow eager tutorial

   recent comments
     * andry on [41]neural networks tutorial     a pathway to deep learning
     * sandipan on [42]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [43]neural networks tutorial     a pathway to deep learning
     * martin on [44]neural networks tutorial     a pathway to deep learning
     * uri on [45]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [46]march 2019
     * [47]january 2019
     * [48]october 2018
     * [49]september 2018
     * [50]august 2018
     * [51]july 2018
     * [52]june 2018
     * [53]may 2018
     * [54]april 2018
     * [55]march 2018
     * [56]february 2018
     * [57]november 2017
     * [58]october 2017
     * [59]september 2017
     * [60]august 2017
     * [61]july 2017
     * [62]may 2017
     * [63]april 2017
     * [64]march 2017

   categories
     * [65]amazon aws
     * [66]cntk
     * [67]convolutional neural networks
     * [68]cross id178
     * [69]deep learning
     * [70]gensim
     * [71]gpus
     * [72]keras
     * [73]id168s
     * [74]lstms
     * [75]neural networks
     * [76]nlp
     * [77]optimisation
     * [78]pytorch
     * [79]recurrent neural networks
     * [80]id23
     * [81]tensorboard
     * [82]tensorflow
     * [83]tensorflow 2.0
     * [84]weight initialization
     * [85]id97

   meta
     * [86]log in
     * [87]entries rss
     * [88]comments rss
     * [89]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [90]thrive themes | powered by [91]wordpress

   (button) close dialog

   session expired

   [92]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[93]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/id97-keras-tutorial/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/id97-keras-tutorial/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/id97-keras-tutorial/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  14. http://adventuresinmachinelearning.com/id97-keras-tutorial/#comments
  15. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  16. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  17. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  18. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  19. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  20. https://arxiv.org/pdf/1310.4546.pdf
  21. https://en.wikipedia.org/wiki/cosine_similarity
  22. https://en.wikipedia.org/wiki/dot_product
  23. https://github.com/adventuresinml/adventures-in-ml-code
  24. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  25. http://mattmahoney.net/dc/
  26. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  27. https://keras.io/getting-started/functional-api-guide/
  28. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  29. https://keras.io/initializers/
  30. http://www.numpy.org/
  31. http://ruder.io/optimizing-gradient-descent/
  32. https://adventuresinmachinelearning.com/gensim-id97-tutorial/
  33. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  34. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  35. https://adventuresinmachinelearning.com/id97-keras-tutorial/#comments/5046
  36. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  37. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  38. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  39. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  40. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  41. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  42. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  43. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  44. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  45. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  46. https://adventuresinmachinelearning.com/2019/03/
  47. https://adventuresinmachinelearning.com/2019/01/
  48. https://adventuresinmachinelearning.com/2018/10/
  49. https://adventuresinmachinelearning.com/2018/09/
  50. https://adventuresinmachinelearning.com/2018/08/
  51. https://adventuresinmachinelearning.com/2018/07/
  52. https://adventuresinmachinelearning.com/2018/06/
  53. https://adventuresinmachinelearning.com/2018/05/
  54. https://adventuresinmachinelearning.com/2018/04/
  55. https://adventuresinmachinelearning.com/2018/03/
  56. https://adventuresinmachinelearning.com/2018/02/
  57. https://adventuresinmachinelearning.com/2017/11/
  58. https://adventuresinmachinelearning.com/2017/10/
  59. https://adventuresinmachinelearning.com/2017/09/
  60. https://adventuresinmachinelearning.com/2017/08/
  61. https://adventuresinmachinelearning.com/2017/07/
  62. https://adventuresinmachinelearning.com/2017/05/
  63. https://adventuresinmachinelearning.com/2017/04/
  64. https://adventuresinmachinelearning.com/2017/03/
  65. https://adventuresinmachinelearning.com/category/amazon-aws/
  66. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  67. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  68. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  69. https://adventuresinmachinelearning.com/category/deep-learning/
  70. https://adventuresinmachinelearning.com/category/nlp/gensim/
  71. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  72. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  73. https://adventuresinmachinelearning.com/category/loss-functions/
  74. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  75. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  76. https://adventuresinmachinelearning.com/category/nlp/
  77. https://adventuresinmachinelearning.com/category/optimisation/
  78. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  79. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  80. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  81. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  82. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  83. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  84. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  85. https://adventuresinmachinelearning.com/category/nlp/id97/
  86. https://adventuresinmachinelearning.com/wp-login.php
  87. https://adventuresinmachinelearning.com/feed/
  88. https://adventuresinmachinelearning.com/comments/feed/
  89. https://wordpress.org/
  90. https://www.thrivethemes.com/
  91. http://www.wordpress.org/
  92. https://adventuresinmachinelearning.com/wp-login.php
  93. http://adventuresinmachinelearning.com/id97-keras-tutorial/

   hidden links:
  95. https://adventuresinmachinelearning.com/author/admin/
