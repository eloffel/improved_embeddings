   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]gab41
     * [9]machine learning
     * [10]data science
     * [11]deep learning
     * [12]lab41 website
     __________________________________________________________________

   [1*oowsowheq5kyj4n0p2ptna.png]
   source [13]http://tomtunguz.com/key-ingredient-machine-learning/

the 10 algorithms machine learning engineers need to know

   go to the profile of james le
   [14]james le (button) blockedunblock (button) followfollowing
   jul 11, 2016

   it is no doubt that the sub-field of machine learning / artificial
   intelligence has increasingly gained more popularity in the past couple
   of years. as big data is the hottest trend in the tech industry at the
   moment, machine learning is incredibly powerful to make predictions or
   calculated suggestions based on large amounts of data. some of the most
   common examples of machine learning are netflix   s algorithms to make
   movie suggestions based on movies you have watched in the past or
   amazon   s algorithms that recommend books based on books you have bought
   before.

   so if you want to learn more about machine learning, how do you start?
   for me, my first introduction is when i took an artificial intelligence
   class when i was studying abroad in copenhagen. my lecturer is a
   full-time applied math and cs professor at the technical university of
   denmark, in which his research areas are logic and artificial, focusing
   primarily on the use of logic to model human-like planning, reasoning
   and problem solving. the class was a mix of discussion of theory/core
   concepts and hands-on problem solving. the textbook that we used is one
   of the ai classics: [15]peter norvig   s artificial intelligence         a
   modern approach, in which we covered major topics including intelligent
   agents, problem-solving by searching, adversarial search, id203
   theory, multi-agent systems, social ai, philosophy/ethics/future of ai.
   at the end of the class, in a team of 3, we implemented simple
   search-based agents solving transportation tasks in a virtual
   environment as a programming project.

   i have learned a tremendous amount of knowledge thanks to that class,
   and decided to keep learning about this specialized topic. in the last
   few weeks, i have been multiple tech talks in san francisco on deep
   learning, neural networks, data architecture         and a machine learning
   conference with a lot of well-known professionals in the field. most
   importantly, i enrolled in udacity   s [16]intro to machine learning
   online course in the beginning of june and has just finished it a few
   days ago. in this post, i want to share some of the most common machine
   learning algorithms that i learned from the course.

   machine learning algorithms can be divided into 3 broad
   categories         supervised learning, unsupervised learning, and
   id23. supervised learning is useful in cases where a
   property (label) is available for a certain dataset (training set), but
   is missing and needs to be predicted for other instances. unsupervised
   learning is useful in cases where the challenge is to discover implicit
   relationships in a given unlabeled dataset (items are not
   pre-assigned). id23 falls between these 2
   extremes         there is some form of feedback available for each predictive
   step or action, but no precise label or error message. since this is an
   intro class, i didn   t learn about id23, but i hope
   that 10 algorithms on supervised and unsupervised learning will be
   enough to keep you interested.

supervised learning

   1. id90: a decision tree is a decision support tool that uses
   a tree-like graph or model of decisions and their possible
   consequences, including chance-event outcomes, resource costs, and
   utility. take a look at the image to get a sense of how it looks like.
   [1*duigb4eoqxg4mdksykvfkg.gif]
   decision tree

   from a business decision point of view, a decision tree is the minimum
   number of yes/no questions that one has to ask, to assess the
   id203 of making a correct decision, most of the time. as a
   method, it allows you to approach the problem in a structured and
   systematic way to arrive at a logical conclusion.

   2. na  ve bayes classification: na  ve bayes classifiers are a family of
   simple probabilistic classifiers based on applying bayes    theorem with
   strong (na  ve) independence assumptions between the features. the
   featured image is the equation         with p(a|b) is posterior id203,
   p(b|a) is likelihood, p(a) is class prior id203, and p(b) is
   predictor prior id203.
   [1*ibfidn5mr6e9yk1qslvoga.jpeg]
   naive bayes classification

   some of real world examples are:

      to mark an email as spam or not spam

      classify a news article about technology, politics, or sports

      check a piece of text expressing positive emotions, or negative
   emotions?

      used for face recognition software.

   3. ordinary least squares regression: if you know statistics, you
   probably have heard of id75 before. least squares is a
   method for performing id75. you can think of linear
   regression as the task of fitting a straight line through a set of
   points. there are multiple possible strategies to do this, and
      ordinary least squares    strategy go like this         you can draw a line,
   and then for each of the data points, measure the vertical distance
   between the point and the line, and add these up; the fitted line would
   be the one where this sum of distances is as small as possible.
   [1*sy_6ipmbh_21kd0_dds1hq.png]
   ordinary least squares regression

   linear refers the kind of model you are using to fit the data, while
   least squares refers to the kind of error metric you are minimizing
   over.

   4. id28: id28 is a powerful statistical
   way of modeling a binomial outcome with one or more explanatory
   variables. it measures the relationship between the categorical
   dependent variable and one or more independent variables by estimating
   probabilities using a logistic function, which is the cumulative
   logistic distribution.
   [1*nsphnzg5aattwbi4jwkitw.png]
   id28

   in general, regressions can be used in real-world applications such as:

      credit scoring

      measuring the success rates of marketing campaigns

      predicting the revenues of a certain product

      is there going to be an earthquake on a particular day?

   5. support vector machines: id166 is binary classification algorithm.
   given a set of points of 2 types in n dimensional place, id166 generates
   a (n         1) dimensional hyperlane to separate those points into 2 groups.
   say you have some points of 2 types in a paper which are linearly
   separable. id166 will find a straight line which separates those points
   into 2 types and situated as far as possible from all those points.
   [1*3mu2_8mbrm2qhnz0tecvug.png]
   support vector machine

   in terms of scale, some of the biggest problems that have been solved
   using id166s (with suitably modified implementations) are display
   advertising, human splice site recognition, image-based gender
   detection, large-scale image classification   

   6. ensemble methods: ensemble methods are learning algorithms that
   construct a set of classifiers and then classify new data points by
   taking a weighted vote of their predictions. the original ensemble
   method is bayesian averaging, but more recent algorithms include
   error-correcting output coding, id112, and boosting.
   [1*bkzn0jpoijhob5g0j5xj5a.png]
   id108 algorithms

   so how do ensemble methods work and why are they superior to individual
   models?

      they average out biases: if you average a bunch of democratic-leaning
   polls and republican-leaning polls together, you will get an average
   something that isn   t leaning either way.

      they reduce the variance: the aggregate opinion of a bunch of models
   is less noisy than the single opinion of one of the models. in finance,
   this is called diversification         a mixed portfolio of many stocks will
   be much less variable than just one of the stocks alone. this is why
   your models will be better with more data points rather than fewer.

      they are unlikely to over-fit: if you have individual models that
   didn   t over-fit, and you are combining the predictions from each model
   in a simple way (average, weighted average, id28), then
   there   s no room for over-fitting.

unsupervised learning

   7. id91 algorithms: id91 is the task of grouping a set of
   objects such that objects in the same group (cluster) are more similar
   to each other than to those in other groups.
   [1*a58qzwbn7aaxfutperirtq.png]
   id91 algorithms

   every id91 algorithm is different, and here are a couple of them:

      centroid-based algorithms

      connectivity-based algorithms

      density-based algorithms

      probabilistic

      id84

      neural networks / deep learning

   8. principal component analysis: pca is a statistical procedure that
   uses an orthogonal transformation to convert a set of observations of
   possibly correlated variables into a set of values of linearly
   uncorrelated variables called principal components.
   [1*wrkdn-nyf0mmumhfoxva2q.png]
   principal component analysis

   some of the applications of pca include compression, simplifying data
   for easier learning, visualization. notice that domain knowledge is
   very important while choosing whether to go forward with pca or not. it
   is not suitable in cases where data is noisy (all the components of pca
   have quite a high variance).

   9. singular value decomposition: in id202, svd is a
   factorization of a real complex matrix. for a given m * n matrix m,
   there exists a decomposition such that m = u  v, where u and v are
   unitary matrices and    is a diagonal matrix.
   [1*khpkx-04jo3utclrxopj0w.png]
   singular value decomposition

   pca is actually a simple application of svd. in id161, the
   1st face recognition algorithms used pca and svd in order to represent
   faces as a linear combination of    eigenfaces   , do dimensionality
   reduction, and then match faces to identities via simple methods;
   although modern methods are much more sophisticated, many still depend
   on similar techniques.

   10. independent component analysis: ica is a statistical technique for
   revealing hidden factors that underlie sets of random variables,
   measurements, or signals. ica defines a generative model for the
   observed multivariate data, which is typically given as a large
   database of samples. in the model, the data variables are assumed to be
   linear mixtures of some unknown latent variables, and the mixing system
   is also unknown. the latent variables are assumed non-gaussian and
   mutually independent, and they are called independent components of the
   observed data.
   [1*rbn-qgygtflsxzforsqoeg.png]
   independent component analysis

   ica is related to pca, but it is a much more powerful technique that is
   capable of finding the underlying factors of sources when these classic
   methods fail completely. its applications include digital images,
   document databases, economic indicators and psychometric measurements.

   now go forth and wield your understanding of algorithms to create
   machine learning applications that make better experiences for people
   everywhere.

   p.s: i recently took andrew ng   s famous machine learning coursera mooc
   to refresh my knowledge of these algorithms. you can get the lecture
   slides and matlab source code for the course exercises [17]all from my
   github here. thanks for the overwhelming response to this post!

               
   if you enjoyed this piece, i   d love it if you hit the clap button      so
   others might stumble upon it. you can find my own code on [18]github,
   and more of my writing and projects at [19]https://jameskle.com/. you
   can also follow me on [20]twitter, [21]email me directly or [22]find me
   on linkedin. [23]sign up for my newsletter to receive my latest
   thoughts on data science, machine learning, and artificial intelligence
   right at your inbox!

     * [24]machine learning
     * [25]data science
     * [26]algorithms
     * [27]artificial intelligence

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of james le

[28]james le

   medium member since apr 2019

   blue ocean thinker ([29]https://jameskle.com/)

     (button) follow
   [30]gab41

[31]gab41

   gab41 is lab41's blog exploring data science, machine learning, and
   artificial intelligence. geek out with us!

     * (button)
       (button) 3k
     * (button)
     *
     *

   [32]gab41
   never miss a story from gab41, when you sign up for medium. [33]learn
   more
   never miss a story from gab41
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://gab41.lab41.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f4bb63f5b2fa
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa&source=--------------------------nav_reg&operation=register
   8. https://gab41.lab41.org/?source=logo-lo_u9243qeotkeh---b9e75b66ab60
   9. https://gab41.lab41.org/tagged/machine-learning
  10. https://gab41.lab41.org/tagged/data-science
  11. https://gab41.lab41.org/tagged/deep-learning
  12. http://www.lab41.org/
  13. http://tomtunguz.com/key-ingredient-machine-learning/
  14. https://gab41.lab41.org/@james_aka_yale
  15. https://www.amazon.com/artificial-intelligence-modern-approach-3rd/dp/0136042597
  16. https://www.udacity.com/course/intro-to-machine-learning--ud120
  17. https://github.com/khanhnaid1131994/machine-learning
  18. https://github.com/khanhnaid1131994
  19. https://jameskle.com/
  20. https://twitter.com/@james_aka_yale
  21. mailto:khanhle.1013@gmail.com
  22. http://www.linkedin.com/in/khanhnaid11394
  23. http://eepurl.com/dewjzb
  24. https://gab41.lab41.org/tagged/machine-learning?source=post
  25. https://gab41.lab41.org/tagged/data-science?source=post
  26. https://gab41.lab41.org/tagged/algorithms?source=post
  27. https://gab41.lab41.org/tagged/artificial-intelligence?source=post
  28. https://gab41.lab41.org/@james_aka_yale
  29. https://jameskle.com/
  30. https://gab41.lab41.org/?source=footer_card
  31. https://gab41.lab41.org/?source=footer_card
  32. https://gab41.lab41.org/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://gab41.lab41.org/@james_aka_yale?source=post_header_lockup
  36. https://medium.com/p/f4bb63f5b2fa/share/twitter
  37. https://medium.com/p/f4bb63f5b2fa/share/facebook
  38. https://gab41.lab41.org/@james_aka_yale?source=footer_card
  39. https://medium.com/p/f4bb63f5b2fa/share/twitter
  40. https://medium.com/p/f4bb63f5b2fa/share/facebook
