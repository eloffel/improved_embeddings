    (button) toggle navigation
     * [1]learning
          + [2]tutorials
          + [3]articles
     * [4]books
     * [5]courses
          + [6]data science
          + [7]machine learning
     * [8]contribute
     * [9][fb_bu1r2s1.svg] 1.2k
     * [10][twitter_gvy5lp8.svg] 2.6k
     * [11][github-ico.svg] 123

   you are reading tutorials
   1.2kshares

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe

featured courses:

   [12]

machine learning by andrew ng, coursera

   [13]

data science (r) by johns hopkins, coursera

   [14]

applied data science with python, coursera

   [15]

dataquest

featured books:

   [16]

introduction to statistical learning (islr)

   [17]

data science from scratch

   [18]

agile data science 2.0

recent articles:

   [19]

top 5 machine learning courses for 2019 - learn machine learning

   [20]

top 7 online data science courses for 2019 - learn data science

   [21]

beginner's guide to using databases with python: postgres, sqlalchemy, and
alembic

   [22]

most recommended data science and machine learning books by top master's
programs

   id116 & other id91 algorithms: a quick intro with python
   data scientist author photo
   author: [23]nikos koufos
   cs & engineering post graduate
   brendan martin
   author: [24]brendan martin
   founder of learndatasci
   [25]unsupervised learning, [26]algorithms, [27]id91

id116 & other id91 algorithms: a quick intro with python

   unsupervised learning via id91 algorithms. let's work with the
   karate club dataset to perform several types of id91 algorithms.
   [lightbulb-brain.svg]

before

   tutorial

you should already know:

     * beginner python
     * data science packages (pandas, matplotlib, seaborn, sklearn)

   these can be learned interactively through [28]datacamp
   [lightbulb-check.svg]

after

   tutorial

you will have learned:

     * id91: id116, agglomerative, spectral, affinity propagation
     * how to plot networks
     * how to evaluate different id91 techniques

   id91 is the grouping of objects together so that objects
   belonging in the same group (cluster) are more similar to each other
   than those in other groups (clusters). in this intro cluster analysis
   tutorial, we'll check out a few algorithms in python so you can get a
   basic understanding of the fundamentals of id91 on a real
   dataset.

article resources

     * source code: [29]github.
     * dataset: available via networkx library (see code below), also see
       paper: [30]an information flow model for conflict and fission in
       small groups

the dataset

   for the id91 problem, we will use the famous zachary   s karate
   club dataset. for more detailed information on the study see the linked
   paper.

   essentially there was a karate club that had an administrator    john a   
   and an instructor    mr. hi   , and a conflict arose between them which
   caused the students to split into two groups; one that followed john
   and one that followed mr. hi.

   the students are the nodes in our graph, and the edges, or links,
   between the nodes are the result of social interactions outside of the
   club between students.

   since there was an eventual split into two groups (clusters) by the end
   of the karate club dispute, and we know which group each student ended
   up in, we can use the results as truth values for our id91 to
   gauge performance between different algorithms.
   karate club model of relationships
   social relationships (edges) of each student (node) outside of the
   karate club

getting started with id91 in python

imports for this tutorial

from sklearn import cluster
import networkx as nx
from collections import defaultdict
import matplotlib.pyplot as plt
from matplotlib import cm
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics.cluster import normalized_mutual_info_score
from sklearn.metrics.cluster import adjusted_rand_score


   we'll be using the scikit-learn, pandas , and numpy stack with the
   addition of matplotlib, seaborn and networkx for graph visualization. a
   simple pip/conda install should work with each of these.

   let's go ahead and import the dataset directly from networkx and also
   set up the spring layout positioning for the graph visuals:
g = nx.karate_club_graph()

pos = nx.spring_layout(g)


   let's go ahead a create a function to let us visualize the dataset:
def draw_communities(g, membership, pos):
    """draws the nodes to a plot with assigned colors for each individual cluste
r
    parameters
    ----------
    g : networkx graph
    membership : list
        a list where the position is the student and the value at the position i
s the student club membership.
        e.g. `print(membership[8]) --> 1` means that student #8 is a member of c
lub 1.
    pos : positioning as a networkx spring layout
        e.g. nx.spring_layout(g)
    """
    fig, ax = plt.subplots(figsize=(16,9))

    # convert membership list to a dict where key=club, value=list of students i
n club
    club_dict = defaultdict(list)
    for student, club in enumerate(membership):
        club_dict[club].append(student)

    # normalize number of clubs for choosing a color
    norm = colors.normalize(vmin=0, vmax=len(club_dict.keys()))

    for club, members in club_dict.items():
        nx.draw_networkx_nodes(g, pos,
                               nodelist=members,
                               node_color=cm.jet(norm(club)),
                               node_size=500,
                               alpha=0.8,
                               ax=ax)

    # draw edges (social connections) and show final plot
    plt.title("zachary's karate club")
    nx.draw_networkx_edges(g, pos, alpha=0.5, ax=ax)


   in order to color the student nodes according to their club membership,
   we're using matplotlib's normalize class to fit the number of clubs
   into the (0, 1) interval. you might be wondering why we need to do this
   when there's only two clubs that resulted from the study, but we'll
   find out later that some id91 algorithms didn't get it right and
   we need to be able to colorize more than two clubs.

   to compare our algorithm's, performance we want the true labels, i.e.
   where each student ended up after the club fission. unfortunately, the
   true labels are not provided in the networkx dataset, but we can
   retrieve them from the study itself and hardcode them here:
# true labels of the group each student (node) unded up in. found via the origin
al paper
y_true = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1
, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]


   now, let's use our draw function to visualize the true student club
   membership and their social connections:
draw_communities(g, y_true, pos)


   result:
   zachary's karate club graph visualization

   this is the true division of the karate club. we'll see how our
   algorithms do when trying to cluster on their own below.

   before that, we need to preprocess the data by transforming the graph
   into a matrix since that is the format required for the clusterers.
   remember this edge_mat variable below, we'll be using this for input to
   our id91 models in the next few sections:
def graph_to_edge_matrix(g):
    """convert a networkx graph into an edge matrix.
    see https://www.wikiwand.com/en/incidence_matrix for a good explanation on e
dge matrices

    parameters
    ----------
    g : networkx graph
    """
    # initialize edge matrix with zeros
    edge_mat = np.zeros((len(g), len(g)), dtype=int)

    # loop to set 0 or 1 (diagonal elements are set to 1)
    for node in g:
        for neighbor in g.neighbors(node):
            edge_mat[node][neighbor] = 1
        edge_mat[node][node] = 1

    return edge_mat


   let's transform our graph into a matrix and print the result to see
   what it contains:
edge_mat = graph_to_edge_matrix(g)
edge_mat

"""
array([[1, 1, 1, ..., 1, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 1, 0],
       ...,
       [1, 0, 0, ..., 1, 1, 1],
       [0, 0, 1, ..., 1, 1, 1],
       [0, 0, 0, ..., 1, 1, 1]])
"""


id91 algorithms

   unsupervised learning via id91 can work quite well in a lot of
   cases, but it can also perform terribly. we'll go through a few
   algorithms that are known to perform very well and see how the do on
   the same dataset. fortunately, we can use the true cluster labels from
   the paper to calculate metrics and determine which is the best
   id91 option.

id116 id91

   (button) step (button) restart

   n (the number of node): 100_________________
   k (the number of cluster): 5___________________
   (button) new

   source: [31]github.com/nitoyon/tech.nitoyon.com

   id116 is considered by many to be the gold standard when it comes to
   id91 due to its simplicity and performance, so it's the first one
   we'll try out.

   when you have no idea at all what algorithm to use, id116 is usually
   the first choice.

   id116 works by defining spherical clusters that are separable in a
   way so that the mean value converges towards the cluster center.
   because of this, id116 may underperform sometimes.

   to simply construct and train a id116 model, we can use sklearn's
   package. before we do, we are going to define the number of clusters we
   know to be true (two), a list to hold results (labels), and a
   dictionary containing each algorithm we end up trying:
k_clusters = 2
results = []
algorithms = {}

algorithms['kmeans'] = cluster.kmeans(n_clusters=k_clusters, n_init=200)


   to fit this model, we could simply call
   algorithms['kmeans'].fit(edge_mat).

   since we'll be testing a bunch of id91 packages, we can loop over
   each and fit them at the same time. see below.

agglomerative id91

   the main idea behind agglomerative id91 is that each node first
   starts in its own cluster, and then pairs of clusters recursively merge
   together in a way that minimally increases a given linkage distance.

   the main advantage of agglomerative id91 (and hierarchical
   id91 in general) is that you don   t need to specify the number of
   clusters. that of course, comes with a price: performance. but, in
   sklearn   s implementation, you can specify the number of clusters to
   assist the algorithm   s performance.

   you can find a lot more information about how this technique works
   mathematically [32]here.

   let's create and train an agglomerative model and add it to our
   dictionary:
algorithms['agglom'] = cluster.agglomerativeid91(n_clusters=k_clusters, li
nkage="ward")


spectral id91

   the spectral id91 technique applies id91 to a projection of
   the normalized [33]laplacian. when it comes to image id91,
   spectral id91 works quite well.
algorithms['spectral'] = cluster.spectralid91(n_clusters=k_clusters, affin
ity="precomputed", n_init=200)


affinity propagation

   affinity propagation is a bit different. unlike the previous
   algorithms, this one does not require the number of clusters to be
   determined before running the algorithm.

   affinity propagation performs really well on several id161
   and biology problems, such as id91 pictures of human faces and
   identifying regulated transcripts, but we'll soon find out it doesn't
   work well for our dataset.

   let's add this final algorithm to our dictionary and wrap it all up by
   fitting each model:
algorithms['affinity'] = cluster.affinitypropagation(damping=0.6)

# fit all models
for model in algorithms.values():
    model.fit(edge_mat)
    results.append(list(model.labels_))


metrics & plotting

   well, it is time to choose which algorithm is more suitable for our
   data. a simple visualization of the result might work on small
   datasets, but imagine a graph with one thousand, or even ten thousand,
   nodes. that would be slightly chaotic for the human eye. so, let
   calculate the adjusted rand score (ars) and the normalized mutual
   information (nmi) metrics for easier interpretation.

   normalized mutual information (nmi)

   mutual information of two random variables is a measure of the mutual
   dependence between the two variables. normalized mutual information is
   a id172 of the mutual information (mi) score to scale the
   results between 0 (no mutual information) and 1 (perfect correlation).
   in other words, 0 means dissimilar and 1 means
   a perfect match.

get a free id203 and statistics workbook

   12 problems and solutions using python
   email address *
   ____________________
     * [x] testing

   ____________________
   subscribe

   adjusted rand score (ars)

   adjusted rand score on the other hand, computes a similarity measure
   between two clusters. ars considers all pairs of samples and counts
   pairs that are assigned in the same or different clusters in the
   predicted and true clusters.

   if that's a little weird to think about, have in mind that, for now, 0
   is the lowest similarity and 1 is the highest.

   in order to plot our scores, let's first calculate them. remember that
   y_true is still a dictionary where the key is a student and the value
   is the club they ended up in. we need to get y_true's values first in
   order to compare them to y_pred:
nmi_results = []
ars_results = []

y_true_val = list(y_true.values()

# append the results into lists
for y_pred in results:
    nmi_results.append(normalized_mutual_info_score(y_true_val, y_pred))
    ars_results.append(adjusted_rand_score(y_true_val, y_pred))


   let's now plot both scores side-by-side along with their averages for a
   better comparison.

   we're using seaborn's barplot along with matplotlib just for simpler
   coloring syntax. most of the following is pretty simple.

   the only thing fancy we added was the text on top of the bars. this is
   achieved on each subplot by referencing each axes (subplot), then using
   the index (i) as the x-coordinate, the score (v) as the y-coordinate,
   followed by the rounded value of v as the actual text to show on top.
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=true, figsize=(16, 5))

x = np.arange(len(y))
avg = [sum(x) / 2 for x in zip(nmi_results, ars_results)]

xlabels = list(algorithms.keys())

sns.barplot(x, nmi_results, palette='blues', ax=ax1)
sns.barplot(x, ars_results, palette='reds', ax=ax2)
sns.barplot(x, avg, palette='greens', ax=ax3)

ax1.set_ylabel('nmi score')
ax2.set_ylabel('ars score')
ax3.set_ylabel('average score')

# # add the xlabels to the chart
ax1.set_xticklabels(xlabels)
ax2.set_xticklabels(xlabels)
ax3.set_xticklabels(xlabels)

# add the actual value on top of each bar
for i, v in enumerate(zip(nmi_results, ars_results, avg)):
    ax1.text(i - 0.1, v[0] + 0.01, str(round(v[0], 2)))
    ax2.text(i - 0.1, v[1] + 0.01, str(round(v[1], 2)))
    ax3.text(i - 0.1, v[2] + 0.01, str(round(v[2], 2)))

# show the final plot
plt.show()


   result:
   id91 algorithm comparison chart

   as you can see in the resulting chart, id116 and agglomerative
   id91 have the best possible outcome for our dataset. that, of
   course, does not mean that spectral and agglomerative are
   low-performing algorithms, just that the did not fit in our particular
   dataset.

   out of curiosity, let's create a new function to plot where each
   algorithm went wrong by comparing the predicted student clusters with
   the true student clusters:
def draw_true_vs_pred(g, y_true, y_pred, pos, algo_name, ax):

    for student, club in y_true.items():
        if y_pred is not none:
            if club == y_pred[student]:
                node_color = [0, 1, 0]
                node_shape = 'o'
            else:
                node_color = [0, 0, 0]
                node_shape = 'x'

        nx.draw_networkx_nodes(g, pos,
                               nodelist=[student],
                               node_color=node_color,
                               node_size=250,
                               alpha=0.7,
                               ax=ax,
                               node_shape=node_shape)

    # draw edges and show final plot
    ax.set_title(algo_name)
    nx.draw_networkx_edges(g, pos, alpha=0.5, ax=ax)


fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=true, sharey=true, fig
size=(10, 10))

for algo_name, ax in zip(algorithms.keys(), [ax1, ax2, ax3, ax4]):
    draw_true_vs_pred(g, y_true, algorithms[algo_name].labels_, pos, algo_name,
ax)


   result:
   id91 correct vs. incorrect comparison

   this graph depicts each algorithm's correct (green circle) and
   incorrect (black x) cluster assignments.

   interestingly, we see that although the affinity algorithm had a higher
   average score, it only put one node in the correct cluster.

   to see the reason why, we can look at the cluster information from that
   algorithm:
cluster_centers_indices = algorithms['affinity'].cluster_centers_indices_
n_clusters_ = len(cluster_centers_indices)
print(n_clusters_)
# 8


   since we were not able to define the number of clusters for affinity
   propagation, it determined that there were eight clusters. our metrics
   didn't hint at this problem since our scoring wasn't based off of
   id91 accuracy.

   let's wrap it up but taking a look at what our affinity algorithm
   actually clustered:
# the resulting clubs
algorithms['affinity'].labels_

# array([0, 2, 2, 2, 1, 1, 1, 2, 4, 3, 1, 1, 2, 2, 4, 4, 1, 2, 4, 4, 4, 2, 4, 5,
 5, 6, 3, 3, 6, 4, 5, 7, 7], dtype=int64)

draw_communities(g, algorithms['affinity'].labels_, pos)


   result:
   affinity id91 of karate club dataset

final words

   thanks for joining us in this id91 introduction. i hope you found
   some value in seeing how we can easily manipulate a public dataset and
   apply and compare several different id91 algorithms using sklearn
   in python.

   if you have any questions, definitely let us know in the comments
   below. also, feel free to attach a id91 project you've
   experimented with!

continue learning

   [34]machine learning with python     coursera

   week four of this course has several great lectures about id91 to
   give you a more in-depth understanding.
     __________________________________________________________________

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe
     __________________________________________________________________

meet the authors

   data scientist author photo
   nikos koufos cs & engineering post graduate

   learndatasci author, postgraduate in computer science & engineering at
   the university ioannina, greece, and computer science undergraduate
   teaching assistant.

   brendan martin
   brendan martin founder of learndatasci

   author and editor at learndatasci. python development and data science
   consultant.
     __________________________________________________________________

   [35]back to blog index

   please enable javascript to view the [36]comments powered by disqus.

     * flattened-logo-ready-for-export

     * [37]best udemy data science courses
     * [38]contact

     * [39]100+ free data science books
     * [40]privacy policy

   copyright    2019 learndatasci. all rights reserved.

be notified when we release new material

   join over 3,500 data science enthusiasts.
   ____________________
   ____________________
   subscribe

references

   visible links
   1. https://www.learndatasci.com/tutorials/id116-id91-algorithms-python-intro/
   2. https://www.learndatasci.com/tutorials/
   3. https://www.learndatasci.com/articles/
   4. https://www.learndatasci.com/books/
   5. https://www.learndatasci.com/tutorials/id116-id91-algorithms-python-intro/
   6. https://www.learndatasci.com/best-data-science-online-courses/
   7. https://www.learndatasci.com/best-machine-learning-courses/
   8. https://www.learndatasci.com/publish/
   9. https://facebook.com/learndatasci
  10. http://twitter.com/learndatasci
  11. https://github.com/learndatasci
  12. https://www.learndatasci.com/out/coursera-machine-learning/
  13. https://www.learndatasci.com/out/coursera-data-science-specialization/
  14. https://www.learndatasci.com/out/coursera-applied-data-science-with-python/
  15. https://www.learndatasci.com/articles/review-dataquests-data-science-python-program/
  16. https://www.learndatasci.com/out/amazon-introduction-statistical-learning/
  17. https://www.learndatasci.com/out/data-science-from-scratch/
  18. https://www.learndatasci.com/out/agile-data-science-20/
  19. https://www.learndatasci.com/best-machine-learning-courses/
  20. https://www.learndatasci.com/best-data-science-online-courses/
  21. https://www.learndatasci.com/tutorials/using-databases-python-postgres-sqlalchemy-and-alembic/
  22. https://www.learndatasci.com/articles/data-science-machine-learning-books-masters/
  23. https://www.learndatasci.com/tutorials/id116-id91-algorithms-python-intro/
  24. https://www.learndatasci.com/tutorials/id116-id91-algorithms-python-intro/
  25. https://www.learndatasci.com/tutorials/t/unsupervised-learning
  26. https://www.learndatasci.com/tutorials/t/algorithms
  27. https://www.learndatasci.com/tutorials/t/id91_1
  28. https://www.learndatasci.com/out/datacamp/
  29. https://github.com/learndatasci/article-resources/tree/master/id116 id91
  30. https://github.com/learndatasci/article-resources/blob/master/karate club id91/zachary1977.pdf
  31. https://github.com/nitoyon/tech.nitoyon.com
  32. https://www.wikiwand.com/en/hierarchical_id91
  33. https://en.wikipedia.org/wiki/laplacian_matrix
  34. https://www.learndatasci.com/out/coursera-ibm-machine-learning-python/
  35. https://www.learndatasci.com/tutorials/
  36. https://disqus.com/?ref_noscript
  37. https://www.learndatasci.com/top-udemy-data-science-courses
  38. https://www.learndatasci.com/contact-us
  39. https://www.learndatasci.com/free-data-science-books
  40. https://www.learndatasci.com/privacy-policy

   hidden links:
  42. https://www.learndatasci.com/
  43. https://www.linkedin.com/sharearticle?mini=true&url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fid116-id91-algorithms-python-intro%2f&summary=id116%20&%20other%20id91%20algorithms:%20a%20quick%20intro%20with%20python
  44. http://twitter.com/share?text=id116%20&%20other%20id91%20algorithms:%20a%20quick%20intro%20with%20python&url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fid116-id91-algorithms-python-intro%2f
  45. https://www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fid116-id91-algorithms-python-intro%2f
  46. http://www.reddit.com/submit?url=https%3a%2f%2fwww.learndatasci.com%2ftutorials%2fid116-id91-algorithms-python-intro%2f&title=id116%20&%20other%20id91%20algorithms:%20a%20quick%20intro%20with%20python
  47. https://news.ycombinator.com/submitlink?uhttps%3a%2f%2fwww.learndatasci.com%2ftutorials%2fid116-id91-algorithms-python-intro%2f&t=learndatasci
