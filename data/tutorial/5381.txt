   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]machine learning for humans
   [7]machine learning for humans
   [8]sign in[9]get started
     __________________________________________________________________

machine learning for humans, part 2.1: supervised learning

the two tasks of supervised learning: regression and classification. linear
regression, id168s, and id119.

   [10]go to the profile of vishal maini
   [11]vishal maini (button) blockedunblock (button) followfollowing
   aug 19, 2017
this series is available as a full-length e-book! [12]download here. free for do
wnload, contributions appreciated ([13]paypal.me/ml4h)

   how much money will we make by spending more dollars on digital
   advertising? will this loan applicant pay back the loan or not? what   s
   going to happen to the stock market tomorrow?

   in supervised learning problems, we start with a data set containing
   training examples with associated correct labels. for example, when
   learning to classify handwritten digits, a supervised learning
   algorithm takes thousands of pictures of handwritten digits along with
   labels containing the correct number each image represents. the
   algorithm will then learn the relationship between the images and their
   associated numbers, and apply that learned relationship to classify
   completely new images (without labels) that the machine hasn   t seen
   before. this is how you   re able to deposit a check by taking a picture
   with your phone!

   to illustrate how supervised learning works, let   s examine the problem
   of predicting annual income based on the number of years of higher
   education someone has completed. expressed more formally, we   d like to
   build a model that approximates the relationship f between the number
   of years of higher education x and corresponding annual income y.
   [1*dbkmjnmkgxfv-oqjwjtfdw.png]
x (input) = years of higher education
y (output) = annual income
f = function describing the relationship between x and y
   (epsilon) = random error term (positive or negative) with mean zero
regarding epsilon:
(1)    represents irreducible error in the model, which is a theoretical limit ar
ound the performance of your algorithm due to inherent noise in the phenomena yo
u are trying to explain. for example, imagine building a model to predict the ou
tcome of a coin flip.
(2) incidentally, mathematician [14]paul erd  s referred to children as    epsilons
    because in calculus (but not in stats!)    denotes an arbitrarily small positiv
e quantity. fitting, no?

   one method for predicting income would be to create a rigid rules-based
   model for how income and education are related. for example:    i   d
   estimate that for every additional year of higher education, annual
   income increases by $5,000.   
income = ($5,000 * years_of_education) + baseline_income
this approach is an example of engineering a solution (vs. learning a solution,
as with the id75 method described below).

   you could come up with a more complex model by including some rules
   about degree type, years of work experience, school tiers, etc. for
   example:    if they completed a bachelor   s degree or higher, give the
   income estimate a 1.5x multiplier.   

   but this kind of explicit rules-based programming doesn   t work well
   with complex data. imagine trying to design an image classification
   algorithm made of if-then statements describing the combinations of
   pixel brightnesses that should be labeled    cat    or    not cat   .

   supervised machine learning solves this problem by getting the computer
   to do the work for you. by identifying patterns in the data, the
   machine is able to form heuristics. the primary difference between this
   and human learning is that machine learning runs on computer hardware
   and is best understood through the lens of computer science and
   statistics, whereas human pattern-matching happens in a biological
   brain (while accomplishing the same goals).

   in supervised learning, the machine attempts to learn the relationship
   between income and education from scratch, by running labeled training
   data through a learning algorithm. this learned function can be used to
   estimate the income of people whose income y is unknown, as long as we
   have years of education x as inputs. in other words, we can apply our
   model to the unlabeled test data to estimate y.

   the goal of supervised learning is to predict y as accurately as
   possible when given new examples where x is known and y is unknown. in
   what follows we   ll explore several of the most common approaches to
   doing so.

the two tasks of supervised learning: regression and classification

regression: predict a continuous numerical value. how much will that house sell
for?
classification: assign a label. is this a picture of a cat or a dog?

   the rest of this section will focus on regression. in [15]part 2.2
   we   ll dive deeper into classification methods.

regression: predicting a continuous value

   regression predicts a continuous target variable y. it allows you to
   estimate a value, such as housing prices or human lifespan, based on
   input data x.

   here, target variable means the unknown variable we care about
   predicting, and continuous means there aren   t gaps (discontinuities) in
   the value that y can take on. a person   s weight and height are
   continuous values. discrete variables, on the other hand, can only take
   on a finite number of values         for example, the number of kids somebody
   has is a discrete variable.

   predicting income is a classic regression problem. your input data x
   includes all relevant information about individuals in the data set
   that can be used to predict income, such as years of education, years
   of work experience, job title, or zip code. these attributes are called
   features, which can be numerical (e.g. years of work experience) or
   categorical (e.g. job title or field of study).

   you   ll want as many training observations as possible relating these
   features to the target output y, so that your model can learn the
   relationship f between x and y.

   the data is split into a training data set and a test data set. the
   training set has labels, so your model can learn from these labeled
   examples. the test set does not have labels, i.e. you don   t yet know
   the value you   re trying to predict. it   s important that your model can
   generalize to situations it hasn   t encountered before so that it can
   perform well on the test data.
regression
y = f(x) +   , where x = (x1, x2   xn)
training: machine learns f from labeled training data
test: machine predicts y from unlabeled testing data
note that x can be a tensor with an any number of dimensions. a 1d tensor is a v
ector (1 row, many columns), a 2d tensor is a matrix (many rows, many columns),
and then you can have tensors with 3, 4, 5 or more dimensions (e.g. a 3d tensor
with rows, columns, and depth). for a review of these terms, see the first few p
ages of this [16]id202 review.

   in our trivially simple 2d example, this could take the form of a .csv
   file where each row contains a person   s education level and income. add
   more columns with more features and you   ll have a more complex, but
   possibly more accurate, model.
   [1*caljbwlrkbllqwflglbiuq.png]

so how do we solve these problems?

   how do we build models that make accurate, useful predictions in the
   real world? we do so by using supervised learning algorithms.

   now let   s get to the fun part: getting to know the algorithms. we   ll
   explore some of the ways to approach regression and classification and
   illustrate key machine learning concepts throughout.

id75 (ordinary least squares)

      draw the line. yes, this counts as machine learning.   

   first, we   ll focus on solving the income prediction problem with linear
   regression, since linear models don   t work well with image recognition
   tasks (this is the domain of deep learning, which we   ll explore later).

   we have our data set x, and corresponding target values y. the goal of
   ordinary least squares (ols) regression is to learn a linear model that
   we can use to predict a new y given a previously unseen x with as
   little error as possible. we want to guess how much income someone
   earns based on how many years of education they received.
x_train = [4, 5, 0, 2,    , 6] # years of post-secondary education
y_train = [80, 91.5, 42, 55,    , 100] # corresponding annual incomes, in thousand
s of dollars

   [0*2j891tbkau5elp_5.]

   id75 is a parametric method, which means it makes an
   assumption about the form of the function relating x and y (we   ll cover
   examples of non-parametric methods later). our model will be a function
   that predicts    given a specific x:
   [0*ta7lgj0wsw64d9_k.]
   in this case, we make the explicit assumption that there is a linear
   relationship between x and y         that is, for each one-unit increase in
   x, we see a constant increase (or decrease) in y.

     0 is the y-intercept and   1 is the slope of our line, i.e. how much
   income increases (or decreases) with one additional year of education.

   our goal is to learn the model parameters (in this case,   0 and   1)
   that minimize error in the model   s predictions.

   to find the best parameters:

     1. define a cost function, or id168, that measures how
     inaccurate our model   s predictions are.

     2. find the parameters that minimize loss, i.e. make our model as
     accurate as possible.

   graphically, in two dimensions, this results in a line of best fit. in
   three dimensions, we would draw a plane, and so on with
   higher-dimensional hyperplanes.
a note on dimensionality: our example is two-dimensional for simplicity, but you
   ll typically have more features (x   s) and coefficients (betas) in your model, e
.g. when adding more relevant variables to improve the accuracy of your model pr
edictions. the same principles generalize to higher dimensions, though things ge
t much harder to visualize beyond three dimensions.

   [0*fg6i9juut5x2vp3r.]

   mathematically, we look at the difference between each real data point
   (y) and our model   s prediction (  ). square these differences to avoid
   negative numbers and penalize larger differences, and then add them up
   and take the average. this is a measure of how well our data fits the
   line.
   [0*4yosvq8ogbg6zawv.]
   n = # of observations. using 2*n instead of n makes the math work out
   more cleanly when taking the derivative to minimize loss, though some
   stats people say this is blasphemy. when you start having opinions on
   this kind of stuff, you   ll know you are all the way in the rabbit hole.

   for a simple problem like this, we can compute a closed form solution
   using calculus to find the optimal beta parameters that minimize our
   id168. but as a cost function grows in complexity, finding a
   closed form solution with calculus is no longer feasible. this is the
   motivation for an iterative approach called id119, which
   allows us to minimize a complex id168.

id119: learn the parameters

      put on a blindfold, take a step downhill. you   ve found the bottom when
   you have nowhere to go but up.   

   id119 will come up over and over again, especially in neural
   networks. machine learning libraries like [17]scikit-learn and
   [18]tensorflow use it in the background everywhere, so it   s worth
   understanding the details.

   the goal of id119 is to find the minimum of our model   s loss
   function by iteratively getting a better and better approximation of
   it.

   imagine yourself walking through a valley with a blindfold on. your
   goal is to find the bottom of the valley. how would you do it?

   a reasonable approach would be to touch the ground around you and move
   in whichever direction the ground is sloping down most steeply. take a
   step and repeat the same process continually until the ground is flat.
   then you know you   ve reached the bottom of a valley; if you move in any
   direction from where you are, you   ll end up at the same elevation or
   further uphill.

   going back to mathematics, the ground becomes our id168, and
   the elevation at the bottom of the valley is the minimum of that
   function.

   let   s take a look at the id168 we saw in regression:
   [0*xnjj4yfpef08f-1l.]

   we see that this is really a function of two variables:   0 and   1. all
   the rest of the variables are determined, since x, y, and n are given
   during training. we want to try to minimize this function.
   [0*zaekarnxngb7-h3f.]

   the function is f(  0,  1)=z. to begin id119, you make some
   guess of the parameters   0 and   1 that minimize the function.

   next, you find the [19]partial derivatives of the id168 with
   respect to each beta parameter: [dz/d  0, dz/d  1]. a partial derivative
   indicates how much total loss is increased or decreased if you increase
     0 or   1 by a very small amount.

   put another way, how much would increasing your estimate of annual
   income assuming zero higher education (  0) increase the loss (i.e.
   inaccuracy) of your model? you want to go in the opposite direction so
   that you end up walking downhill and minimizing loss.

   similarly, if you increase your estimate of how much each incremental
   year of education affects income (  1), how much does this increase loss
   (z)? if the partial derivative dz/  1 is a negative number, then
   increasing   1 is good because it will reduce total loss. if it   s a
   positive number, you want to decrease   1. if it   s zero, don   t change   1
   because it means you   ve reached an optimum.

   keep doing that until you reach the bottom, i.e. the algorithm
   converged and loss has been minimized. there are lots of tricks and
   exceptional cases beyond the scope of this series, but generally, this
   is how you find the optimal parameters for your parametric model.

overfitting

   overfitting:    sherlock, your explanation of what just happened is too
   specific to the situation.    id173:    don   t overcomplicate
   things, sherlock. i   ll punch you for every extra word.    hyperparameter
   (  ):    here   s the strength with which i will punch you for every extra
   word.   

   a common problem in machine learning is overfitting: learning a
   function that perfectly explains the training data that the model
   learned from, but doesn   t generalize well to unseen test data.
   overfitting happens when a model overlearns from the training data to
   the point that it starts picking up idiosyncrasies that aren   t
   representative of patterns in the real world. this becomes especially
   problematic as you make your model increasingly complex. underfitting
   is a related issue where your model is not complex enough to capture
   the underlying trend in the data.
id160
bias is the amount of error introduced by approximating real-world phenomena wit
h a simplified model.
variance is how much your model's test error changes based on variation in the t
raining data. it reflects the model's sensitivity to the idiosyncrasies of the d
ata set it was trained on.
as a model increases in complexity and it becomes more wiggly (flexible), its bi
as decreases (it does a good job of explaining the training data), but variance
increases (it doesn't generalize as well). ultimately, in order to have a good m
odel, you need one with low bias and low variance.

   [1*lb7leh2ob5pajltnaygsba.png]
   source: coursera   s [20]ml course, taught by andrew ng

   remember that the only thing we care about is how the model performs on
   test data. you want to predict which emails will be marked as spam
   before they   re marked, not just build a model that is 100% accurate at
   reclassifying the emails it used to build itself in the first place.
   hindsight is 20/20         the real question is whether the lessons learned
   will help in the future.

   the model on the right has zero loss for the training data because it
   perfectly fits every data point. but the lesson doesn   t generalize. it
   would do a horrible job at explaining a new data point that isn   t yet
   on the line.

   two ways to combat overfitting:

   1. use more training data. the more you have, the harder it is to
   overfit the data by learning too much from any single training example.

   2. use id173. add in a penalty in the id168 for
   building a model that assigns too much explanatory power to any one
   feature or allows too many features to be taken into account.
   [1*rft6mtu45dit0ojhlgdcbg.png]

   the first piece of the sum above is our normal cost function. the
   second piece is a id173 term that adds a penalty for large
   beta coefficients that give too much explanatory power to any specific
   feature. with these two elements in place, the cost function now
   balances between two priorities: explaining the training data and
   preventing that explanation from becoming overly specific.

   the lambda coefficient of the id173 term in the cost function
   is a hyperparameter: a general setting of your model that can be
   increased or decreased (i.e. tuned) in order to improve performance. a
   higher lambda value will more harshly penalize large beta coefficients
   that could lead to potential overfitting. to decide the best value of
   lambda, you   d use a method called cross-validation which involves
   holding out a portion of the training data during training, and then
   seeing how well your model explains the held-out portion. we   ll go over
   this in more depth

woo! we made it.

   here   s what we covered in this section:
     * how supervised machine learning enables computers to learn from
       labeled training data without being explicitly programmed
     * the tasks of supervised learning: regression and classification
     * id75, a bread-and-butter parametric algorithm
     * learning parameters with id119
     * overfitting and id173

   in the next section         [21]part 2.2: supervised learning ii         we   ll talk
   about two foundational methods of classification: id28
   and support vector machines.

practice materials & further reading

2.1a         id75

   for a more thorough treatment of id75, read chapters 1   3
   of [22]an introduction to statistical learning. the book is available
   for free online and is an excellent resource for understanding machine
   learning concepts with accompanying exercises.

   for more practice:
     * play with the [23]boston housing dataset. you can either use
       software with nice guis like minitab and excel or do it the hard
       (but more rewarding) way with [24]python or [25]r.
     * try your hand at a [26]kaggle challenge, e.g. [27]housing price
       prediction, and see how others approached the problem after
       attempting it yourself.

2.1b         implementing id119

   to actually implement id119 in python, check out [28]this
   tutorial. and [29]here is a more mathematically rigorous description of
   the same concepts.

   in practice, you   ll rarely need to implement id119 from
   scratch, but understanding how it works behind the scenes will allow
   you to use it more effectively and understand why things break when
   they do.
     __________________________________________________________________

enter your email below if you   d like to stay up-to-date with future content     

   iframe: [30]/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=740383a2feab

on twitter? so are we. feel free to keep in touch         [31]vishal and
[32]samer         
     __________________________________________________________________

   more from machine learning for humans         
     * [33]part 1: why machine learning matters    
     * part 2.1: supervised learning    
     * [34]part 2.2: supervised learning ii
     * [35]part 2.3: supervised learning iii
     * [36]part 3: unsupervised learning
     * [37]part 4: neural networks & deep learning
     * [38]part 5: id23
     * [39]appendix: the best machine learning resources

   thanks to [40]sachin maini and [41]sachin maini.
     * [42]machine learning
     * [43]deep learning
     * [44]supervised learning
     * [45]tech
     * [46]artificial intelligence

   (button)
   (button)
   (button) 10.9k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of vishal maini

[48]vishal maini

   strategy & communications [49]@deepmindai. previously [50]@upstart,
   [51]@yale, [52]@trueventurestec.

     (button) follow
   [53]machine learning for humans

[54]machine learning for humans

   demystifying artificial intelligence & machine learning. discussions on
   safe and intentional application of ai for positive social impact.

     * (button)
       (button) 10.9k
     * (button)
     *
     *

   [55]machine learning for humans
   never miss a story from machine learning for humans, when you sign up
   for medium. [56]learn more
   never miss a story from machine learning for humans
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/740383a2feab
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/machine-learning-for-humans?source=avatar-lo_rk1rzt8wlzgc-e8dd9a6c82a5
   7. https://medium.com/machine-learning-for-humans?source=logo-lo_rk1rzt8wlzgc---e8dd9a6c82a5
   8. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@v_maini?source=post_header_lockup
  11. https://medium.com/@v_maini
  12. https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0
  13. http://paypal.me/ml4h
  14. https://en.wikipedia.org/wiki/paul_erd  s
  15. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  16. http://www.deeplearningbook.org/contents/linear_algebra.html
  17. http://scikit-learn.org/stable/
  18. https://www.tensorflow.org/
  19. https://en.wikipedia.org/wiki/partial_derivative
  20. https://medium.com/@v_maini/machine-learning-for-humans-part-1-why-machine-learning-matters-965dfadf19d4
  21. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  22. http://www-bcf.usc.edu/~gareth/isl/index.html
  23. http://www.cs.toronto.edu/~delve/data/boston/bostondetail.html
  24. https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef
  25. https://datascienceplus.com/linear-regression-from-scratch-in-r/
  26. http://www.kaggle.com/
  27. https://www.kaggle.com/c/house-prices-advanced-regression-techniques
  28. https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
  29. http://eli.thegreenplace.net/2016/understanding-gradient-descent/
  30. https://medium.com/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=740383a2feab
  31. https://twitter.com/v_maini
  32. https://twitter.com/seriousssam
  33. https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
  34. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  35. https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930
  36. https://medium.com/@v_maini/unsupervised-learning-f45587588294
  37. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  38. https://medium.com/@v_maini/reinforcement-learning-6eacf258b265
  39. https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1
  40. https://medium.com/@smaini11?source=post_page
  41. https://medium.com/@sachinmaini?source=post_page
  42. https://medium.com/tag/machine-learning?source=post
  43. https://medium.com/tag/deep-learning?source=post
  44. https://medium.com/tag/supervised-learning?source=post
  45. https://medium.com/tag/tech?source=post
  46. https://medium.com/tag/artificial-intelligence?source=post
  47. https://medium.com/@v_maini?source=footer_card
  48. https://medium.com/@v_maini
  49. http://twitter.com/deepmindai
  50. http://twitter.com/upstart
  51. http://twitter.com/yale
  52. http://twitter.com/trueventurestec
  53. https://medium.com/machine-learning-for-humans?source=footer_card
  54. https://medium.com/machine-learning-for-humans?source=footer_card
  55. https://medium.com/machine-learning-for-humans
  56. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  58. https://medium.com/p/740383a2feab/share/twitter
  59. https://medium.com/p/740383a2feab/share/facebook
  60. https://medium.com/p/740383a2feab/share/twitter
  61. https://medium.com/p/740383a2feab/share/facebook
