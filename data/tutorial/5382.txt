   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]machine learning for humans
   [7]machine learning for humans
   [8]sign in[9]get started
     __________________________________________________________________

machine learning for humans, part 2.2: supervised learning ii

classification with id28 and support vector machines (id166s).

   [10]go to the profile of vishal maini
   [11]vishal maini (button) blockedunblock (button) followfollowing
   aug 19, 2017
this series is available as a full-length e-book! [12]download here. free for do
wnload, contributions appreciated ([13]paypal.me/ml4h)

classification: predicting a label

   is this email spam or not? is that borrower going to repay their loan?
   will those users click on the ad or not? who is that person in your
   facebook picture?

   classification predicts a discrete target label y. classification is
   the problem of assigning new observations to the class to which they
   most likely belong, based on a classification model built from labeled
   training data.

   the accuracy of your classifications will depend on the effectiveness
   of the algorithm you choose, how you apply it, and how much useful
   training data you have.
   [1*m-5haoh9r0o775rw6jwuqw.png]

id28: 0 or 1?

   id28 is a method of classification: the model outputs
   the id203 of a categorical target variable y belonging to a
   certain class.
a good example of classification is determining whether a loan application is fr
audulent.
ultimately, the lender wants to know whether they should give the borrower a loa
n or not, and they have some tolerance for risk that the application is in fact
fraudulent. in this case, the goal of id28 is to calculate the pr
obability (between 0% and 100%) that the application is fraud. with these probab
ilities, we can set some threshold above which we   re willing to lend to the borr
ower, and below which we deny their loan application or flag the application for
 further review.

   though id28 is often used for binary classification
   where there are two classes, keep in mind that classification can
   performed with any number of categories (e.g. when assigning
   handwritten digits a label between 0 and 9, or using facial recognition
   to detect which friends are in a facebook picture).

   can i just use ordinary least squares?

   nope. if you trained a id75 model on a bunch of examples
   where y = 0 or 1, you might end up predicting some probabilities that
   are less than 0 or greater than 1, which doesn   t make sense. instead,
   we   ll use a id28 model (or logit model) which was
   designed for assigning a id203 between 0% and 100% that y belongs
   to a certain class.

   how does the math work?

   note: the math in this section is interesting but might be on the more
   technical side. feel free to skim through it if you   re more interested
   in the high-level concepts.

   the logit model is a modification of id75 that makes sure
   to output a id203 between 0 and 1 by applying the sigmoid
   function, which, when graphed, looks like the characteristic s-shaped
   curve that you   ll see a bit later.
   [1*akcgcgbljpjrmakfb4efta.png]
   sigmoid function, which squashes values between 0 and 1.

   recall the original form of our simple id75 model, which
   we   ll now call g(x) since we   re going to use it within a compound
   function:
   [1*0zbuxdoyvcorgye3vltg5w.png]

   now, to solve this issue of getting model outputs less than 0 or
   greater than 1, we   re going to define a new function f(g(x)) that
   transforms g(x) by squashing the output of id75 to a value
   in the [0,1] range. can you think of a function that does this?

   are you thinking of the sigmoid function? bam! presto! you   re correct.

   so we plug g(x) into the sigmoid function above, resulting in a
   function of our original function (yes, things are getting meta) that
   outputs a id203 between 0 and 1:
   [1*mofhmrlr7ozvxmw7trgqcg.png]
   in other words, we   re calculating the id203 that the training
   example belongs to a certain class: p(y=1).

   here we   ve isolated p, the id203 that y=1, on the left side of
   the equation. if we want to solve for a nice clean   0 +   1x +    on the
   right side so we can straightforwardly interpret the beta coefficients
   we   re going to learn, we   d instead end up with the log-odds ratio, or
   logit, on the left side         hence the name    logit model   :
   [1*aetaom7gzehnafjzi4gl9a.png]

   the log-odds ratio is simply the natural log of the odds ratio,
   p/(1-p), which crops up in everyday conversations:

        yo, what do you think are the odds that tyrion lannister dies in
     this season of game of thrones?   

        id48. it   s definitely 2x more likely to happen than not. 2-to-1
     odds. sure, he might seem too important to be killed, but we all saw
     what they did to ned stark      

   [1*4lhjwipvybragei95gx7qg.png]
   note that in the logit model,   1 now represents the rate of change in
   the log-odds ratio as x changes. in other words, it   s the    slope of
   log-odds   , not the    slope of the id203   .

   log-odds might be slightly unintuitive but it   s worth understanding
   since it will come up again when you   re interpreting the output of
   neural networks performing classification tasks.

   using the output of a id28 model to make decisions

   the output of the id28 model from above looks like an
   s-curve showing p(y=1) based on the value of x:
   [0*xblfyr2kknxajfqp.]
   source: [14]wikipedia

   to predict the y label         spam/not spam, cancer/not cancer, fraud/not
   fraud, etc.         you have to set a id203 cutoff, or threshold, for a
   positive result. for example:    if our model thinks the id203 of
   this email being spam is higher than 70%, label it spam. otherwise,
   don   t.   

   the threshold depends on your tolerance for false positives vs. false
   negatives. if you   re diagnosing cancer, you   d have a very low tolerance
   for false negatives, because even if there   s a very small chance the
   patient has cancer, you   d want to run further tests to make sure. so
   you   d set a very low threshold for a positive result.

   in the case of fraudulent loan applications, on the other hand, the
   tolerance for false positives might be higher, particularly for smaller
   loans, since further vetting is costly and a small loan may not be
   worth the additional operational costs and friction for non-fraudulent
   applicants who are flagged for further processing.

minimizing loss with id28

   as in the case of id75, we use id119 to learn
   the beta parameters that minimize loss.

   in id28, the cost function is basically a measure of how
   often you predicted 1 when the true answer was 0, or vice versa. below
   is a regularized cost function just like the one we went over for
   id75.
   [1*tn4b-rymcb45yseizm5ktg.png]

   don   t panic when you see a long equation like this! break it into
   chunks and think about what   s going on in each part conceptually. then
   the specifics will start to make sense.

   the first chunk is the data loss, i.e. how much discrepancy there is
   between the model   s predictions and reality. the second chunk is the
   id173 loss, i.e. how much we penalize the model for having
   large parameters that heavily weight certain features (remember, this
   prevents overfitting).

   we   ll minimize this cost function with id119, as above, and
   voil  ! we   ve built a id28 model to make class
   predictions as accurately as possible.

support vector machines (id166s)

      we   re in a room full of marbles again. why are we always in a room
   full of marbles? i could   ve sworn we already lost them.   

   id166 is the last parametric model we   ll cover. it typically solves the
   same problem as id28         classification with two
   classes         and yields similar performance. it   s worth understanding
   because the algorithm is geometrically motivated in nature, rather than
   being driven by probabilistic thinking.

   a few examples of the problems id166s can solve:
     * is this an image of a cat or a dog?
     * is this review positive or negative?
     * are the dots in the 2d plane red or blue?

   we   ll use the third example to illustrate how id166s work. problems like
   these are called toy problems because they   re not real         but nothing is
   real, so it   s fine.
   [0*8irupib6xjkosihp.]

   in this example, we have points in a 2d space that are either red or
   blue, and we   d like to cleanly separate the two.

   the training set is plotted the graph above. we would like to classify
   new, unclassified points in this plane. to do this, id166s use a
   separating line (or, in more than two dimensions, a multi-dimensional
   hyperplane) to split the space into a red zone and a blue zone. you can
   already imagine how a separating line might look in the graph above.

   how, specifically, do we choose where to draw the line?

   below are two examples of such a line:
   [0*_j_t8bqq8g46gj6n.]
   these charts were made with microsoft paint, which was [15]deprecated a
   few weeks ago after 32 wonderful years. rip paint :(
   [0*uqvkhdvtlxy9lzg6.]

   hopefully, you share the intuition that the first line is superior. the
   distance to the nearest point on either side of the line is called the
   margin, and id166 tries to maximize the margin. you can think about it
   like a safety space: the bigger that space, the less likely that noisy
   points get misclassified.

   based on this short explanation, a few big questions come up.

   1. how does the math behind this work?

   we want to find the optimal hyperplane (a line, in our 2d example).
   this hyperplane needs to (1) separate the data cleanly, with blue
   points on one side of the line and red points on the other side, and
   (2) maximize the margin. this is an optimization problem. the solution
   has to respect constraint (1) while maximizing the margin as is
   required in (2).

   the human version of solving this problem would be to take a ruler and
   keep trying different lines separating all the points until you get the
   one that maximizes the margin.

   it turns out there   s a clean mathematical way to do this maximization,
   but the specifics are beyond our scope. to explore it further, here   s a
   [16]video lecture that shows how it works using [17]lagrangian
   optimization.

   the solution hyperplane you end up with is defined in relation to its
   position with respect to certain x_i   s, which are called the support
   vectors, and they   re usually the ones closest to the hyperplane.

   2. what happens if you can   t separate the data cleanly?

   there are two methods for dealing with this problem.

   2.1. soften the definition of    separate   .

   we allow a few mistakes, meaning we allow some blue points in the red
   zone or some red points in the blue zone. we do that by adding a cost c
   for misclassified examples in our id168. basically, we say it   s
   acceptable but costly to misclassify a point.

   2.2. throw the data into higher dimensions.

   we can create nonlinear classifiers by increasing the number of
   dimensions, i.e. include x  , x  , even cos(x), etc. suddenly, you have
   boundaries that can look more squiggly when we bring them back to the
   lower dimensional representation.

   intuitively, this is like having red and blue marbles lying on the
   ground such that they can   t be cleanly separated by a line         but if you
   could make all the red marbles levitate off the ground in just the
   right way, you could draw a plane separating them. then you let them
   fall back to the ground knowing where the blues stop and reds begin.
   [0*ln9m5l2yuae57r_t.png]
   a nonseparable dataset in a two-dimensional space r  , and the same
   dataset mapped onto threedimensions with the third dimension being
   x  +y   (source:
   [18]http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)
   [0*sye5ojcsfe1mwdqc.png]
   the decision boundary is shown in green, first in the three-dimensional
   space (left), then back in the two-dimensional space (right). same
   source as previous image.

   in summary, id166s are used for classification with two classes. they
   attempt to find a plane that separates the two classes cleanly. when
   this isn   t possible, we either soften the definition of    separate,    or
   we throw the data into higher dimensions so that we can cleanly
   separate the data.

success!

   in this section we covered:
     * the classification task of supervised learning
     * two foundational classification methods: id28 and
       support vector machines (id166s)
     * recurring concepts: the sigmoid function, log-odds (   logit   ), and
       false positives vs. false negatives,

   in [19]part 2.3: supervised learning iii, we   ll go into non-parametric
   supervised learning, where the ideas behind the algorithms are very
   intuitive and performance is excellent for certain kinds of problems,
   but the models can be harder to interpret.

practice materials & further reading

2.2a         id28

   data school has an excellent [20]in-depth guide to id28.
   we   ll also continue to refer you to [21]an introduction to statistical
   learning. see chapter 4 on id28, and chapter 9 on
   support vector machines.

   to implement id28, we recommend working on [22]this
   problem set. you have to register on the site to work through it,
   unfortunately. c   est la vie.

2.2b   down the id166 rabbit hole

   to dig into the math behind id166s, watch prof. patrick winston   s
   [23]lecture from mit 6.034: artificial intelligence. and check out
   [24]this tutorial to work through a python implementation.
     __________________________________________________________________

enter your email below if you   d like to stay up-to-date with future content     

   iframe: [25]/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=5c1c23f3560d

on twitter? so are we. feel free to keep in touch         [26]vishal and
[27]samer         
     __________________________________________________________________

   more from machine learning for humans         
     * [28]part 1: why machine learning matters    
     * [29]part 2.1: supervised learning    
     * part 2.2: supervised learning ii    
     * [30]part 2.3: supervised learning iii
     * [31]part 3: unsupervised learning
     * [32]part 4: neural networks & deep learning
     * [33]part 5: id23
     * [34]appendix: the best machine learning resources

   thanks to [35]sachin maini.
     * [36]machine learning
     * [37]artificial intelligence
     * [38]supervised learning
     * [39]tech
     * [40]statistics

   (button)
   (button)
   (button) 3.5k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of vishal maini

[42]vishal maini

   strategy & communications [43]@deepmindai. previously [44]@upstart,
   [45]@yale, [46]@trueventurestec.

     (button) follow
   [47]machine learning for humans

[48]machine learning for humans

   demystifying artificial intelligence & machine learning. discussions on
   safe and intentional application of ai for positive social impact.

     * (button)
       (button) 3.5k
     * (button)
     *
     *

   [49]machine learning for humans
   never miss a story from machine learning for humans, when you sign up
   for medium. [50]learn more
   never miss a story from machine learning for humans
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5c1c23f3560d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/machine-learning-for-humans?source=avatar-lo_ujoxcjkeujhi-e8dd9a6c82a5
   7. https://medium.com/machine-learning-for-humans?source=logo-lo_ujoxcjkeujhi---e8dd9a6c82a5
   8. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@v_maini?source=post_header_lockup
  11. https://medium.com/@v_maini
  12. https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0
  13. http://paypal.me/ml4h
  14. https://en.wikipedia.org/wiki/logistic_regression
  15. https://www.theguardian.com/technology/2017/jul/24/microsoft-paint-kill-off-after-32-years-graphics-editing-program
  16. https://www.youtube.com/watch?v=_pwhiwxhk8o
  17. https://en.wikipedia.org/wiki/lagrange_multiplier
  18. http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html
  19. https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930
  20. http://www.dataschool.io/guide-to-logistic-regression/
  21. http://www-bcf.usc.edu/~gareth/isl/
  22. https://datahack.analyticsvidhya.com/contest/practice-problem-1/
  23. https://www.youtube.com/watch?v=_pwhiwxhk8o
  24. https://pythonprogramming.net/id166-in-python-machine-learning-tutorial/
  25. https://medium.com/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=5c1c23f3560d
  26. https://twitter.com/v_maini
  27. https://twitter.com/seriousssam
  28. https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
  29. https://medium.com/@v_maini/supervised-learning-740383a2feab
  30. https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930
  31. https://medium.com/@v_maini/unsupervised-learning-f45587588294
  32. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  33. https://medium.com/@v_maini/reinforcement-learning-6eacf258b265
  34. https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1
  35. https://medium.com/@sachinmaini?source=post_page
  36. https://medium.com/tag/machine-learning?source=post
  37. https://medium.com/tag/artificial-intelligence?source=post
  38. https://medium.com/tag/supervised-learning?source=post
  39. https://medium.com/tag/tech?source=post
  40. https://medium.com/tag/statistics?source=post
  41. https://medium.com/@v_maini?source=footer_card
  42. https://medium.com/@v_maini
  43. http://twitter.com/deepmindai
  44. http://twitter.com/upstart
  45. http://twitter.com/yale
  46. http://twitter.com/trueventurestec
  47. https://medium.com/machine-learning-for-humans?source=footer_card
  48. https://medium.com/machine-learning-for-humans?source=footer_card
  49. https://medium.com/machine-learning-for-humans
  50. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  52. https://medium.com/p/5c1c23f3560d/share/twitter
  53. https://medium.com/p/5c1c23f3560d/share/facebook
  54. https://medium.com/p/5c1c23f3560d/share/twitter
  55. https://medium.com/p/5c1c23f3560d/share/facebook
