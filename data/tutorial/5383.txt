   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]machine learning for humans
   [7]machine learning for humans
   [8]sign in[9]get started
     __________________________________________________________________

machine learning for humans, part 2.3: supervised learning iii

non-parametric models: k-nearest neighbors, id90, and random
forests. introducing cross-validation, hyperparameter tuning, and
ensemble models.

   [10]go to the profile of vishal maini
   [11]vishal maini (button) blockedunblock (button) followfollowing
   aug 19, 2017
this series is available as a full-length e-book! [12]download here. free for do
wnload, contributions appreciated ([13]paypal.me/ml4h)

non-parametric learners.

   things are about to get a little    wiggly.

   in contrast to the methods we   ve covered so far         id75,
   id28, and id166s where the form of the model was
   pre-defined         non-parametric learners do not have a model structure
   specified a priori. we don   t speculate about the form of the function f
   that we   re trying to learn before training the model, as we did
   previously with id75. instead, the model structure is
   purely determined from the data.

   these models are more flexible to the shape of the training data, but
   this sometimes comes at the cost of interpretability. this will make
   more sense soon. let   s jump in.

k-nearest neighbors (id92)

      you are the average of your k closest friends.   

   id92 seems almost too simple to be a machine learning algorithm. the
   idea is to label a test data point x by finding the mean (or mode) of
   the k closest data points    labels.

   take a look at the image below. let   s say you want to figure out
   whether mysterious green circle is a red triangle or a blue square.
   what do you do?

   you could try to come up with a fancy equation that looks at where
   green circle lies on the coordinate plane below and makes a prediction
   accordingly. or, you could just look its three nearest neighbors, and
   guess that green circle is probably a red triangle. you could also
   expand the circle further and look at the five nearest neighbors, and
   make a prediction that way (3/5 of its five nearest neighbors are blue
   squares, so we   d guess that mysterious green circle is a blue square
   when k=5).
   [0*chgrjfn_y5s0qkso.]
   id92 illustration with k=1, 3, and 5. to classify the mysterious green
   circle (x) above, look at its single nearest neighbor, a    red
   triangle   . so, we   d guess that    =    red triangle   . with k=3, look at
   the 3 nearest neighbors: the mode of these is again    red triangle    so
     =    red triangle   . with k=5, we take the mode of the 5 nearest
   neighbors instead. now, notice that    becomes    blue square   . image from
   [14]wikipedia.

   that   s it. that   s k-nearest neighbors. you look at the k closest data
   points and take the average of their values if variables are continuous
   (like housing prices), or the mode if they   re categorical (like cat vs.
   dog).

   if you wanted to guess unknown house prices, you could just take the
   average of some number of geographically nearby houses, and you   d end
   up with some pretty nice guesses. these might even outperform a
   parametric regression model built by some economist that estimates
   model coefficients for # of beds/baths, nearby schools, distance to
   public transport, etc.
how to use id92 to predict housing prices:
1) store the training data, a matrix x of features like zip code, neighborhood,
# of bedrooms, square feet, distance from public transport, etc., and a matrix y
 of corresponding sale prices.
2) sort the houses in your training data set by similarity to the house in quest
ion, based on the features in x. we   ll define    similarity    below.
3) take the mean of the k closest houses. that is your guess at the sale price (
i.e.   )

   the fact that id92 doesn   t require a pre-defined parametric function
   f(x) relating y to x makes it well-suited for situations where the
   relationship is too complex to be expressed with a simple linear model.

distance metrics: defining and calculating    nearness   

   how do you calculate distance from the data point in question when
   finding the    nearest neighbors   ? how do you mathematically determine
   which of the blue squares and red triangles in the example above are
   closest to green circle, especially if you can   t just draw a nice 2d
   graph and eyeball it?

   the most straightforward measure is euclidean distance (a straight
   line,    as the crow flies   ). another is manhattan distance, like walking
   city blocks. you could imagine that manhattan distance is more useful
   in a model involving fare calculation for uber drivers, for example.
   [0*m00nblftussmsoem.]
   green line = euclidean distance. blue line = manhattan distance.
   source: [15]wikipedia

   remember the pythagorean theorem for finding the length of the
   hypotenuse of a right triangle?
   [1*my-jaqc6a1ii6ekwa7lklg.png]
   c = length of hypotenuse (green line above). a and b = length of the
   other sides, at a right angle (red lines above).

   solving in terms of c, we find the length of the hypotenuse by taking
   the square root of the sum of squared lengths of a and b, where a and b
   are orthogonal sides of the triangle (i.e. they are at a 90-degree
   angle from one another, going in perpendicular directions in space).
   [1*hyudqr79zfdke-ybh8vneg.png]

   this idea of finding the length of the hypotenuse given vectors in two
   orthogonal directions generalizes to many dimensions, and this is how
   we derive the formula for euclidean distance d(p,q) between points p
   and q in n-dimensional space:
   [1*tievrfp_qr7-5tsih7l0aa.png]
   formula for euclidean distance, derived from the pythagorean theorem.

   with this formula, you can calculate the nearness of all the training
   data points to the data point you   re trying to label, and take the
   mean/mode of the k nearest neighbors to make your prediction.

   typically you won   t need to calculate any distance metrics by hand         a
   quick google search reveals pre-built functions in numpy or scipy that
   will do this for you, e.g.euclidean_dist = numpy.linalg.norm(p-q)    but
   it   s fun to see how geometry concepts from eighth grade end up being
   helpful for building ml models today!

choosing k: tuning hyperparameters with cross-validation

   to decide which value of k to use, you can test different id92 models
   using different values of k with cross-validation:
    1. split your training data into segments, and train your model on all
       but one of the segments; use the held-out segment as the    test   
       data.
    2. see how your model performs by comparing your model   s predictions
       (  ) to the actual values of the test data (y).
    3. pick whichever yields the lowest error, on average, across all
       iterations.

   [0*djxkabpzfuyctka8.]
   cross-validation illustrated. the number of splits and iterations can
   be varied.

   higher k prevents overfitting

   higher values of k help address overfitting, but if the value of k is
   too high your model will be very biased and inflexible. to take an
   extreme example: if k = n (the total number of data points), the model
   would just dumbly blanket-classify all the test data as the mean or
   mode of the training data.

   if the single most common animal in a data set of animals is a scottish
   fold kitten, id92 with k set to n (the # of training observations)
   would then predict that every other animal in the world is also a
   scottish fold kitten. which, in vishal   s opinion, would be awesome.
   samer disagrees.
   [1*xcdqlyz7ce4f8hhuyr798q.gif]
   completely gratuitous scottish fold .gif. we   ll call it a study
   break.     

where to use id92 in the real world

   some examples of where you can use id92:
     * classification: fraud detection. the model can update virtually
       instantly with new training examples since you   re just storing more
       data points, which allows quick adaptation to new methods of fraud.
     * regression: predicting housing prices. in housing price prediction,
       literally being a    near neighbor    is actually a good indicator of
       being similar in price. id92 is useful in domains where physical
       proximity matters.
     * imputing missing training data. if one of the columns in your .csv
       has lots of missing values, you can impute the data by taking the
       mean or mode. id92 could give you a somewhat more accurate guess at
       each missing value.

id90, id79s

   making a good decision tree is like playing a game of    20 questions   .
   [0*pz0pgqt3i-cfpyaf.]
   the decision tree on the right describes survival patterns on
   the titanic.

   the first split at the root of a decision tree should be like the first
   question you should ask in 20 questions: you want to separate the data
   as cleanly as possible, thereby maximizing information gain from that
   split.

   if your friend says    i   m thinking of a noun, ask me up to 20 yes/no
   questions to guess what it is    and your first question is    is it a
   potato?   , then you   re a dumbass, because they   re going to say no and
   you gained almost no information. unless you happen to know your friend
   thinks about potatoes all the time, or is thinking about one right now.
   then you did a great job.

   instead, a question like    is it an object?    might make more sense.

   this is kind of like how hospitals triage patients or approach
   differential diagnoses. they ask a few questions up front and check
   some basic vitals to determine if you   re going to die imminently or
   something. they don   t start by doing a biopsy to check if you have
   pancreatic cancer as soon as you walk in the door.

   there are ways to quantify information gain so that you can essentially
   evaluate every possible split of the training data and maximize
   information gain for every split. this way you can predict every label
   or value as efficiently as possible.

   now, let   s look at a particular data set and talk about how we choose
   splits.

the titanic dataset

   kaggle has a titanic [16]dataset that is used for a lot of machine
   learning intros. when the titanic sunk, 1,502 out of 2,224 passengers
   and crew were killed. even though there was some luck involved, women,
   children, and the upper-class were more likely to survive. if you look
   back at the decision tree above, you   ll see that it somewhat reflects
   this variability across gender, age, and class.

choosing splits in a decision tree

   id178 is the amount of disorder in a set (measured by [17]gini index
   or [18]cross-id178). if the values are really mixed, there   s lots of
   id178; if you can cleanly split values, there   s no id178. for every
   split at a parent node, you want the child nodes to be as pure as
   possible         minimize id178. for example, in the titanic, gender is a
   big determinant of survival, so it makes sense for this feature to be
   used in the first split as it   s the one that leads to the most
   information gain.

   let   s take a look at our titanic variables:
   [1*s-rhrhav4h3ehzhg44dbfq.png]
   source: [19]kaggle

   we build a tree by picking one of these variables and splitting the
   dataset according to it.
   [0*f-3buln8oa_xrohp.]

   the first split separates our dataset into men and women. then, the
   women branch gets split again in age (the split that minimizes
   id178). similarly, the men branch gets split by class. by following
   the tree for a new passenger, you can use the tree to make a guess at
   whether they died.

   the titanic example is solving a classification problem (   survive    or
      die   ). if we were using id90 for regression         say, to
   predict housing prices         we would create splits on the most important
   features that determine housing prices. how many square feet: more than
   or less than ___? how many bedrooms & bathrooms: more than or less than
   ___?

   then, during testing, you would run a specific house through all the
   splits and take the average of all the housing prices in the final leaf
   node (bottom-most node) where the house ends up as your prediction for
   the sale price.

   there are a few hyperparameters you can tune with id90
   models, including max_depth and max_leaf_nodes. see the
   [20]scikit-learn module on id90 for advice on defining these
   parameters.

   id90 are effective because they are easy to read, powerful
   even with messy data, and computationally cheap to deploy once after
   training. id90 are also good for handling mixed data
   (numerical or categorical).

   that said, id90 are computationally expensive to train, carry
   a big risk of overfitting, and tend to find local optima because they
   can   t go back after they have made a split. to address these
   weaknesses, we turn to a method that illustrates the power of combining
   many id90 into one model.

id79: an ensemble of id90

   a model comprised of many models is called an ensemble model, and this
   is usually a winning strategy.

   a single decision tree can make a lot of wrong calls because it has
   very black-and-white judgments. a id79 is a meta-estimator
   that aggregates many id90, with some helpful modifications:
    1. the number of features that can be split on at each node is limited
       to some percentage of the total (this is a hyperparameter you can
       choose         see [21]scikit-learn documentation for details). this
       ensures that the ensemble model does not rely too heavily on any
       individual feature, and makes fair use of all potentially
       predictive features.
    2. each tree draws a random sample from the original data set when
       generating its splits, adding a further element of randomness that
       prevents overfitting.

   these modifications also prevent the trees from being too highly
   correlated. without #1 and #2 above, every tree would be identical,
   since recursive binary splitting is deterministic.

   to illustrate, see these nine decision tree classifiers below.
   [0*rin05ysbfrzblcxi.]
   source: [22]http://xenon.stanford.edu/~jianzh/ml/

   these decision tree classifiers can be aggregated into a id79
   ensemble which combines their input. think of the horizontal and
   vertical axes of each decision tree output as features x1 and x2. at
   certain values of each feature, the decision tree outputs a
   classification of    blue   ,    green   ,    red   , etc.
   [0*bz2nr-jypcyhb3wj.]
   source: [23]http://xenon.stanford.edu/~jianzh/ml/

   these results are aggregated, through modal votes or averaging, into a
   single ensemble model that ends up outperforming any individual
   decision tree   s output.

   id79s are an excellent starting point for the modeling
   process, since they tend to have strong performance with a high
   tolerance for less-cleaned data and can be useful for figuring out
   which features actually matter among many features.

   there are many other clever ensemble models that combine id90
   and yield excellent performance         check out [24]xgboost (extreme
   gradient boosting) as an example.

and with that, we conclude our study of supervised learning!

   nice work. in this section we   ve covered:
     * two non-parametric supervised learning algorithms: id92 and
       id90
     * measures of distance and information gain
     * id79s, which are an example of an ensemble model
     * cross-validation and hyperparameter tuning

   hopefully, you now have some solid intuitions for how we learn f given
   a training data set and use this to make predictions with the test
   data.

   next, we   ll talk about how to approach problems where we don   t have any
   labeled training data to work with, in [25]part 3: unsupervised
   learning.

practice materials & further reading

2.3a         implementing id92

   try this [26]walkthrough for implementing id92 from scratch in python.
   you may also want to take a look at the [27]scikit-learn documentation
   to get a sense of how pre-built implementations work.

2.3b         id90

   try the id90 lab in chapter 8 of [28]an introduction to
   statistical learning. you can also play with the [29]titanic dataset,
   and check out this [30]tutorial which covers the same concepts as above
   with accompanying code. here is the [31]scikit-learn implementation of
   id79 for out-of-the-box use on data sets.
     __________________________________________________________________

enter your email below if you   d like to stay up-to-date with future content     

   iframe: [32]/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=b1551b9c4930

on twitter? so are we. feel free to keep in touch         [33]vishal and
[34]samer         
     __________________________________________________________________

   more from machine learning for humans         
     * [35]part 1: why machine learning matters    
     * [36]part 2.1: supervised learning    
     * [37]part 2.2: supervised learning ii    
     * part 2.3: supervised learning iii    
     * [38]part 3: unsupervised learning
     * [39]part 4: neural networks & deep learning
     * [40]part 5: id23
     * [41]appendix: the best machine learning resources

   thanks to [42]edo and [43]sachin maini.
     * [44]machine learning
     * [45]artificial intelligence
     * [46]supervised learning
     * [47]tech
     * [48]programming

   (button)
   (button)
   (button) 3.2k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [49]go to the profile of vishal maini

[50]vishal maini

   strategy & communications [51]@deepmindai. previously [52]@upstart,
   [53]@yale, [54]@trueventurestec.

     (button) follow
   [55]machine learning for humans

[56]machine learning for humans

   demystifying artificial intelligence & machine learning. discussions on
   safe and intentional application of ai for positive social impact.

     * (button)
       (button) 3.2k
     * (button)
     *
     *

   [57]machine learning for humans
   never miss a story from machine learning for humans, when you sign up
   for medium. [58]learn more
   never miss a story from machine learning for humans
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b1551b9c4930
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/machine-learning-for-humans?source=avatar-lo_aiqwhfkniw26-e8dd9a6c82a5
   7. https://medium.com/machine-learning-for-humans?source=logo-lo_aiqwhfkniw26---e8dd9a6c82a5
   8. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@v_maini?source=post_header_lockup
  11. https://medium.com/@v_maini
  12. https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0
  13. http://paypal.me/ml4h
  14. https://en.wikipedia.org/wiki/k-nearest_neighbors_algorithm
  15. https://en.wikipedia.org/wiki/metric_(mathematics)
  16. https://www.kaggle.com/c/titanic
  17. https://en.wikipedia.org/wiki/gini_coefficient
  18. https://en.wikipedia.org/wiki/cross_id178
  19. https://www.kaggle.com/c/titanic/data
  20. http://scikit-learn.org/stable/modules/tree.html
  21. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.randomforestclassifier.html
  22. http://xenon.stanford.edu/~jianzh/ml/
  23. http://xenon.stanford.edu/~jianzh/ml/
  24. http://xgboost.readthedocs.io/en/latest/model.html
  25. https://medium.com/@v_maini/unsupervised-learning-f45587588294
  26. http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/
  27. http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighborsclassifier.html
  28. http://www-bcf.usc.edu/~gareth/isl/
  29. https://www.kaggle.com/c/titanic/data
  30. https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
  31. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.randomforestclassifier.html
  32. https://medium.com/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=b1551b9c4930
  33. https://twitter.com/v_maini
  34. https://twitter.com/seriousssam
  35. https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
  36. https://medium.com/@v_maini/supervised-learning-740383a2feab
  37. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  38. https://medium.com/@v_maini/unsupervised-learning-f45587588294
  39. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  40. https://medium.com/@v_maini/reinforcement-learning-6eacf258b265
  41. https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1
  42. https://medium.com/@jehc?source=post_page
  43. https://medium.com/@sachinmaini?source=post_page
  44. https://medium.com/tag/machine-learning?source=post
  45. https://medium.com/tag/artificial-intelligence?source=post
  46. https://medium.com/tag/supervised-learning?source=post
  47. https://medium.com/tag/tech?source=post
  48. https://medium.com/tag/programming?source=post
  49. https://medium.com/@v_maini?source=footer_card
  50. https://medium.com/@v_maini
  51. http://twitter.com/deepmindai
  52. http://twitter.com/upstart
  53. http://twitter.com/yale
  54. http://twitter.com/trueventurestec
  55. https://medium.com/machine-learning-for-humans?source=footer_card
  56. https://medium.com/machine-learning-for-humans?source=footer_card
  57. https://medium.com/machine-learning-for-humans
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/p/b1551b9c4930/share/twitter
  61. https://medium.com/p/b1551b9c4930/share/facebook
  62. https://medium.com/p/b1551b9c4930/share/twitter
  63. https://medium.com/p/b1551b9c4930/share/facebook
