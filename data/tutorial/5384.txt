   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]machine learning for humans
   [7]machine learning for humans
   [8]sign in[9]get started
     __________________________________________________________________

machine learning for humans, part 3: unsupervised learning

id91 and id84: id116 id91, hierarchical
id91, principal component analysis (pca), singular value decomposition
(svd)

   [10]go to the profile of vishal maini
   [11]vishal maini (button) blockedunblock (button) followfollowing
   aug 19, 2017
this series is available as a full-length e-book! [12]download here. free for do
wnload, contributions appreciated ([13]paypal.me/ml4h)

   how do you find the underlying structure of a dataset? how do you
   summarize it and group it most usefully? how do you effectively
   represent data in a compressed format? these are the goals of
   unsupervised learning, which is called    unsupervised    because you start
   with unlabeled data (there   s no y).

   the two unsupervised learning tasks we will explore are id91 the
   data into groups by similarity and reducing dimensionality to compress
   the data while maintaining its structure and usefulness.
examples of where unsupervised learning methods might be useful:
- an advertising platform segments the u.s. population into smaller groups with
similar demographics and purchasing habits so that advertisers can reach their t
arget market with relevant ads.
- airbnb groups its housing listings into neighborhoods so that users can naviga
te listings more easily.
- a data science team reduces the number of dimensions in a large data set to si
mplify modeling and reduce file size.

   in contrast to supervised learning, it   s not always easy to come up
   with metrics for how well an unsupervised learning algorithm is doing.
      performance    is often subjective and domain-specific.

id91

   an interesting example of id91 in the real world is marketing
   data provider acxiom   s life stage id91 system, personicx. this
   service segments u.s. households into 70 distinct clusters within 21
   life stage groups that are used by advertisers when targeting facebook
   ads, display ads, direct mail campaigns, etc.
   [1*ldqddwp4gygvhucg4q883q.png]
   a selection of personicx demographic clusters

   their [14]white paper reveals that they used centroid id91 and
   principal component analysis, both of which are techniques covered in
   this section.

   you can imagine how having access to these clusters is extremely useful
   for advertisers who want to (1) understand their existing customer base
   and (2) use their ad spend effectively by targeting potential new
   customers with relevant demographics, interests, and lifestyles.
   [1*6j6ppcraj8uawhplg_4b9a.png]
   you can actually find out which cluster you personally would belong to
   by answering a few simple questions in acxiom   s [15]   what   s my
   cluster?    tool.

   let   s walk through a couple of id91 methods to develop intuition
   for how this task can be performed.

id116 id91

      and k rings were given to the race of centroids, who above all else,
   desire power.   

   the goal of id91 is to create groups of data points such that
   points in different clusters are dissimilar while points within a
   cluster are similar.

   with id116 id91, we want to cluster our data points into k
   groups. a larger k creates smaller groups with more granularity, a
   lower id116 larger groups and less granularity.

   the output of the algorithm would be a set of    labels    assigning each
   data point to one of the k groups. in id116 id91, the way these
   groups are defined is by creating a centroid for each group. the
   centroids are like the heart of the cluster, they    capture    the points
   closest to them and add them to the cluster.

   think of these as the people who show up at a party and soon become the
   centers of attention because they   re so magnetic. if there   s just one
   of them, everyone will gather around; if there are lots, many smaller
   centers of activity will form.
here are the steps to id116 id91:
1. define the k centroids. initialize these at random (there are also fancier al
gorithms for initializing the centroids that end up converging more effectively)
.
2. find the closest centroid & update cluster assignments. assign each data poin
t to one of the k clusters. each data point is assigned to the nearest centroid
   s cluster. here, the measure of    nearness    is a hyperparameter     often euclidea
n distance.
3. move the centroids to the center of their clusters. the new position of each
centroid is calculated as the average position of all the points in its cluster.
keep repeating steps 2 and 3 until the centroid stop moving a lot at each iterat
ion (i.e., until the algorithm converges).

   that, in short, is how id116 id91 works! check out this
   [16]visualization of the algorithm         read it like a comic book. each
   point in the plane is colored according the centroid that it is closest
   to at each moment. you   ll notice that the centroids (the larger blue,
   red, and green circles) start randomly and then quickly adjust to
   capture their respective clusters.
   [0*i8ersxipzpjnwjk4.]

   another real-life application of id116 id91 is classifying
   handwritten digits. suppose we have images of the digits as a long
   vector of pixel brightnesses. let   s say the images are black and white
   and are 64x64 pixels. each pixel represents a dimension. so the world
   these images live in has 64x64=4,096 dimensions. in this
   4,096-dimensional world, id116 id91 allows us to group the
   images that are close together and assume they represent the same
   digit, which can achieve [17]pretty good results for digit recognition.

hierarchical id91

      let   s make a million options become seven options. or five. or twenty?
   meh, we can decide later.   

   hierarchical id91 is similar to regular id91, except that
   you   re aiming to build a hierarchy of clusters. this can be useful when
   you want flexibility in how many clusters you ultimately want. for
   example, imagine grouping items on an online marketplace like etsy or
   amazon. on the homepage you   d want a few broad categories of items for
   simple navigation, but as you go into more specific shopping categories
   you   d want increasing levels of granularity, i.e. more distinct
   clusters of items.

   in terms of outputs from the algorithm, in addition to cluster
   assignments you also build a nice tree that tells you about the
   hierarchies between the clusters. you can then pick the number of
   clusters you want from this tree.
here are the steps for hierarchical id91:
1. start with n clusters, one for each data point.
2. merge the two clusters that are closest to each other. now you have n-1 clust
ers.
3. recompute the distances between the clusters. there are several ways to do th
is (see this [18]tutorial for more details). one of them (called average-linkage
 id91) is to consider the distance between two clusters to be the average
distance between all their respective members.
4. repeat steps 2 and 3 until you get one cluster of n data points. you get a tr
ee (also known as a dendrogram) like the one below.
5. pick a number of clusters and draw a horizontal line in the dendrogram. for e
xample, if you want k=2 clusters, you should draw a horizontal line around    dist
ance=20000.    you   ll get one cluster with data points 8, 9, 11, 16 and one cluste
r with the rest of the data points. in general, the number of clusters you get i
s the number of intersection points of your horizontal line with the vertical li
nes in the dendrogram.

   [1*dsrma_nuw4w9nlsccmaeyq.png]
   source: [19]solver.com. for more detail on hierarchical id91, you
   can check [20]this video out.

id84

      it is not the daily increase, but the daily decrease. hack away at the
   unessential.            bruce lee

   id84 looks a lot like compression. this is about
   trying to reduce the complexity of the data while keeping as much of
   the relevant structure as possible. if you take a simple 128 x 128 x 3
   pixels image (length x width x rgb value), that   s 49,152 dimensions of
   data. if you   re able to reduce the dimensionality of the space in which
   these images live without destroying too much of the meaningful content
   in the images, then you   ve done a good job at id84.

   we   ll take a look at two common techniques in practice: principal
   component analysis and singular value decomposition.

principal component analysis (pca)

   first, a little id202 refresher         let   s talk about spaces and
   bases.

   you   re familiar with the coordinate plane with origin o(0,0) and basis
   vectors i(1,0) and j(0,1). it turns out you can choose a completely
   different basis and still have all the math work out. for example, you
   can keep o as the origin and choose the basis to vectors i   =(2,1) and
   j   =(1,2). if you have the patience for it, you   ll convince yourself
   that the point labeled (2,2) in the i   , j    coordinate system is labeled
   (6, 6) in the i, j system.
   [0*ytpe9mj2x1otlzyq.]
   plotted using mathisfun   s    [21]interactive cartesian coordinates   

   this means we can change the basis of a space. now imagine much
   higher-dimensional space. like, 50k dimensions. you can select a basis
   for that space, and then select only the 200 most significant vectors
   of that basis. these basis vectors are called principal components, and
   the subset you select constitute a new space that is smaller in
   dimensionality than the original space but maintains as much of the
   complexity of the data as possible.

   to select the most significant principal components, we look at how
   much of the data   s variance they capture and order them by that metric.

   another way of thinking about this is that pca remaps the space in
   which our data exists to make it more compressible. the transformed
   dimension is smaller than the original dimension.

   by making use of the first several dimensions of the remapped space
   only, we can start gaining an understanding of the dataset   s
   organization. this is the promise of id84: reduce
   complexity (dimensionality in this case) while maintaining structure
   (variance). here   s a [22]fun paper samer wrote on using pca (and
   diffusion mapping, another technique) to try to make sense of the
   wikileaks cable release.

singular value decomposition (svd)

   let   s represent our data like a big a = m x n matrix. svd is a
   computation that allows us to decompose that big matrix into a product
   of 3 smaller matrices (u=m x r, diagonal matrix   =r x r, and v=r x n
   where r is a small number).

   here   s a more visual illustration of that product to start with:
   [1*uqm44l4wszyg0ndgtrshdq.png]

   the values in the r*r diagonal matrix    are called singular values.
   what   s cool about them is that these singular values can be used to
   compress the original matrix. if you drop the smallest 20% of singular
   values and the associated columns in matrices u and v, you save quite a
   bit of space and still get a decent representation of the underlying
   matrix.

   to examine what that means more precisely, let   s work with this image
   of a dog:
   [0*q6djhrs0h8t1beye.]

   we   ll use the code written in andrew gibiansky   s [23]post on svd.
   first, we show that if we rank the singular values (the values of the
   matrix   ) by magnitude, the first 50 singular values contain 85% of the
   magnitude of the whole matrix   .
   [0*topsisdw8kczutnl.]

   we can use this fact to discard the next 250 values of sigma (i.e., set
   them to 0) and just keep a    rank 50    version of the image of the dog.
   here, we create a rank 200, 100, 50, 30, 20, 10, and 3 dog. obviously,
   the picture is smaller, but let   s agree that the rank 30 dog is still
   good. now let   s see how much compression we achieve with this dog. the
   original image matrix is 305*275 = 83,875 values. the rank 30 dog is
   305*30+30+30*275=17,430         almost 5 times fewer values with very little
   loss in image quality. the reason for the calculation above is that we
   also discard the parts of the matrix u and v that get multiplied by
   zeros when the operation u     v is carried out (where       is the modified
   version of    that only has the first 30 values in it).
   [0*rsigv0o1n-shaoym.]

   unsupervised learning is often used to preprocess the data. usually,
   that means compressing it in some meaning-preserving way like with pca
   or svd before feeding it to a deep neural net or another supervised
   learning algorithm.

onwards!

   now that you   ve finished this section, you   ve earned an awful,
   horrible, never-to-be-mentioned-again joke about unsupervised learning.
   here goes   

   person-in-joke-#1: y would u ever need to use unsupervised tho?

   person-in-joke-#2: y? there   s no y.

   next up    [24]part 4: neural networks & deep learning!

practice materials & further reading

3a         id116 id91

   play around with this id91 [25]visualization to build intuition
   for how the algorithm works. then, take a look at this implementation
   of [26]id116 id91 for handwritten digits and the associated
   tutorial.

3b         svd

   for a good reference on svd, go no further than andrew gibiansky   s
   [27]post.
     __________________________________________________________________

enter your email below if you   d like to stay up-to-date with future content     

   iframe: [28]/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=f45587588294

on twitter? so are we. feel free to keep in touch         [29]vishal and
[30]samer         
     __________________________________________________________________

   more from machine learning for humans         
     * [31]part 1: why machine learning matters    
     * [32]part 2.1: supervised learning    
     * [33]part 2.2: supervised learning ii    
     * [34]part 2.3: supervised learning iii    
     * part 3: unsupervised learning    
     * [35]part 4: neural networks & deep learning
     * [36]part 5: id23
     * [37]appendix: the best machine learning resources

   thanks to [38]edo and [39]sachin maini.
     * [40]machine learning
     * [41]artificial intelligence
     * [42]deep learning
     * [43]unsupervised learning
     * [44]tech

   (button)
   (button)
   (button) 2.8k claps
   (button) (button) (button) 11 (button) (button)

     (button) blockedunblock (button) followfollowing
   [45]go to the profile of vishal maini

[46]vishal maini

   strategy & communications [47]@deepmindai. previously [48]@upstart,
   [49]@yale, [50]@trueventurestec.

     (button) follow
   [51]machine learning for humans

[52]machine learning for humans

   demystifying artificial intelligence & machine learning. discussions on
   safe and intentional application of ai for positive social impact.

     * (button)
       (button) 2.8k
     * (button)
     *
     *

   [53]machine learning for humans
   never miss a story from machine learning for humans, when you sign up
   for medium. [54]learn more
   never miss a story from machine learning for humans
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f45587588294
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/machine-learning-for-humans?source=avatar-lo_rpy21rkcumrm-e8dd9a6c82a5
   7. https://medium.com/machine-learning-for-humans?source=logo-lo_rpy21rkcumrm---e8dd9a6c82a5
   8. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@v_maini?source=post_header_lockup
  11. https://medium.com/@v_maini
  12. https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0
  13. http://paypal.me/ml4h
  14. http://c.ymcdn.com/sites/dema.site-ym.com/resource/resmgr/member_resources/lifestage_id91.pdf
  15. https://isapps.acxiom.com/personicx/personicx.aspx
  16. https://www.naftaliharris.com/blog/visualizing-id116-id91/
  17. http://ieeexplore.ieee.org/document/6755106/?reload=true
  18. https://home.deib.polimi.it/matteucc/id91/tutorial_html/hierarchical.html
  19. https://www.solver.com/hierarchical-id91-example
  20. https://www.youtube.com/watch?v=ocoe7jlbxvy
  21. https://www.mathsisfun.com/data/cartesian-coordinates-interactive.html
  22. http://bit.ly/2v0nxra
  23. http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/
  24. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  25. https://www.naftaliharris.com/blog/visualizing-id116-id91/
  26. https://github.com/datamine/mnist-id116-id91
  27. http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/
  28. https://medium.com/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=f45587588294
  29. https://twitter.com/v_maini
  30. https://twitter.com/seriousssam
  31. https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
  32. https://medium.com/@v_maini/supervised-learning-740383a2feab
  33. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  34. https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930
  35. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  36. https://medium.com/@v_maini/reinforcement-learning-6eacf258b265
  37. https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1
  38. https://medium.com/@jehc?source=post_page
  39. https://medium.com/@sachinmaini?source=post_page
  40. https://medium.com/tag/machine-learning?source=post
  41. https://medium.com/tag/artificial-intelligence?source=post
  42. https://medium.com/tag/deep-learning?source=post
  43. https://medium.com/tag/unsupervised-learning?source=post
  44. https://medium.com/tag/tech?source=post
  45. https://medium.com/@v_maini?source=footer_card
  46. https://medium.com/@v_maini
  47. http://twitter.com/deepmindai
  48. http://twitter.com/upstart
  49. http://twitter.com/yale
  50. http://twitter.com/trueventurestec
  51. https://medium.com/machine-learning-for-humans?source=footer_card
  52. https://medium.com/machine-learning-for-humans?source=footer_card
  53. https://medium.com/machine-learning-for-humans
  54. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  56. https://medium.com/p/f45587588294/share/twitter
  57. https://medium.com/p/f45587588294/share/facebook
  58. https://medium.com/p/f45587588294/share/twitter
  59. https://medium.com/p/f45587588294/share/facebook
