   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]machine learning for humans
   [7]machine learning for humans
   [8]sign in[9]get started
     __________________________________________________________________

machine learning for humans, part 5: id23

exploration and exploitation. id100. id24, policy
learning, and deep id23.

   [10]go to the profile of vishal maini
   [11]vishal maini (button) blockedunblock (button) followfollowing
   aug 19, 2017
this series is available as a full-length e-book! [12]download here. free for do
wnload, contributions appreciated ([13]paypal.me/ml4h)

   i just ate some chocolate for finishing the last section.   

   in supervised learning, training data comes with an answer key from
   some godlike    supervisor   . if only life worked that way!

   in id23 (rl) there   s no answer key, but your
   id23 agent still has to decide how to act to perform
   its task. in the absence of existing training data, the agent learns
   from experience. it collects the training examples (   this action was
   good, that action was bad   ) through trial-and-error as it attempts its
   task, with the goal of maximizing long-term reward.

   in this final section of machine learning for humans, we will explore:
     * the exploration/exploitation tradeoff
     * id100 (mdps), the classic setting for rl tasks
     * id24, policy learning, and deep id23
     * and lastly, the value learning problem

   at the end, as always, we   ve compiled some favorite resources for
   further exploration.

let   s put a robot mouse in a maze

   the easiest context in which to think about id23 is
   in games with a clear objective and a point system.

   say we   re playing a game where our mouse      is seeking the ultimate
   reward of cheese at the end of the maze (     +1000 points), or the lesser
   reward of water along the way (    +10 points). meanwhile, robo-mouse
   wants to avoid locations that deliver an electric shock (    -100
   points).
   [1*jne9wcy21o_e_ztlyqspsw.png]
   the reward is cheese.

   after a bit of exploration, the mouse might find the mini-paradise of
   three water sources clustered near the entrance and spend all its time
   exploiting that discovery by continually racking up the small rewards
   of these water sources and never going further into the maze to pursue
   the larger prize.

   but as you can see, the mouse would then miss out on an even better
   oasis further in the maze, or the ultimate reward of cheese at the end!

   this brings up the exploration/exploitation tradeoff. one simple
   strategy for exploration would be for the mouse to take the best known
   action most of the time (say, 80% of the time), but occasionally
   explore a new, randomly selected direction even though it might be
   walking away from known reward.

   this strategy is called the epsilon-greedy strategy, where epsilon is
   the percent of the time that the agent takes a randomly selected action
   rather than taking the action that is most likely to maximize reward
   given what it knows so far (in this case, 20%). we usually start with a
   lot of exploration (i.e. a higher value for epsilon). over time, as the
   mouse learns more about the maze and which actions yield the most
   long-term reward, it would make sense to steadily reduce epsilon to 10%
   or even lower as it settles into exploiting what it knows.

   it   s important to keep in mind that the reward is not always immediate:
   in the robot-mouse example, there might be a long stretch of the maze
   you have to walk through and several decision points before you reach
   the cheese.
   [1*hvolc50dpq1eskuejhichg.png]
   the agent observes the environment, takes an action to interact with
   the environment, and receives positive or negative reward. diagram from
   berkeley   s [14]cs 294: deep id23 by john schulman &
   pieter abbeel

id100 (mdps)

   the mouse   s wandering through the maze can be formalized as a markov
   decision process, which is a process that has specified transition
   probabilities from state to state. we will explain it by referring to
   our robot-mouse example. mdps include:
    1. a finite set of states. these are the possible positions of our
       mouse within the maze.
    2. a set of actions available in each state. this is {forward, back}
       in a corridor and {forward, back, left, right} at a crossroads.
    3. transitions between states. for example, if you go left at a
       crossroads you end up in a new position. these can be a set of
       probabilities that link to more than one possible state (e.g. when
       you use an attack in a game of pok  mon you can either miss, inflict
       some damage, or inflict enough damage to knock out your opponent).
    4. rewards associated with each transition. in the robot-mouse
       example, most of the rewards are 0, but they   re positive if you
       reach a point that has water or cheese and negative if you reach a
       point that has an electric shock.
    5. a discount factor    between 0 and 1. this quantifies the difference
       in importance between immediate rewards and future rewards. for
       example, if    is .9, and there   s a reward of 5 after 3 steps, the
       present value of that reward is .9  *5.
    6. memorylessness. once the current state is known, the history of the
       mouse   s travels through the maze can be erased because the current
       markov state contains all useful information from the history. in
       other words,    the future is independent of the past given the
       present   .

   now that we know what an mdp is, we can formalize the mouse   s
   objective. we   re trying to maximize the sum of rewards in the long
   term:
   [0*qho_9ufmqd7kjlr2.]

   let   s look at this sum term by term. first of all, we   re summing across
   all time steps t. let   s set    at 1 for now and forget about it. r(x,a)
   is a reward function. for state x and action a (i.e., go left at a
   crossroads) it gives you the reward associated with taking that action
   a at state x. going back to our equation, we   re trying to maximize the
   sum of future rewards by taking the best action in each state.

   now that we   ve set up our id23 problem and formalized
   the goal, let   s explore some possible solutions.

id24: learning the action-value function

   id24 is a technique that evaluates which action to take based on
   an action-value function that determines the value of being in a
   certain state and taking a certain action at that state.

   we have a function q that takes as an input one state and one action
   and returns the expected reward of that action (and all subsequent
   actions) at that state. before we explore the environment, q gives the
   same (arbitrary) fixed value. but then, as we explore the environment
   more, q gives us a better and better approximation of the value of an
   action a at a state s. we update our function q as we go.

   this equation from the [15]wikipedia page on id24 explains it all
   very nicely. it shows how we update the value of q based on the reward
   we get from our environment:
   [0*q8dnp4guvdd230if.]

   let   s ignore the discount factor    by setting it to 1 again. first,
   keep in mind that q is supposed to show you the full sum of rewards
   from choosing action q and all the optimal actions afterward.

   now let   s go through the equation from left to right. when we take
   action at in state st, we update our value of q(st,at) by adding a term
   to it. this term contains:
     * learning rate alpha: this is how aggressive we want to be when
       updating our value. when alpha is close to 0, we   re not updating
       very aggressively. when alpha is close to 1, we   re simply replacing
       the old value with the updated value.
     * the reward is the reward we got by taking action at at state st. so
       we   re adding this reward to our old estimate.
     * we   re also adding the estimated future reward, which is the maximum
       achievable reward q for all available actions at xt+1.
     * finally, we subtract the old value of q to make sure that we   re
       only incrementing or decrementing by the difference in the estimate
       (multiplied by alpha of course).

   now that we have a value estimate for each state-action pair, we can
   select which action to take according to our action-selection strategy
   (we don   t necessarily just choose the action that leads to the most
   expected reward every time, e.g. with an epsilon-greedy exploration
   strategy we   d take a random action some percentage of the time).

   in the robot mouse example, we can use id24 to figure out the
   value of each position in the maze and the value of the actions
   {forward, backward, left, right} at each position. then we can use our
   action-selection strategy to choose what the mouse actually does at
   each time step.

policy learning: a map from state to action

   in the id24 approach, we learned a value function that estimated
   the value of each state-action pair.

   policy learning is a more straightforward alternative in which we learn
   a policy function,   , which is a direct map from each state to the best
   corresponding action at that state. think of it as a behavioral policy:
      when i observe state s, the best thing to do is take action a   . for
   example, an autonomous vehicle   s policy might effectively include
   something like:    if i see a yellow light and i am more than 100 feet
   from the intersection, i should brake. otherwise, keep moving forward.   
   [1*k-34jr4lncf2uddhbwbnfq.png]
   a policy is a map from state to action.

   so we   re learning a function that will maximize expected reward. what
   do we know that   s really good at learning complex functions? deep
   neural networks!

   andrej karpathy   s [16]pong from pixels provides an excellent
   walkthrough on using deep id23 to learn a policy for
   the atari game pong that takes raw pixels from the game as the input
   (state) and outputs a id203 of moving the paddle up or down
   (action).
   [1*nr9owgvdowrkqmhw0gupfw.png]
   in a policy gradient network, the agent learns the optimal policy by
   adjusting its weights through id119 based on reward signals
   from the environment. image via
   [17]http://karpathy.github.io/2016/05/31/rl/

   if you want to get your hands dirty with deep rl, work through andrej   s
   post. you will implement a 2-layer policy network in 130 lines of code,
   and will also learn how to plug into openai   s [18]gym, which allows you
   to quickly get up and running with your first id23
   algorithm, test it on a variety of games, and see how its performance
   compares to other submissions.

id25s, a3c, and advancements in deep rl

   in 2015, deepmind used a method called deep q-networks (id25), an
   approach that approximates q-functions using deep neural networks, to
   beat human benchmarks across many atari games:

   we demonstrate that the deep q-network agent, receiving only the pixels
   and the game score as inputs, was able to surpass the performance of
   all previous algorithms and achieve a level comparable to that of a
   professional human games tester across a set of 49 games, using the
   same algorithm, network architecture and hyperparameters. this work
   bridges the divide between high-dimensional sensory inputs and actions,
   resulting in the first artificial agent that is capable of learning to
   excel at a diverse array of challenging tasks. ([19]silver et al.,
   2015)

   here is a snapshot of where id25 agents stand relative to linear
   learners and humans in various domains:
   [1*ebpjtl7wylg1kh 5todw.png]
   these are normalized with respect to professional human games testers:
   0% = random play, 100% = human performance. source: deepmind   s id25
   paper, [20]human-level control through deep id23

   to help you build some intuition for how advancements are made in rl
   research, here are some examples of improvements on attempts at
   non-linear q-function approximators that can improve performance and
   stability:
     * experience replay, which learns by randomizing over a longer
       sequence of previous observations and corresponding reward to avoid
       overfitting to recent experiences. this idea is [21]inspired by
       biological brains: rats traversing mazes, for example,    replay   
       patterns of neural activity during sleep in order to optimize
       future behavior in the maze.
     * recurrent neural networks (id56s) augmenting id25s. when an agent can
       only see its immediate surroundings (e.g. robot-mouse only seeing a
       certain segment of the maze vs. a birds-eye view of the whole
       maze), the agent needs to remember the bigger picture so it
       remembers where things are. this is similar to how humans babies
       develop [22]object permanence to know things exist even if they
       leave the baby   s visual field. id56s are    recurrent   , i.e. [23]they
       allow information to persist on a longer-term basis. here   s an
       impressive video of a deep recurrent q-network (dqrn) playing doom.

   iframe: [24]/media/2286543cfd01ba0ac858ada4857dc635?postid=6eacf258b265

   paper: [25]https://arxiv.org/abs/1609.05521. source: arthur juliani   s
   [26]simple id23 with tensorflow series

   in 2016, just one year after the id25 paper, deepmind revealed another
   algorithm called asynchronous advantage actor-critic (a3c) that
   surpassed state-of-the-art performance on atari games after training
   for half as long ([27]mnih et al., 2016). a3c is an actor-critic
   algorithm that combines the best of both approaches we explored
   earlier: it uses an actor (a policy network that decides how to act)
   and a critic (a q-network that decides how valuable things are). arthur
   juliani has a nice [28]writeup on how a3c works specifically. a3c is
   now openai   s [29]universe starter agent.

   since then, there have been countless fascinating breakthroughs         from
   ais [30]inventing their own language to [31]teaching themselves to walk
   in a variety of terrains. this series only scratches the surface on the
   cutting edge of rl, but hopefully it will serve as a starting point for
   further exploration!

   as a parting note, we   d like to share this incredible video of
   deepmind   s agents that learned to walk    with added sound. grab some
   popcorn, turn up the volume, and witness the full glory of artificial
   intelligence.

   iframe: [32]/media/e7187ecd760a815468c4e79c622dc625?postid=6eacf258b265

               

practice materials & further reading

code

     * andrej karpathy   s [33]pong from pixels will get you up-and-running
       quickly with your first id23 agent. as the
       article describes,    we   ll learn to play an atari game (pong!) with
       pg, from scratch, from pixels, with a deep neural network, and the
       whole thing is 130 lines of python only using numpy as a dependency
       ([34]gist link).   
     * next, we   d highly recommend arthur juliani   s [35]simple
       id23 with tensorflow tutorial. it walks through
       id25s, policy learning, actor-critic methods, and strategies for
       exploration with implementations using tensorflow. try
       understanding and then re-implementing the methods covered.

reading + lectures

     * richard sutton   s book, [36]id23: an
       introduction         a fantastic book, very readable
     * john schulman   s [37]cs 294: deep id23 (berkeley)
     * david silver   s [38]id23 course (ucl)

you did it!

     if you   ve made it this far, that is all the reward we could
     hope for.

     we hope you enjoyed the series as an introduction to machine
     learning. we   ve compiled some of our favorite ml resources in the
     [39]appendix if you   re ready to see how deep this rabbit hole goes.

     please don   t hesitate to [40]reach out with thoughts, questions,
     feedback, or your favorite gifs!

     until next time,

     vishal and samer

closing thoughts
there is a fundamental question that inspired this series, and we'd like to pose
 it to you as well.
what is our objective function, as humans? how do we define the reward that we m
aximize in our real lives? beyond base pleasure and pain, our definition of rewa
rd also tends to include messy things like right and wrong, fulfillment, love, s
pirituality, and purpose.
there has been an intellectual field dedicated to the question of what our objec
tive functions are or should be since ancient times, and it   s called moral philo
sophy. the central question of moral philosophy is: what ought we do? how should
 we live? which actions are right or wrong? the answer is, quite clearly: it dep
ends on your values.
as we create more and more advanced ai, it will start to depart from the realm o
f toy problems like atari games, where    reward    is cleanly defined by how many p
oints are won in the game,  and exist more and more in the real world. autonomou
s vehicles, for example, have to make decisions with a somewhat more complex def
inition of reward. at first, reward might be tied to something like    getting saf
ely to the destination". but if forced to choose between staying the course and
hitting five pedestrians or swerving and hitting one, should the vehicle swerve?
 what if the one pedestrian is a child, or a gunman on the loose, or the next ei
nstein? how does that change the decision, and why? what if swerving also destro
ys a piece of valuable art? suddenly we have a much more complex problem when we
 try to define the objective function, and the answers are not as simple.
in this series, we explored why it   s difficult to specify explicitly to a comput
er what a cat looks like - if asked how we know ourselves, the answer is, most s
imply,    intuition    - but we   ve explored machine vision approaches to teaching th
e machine to learn this intuition by itself. similarly, in the domain of machine
 morality, it might be difficult to specify exactly how to evaluate the rightnes
s or wrongness of one action vs. another, but perhaps it is possible for a machi
ne to learn these values in some way. this is called the values learning problem
, and it may be one of the most important technical problems humans will ever ha
ve to solve.
for more on this topic, see this synoptic post on the [41]risks of artificial in
telligence. and as you go forth into the world of making machine smarter and sma
rter, we'd encourage you to keep in mind that ai progress is a double-edged swor
d, of particular keenness on both sides.
     __________________________________________________________________

enter your email below if you   d like to stay up-to-date with future content     

   iframe: [42]/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=6eacf258b265

on twitter? so are we. feel free to keep in touch         [43]vishal and
[44]samer         
     __________________________________________________________________

     * [45]part 1: why machine learning matters    
     * [46]part 2.1: supervised learning    
     * [47]part 2.2: supervised learning ii    
     * [48]part 2.3: supervised learning iii    
     * [49]part 3: unsupervised learning    
     * [50]part 4: neural networks & deep learning    
     * part 5: id23    
     * [51]appendix: the best machine learning resources

   thanks to [52]sachin maini.
     * [53]machine learning
     * [54]artificial intelligence
     * [55]deep learning
     * [56]id23
     * [57]tech

   (button)
   (button)
   (button) 5k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [58]go to the profile of vishal maini

[59]vishal maini

   strategy & communications [60]@deepmindai. previously [61]@upstart,
   [62]@yale, [63]@trueventurestec.

     (button) follow
   [64]machine learning for humans

[65]machine learning for humans

   demystifying artificial intelligence & machine learning. discussions on
   safe and intentional application of ai for positive social impact.

     * (button)
       (button) 5k
     * (button)
     *
     *

   [66]machine learning for humans
   never miss a story from machine learning for humans, when you sign up
   for medium. [67]learn more
   never miss a story from machine learning for humans
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6eacf258b265
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/machine-learning-for-humans?source=avatar-lo_iotiucjbviwy-e8dd9a6c82a5
   7. https://medium.com/machine-learning-for-humans?source=logo-lo_iotiucjbviwy---e8dd9a6c82a5
   8. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@v_maini?source=post_header_lockup
  11. https://medium.com/@v_maini
  12. https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0
  13. http://paypal.me/ml4h
  14. http://rll.berkeley.edu/deeprlcourse-fa15/
  15. https://en.wikipedia.org/wiki/id24
  16. http://karpathy.github.io/2016/05/31/rl/
  17. http://karpathy.github.io/2016/05/31/rl/
  18. https://gym.openai.com/
  19. https://storage.googleapis.com/deepmind-media/id25/id25naturepaper.pdf
  20. https://storage.googleapis.com/deepmind-media/id25/id25naturepaper.pdf
  21. https://deepmind.com/blog/ai-and-neuroscience-virtuous-circle/
  22. https://en.wikipedia.org/wiki/object_permanence
  23. http://colah.github.io/posts/2015-08-understanding-lstms/
  24. https://medium.com/media/2286543cfd01ba0ac858ada4857dc635?postid=6eacf258b265
  25. https://arxiv.org/abs/1609.05521
  26. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  27. https://arxiv.org/pdf/1602.01783.pdf
  28. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2
  29. https://github.com/openai/universe-starter-agent
  30. https://blog.openai.com/learning-to-cooperate-compete-and-communicate/
  31. https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/
  32. https://medium.com/media/e7187ecd760a815468c4e79c622dc625?postid=6eacf258b265
  33. http://karpathy.github.io/2016/05/31/rl/
  34. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5
  35. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  36. http://people.inf.elte.hu/lorincz/files/rl_2006/suttonbook.pdf
  37. http://rll.berkeley.edu/deeprlcourse/
  38. http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching.html
  39. https://medium.com/@v_maini/machine-learning-for-humans-part-5-the-best-machine-learning-24d53bb64aa1
  40. mailto:ml4humans@gmail.com
  41. https://thinkingwires.com/posts/2017-07-05-risks.html
  42. https://medium.com/media/ba3502c3c2cc3159fdd3bda87c87164c?postid=6eacf258b265
  43. https://twitter.com/v_maini
  44. https://twitter.com/seriousssam
  45. https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12
  46. https://medium.com/@v_maini/supervised-learning-740383a2feab
  47. https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d
  48. https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930
  49. https://medium.com/@v_maini/unsupervised-learning-f45587588294
  50. https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b
  51. https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1
  52. https://medium.com/@sachinmaini?source=post_page
  53. https://medium.com/tag/machine-learning?source=post
  54. https://medium.com/tag/artificial-intelligence?source=post
  55. https://medium.com/tag/deep-learning?source=post
  56. https://medium.com/tag/reinforcement-learning?source=post
  57. https://medium.com/tag/tech?source=post
  58. https://medium.com/@v_maini?source=footer_card
  59. https://medium.com/@v_maini
  60. http://twitter.com/deepmindai
  61. http://twitter.com/upstart
  62. http://twitter.com/yale
  63. http://twitter.com/trueventurestec
  64. https://medium.com/machine-learning-for-humans?source=footer_card
  65. https://medium.com/machine-learning-for-humans?source=footer_card
  66. https://medium.com/machine-learning-for-humans
  67. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  69. https://medium.com/p/6eacf258b265/share/twitter
  70. https://medium.com/p/6eacf258b265/share/facebook
  71. https://medium.com/p/6eacf258b265/share/twitter
  72. https://medium.com/p/6eacf258b265/share/facebook
