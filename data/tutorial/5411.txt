   #[1]deep ideas    feed [2]deep ideas    comments feed [3]deep ideas   
   deep learning from scratch iii: training criterion comments feed
   [4]alternate [5]alternate

   [6]facebook[7]twitter[8]email[9]rss[10]linkedin[11]tumblr

   [12]deep ideas logo deep ideas retina logo

a blog on artificial intelligence, deep learning and cognitive science

     * [13]home
     * [14]list of articles
     * [15]about me
     * ____________________
          

   [16]previous [17]next

     * [18]view larger image

deep learning from scratch iii: training criterion

   this is part 3 of a series of tutorials, in which we develop the
   mathematical and algorithmic underpinnings of deep neural networks from
   scratch and implement our own neural network library in python,
   mimicing the [19]tensorflow api. start with the first part: [20]i:
   computational graphs.
     * [21]part i: computational graphs
     * [22]part ii: id88s
     * [23]part iii: training criterion
     * [24]part iv: id119 and id26
     * [25]part v: multi-layer id88s
     * [26]part vi: tensorflow

training criterion

   great, so now we are able to classify points using a linear classifier
   and compute the id203 that the point belongs to a certain class,
   provided that we know the appropriate parameters for the weight matrix
   $w$ and bias $b$. the natural question that arises is how to come up
   with appropriate values for these. in the red/blue example, we just
   looked at the training points and guessed a line that nicely separated
   the training points. but generally we do not want to specify the
   separating line by hand. rather, we just want to supply the training
   points to the computer and let it come up with a good separating line
   on its own. but how do we judge whether a separating line is good or
   bad?

the misclassification rate

   ideally, we want to find a line that makes as few errors as possible.
   for every point $x$ and class $c(x)$ drawn from the true but unknown
   data-generating distribution $p_\text{data}(x, c(x))$, we want to
   minimize the id203 that our id88 classifies it incorrectly
       the id203 of misclassification:

   $$\underset{w, b}{\operatorname{argmin}} p(\hat{c}(x) \neq c(x) \mid x,
   c(x) \, \tilde{} \, p_\text{data} )$$

   generally, we do not know the data-generating distribution
   $p_\text{data}$, so it is impossible to compute the exact id203
   of misclassification. instead, we are given a finite list of $n$
   training points consisting of the values of $x$ with their
   corresponding classes. in the following, we represent the list of
   training points as a matrix $x \in \mathbb{r}^{n \times d}$ where each
   row corresponds to one training point and each column to one dimension
   of the input space. moreover, we represent the true classes as a matrix
   $c \in \mathbb{r}^{n \times c}$ where $c_{i, j} = 1$ if the $i$-th
   training sample has class $j$. similarly, we represent the predicted
   classes as a matrix $\hat{c} \in \mathbb{r}^{n \times c}$ where
   $\hat{c}_{i, j} = 1$ if the $i$-th training sample has a predicted
   class $j$. finally, we represent the output probabilities of our model
   as a matrix $p \in \mathbb{r}^{n \times c}$ where $p_{i, j}$ contains
   the id203 that the $i$-th training sample belongs to the j-th
   class.

   we could use the training data to find a classifier that minimizes the
   misclassification rate on the training samples:

   $$
   \underset{w, b}{\operatorname{argmin}} \frac{1}{n} \sum_{i = 1}^n
   i(\hat{c}_i \neq c_i)
   $$

   however, it turns out that finding a linear classifier that minimizes
   the misclassification rate is an intractable problem, i.e. its
   computational complexity is exponential in the number of input
   dimensions, rendering it unpractical. moreover, even if we have found a
   classifier that minimizes the misclassification rate on the training
   samples, it might be possible to make the classifier more robust to
   unseen samples by pushing the classes further apart, even if this does
   not reduce the misclassification rate on the training samples.

id113

   an alternative is to use [27]id113, where we
   try to find the parameters that maximize the id203 of the
   training data:
   \begin{align}
   \underset{w, b}{\operatorname{argmax}} p(\hat{c} = c) \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmax}} \prod_{i=1}^n p(\hat{c}_i =
   c_i) \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmax}} \prod_{i=1}^n \prod_{j=1}^c
   p_{i, j}^{i(c_i = j)} \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmax}} \prod_{i=1}^n \prod_{j=1}^c
   p_{i, j}^{c_{i, j}} \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmax}} log \prod_{i=1}^n
   \prod_{j=1}^c p_{i, j}^{c_{i, j}} \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmax}} \sum_{i=1}^n \sum_{j=1}^c
   c_{i, j} \cdot log \, p_{i, j} \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmin}}     \sum_{i=1}^n \sum_{j=1}^c
   c_{i, j} \cdot log \, p_{i, j} \\
   \end{align}\begin{align}
   = \underset{w, b}{\operatorname{argmin}} j
   \end{align}

   we refer to $j =     \sum_{i=1}^n \sum_{j=1}^c c_{i, j} \cdot log \,
   p_{i, j}$ as the cross-id178 loss. we want to minimize $j$.

   we can view $j$ as yet another operation in our computational graph
   that takes the input data $x$, the true classes $c$ and our predicted
   probabilities $p$ (which are the output of the $\sigma$ operation) as
   input and computes a real number designating the loss:

   [loss_graph.png]

building an operation that computes $j$

   we can build up $j$ from various more primitive operations. using the
   element-wise id127 $\odot$, we can rewrite $j$ as
   follows:

   $$
       \sum_{i=1}^n \sum_{j=1}^c (c \odot log \, p)_{i, j}
   $$

   going from the inside out, we can see that we need to implement the
   following operations:
     * $log$: the element-wise logarithm of a matrix or vector
     * $\odot$: the element-wise product of two matrices
     * $\sum_{j=1}^c$: sum over the columns of a matrix
     * $\sum_{i=1}^n$: sum over the rows of a matrix
     * $-$: taking the negative

   let   s implement these operations.

log

   this computes the element-wise logarithm of a tensor.

   iframe:
   [28]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/training-criterion/132311/log

multiply / $\odot$

   this computes the element-wise product of two tensors of the same
   shape.

   iframe:
   [29]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/training-criterion/132312/multiply

reduce_sum

   we   ll implement the summation over rows, columns, etc. in a single
   operation where we specify an axis. this way, we can use the same
   method for all types of summations. for example, axis = 0 sums over the
   rows, axis = 1 sums over the columns, etc. this is exactly what
   numpy.sum does.

   iframe:
   [30]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/training-criterion/132313/reduce_sum

negative

   this computes the element-wise negative of a tensor.

   iframe:
   [31]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/training-criterion/132314/negative

putting it all together

   using these operations, we can now compute $j =     \sum_{i=1}^n
   \sum_{j=1}^c (c \odot log \, p)_{i, j}
   $ as follows:
j = negative(reduce_sum(reduce_sum(multiply(c, log(p)), axis=1)))

example

   let   s now compute the loss of our red/blue id88.

   iframe:
   [32]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/training-criterion/132315/loss


   if you have any questions, feel free to leave a comment. otherwise,
   continue with the next part: [33]iv: id119 and
   id26

share this:

     * [34]click to share on twitter (opens in new window)
     * [35]click to share on facebook (opens in new window)
     * [36]click to share on google+ (opens in new window)
     * [37]click to share on skype (opens in new window)
     * [38]click to share on linkedin (opens in new window)
     * [39]click to print (opens in new window)
     * [40]click to share on pocket (opens in new window)
     * [41]click to share on tumblr (opens in new window)
     * [42]click to share on reddit (opens in new window)
     * [43]click to share on telegram (opens in new window)
     * [44]click to share on pinterest (opens in new window)
     * [45]click to share on whatsapp (opens in new window)
     *

related

   by [46]daniel| 2017-11-09t17:04:54+00:00 august 26th,
   2017|[47]artificial intelligence, [48]deep learning, [49]machine
   learning, [50]python, [51]tensorflow|[52]9 comments

stay updated

   subscribe to the mailing list and get updated about new blog posts by
   email.
   ____________________
   [ ] i consent to my submitted data being collected via this form*
   sign up now

   thank you for subscribing.

   something went wrong.

   i respect your privacy and take protecting it seriously

follow me on twitter

   [53]my tweets

   copyright 2017 daniel sabinasz
   [54]facebook[55]twitter[56]email[57]rss[58]linkedin[59]tumblr

references

   visible links
   1. http://www.deepideas.net/feed/
   2. http://www.deepideas.net/comments/feed/
   3. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/feed/
   4. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
   5. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/&format=xml
   6. http://fb.me/deepideas.net
   7. https://twitter.com/deepideas_net
   8. mailto:daniel@sabinasz.net
   9. http://www.deepideas.net/feed/
  10. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  11. https://deepideas.tumblr.com/
  12. http://www.deepideas.net/
  13. http://www.deepideas.net/
  14. http://www.deepideas.net/list-of-articles/
  15. http://www.deepideas.net/about-me/
  16. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  17. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  18. https://i2.wp.com/www.deepideas.net/wp-content/uploads/2017/08/neuron.jpg?fit=900,300
  19. http://www.tensorflow.org/
  20. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  21. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  22. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  23. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  24. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  25. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  26. http://www.deepideas.net/deep-learning-from-scratch-vi-tensorflow/
  27. https://en.wikipedia.org/wiki/maximum_likelihood_estimation
  28. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/training-criterion/132311/log
  29. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/training-criterion/132312/multiply
  30. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/training-criterion/132313/reduce_sum
  31. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/training-criterion/132314/negative
  32. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/training-criterion/132315/loss
  33. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  34. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=twitter
  35. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=facebook
  36. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=google-plus-1
  37. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=skype
  38. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=linkedin
  39. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/#print
  40. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=pocket
  41. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=tumblr
  42. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=reddit
  43. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=telegram
  44. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/?share=pinterest
  45. https://api.whatsapp.com/send?text=deep learning from scratch iii: training criterion http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  46. http://www.deepideas.net/author/daniel/
  47. http://www.deepideas.net/category/artificial-intelligence/
  48. http://www.deepideas.net/category/artificial-intelligence/machine-learning/deep-learning/
  49. http://www.deepideas.net/category/artificial-intelligence/machine-learning/
  50. http://www.deepideas.net/category/python/
  51. http://www.deepideas.net/category/artificial-intelligence/machine-learning/tensorflow/
  52. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/#comments
  53. https://twitter.com/deepideas_net
  54. http://fb.me/deepideas.net
  55. https://twitter.com/deepideas_net
  56. mailto:daniel@sabinasz.net
  57. http://www.deepideas.net/feed/
  58. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  59. https://deepideas.tumblr.com/

   hidden links:
  61. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  62. https://www.facebook.com/deepideas.net/
