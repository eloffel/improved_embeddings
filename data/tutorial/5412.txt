   #[1]deep ideas    feed [2]deep ideas    comments feed [3]deep ideas   
   deep learning from scratch iv: id119 and id26
   comments feed [4]alternate [5]alternate

   [6]facebook[7]twitter[8]email[9]rss[10]linkedin[11]tumblr

   [12]deep ideas logo deep ideas retina logo

a blog on artificial intelligence, deep learning and cognitive science

     * [13]home
     * [14]list of articles
     * [15]about me
     * ____________________
          

   [16]previous [17]next

     * [18]view larger image

deep learning from scratch iv: id119 and id26

   this is part 4 of a series of tutorials, in which we develop the
   mathematical and algorithmic underpinnings of deep neural networks from
   scratch and implement our own neural network library in python,
   mimicing the [19]tensorflow api. start with the first part: [20]i:
   computational graphs.
     * [21]part i: computational graphs
     * [22]part ii: id88s
     * [23]part iii: training criterion
     * [24]part iv: id119 and id26
     * [25]part v: multi-layer id88s
     * [26]part vi: tensorflow

id119

   generally, if we want to find the minimum of a function, we set the
   derivative to zero and solve for the parameters. it turns out, however,
   that it is impossible to obtain a closed-form solution for $w$ and $b$.
   instead, we iteratively search for a minimum using a method called
   id119.

   as a visual analogy, imagine yourself standing on a mountain and trying
   to find the way down. at every step, you walk into the steepest
   direction, since this direction is the most promising to lead you
   towards the bottom.

   [gradient_descent.png]

   if taking steep steps seems a little dangerous to you, imagine that you
   are a mountain goat (which are [27]amazing rock climbers).

   id119 operates in a similar way when trying to find the
   minimum of a function: it starts at a random location in parameter
   space and then iteratively reduces the error $j$ until it reaches a
   local minimum. at each step of the iteration, it determines the
   direction of steepest descent and takes a step along that direction.
   this process is depicted for the 1-dimensional case in the following
   image.

   [gradient_descent_2.png]

   as you might remember, the direction of steepest ascent of a function
   at a certain point is given by the gradient at that point. therefore,
   the direction of steepest descent is given by the negative of the
   gradient. so now we have a rough idea how to minimize $j$:
    1. start with random values for $w$ and $b$
    2. compute the gradients of $j$ with respect to $w$ and $b$
    3. take a small step along the direction of the negative gradient
    4. go back to 2

   let   s implement an operation that minimizes the value of a node using
   id119. we require the user to specify the magnitude of the
   step along the gradient as a parameter called learning_rate.

   iframe:
   [28]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132316/gradient%20descent%20op
   timizer

   the following image depicts an example iteration of id119.
   we start out with a random separating line (marked as 1), take a step,
   arrive at a slightly better line (marked as 2), take another step, and
   another step, and so on until we arrive at a good separating line.

   [gradient_descent_3.png]

id26

   in our implementation of id119, we have used a function
   compute_gradient(loss) that computes the gradient of a $loss$ operation
   in our computational graph with respect to the output of every other
   node $n$ (i.e. the direction of change for $n$ along which the loss
   increases the most). we now need to figure out how to compute
   gradients.

   consider the following computational graph:

   [abcde.png]

   by the [29]chain rule, we have

   $$\frac{\partial e}{\partial a} = \frac{\partial e}{\partial b} \cdot
   \frac{\partial b}{\partial a} = \frac{\partial e}{\partial c} \cdot
   \frac{\partial c}{\partial b} \cdot \frac{\partial b}{\partial a} =
   \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial c} \cdot
   \frac{\partial c}{\partial b} \cdot \frac{\partial b}{\partial a}$$

   as we can see, in order to compute the gradient of $e$ with respect to
   $a$, we can start at $e$ an go backwards towards $a$, computing the
   gradient of every node   s output with respect to its input along the way
   until we reach $a$. then, we multiply them all together.

   now consider the following scenario:

   [abcde2.png]

   in this case, $a$ contributes to $e$ along two paths: the path $a$,
   $b$, $d$, $e$ and the path $a$, $c$, $d$, $e$. hence, the [30]total
   derivative of $e$ with respect to $a$ is given by:

   $$
   \frac{\partial e}{\partial a}
   = \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial a}
   = \frac{\partial e}{\partial d} \cdot \left( \frac{\partial d}{\partial
   b} \cdot \frac{\partial b}{\partial a} + \frac{\partial d}{\partial c}
   \cdot \frac{\partial c}{\partial a} \right)
   = \frac{\partial e}{\partial d} \cdot \frac{\partial d}{\partial b}
   \cdot \frac{\partial b}{\partial a} + \frac{\partial e}{\partial d}
   \cdot \frac{\partial d}{\partial c} \cdot \frac{\partial c}{\partial a}
   $$

   this gives us an intuition for the general algorithm that computes the
   gradient of the loss with respect to another node: we perform a
   backwards [31]breadth-first search starting from the loss node. at each
   node $n$ that we visit, we compute the gradient of the loss with
   respect do $n$   s output by doing the following for each of $n$   s
   consumers $c$:
     * retrieve the gradient $g$ of the loss with respect to the output of
       $c$
     * multiply $g$ by the gradient of $c$   s output with respect to $n$   s
       output

   and then we sum those gradients over all consumers.

   as a prerequisite to implementing id26, we need to specify a
   function for each operation that computes the gradients with respect to
   the inputs of that operation, given the gradients with respect to the
   output. let   s define a decorator @registergradient(operation_name) for
   this purpose:

   iframe:
   [32]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132317/register%20gradient%20d
   ecorator

   now assume that our _gradient_registry dictionary is already filled
   with gradient computation functions for all of our operations. we can
   now implement id26:

   iframe:
   [33]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132318/id26

gradient of each operation

   for each of our operations, we now need to define a function that turns
   a gradient of the loss with respect to the operation   s output into a
   list of gradients of the loss with respect to each of the operation   s
   inputs. computing a gradient with respect to a matrix can be somewhat
   tedious. therefore, the details have been omitted and i just present
   the results. you may skip this section and still understand the overall
   picture.

   if you want to comprehend how to arrive at the results, the general
   approach is as follows:
     * find the partial derivative of each output value with respect to
       each input value (this can be a tensor of a rank greater than 2,
       i.e. neither scalar nor vector nor matrix, involving a lot of
       summations)
     * compute the gradient of the loss with respect to the node   s inputs
       given a gradient with respect to the node   s output by applying the
       chain rule. this is now a tensor of the same shape as the input
       tensor, so if the input is a matrix, the result is also a matrix
     * rewrite this result as a sequence of matrix operations in order to
       compute it efficiently. this step can be somewhat tricky.

gradient for negative

   given a gradient $g$ with respect to $-x$, the gradient with respect to
   $x$ is given by $-g$.

   iframe:
   [34]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132319/gradient%20for%20negati
   ve

gradient for log

   given a gradient $g$ with respect to $log(x)$, the gradient with
   respect to $x$ is given by $\frac{g}{x}$.

   iframe:
   [35]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132320/gradient%20for%20log

gradient for sigmoid

   given a gradient $g$ with respect to $\sigma(a)$, the gradient with
   respect to $a$ is given by $g \cdot \sigma(a) \cdot \sigma(1-a)$.

   iframe:
   [36]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132321/gradient%20for%20sigmoi
   d

gradient for multiply

   given a gradient $g$ with respect to $a \odot b$, the gradient with
   respect to $a$ is given by $g \odot b$ and the gradient with respect to
   $b$ is given by $g \odot a$.

   iframe:
   [37]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132322/gradient%20for%20multip
   ly

gradient for matmul

   given a gradient $g$ with respect to $ab$, the gradient with respect to
   $a$ is given by $gb^t$ and the gradient with respect to $b$ is given by
   $a^tg$.

   iframe:
   [38]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132323/gradient%20for%20matmul

gradient for add

   given a gradient $g$ with respect to $a + b$, the gradient with respect
   to $a$ is given by $g$ and the gradient with respect to $b$ is also
   given by $g$, provided that $a$ and $b$ are of the same shape. if $a$
   and $b$ are of different shapes, e.g. one matrix $a$ with 100 rows and
   one row vector $b$, we assume that $b$ is added to each row of $a$. in
   this case, the gradient computation is a little more involved, but i
   will not spell out the details here.

   iframe:
   [39]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132324/gradient%20for%20add

gradient for reduce_sum

   given a gradient $g$ with respect to the output of reduce_sum, the
   gradient with respect to the input $a$ is given by repeating $g$ along
   the specified axis.

   iframe:
   [40]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132325/gradient%20for%20reduce
   _sum

gradient for softmax

   iframe:
   [41]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132326/gradient%20for%20softma
   x

example

   let   s now test our implementation to determine the optimal weights for
   our id88.

   iframe:
   [42]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132327/example%3a%20training%2
   0the%20id88

   notice that we started out with a rather high loss and incrementally
   reduced it. let   s plot the final line to check that it is a good
   separator:

   iframe:
   [43]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/gradient-descent-and-id26/132328/example%3a%20plot%20res
   ult

   if you have any questions, feel free to leave a comment. otherwise,
   continue with the next part: [44]v: multi-layer id88s

share this:

     * [45]click to share on twitter (opens in new window)
     * [46]click to share on facebook (opens in new window)
     * [47]click to share on google+ (opens in new window)
     * [48]click to share on skype (opens in new window)
     * [49]click to share on linkedin (opens in new window)
     * [50]click to print (opens in new window)
     * [51]click to share on pocket (opens in new window)
     * [52]click to share on tumblr (opens in new window)
     * [53]click to share on reddit (opens in new window)
     * [54]click to share on telegram (opens in new window)
     * [55]click to share on pinterest (opens in new window)
     * [56]click to share on whatsapp (opens in new window)
     *

related

   by [57]daniel| 2017-11-18t13:09:59+00:00 august 26th,
   2017|[58]artificial intelligence, [59]deep learning, [60]machine
   learning, [61]python, [62]tensorflow|[63]13 comments

stay updated

   subscribe to the mailing list and get updated about new blog posts by
   email.
   ____________________
   [ ] i consent to my submitted data being collected via this form*
   sign up now

   thank you for subscribing.

   something went wrong.

   i respect your privacy and take protecting it seriously

follow me on twitter

   [64]my tweets

   copyright 2017 daniel sabinasz
   [65]facebook[66]twitter[67]email[68]rss[69]linkedin[70]tumblr

references

   visible links
   1. http://www.deepideas.net/feed/
   2. http://www.deepideas.net/comments/feed/
   3. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/feed/
   4. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
   5. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/&format=xml
   6. http://fb.me/deepideas.net
   7. https://twitter.com/deepideas_net
   8. mailto:daniel@sabinasz.net
   9. http://www.deepideas.net/feed/
  10. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  11. https://deepideas.tumblr.com/
  12. http://www.deepideas.net/
  13. http://www.deepideas.net/
  14. http://www.deepideas.net/list-of-articles/
  15. http://www.deepideas.net/about-me/
  16. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  17. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  18. https://i2.wp.com/www.deepideas.net/wp-content/uploads/2017/08/neuron.jpg?fit=900,300
  19. http://www.tensorflow.org/
  20. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  21. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  22. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  23. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  24. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  25. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  26. http://www.deepideas.net/deep-learning-from-scratch-vi-tensorflow/
  27. https://www.youtube.com/watch?v=hzwxlxriwus
  28. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132316/id119 optimizer
  29. https://en.wikipedia.org/wiki/chain_rule
  30. https://en.wikipedia.org/wiki/total_derivative
  31. https://en.wikipedia.org/wiki/breadth-first_search
  32. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132317/register gradient decorator
  33. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132318/id26
  34. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132319/gradient for negative
  35. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132320/gradient for log
  36. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132321/gradient for sigmoid
  37. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132322/gradient for multiply
  38. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132323/gradient for matmul
  39. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132324/gradient for add
  40. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132325/gradient for reduce_sum
  41. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132326/gradient for softmax
  42. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132327/example: training the id88
  43. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/gradient-descent-and-id26/132328/example: plot result
  44. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  45. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=twitter
  46. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=facebook
  47. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=google-plus-1
  48. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=skype
  49. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=linkedin
  50. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/#print
  51. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=pocket
  52. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=tumblr
  53. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=reddit
  54. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=telegram
  55. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/?share=pinterest
  56. https://api.whatsapp.com/send?text=deep learning from scratch iv: id119 and id26 http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  57. http://www.deepideas.net/author/daniel/
  58. http://www.deepideas.net/category/artificial-intelligence/
  59. http://www.deepideas.net/category/artificial-intelligence/machine-learning/deep-learning/
  60. http://www.deepideas.net/category/artificial-intelligence/machine-learning/
  61. http://www.deepideas.net/category/python/
  62. http://www.deepideas.net/category/artificial-intelligence/machine-learning/tensorflow/
  63. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/#comments
  64. https://twitter.com/deepideas_net
  65. http://fb.me/deepideas.net
  66. https://twitter.com/deepideas_net
  67. mailto:daniel@sabinasz.net
  68. http://www.deepideas.net/feed/
  69. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  70. https://deepideas.tumblr.com/

   hidden links:
  72. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  73. https://www.facebook.com/deepideas.net/
