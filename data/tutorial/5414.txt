   #[1]deep ideas    feed [2]deep ideas    comments feed [3]deep ideas   
   deep learning from scratch ii: id88s comments feed [4]alternate
   [5]alternate

   [6]facebook[7]twitter[8]email[9]rss[10]linkedin[11]tumblr

   [12]deep ideas logo deep ideas retina logo

a blog on artificial intelligence, deep learning and cognitive science

     * [13]home
     * [14]list of articles
     * [15]about me
     * ____________________
          

   [16]previous [17]next

     * [18]view larger image

deep learning from scratch ii: id88s

   this is part 2 of a series of tutorials, in which we develop the
   mathematical and algorithmic underpinnings of deep neural networks from
   scratch and implement our own neural network library in python,
   mimicing the [19]tensorflow api. start with the first part: [20]i:
   computational graphs.
     * [21]part i: computational graphs
     * [22]part ii: id88s
     * [23]part iii: training criterion
     * [24]part iv: id119 and id26
     * [25]part v: multi-layer id88s
     * [26]part vi: tensorflow

id88s

a motivating example

   id88s are a miniature form of neural network and a basic building
   block of more complex architectures. before going into the details,
   let   s motivate them by an example. assume that we are given a dataset
   consisting of 100 points in the plane. half of the points are red and
   half of the points are blue.

   iframe:
   [27]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132302/example

   iframe:
   [28]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132303/example

   as we can see, the red points are centered at $(-2, -2)$ and the blue
   points are centered at $(2, 2)$. now, having seen this data, we can ask
   ourselves whether there is a way to determine if a point should be red
   or blue. for example, if someone asks us what the color of the point
   $(3, 2)$ should be, we   d best respond with blue. even though this point
   was not part of the data we have seen, we can infer this since it is
   located in the blue region of the space.

   but what is the general rule to determine if a point is more likely to
   be blue than red? apparently, we can draw a line $y = -x$ that nicely
   separates the space into a red region and a blue region:

   iframe:
   [29]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132304/example

   we can implicitly represent this line using a weight vector $w$ and a
   bias $b$. the line then corresponds to the set of points $x$ where

   $$w^t x + b = 0.$$

   in the case above, we have $w = (1, 1)^t$ and $b = 0$. now, in order to
   test whether the point is blue or red, we just have to check whether it
   is above or below the line. this can be achieved by checking the sign
   of $w^t x + b$. if it is positive, then $x$ is above the line. if it is
   negative, then $x$ is below the line. let   s perform this test for our
   example point $(3, 2)^t$:

   $$
   \begin{pmatrix}
   1 & 1
   \end{pmatrix}
   \cdot \begin{pmatrix}
   3 \\
   2
   \end{pmatrix} = 5
   $$

   since 5 > 0, we know that the point is above the line and, therefore,
   should be classified as blue.

id88 definition

   in general terms, a classifier is a function $\hat{c} : \mathbb{r}^d
   \rightarrow \{1, 2,    , c\}$ that maps a point onto one of $c$ classes.
   a binary classifier is a classifier where $c = 2$, i.e. we have two
   classes. a id88 with weight $w \in \mathbb{r}^d$ and bias $b \in
   \mathbb{r}^d$ is a binary classifier where

   $$
   \hat{c}(x) =
   \begin{cases}
   1, & \text{if } w^t x + b \geq 0 \\
   2, & \text{if } w^t x + b < 0
   \end{cases}
   $$

   $\hat{c}$ partitions $\mathbb{r}^d$ into two half-spaces, each
   corresponding to one of the two classes. in the 2-dimensional example
   above, the partitioning is along a line. in general, the partitioning
   is along a $d-1$ dimensional hyperplane.

from classes to probabilities

   depending on the application, we may be interested not only in
   determining the most likely class of a point, but also the id203
   with which it belongs to that class. note that the higher the value of
   $w^t x + b$, the higher is its distance to the separating line and,
   therefore, the higher is our confidence that it belongs to the blue
   class. but this value can be arbitrarily high. in order to turn this
   value into a id203, we need to    squash    the values to lie between
   0 and 1. one way to do this is by applying the sigmoid function
   $\sigma$:

   $$p(\hat{c}(x) = 1 \mid x) = \sigma(w^t x + b)$$

   where $$\sigma(a) = \frac{1}{1 + e^{-a}}$$

   let   s take a look at what the sigmoid function looks like:

   iframe:
   [30]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132305/sigmoid%20plot

   as we can see, the sigmoid function assigns a id203 of 0.5 to
   values where $w^t x + b = 0$ (i.e. points on the line) and asymptotes
   towards 1 the higher the value of $w^t x + b$ becomes, and towards 0
   the lower it becomes, which is exactly what we want.

   let   s now define the sigmoid function as an operation, since we   ll need
   it later:

   iframe:
   [31]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132306/sigmoid

   the entire computational graph of the id88 now looks as follows:

   [id88.png?1]

example

   using what we have learned, we can now build a id88 for the
   red/blue example in python.

   iframe:
   [32]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132307/building%20a%20id88

   let   s use this id88 to compute the id203 that $(3, 2)^t$ is
   a blue point:

   iframe:
   [33]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132308/using%20the%20id88

multi-class id88

   so far, we have used the id88 as a binary classifier, telling us
   the id203 $p$ that a point $x$ belongs to one of two classes. the
   id203 of $x$ belonging to the respective other class is then
   given by $1-p$. generally, however, we have more than two classes. for
   example, when classifying an image, there may be numerous output
   classes (dog, chair, human, house,    ). we can extend the id88 to
   compute multiple output probabilities.

   let $c$ denote the number of output classes. we want to perform
   multiple linear classifications in parallel, one for each of the $c$
   classes. to do this, we introduce a weight vector for each class ($w_1,
   w_2,    , w_c$) and a bias value for each class ($b_1, b_2,    , b_c$). the
   output of our id88 should now be a $c$-dimensional vector, each
   of whose entries contains the id203 of the respective class:

   $$
   output = \left(
   \begin{array}{c}
   \sigma(w_1^tx + b_1)\\
   \sigma(w_2^tx + b_2)\\
      \\
   \sigma(w_c^tx + b_c)\\
   \end{array}
   \right)
   $$

   in order to do this efficiently, we arrange the weight vectors in a
   matrix $w = (w_1, w_2,    , w_c) \in \mathbb{r}^{d \times c}$ whose
   columns correspond to the $w_i$s. similarly, we arrange the bias values
   in a vector $b = (b_1, b_2,    , b_c)^t \in \mathbb{r}^c$.

   instead of computing each entry of the output individually, we do the
   following:
    1. compute $x \cdot w$, which yields the following:
       $$
       \left(
       \begin{array}{c}
       w_1^tx\\
       w_2^tx\\
          \\
       w_c^tx\\
       \end{array}
       \right)
       $$
    2. add the bias vector $b$, which yields the following:
       $$
       \left(
       \begin{array}{c}
       w_1^tx + b_1\\
       w_2^tx + b_2\\
          \\
       w_c^tx + b_c\\
       \end{array}
       \right)
       $$
    3. apply the sigmoid element-wise, which yields our intended output:
       $$
       \left(
       \begin{array}{c}
       \sigma(w_1^tx + b_1)\\
       \sigma(w_2^tx + b_2)\\
          \\
       \sigma(w_c^tx + b_c)\\
       \end{array}
       \right)
       $$

   the following computational graph describes this procedure:

   [id882.png?1]

softmax

   when the output classes are disjoint, the output probabilities should
   sum up to one. in this case, we apply the softmax function to the
   vector $a = xw + b$ instead of the element-wise sigmoid, which makes
   sure that each id203 is between 0 and 1 and the sum of the
   probabilities is 1:

   $$
   \sigma(a)_i = \frac{e^{a_i}}{\sum_{j = 1}^c e^{a_j}}
   $$

   iframe:
   [34]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132309/softmax

batch computation

   the matrix form allows us to feed in more than one point at a time.
   that is, instead of a single point $x$, we could feed in a matrix $x
   \in \mathbb{r}^{n \times d}$ containing one point per row (i.e. $n$
   rows of $d$-dimensional points). we refer to such a matrix as a batch.
   instead of $xw$, we compute $xw$. this returns an $n \times c$ matrix,
   each of whose rows contains $xw$ for one point $x$. to each row, we add
   a bias vector $b$, which is now an $1 \times m$ row vector. the whole
   procedure thus computes a function $f : \mathbb{r}^{n \times d}
   \rightarrow \mathbb{r}^{m}$ where $f(x) = \sigma(xw + b)$. the
   computational graph looks as follows:

   [id883.png?2]

example

   let   s now generalize our red/blue id88 to allow for batch
   computation and multiple output classes.

   iframe:
   [35]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/id88s/132310/multi-class%20id88

   since the first 10 points in our data are all blue, the id88
   outputs high probabilities for blue (left column) and low probabilities
   for red (right column), as expected.

   if you have any questions, feel free to leave a comment. otherwise,
   continue with the next part: [36]iii: training criterion

share this:

     * [37]click to share on twitter (opens in new window)
     * [38]click to share on facebook (opens in new window)
     * [39]click to share on google+ (opens in new window)
     * [40]click to share on skype (opens in new window)
     * [41]click to share on linkedin (opens in new window)
     * [42]click to print (opens in new window)
     * [43]click to share on pocket (opens in new window)
     * [44]click to share on tumblr (opens in new window)
     * [45]click to share on reddit (opens in new window)
     * [46]click to share on telegram (opens in new window)
     * [47]click to share on pinterest (opens in new window)
     * [48]click to share on whatsapp (opens in new window)
     *

related

   by [49]daniel| 2017-11-18t14:37:42+00:00 august 26th,
   2017|[50]artificial intelligence, [51]deep learning, [52]machine
   learning, [53]python, [54]tensorflow|[55]5 comments

stay updated

   subscribe to the mailing list and get updated about new blog posts by
   email.
   ____________________
   [ ] i consent to my submitted data being collected via this form*
   sign up now

   thank you for subscribing.

   something went wrong.

   i respect your privacy and take protecting it seriously

follow me on twitter

   [56]my tweets

   copyright 2017 daniel sabinasz
   [57]facebook[58]twitter[59]email[60]rss[61]linkedin[62]tumblr

references

   visible links
   1. http://www.deepideas.net/feed/
   2. http://www.deepideas.net/comments/feed/
   3. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/feed/
   4. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
   5. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/&format=xml
   6. http://fb.me/deepideas.net
   7. https://twitter.com/deepideas_net
   8. mailto:daniel@sabinasz.net
   9. http://www.deepideas.net/feed/
  10. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  11. https://deepideas.tumblr.com/
  12. http://www.deepideas.net/
  13. http://www.deepideas.net/
  14. http://www.deepideas.net/list-of-articles/
  15. http://www.deepideas.net/about-me/
  16. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  17. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  18. https://i2.wp.com/www.deepideas.net/wp-content/uploads/2017/08/neuron.jpg?fit=900,300
  19. http://www.tensorflow.org/
  20. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  21. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  22. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  23. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  24. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  25. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  26. http://www.deepideas.net/deep-learning-from-scratch-vi-tensorflow/
  27. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132302/example
  28. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132303/example
  29. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132304/example
  30. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132305/sigmoid plot
  31. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132306/sigmoid
  32. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132307/building a id88
  33. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132308/using the id88
  34. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132309/softmax
  35. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/id88s/132310/multi-class id88
  36. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  37. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=twitter
  38. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=facebook
  39. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=google-plus-1
  40. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=skype
  41. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=linkedin
  42. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/#print
  43. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=pocket
  44. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=tumblr
  45. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=reddit
  46. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=telegram
  47. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/?share=pinterest
  48. https://api.whatsapp.com/send?text=deep learning from scratch ii: id88s http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  49. http://www.deepideas.net/author/daniel/
  50. http://www.deepideas.net/category/artificial-intelligence/
  51. http://www.deepideas.net/category/artificial-intelligence/machine-learning/deep-learning/
  52. http://www.deepideas.net/category/artificial-intelligence/machine-learning/
  53. http://www.deepideas.net/category/python/
  54. http://www.deepideas.net/category/artificial-intelligence/machine-learning/tensorflow/
  55. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/#comments
  56. https://twitter.com/deepideas_net
  57. http://fb.me/deepideas.net
  58. https://twitter.com/deepideas_net
  59. mailto:daniel@sabinasz.net
  60. http://www.deepideas.net/feed/
  61. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  62. https://deepideas.tumblr.com/

   hidden links:
  64. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  65. https://www.facebook.com/deepideas.net/
