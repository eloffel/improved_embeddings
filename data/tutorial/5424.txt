   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id180 in neural networks

sigmoid, tanh, softmax, relu, leaky relu explained !!!

   [16]go to the profile of sagar sharma
   [17]sagar sharma (button) blockedunblock (button) followfollowing
   sep 6, 2017
   [1*gipiadqyoa8wuokhal-mjg.gif]

what is activation function?

     it   s just a thing function that you use to get the output of node.
     it is also known as transfer function.
     __________________________________________________________________

why we use id180 with neural networks?

     it is used to determine the output of neural network like yes or no.
     it maps the resulting values in between 0 to 1 or -1 to 1 etc.
     (depending upon the function).

   the id180 can be basically divided into 2 types-
    1. linear activation function
    2. non-linear id180

     fyi: the cheat sheet is given below.
     __________________________________________________________________

linear or identity activation function

   as you can see the function is a line or linear. therefore, the output
   of the functions will not be confined between any range.
   [1*tldigydqwqm-smwp7m3bww.png]
   fig: linear activation function

   equation : f(x) = x

   range : (-infinity to infinity)

   it doesn   t help with the complexity or various parameters of usual data
   that is fed to the neural networks.

non-linear activation function

   the nonlinear id180 are the most used activation
   functions. nonlinearity helps to makes the graph look something like
   this
   [1*cxnqe_cmez7vuikcluh8pa.png]
   fig: non-linear activation function

   it makes it easy for the model to generalize or adapt with variety of
   data and to differentiate between the output.

   the main terminologies needed to understand for nonlinear functions
   are:

     derivative or differential: change in y-axis w.r.t. change in
     x-axis.it is also known as slope.

     monotonic function: a function which is either entirely
     non-increasing or non-decreasing.

   the nonlinear id180 are mainly divided on the basis of
   their range or curves-

1. sigmoid or logistic activation function

   the sigmoid function curve looks like a s-shape.
   [1*xu7b5y9gp0il5oobj7ltww.png]
   fig: sigmoid function

   the main reason why we use sigmoid function is because it exists
   between (0 to 1). therefore, it is especially used for models where we
   have to predict the id203 as an output.since id203 of
   anything exists only between the range of 0 and 1, sigmoid is the right
   choice.

   the function is differentiable.that means, we can find the slope of the
   sigmoid curve at any two points.

   the function is monotonic but function   s derivative is not.

   the logistic sigmoid function can cause a neural network to get stuck
   at the training time.

   the softmax function is a more generalized logistic activation function
   which is used for multiclass classification.

2. tanh or hyperbolic tangent activation function

   tanh is also like logistic sigmoid but better. the range of the tanh
   function is from (-1 to 1). tanh is also sigmoidal (s - shaped).
   [1*f9erbyysvjtjohffdnkjyq.jpeg]
   fig: tanh v/s logistic sigmoid

   the advantage is that the negative inputs will be mapped strongly
   negative and the zero inputs will be mapped near zero in the tanh
   graph.

   the function is differentiable.

   the function is monotonic while its derivative is not monotonic.

   the tanh function is mainly used classification between two classes.

     both tanh and logistic sigmoid id180 are used in
     feed-forward nets.

3. relu (rectified linear unit) activation function

   the relu is the most used activation function in the world right
   now.since, it is used in almost all the convolutional neural networks
   or deep learning.
   [1*xxxia0jjvprhejhd4z893g.png]
   fig: relu v/s logistic sigmoid

   as you can see, the relu is half rectified (from bottom). f(z) is zero
   when z is less than zero and f(z) is equal to z when z is above or
   equal to zero.

   range: [ 0 to infinity)

   the function and its derivative both are monotonic.

   but the issue is that all the negative values become zero immediately
   which decreases the ability of the model to fit or train from the data
   properly. that means any negative input given to the relu activation
   function turns the value into zero immediately in the graph, which in
   turns affects the resulting graph by not mapping the negative values
   appropriately.

4. leaky relu

   it is an attempt to solve the dying relu problem
   [1*a_bzn0cjugoxtpcjknklqa.jpeg]
   fig : relu v/s leaky relu

   can you see the leak?     

   the leak helps to increase the range of the relu function. usually, the
   value of a is 0.01 or so.

   when a is not 0.01 then it is called randomized relu.

   therefore the range of the leaky relu is (-infinity to infinity).

   both leaky and randomized relu functions are monotonic in nature. also,
   their derivatives also monotonic in nature.

why derivative/differentiation is used ?

     when updating the curve, to know in which direction and how much to
     change or update the curve depending upon the slope.that is why we
     use differentiation in almost every part of machine learning and
     deep learning.

   [1*p_hyqatyi8pbt2kel6sioq.png]
   fig: activation function cheetsheet
   [1*n1hfbpwv21fcazgjmwt1sg.png]
   fig: derivative of id180
     __________________________________________________________________

   [1*b_eoc2l6eikmgqrkj4g_lg.gif]

   i will be posting 2 posts per week so don   t miss the tutorial.

   so, follow me on [18]medium, [19]facebook, [20]twitter, [21]linkedin,
   [22]google+, [23]quora to see similar posts.

   any comments or if you have any question, write it in the comment.

   clap it! share it! follow me!

   happy to be helpful. kudos   ..
     __________________________________________________________________

previous stories you will love:

   [24]what is id75 and how does it work? - theffork
   it is a method used for predicting future values by finding a linear
   pattern in the previously given data. the linear   theffork.com
   [25]what the hell is    tensor    in    tensorflow   ?
   i didn   t know it   hackernoon.com
   [26]epoch vs batch size vs iterations
   know your code   towardsdatascience.com
   [27]id169
   mcts for every data science enthusiasttowardsdatascience.com
   [28]policy networks vs value networks in id23
   in id23, the agents take random decisions in their
   environment and learns on selecting the right
   one   towardsdatascience.com
   [29]tensorflow image recognition python api tutorial
   on cpu with inception-v3(in seconds)towardsdatascience.com
   [30]how to send emails using python
   design professional mails using flask!medium.com

     * [31]machine learning
     * [32]activation function
     * [33]neural networks
     * [34]artificial intelligence
     * [35]deep learning

   (button)
   (button)
   (button) 7.7k claps
   (button) (button) (button) 37 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of sagar sharma

[37]sagar sharma

   [38]https://theffork.com i   m interested in programming (python, c++),
   arduino, machine learning :). i also like to w

     (button) follow
   [39]towards data science

[40]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 7.7k
     * (button)
     *
     *

   [41]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [42]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1cbd9f8d91d6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_jlaeg0nnwizj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sagarsharma4244?source=post_header_lockup
  17. https://towardsdatascience.com/@sagarsharma4244
  18. https://medium.com/@sagarsharma4244
  19. https://www.facebook.com/profile.php?id=100003188718299
  20. https://twitter.com/sagarsharma4244
  21. https://www.linkedin.com/in/sagar-sharma-232a06148/
  22. https://plus.google.com/u/0/+sagarsharma4244
  23. https://www.quora.com/profile/sagar-sharma-71
  24. https://theffork.com/what-is-linear-regression-and-how-does-it-work/
  25. https://hackernoon.com/what-the-hell-is-tensor-in-tensorflow-e40dbf0253ee
  26. https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9
  27. https://towardsdatascience.com/monte-carlo-tree-search-158a917a8baa
  28. https://towardsdatascience.com/policy-networks-vs-value-networks-in-reinforcement-learning-da2776056ad2
  29. https://towardsdatascience.com/tensorflow-image-recognition-python-api-e35f7d412a70
  30. https://medium.com/@sagarsharma4244/how-to-send-emails-using-python-4293dacc57d9
  31. https://towardsdatascience.com/tagged/machine-learning?source=post
  32. https://towardsdatascience.com/tagged/activation-functions?source=post
  33. https://towardsdatascience.com/tagged/neural-networks?source=post
  34. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  35. https://towardsdatascience.com/tagged/deep-learning?source=post
  36. https://towardsdatascience.com/@sagarsharma4244?source=footer_card
  37. https://towardsdatascience.com/@sagarsharma4244
  38. https://theffork.com/
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://theffork.com/what-is-linear-regression-and-how-does-it-work/
  45. https://hackernoon.com/what-the-hell-is-tensor-in-tensorflow-e40dbf0253ee
  46. https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9
  47. https://towardsdatascience.com/monte-carlo-tree-search-158a917a8baa
  48. https://towardsdatascience.com/policy-networks-vs-value-networks-in-reinforcement-learning-da2776056ad2
  49. https://towardsdatascience.com/tensorflow-image-recognition-python-api-e35f7d412a70
  50. https://medium.com/@sagarsharma4244/how-to-send-emails-using-python-4293dacc57d9
  51. https://medium.com/p/1cbd9f8d91d6/share/twitter
  52. https://medium.com/p/1cbd9f8d91d6/share/facebook
  53. https://medium.com/p/1cbd9f8d91d6/share/twitter
  54. https://medium.com/p/1cbd9f8d91d6/share/facebook
