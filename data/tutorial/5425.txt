   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

multi-state lstms for categorical features

   [16]go to the profile of kwyk
   [17]kwyk (button) blockedunblock (button) followfollowing
   aug 7, 2017

context

   neural networks are now widely used in many ways. from image caption
   generation to breast cancer prediction, this great diversity of
   applications is a natural consequence of the important variety of
   neural architectures (feed forward neural networks, convolutional
   neural networks, etc   ). among all these architectures, long short term
   memory (lstm)         a particular case of recurrent neural networks         have
   proven very successful on tasks such as [18]machine translation,
   [19]time series prediction or generally anything where the data is
   sequential. this is mainly due to their ability to memorize relatively
   long term dependencies, which is achieved by taking into account
   previous information for further predictions.

     but lstms alone aren   t always enough. sometimes we need to tweak
     these layers and adapt them to the task at hand.

   at [20]kwyk, we provide online math exercises. and every time an
   exercise is answered, we collect some data which is then used to tailor
   homework for each student using the website. in order to determine
   which exercises are the most likely to make a student progress, we need
   to know how likely he/she is to succeed on each exercise at any given
   point in time. and by being able to correctly predict these
   probabilities of success we can choose the homework that maximizes the
   overall progress.

   this post aims to introduce our modified lstm cell, the    multi-state
   lstm   , which we based our model on to try and solve this problem.

a quick data overview

   when an exercise is answered, we save information about both the
   student (or    user   ) and the exercise, along with an additional score
   value that is either (0) or (1) depending the user   s success. the
   information we collect is in the form of categorical features that tell
   us :    which exercise is it ?   ,    what chapter is it ?   ,    what student is
   it ?   ,    from which grade is he/she ?        this results in features that
   have from tens to thousands of modalities, which we use to predict the
      score    variable and get success probabilities.
   [1*z0a4qbm1kaezirdabjew-a.png]
   a sample of our data

lstm-based model

   there are two main motivations behind the choice of an lstm-based
   model.

   the first one comes from the fact that all the features we   re using are
   categorical. that   s the real source of information we want to learn
   from. and by manually cooking features such as moving averages of
   previous scores, we introduce biases due to the subjective choices we
   make. therefore, we chose to rely on neural networks, directly feed our
   data and let the features form automatically and    objectively   .

   the second motivation concerns the choice of lstms among all available
   architectures. this is simply due to our belief that for each student
   the id203 of success on each exercise depends on all previous
   results in time and therefore is sequential.

     with this in mind, we thought of an architecture where for each
     categorical feature, a shared lstm would keep a history of previous
     results for each modality.

   let   s take the    user_id    feature for example. in this case, we need to
   adapt an lstm cell to memorize on the fly but independently for each
   student a history of how he/she was successful in the past. this is
   done by adding what we call a    multi-state    to the basic lstm cell :
   [1*ad9h0gbvw12diz2neb6lxw.png]
   left : basic lstm cell / right : multi-state lstm cell. note : if you
   are not familiar with lstms you can refer to [21]this great post by
   christopher olah.

   every time a student   s label is fed to the multi-state lstm cell, it
   starts by looking up the corresponding state and previous score. then,
   the student   s state is sent to update a shared lstm cell. in meantime,
   the previous score is fed to this lstm cell which produces an output.
   next, the resulting state goes back to update the student   s data in the
   multi-state. finally, once we observe the actual score, this value is
   in turn sent to update the student   s previous score in the multi-state.

   this modified version of the basic lstm cell works similarly but has an
   additional advantage : it can now directly take in a sequence of
   student labels as inputs :
   [1*oecftxiquao4kldofwr_zg.png]
   an unrolled multi-state lstm.

   for the sake of our previous example we considered the    user_id   
   categorical feature, but this can actually be applied to all of our
   categorical features. in fact, using a multi-state lstm on the
      exercise_id    feature for example would result in it learning
   historical success rates for each exercise. from here, the complete
   network we used is pretty straightforward :
   [1*r9i7vzqh7cfsimarnxg71q.png]
   the complete graph we used to solve our problem.

   all in all, this architecture tries to learn separate histories for
   each modality of each categorical feature then uses the lstm   s outputs
   to predict a success id203 for a given student on a given
   exercise.

results

baseline

   to assess our lstm-based model we need a baseline. before trying any
   deep learning methods, we used to have a boosting algorithm
   ([22]gradient boosting) that used some handmade features. from all the
   categorical features, we cooked up some fast and slow [23]moving
   averages of previous scores per each modality of each feature. then, we
   fed these features to a gradient boosting classifier and let the magic
   happen.

comparison

   both classifiers have the same approach. in fact, the baseline model
   uses handmade moving averages as features and the lstm-based method
   automatically learns these histories to predict the scores. sure
   enough, both algorithms got similar accuracy scores (baseline : 74.61%,
   lstm-based network : 75.34%). but the predicted distributions are very
   different :
   [1*fw4wfcywfabosx-pje-ymw.png]
   we can see that the lstm-based model is much better at segregating
   students.

   in fact, the lstm-based model seems to separate 3 to 4 different
   populations :
     * one for students who shouldn   t be able to answer correctly
       (id203<10%)
     * one for those who have a moderate to good chance to succeed
       (50<id203<80%). these can be students for whom we don   t have
       enough data, or simply good-average students.
     * one for those who have a very important chance of succeeding
       (id203>90%).

   conversely, the baseline only predicts a skewed normal distribution
   around the overall average success rate with no particular
   discrimination.

   but to further investigate the difference between the two models, we
   need to have a more detailed look at the mse. in fact, judging by the
   mse alone, the two models have similar performance :
     * baseline : 0.17
     * lstm-based network : 0.16

   but as we discussed in a [24]previous post, when real randomness is
   associated with the targets (here, predicting human behavior) we get a
   better assessment of the performance of a classifier by looking at its
   reliability measure (rel).

   reliability is computed by sorting the data into bins of similar
   predicted probabilities. then, we compute an mse for each bin against
   the average target in the bin. finally, we take an overall average to
   get our rel value.

   the metrics obtained for each model are :
     * baseline : 3.86 e-3
     * lstm-based network : 2.86 e-4

   indeed, we see that the lstm-based network is more than 10 times more
   precise than the gradient boosting baseline, which should explain the
   better segregation we observed when comparing both distributions.

conclusion

   to sum up, we   ve seen on a real life example how lstms can be effective
   for sequential data classification. but we also saw how our modified
   version of the lstm         the    multi-state lstm            was able to reach a
   greater performance without any preprocessing or feature engineering at
   all. in fact, this new lstm cell can directly take in a sequence of
   labels as inputs, which means that it can be used categorical features
   only and still produce good results.

   to further improve on this multi-state lstm, a next step would be to
   take into account the correlations between multiple labels. in fact,
   when predicting the performance of a student on a given pair of similar
   exercises, the predicted probabilities should be very similar. one way
   to try and ensure this behavior could be to integrate an [25]exercise
   embedding into the network and let it learn the dependencies.

   author : [26]hicham el boukkouri

     * [27]machine learning
     * [28]deep learning
     * [29]data science
     * [30]education
     * [31]neural networks

   (button)
   (button)
   (button) 494 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of kwyk

[33]kwyk

   startup company in education

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 494
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/66cc974df1dc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-state-lstms-for-categorical-features-66cc974df1dc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/multi-state-lstms-for-categorical-features-66cc974df1dc&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_e0vgmfdz5dsh---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@kwykedu?source=post_header_lockup
  17. https://towardsdatascience.com/@kwykedu
  18. https://www.tensorflow.org/tutorials/id195
  19. http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
  20. https://www.kwyk.fr/
  21. http://colah.github.io/posts/2015-08-understanding-lstms/
  22. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.gradientboostingclassifier.html#sklearn.ensemble.gradientboostingclassifier
  23. https://en.wikipedia.org/wiki/moving_average#exponential_moving_average
  24. https://medium.com/towards-data-science/metrics-random-processes-in-classification-fd5bafa79505
  25. https://medium.com/towards-data-science/a-non-nlp-application-of-id97-c637e35d3668
  26. https://www.linkedin.com/in/hichamelboukkouri
  27. https://towardsdatascience.com/tagged/machine-learning?source=post
  28. https://towardsdatascience.com/tagged/deep-learning?source=post
  29. https://towardsdatascience.com/tagged/data-science?source=post
  30. https://towardsdatascience.com/tagged/education?source=post
  31. https://towardsdatascience.com/tagged/neural-networks?source=post
  32. https://towardsdatascience.com/@kwykedu?source=footer_card
  33. https://towardsdatascience.com/@kwykedu
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/66cc974df1dc/share/twitter
  40. https://medium.com/p/66cc974df1dc/share/facebook
  41. https://medium.com/p/66cc974df1dc/share/twitter
  42. https://medium.com/p/66cc974df1dc/share/facebook
