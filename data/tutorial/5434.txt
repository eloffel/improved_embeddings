   #[1]intel ai    feed [2]intel ai    comments feed [3]alternate
   [4]alternate

   search ____________________ search

   [5]intel ai
     * ____________________
     * [6]technologies
          + [7]frameworks
          + [8]hardware
          + [9]software libraries
          + [10]research projects
          + [11]tools
          + [12]featured solutions
     * [13]industries
          + [14]health & life sciences
          + [15]telecommunications
          + [16]retail
          + [17]media & entertainment
          + [18]financial services (fsi)
          + [19]energy
          + [20]government
     * [21]library
          + [22]blog
          + [23]videos
          + [24]white papers
          + [25]podcasts
          + [26]docs
          + [27]news
     * [28]research
          + [29]publications
          + [30]researchers
          + [31]research programs
          + [32]research projects
     * [33]careers
     * programs
          + [34]partners
          + [35]research residency
          + [36]intel ai developer program
          + [37]ai student ambassadors
          + [38]ai4socialgood
     * [39]blog

   ____________________
   show the menu

   [40]blog
   authors
   jd co-reyes

   [41]jd co-reyes

   nervana systems intern
   tags

   [42]image/video captioning [43]language models

   [44]blog
   11.04.15

   11.04.15

intern spotlight: implementing language models

recurrent neural networks

   during my internship at nervana systems, i got to implement a few
   language models using recurrent neural networks (id56   s) and achieved a
   significant speedup in training image captioning models. id56   s are good
   at learning relationships over sequences of data. so for example, a id56
   could be fed characters of shakespearean text, learn an internal
   representation of what shakespearean text looks like, and then sample
   its predictions to generate some new shakespeare. a vanilla id56 inputs
   a sequence of data, computes some hidden state over time, and then
   outputs a sequence.

   understanding lstms

   ^(christopher olah,
   [45]http://colah.github.io/posts/2015-08-understanding-lstms/)

   a variant of the id56, the long short term memory network (lstm), helps
   in resolving certain issues the id56 has including learning long term
   dependencies. using a lstm in nervana   s deep learning library,
   [46]neon   , i was able to produce some coherent shakespeare-like text:

   baptista: where is the matter? what says my brother?

   benvolio: i will not be a soldier.

   don adriano de armado: i will not be a consent of my soul.

   aemelia: i have seen thee as the sea of the state of the state

   lstm   s have also achieved state of the art at other nlp tasks like
   id103, translation, and id53 (see andrej
   karpathy   s excellent [47]id56 blog post).

image captioning and optimizing lstm   s

   a downside of using lstm   s is that training can take a long time
   especially if the number of time steps used is large.  one of my
   projects involved implementing an image captioning model in neon    and
   optimizing training time. the image caption model from google
   ([48]paper) takes the output features of the second to last layer of a
   convolutional neural network (id98) and then feeds those to a lstm to
   generate a sentence. this model treats the image features as the first
   word of a sentence and then recursively predicts the next word until a
   stop token is predicted.

   optimizing lstm   s

   ^(vinyals et al., 2015)

   one way to speed up training is to process a batch of training examples
   at the same time. this lets you perform large id127s
   which are optimized on a gpu. to deal with variable length sentences,
   we can pad shorter sentences up to the maximum sentence length and then
   apply a bit mask to ignore the padded parts. using the nvidia visual
   profiler, we can also look for periods where the gpu is not being fully
   utilized (indicated by gaps between bars below).

   using the nvidia visual profiler

   data loading can be performed asynchronously with computation. moving
   data from the cpu to the gpu is generally very expensive and should be
   minimized or done in parallel so that we can reduce the gaps and time
   where the gpu isn   t being used for computation. additionally, we can
   look at which kernel calls are being used the most and check if we can
   save computation by storing temporary values. certain parts of the lstm
   computation can be sped up by compounding id127s which
   can reduce the number of kernel calls made to the gpu and reduce
   latency. we also want to choose the correct sizes for partitioning the
   data to allow for optimal memory access. finally, at an even lower
   level, the various kernels themselves can be optimized for 100%
   utilization. with these tricks, training an image captioning modeling
   on the flickr8k dataset (approx. 8000 images and 40,000 sentences)
   takes around 20s for one iteration through the entire dataset on a
   maxwell gpu. existing code which only ran on a cpu ([49]neuraltalk)
   takes about 1.1hr for one iteration. this means roughly a 200 x speedup
   can be achieved on a gpu. the model can achieve reasonable performance
   in about 15 iterations so the difference in total training time is 5
   minutes versus 16.5hr. lower training time makes tweaking your models
   much easier because you can run multiple models with different
   hyperparameters simultaneously and then choose which model achieved the
   greatest performance.

internship experience

   interning at nervana was an amazing experience. i was able to work on a
   few big projects in deep learning including googlenet, image
   captioning, and helping implement a fast lstm. i was given complete
   responsibility in how i wanted to complete these projects and was also
   able to shape how large parts of the neon    library worked to make it
   more modular and extensible. the people at nervana systems are
   extremely smart and collaborative. i could ask for help from my mentor
   on a variety of problems and he would immediately know a solution for
   things like how to optimize code for the gpu or how to design simple
   network layers. in addition, i could ask questions and gain insight
   from the various experts there in fields ranging from gpu programming
   to distributed computing.

   the workplace and living in san diego were also quite fun. san diego is
   famous for its craft beer and on weekends we would visit the local
   breweries. most of us were into the outdoors and the interns would
   organize regular outings for surfing, rock climbing, tennis, and
   mountain biking. co-workers would hold events like picnics at the park,
   house warming parties, and beer mile races. the office had an endless
   supply of snacks, its own squat rack, a 2 wheel self balancing scooter,
   and two cute dogs that co-workers brought in. overall, i had an awesome
   experience while being able to build scalable deep learning models.

   jd co-reyes
   algorithms intern
   citations:
   [figure 1.
   [50]http://colah.github.io/posts/2015-08-understanding-lstms/]
   [figure 2. [51]http://arxiv.org/pdf/1411.4555v2.pdf]

   authors
   jd co-reyes

   [52]jd co-reyes

   nervana systems intern
   tags

   [53]image/video captioning [54]language models

   related posts
     * [55]analyzing and understanding visual data
       currently, more than 75% of all internet traffic is visual
       (video/images). total traffic is exploding, projected to jump from
       1.2   
       [56]read more
     * [57]ai-based sports production with intel technology
    [58]ai-based sports production with intel technology
       keemotion, an independent software vendor (isv) and member of the
       intel   ai builders program, provides broadcasters, leagues, and
       sports teams   
       [59]read more
     * [60]neon-v1.1.5
    [61]neon v1.1.0 released!
       highlights from this new release include: * id31
       support (lstm lookuptable based), new imdb example network *
       support for   
       [62]read more
     * [63]neon v1.1.3 released!
       highlights from this release include: * deconvolution and weight
       histogram visualization examples and documentation * cpu
       convolution and pooling layer   
       [64]read more

   stay connected
   keep tabs on all the latest news with our monthly newsletter.
   (button) subscribe
     * [65]@intelai
     * [66]@intelaidev
     * [67]@intelai
     * [68]intel-ai
     * [69]nervanasystems

   (button) x

stay connected with intel   ai

   first name * ____________________
   last name * ____________________
   business email address * ____________________
   country/region * [please select...____________________________]
   job title * ____________________
   company * ____________________
   profession * [please select..._____________________________]
   [x] yes, i would like to subscribe to the intel ai newsletter to stay
   connected to the latest intel technologies and industry trends by
   email. i can unsubscribe at any time. *
   (button) submit

   by submitting this form, you are confirming you are an adult 18 years
   or older and you agree to share your personal information with intel to
   use for this business request. you also agree to subscribe to stay
   connected to the latest intel technologies and industry trends by
   email. you may unsubscribe at any time. intel   s web sites and
   communications are subject to our [70]privacy notice and [71]terms of
   use.

   thank you!

   sorry, there was an error in your submission.
     *   intel corporation
     * [72]terms of use
     * [73]*trademarks
     * [74]privacy
     * [75]cookies
     * [76]supply chain transparency
     * [77]site map

references

   visible links
   1. https://www.intel.ai/feed/
   2. https://www.intel.ai/comments/feed/
   3. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/intern-spotlight-implementing-language-models/
   4. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/intern-spotlight-implementing-language-models/&format=xml
   5. https://www.intel.ai/
   6. https://www.intel.ai/technology/
   7. https://www.intel.ai/framework-optimizations/
   8. https://www.intel.ai/hardware/
   9. https://www.intel.ai/software-libraries/
  10. https://www.intel.ai/research-projects/
  11. https://www.intel.ai/tools/
  12. https://www.intel.ai/featured-solutions/
  13. https://www.intel.ai/industries/
  14. https://www.intel.ai/health-and-life-sciences/
  15. https://www.intel.ai/telecommunications/
  16. https://www.intel.ai/retail/
  17. https://www.intel.ai/media-and-entertainment/
  18. https://www.intel.ai/financial-services/
  19. https://www.intel.ai/energy/
  20. https://www.intel.ai/government/
  21. https://www.intel.ai/library/
  22. https://www.intel.ai/blog/
  23. https://www.intel.ai/videos/
  24. https://www.intel.ai/white-papers/
  25. https://www.intel.ai/podcasts/
  26. https://www.intel.ai/docs/
  27. https://www.intel.ai/news/
  28. https://www.intel.ai/research/
  29. https://www.intel.ai/publications/
  30. https://www.intel.ai/researchers/
  31. https://www.intel.ai/research-programs/
  32. https://www.intel.ai/research-projects/
  33. https://jobs.intel.com/page/show/artificial-intelligence-careers
  34. https://www.intel.ai/partners/
  35. https://www.intel.ai/research-programs/
  36. https://software.intel.com/en-us/ai-academy
  37. https://software.intel.com/en-us/ai-academy/ambassadors
  38. https://ai.intel.com/ai/ai4socialgood/
  39. https://www.intel.ai/blog/
  40. https://www.intel.ai/blog/
  41. https://www.intel.ai/bio/jd-co-reyes/
  42. https://www.intel.ai/library/?use-case=image-video-captioning
  43. https://www.intel.ai/library/?post_tag=language-models
  44. https://www.intel.ai/blog/
  45. http://colah.github.io/posts/2015-08-understanding-lstms/
  46. https://github.com/nervanasystems/neon
  47. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  48. http://simplecore.intel.com/nervana/wp-content/uploads/sites/55/2015/11/1411.4555v2.pdf
  49. https://github.com/karpathy/neuraltalk
  50. http://colah.github.io/posts/2015-08-understanding-lstms/
  51. http://arxiv.org/pdf/1411.4555v2.pdf
  52. https://www.intel.ai/bio/jd-co-reyes/
  53. https://www.intel.ai/library/?use-case=image-video-captioning
  54. https://www.intel.ai/library/?post_tag=language-models
  55. https://www.intel.ai/analyzing-and-understanding-visual-data/
  56. https://www.intel.ai/analyzing-and-understanding-visual-data/
  57. https://www.intel.ai/ai-based-sports-production-with-intel-technology/
  58. https://www.intel.ai/ai-based-sports-production-with-intel-technology/
  59. https://www.intel.ai/ai-based-sports-production-with-intel-technology/
  60. https://www.intel.ai/neon-v1-1-0-released/
  61. https://www.intel.ai/neon-v1-1-0-released/
  62. https://www.intel.ai/neon-v1-1-0-released/
  63. https://www.intel.ai/neon-v1-1-3-released/
  64. https://www.intel.ai/neon-v1-1-3-released/
  65. https://twitter.com/intelai
  66. https://twitter.com/intelaidev
  67. https://www.facebook.com/intelai/
  68. https://www.linkedin.com/company/3628074/
  69. https://github.com/nervanasystems
  70. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html?elqtrackid=e6a9dfea0f904f66aa7346e8f8e74476&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  71. https://www.intel.com/content/www/us/en/legal/terms-of-use.html?elqtrackid=8445f4bedfeb45dcb04ff6097e1001e8&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  72. https://www.intel.com/content/www/us/en/legal/terms-of-use.html
  73. https://www.intel.com/content/www/us/en/legal/trademarks.html
  74. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html
  75. https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html
  76. https://www.intel.com/content/www/us/en/policy/policy-human-trafficking-and-slavery.html
  77. https://www.intel.com/content/www/us/en/siteindex.html

   hidden links:
  79. https://www.intel.com/content/www/us/en/homepage.html
  80. https://www.intel.ai/intern-spotlight-implementing-language-models/#menus
  81. https://www.intel.ai/analyzing-and-understanding-visual-data/
  82. https://www.intel.ai/neon-v1-1-3-released/
  83. https://www.intel.ai/intern-spotlight-implementing-language-models/#top
