   #[1]intel ai    feed [2]intel ai    comments feed [3]alternate
   [4]alternate

   search ____________________ search

   [5]intel ai
     * ____________________
     * [6]technologies
          + [7]frameworks
          + [8]hardware
          + [9]software libraries
          + [10]research projects
          + [11]tools
          + [12]featured solutions
     * [13]industries
          + [14]health & life sciences
          + [15]telecommunications
          + [16]retail
          + [17]media & entertainment
          + [18]financial services (fsi)
          + [19]energy
          + [20]government
     * [21]library
          + [22]blog
          + [23]videos
          + [24]white papers
          + [25]podcasts
          + [26]docs
          + [27]news
     * [28]research
          + [29]publications
          + [30]researchers
          + [31]research programs
          + [32]research projects
     * [33]careers
     * programs
          + [34]partners
          + [35]research residency
          + [36]intel ai developer program
          + [37]ai student ambassadors
          + [38]ai4socialgood
     * [39]blog

   ____________________
   show the menu

   [40]blog
   authors
   anthony ndirango

   [41]anthony ndirango

   tags

   [42]id103 [43]model zoo [44]neon    [45]id103

   [46]blog
   12.08.16

   12.08.16

end-to-end id103 with neon

   by: anthony ndirango and [47]tyler lee

   speech is an intrinsically temporal signal. the information-bearing
   elements present in speech evolve over a multitude of timescales. the
   fine changes in air pressure at rates of hundreds to thousands of hertz
   convey information about the speakers, their location, and help us
   separate them from a noisy world. slower changes in the power spectrum
   of speech denote the progression of phonemes, the building blocks of
   the spoken word. beyond these are more slowly developing sequences of
   words, turning into phrases and narrative structure. there are,
   however, no strict demarcations between elements within and across
   timescales. instead, elements at each scale blend together, temporal
   context is critical, and silence is a rare and ambiguous indicator of
   the transition between elements. automatic id103 (asr)
   systems must make sense of this noisy multi-scale stream of data,
   transforming it into an accurate sequence of words.

   as of this writing, the most popular and successful approach to
   building id103 engines involves hybrid systems that
   combine deep neural networks (dnns) with an amalgam of [48]hidden
   markov models (id48s), [49]context-dependent phone models,
   [50]id165[51] language models, and sophisticated variants of
   [52]viterbi search algorithms. these are complex models that require
   elaborate training recipes and demand a fair amount of expertise on the
   part of the model-builder. if the success of deep learning has taught
   us anything, it   s that we can often replace complex, multi-faceted
   machine learning approaches with generic neural networks that are
   trained to optimize a differentiable cost function. when applied to
   id103, this approach, which we   ll loosely term    pure    dnn,
   has met with resounding success. it is now much easier to build a
   state-of-the-art large vocabulary continuous id103 (lvcsr)
   system as long as one has access to relatively large amounts of
   training data and sufficient computational resources.

   the goal of the present exposition is to provide a relatively simple
   guide to using neon to build a id103 system using    pure   
   dnns, following an approach pioneered by [53]graves and collaborators
   and further developed into a complete end-to-end asr pipeline by ai
   researchers at [54]baidu.  we will be releasing an [55]open-source
   implementation of our end-to-end id103 engine to
   complement this blog post. in its most rudimentary form, the system
   uses bi-directional recurrent neural networks (biid56s) to train a model
   to generate transcriptions directly from spectrograms without having to
   explicitly align the audio frames to the transcripts. instead, implicit
   alignment is done using graves    [56]connectionist temporal
   classification (ctc) algorithm.

   though    pure    dnn approaches now allow one to train lvcsr systems
   capable of state-of-the-art performance, an explicit decoding step    
   converting model outputs into sensible sequences of words     remains
   critical during evaluation. techniques for decoding are varied, often
   involving a mixture of weighted finite state transducers and neural
   network language models. this topic would require an in-depth article
   in its own right, and thus we have chosen to limit this article
   primarily to the training portion of the asr pipeline. when necessary,
   we provide the reader with external references to fill in these gaps to
   hopefully convey a complete view of what goes into constructing an
   end-to-end id103 engine.

   end-to-end id103 in its barest outline, an end-to-end
   id103 pipeline consists of three main components
    1. a feature extraction stage which takes raw audio signals (e.g. from
       a wav file) as inputs and generates a sequence of feature vectors,
       with one feature vector for a given frame of audio input. examples
       of outputs of the feature extraction stage include slices of the
       raw waveform, [57]spectrograms and the equally popular
       [58]mel-frequency cepstral coefficients (mfccs).

    2. an acoustic model which takes sequences of feature vectors as
       inputs and generates probabilities of either character or phoneme
       sequences conditioned on the feature vector input.

    3. a decoder which takes two inputs     the acoustic model   s outputs as
       well as a language model     and searches for the most likely
       transcript given the sequences generated by the acoustic model
       constrained by the linguistic rules encoded in the language model.


handling data

   an efficient mechanism for loading data is critical when building an
   end-to-end id103 system. we will take full advantage of
   the fact that neon, as of version 1.7, comes with a superb dataloader
      [59]aeon     that supports image, audio and video data. using aeon
   substantially simplifies our work as it allows us to train the acoustic
   model directly from raw audio files without worrying about explicitly
   pre-processing the data. furthermore, aeon allows us to easily specify
   the type of spectral features we   d like to use during training.

ingesting data

   typically, speech data is distributed as raw audio files in some
   standard audio format, along with a series of text files containing the
   corresponding transcripts. in many cases, the transcript file will
   contain lines of the form

   <path to audio file>, <transcription of speech in audio file>
   meaning that the listed path points to an audio file containing the
   listed transcript. however, in many cases, the path listed in the
   transcript file is not an absolute path, but a path relative to some
   assumed directory structure. to deal with the specifics of various data
   packaging scenarios, aeon requires that the user generate a    manifest
   file    that contains pairs of absolute paths, with one path pointing to
   the audio file and the other path pointing to the corresponding
   transcript. we refer the reader to neon   s speech example [include link]
   and the [60]aeon documentaton for further details.

   in addition to the manifest file, aeon also requires that the user
   provide the length of the longest utterance in the dataset as well as
   the length of the longest transcript. these lengths can be extracted
   while generating the manifest files, e.g. using the popular [61]sox
   program to extract the duration of audio files.

   training a deep neural network we build our acoustic models by training
   a deep neural network comprised of convolutional (conv) layers,
   bi-directional recurrent (biid56) layers and fully connected (fc) layers
   (essentially following    [62]deep speech 2   ) as shown schematically in
   the adjacent figure.

   with the exception of the output layer which uses a softmax activation
   function, all other layers employ the [63]relu activation function.

   as shown in the figure, the network takes spectral feature vectors as
   input. using the [64]aeon dataloader, neon currently supports four
   types of input features: raw waveforms, spectrograms, mel-frequency
   spectral coefficients (mfcss) and mel-frequency cepstral coefficients
   (mfccs). mfscs and mfccs are derived from spectrograms and essentially
   turn each column of a spectrogram into a relatively small number of
   independent coefficients which more closely align with the perceptual
   frequency scale of the human ear.  in our experiments, we have also
   observed that, all else being equal,  models trained with mel-features
   as inputs perform slightly better than models trained with
   spectrograms.

   the spectral inputs are fed into a conv layer. in general, one could
   consider architectures with multiple conv layers employing either 1d or
   2d convolutions. we will take advantage of strided convolutions which
   effectively allow the network to operate on    wider contexts    of the
   input. strided convolutions also reduce the overall length of the
   sequences, which in turn significantly reduces both the memory
   footprint and the number of computations carried out by the network.
   this allows us to train even deeper models which leads to improved
   performance without significantly increasing the required computational
   resources.

   the outputs from the conv layer(s) feed into a stack of biid56 layers.
   each biid56 layer is comprised of a pair of id56s running in tandem, with
   the input sequence presented in opposite directions as indicated in the
   figure.

   processing speech signals

   the outputs from the pair of id56s are then concatenated as shown. biid56
   layers are particularly suited to processing speech signals as they
   allow the network access to both future and past contexts at every
   given point of the input sequence [[65]1]. when training ctc-based
   acoustic models, we found it beneficial to employ    vanilla    id56s as
   opposed to their gated variants (grus or lstms) mainly because the
   latter come with significant computational overhead. following [[66]2],
   we also apply batch id172 to the biid56 layers to reduce the
   overall training time with little impact on the accuracy of the model
   measured in terms of the overall word error rate (wer).

   the outputs from the biid56 layers at each timestep are then fed into a
   fully connected layer which in turn feeds into a softmax layer. each
   unit in the softmax layer corresponds to a single character in the
   alphabet characterizing the target vocabulary. for example, if the
   training data is drawn from an english corpus, the alphabet will
   typically include the characters a through z, as well as any relevant
   punctuation symbols, including a symbol for the    space    character
   separating words in the transcripts. ctc-based models also typically
   require that the alphabet include a special    blank    character. the
   blank character allows the model to reliably deal with predicting
   consecutive repeated symbols, as well as artifacts in speech signals,
   e.g. pauses, background noise and other    non-speech    events.

   thus, given a sequence of frames corresponding to an utterance, the
   model is required to produce, for each frame, a id203
   distribution over the alphabet. during the training phase, the softmax
   outputs are fed into a ctc cost function (more on this shortly) which
   uses the actual transcripts to (i) score the model   s predictions, and
   (ii) generate an error signal quantifying the accuracy of the model   s
   predictions. the overall goal is to train the model to increase the
   overall score of its predictions relative to the actual transcripts.

training

   empirically, we have found that using stochastic id119 with
   momentum paired with gradient clipping leads to the best performing
   models. deeper networks (seven layers or more) also tend to perform
   better in general.

   we train our models using nesterov   s accelerated id119
   following the implementation in [67]sutskever, et al. most of the
   model   s hyperparameters, e.g. the depth of the network, the number of
   units in a given layer, the learning rate, the annealing rate, the
   momentum, etc., are chosen empirically based on a held-out development
   dataset. we use [68]   xavier    initialization for all layers in our
   models, although we have not systematically investigated whether there
   are any improvements to be had by using alternate forms of
   initialization.

   all our models are trained using the ctc loss criterion. a detailed
   explanation of the inner-workings of the ctc computation is beyond the
   scope of this blog post. we will provide a brief overview here and
   refer the reader to [alex graves    dissertation] for an in-depth
   treatment.

   the ctc computation is centered around the action of a    collapse   
   function which takes a sequence of characters as input and produces an
   output sequence by first removing all repeated characters in the input
   string followed by removing all    blank    symbols. for example, if we use
      _    to denote the blank symbol, then

   ctc computation

   given an utterance of length t and its corresponding    ground truth   
   transcript, the ctc algorithm constructs the    inverse    of the collapse
   function defined as the set of all possible character sequences of
   length t which collapse onto the    ground truth    transcript. the
   id203 of observing any sequence that appears in this    inverse   
   set can be computed directly from the softmax outputs of the neural
   network. the ctc cost is then defined as a logarithmic function of the
   sum of probabilities of sequences in the    inverse    set. this function
   is differentiable with respect to the softmax outputs which is all we
   need to compute the error gradients required for id26.

   taking a simple example for illustrative purposes, suppose that the
   input utterance has 3 frames and that the corresponding transcription
   is the word    ox   . again using    _    to denote the blank symbol, the set
   of 3-character sequences that collapse to ox contains _ox, o_x. oox,
   oxx, and ox_. the ctc computation sets

   ctc computation sets

   where p(abc) = p(a,1)p(b,2)p(c,3) with p(u,t) given by the model   s
   softmax output for unit    u    at time (frame) t. the ctc algorithm thus
   requires enumerating all sequences of a given fixed length which
   collapse to a given the target sequence. when dealing with very long
   sequences, the enumerative combinatorics is efficiently carried out
   using a forward-backward algorithm that   s very close in spirit to that
   employed in id48s.

evaluation

   once the model is trained, we evaluate its performance by testing it on
   previously unseen utterances from a test set. recall that the model
   generates sequences of id203 vectors as outputs, so we need to
   build a decoder to transform the model   s output into word sequences.

   the decoder   s job is to search through the model   s outputs and generate
   the most probable sequence as the transcription. the simplest approach
   is to compute

   ctc algorithm

   where collapse(     ) is the mapping defined above.

   despite being trained on character sequences, our models are still able
   to learn an implicit language model and are already quite adept at
   spelling out words    phonetically    (see table 1). the models    spelling
   performance is typically measured using character error rates (cers)
   calculated using the [69]levenshtein distance at the character level.
   we have observed that many of the errors in the models    predictions
   occur in words that do not appear in the training set. it is thus
   reasonable to expect that the overall cer would continue to improve as
   one increased the size of the training set. this expectation is borne
   out in the results obtained in [70]deep speech 2, where the training
   set consists of over 12000 hours of speech data.


   model output without lm constraints     ground truth    transcription
   younited presidentiol is a lefe in surance company  united presidential
   is a life insurance company
   that was sertainly true last week that was certainly true last week
   we   re now ready to say we   re intechnical default a spokesman said we   re
   not ready to say we   re in technical default a spokesman said

   table 1: a sample of the model   s predictions on the wall street journal
   evaluation dataset. we intentionally chose examples that the model
   struggles with. as shown, incorporating language model constraints
   essentially eliminates all    spelling errors    that the model makes
   without a language model.

   although our models exhibit excellent cers, their tendency to spell out
   words phonetically results in relatively high word error rates.  one
   can improve the models    performance (wer) by allowing the decoder to
   incorporate constraints from an external lexicon and language model.
   following [[71]3, [72]4], we have found using [73]weighted finite state
   transducers (wfsts) to be a particularly effective approach to this
   task. we have observed relative wer improvements of up to 25% on the
   wsj and librispeech datasets.

   table 2 lists various end-to-end id103 systems trained
   using the [74]wall street journal (wsj) corpus. to allow an
   apples-to-apples comparison, we have chosen to compare published
   results on systems trained and evaluated using only the wsj dataset.
   however, we should point out that hybrid dnn-id48 systems trained and
   evaluated on the same dataset have been shown to perform much better
   than systems using purely deep neural network architectures [[75]6]. on
   the other hand, it has been established that by training on much larger
   datasets, purely deep neural network architectures are able to achieve
   the same performance as their hybrid dnn-id48 counterparts [76](see deep
   speech 2).


   reference

   cer

   (no lm)
   wer

   (no lm)
   wer

   (trigram lm)

   wer

   (trigram lm w/ enhancements)
   hannun, et al. (2014)      10.7         35.8         14.1         n/a
   graves-jaitly (icml 2014)  9.2          30.1         not reported 8.7
   hwang-sung (icml 2016)     10.6         38.4         8.88         8.1
   miao et al. (2015) [eesen] not reported not reported 9.1          7.3
   bahdanau et al. (2016      6.4          18.6         10.8         9.3
   our implementation         8.64         32.5         8.4          n/a

   ^table 2: performance of various end-to-end id103 systems
   trained and evaluated using only the wall street journal (wsj) dataset.
   cer refers to the character error rate comparing the sequence of
   characters from the model to the sequences of characters in the actual
   transcripts. lm refers to the language model. the final column accounts
   for cases where the decoding was carried out using additional
   techniques like rescoring, model aggregation, etc.

future work

   the incorporation of the ctc objective function into neural network
   models for id103 provided a first glimpse of the ability
   of pure dnn methods. more recently, however, encoder-decoder id56 models
   augmented with the so-called attention mechanism have emerged as viable
   alternatives to id56 models trained using the ctc criterion [[77]4,
   [78]5]. both attention-based encoder-decoder models and ctc-based
   models are trained to map sequences of acoustic inputs to sequences of
   characters/phonemes. as discussed above, ctc-based models are trained
   to predict a character for each frame of speech input and proceed to
   search for possible alignments between the frame-wise predictions and
   the target sequence. in contrast, encoder-decoder attention-based
   models first read in the entire input sequence before predicting the
   output sequence. a conceptual advantage to this approach is that one
   does not have to assume that the predicted characters in the output
   sequence are independent. the ctc algorithm explicitly makes this
   assumption which is clearly unfounded     the sequences of characters
   appearing in words are very much conditioned on characters that appear
   earlier in the sequence. recent work on attention-based encoder-decoder
   models for lvcsr has shown significant improvements in character error
   rates relative to ctc-based models [[79]4]. this is true when both
   approaches are evaluated prior to integrating a language model, lending
   support to the assertion that the attention-based models produce better
   acoustic models than their ctc-based counterparts. however, it is worth
   pointing out that the difference in performance disappears when
   language models are used to determine word error rates.

   we are actively working on a neon implementation of attention-based
   encoder-decoder networks for asr applications and wholeheartedly
   welcome contributions to this effort from the community at large.

   code accompanying this blog post can be found at
   [80]https://github.com/nervanasystems/deepspeech.git.

   authors
   anthony ndirango

   [81]anthony ndirango

   tags

   [82]id103 [83]model zoo [84]neon    [85]id103

   stay connected
   keep tabs on all the latest news with our monthly newsletter.
   (button) subscribe
     * [86]@intelai
     * [87]@intelaidev
     * [88]@intelai
     * [89]intel-ai
     * [90]nervanasystems

   (button) x

stay connected with intel   ai

   first name * ____________________
   last name * ____________________
   business email address * ____________________
   country/region * [please select...____________________________]
   job title * ____________________
   company * ____________________
   profession * [please select..._____________________________]
   [x] yes, i would like to subscribe to the intel ai newsletter to stay
   connected to the latest intel technologies and industry trends by
   email. i can unsubscribe at any time. *
   (button) submit

   by submitting this form, you are confirming you are an adult 18 years
   or older and you agree to share your personal information with intel to
   use for this business request. you also agree to subscribe to stay
   connected to the latest intel technologies and industry trends by
   email. you may unsubscribe at any time. intel   s web sites and
   communications are subject to our [91]privacy notice and [92]terms of
   use.

   thank you!

   sorry, there was an error in your submission.
     *   intel corporation
     * [93]terms of use
     * [94]*trademarks
     * [95]privacy
     * [96]cookies
     * [97]supply chain transparency
     * [98]site map

references

   visible links
   1. https://www.intel.ai/feed/
   2. https://www.intel.ai/comments/feed/
   3. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/end-end-speech-recognition-neon/
   4. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/end-end-speech-recognition-neon/&format=xml
   5. https://www.intel.ai/
   6. https://www.intel.ai/technology/
   7. https://www.intel.ai/framework-optimizations/
   8. https://www.intel.ai/hardware/
   9. https://www.intel.ai/software-libraries/
  10. https://www.intel.ai/research-projects/
  11. https://www.intel.ai/tools/
  12. https://www.intel.ai/featured-solutions/
  13. https://www.intel.ai/industries/
  14. https://www.intel.ai/health-and-life-sciences/
  15. https://www.intel.ai/telecommunications/
  16. https://www.intel.ai/retail/
  17. https://www.intel.ai/media-and-entertainment/
  18. https://www.intel.ai/financial-services/
  19. https://www.intel.ai/energy/
  20. https://www.intel.ai/government/
  21. https://www.intel.ai/library/
  22. https://www.intel.ai/blog/
  23. https://www.intel.ai/videos/
  24. https://www.intel.ai/white-papers/
  25. https://www.intel.ai/podcasts/
  26. https://www.intel.ai/docs/
  27. https://www.intel.ai/news/
  28. https://www.intel.ai/research/
  29. https://www.intel.ai/publications/
  30. https://www.intel.ai/researchers/
  31. https://www.intel.ai/research-programs/
  32. https://www.intel.ai/research-projects/
  33. https://jobs.intel.com/page/show/artificial-intelligence-careers
  34. https://www.intel.ai/partners/
  35. https://www.intel.ai/research-programs/
  36. https://software.intel.com/en-us/ai-academy
  37. https://software.intel.com/en-us/ai-academy/ambassadors
  38. https://ai.intel.com/ai/ai4socialgood/
  39. https://www.intel.ai/blog/
  40. https://www.intel.ai/blog/
  41. https://www.intel.ai/bio/anthony-ndirango/
  42. https://www.intel.ai/library/?use-case=speech-recognition
  43. https://www.intel.ai/library/?post_tag=model-zoo
  44. https://www.intel.ai/library/?post_tag=neon
  45. https://www.intel.ai/library/?post_tag=speech-recognition
  46. https://www.intel.ai/blog/
  47. https://www.nervanasys.com/author/tyler/
  48. https://en.wikipedia.org/wiki/hidden_markov_model
  49. https://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2016/12/42651.pdf
  50. https://en.wikipedia.org/wiki/id165
  51. https://en.wikipedia.org/wiki/id165
  52. https://en.wikipedia.org/wiki/viterbi_algorithm
  53. https://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2016/12/graves14.pdf
  54. https://arxiv.org/abs/1512.02595
  55. https://github.com/nervanasystems/deepspeech
  56. https://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2016/12/preprint.pdf
  57. https://en.wikipedia.org/wiki/spectrogram
  58. https://en.wikipedia.org/wiki/mel-frequency_cepstrum
  59. https://github.com/nervanasystems/aeon
  60. http://aeon.nervanasys.com/index.html/etl_audio.html
  61. http://sox.sourceforge.net/
  62. https://arxiv.org/abs/1512.02595
  63. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  64. http://aeon.nervanasys.com/index.html/index.html
  65. https://ai.intel.com/wp-content/uploads/sites/71/bid56.pdf
  66. https://arxiv.org/abs/1512.02595
  67. https://ai.intel.com/wp-content/uploads/sites/71/sutskever13.pdf
  68. https://ai.intel.com/wp-content/uploads/sites/71/glorot10a.pdf
  69. https://en.wikipedia.org/wiki/levenshtein_distance
  70. https://arxiv.org/abs/1512.02595
  71. https://arxiv.org/abs/1507.08240
  72. https://arxiv.org/abs/1508.04395
  73. https://ai.intel.com/wp-content/uploads/sites/71/hbka.pdf
  74. https://catalog.ldc.upenn.edu/ldc94s13a
  75. https://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/53/2017/06/2015_interspeech_multisplice.pdf
  76. https://arxiv.org/abs/1512.02595
  77. https://arxiv.org/abs/1508.04395
  78. https://arxiv.org/abs/1508.01211
  79. https://arxiv.org/abs/1508.04395
  80. https://github.com/nervanasystems/deepspeech.git
  81. https://www.intel.ai/bio/anthony-ndirango/
  82. https://www.intel.ai/library/?use-case=speech-recognition
  83. https://www.intel.ai/library/?post_tag=model-zoo
  84. https://www.intel.ai/library/?post_tag=neon
  85. https://www.intel.ai/library/?post_tag=speech-recognition
  86. https://twitter.com/intelai
  87. https://twitter.com/intelaidev
  88. https://www.facebook.com/intelai/
  89. https://www.linkedin.com/company/3628074/
  90. https://github.com/nervanasystems
  91. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html?elqtrackid=e6a9dfea0f904f66aa7346e8f8e74476&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  92. https://www.intel.com/content/www/us/en/legal/terms-of-use.html?elqtrackid=8445f4bedfeb45dcb04ff6097e1001e8&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  93. https://www.intel.com/content/www/us/en/legal/terms-of-use.html
  94. https://www.intel.com/content/www/us/en/legal/trademarks.html
  95. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html
  96. https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html
  97. https://www.intel.com/content/www/us/en/policy/policy-human-trafficking-and-slavery.html
  98. https://www.intel.com/content/www/us/en/siteindex.html

   hidden links:
 100. https://www.intel.com/content/www/us/en/homepage.html
 101. https://www.intel.ai/end-end-speech-recognition-neon/#menus
 102. https://www.intel.ai/end-end-speech-recognition-neon/#top
