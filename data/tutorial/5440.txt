   #[1]intel ai    feed [2]intel ai    comments feed [3]alternate
   [4]alternate

   search ____________________ search

   [5]intel ai
     * ____________________
     * [6]technologies
          + [7]frameworks
          + [8]hardware
          + [9]software libraries
          + [10]research projects
          + [11]tools
          + [12]featured solutions
     * [13]industries
          + [14]health & life sciences
          + [15]telecommunications
          + [16]retail
          + [17]media & entertainment
          + [18]financial services (fsi)
          + [19]energy
          + [20]government
     * [21]library
          + [22]blog
          + [23]videos
          + [24]white papers
          + [25]podcasts
          + [26]docs
          + [27]news
     * [28]research
          + [29]publications
          + [30]researchers
          + [31]research programs
          + [32]research projects
     * [33]careers
     * programs
          + [34]partners
          + [35]research residency
          + [36]intel ai developer program
          + [37]ai student ambassadors
          + [38]ai4socialgood
     * [39]blog

   ____________________
   show the menu

   [40]blog
   pre trained deep learning models
   authors
   tambet matiisen

   [41]tambet matiisen

   phd student in university of tartu
   tags

   [42]coursera [43]neon   

   [44]blog
   12.29.15
   pre trained deep learning models

   12.29.15

guest post (part ii): deep id23 with neon

   this is part 2 of a blog series on deep id23. see
   part 1    [45]demystifying deep id23    for an
   introduction to the topic.

   the first time we read deepmind   s paper    [46]playing atari with deep
   id23    in our research group, we immediately knew that
   we wanted to replicate this incredible result. it was the beginning of
   2014, [47]cuda-convnet2 was the top-performing convolutional network
   implementation and rmsprop was just [48]one slide in hinton   s coursera
   course. we struggled with debugging, learned a lot, but when deepmind
   also published their code alongside their nature paper    [49]human-level
   control through deep id23   , we started working on
   their code instead.

   the deep learning ecosystem has evolved a lot since then. supposedly a
   new deep learning toolkit was released once [50]every 22 days in 2015.
   amongst the popular ones are both the old-timers like [51]theano,
   [52]torch7 and [53]caffe, as well as the newcomers like [54]neon,
   [55]keras and [56]tensorflow. new algorithms are getting implemented
   within days of publishing.

   at some point i realized that all the complicated parts that caused us
   headaches a year earlier are now readily implemented in most deep
   learning toolkits. and when [57]arcade learning environment     the
   system used for emulating atari 2600 games     released a [58]native
   python api, the time was right for a new deep id23
   implementation. writing the main code took just a weekend, followed by
   weeks of debugging. but finally it worked! you can see the result here:
   [59]https://github.com/tambetm/simple_id25

   i chose to base it on neon, because it has
     * the [60]fastest convolutional kernels,
     * all the required algorithms implemented (e.g. rmsprop),
     * a reasonable python api.

   i tried to keep my code simple and easy to extend, while also keeping
   performance in mind. currently the most notable restriction in neon is
   that it only runs on the latest nvidia maxwell gpus, but that   s
   [61]about to change soon.

   in the following i will describe:
    1. how to install simple-id25
    2. what you can do with simple-id25
    3. how does simple-id25 compare to others
    4. and finally how to modify simple-id25.

how to install simple-id25?

   there is not much to talk about here, just follow the instructions in
   the [62]readme. basically all you need are [63]neon, [64]arcade
   learning environment and [65]simple_id25 itself. for trying out
   pre-trained models you don   t even need a gpu, because they also run on
   cpu, albeit slowly.

what you can do with simple-id25?

running a pre-trained model

   once you have everything installed, the first thing to try out is
   running pre-trained models. for example to run a pre-trained model for
   breakout, type:

   ./play.sh snapshots/breakout_77.pkl

   or if you don   t have a (maxwell) gpu available, then you can switch to
   cpu:

   ./play.sh snapshots/breakout_77.pkl    backend cpu

   you should be seeing something like this, possibly even accompanied by
   annoying sounds     

   pre trained deep learning models

   you can switch off the ai and take control in the game by pressing    m   
   on keyboard     see how long you last! you can give the control back to
   the ai by pressing    m    again.

   it is actually quite useful to slow down the gameplay by pressing    a   
   repeatedly     this way you can observe what is really happening in the
   game. press    s    to speed the game up again.

recording a video

   just as easily you can also record a video of one game (in breakout
   until 5 lives are lost):

   ./record.sh snapshots/breakout_77.pkl

   the recorded video is stored as videos/breakout_77.mov and screenshots
   from the game can be found in videos/breakout. you can watch an example
   video here:

   iframe: [66]https://www.youtube.com/embed/kkif0ok5gce?feature=oembed

training a new model

   to train a new model, you first need an atari 2600 rom file for the
   game. once you have the rom, save it to roms folder and run training
   like this:

   ./train.sh roms/pong.rom

   as a result of training the following files are created:
     * results/pong.csv contains various statistics of the training
       process,
     * snapshots/pong_<epoch>.pkl are model snapshots after every epoch.

testing a model

   during training there is a testing phase after each epoch. if you would
   like to re-test your pre-trained model later, you can use the testing
   script:

   ./test.sh snapshots/pong_141.pkl

   it prints the testing results to console. to save the results to file,
   add    csv_file <filename> parameter.

plotting statistics

   during and after training you might like to see how your model is
   doing. there is a simple plotting script to produce graphs from the
   statistics file. for example:

   ./plot.sh results/pong.csv

   this produces the file results/pong.png. evaluate the performance of
   deep learning models

   by default it produces four plots:
    1. average score,
    2. number of played games,
    3. average maximum q-value of validation set states,
    4. average training loss.

   for all the plots you can see the random baseline (where it makes
   sense) and the result from training phase and testing phase. you can
   actually plot any field from the statistics file, by listing names of
   the fields in the    fields parameter. the default plot is achieved with
      fields average_reward,meanq,nr_games,meancost. names of the fields can
   be taken from the first line of the csv file.

visualizing the filters

   the most exciting thing you can do with this code is to peek into the
   mind of the ai. for that we are going to use [67]guided
   id26, that comes [68]out-of-the-box with neon. in simplified
   terms, for each convolutional filter it finds an image from a given
   dataset that activates this filter the most. then it performs
   id26 with respect to the input image, to see which parts of
   the image affect the    activeness    of that filter most. this can be seen
   as a form of saliency detection.

   to perform filter visualization run the following:

   ./nvis.sh snapshots/breakout_77.pkl

   it starts by playing a game to collect sample data and then performs
   the guided id26. the results can be found in
   results/breakout.html and they look like this filter visualization for
   pre-trained model

   there are 3 convolutional layers (named 0000, 0002 and 0004) and for
   this post i have visualized only 2 filters from each (feature map 0-1).
   there is also a more detailed [69]file for breakout which has 16
   filters visualized. for each filter an image was chosen that activates
   it the most. right image shows the original input, left image shows the
   guided id26 result. you can think of every filter as an
      eye    of the ai. the left image shows where this particular    eye    was
   directed to, given the image on the right. you can use the mouse wheel
   to zoom in!

   because the input to our network is a sequence of 4 grayscale images,
   it   s not very clear how to visualize it. i made a following
   simplification: i   m using only the last 3 screens of a state and
   putting them into different rgb color channels. so everything that is
   gray hasn   t changed over 3 images; blue was the most recent change,
   then green, then red. you can easily follow the idea if you zoom in to
   the ball     it   s trajectory is marked by red-green-blue. it   s harder to
   make sense of the id26 result, but sometimes you can guess
   that the filter tracks movement     the color from one corner to another
   progresses from red to green to blue.

   filter visualization is an immensely useful tool, you can immediately
   make interesting observations.
     * the first layer filters focus on abstract patterns and cannot be
       reliably associated with any particular object. they often activate
       most on score and lives, possibly because these have many edges and
       corners. still one can spot occasional filters focused on score and
       lives even in higher layers.
     * as expected, there are filters that track the ball and the paddle.
       there are also filters that activate the most when the ball is
       about to hit a brick or the paddle.
     * also as expected, higher layer filters have bigger receptive
       fields. this is not so evident in breakout, but can be clearly seen
       in this [70]file for pong. it   s interesting that filters in
       different layers are more similar in breakout than in pong.

   the guided id26 is implemented in neon as a callback, called
   at the end of training. i made a [71]simple wrapper that allows using
   it on pre-trained models. one advantage of using the wrapper is that it
   incorporates guided id26 and visualization into one step and
   doesn   t need a temporary file to write the intermediate results. but
   for that i needed to make few modifications to the neon code, that are
   stored in the [72]nvis folder. [/fusion_text][fusion_text]

how does simple-id25 compare to others?

   there are a few other deep id23 implementations out
   there and it would be interesting to see how the implementation in neon
   compares to them. the most well-known is the [73]original deepmind code
   published with their nature article. another maintained version is
   [74]deep_q_rl created by nathan sprague from james madison university
   in virginia.

   the most common metric used in deep id23 is the
   average score per game. to calculate it for simple_id25 i used the same
   evaluation procedure as in the nature article     average scores of 30
   games, played with different initial random conditions and an   -greedy
   policy with   =0.05. i didn   t bother to implement the 5 minutes per game
   restriction, because games in breakout and pong don   t last that long.
   for deepmind i used the values from nature paper. for deep_q_rl i i
   asked the deep-id24 list members to provide the numbers. their
   scores are not collected using exactly the same protocol (the
   particular number below was average of 11 games) and may be therefore
   considered a little bit inflated.

   another interesting measure is the number of training steps per second.
   deepmind and simple_id25 report average number of steps per second for
   each epoch (250000 steps). deep_q_rl reports number of steps per second
   on the fly and i just used a perceived average of numbers flowing over
   the screen :). in all cases i looked at the first epoch, where
   exploration rate is close to 1 and the results therefore reflect more
   of the training speed than the prediction speed. all tests were done on
   nvidia geforce titan x. best id25 network

   as it can be seen, in terms of speed simple_id25 compares quite
   favorably against the others. the learning results are not on-par with
   deepmind yet, but they are close enough to run interesting experiments
   with it.

how can you modify simple-id25?

   the main idea in publishing the simple_id25 code was to show how simple
   the implementation could actually be and that everybody can extend it
   to perform interesting research.

   there are four main classes in the code: environment, replaymemeory,
   deepqnetwork and agent. there is also main.py, that handles parameter
   parsing and instantiates the above classes, and the statistics class
   that implements the basic callback mechanism to keep statistics
   collection separate from the main loop. but the gist of the deep
   id23 algorithm is implemented in the aforementioned
   four classes.  how to implement id25

environment

   environment is just a lightweight wrapper around the [75]a.l.e python
   api. it should be easy enough to add other environments besides a.l.e,
   for example [76]flappy bird or [77]torcs     you just have to implement a
   new environment class. give it a try and let me know!

replaymemory

   replay memory stores state transitions or experiences. basically it   s
   just four big arrays for screens, actions, rewards and terminal state
   indicators.

   replay memory is stored as a sequence of screens, not as a sequence of
   states consisting of four screens. this results in a huge decrease in
   memory usage, without significant loss in performance. assembling
   screens into states can be done fast with numpy array slicing.

   datatype for the screen pixels is uint8, which means 1m experiences
   take 6.57gb     you can run it with only 8gb of memory! default would
   have been float32, which would have taken ~26gb.

   if you would like to implement [78]prioritized experience replay, then
   this is the main class you need to change.

deepqnetwork

   this class implements the deep q-network. it is actually the only class
   that depends on neon.

   because deep id23 handles minibatching differently,
   there was no reason to use neon   s dataiterator class. therefore the
   lower level model.fprop() and model.bprop() are used instead. a few
   suggestions for anybody attempting to do the same thing:
    1. you need to call model.initialize() after constructing the model.
       this allocates the tensors for layer activations and weights in gpu
       memory.
    2. neon tensors have dimensions (channels, height, width, batch_size).
       in particular batch size is the last dimension. this data layout
       allows for the fastest convolution kernels.
    3. after transposing the dimensions of a numpy array to match neon
       tensor requirements, you need to make a copy of that array!
       otherwise the actual memory layout hasn   t changed and the array
       cannot be directly copied to gpu.
    4. instead of accessing single tensor elements with array indices
       (like tensor[i,j]), try to copy the tensor to a numpy array as a
       whole with tensor.set() and tensor.get() methods. this results in
       less round-trips between cpu and gpu.
    5. consider doing tensor arithmetics in gpu, the [79]neon backend
       provides nice methods for that. also note that these operations are
       not immediately evaluated, but stored as a optree. the tensor will
       be actually evaluated when you use it or transfer it to cpu.

   if you would like to implement [80]double id24, then this is the
   class that needs modifications.

agent

   agent class just ties everything together and implements the main
   logic.

conclusion

   i have shown, that using a well-featured deep learning toolkit such as
   neon, implementing deep id23 for atari video games is
   a breeze. filter visualization features in neon provide important
   insights into what the model has actually learned.

   while not the goal on its own, computer games provide an excellent
   sandbox for trying out new id23 approaches. hopefully
   my [81]simple_id25 implementation provides a stepping stone for a number
   of people into the fascinating area of deep id23
   research.

credits

   thanks to ardi tampuu, jaan aru, tanel p  rnamaa, raul vicente, arjun
   bansal and urs k  ster for comments and suggestions on the drafts of
   this post.

   [this blog was first published at:
   [82]http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/]

   authors
   tambet matiisen

   [83]tambet matiisen

   phd student in university of tartu
   tags

   [84]coursera [85]neon   

   stay connected
   keep tabs on all the latest news with our monthly newsletter.
   (button) subscribe
     * [86]@intelai
     * [87]@intelaidev
     * [88]@intelai
     * [89]intel-ai
     * [90]nervanasystems

   (button) x

stay connected with intel   ai

   first name * ____________________
   last name * ____________________
   business email address * ____________________
   country/region * [please select...____________________________]
   job title * ____________________
   company * ____________________
   profession * [please select..._____________________________]
   [x] yes, i would like to subscribe to the intel ai newsletter to stay
   connected to the latest intel technologies and industry trends by
   email. i can unsubscribe at any time. *
   (button) submit

   by submitting this form, you are confirming you are an adult 18 years
   or older and you agree to share your personal information with intel to
   use for this business request. you also agree to subscribe to stay
   connected to the latest intel technologies and industry trends by
   email. you may unsubscribe at any time. intel   s web sites and
   communications are subject to our [91]privacy notice and [92]terms of
   use.

   thank you!

   sorry, there was an error in your submission.
     *   intel corporation
     * [93]terms of use
     * [94]*trademarks
     * [95]privacy
     * [96]cookies
     * [97]supply chain transparency
     * [98]site map

references

   visible links
   1. https://www.intel.ai/feed/
   2. https://www.intel.ai/comments/feed/
   3. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/deep-reinforcement-learning-with-neon/
   4. https://www.intel.ai/wp-json/oembed/1.0/embed?url=https://www.intel.ai/deep-reinforcement-learning-with-neon/&format=xml
   5. https://www.intel.ai/
   6. https://www.intel.ai/technology/
   7. https://www.intel.ai/framework-optimizations/
   8. https://www.intel.ai/hardware/
   9. https://www.intel.ai/software-libraries/
  10. https://www.intel.ai/research-projects/
  11. https://www.intel.ai/tools/
  12. https://www.intel.ai/featured-solutions/
  13. https://www.intel.ai/industries/
  14. https://www.intel.ai/health-and-life-sciences/
  15. https://www.intel.ai/telecommunications/
  16. https://www.intel.ai/retail/
  17. https://www.intel.ai/media-and-entertainment/
  18. https://www.intel.ai/financial-services/
  19. https://www.intel.ai/energy/
  20. https://www.intel.ai/government/
  21. https://www.intel.ai/library/
  22. https://www.intel.ai/blog/
  23. https://www.intel.ai/videos/
  24. https://www.intel.ai/white-papers/
  25. https://www.intel.ai/podcasts/
  26. https://www.intel.ai/docs/
  27. https://www.intel.ai/news/
  28. https://www.intel.ai/research/
  29. https://www.intel.ai/publications/
  30. https://www.intel.ai/researchers/
  31. https://www.intel.ai/research-programs/
  32. https://www.intel.ai/research-projects/
  33. https://jobs.intel.com/page/show/artificial-intelligence-careers
  34. https://www.intel.ai/partners/
  35. https://www.intel.ai/research-programs/
  36. https://software.intel.com/en-us/ai-academy
  37. https://software.intel.com/en-us/ai-academy/ambassadors
  38. https://ai.intel.com/ai/ai4socialgood/
  39. https://www.intel.ai/blog/
  40. https://www.intel.ai/blog/
  41. https://www.intel.ai/bio/tambet-matiisen/
  42. https://www.intel.ai/library/?company=coursera
  43. https://www.intel.ai/library/?post_tag=neon
  44. https://www.intel.ai/blog/
  45. https://www.intel.ai/demystifying-deep-reinforcement-learning
  46. http://arxiv.org/abs/1312.5602
  47. https://code.google.com/p/cuda-convnet2/
  48. http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/lecture_slides_lec6.pdf
  49. http://www.nature.com/articles/nature14236
  50. https://twitter.com/kcimc/status/664217437840257024
  51. http://deeplearning.net/software/theano/
  52. http://torch.ch/
  53. http://caffe.berkeleyvision.org/
  54. http://neon.nervanasys.com/
  55. http://keras.io/
  56. https://tensorflow.org/
  57. http://www.arcadelearningenvironment.org/
  58. https://github.com/bbitmaster/ale_python_interface/wiki/code-tutorial
  59. https://github.com/tambetm/simple_id25
  60. https://github.com/soumith/convnet-benchmarks
  61. https://github.com/nervanasystems/neon/issues/80
  62. https://github.com/tambetm/simple_id25
  63. https://github.com/nervanasystems/neon
  64. https://github.com/mgbellemare/arcade-learning-environment
  65. https://github.com/tambetm/simple_id25
  66. https://www.youtube.com/embed/kkif0ok5gce?feature=oembed
  67. http://arxiv.org/abs/1412.6806
  68. http://neon.nervanasys.com/docs/latest/tools.html#layer-deconvolution-visualization
  69. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/breakout_16.html
  70. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/pong_16.html
  71. https://github.com/tambetm/simple_id25/blob/master/src/visualization.py
  72. https://github.com/tambetm/simple_id25/tree/master/src/nvis
  73. https://sites.google.com/a/deepmind.com/id25/
  74. https://github.com/spragunr/deep_q_rl
  75. https://github.com/bbitmaster/ale_python_interface/wiki/code-tutorial
  76. https://github.com/sourabhv/flappybirdclone
  77. http://torcs.sourceforge.net/
  78. http://arxiv.org/abs/1511.05952
  79. http://neon.nervanasys.com/docs/latest/generated/neon.backends.backend.backend.html
  80. http://arxiv.org/abs/1511.05952
  81. https://github.com/tambetm/simple_id25
  82. http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
  83. https://www.intel.ai/bio/tambet-matiisen/
  84. https://www.intel.ai/library/?company=coursera
  85. https://www.intel.ai/library/?post_tag=neon
  86. https://twitter.com/intelai
  87. https://twitter.com/intelaidev
  88. https://www.facebook.com/intelai/
  89. https://www.linkedin.com/company/3628074/
  90. https://github.com/nervanasystems
  91. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html?elqtrackid=e6a9dfea0f904f66aa7346e8f8e74476&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  92. https://www.intel.com/content/www/us/en/legal/terms-of-use.html?elqtrackid=8445f4bedfeb45dcb04ff6097e1001e8&elq=00000000000000000000000000000000&elqaid=21534&elqat=2&elqcampaignid=
  93. https://www.intel.com/content/www/us/en/legal/terms-of-use.html
  94. https://www.intel.com/content/www/us/en/legal/trademarks.html
  95. https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html
  96. https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html
  97. https://www.intel.com/content/www/us/en/policy/policy-human-trafficking-and-slavery.html
  98. https://www.intel.com/content/www/us/en/siteindex.html

   hidden links:
 100. https://www.intel.com/content/www/us/en/homepage.html
 101. https://www.intel.ai/deep-reinforcement-learning-with-neon/#menus
 102. https://www.intel.ai/deep-reinforcement-learning-with-neon/#top
