   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

neural net in 10 frameworks (lessons learned)

   [9]go to the profile of ilia karmanov
   [10]ilia karmanov (button) blockedunblock (button) followfollowing
   sep 6, 2017

   [11]github project link         deep learning frameworks
   [1*legv2bqhgeh0ggbmoyabka.png]

   beyond all the technical elements, what i found the most interesting
   about this project was the amazing contributions it gathered from the
   open-source community. the [12]pull-requests and [13]issues raised by
   the community helped immensely to unite all the frameworks in terms of
   accuracy and also training-time. it was amazing to see the
   contributions from fair-researchers, original creators of frameworks
   (such as yangqing) and other github users to a microsoft employee   s
   github, and this wouldn   t have been possible for me without them. not
   only were code suggestions offered, but also [14]whole notebooks for
   different frameworks were supplied!

   you can see the [15]original state of the repo here, before the
   contributions.

the issue

   a search for tensorflow + mnist produces this [16]complicated-looking
   tutorial, which avoids higher-level apis (tf.layers or tf.nn) and
   doesn   t seem detached enough from the input-data that one would feel
   comfortable replacing mnist with cifar (for example). some tutorials,
   to avoid the verbose loading of mnist, have a custom wrapper like:
   framework.datasets.mnist, however i have two issues with this:
    1. for a beginner it may not be obvious how to re-run on their own
       data
    2. comparing this to another framework may be more tricky (does
       pre-processing differ?)

   other tutorials save mnist to disk as a text-file (or even a custom
   database) and then load it again, this time using some
   textreaderdataloader. the idea is to show what would happen if the user
   had a huge data-set, that was too big to load into ram and required
   lots of on-the-fly-transformations. for a beginner it may be misleading
   and intimidating and i often hear the question:    why do i need to save
   it, i have it right here as an array!   

the goal

   the goal was to show how to write the same neural-net (on a common,
   custom data-set) using the 8 most popular frameworks - a rosetta stone
   of deep-learning frameworks, to allow data-scientists to easily
   leverage their expertise from one framework to another (by translating,
   rather than learning from scratch). a consequence of having the same
   model in different frameworks is that the frameworks become more
   transparent in terms of training-time and default-options and we can
   even compare certain elements.

   by being able to quickly translate your model to another framework
   means you can swap hats if another framework has a layer you may need
   to write from scratch, deals with your data-source in a more efficient
   manner, or is more suited to the platform you are running on (e.g.
   android).

   for these tutorials, i try to use the highest-level api possible
   conditional on being able to override conflicting default options, to
   allow an easier comparison between frameworks. this means that the
   notebooks are not specifically written for speed.

   it will demonstrated that the code structure becomes very similar once
   higher-level apis are used and can be roughly represented as:
     * load data into ram; x_train, x_test, y_train, y_test =
       cifar_for_library(channel_first=?, one_hot=?)
     * generate id98 symbol (usually no activation on final dense-layer)
     * specify loss (cross-id178 often bundled with softmax), optimiser
       and initialise weights + maybe sessions
     * train on mini-batches from train-set using custom iterator (common
       data-source for all frameworks)
     * predict on fresh mini-batches from test-set, perhaps specifying
       test-flag for layers like drop-out
     * evaluate accuracy

some caveats

   since we are essentially comparing a series of deterministic
   mathematical operations (albeit with a random initialization), it does
   not make sense to compare the accuracy across frameworks and instead
   they are reported as checks we want to match, to make sure we are
   comparing the same model architecture.

   the reasons i mention that a speed comparison doesn   t make much sense
   are because:
     * using native data-loaders would likely shave off a few seconds
       (only) since the shuffling would be performed asynchronously.
       however, for a proper project, your data is unlikely to fit into
       ram and also may require lots of pre-processing and manipulation
       (data augmentation). this is what data-loaders are for. yangqing
       mentions:

     we   ve experienced i/o being the main bottleneck for several of our
     in-production networks, so it would be nice to notify people that
     when one cares about top performances, using asynchronous i/o would
     help a lot

     * only a few layers are used in this example (conv2d, max_pool2d,
       dropout, fully-connected). for a proper project you may have 3d
       convolutions, grus, lstms, etc.
     * the ease of adding your own custom layers (or perhaps the
       availability of layers such as k-max-pooling or hierarchical
       softmax), along with the speed at which they run, can make or break
       your choice of framework. being able to write a custom-layer in
       python-code and having it execute quickly is vital for research
       projects
     * in reality you want to make use of advanced logging (such as
       tensorboard) to see if your model is converging and also to assist
       with hyper-parameter tuning. in this example we take all that as
       exogenous.

[17]lessons learned (matching accuracy/time)

   the below offers some insights i gained after trying to match
   test-accuracy across frameworks and from all the github issues/prs
   raised.
   [1*ehmsv7oyei6gnjypcuhmfq.png]
    1. the above examples (except for keras), for ease of comparison, try
       to use the same level of api and so all use the same
       generator-function. for [18]mxnet and [19]cntk i have experimented
       with a higher-level api, where i use the framework   s training
       generator function. the speed improvement is neglible in this
       example because the whole dataset is loaded as numpy array in ram
       and the only processing done each epoch is a shuffle. i suspect the
       framework   s generators perform the shuffle asynchronously.
       curiously, it seems that the frameworks shuffle on a batch-level,
       rather than on an observation level, and thus ever so slightly
       decreases the test-accuracy (at least after 10 epochs). for
       scenarios where we have io activity and perhaps pre-processing and
       data-augmentation on the fly, custom generators would have a much
       bigger impact on performance.
    2. enabling cudnn   s auto-tune/exhaustive search parameter (which
       selects the most efficient id98 algorithm for images of fixed-size)
       has a huge performance boost. this had to be manually enabled for
       caffe2, pytorch and theano. it appears cntk, mxnet and tensorflow
       have this enabled by default. i   m not sure about chainer. yangqing
       mentions that the performance boost between cudnnget (default) and
       cudnnfind is, however, much smaller on the titan x gpu; it seems
       that the k80 + new cudnn makes the problem more prominent in this
       case. running cudnnfind for every combination of size in object
       detection has serious performance regressions, however, so
       exhaustive_search should be disabled for id164
    3. when using keras it   s important to choose the [nchw] ordering that
       matches the back-end framework. cntk operates with channels first
       and by mistake i had keras configured to expect channels last. it
       then must have changed the order at each batch which degraded
       performance severely.
    4. tensorflow, pytorch, caffe2 and theano required a boolean supplied
       to the pooling-layer indicating whether we were training or not
       (this had a huge impact on test-accuracy, 72 vs 77%).
    5. tensorflow was a bit annoying and required two more changes: speed
       was improved a lot by enabling tf_enable_winograd_nonfused and also
       changing the dimensions supplied to channel first rather than last
       (data_format=   channels_first   ). enabling the winograd for
       convolutions also, naturally, improved keras with tf as a backend
    6. softmax is usually bundled with cross_id178_loss() for most
       functions and it   s worth checking if you need an activation on your
       final fully-connected layer to save time applying it twice
    7. kernel initializer for different frameworks can vary (i   ve found
       this to have +/- 1% effect on accuracy) and i try to specify
       xavier/glorot uniform whenever possible/not too verbose
    8. type of momentum implemented for sgd-momentum; i had to turn off
       unit_gain (which was on by default in cntk) to match other
       frameworks    implementations
    9. caffe2 has an extra optimisation for the first layer of a network
       (no_gradient_to_input=1) that produces a small speed-boost by not
       computing gradients for input. it   s possible that tensorflow and
       mxnet already enable this by default. computing this gradient could
       be useful for research purposes and for networks like deep-dream
   10. applying the relu activation after max-pooling (insteaad of before)
       means you perform a calculation after dimensionality-reduction and
       thus shave off a few seconds. this helped reduce mxnet time by 3
       seconds
   11. some further checks which may be useful:

     * specifying kernel as (3) becomes a symmetric tuple (3, 3) or 1d
       convolution (3, 1)?
     * strides (for max-pooling) are (1, 1) by default or equal to kernel
       (keras does this)?
     * default padding is usually off (0, 0)/valid but useful to check
       it   s not on/   same   
     * is the default activation on a convolutional layer    none    or    relu   
       (lasagne)
     * the bias initializer may vary (sometimes no bias is included)
     * gradient clipping and treatment of inifinty/nans may differ across
       frameworks
     * some frameworks support sparse labels instead of one-hot (which i
       use if available, e.g. tensorflow has
       f.nn.sparse_softmax_cross_id178_with_logits)
     * data-type assumptions may be different         i try to use float32 and
       int32 for x and y but, for example, torch needs double for y (to be
       coerced into torch.longtensor(y).cuda)
     * if the framework has a slightly lower-level api make sure during
       testing you don   t compute the gradient by setting something like
       training=false

     * [20]deep learning
     * [21]tensorflow
     * [22]neural networks
     * [23]machine learning

   (button)
   (button)
   (button) 203 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of ilia karmanov

[25]ilia karmanov

     * (button)
       (button) 203
     * (button)
     *
     *

   [26]go to the profile of ilia karmanov
   never miss a story from ilia karmanov, when you sign up for medium.
   [27]learn more
   never miss a story from ilia karmanov
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6a5e8e78b481
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@iliakarmanov/neural-net-in-8-frameworks-lessons-learned-6a5e8e78b481&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@iliakarmanov/neural-net-in-8-frameworks-lessons-learned-6a5e8e78b481&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@iliakarmanov?source=post_header_lockup
  10. https://medium.com/@iliakarmanov
  11. https://github.com/ilkarman/deeplearningframeworks
  12. https://github.com/ilkarman/deeplearningframeworks/pulls?q=is:pr+is:closed
  13. https://github.com/ilkarman/deeplearningframeworks/issues?q=is:issue+is:closed
  14. https://github.com/ilkarman/deeplearningframeworks/pull/1
  15. https://github.com/ilkarman/deeplearningframeworks/tree/0143957489e8adbecaa975f9b541443421db5c4b
  16. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py
  17. https://github.com/ilkarman/deeplearningframeworks#lessons-learned
  18. https://github.com/ilkarman/deeplearningframeworks/blob/master/mxnet_cifar_highapi.ipynb
  19. https://github.com/ilkarman/deeplearningframeworks/blob/master/cntk_cifar_highapi.ipynb
  20. https://medium.com/tag/deep-learning?source=post
  21. https://medium.com/tag/tensorflow?source=post
  22. https://medium.com/tag/neural-networks?source=post
  23. https://medium.com/tag/machine-learning?source=post
  24. https://medium.com/@iliakarmanov?source=footer_card
  25. https://medium.com/@iliakarmanov
  26. https://medium.com/@iliakarmanov
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/p/6a5e8e78b481/share/twitter
  30. https://medium.com/p/6a5e8e78b481/share/facebook
  31. https://medium.com/p/6a5e8e78b481/share/twitter
  32. https://medium.com/p/6a5e8e78b481/share/facebook
