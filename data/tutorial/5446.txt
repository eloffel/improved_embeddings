   #[1]ofir press

   iframe: [2]https://www.googletagmanager.com/ns.html?id=gtm-pzggp5q

   [3][logo.png]

[4]ofir press

   [5]about & publications

neural language models explained

   language models assign id203 values to sequences of words. those
   three words that appear right above your keyboard on your phone that
   try to predict the next word you   ll type are one of the uses of
   id38. in the case shown below, the language model is
   predicting that    from   ,    on    and    it    have a high id203 of being
   the next word in the given sentence. internally, for each word in its
   vocabulary, the language model computes the id203 that it will be
   the next word, but the user only gets to see the top three most
   probable words.
   [keyboard.png]

   neural language models are a fundamental part of many systems that
   attempt to solve natural language processing tasks such as machine
   translation and id103. currently, all state of the art
   language models are neural networks.

   the first part of this post presents a simple feedforward neural
   network that solves this task. in the second part of the post, we will
   improve the simple model by adding to it a recurrent neural network
   (id56). the final part will discuss two recently proposed id173
   techniques for improving id56 based language models.

a simple model

   to begin we will build a simple model that given a single word taken
   from some sentence tries predicting the word following it.
   [w2v.svg]

   we represent words using one-hot vectors: we decide on an arbitrary
   ordering of the words in the vocabulary and then represent the nth word
   as a vector of the size of the vocabulary (n), which is set to 0
   everywhere except element n which is set to 1.

   the model can be separated into two components:
     * we start by encoding the input word. this is done by taking the one
       hot vector representing the input word (c in the diagram), and
       multiplying it by a matrix of size (n,200) which we call the input
       embedding (u). this multiplication results in a vector of size 200,
       which is also referred to as a id27. this embedding is a
       dense representation of the current input word. this representation
       is both of a much smaller size than the one-hot vector representing
       the same word, and also has some other interesting properties. for
       example, while the distance between every two words represented by
       a one-hot vectors is always the same, these dense representations
       have the property that words that are close in meaning will have
       representations that are close in the embedding space.
     * the second component can be seen as a decoder. after the encoding
       step, we have a representation of the input word. we multiply it by
       a matrix of size (200,n), which we call the output embedding (v).
       the resulting vector of size n is then passed through the softmax
       function, normalizing its values into a id203 distribution
       (meaning each one of the values is between 0 and 1, and their sum
       is 1). this distribution is denoted by p in the diagram above.

   the decoder is a simple function that takes a representation of the
   input word and returns a distribution which represents the model   s
   predictions for the next word: the model assigns to each word the
   id203 that it will be the next word in the sequence.

   to train this model, we need pairs of input and target output words.
   for the (input, target-output) pairs we use the id32 dataset
   which contains around 40k sentences from news articles, and has a
   vocabulary of exactly 10,000 words. to generate word pairs for the
   model to learn from, we will just take every pair of neighboring words
   from the text and use the first one as the input word and the second
   one as the target output word. so for example for the sentence    the cat
   is on the mat    we will extract the following word pairs for training:
   (the, cat), (cat, is), (is, on), and so on.

   we use stochastic id119 to update the model during training,
   and the loss used is the cross-id178 loss. intuitively, this loss
   measures the distance between the output distribution predicted by the
   model and the target distribution for each pair of training words. the
   target distribution for each pair is a one-hot vector representing the
   target word.

   the metric used for reporting the performance of a language model is
   its perplexity on the test set. it is defined as , where is the
   id203 given by the model to the ith target word. perplexity is a
   decreasing function of the average log id203 that the model
   assigns to each target word. we want to maximize the id203 that
   we give to each target word, which means that we want to minimize the
   perplexity (the optimal perplexity is 1).

   the perplexity for the simple model^[6]1 is about 183 on the test set,
   which means that on average it assigns a id203 of about to the
   correct target word in each pair in the test set. it   s much better than
   a naive model which would assign an equal id203 to each word
   (which would assign a id203 of to the correct word), but we can
   do much better.

using id56s to improve performance

   the biggest problem with the simple model is that to predict the next
   word in the sentence, it only uses a single preceding word. if we could
   build a model that would remember even just a few of the preceding
   words there should be an improvement in its performance. to understand
   why adding memory helps, think of the following example: what words
   follow the word    drink   ? you   d probably say that    coffee   ,    beer    and
      soda    have a high probably of following it. if i told you the word
   sequence was actually    cows drink   , then you would completely change
   your answer.

   we can add memory to our model by augmenting it with a [7]recurrent
   neural network (id56), as shown below.
   [id56lm.svg]

   this model is similar to the simple one, just that after encoding the
   current input word we feed the resulting representation (of size 200)
   into a two layer [8]lstm, which then outputs a vector also of size 200
   (at every time step the lstm also receives a vector representing its
   previous state- this is not shown in the diagram). then, just like
   before, we use the decoder to convert this output vector into a vector
   of id203 values. (lstm is just a fancier id56 that is better at
   remembering the past. its    api    is identical to the    api    of an id56-
   the lstm at each time step receives an input and its previous state,
   and uses those two inputs to compute an updated state and an output
   vector^[9]2.)

   now we have a model that at each time step gets not only the current
   word representation, but also the state of the lstm from the previous
   time step, and uses this to predict the next word. the state of the
   lstm is a representation of the previously seen words (note that words
   that we saw recently have a much larger impact on this state than words
   we saw a while ago).

   as expected, performance improves and the perplexity of this model on
   the test set is about 114. an implementation of this model^[10]3, along
   with a detailed explanation, is available in [11]tensorflow.

the importance of id173

   114 perplexity is good but we can still do much better. in this section
   i   ll present some recent advances that improve the performance of id56
   based language models.

dropout

   we could try improving the network by increasing the size of the
   embeddings and lstm layers (until now the size we used was 200), but
   soon enough this stops increasing the performance because the network
   overfits the training data (it uses its increased capacity to remember
   properties of the training set which leads to inferior generalization,
   i.e. performance on the unseen test set). one way to counter this, by
   regularizing the model, is to use dropout.

   the diagram below is a visualization of the id56 based model unrolled
   across three time steps. x and y are the input and output sequences,
   and the gray boxes represent the lstm layers. vertical arrows represent
   an input to the layer that is from the same time step, and horizontal
   arrows represent connections that carry information from previous time
   steps.
   [no_dropout.png]

   we can apply dropout on the vertical (same time step) connections:
   [regular_dropout.png]

   the arrows are colored in places where we apply dropout. a dropout mask
   for a certain layer indicates which of that layers activations are
   zeroed. in this case, we use different dropout masks for the different
   layers (this is indicated by the different colors in the diagram).

   applying dropout to the recurrent connections harms the performance,
   and so in this initial use of dropout we use it only on connections
   within the same time step. using two lstm layers, with each layer
   containing 1500 lstm units, we achieve a perplexity of 78 (we dropout
   activations with a id203 of 0.65)^[12]4.

   the recently introduced [13]variational dropout solves this problem and
   improves the model   s performance even more (to 75 perplexity) by using
   the same dropout masks at each time step.
   [variational_dropout.png]

weight tying

   the input embedding and output embedding have a few properties in
   common. the first property they share is that they are both of the same
   size (in our id56 model with dropout they are both of size
   (10000,1500)).

   the second property that they share in common is a bit more subtle. in
   the input embedding, words that have similar meanings are represented
   by similar vectors (similar in terms of [14]cosine similarity). this is
   because the model learns that it needs to react to similar words in a
   similar fashion (the words that follow the word    quick    are similar to
   the ones that follow the word    rapid   ).

   this also occurs in the output embedding. the output embedding receives
   a representation of the id56s belief about the next output word (the
   output of the id56) and has to transform it into a distribution. given
   the representation from the id56, the id203 that the decoder
   assigns a word depends mostly on its representation in the output
   embedding (the id203 is exactly the softmax normalized dot
   product of this representation and the output of the id56).

   given the id56 output at a certain time step, the model would like to
   assign similar id203 values to similar words. therefore, similar
   words are represented by similar vectors in the output embedding.
   (again, if a certain id56 output results in a high id203 for the
   word    quick   , we expect that the id203 for the word    rapid    will
   be high as well.)

   these two similarities led us to recently propose a very simple method,
   [15]weight tying, to lower the model   s parameters and improve its
   performance. we simply tie its input and output embedding (i.e. we set
   u=v, meaning that we now have a single embedding matrix that is used
   both as an input and output embedding). this reduces the perplexity of
   the id56 model that uses dropout to 73, and its size is reduced by more
   than 20%^[16]5.

why does weight tying work?

   the perplexity of the variational dropout id56 model on the test set is
   75. the same model achieves 24 perplexity on the training set. so the
   model performs much better on the training set then it does on the test
   set. this means that it has started to remember certain patterns or
   sequences that occur only in the train set and do not help the model to
   generalize to unseen data. one of the ways to counter this overfitting
   is to reduce the model   s ability to    memorize    by reducing its capacity
   (number of parameters). by applying weight tying, we remove a large
   number of parameters.

   in addition to the regularizing effect of weight tying we presented
   another reason for the improved results. we showed that in untied
   language models the word representations in the output embedding are of
   much higher quality than the ones in the input embedding. this is shown
   using embedding evaluation benchmarks such as [17]siid113x999. in a
   weight tied model, because the tied embedding   s parameter updates at
   each training iteration are very similar to the updates of the output
   embedding of the untied model, the tied embedding performs similarly to
   the output embedding of the untied model. so in the tied model, we use
   a single high quality embedding matrix in two places in the model. this
   contributes to the improved performance of the tied model^[18]6.

   to summarize, this post presented how to improve a very simple
   feedforward neural network language model, by first adding an id56, and
   then adding variational dropout and weight tying to it.

   in recent months, we   ve seen further improvements to the state of the
   art in id56 id38. the current state of the art results are
   held by two recent papers by [19]melis et al. and [20]merity et al..
   these models make use of most, if not all, of the methods shown above,
   and extend them by using better optimization techniques, new
   id173 methods, and by finding better hyperparameters for
   existing models.
     __________________________________________________________________

    1. this model is the skip-gram id97 model presented in
       [21]efficient estimation of word representations in vector
       space. [22]   
    2. for a detailed explanation of this watch edward grefenstette   s
       [23]beyond id195 with augmented id56s lecture. [24]   
    3. this model is the small model presented in [25]recurrent neural
       network id173. [26]   
    4. this is the large model from [27]recurrent neural network
       id173. [28]   
    5. in parallel to our work, an explanation for weight tying based on
       [29]distilling the knowledge in a neural network was presented in
       [30]tying word vectors and word classifiers: a loss framework for
       id38. [31]   
    6. our [32]paper explains this in detail. [33]   

   written on september 7, 2017

references

   visible links
   1. https://ofir.io/feed.xml
   2. https://www.googletagmanager.com/ns.html?id=gtm-pzggp5q
   3. https://ofir.io/
   4. https://ofir.io/
   5. https://ofir.io/about
   6. https://ofir.io/neural-language-modeling-from-scratch/#fn:sg
   7. http://karpathy.github.io/2015/05/21/id56-effectiveness/
   8. http://colah.github.io/posts/2015-08-understanding-lstms/
   9. https://ofir.io/neural-language-modeling-from-scratch/#fn:api
  10. https://ofir.io/neural-language-modeling-from-scratch/#fn:zaremba
  11. https://www.tensorflow.org/tutorials/recurrent
  12. https://ofir.io/neural-language-modeling-from-scratch/#fn:zarembalarge
  13. https://arxiv.org/abs/1512.05287
  14. https://en.wikipedia.org/wiki/cosine_similarity#definition
  15. http://aclweb.org/anthology/e/e17/e17-2025.pdf
  16. https://ofir.io/neural-language-modeling-from-scratch/#fn:inan
  17. https://www.cl.cam.ac.uk/~fh295/siid113x.html
  18. https://ofir.io/neural-language-modeling-from-scratch/#fn:paper
  19. https://arxiv.org/abs/1707.05589
  20. https://arxiv.org/abs/1708.02182
  21. https://arxiv.org/abs/1301.3781
  22. https://ofir.io/neural-language-modeling-from-scratch/#fnref:sg
  23. http://videolectures.net/deeplearning2016_grefenstette_augmented_id56/
  24. https://ofir.io/neural-language-modeling-from-scratch/#fnref:api
  25. https://arxiv.org/abs/1409.2329
  26. https://ofir.io/neural-language-modeling-from-scratch/#fnref:zaremba
  27. https://arxiv.org/abs/1409.2329
  28. https://ofir.io/neural-language-modeling-from-scratch/#fnref:zarembalarge
  29. https://arxiv.org/abs/1503.02531
  30. https://arxiv.org/abs/1611.01462
  31. https://ofir.io/neural-language-modeling-from-scratch/#fnref:inan
  32. http://aclweb.org/anthology/e/e17/e17-2025.pdf
  33. https://ofir.io/neural-language-modeling-from-scratch/#fnref:paper

   hidden links:
  35. mailto:ofirp@cs.washington.edu
  36. https://www.twitter.com/ofirpress
