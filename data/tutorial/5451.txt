   #[1]machine thoughts    feed [2]machine thoughts    comments feed
   [3]machine thoughts    vector semantics or vector categories comments
   feed [4]grounding [5]wittgenstein and mentalese [6]alternate
   [7]alternate [8]machine thoughts [9]wordpress.com

   [10]machine thoughts
   [11]skip to content
     * [12]home
     * [13]about

   [14]    grounding
   [15]wittgenstein and mentalese    

[16]vector semantics or vector categories

   posted on [17]august 2, 2014 by [18]mcallester

this was originally posted on wednesday august 7, 2013

   the basic idea in vector semantics is that the    meaning    of a word (or
   phrase or sentence) can be expressed as a vector.  for a long time i
   thought that this was just silly.  however, i have recently become more
   sympathetic to the work done in this area.  it seems to me, however,
   that what is really being done is    vector categories    rather than
      vector semantics   .

   before going any further i will look up a sentence from
   news.google.com.  the first sentence of the lead story at the moment of
   this writing is:

   with his decision to cancel next month   s planned summit with russia   s
   president vladimir putin, president obama took a principled stance that
   has lawmakers on both sides of the aisle cheering him for finally
   standing up to an international bully.

   i believe that reference is fundamental to semantics.  i think
   reference should be viewed as a mapping from a mention (a phrase in a
   sentence) to an entity in a database model of reality.  in the above
   sentence the phrases    obama   ,    putin   ,    next month   s summit   ,
      lawmakers   and    his decision    all refer to entities that the author
   seems to expect readers are already familiar with. the phrases    took a
   stance    and    stood up to    seem to be co-referential with    his decision   
   and, at the same time, assert properties of, or descriptions of, the
   entity (the action).  the reference interpretation of semantics is
   emphasized in [19]a recent post on google   s research blog.

   vector semantics seems to be in direct conflict with modeling reality
   by a database and taking reference     the mapping from mentions to
   database entities     as core to semantics.  however, if we interpret the
   vectors as vector categories  rather than as a denotational meaning (a
   referent) then these vectors seem quite useful.

   my first exposure to vector semantics was in the context of [20]latent
   semantic indexing (lsi) which assigns a vector to each word by doing
   singular value decomposition on a word-document matrix. the vector
   assigned to a word in this way can perhaps be viewed as a
   representation of a distribution over topics.  indeed we might expect
      was not cancerous    and    was cancerous    to have the same topic
   distribution     they both will be common in certain medical diagnosis
   documents     even though they have opposite meanings.

   a more interesting interpretation of vector semantics (in my opinion)
   was pointed out to me by
   michael collins.  he has been doing work on using id106 for
   [21]learning of latent syntactic subcategories.  here we assume that
   each standard syntactic category, such as np, has some latent set of
   subcategories     perhaps    person       place    or    thing   .  we can then
   assign a vector to a phrase where the vector can be interpreted as
   assigning a score for each possible latent subcategory.

   vector representations for the purpose of scoring    grammaticality   
   naturally yield factored representations analogous to [22]formal models
   of english grammar based on feature structures.
   the idea is that, for the purpose of determining grammaticality, the
   syntactic category of a phrase has features such as gender, person, and
   number.  feature structures can be viewed as a simple kind of vector
   with discrete values along each coordinate.  this interpretation of
   vector semantics is especially suggested by the observation that
   certain directions in the vector space can be interpreted as gender as
   in the equation phi(queen) = phi(king)     phi(man) + phi(woman) where
   phi(w) is the vector associated with word w [[23]paper].

   perhaps even more interestingly, we can abandon any direct
   interpretation of the vectors associated with phrases other than that
   they be useful in parsing and then discriminatively train parsers which
   compute latent vectors at each node.  the vectors can be even be
   computed by deep neural networks trained for this purpose [[24]paper].
    while the precise meaning of the vectors remains obscure, the fact
   that they are trained to produce accurate parsing would seem to imply
   that they encode information relevant to the selectivity of words for
   arguments     what things do people    give    or    eat   .  many parsers use
   bilexical statistics     actual verb-object pair statistics     but
   abstracting the words to vectors should allow greater generalization to
   unseen pairs.

   it has also been proposed that vector semantics can usefully encode
   relations analogous to the relations of a database [[25]paper].
   although vectors, like databases and bit strings, are
   information-theoretically universal (a single real number can hold an
   infinite number of bits), i do not believe that vectors should replace
   databases as a model of the facts of reality.  however, it does seem
   possible that vectors have an important role to play in uncertain
   id136 such as that modeled by [26]markov logic networks.  a markov
   logic network is essentially a weighted sat formula where the sum of
   the weight of violated clauses is interpreted as an energy (cost) for
   any given truth assignment to the boolean variables.  each boolean
   variable typically has some internal structure, such as r(b,c) where r
   is a relation and b and c are entities.  we can assign entities vector
   categories and assign r a matrix for combining the categories of its
   arguments.  we can then compute an energy r(b,c) where this energy
   intuitively represents a negative log prior id203 of r(b,c).  in
   this way vector categories could play an important role in some variant
   of markov logic and could even be trained so as to produce correct
   id136s.

   the bottom line is that the term    vector categories    seems more
   appropriate than   vector semantics   . semantics remains, for me, the
   relationship between language and a database model of reality.
   advertisements

share this:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

   this entry was posted in [29]uncategorized. bookmark the [30]permalink.
   [31]    grounding
   [32]wittgenstein and mentalese    

leave a reply [33]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [34]googleplus-sign-in

     *
     *

   [35]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [36]log out /
   [37]change )
   google photo

   you are commenting using your google account. ( [38]log out /
   [39]change )
   twitter picture

   you are commenting using your twitter account. ( [40]log out /
   [41]change )
   facebook photo

   you are commenting using your facebook account. ( [42]log out /
   [43]change )
   [44]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * recent posts
          + [45]thoughts from ttic31230: rethinking generalization.
          + [46]thoughts from ttic31230: langevin dynamics and the
            struggle for the soul of sgd.
          + [47]thoughts from ttic31230: hyperparameter conjugacy
          + [48]superintelligence and the truth
          + [49]predictive coding and mutual information
          + [50]rl and the game of mathematics
          + [51]the role of theory in deep learning
          + [52]choice as a natural kind term
          + [53]ctc and the eg algotithm: discrete latent choices without
            id23
          + [54]vae = em
          + [55]deep meaning beyond thought vectors
          + [56]the plausibility of near-term machine sentience.
          + [57]formalism, platonism and mentalese
          + [58]comprehension based id38
          + [59]cognitive architectures
          + [60]architectures and language instincts
          + [61]why we need a new foundation of mathematics
          + [62]the foundations of mathematics.
          + [63]friendly ai and the servant mission
          + [64]ai and free will: when do choices exist?
       advertisements

   [65]machine thoughts
   [66]wordpress.com.

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [67]cookie policy

   iframe: [68]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinethoughts.wordpress.com/feed/
   2. https://machinethoughts.wordpress.com/comments/feed/
   3. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/feed/
   4. https://machinethoughts.wordpress.com/2014/08/02/grounding/
   5. https://machinethoughts.wordpress.com/2014/08/02/wittgenstein-and-mentalese/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/&for=wpcom-auto-discovery
   8. https://machinethoughts.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinethoughts.wordpress.com/
  11. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#content
  12. https://machinethoughts.wordpress.com/
  13. https://machinethoughts.wordpress.com/about/
  14. https://machinethoughts.wordpress.com/2014/08/02/grounding/
  15. https://machinethoughts.wordpress.com/2014/08/02/wittgenstein-and-mentalese/
  16. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  17. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  18. https://machinethoughts.wordpress.com/author/dmcallester/
  19. http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html
  20. http://www.cob.unt.edu/itds/faculty/evangelopoulos/dsci5910/lsa_deerwester1990.pdf
  21. http://delivery.acm.org/10.1145/2400000/2390556/p223-cohen.pdf?ip=128.135.8.37&id=2390556&acc=open&key=d22043b48bb0dd1814ede8873b041416f529ce01effa0bc2&cfid=352436571&cftoken=10591263&__acm__=1375900597_bf256d2221e0275920ddf5608f043646
  22. http://www.3gnow.ru/other/dvd-009/sag_a.i.,_wasow_t._syntactic_theory[c]_a_formal_introduction_(1999)(en)(475s).pdf
  23. http://acl.eldoc.ub.rug.nl/mirror/n/n13/n13-1090.pdf
  24. http://www-cs.stanford.edu/people/ang/papers/nipsdlufl10-learningcontinuousphraserepresentations.pdf
  25. http://arxiv.org/pdf/1301.3618.pdf
  26. http://link.springer.com/article/10.1007/s10994-006-5833-1#page-1
  27. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/?share=twitter
  28. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/?share=facebook
  29. https://machinethoughts.wordpress.com/category/uncategorized/
  30. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  31. https://machinethoughts.wordpress.com/2014/08/02/grounding/
  32. https://machinethoughts.wordpress.com/2014/08/02/wittgenstein-and-mentalese/
  33. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#respond
  34. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://machinethoughts.wordpress.com&color_scheme=light
  35. https://gravatar.com/site/signup/
  36. javascript:highlandercomments.doexternallogout( 'wordpress' );
  37. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  38. javascript:highlandercomments.doexternallogout( 'googleplus' );
  39. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  40. javascript:highlandercomments.doexternallogout( 'twitter' );
  41. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  42. javascript:highlandercomments.doexternallogout( 'facebook' );
  43. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/
  44. javascript:highlandercomments.cancelexternalwindow();
  45. https://machinethoughts.wordpress.com/2019/04/03/thoughts-from-ttic21230-rethinking-generalization/
  46. https://machinethoughts.wordpress.com/2019/03/27/thoughts-from-ttic31230-langevin-dynamics-and-the-struggle-for-the-soul-of-sgd/
  47. https://machinethoughts.wordpress.com/2019/03/20/thoughts-from-ttic31230-hyperparameter-conjugacy/
  48. https://machinethoughts.wordpress.com/2018/09/18/superintelligence-and-the-truth/
  49. https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/
  50. https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/
  51. https://machinethoughts.wordpress.com/2017/12/08/the-role-of-theory-in-deep-learning/
  52. https://machinethoughts.wordpress.com/2017/11/29/choice-as-a-natural-kind-term/
  53. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  54. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  55. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  56. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  57. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  58. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  59. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  60. https://machinethoughts.wordpress.com/2016/06/12/architectures-and-language-instincts/
  61. https://machinethoughts.wordpress.com/2016/01/01/why-we-need-a-new-foundation-of-mathematics/
  62. https://machinethoughts.wordpress.com/2015/01/27/the-foundations-of-mathematics-2/
  63. https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/
  64. https://machinethoughts.wordpress.com/2014/08/03/ai-and-free-will-when-do-choices-exist/
  65. https://machinethoughts.wordpress.com/
  66. https://wordpress.com/?ref=footer_custom_com
  67. https://automattic.com/cookies
  68. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  70. https://machinethoughts.wordpress.com/
  71. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#comment-form-guest
  72. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#comment-form-load-service:wordpress.com
  73. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#comment-form-load-service:twitter
  74. https://machinethoughts.wordpress.com/2014/08/02/vector-semantics-or-vector-categories/#comment-form-load-service:facebook
