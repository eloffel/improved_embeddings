   #[1]ricky han blog

   [2]ricky han blog

   [3]about

gradient trader part 1: the surprising usefulness of autoencoders

   sep 14, 2017

using autoencoders to learn most salient features from time series

   this post is about a simple tool in deep learning toolbox: autoencoder.
   it can be applied to multi-dimensional financial time series.

autoencoder

   autoencoding is the practice of copying input to output or learning the
   identity function. it has an internal state called latent space which
   is used to represent the input. usually, this dimension is chosen to be
   smaller than the input(called undercomplete). autoencoder is composed
   of two parts: an encoder and a decoder .

   autoencoder(keras)

   the hidden dimension should be smaller than , the input dimension. this
   way, is forced to take on useful properties and most salient features
   of the input space.

   train an autoencoder to find function such that:

recurrent autoencoder

   for time series data, recurrent autoencoder are especially useful. the
   only difference is that the encoder and decoder are replaced by id56s
   such as lstms. think of id56 as a for loop over time step so the state
   is kept. it can be unrolled into a feedforward network.

   first, the input is encoded into an undercomplete latent vector which
   is then decoded by the decoder. recurrent autoencoder is a special case
   of sequence-to-sequence(id195) architecture which is extremely
   powerful in id4 where the neural network maps
   one language sequence to another.

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

task

   copy a tensor of two sine functions that are initialized out of phase.

   the shape of the input is a tensor of shape .

   is the number of batches for training. looping over each sample is
   slower than applying a tensor operation on a batch of several samples.

   is the number of timeframes for the id56 to iterate over. in this
   tutorial it is 10 because 10 points are generated.

   is the number of data points at each timestep. here we have 2
   functions, so this number is 2.

   to deal with financial data, simply replace the axis with desired data
   points.
     * bid, ask, spread, volume, rsi. for this setup, the would be 5.
     * order book levels. we can rebin the order book along tick axis so
       each tick aggregates more liquidity. an example would be 10 levels
       that are 1 stdev apart. then would be 10.

   here is an artist   s rendition of a recurrent autoencoder.
plt.xkcd()

x1 = np.linspace(-np.pi, np.pi)
y1 = np.sin(x1)
phi = 3
x2 = np.linspace(-np.pi+phi, np.pi+phi)
y2 = np.sin(x2)

f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=true, sharey=true)
ax1.plot(x1, y1)
ax1.set_title('recurrent autoencoder')
ax2.plot(x1, y1)
ax3.plot(x2, y2)
ax4.plot(x2, y2)
plt.show()

   png

generator

   for each batch, generate 2 sine functions, each with 10 datapoints. the
   phase of the sine function is random.
import random
def gen(batch_size):
    seq_length = 10

    batch_x = []
    batch_y = []
    for _ in range(batch_size):
        rand = random.random() * 2 * np.pi

        sig1 = np.sin(np.linspace(0.0 * np.pi + rand,
                                  3.0 * np.pi + rand, seq_length * 2))
        sig2 = np.cos(np.linspace(0.0 * np.pi + rand,
                                  3.0 * np.pi + rand, seq_length * 2))
        x1 = sig1[:seq_length]
        y1 = sig1[seq_length:]
        x2 = sig2[:seq_length]
        y2 = sig2[seq_length:]

        x_ = np.array([x1, x2])
        y_ = np.array([y1, y2])
        x_, y_ = x_.t, y_.t

        batch_x.append(x_)
        batch_y.append(y_)

    batch_x = np.array(batch_x)
    batch_y = np.array(batch_y)

    return batch_x, batch_x#batch_y

model

   the goal is to use two numbers to represent the sine functions.
   normally, we use to represent the phase angle for a trignometric
   function. let   s see if the neural network can learn this phase angle.
   the big picture here is to compress the input sine functions into two
   numbers and then decode them back.

   define the architecture and let the neural network do its trick. it is
   a model with 3 layers, a lstm encoder that    encodes    the input time
   series into a fixed length vector(in this case 2). a repeatvector that
   repeats the fixed length vector to 10 timesteps to be used as input to
   the lstm decoder. for the decoder, we can either initialize the hidden
   state(memory) with the latent vector and use output at time as input
   for time or we can use latent vector as the input at each timestep.
   these are called conditional and unconditional decoders.
from keras.models import sequential, model
from keras.layers import lstm, repeatvector

batch_size = 100
x_train, _ = gen(batch_size)

m = sequential()
m.add(lstm(2, input_shape=(10, 2)))
m.add(repeatvector(10))
m.add(lstm(2, return_sequences=true))
print m.summary()
m.compile(loss='mse', optimizer='adam')
history = m.fit(x_train, x_train, nb_epoch=2000, batch_size=100)

epoch 1/2000
100/100 [==============================] - 0s - loss: 0.4845
epoch 2000/2000
100/100 [==============================] - 0s - loss: 0.0280

m.summary()

_________________________________________________________________
layer (type)                 output shape              param #
=================================================================
lstm_37 (lstm)               (none, 2)                 40
_________________________________________________________________
repeat_vector_12 (repeatvect (none, 10, 2)             0
_________________________________________________________________
lstm_38 (lstm)               (none, 10, 2)             40
=================================================================
total params: 80
trainable params: 80
non-trainable params: 0
_________________________________________________________________

   since this post is a demonstration of the technique, we use the
   smallest model possible which happens to be the best in this case. the
      best    dimensionality will be one that results in the highest lossless
   compression. in practice, it   s mostly an art than science. in
   production, there are a plethora of trick to accelerate training and
   finding the right capacity of the latent vector. topics include:
   architectures for dealing with asynchronus, non-stationary time series,
   preprocessing techniques such as wavelet transforms and fft.
plt.plot(history.history['loss'])
plt.ylabel("loss")
plt.xlabel("epoch")
plt.show()

   png

   you may think that the neural network suddenly    got it    during training
   but this is just the optimzer escaping a saddle point. in fact, as we
   demonstrate below, the neural network(at least the decoder) doesn   t
   learn the math behind it at all.
x_test, _ = gen(1)
decoded_imgs = m.predict(x_test)

for i in range(2):
    plt.plot(range(10), decoded_imgs[0, :, i])
    plt.plot(range(10), x_test[0, :, i])
plt.title(dos_numeros)
plt.show()

   png

   with more training, it would be even closer to input but that is not
   necessary for this demonstration.

   visualize the latent vector can be useful sometimes but not in this
   case.
encoder = model(m.layers[0].input, m.layers[0].output)
encoded_imgs = encoder.predict(x_test)
for i in range(len(encoded_imgs)):
    plt.imshow(encoded_imgs[i].reshape((1, 2)))
    plt.gray()
plt.title(encoded_imgs[i])
dos_numeros = encoded_imgs[i]
plt.show()


   png
m.save('m.h5')
encoder.save('enc.h5')
decoder.save('dec.h5')

visualization

   the model successfully compressed 20 points to 2 points. it learned the
   most salient features from data. however in this case, the model is
   useless in that it has successfully learned everywhere. autoencoder are
   designed to copy imperfectly. like all neural networks, autoencoders
   exploit the idea that data concentrates around a lower-dimensional
   manifold or a set of manifolds. the goal is to learn a function that
   behaves correctly on the data points from this manifold and exhibit
   unusual behavior everywhere off this manifold. in this case, we found a
   2-d manifold tangent to the 20-d space. here we discuss some
   visualization techniques.

   below we demonstrate what the manifold looks like when it is projected
   back to 20d with an animation. ideally this shows how the decoder
   works.
x = input(shape=(2,))
decoded = m.layers[1](x)
decoded = m.layers[2](decoded)
decoder = model(x, decoded)

import matplotlib.animation as animation
from ipython.display import html
from numpy import random


fig, ax = plt.subplots()

x = range(10)

arr = decoder.predict(np.array([[-3.3, 0]]))
linea, = ax.plot(x, arr[0, :, 0])
lineb, = ax.plot(x, arr[0, :, 1])
ax.set_ylim(-1, 1)
ax.set_xlim(-0.1, 10.1)

def animate(i):
    ax.set_title("{0:.2f}, 0".format(-3.3+i/30.))
    arr = decoder.predict(np.array([[-3.3+i/30., 0]]))
    linea.set_ydata(arr[0, :, 0])
    lineb.set_ydata(arr[0, :, 1])
    return line,


# init only required for blitting to give a clean slate.
def init():
    line.set_ydata(np.ma.array(x, mask=true))
    return line,

ani = animation.funcanimation(fig, animate, np.arange(1, 200), init_func=init,
                              interval=25, blit=true)
html(ani.to_html5_video())

   your browser does not support the video tag.
fig, ax = plt.subplots()

x = range(10)

arr = decoder.predict(np.array([[-3.3, -3.3]]))
linea, = ax.plot(x, arr[0, :, 0])
lineb, = ax.plot(x, arr[0, :, 1])
ax.set_ylim(-1, 1)
ax.set_xlim(-0.1, 10.1)

def animate(i):
    ax.set_title("{0:.2f}, {0:.2f}".format(-3.3+i/30.))
    arr = decoder.predict(np.array([[-3.3+i/30., -3.3+i/30.]]))
    linea.set_ydata(arr[0, :, 0])
    lineb.set_ydata(arr[0, :, 1])
    return line,


# init only required for blitting to give a clean slate.
def init():
    line.set_ydata(np.ma.array(x, mask=true))
    return line,

ani = animation.funcanimation(fig, animate, np.arange(1, 200), init_func=init,
                              interval=25, blit=true)
html(ani.to_html5_video())

   your browser does not support the video tag.

   as we can see from the animation above. the neural network is not using
   phase angle as the latent space. instead, it is using a complex
   interaction between these two numbers caused by the gradient of lstm
   during the training process. as , the decoder . this is understood as
   the decoder does not learn the underlying trig function but merely
   approximated a manifold of the high dimensional space.

   now let us visualize the latent space in its entirety. for higher
   dimensional latent space we may encounter in financial data, we can use
   pca and tsne but these are beyond the scope of this post.
x_test, _ = gen(1000)
latent_vec = encoder.predict(x_test)
plt.scatter(latent_vec[:, 0], latent_vec[:, 1])
plt.xlim(-1,1)
plt.ylim(-1,1)
plt.show()

   png

   this circle you see above is the image under the encoder in the latent
   space . as you may have guessed from the beginning, since the trig map
   projects cartesian coordinate to a polar coordinate. the encoded vector
   is a linear subspace. a circle in polar coordinate has the equation . i
   hope this is a good enough punchline for those who didn   t hypothesize
   this result.

   with pca, any high-dimensional space can be projected into several
   linear subspaces using the gram-schmidt algorithm in id202.

anomaly detection

   as you can see, autoencoding is an extremely powerful technique in data
   visualization and exploration. but there are more depth to autoencoding
   besides creating pretty graphs. one application is anomaly detection.

   regulators can identify illegal trading strategies by building an
   unsupervised deep learning algorithm.

   in data mining, anomaly detection (or outlier detection) is the
   identification of observations that do not conform to an expected
   pattern in a dataset. the ability to train a neural network in
   unsupervised manner allows autoencoders to shine in this area. auto
   encoders provide a very powerful alternative to traditional methods for
   signal reconstruction and anomaly detection in time series.

   training an autoencoder is conceptually simple:
    1. train with training set with id173.
    2. evaluate on and determine the capacity of the autoencoder.
    3. when we believe it   s good enough after inspecting error
       visualization, choose a threshold of error(such as ) to determine
       whether the data point is an anomaly or not. of course, since we
       are dealing with time series, this threshold should be chosen on a
       rolling basis (sliding window).

   in financial time series(and other series), we often want to detect
   regime shifts, i.e. know before the price shifts abruptly. in my
   experiements, this is particularly effective in detecting iceberg
   orders which is a single large order divided into severel smaller ones
   for the purpose of hiding the actual order quantity. detecting iceberg
   orders using ordinary machine learning methods is difficult, but
   obvious upon human inspection.

   in the next part we will simulate how anomaly detection may work. first
   let us plot the mean square error of the autoencoder output.
x_test, _ = gen(1)
decoded_imgs = m.predict(x_test)

for i in range(2):
    plt.plot(range(10), np.square(np.abs(x_test[0, :, i] - decoded_imgs[0, :, i]
)), label="mse")

plt.ylim(-1,1)
plt.xlim(0,10)
plt.title(dos_numeros)
plt.show()

   png

   now let   s change one data point in the generator. this signifies a
   regime shift.
x_test, _ = gen(1)
x_test[0, 4, 0] = x_test[0, 4, 0] + 0.5
decoded_imgs = m.predict(x_test)

for i in range(2):
    plt.plot(decoded_imgs[0, :, i])
    plt.plot(x_test[0, :, i])

plt.ylim(-1,1)
plt.xlim(0,10)
plt.show()

   png
for i in range(2):
    mse = np.square(np.abs(x_test[0, :, i] - decoded_imgs[0, :, i]))
    anomaly = np.argmax(mse)
    if mse[anomaly] > 0.4:
        plt.title("anomaly detected in series {} at time step: {}".format(i, ano
maly))
    plt.plot(range(10), mse, label="mse")

plt.ylim(-1,1)
plt.xlim(0,10)
plt.show()

   png

a note on mse

   also, assume the financial series is normalized to be a zero-centered
   signal, mse would be a mal-adapted measure. because if the an outlier
   is near 0, then the mse will likely be less than the mse of the
   original signal(centered at 0). so it is imperative to devise a more
   sophisticated id168 by adding a moving average to the mse. this
   way, we can judge based on the deviations from the rolling average.

a note on online training / id136

   this algorithm, like all signal processing neural networks, should be
   able to operate on a rolling basis. the algorithm just needs a segment
   of the data from a sliding window.

   when attention mechanism is introduced, online training(input partially
   observed) becomes more involved. online and linear-time attention
   alignment can be achieved with memory mechanism. i might cover
   attention mechanism for financial data in a future post.

pattern search

   given an example trading pattern, quickly identify similar patterns in
   a multi-dimensional data series.

   implementation:
     * train a stacked recurrent autoencoder on a dataset of sequences.
     * encode the query sequence pattern to obtain .
     * then for all historical data, use the encoder to produce fixed
       length vectors .
     * for each , find .

   it is basically a fuzzy hash function. this i have not tried since
   calculating the hash for all sequence requires serious compute only
   available to big instituional players.

stacked autoencoders

   this is a good place to introduce how stacked autoencoders work.

   as we have seen above, a simple recurrent autoencoder has 3 layers:
   encoder lstm layer, hidden layer, and decoder lstm layer. stacked
   autoencoders is constructed by stacking several single-layer
   autoencoders. the first single-layer autoencoder maps input to the
   first hidden vector. after training the first autoencoder, we discard
   the first decoder layer which is then replaced by the second
   autoencoder, which has a smaller latent vector dimension. repeat this
   process and determine the correct depth and size by trial and error(if
   you know a better way please let me know.) the depth of stacked
   autoencoders enable feature invariance and allow for more abstractness
   in extracted features.

a note on supervised learning

   this post only covered the unsupervised usage of autoencoders.
   variational recurrent autoencoders can    denoise    artifically corrupted
   data input. i never had any luck with it so i can   t recommend the usage
   on financial data.

   financial data is extremely noisy. but when trained with a smaller
   capacity, autoencoders have the built-in ability of denoising because
   they only learn the most salient features anyway.

a note on variational id136 / bayesian method

   please refer to this [4]article.

conclusion

   autoencoders are powerful tools despite their simplistic nature. we use
   autoencoders to visualize manifold on high dimensional space and detect
   anomalies in time series.

reference

https://arxiv.org/pdf/1511.01432.pdf

http://www1.icsi.berkeley.edu/~vinyals/files/id56_denoise_2012.pdf

http://www.deeplearningbook.org/contents/autoencoders.html

http://wp.doc.ic.ac.uk/hipeds/wp-content/uploads/sites/78/2017/01/steven_hutt_de
ep_networks_financial.pdf

https://www.cs.cmu.edu/~epxing/class/10708-17/project-reports/project8.pdf

https://arxiv.org/pdf/1610.09513.pdf

https://arxiv.org/pdf/1012.0349.pdf

https://arxiv.org/pdf/1204.1381.pdf


ricky han blog

     * ricky han blog
     * [5]rickyhan+blog@rickyhan.com
     * [6]subscribe

   programming demos, tips, thoughts.

references

   visible links
   1. http://rickyhan.com/feed.xml
   2. https://rickyhan.com/
   3. https://rickyhan.com/about/
   4. https://jaan.io/what-is-variational-autoencoder-vae-tutorial/
   5. mailto:rickyhan+blog@rickyhan.com
   6. https://tinyletter.com/rickyhan

   hidden links:
   8. https://rickyhan.com/jekyll/update/2017/09/14/autoencoders.html
