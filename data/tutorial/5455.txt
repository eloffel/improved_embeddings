   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]sicara's blog
     * [9]machine learning
     * [10]id161
     * [11]data engineering
     * [12]dataviz
     * [13]best of ai
     * [14]about sicara
     __________________________________________________________________

keras tutorial: content based id162 using a convolutional denoising
autoencoder

   [15]go to the profile of adil baaj
   [16]adil baaj (button) blockedunblock (button) followfollowing
   sep 14, 2017

   content based id162 (cbir) systems enable to find similar
   images to a query image among an image dataset. the most famous cbir
   system is the search per image feature of google search. this article
   uses the [17]keras deep learning framework to perform id162
   on the [18]mnist dataset.

   our cbir system will be based on a convolutional denoising autoencoder.
   it is a class of unsupervised deep learning algorithms.

content based id162

   to explain what content based id162 (cbir) is, i am going to
   quote this [19]research paper.

     there are two [id162] frameworks: text-based and
     content-based. the text-based approach can be tracked back to 1970s.
     in such systems, the images are manually annotated by text
     descriptors, which are then used by a database management system to
     perform id162. there are two disadvantages with this
     approach. the first is that a considerable level of human labour is
     required for manual annotation. the second is the annotation
     inaccuracy due to the subjectivity of human perception. to overcome
     the above disadvantages in text-based retrieval system,
     content-based id162 (cbir) was introduced in the early
     1980s. in cbir, images are indexed by their visual content, such as
     color, texture, shapes.

   basically we first extract features from an image database and store
   it. then we compute the features associated with a query image. finally
   we retrieve images with the closest features.
   [1*a8xgoirfgustxdfmglrkww.png]

feature extraction for content based id162

   the key point about content based id162 is the feature
   extraction. the features correspond to the way we represent an image on
   a high level. how to describe the colours on an image? its texture? the
   shapes on it? the features we extract should also allow an efficient
   retrieval of the images. this is especially true if we have a big image
   database.

   there are many ways to extract these features.

   one way is to use what we call hand crafted features. examples are:
   [20]histogram of colours to define colours, [21]histogram of oriented
   gradients to define shapes.

   other descriptors like [22]sift and [23]surf have proven to be robust
   for id162 applications.

   another possibility is to use deep learning algorithms. in this
   [24]research paper the authors demonstrate that [25]convolutional
   neural networks (id98) trained for classification purposes can be used
   to extract a    neural code    for images. these neural codes are the
   features used to describe images. it also demonstrates that it performs
   as well as state of the art approaches on many datasets. the problem
   about this approach is that we first need labelled data to train the
   neural network. the labelling task can be costly and time consuming.
   another way to generate these    neural codes    for our id162
   task is to use an unsupervised deep learning algorithm. this is where
   the denoising autoencoder comes.

denoising autoencoder

   a denoising autoencoder is a feed forward neural network that learns to
   denoise images. by doing so the neural network learns interesting
   features on the images used to train it. then it can be used to extract
   features from similar images to the training set.
   [1*g0v4dz4rktkgpebeoswb0a.png]

   if you are not familiar with autoencoders, i highly recommend to first
   browse these three sources:
     * [26]deep learning book
     * course [27]videos by hugo larochelle
     * [28]autoencoder keras tutorial

denoising autoencoder for content based id162

   we use the convolutional denoising autoencoder algorithm provided on
   [29]keras tutorial.

training the model

   iframe: [30]/media/0a37bef5b15c9782a6548bd5c7b163a4?postid=dc91450cc511

   for the general explanations on the above lines of code please refer to
   [31]keras tutorial.

   notice that there are small differences compared to the tutorial. the
   first difference is this line:
encoded = maxpooling2d((2, 2), padding='same', name='encoder')(x)

   we set a name to the encoder layer in order to be able to access it.

   we also saved the learned model by adding:
autoencoder.save('autoencoder.h5')

   this will enable us to load it later in order to test it.

   finally, we reduced the number of epochs from 100 to 20 in order to
   save time :).

denoising an image

   let   s try our learned model to denoise an input test image.

   first we regenerate the noisy data and load the previously trained
   autoencoder.

   iframe: [32]/media/a405206214b7b0ccc8a77a35f370b9c6?postid=dc91450cc511

   then we call the following function that denoises the first noisy test
   image and plot it:

   iframe: [33]/media/da9d96537af02246aac6dc83b291a7d5?postid=dc91450cc511

   the result is
   [1*ontq0fsdqomnlzznpyg75a.jpeg]
   noisy image
   [1*gdxkinml31jhwcckk75rqw.jpeg]
   denoised image

computing the features of the training dataset

   our image database is the mnist training dataset.

   our goal is to provide a query image and find the closest mnist images.

   first, we compute the features of the training dataset and the query
   images:

   iframe: [34]/media/b32468d2c385fc1369948632a30b3286?postid=dc91450cc511

scoring function

   before scoring our model we need to understand the scoring function we
   will use.

   to assess the model, we use the [35]scikit learn function:
   [36]label_ranking_average_precision_score. this function takes two
   arrays as input. first an array of zeros and ones. second an array of
   relevance scores.

   in our case, we compute the relevance score from the computed distance
   between the feature of the query image and the images of the database.
   the lower the distance the higher the relevance score should be.

   we construct the first array following this rule: for each image on
   database, if the image has the same label as the query image, we append
   a    1    to the array. otherwise we append a    0   .

   this scoring function returns a maximum score of 1 if the closest
   images have the same label as the query image. if there are images with
   a different label that are closer to the query image, the score
   decreases.

   to get a feel of what it does let   s compute the value of this scoring
   function on some examples.

   suppose we have a query image with label    7    and that we have four
   images in our database with following labels :    7   ,    7   ,    1   ,    0   . the
   first two images of the database are relevant regarding the query
   image, and the two last ones are not. the first array that we pass to
   the scoring function should be [1, 1, 0, 0]. for each image on our
   image database we will compute a relevance score:

   iframe: [37]/media/5c61772e0d45f843f1f61e4aa9c20e11?postid=dc91450cc511

scoring the model

   for each query image feature, we compute the euclidian distance to the
   training dataset images features. the closer the distance the higher
   the relevance score should be. then we apply the scoring function
   label_ranking_average_precision_score to our results.

   iframe: [38]/media/30287236c65a45b35765af2892f0d100?postid=dc91450cc511

results

   the y axis correspond to the score computed with the label ranking
   average precision scoring function. the x axis corresponds to the n
   first results assessed.
   [1*ehup-nzbf2sec_97dhl3lw.png]

   to better understand this graph i will give an example. suppose we have
   a database of 3images with labels 7, 7, 1 . and suppose the input image
   has a label 7. if our algorithm sorts the results on the following
   order: 7, 1, 7. first we will score only the first returned image [7]:
   the scoring function returns 1. then we assess the first two images
   returned [7, 1]: the scoring function return 1. then we assess the
   first three results [7, 1, 7]: the score decreases and is now equal to
   0.83 etc   

   overall the more retrieved images we assess the worse the score is.

   example:
   [1*7dlcu2hgbai48b7lbudnpg.png]
   query image
   [1*2ijrcgyhp98m1gr72e_giw.png]
   first 10 retrieved images

   you can find the full code [39]here.

conclusion

   we tested an id162 deep learning algorithm on a basic
   dataset. our convolutional denoising autoencoder is efficient when
   considering the first retrieved images. but we tested it on similar
   images. we didn   t have to deal with color, scaling and rotation issues.

want to go further?

   to learn more on autoencoders for cbir you can read this [40]research
   paper from alex krizhevsky and georey hinton.
     __________________________________________________________________

   if you liked this article please share it or leave your feedback below.

   want more articles like this one? don   t forget to hit the follow
   button!

   iframe: [41]/media/4832dea2286a7211b06f2907b8ed6dea?postid=dc91450cc511

     * [42]machine learning
     * [43]deep learning
     * [44]neural networks
     * [45]python
     * [46]id161

   (button)
   (button)
   (button) 713 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of adil baaj

[48]adil baaj

   at the intersection of data science, web applications development and
   ui / ux. working [49]@sicara_fr

     (button) follow
   [50]sicara's blog

[51]sicara's blog

   we build tailor-made ai and big data solutions for amazing clients

     * (button)
       (button) 713
     * (button)
     *
     *

   [52]sicara's blog
   never miss a story from sicara's blog, when you sign up for medium.
   [53]learn more
   never miss a story from sicara's blog
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.sicara.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/dc91450cc511
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.sicara.com/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511&source=--------------------------nav_reg&operation=register
   8. https://blog.sicara.com/?source=logo-lo_io6zlo5c3c1f---fd4c083fbb93
   9. https://blog.sicara.com/machine-learning/home
  10. https://blog.sicara.com/computer-vision/home
  11. https://blog.sicara.com/data-engineering/home
  12. https://blog.sicara.com/data-visualization/home
  13. https://blog.sicara.com/best-of-ai/home
  14. https://www.sicara.com/en
  15. https://blog.sicara.com/@adilbaaj?source=post_header_lockup
  16. https://blog.sicara.com/@adilbaaj
  17. https://keras.io/
  18. http://yann.lecun.com/exdb/mnist/
  19. http://www.baskent.edu.tr/~hogul/ra1.pdf
  20. https://en.wikipedia.org/wiki/color_histogram
  21. https://en.wikipedia.org/wiki/histogram_of_oriented_gradients
  22. https://en.wikipedia.org/wiki/scale-invariant_feature_transform
  23. https://en.wikipedia.org/wiki/speeded_up_robust_features
  24. https://arxiv.org/pdf/1404.1777.pdf
  25. http://cs231n.github.io/convolutional-networks/
  26. http://www.deeplearningbook.org/
  27. https://www.youtube.com/watch?v=fzs3tml4nsc
  28. https://blog.keras.io/building-autoencoders-in-keras.html
  29. https://blog.keras.io/building-autoencoders-in-keras.html
  30. https://blog.sicara.com/media/0a37bef5b15c9782a6548bd5c7b163a4?postid=dc91450cc511
  31. https://blog.keras.io/building-autoencoders-in-keras.html
  32. https://blog.sicara.com/media/a405206214b7b0ccc8a77a35f370b9c6?postid=dc91450cc511
  33. https://blog.sicara.com/media/da9d96537af02246aac6dc83b291a7d5?postid=dc91450cc511
  34. https://blog.sicara.com/media/b32468d2c385fc1369948632a30b3286?postid=dc91450cc511
  35. http://scikit-learn.org/stable/
  36. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html
  37. https://blog.sicara.com/media/5c61772e0d45f843f1f61e4aa9c20e11?postid=dc91450cc511
  38. https://blog.sicara.com/media/30287236c65a45b35765af2892f0d100?postid=dc91450cc511
  39. https://github.com/adilbaaj/unsupervised-image-retrieval
  40. http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf
  41. https://blog.sicara.com/media/4832dea2286a7211b06f2907b8ed6dea?postid=dc91450cc511
  42. https://blog.sicara.com/tagged/machine-learning?source=post
  43. https://blog.sicara.com/tagged/deep-learning?source=post
  44. https://blog.sicara.com/tagged/neural-networks?source=post
  45. https://blog.sicara.com/tagged/python?source=post
  46. https://blog.sicara.com/tagged/computer-vision?source=post
  47. https://blog.sicara.com/@adilbaaj?source=footer_card
  48. https://blog.sicara.com/@adilbaaj
  49. http://twitter.com/sicara_fr
  50. https://blog.sicara.com/?source=footer_card
  51. https://blog.sicara.com/?source=footer_card
  52. https://blog.sicara.com/
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://medium.com/p/dc91450cc511/share/twitter
  56. https://medium.com/p/dc91450cc511/share/facebook
  57. https://medium.com/p/dc91450cc511/share/twitter
  58. https://medium.com/p/dc91450cc511/share/facebook
