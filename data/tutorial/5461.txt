   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

deep learning for id164: a comprehensive review

   [16]go to the profile of joyce xu
   [17]joyce xu (button) blockedunblock (button) followfollowing
   sep 11, 2017
   [1*fttevgsx0jfvusfb6x5mqg.jpeg]

   with the rise of autonomous vehicles, smart video surveillance, facial
   detection and various people counting applications, fast and accurate
   id164 systems are rising in demand. these systems involve
   not only recognizing and classifying every object in an image, but
   localizing each one by drawing the appropriate bounding box around it.
   this makes id164 a significantly harder task than its
   traditional id161 predecessor, image classification.

   fortunately, however, the most successful approaches to object
   detection are currently extensions of image classification models. a
   few months ago, google released a [18]new id164 api for
   tensorflow. with this release came the pre-built architectures and
   weights for a [19]few specific models:
     * [20]single shot multibox detector (ssd) with [21]mobilenets
     * ssd with [22]inception v2
     * [23]region-based fully convolutional networks (r-fcn) with
       [24]resnet 101
     * [25]faster rid98 with resnet 101
     * [26]faster rid98 with [27]inception resnet v2

   in my [28]last blog post, i covered the intuition behind the three base
   network architectures listed above: mobilenets, inception, and resnet.
   this time around, i want to do the same for tensorflow   s object
   detection models: faster r-id98, r-fcn, and ssd. by the end of this
   post, we will hopefully have gained an understanding of how deep
   learning is applied to id164, and how these id164
   models both inspire and diverge from one another.

faster r-id98

   faster r-id98 is now a canonical model for deep learning-based object
   detection. it helped inspire many detection and segmentation models
   that came after it, including the two others we   re going to examine
   today. unfortunately, we can   t really begin to understand faster r-id98
   without understanding its own predecessors, r-id98 and fast r-id98, so
   let   s take a quick dive into its ancestry.

r-id98

   r-id98 is the grand-daddy of faster r-id98. in other words, r-id98 really
   kicked things off.

   r-id98, or region-based convolutional neural network, consisted of 3
   simple steps:
    1. scan the input image for possible objects using an algorithm called
       selective search, generating ~2000 region proposals
    2. run a convolutional neural net (id98) on top of each of these region
       proposals
    3. take the output of each id98 and feed it into a) an id166 to classify
       the region and b) a linear regressor to tighten the bounding box of
       the object, if such an object exists.

   these 3 steps are illustrated in the image below:
   [1*d2sfql329qkkx4tvl31ihq.png]

   in other words, we first propose regions, then extract features, and
   then classify those regions based on their features. in essence, we
   have turned id164 into an image classification problem.
   r-id98 was very intuitive, but very slow.

fast r-id98

   r-id98   s immediate descendant was fast-r-id98. fast r-id98 resembled the
   original in many ways, but improved on its detection speed through two
   main augmentations:
    1. performing feature extraction over the image before proposing
       regions, thus only running one id98 over the entire image instead of
       2000 id98   s over 2000 overlapping regions
    2. replacing the id166 with a softmax layer, thus extending the neural
       network for predictions instead of creating a new model

   the new model looked something like this:
   [1*iwyuwipo-5ka2ecafaapsg.png]

   as we can see from the image, we are now generating region proposals
   based on the last feature map of the network, not from the original
   image itself. as a result, we can train just one id98 for the entire
   image.

   in addition, instead of training many different id166   s to classify each
   object class, there is a single softmax layer that outputs the class
   probabilities directly. now we only have one neural net to train, as
   opposed to one neural net and many id166   s.

   fast r-id98 performed much better in terms of speed. there was just one
   big bottleneck remaining: the selective search algorithm for generating
   region proposals.

faster r-id98

   at this point, we   re back to our original target: faster r-id98. the
   main insight of faster r-id98 was to replace the slow selective search
   algorithm with a fast neural net. specifically, it introduced the
   region proposal network (rpn).

   here   s how the rpn worked:
     * at the last layer of an initial id98, a 3x3 sliding window moves
       across the feature map and maps it to a lower dimension (e.g.
       256-d)
     * for each sliding-window location, it generates multiple possible
       regions based on k fixed-ratio anchor boxes (default bounding
       boxes)
     * each region proposal consists of a) an    objectness    score for that
       region and b) 4 coordinates representing the bounding box of the
       region

   in other words, we look at each location in our last feature map and
   consider k different boxes centered around it: a tall box, a wide box,
   a large box, etc. for each of those boxes, we output whether or not we
   think it contains an object, and what the coordinates for that box are.
   this is what it looks like at one sliding window location:
   [1*7hex-no7cdqllky-gwgbfq.png]

   the 2k scores represent the softmax id203 of each of the k
   bounding boxes being on    object.    notice that although the rpn outputs
   bounding box coordinates, it does not try to classify any potential
   objects: its sole job is still proposing object regions. if an anchor
   box has an    objectness    score above a certain threshold, that box   s
   coordinates get passed forward as a region proposal.

   once we have our region proposals, we feed them straight into what is
   essentially a fast r-id98. we add a pooling layer, some fully-connected
   layers, and finally a softmax classification layer and bounding box
   regressor. in a sense, faster r-id98 = rpn + fast r-id98.
   [1*lhk_cczzfp9mzw280kg70w.png]

   altogether, faster r-id98 achieved much better speeds and a
   state-of-the-art accuracy. it is worth noting that although future
   models did a lot to increase detection speeds, few models managed to
   outperform faster r-id98 by a significant margin. in other words, faster
   r-id98 may not be the simplest or fastest method for id164,
   but it is still one of the best performing. case in point, tensorflow   s
   faster r-id98 with inception resnet is their [29]slowest but most
   accurate model.

   at the end of the day, faster r-id98 may look complicated, but its core
   design is the same as the original r-id98: hypothesize object regions
   and then classify them. this is now the predominant pipeline for many
   id164 models, including our next one.

r-fcn

   remember how fast r-id98 improved on the original   s detection speed by
   sharing a single id98 computation across all region proposals? that kind
   of thinking was also the motivation behind r-fcn: increase speed by
   maximizing shared computation.

   r-fcn, or region-based fully convolutional net, shares 100% of the
   computations across every single output. being [30]fully convolutional,
   it ran into a unique problem in model design.

   on the one hand, when performing classification of an object, we want
   to learn location invariance in a model: regardless of where the cat
   appears in the image, we want to classify it as a cat. on the other
   hand, when performing detection of the object, we want to learn
   location variance: if the cat is in the top left-hand corner, we want
   to draw a box in the top left-hand corner. so if we   re trying to share
   convolutional computations across 100% of the net, how do we compromise
   between location invariance and location variance?

   r-fcn   s solution: position-sensitive score maps.

   each position-sensitive score map represents one relative position of
   one object class. for example, one score map might activate wherever it
   detects the top-right of a cat. another score map might activate where
   it sees the bottom-left of a car. you get the point. essentially, these
   score maps are convolutional feature maps that have been trained to
   recognize certain parts of each object.

   now, r-fcn works as follows:
    1. run a id98 (in this case, resnet) over the input image
    2. add a fully convolutional layer to generate a score bank of the
       aforementioned    position-sensitive score maps.    there should be
       k  (c+1) score maps, with k   representing the number of relative
       positions to divide an object (e.g. 3   for a 3 by 3 grid) and c+1
       representing the number of classes plus the background.
    3. run a fully convolutional region proposal network (rpn) to generate
       regions of interest (roi   s)
    4. for each roi, divide it into the same k      bins    or subregions as
       the score maps
    5. for each bin, check the score bank to see if that bin matches the
       corresponding position of some object. for example, if i   m on the
          upper-left    bin, i will grab the score maps that correspond to the
          upper-left    corner of an object and average those values in the
       roi region. this process is repeated for each class.
    6. once each of the k   bins has an    object match    value for each
       class, average the bins to get a single score per class.
    7. classify the roi with a softmax over the remaining c+1 dimensional
       vector

   altogether, r-fcn looks something like this, with an rpn generating the
   roi   s:
   [1*chevy3e2hw65af-mpemwog.png]

   even with the explanation and the image, you might still be a little
   confused on how this model works. honestly, r-fcn is much easier to
   understand when you can visualize what it   s doing. here is one such
   example of an r-fcn in practice, detecting a baby:
   [1*q20ddanzqbvbjg4dlvjkgg.png]

   simply put, r-fcn considers each region proposal, divides it up into
   sub-regions, and iterates over the sub-regions asking:    does this look
   like the top-left of a baby?   ,    does this look like the top-center of a
   baby?       does this look like the top-right of a baby?   , etc. it repeats
   this for all possible classes. if enough of the sub-regions say    yes, i
   match up with that part of a baby!   , the roi gets classified as a baby
   after a softmax over all the classes.

   with this setup, r-fcn is able to simultaneously address location
   variance by proposing different object regions, and location invariance
   by having each region proposal refer back to the same bank of score
   maps. these score maps should learn to classify a cat as a cat,
   regardless of where the cat appears. best of all, it is fully
   convolutional, meaning all of the computation is shared throughout the
   network.

   as a result, r-fcn is several times faster than faster r-id98, and
   achieves comparable accuracy.

ssd

   our final model is ssd, which stands for single-shot detector. like
   r-fcn, it provides enormous speed gains over faster r-id98, but does so
   in a markedly different manner.

   our first two models performed region proposals and region
   classifications in two separate steps. first, they used a region
   proposal network to generate regions of interest; next, they used
   either fully-connected layers or position-sensitive convolutional
   layers to classify those regions. ssd does the two in a    single shot,   
   simultaneously predicting the bounding box and the class as it
   processes the image.

   concretely, given an input image and a set of ground truth labels, ssd
   does the following:
    1. pass the image through a series of convolutional layers, yielding
       several sets of feature maps at different scales (e.g. 10x10, then
       6x6, then 3x3, etc.)
    2. for each location in each of these feature maps, use a 3x3
       convolutional filter to evaluate a small set of default bounding
       boxes. these default bounding boxes are essentially equivalent to
       faster r-id98   s anchor boxes.
    3. for each box, simultaneously predict a) the bounding box offset and
       b) the class probabilities
    4. during training, match the ground truth box with these predicted
       boxes based on [31]iou. the best predicted box will be labeled a
          positive,    along with all other boxes that have an iou with the
       truth >0.5.

   ssd sounds straightforward, but training it has a unique challenge.
   with the previous two models, the region proposal network ensured that
   everything we tried to classify had some minimum id203 of being
   an    object.    with ssd, however, we skip that filtering step. we
   classify and draw bounding boxes from every single position in the
   image, using multiple different shapes, at several different scales. as
   a result, we generate a much greater number of bounding boxes than the
   other models, and nearly all of the them are negative examples.

   to fix this imbalance, ssd does two things. firstly, it uses
   [32]non-maximum suppression to group together highly-overlapping boxes
   into a single box. in other words, if four boxes of similar shapes,
   sizes, etc. contain the same dog, nms would keep the one with the
   highest confidence and discard the rest. secondly, the model uses a
   technique called [33]hard negative mining to balance classes during
   training. in hard negative mining, only a subset of the negative
   examples with the highest training loss (i.e. false positives) are used
   at each iteration of training. ssd keeps a 3:1 ratio of negatives to
   positives.

   its architecture looks like this:
   [1*p-lsawysbsibzlcwz9_umw.png]

   as i mentioned above, there are    extra feature layers    at the end that
   scale down in size. these varying-size feature maps help capture
   objects of different sizes. for example, here is ssd in action:
   [1*juhjyuwxgfxmmoa4siklka.png]

   in smaller feature maps (e.g. 4x4), each cell covers a larger region of
   the image, enabling them to detect larger objects. region proposal and
   classification are performed simultaneously: given p object classes,
   each bounding box is associated with a (4+p)-dimensional vector that
   outputs 4 box offset coordinates and p class probabilities. in the last
   step, softmax is again used to classify the object.

   ultimately, ssd is not so different from the first two models. it
   simply skips the    region proposal    step, instead considering every
   single bounding box in every location of the image simultaneously with
   its classification. because ssd does everything in one shot, it is the
   [34]fastest of the three models, and still performs quite comparably.

conclusion

   faster r-id98, r-fcn, and ssd are three of the best and most widely used
   id164 models out there right now. other popular models tend
   to be fairly similar to these three, all relying on deep id98   s (read:
   resnet, inception, etc.) to do the initial heavy lifting and largely
   following the same proposal/classification pipeline.

   at this point, putting these models to use just requires knowing
   tensorflow   s api. tensorflow has a starter tutorial on using these
   models [35]here. give it a try, and happy hacking!

     * [36]machine learning
     * [37]data science
     * [38]artificial intelligence
     * [39]id161
     * [40]towards data science

   (button)
   (button)
   (button) 5.8k claps
   (button) (button) (button) 24 (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of joyce xu

[42]joyce xu

   deep learning, rl, nlp, comp. vision, and all that jazz.
   [43]@deepmindai, [44]@stanford

     (button) follow
   [45]towards data science

[46]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 5.8k
     * (button)
     *
     *

   [47]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [48]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/73930816d8d9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vzsm8ofakwu2---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@joycex99?source=post_header_lockup
  17. https://towardsdatascience.com/@joycex99
  18. https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html
  19. https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md
  20. https://arxiv.org/abs/1512.02325
  21. http://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html
  22. https://arxiv.org/abs/1512.00567
  23. https://arxiv.org/abs/1605.06409
  24. https://arxiv.org/abs/1512.03385
  25. https://arxiv.org/abs/1506.01497
  26. https://arxiv.org/abs/1506.01497
  27. https://arxiv.org/abs/1602.07261
  28. https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41
  29. https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md
  30. https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_segmentation.html
  31. https://en.wikipedia.org/wiki/jaccard_index
  32. https://docs.microsoft.com/en-us/cognitive-toolkit/object-detection-using-fast-r-id98#algorithm-details
  33. https://arxiv.org/pdf/1608.02236.pdf
  34. https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md
  35. https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb
  36. https://towardsdatascience.com/tagged/machine-learning?source=post
  37. https://towardsdatascience.com/tagged/data-science?source=post
  38. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  39. https://towardsdatascience.com/tagged/computer-vision?source=post
  40. https://towardsdatascience.com/tagged/towards-data-science?source=post
  41. https://towardsdatascience.com/@joycex99?source=footer_card
  42. https://towardsdatascience.com/@joycex99
  43. http://twitter.com/deepmindai
  44. http://twitter.com/stanford
  45. https://towardsdatascience.com/?source=footer_card
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/
  48. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  50. https://medium.com/p/73930816d8d9/share/twitter
  51. https://medium.com/p/73930816d8d9/share/facebook
  52. https://medium.com/p/73930816d8d9/share/twitter
  53. https://medium.com/p/73930816d8d9/share/facebook
