   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]buzzrobot
     * [9]about
     * [10]advisors
     * [11]subscribe
     __________________________________________________________________

   [1*ebb-c3-4k5ijp4hpb53phw.jpeg]
   artwork by [12]robert aguilera

5 ways to get started with id23

   [13]go to the profile of harshvardhan gupta
   [14]harshvardhan gupta (button) blockedunblock (button) followfollowing
   sep 5, 2017

   machine learning algorithms, and neural networks in particular, are
   considered to be the cause of a new ai    revolution   . in this article i
   will introduce the concept of id23 but with limited
   technical details so that readers with a variety of backgrounds can
   understand the essence of the technique, its capabilities and
   limitations.

   at the end of the article, i will provide links to a few resources for
   implementing rl.

what is id23?

   broadly speaking, data-driven algorithms can be categorized into three
   types: supervised, unsupervised, and id23.

   the first two are generally used to perform tasks such as image
   classification, detection, etc. while their accuracy is remarkable,
   these tasks differ from those that we would expect from an
      intelligent    being.

   this is where id23 comes in. the concept itself is
   very simple, and much like our evolutionary process: the environment
   rewards the agent for things that it gets right and penalizes it for
   things that it gets wrong. the main challenge is developing the
   capacity to learn several million possible ways of doing things.

id24 & deep id24

   id24 is a widely used id23 algorithm. without
   going into the detailed math, the given quality of an action is
   determined by what state the agent is in. the agent usually performs
   the action which gives it the maximum reward. the detailed math can be
   found[15] here.

   in this algorithm, the agent learns the quality(q value) of each action
   (action is also called policy) based on how much reward the environment
   gave it. the value of each environment   s state, along with the q value
   is usually stored in a table. as the agent interacts with the
   environment, the q values get updated from random values to values that
   actually help maximize reward.

deep id24

   the problem with using id24 with tables is that it doesn   t scale
   well. if the number of states is too high, the table will not fit in
   memory. this is where deep id24 could be applied. deep learning
   is basically just a universal approximation machine which can
   understand and come up with abstract representations. deep learning can
   be used to approximate q values, and it can also easily learn optimal q
   values by using id119.

     fun fact:

     google has a patent on some elements of deep id24:[16]
     https://www.google.com/patents/us20150100530

exploration vs exploitation

   it is often the case that the agent memorizes one path and will never
   try to explore any other paths. in general, we would like an agent to
   not only exploit good paths, but also sometimes explore new paths that
   it can perform actions in. therefore, a hyper-parameter, named   , is
   used to govern how much to explore new paths vs how much to exploit old
   paths.

experience replay

   when training a neural network, data imbalance plays a very important
   role. if a model is trained as the agent interacts with the
   environment, there will be imbalances. the most recent play will
   obviously have more bearing than older plays.

   therefore, all the states, along with related data, is stored in the
   memory, and the neural network can randomly pick a batch of some
   interactions and learn (this makes it very similar to supervised
   learning).

the training framework

   this is what the whole framework for deep id24 looks like. note
   the     . this represents the discounted reward. it is a hyperparameter
   that controls how much weight the future reward will have. the symbol  
   denotes next. e.g. s   denotes next state.
   [1*d4fesdreeotpl9thhggflg.png]
   figure 1.0 deep id24 training framework. credit:
   [17]robert aguilera

extending id23

   id23 works well with many things (such as[18]
   alphago), but it often fails in places where the feedback is sparse.
   the agent will not explore behaviors that are actually beneficial in
   the long term. sometimes, exploring some actions is needed for its own
   sake (intrinsic motivation) instead of directly trying to solve
   problems.

   doing this allows the agent to perform complicated actions and
   essentially allows the agent to    plan    things.[19] hierarchical
   learning allows for such kinds of abstract learning.
   [1*uvj314umqy_fx1nkgpgk2g.png]
   figure 2.0 hierarchical deep id24

   in this kind of a setup, there are two q networks. they are represented
   as the controller and meta-controller. the meta controller looks at the
   raw states and calculates which    goal    to follow. the controller takes
   in the states along with the goal and outputs a policy to solve the
   goal. the critic checks if the goal is reached and gives some reward to
   the controller. the controller stops when the episode ends, or when the
   goal is reached. the meta controller then chooses a new goal, and this
   repeats.

   the    goal    is something that will eventually help the agent get to the
   final reward. this is better because it   s possible to have id24
   on top of id24 in a hierarchical fashion.

introductory resources for id23

   this list will be helpful for those who are looking to get started with
   id23:
    1. [20]the basics of deep id24. very helpful for understanding
       the math and processes of id23.
    2. [21]the hierarchical learning paper, for those who want to
       understand hierarchical learning in detail.
    3. [22]hierarchical learning paper explanation video from the authors.
    4. [23]deep rl: an overview what i would consider the reinforcement
       learning handbook. it covers nearly every aspect of rl that is
       required to understand the current level of research. it delves
       deep into the math, but also provides high-level overviews.
    5. [24]implementing deep id24 with a single python script.
       perhaps the simplest deep id24 implementation. this is very
       easy to read and a great starting point.

   [1*_us0tm4xocy1qeaznwdtka.gif]
   figure 3.0 deep id24 in action. output of python script in
   point 5
     __________________________________________________________________

call to action

   if you think this article was helpful, hold the clap icon                  for as
   long as you want. it lets me know my article was helpful. if you have
   comments or questions, feel free to respond to this article below.

   big thanks to [25]robert aguilera for making the artwork and the
   flowchart.

     * [26]artificial intelligence
     * [27]machine learning
     * [28]data science
     * [29]id23
     * [30]technology

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of harshvardhan gupta

[32]harshvardhan gupta

   stupid and hungry

     (button) follow
   [33]buzzrobot

[34]buzzrobot

   the publication aims to cover practical aspects of ai technology along
   with interviews with notable people in the ai field.

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [35]buzzrobot
   never miss a story from buzzrobot, when you sign up for medium.
   [36]learn more
   never miss a story from buzzrobot
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://buzzrobot.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b96d1989c575
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575&source=--------------------------nav_reg&operation=register
   8. https://buzzrobot.com/?source=logo-lo_i5xltfwzoy34---47d409e68707
   9. https://buzzrobot.com/about
  10. https://buzzrobot.com/advisors-38cd35a7a8e1
  11. https://upscri.be/3b4d4f/
  12. http://robertaguileradesign.com/
  13. https://buzzrobot.com/@harshsayshi?source=post_header_lockup
  14. https://buzzrobot.com/@harshsayshi
  15. https://en.wikipedia.org/wiki/id24
  16. https://www.google.com/patents/us20150100530
  17. http://robertaguileradesign.com/
  18. https://deepmind.com/research/alphago/
  19. https://arxiv.org/pdf/1604.06057.pdf
  20. https://www.intelnervana.com/demystifying-deep-reinforcement-learning/
  21. https://arxiv.org/pdf/1604.06057.pdf
  22. https://www.youtube.com/watch?v=tyruql_zr7q
  23. https://arxiv.org/abs/1701.07274
  24. https://gist.github.com/edersantana/c7222daa328f0e885093#file-qlearn-py-l157
  25. http://robertaguileradesign.com/
  26. https://buzzrobot.com/tagged/artificial-intelligence?source=post
  27. https://buzzrobot.com/tagged/machine-learning?source=post
  28. https://buzzrobot.com/tagged/data-science?source=post
  29. https://buzzrobot.com/tagged/reinforcement-learning?source=post
  30. https://buzzrobot.com/tagged/technology?source=post
  31. https://buzzrobot.com/@harshsayshi?source=footer_card
  32. https://buzzrobot.com/@harshsayshi
  33. https://buzzrobot.com/?source=footer_card
  34. https://buzzrobot.com/?source=footer_card
  35. https://buzzrobot.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/b96d1989c575/share/twitter
  39. https://medium.com/p/b96d1989c575/share/facebook
  40. https://medium.com/p/b96d1989c575/share/twitter
  41. https://medium.com/p/b96d1989c575/share/facebook
