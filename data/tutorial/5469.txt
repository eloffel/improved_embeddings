
   [1]

the stanford natural language processing group

the stanford nlp group

     * [2]people
     * [3]publications
     * [4]research blog
     * [5]software
     * [6]teaching
     * [7]local

[8]software > [9]stanford parser > neural network dependency parser

introduction

   a dependency parser analyzes the grammatical structure of a sentence,
   establishing relationships between "head" words and words which modify
   those heads. the figure below shows a dependency parse of a short
   sentence. the arrow from the word moving to the word faster indicates
   that faster modifies moving, and the label advmod assigned to the arrow
   describes the exact nature of the dependency.
   [nndep-example.png]

   we have built a super-fast transition-based parser which produces typed
   dependency parses of natural language sentences. the parser is powered
   by a neural network which accepts id27 inputs, as described
   in the paper:

     danqi chen and christopher manning. 2014. [10]a fast and accurate
     dependency parser using neural networks. in proceedings of emnlp
     2014.

   this parser supports english (with [11]universal dependencies,
   [12]stanford dependencies and conll dependencies) and chinese (with
   conll dependencies). future versions of the software will support other
   languages.

how transition-based parsing works

   for a quick introduction to the standard approach to transition-based
   id33, see [13]joakim nivre's eacl 2014 tutorial.

   this parser builds a parse by performing a linear-time scan over the
   words of a sentence. at every step it maintains a partial parse, a
   stack of words which are currently being processed, and a buffer of
   words yet to be processed.

   the parser continues to apply transitions to its state until its buffer
   is empty and the dependency graph is completed.

   the initial state is to have all of the words in order on the buffer,
   with a single dummy root node on the stack. the following transitions
   can be applied:
     * left-arc: marks the second item on the stack as a dependent of the
       first item, and removes the second item from the stack (if the
       stack contains at least two items).
     * right-arc: marks the first item on the stack as a dependent of the
       second item, and removes the first item from the stack (if the
       stack contains at least two items).
     * shift: removes a word from the buffer and pushes it onto the stack
       (if the buffer is not empty).

   with just these three types of transitions, a parser can generate any
   projective dependency parse. note that for a typed dependency parser,
   with each transition we must also specify the type of the relationship
   between the head and dependent being described.

   the parser decides among transitions at each state using a neural
   network classifier. distributed representations (dense, continuous
   vector representations) of the parser's current state are provided as
   inputs to this classifier, which then chooses among the possible
   transitions to make next. these representations describe various
   features of the current stack and buffer contents in the parser state.

   the classifier which powers the parser is trained using an oracle. this
   oracle takes each sentence in the training data and produces many
   training examples indicating which transition should be taken at each
   state to reach the correct final parse. the neural network is trained
   on these examples using adaptive id119 (adagrad) with hidden
   unit dropout.

obtaining the software

   you may download either of the following packages:
     * [14]the stanford parser and the [15]stanford pos tagger; or
     * all of [16]stanford corenlp, which contains the parser, the tagger,
       and other things which you may or may not need.

models

   trained models for use with this parser are included in either of the
   packages. the list of models currently distributed is:

     edu/stanford/nlp/models/parser/nndep/english_ud.gz (default,
     english, [17]universal dependencies)
     edu/stanford/nlp/models/parser/nndep/english_sd.gz (english,
     [18]stanford dependencies)
     edu/stanford/nlp/models/parser/nndep/ptb_conll_params.txt.gz
     (english, [19]conll dependencies)
     edu/stanford/nlp/models/parser/nndep/ctb_conll_params.txt.gz
     (chinese, [20]conll dependencies)

   note that these models were trained with an earlier matlab version of
   the code, and your results training with the java code may be slightly
   worse.

usage

command-line interface

   the dependency parser can be run as part of the larger corenlp
   pipeline, or run directly (external to the pipeline).

using the stanford corenlp pipeline

   this parser is integrated into [21]stanford corenlp as a new annotator.

   if you want to use the transition-based parser from the command line,
   invoke stanfordcorenlp with the depparse annotator. this annotator has
   dependencies on the tokenize, ssplit, and pos annotators. an example
   invocation follows (assuming corenlp is on your classpath):

     java edu.stanford.nlp.pipeline.stanfordcorenlp -annotators
     tokenize,ssplit,pos,depparse -file <input_file>

direct access (with stanford parser or corenlp)

   it is also possible to access the parser directly in the stanford
   parser or stanford corenlp packages. with direct access to the parser,
   you can train new models, evaluate models with test treebanks, or parse
   raw sentences. note that this package currently still reads and writes
   conll-x files, notconll-u files.

   the main program to use is the class
   [22]edu.stanford.nlp.parser.nndep.dependencyparser. the javadoc for
   this class' [23]main method describes all possible options in details.
   some usage examples follow:
     * parse raw text from a file:

     java edu.stanford.nlp.parser.nndep.dependencyparser -model
     modeloutputfile.txt.gz -textfile rawtexttoparse -outfile
     dependenciesoutputfile.txt
     * parse raw text from standard input, writing to standard output:

     java edu.stanford.nlp.parser.nndep.dependencyparser -model
     modeloutputfile.txt.gz -textfile - -outfile -
     * parse a conll-x file, writing the output as a conll-x file:

     java edu.stanford.nlp.parser.nndep.dependencyparser -model
     edu/stanford/nlp/models/parser/nndep/english_ud.gz -testfile
     test.conllx -outfile test-output.conllx

   the options for specifying files to the parser at training and test
   time are:
   option required for training required for testing / parsing description
   -devfile optional no path to a development-set treebank in [24]conll-x
   format. if provided, the dev set performance is monitored during
   training.
   -embedfile optional (recommended!) no a id27 file, containing
   distributed representations of english words. each line of the provided
   file should contain a single word followed by the elements of the
   corresponding id27 (space-delimited). it is not absolutely
   necessary that all words in the treebank be covered by this embedding
   file, though the parser's performance will generally improve if you are
   able to provide better embeddings for more words.
   -model yes yes path to a model file. if the path ends in .gz, the model
   will be read as a gzipped model file. during training, we write to this
   path; at test time we read a pre-trained model from this path.
   -textfile no yes (or testfile) path to a plaintext file containing
   sentences to be parsed.
   -testfile no yes (or textfile) path to a test-set treebank in
   [25]conll-x format for final evaluation of the parser.
   -trainfile yes no path to a training treebank in [26]conll-x format.

programmatic access

included demo

   it's also possible to use this parser directly in your own java code.
   there is an dependencyparserdemo example class in the package
   [27]edu.stanford.nlp.parser.nndep.demo, included in the source of the
   stanford parser and the source of corenlp.

java api

   the parser exposes an api for both training and testing. you can find
   more information in [28]our javadoc.

training your own parser

   you can train a new dependency parser using your own data in the
   [29]conll-x data format. (many dependency treebanks are provided in
   this format by default; even if not, conversion is often trivial.)

basic guidelines

   to train a new english model, you need the following pieces of data:
     * a dependency treebank, split into training, development, and test
       segments. (most treebanks come with a predetermined split.)
     * a id27 file, containing distributed representations of
       english words. it is not absolutely necessary that all words in the
       treebank be covered by this embedding file, though the parser's
       performance will generally improve if you are able to provide
       better embeddings for more words.
       this id27 file is only used for training. the parser will
       build its own improved embeddings and save them as part of the
       learned model.

   to start training with the data described above, run this command with
   the parser on your classpath:

     java edu.stanford.nlp.parser.nndep.dependencyparser -trainfile
     <trainpath> -devfile <devpath> -embedfile <wordembeddingfile>
     -embeddingsize <wordembeddingdimensionality> -model
     nndep.model.txt.gz

   on the stanford nlp machines, training data is available in
   /u/nlp/data/depparser/nn/data:

     java edu.stanford.nlp.parser.nndep.dependencyparser \
         -trainfile
     /u/nlp/data/depparser/nn/data/dependency_treebanks/ptb_stanford/trai
     n.conll \
         -devfile
     /u/nlp/data/depparser/nn/data/dependency_treebanks/ptb_stanford/dev.
     conll \
         -embedfile /u/nlp/data/depparser/nn/data/embeddings/en-cw.txt
     -embeddingsize 50 \
         -model nndep.model.txt.gz

training models for other languages

   to train the parser for languages other than english, you need the data
   as described in the [30]previous section, along with a
   treebanklanguagepack describing the particularities of your treebank
   and the language it contains. (the stanford parser package may already
   contain a tlp for your language of choice: check the package
   [31]edu.stanford.nlp.trees.international.)

   note that at test time, a language appropriate tagger will also be
   necessary.

   for example, here is a command used to train a chinese model. the only
   difference from the english case (apart from the fact that we changed
   datasets) is that we also provide a different treebanklanguagepack
   class with the -tlp option.

     java edu.stanford.nlp.parser.nndep.dependencyparser -tlp
     edu.stanford.nlp.trees.international.pennchinese.chinesetreebanklang
     uagepack -trainfile chinese/train.conll -devfile chinese/dev.conll
     -embedfile chinese/embeddings.txt -embeddingsize 50 -model
     nndep.chinese.model.txt.gz

   the only complicated part here is the treebanklanguagepack, which is a
   java class you need to provide. it's not hard to write. it's only used
   for a couple of things: a default character encoding, a list of
   punctuation pos tags and sentence final punctuation words, and to
   specify a tokenizer (which you might also need to write). some of
   these, like the tokenizer, are only needed for running the parser on
   raw text, and you can train and test on conll files without one.
   getting started, if your language uses the latin alphabet, you can
   probably get away with using the default english treebanklanguagepack,
   penntreebanklanguagepack.

additional training options

   option default description
   -adaalpha 0.01 global learning rate for adagrad training.
   -adaeps 1e-6 epsilon value added to the denominator of adagrad update
   expression for numerical stability.
   -batchsize 10000 size of mini-batch used for training.
   -dropprob 0.5 dropout id203. for each training example we
   randomly choose some amount of units to disable in the neural network
   classifier. this parameter controls the proportion of units "dropped
   out."
   -embeddingsize 50 dimensionality of id27s provided.
   -evalperiter 100 run full uas (unlabeled attachment score) evaluation
   on the development set every time we finish this number of iterations.
   -hiddensize 200 dimensionality of hidden layer in neural network
   classifier.
   -initrange 0.01 bounds of range within which weight matrix elements
   should be initialized. each element is drawn from a uniform
   distribution over the range [-initrange, initrange].
   -maxiter 20000 number of training iterations to complete before
   stopping and saving the final model.
   -numprecomputed 100000 the parser pre-computes hidden-layer unit
   activations for particular inputs words at both training and testing
   time in order to speed up feedforward computation in the neural
   network. this parameter determines how many words for which we should
   compute hidden-layer activations.
   -regparameter 1e-8 id173 parameter for training.
   -trainingthreads 1 number of threads to use during training. note that
   depending on training batch size, it may be unwise to simply choose the
   maximum amount of threads for your machine. on our 16-core test
   machines: a batch size of 10,000 runs fastest with around 6 threads; a
   batch size of 100,000 runs best with around 10 threads.
   -wordcutoff 1 the parser can optionally ignore rare words by simply
   choosing an arbitrary "unknown" feature representation for words that
   appear with frequency less than n in the corpus. this n is controlled
   by the wordcutoff parameter.

runtime parsing options

   option default description
   -escaper n/a only applicable for testing with -textfile. if provided,
   use this word-escaper when parsing raw sentences. should be a
   fully-qualified class name like
   edu.stanford.nlp.trees.international.arabic.atbescaper.
   -numprecomputed 100000 the parser pre-computes hidden-layer unit
   activations for particular inputs words at both training and testing
   time in order to speed up feedforward computation in the neural
   network. this parameter determines how many words for which we should
   compute hidden-layer activations.
   -sentencedelimiter n/a only applicable for testing with -textfile. if
   provided, assume that the given textfile has already been
   sentence-split, and that sentences are separated by this delimiter.
   -tagger.model
   edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3word
   s-distsim.tagger only applicable for testing with -textfile. path to a
   part-of-speech tagger to use to pre-tag the raw sentences before
   parsing.

performance

   the table below describes this parser's performance on the penn
   treebank, converted to dependencies using [32]stanford dependencies.
   the part-of-speech tags used as input for training and testing were
   generated by the [33]stanford pos tagger (using the bidirectional5words
   model).

                                data       metric
   development
                          (1700 sentences) uas 92.0
                          las              89.7
   test
                          (2416 sentences) uas 91.7
                          las              89.5

stanford nlp group

   gates computer science building
   353 serra mall
   stanford, ca 94305-9020
   [34]directions and parking

affiliated groups

     * [35]stanford ai lab
     * [36]stanford infolab
     * [37]csli

connect

     * [38]stack overflow
     * [39]github
     * [40]twitter

local links

   [41]nlp lunch    [42]nlp reading group
   [43]nlp seminar    [44]calendar
   [45]javanlp ([46]javadocs)    [47]machines
   [48]ai speakers    [49]q&a

references

   visible links
   1. https://nlp.stanford.edu/
   2. https://nlp.stanford.edu/people/
   3. https://nlp.stanford.edu/pubs/
   4. https://nlp.stanford.edu/blog/
   5. https://nlp.stanford.edu/software/
   6. https://nlp.stanford.edu/teaching/
   7. https://nlp.stanford.edu/new_local/
   8. https://nlp.stanford.edu/software/
   9. https://nlp.stanford.edu/software/lex-parser.html
  10. http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf
  11. http://universaldependencies.github.com/docs/
  12. https://nlp.stanford.edu/software/stanford-dependencies.html
  13. http://stp.lingfil.uu.se/~nivre/docs/eacl3.pdf
  14. https://nlp.stanford.edu/software/lex-parser.html
  15. https://nlp.stanford.edu/software/tagger.html
  16. https://nlp.stanford.edu/software/corenlp.html
  17. http://universaldependencies.github.com/docs/
  18. https://nlp.stanford.edu/software/stanford-dependencies.html
  19. http://nlp.cs.lth.se/software/treebank_converter/
  20. http://stp.lingfil.uu.se/~nivre/research/penn2malt.html
  21. https://nlp.stanford.edu/software/corenlp.html
  22. http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.0/edu/stanford/nlp/parser/nndep/dependencyparser.html
  23. http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.0/edu/stanford/nlp/parser/nndep/dependencyparser.html#main-java.lang.string:a-
  24. http://ilk.uvt.nl/conll/#dataformat
  25. http://ilk.uvt.nl/conll/#dataformat
  26. http://ilk.uvt.nl/conll/#dataformat
  27. https://github.com/stanfordnlp/corenlp/blob/master/src/edu/stanford/nlp/parser/nndep/demo/dependencyparserdemo.java
  28. http://nlp.stanford.edu/nlp/javadoc/javanlp-3.5.0/edu/stanford/nlp/parser/nndep/dependencyparser.html
  29. http://ilk.uvt.nl/conll/#dataformat
  30. https://nlp.stanford.edu/software/nndep.shtml#training-basic
  31. https://github.com/stanfordnlp/corenlp/tree/master/src/edu/stanford/nlp/trees/international
  32. https://nlp.stanford.edu/software/stanford-dependencies.html
  33. https://nlp.stanford.edu/software/tagger.html
  34. http://forum.stanford.edu/visitors/directions/gates.php
  35. http://ai.stanford.edu/
  36. http://infolab.stanford.edu/
  37. https://www-csli.stanford.edu/
  38. http://stackoverflow.com/tags/stanford-nlp
  39. https://github.com/stanfordnlp/corenlp
  40. https://twitter.com/stanfordnlp
  41. https://nlp.stanford.edu/local/nlp_lunch.shtml
  42. http://nlp.stanford.edu/read/
  43. http://nlp.stanford.edu/seminar/
  44. https://nlp.stanford.edu/local/calendar.shtml
  45. https://nlp.stanford.edu/javanlp/
  46. https://nlp.stanford.edu/nlp/javadoc/javanlp/
  47. https://nlp.stanford.edu/local/machines.shtml
  48. http://ai.stanford.edu/portfolio-view/distinguished-speaker-series
  49. https://nlp.stanford.edu/local/qa/

   hidden links:
  51. https://nlp.stanford.edu/software/nndep.shtml
