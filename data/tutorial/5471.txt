   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets    keras
   tutorial: recognizing tic-tac-toe winners with neural networks comments
   feed [5]cartoon: what else can ai guess from your face? [6]top stories,
   sep 11-17: python vs r     who is really ahead in data science, machine
   learning?; data science and the imposter syndrome

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]sep    [29]tutorials,
   overviews    keras tutorial: recognizing tic-tac-toe winners with neural
   networks ( [30]17:n36 )

keras tutorial: recognizing tic-tac-toe winners with neural networks

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 87
   tags: [33]games, [34]keras, [35]neural networks, [36]python

   in this tutorial, we will build a neural network with keras to
   determine whether or not tic-tac-toe games have been won by player x
   for given endgame board configurations. introductory neural network
   concerns are covered.
     __________________________________________________________________

   by [37]matthew mayo, kdnuggets.

   "[38]tic-tac-toe endgame" was the very first dataset i used to build a
   neural network some years ago. i didn't really know what i was doing at
   the time, and so things didn't go so well. as i have been spending a
   lot of time with keras recently, i thought i would take another stab at
   this dataset in order to demonstrate building a simple neural network
   with keras.


the data


   the dataset, [39]available here, is a collection of 958 possible
   tac-tac-toe end-of-game board configurations, with 9 variables
   representing the 9 squares of a tic-tac-toe board, and a tenth class
   variable which designates if the described board configuration is a
   winning (positive) or not (negative) ending configuration for player x.
   in short, does a particular collection of xs and os on a board mean a
   win for x? incidentally, [40]there are 255,168 possible ways of playing
   a game of tic-tac-toe.

   here is a more formal description of the data:
1. top-left-square: {x,o,b}
2. top-middle-square: {x,o,b}
3. top-right-square: {x,o,b}
4. middle-left-square: {x,o,b}
5. middle-middle-square: {x,o,b}
6. middle-right-square: {x,o,b}
7. bottom-left-square: {x,o,b}
8. bottom-middle-square: {x,o,b}
9. bottom-right-square: {x,o,b}
10. class: {positive,negative}

   as is visible, each square can be designated as marked with an x, an o,
   or left blank (b) at game's end. the mapping of variables to physical
   squares is shown in figure 1. remember, the outcomes are positive or
   negative based on x winning. figure 2 portrays a example board endgame
   layout, followed by its dataset representation.

                                  figure 1
   figure 1. mapping of variables to physical tic-tac-toe board locations.

                                  figure 2
                   figure 2. example board endgame layout.

b,x,o,x,x,b,o,x,o,positive

the preparation


   in order to use the dataset to construct a neural network, some data
   preparation and transformations will be necessary. these are outlined
   below, as well as further commented upon in the code further down.
     * encode categorical variables as numeric - each square is
       represented as a single variable in the original dataset. this is
       unacceptable for our purposes, however, as neural networks require
       variables with continuous values. we must first convert {x, o, b}
       to {0, 1, 2} for each variable. the [41]labelencoder class of
       scikit-learn [42]preprocessing module will be used to accomplish
       this.
     * one-hot encode all independent categorical variables - once
       categorical variables are encoded to numeric, they required further
       processing. as the variables are unordered (x is not greater than
       o, for example), these variables must be represented as a series of
       bit equivalents. this is accomplished using one-hot encoding. as
       there are 3 possibilities for each square, 2 bits will be required
       for encoding. this will be accomplished with the [43]onehotencoder
       class from scikit-learn.
     * remove every third column to avoid dummy variable trap - as the
       one-hot encoding process in scikit-learn creates as many columns
       for each variables as there are possible options (as per the
       dataset), one column needs to be removed in order to avoid what is
       referred to as the dummy variable trap. this is so to avoid
       redundant data which could bias results. in our case, each square
       has 3 possible options (27 columns), which can be expressed with 2
       'bit' columns (i leave this to you to confirm), and so we remove
       every third column from the newly formed dataset (leaving us with
       18).
     * encode target categorical variable - similarly to the independent
       variables, the target categorical variable must be changed from
       {positive, negative} to {0, 1}.
     * train/test split - this should be understandable to all. we set our
       test set to 20% of the dataset. we will use scikit-learn's
       [44]train/test split functionality to achieve this.


the network


   next, we need to construct the neural network, which we will do using
   [45]keras. let's keep in mind that our processed dataset has 18
   variables to use as input, and that we are making binary class
   predictions as output.
     * input neurons - we have 18 independent variables; therefore, we
       need 18 input neurons.
     * hidden layers - a simple (perhaps overly so) -- and possibly
       totally nonsensical -- starting point to determining the number of
       neurons per hidden layer is the rule of thumb: add the number of
       independent variables and the number of output variables and divide
       by 2. if we decide to round down, this gives us 9. what about the
       number of hidden layers? best idea is to start with a low number
       (of hidden layers) and add them until network accuracy does not
       improve. we will create our network with 2 hidden layers.
     * id180 - hidden and output layer neurons require
       id180. [46]general rules dictate: hidden layer
       neurons get the relu function by default, while binary
       classification output layer neurons get the sigmoid function. we
       will follow convention.
     * optimizer - we will use the [47]adam optimizer in our network
     * id168 - we will use the [48]cross-id178 id168 in
       our network.
     * weight initialization - we will randomly set the initial random
       weights of our network layer neurons.

   below is what our network will ultimately look like.


                                  figure 3
                 figure 3. visualization of network layers.

the code


   here is the code which does everything outlined above.

the results


   the maximum accuracy of the trained network reached 98.69%. on the
   unseen test set, the neural network correctly predicted the class of
   180 of the 186 instances.

   by changing the random state of the train/test split (which changes
   which particular 20% of dataset instances are used to test the
   resultant network with), we were able to increase this to 184 out of
   186 instances. this is not necessarily a true metric improvement,
   however. an independent verification set could possibly shed light on
   this... if we had one.

   by using different optimizers (as opposed to adam), a well as changing
   the number of hidden layers and the number of neurons per hidden layer,
   the resulting trained networks did not result in better loss, accuracy,
   or correctly classified test instances beyond what is reported above.

   i hope this has been a useful introduction to constructing neural
   networks with keras. the next such tutorial will attack convolutional
   neural network construction.


   related:
     * [49]deep learning with r + keras
     * [50]7 steps to understanding deep learning
     * [51]an intuitive guide to deep network architectures
     __________________________________________________________________

   [52][prv.gif] previous post
   [53]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [54]another 10 free must-read books for machine learning and data
       science
    2. [55]9 must-have skills you need to become a data scientist, updated
    3. [56]who is a typical data scientist in 2019?
    4. [57]the pareto principle for data scientists
    5. [58]what no one will tell you about data science job applications
    6. [59]19 inspiring women in ai, big data, data science, machine
       learning
    7. [60]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [61]id158s optimization using genetic algorithm
       with python
    2. [62]who is a typical data scientist in 2019?
    3. [63]8 reasons why you should get a microsoft azure certification
    4. [64]the pareto principle for data scientists
    5. [65]r vs python for data visualization
    6. [66]how to work in data science, ai, big data
    7. [67]the deep learning toolset     an overview

[68]latest news

     * [69]download your datax guide to ai in marketing
     * [70]kdnuggets offer: save 20% on strata in london
     * [71]training a champion: building deep neural nets for big ...
     * [72]building a recommender system
     * [73]predict age and gender using convolutional neural netwo...
     * [74]top tweets, mar 27     apr 02: here is a great ex...

   [75]kdnuggets home    [76]news    [77]2017    [78]sep    [79]tutorials,
   overviews    keras tutorial: recognizing tic-tac-toe winners with neural
   networks ( [80]17:n36 )
      2019 kdnuggets. [81]about kdnuggets.  [82]privacy policy. [83]terms
   of service

   [84]subscribe to kdnuggets news
   [85][tw_c48.png] [86]facebook [87]linkedin
   x

   [envelope.png] [88]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/09/neural-networks-tic-tac-toe-keras.html/feed
   5. https://www.kdnuggets.com/2017/09/cartoon-ai-guess-from-face.html
   6. https://www.kdnuggets.com/2017/09/top-news-week-0911-0917.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/09/index.html
  29. https://www.kdnuggets.com/2017/09/tutorials.html
  30. https://www.kdnuggets.com/2017/n36.html
  31. https://www.kdnuggets.com/2017/09/cartoon-ai-guess-from-face.html
  32. https://www.kdnuggets.com/2017/09/top-news-week-0911-0917.html
  33. https://www.kdnuggets.com/tag/games
  34. https://www.kdnuggets.com/tag/keras
  35. https://www.kdnuggets.com/tag/neural-networks
  36. https://www.kdnuggets.com/tag/python
  37. https://www.kdnuggets.com/author/matt-mayo
  38. https://archive.ics.uci.edu/ml/datasets/tic-tac-toe+endgame
  39. https://archive.ics.uci.edu/ml/datasets/tic-tac-toe+endgame
  40. https://www.jesperjuul.net/ludologist/2003/12/28/255168-ways-of-playing-tic-tac-toe/
  41. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.labelencoder.html
  42. http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing
  43. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.onehotencoder.html
  44. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
  45. https://keras.io/
  46. https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html
  47. https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218
  48. http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-id178_cost_function
  49. https://www.kdnuggets.com/2017/06/deep-learning-r-keras.html
  50. https://www.kdnuggets.com/2016/01/seven-steps-deep-learning.html
  51. https://www.kdnuggets.com/2017/08/intuitive-guide-deep-network-architectures.html
  52. https://www.kdnuggets.com/2017/09/cartoon-ai-guess-from-face.html
  53. https://www.kdnuggets.com/2017/09/top-news-week-0911-0917.html
  54. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  55. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  56. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  57. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  58. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  59. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  60. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  61. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  62. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  63. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  64. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  65. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  66. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  67. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  68. https://www.kdnuggets.com/news/index.html
  69. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  70. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  71. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  72. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  73. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  74. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  75. https://www.kdnuggets.com/
  76. https://www.kdnuggets.com/news/index.html
  77. https://www.kdnuggets.com/2017/index.html
  78. https://www.kdnuggets.com/2017/09/index.html
  79. https://www.kdnuggets.com/2017/09/tutorials.html
  80. https://www.kdnuggets.com/2017/n36.html
  81. https://www.kdnuggets.com/about/index.html
  82. https://www.kdnuggets.com/news/privacy-policy.html
  83. https://www.kdnuggets.com/terms-of-service.html
  84. https://www.kdnuggets.com/news/subscribe.html
  85. https://twitter.com/kdnuggets
  86. https://facebook.com/kdnuggets
  87. https://www.linkedin.com/groups/54257
  88. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  90. https://www.kdnuggets.com/
  91. https://www.kdnuggets.com/
