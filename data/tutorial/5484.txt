   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]yellowblog
     * [9]about us
     * [10]contact
     * [11]website
     __________________________________________________________________

using categorical data in machine learning with python

   [12]go to the profile of yonatan hadar
   [13]yonatan hadar (button) blockedunblock (button) followfollowing
   sep 19, 2017

     from dummy variables to deep category embedding and cat2vec         part 1
     (basic methods)

   [1*i4zvi0p_juaq2fmyvmqg1q.jpeg]

     intro

   categorical data is very common in business datasets. for example,
   users are typically described by country, gender, age group etc.,
   products are often described by product type, manufacturer, seller
   etc., and so on.

   categorical data is very convenient for people but very hard for most
   machine learning algorithms, due to several reasons:
    1. high cardinality- categorical variables may have a very large
       number of levels (e.g., city or url), were most of the levels
       appear in a relatively small number of instances.
    2. many machine learning models (e.g., id166) are algebraic, thus their
       input must be numerical. using these models, categories must be
       transformed into numbers first before we can apply the learning
       algorithm.
    3. while some ml packages might transform categorical data to numeric
       automatically based on some default embedding method, many other ml
       packages don   t support such inputs (like our beloved scikit-learn).
    4. for the machine (and we are in machine learning :)), categorical
       data doesn   t contain the same context or information that we humans
       can easily associate and understand. for example, when looking on a
       feature called    city   , we humans can understand that for many
       business aspects new york is a similar concept to new jersey, while
       new york and tehran are much different. but for the machine, new
       york, new jersey and tehran, are just three different levels
       (possible values) of the same feature    city   . if we won   t represent
       the additional contextual information it will be impossible for the
       machine to generalize out of this information.

   in this post i will share some basic strategies of using categorical
   data that worked for us (at [14]yellowroad) in recent projects, while
   on [15]part 2, i will share some more advanced methods. i will discuss
   on why they work and why different methods are better for different
   algorithms in different scenarios, while sharing my code implementation
   in python.

   we will cover different methods, from a simple dummy variables to
   complex methods like leveraging deep learning for category embedding.

     evaluation

   we will evaluate every method on a sample of 2m rows from the avatzo
   ctr prediction kaggle challenge [16]dataset that has many categorical
   features. our evaluation metric will be [17]logarithmic loss (like in
   the contest). on every feature representation i will apply logistic
   regression and id79 algorithms to evaluate it   s performance. i
   will review 6 different embedding strategies in a series of two posts
   (the more advanced methods will be in a following post).

   as with any ml solution, we will start by creating a simple baseline
   method, and assess it   s performance. for that, our baseline prediction
   will be to predict a constant ctr based on the mean ctr proportion that
   was seen in the training set:
import numpy as np
import pandas
from sklearn.metrics import log_loss
train_file = "train.csv"
train = pandas.read_csv(train_file)
msk = np.random.rand(len(train)) < 0.8
features = [3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23]
x_train = train[msk].iloc[:,features]
x_test = train[~msk].iloc[:,features]
y_train = train[msk].iloc[:,1]
y_test = train[~msk].iloc[:,1]
print(log_loss(y_test,np.ones(len(y_test))*y_train.mean()))
     __________________________________________________________________

   we get a log-loss of 0.457.

     embedding methods

method 1: encoding to ordinal variables

   our first method is changing every categorical feature to an ordinal
   one. the order will be selected randomly (for example, like the order
   in the dataset or in an alphabetical order). this method does not make
   much sense because if we are encoding new york as 1, tehran as 2 and
   new jersey as 3, our algorithm will assume that tehran is more similar
   to new york than new jersey. let   s try this method, anyway:
from sklearn import preprocessing
from sklearn.ensemble import randomforestclassifier
from sklearn.linear_model import logisticregression
x_train_ordinal = x_train.values
x_test_ordinal = x_test.values
les = []
l = logisticregression()
r = randomforestclassifier(n_estimators=25,max_depth=10)
for i in range(x_train_ordinal.shape[1]):
    le = preprocessing.labelencoder()
    le.fit(data.iloc[:,features].iloc[:, i])
    les.append(le)
    x_train_ordinal[:, i] = le.transform(x_train_ordinal[:, i])
    x_test_ordinal[:, i] = le.transform(x_test_ordinal[:, i])
l.fit(x_train_ordinal,y_train)
y_pred = l.predict_proba(x_test_ordinal)
print(log_loss(y_test,y_pred))
r.fit(x_train_ordinal,y_train)
y_pred = r.predict_proba(x_test_ordinal)
print(log_loss(y_test,y_pred))

   our id28 got a logloss almost similar to the baseline
   (0.450) but the id79 made a nice improvement to 0.422. how is
   it possible? where in id28 we apply a coefficient to
   each explaining feature (most of which contain merely noise, due to the
   nonsense encoding), random noise includes an inherent feature selection
   mechanism. most probably, id79 selected these features in
   which the encoding somehow correlated with the ctr, and used mainly
   these features to explain the target.

method 2: one hot encoding (or dummy variabales)

   our second method is encoding each category as a one hot encoding (ohe)
   vector (or dummy variables). ohe is a representation method that takes
   each category value and turns it into a binary vector of size
   |i|(number of values in category i) where all columns are equal to zero
   besides the category column. here is a little example:
   [1*wxpois7hxrc-uwjpysy1dg.png]

   ohe is probably the most common method for dealing with categorical
   data and is also very common in nlp (bag of word models). let us run
   it.
from sklearn.preprocessing import onehotencoder
enc = onehotencoder(handle_unknown='ignore')
enc.fit(x_train_ordinal)
x_train_one_hot = enc.transform(x_train_ordinal)
x_test_one_hot = enc.transform(x_test_ordinal)
l.fit(x_train_one_hot,y_train)
y_pred = l.predict_proba(x_test_one_hot)
print(log_loss(y_test,y_pred))
r.fit(x_train_one_hot,y_train)
y_pred = r.predict_proba(x_test_one_hot)
print(log_loss(y_test,y_pred))
print(x_train_one_hot.shape)

   we got a 0.398 logloss with id28 and 0.445 with random
   forest. this is a nice result but why is id28 better
   than id79? in this method we represented our data with a huge
   amount of features ([18]curse of dimensionality). having many features
   we need a very simple classifier in order to not overfit the data.
   id28 is way more simple than id79. moreover,
   the sparsity of the data makes it very hard for the id79 to
   find good splits that will help in separating the classes.

   ohe has some significant shortcomings:
    1. ohe representation produces very high dimensionality, this causes
       an increase in the model   s training and serving time and memory
       consumption.
    2. ohe can easily cause a model to overfit the data.
    3. ohe can   t handle categories that weren   t in the training data (like
       new urls, new device types etc), this can be problematic in domains
       that change all the time.

   we can overcome some of this disadvantages by encoding all rare
   categories to the same features (   rare value   ). this method can reduce
   the dimensionality drastically in some datasets with a small decrease
   in performance (or even an increase). let us try it.
import copy
x_train_rare = copy.copy(x_train)
x_test_rare = copy.copy(x_test)
x_train_rare["test"]=0
x_test_rare["test"]=1
temp_df = pandas.concat([x_train_rare,x_test_rare],axis=0)
names = list(x_train_rare.columns.values)
temp_df = pandas.concat([x_train_rare,x_test_rare],axis=0)
for i in names:
    temp_df.loc[temp_df[i].value_counts()[temp_df[i]].values < 20, i] = "rare_va
lue"
for i in range(temp_df.shape[1]):
    temp_df.iloc[:,i]=temp_df.iloc[:,i].astype('str')
x_train_rare = temp_df[temp_df["test"]=="0"].iloc[:,:-1].values
x_test_rare = temp_df[temp_df["test"]=="1"].iloc[:,:-1].values
for i in range(x_train_rare.shape[1]):
    le = preprocessing.labelencoder()
    le.fit(temp_df.iloc[:,:-1].iloc[:, i])
    les.append(le)
    x_train_rare[:, i] = le.transform(x_train_rare[:, i])
    x_test_rare[:, i] = le.transform(x_test_rare[:, i])
enc.fit(x_train_rare)
x_train_rare = enc.transform(x_train_rare)
x_test_rare = enc.transform(x_test_rare)
l.fit(x_train_rare,y_train)
y_pred = l.predict_proba(x_test_rare)
print(log_loss(y_test,y_pred))
r.fit(x_train_rare,y_train)
y_pred = r.predict_proba(x_test_rare)
print(log_loss(y_test,y_pred))
print(x_train_rare.shape)

   we got almost the same log lost (0.399 for id28 and
   0.433 for id79) but our number of features has decreased
   dramatically from ~270,000 with ohe to ~8,000!

method 3: feature hashing (a.k.a the hashing trick)

   feature hashing is a very cool technique to represent categories in a
      one hot encoding style    as a sparse matrix but with a much lower
   dimensions. in feature hashing we apply a hashing function to the
   category and then represent it by its indices. for example, if we
   choose a dimension of 5 to represent    new york    we will calculate h(new
   york) mod 5 = 3 (for example) so new york representation will be
   (0,0,1,0,0). this technique became very common and was used in the
   winning solution for the avatzo ctr prediction kaggle challenge which
   we are now trying to solve.

   feature hashing has some major pros: it is low dimensional thus it is
   very efficient in processing time and memory, it can be computed with
   online learning because as opposed to one hot encoding we don   t need to
   go over all the data and build a dictionary of all possible categories
   and their mapping and it is not affected by new kinds of categories.

   but it also have some cons. as we know, hashing functions sometimes
   have collision so if h(new york) = h(tehran) the model can   t know what
   city were in the data. there are some sophisticated hashing function
   that try to reduce the number of collision but anyway, studies have
   shown that collisions usually doesn   t affect significantly on the
   models performance. second shortcoming is that hashed features are not
   interpretable so doing things like feature importance and model
   debugging is very hard.

   lets implement the hashing trick:
from sklearn.feature_extraction import featurehasher
x_train_hash = copy.copy(x_train)
x_test_hash = copy.copy(x_test)
for i in range(x_train_hash.shape[1]):
    x_train_hash.iloc[:,i]=x_train_hash.iloc[:,i].astype('str')
for i in range(x_test_hash.shape[1]):
    x_test_hash.iloc[:,i]=x_test_hash.iloc[:,i].astype('str')
h = featurehasher(n_features=100,input_type="string")
x_train_hash = h.transform(x_train_hash.values)
x_test_hash = h.transform(x_test_hash.values)
l.fit(x_train_hash,y_train)
y_pred = l.predict_proba(x_test_hash)
print(log_loss(y_test,y_pred))#0.4
r.fit(x_train_hash,y_train)
y_pred = r.predict_proba(x_test_hash)
print(log_loss(y_test,y_pred))

   we got a result of 0.411 for lr and 0.426 for rf with a much smaller
   dimension than one hot encoding of 1000 and 0.402 for lr and 0.435 for
   rf with a dimension of 10,000. the reason for the difference between
   the two classifiers is probably that rf usually doesn   t work well with
   sparse data.

summary

   so far we tried 3 basic methods for using categorical data in our
   machine learning models. on our dataset the one hot encoding with
   id28 gave the best performance but due to it   s high
   dimensionality, one hot encoding with rare values is probably the best
   option.
   [1*ehzeqv0pqfxwmcydjdnqzg.png]

   [19]on our next post we will cover more advanced methods like using
   cat2vec and category embedding with deep neural networks.

   hope you enjoyed my post and you are more than welcomed to read and
   follow our blog at [20]yellowblog.

     * [21]machine learning
     * [22]artificial intelligence
     * [23]deep learning
     * [24]python
     * [25]data science

   (button)
   (button)
   (button) 876 claps
   (button) (button) (button) 11 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of yonatan hadar

[27]yonatan hadar

     (button) follow
   [28]yellowblog

[29]yellowblog

   blog written by yellowroad         when machine learning and big data meet
   business

     * (button)
       (button) 876
     * (button)
     *
     *

   [30]yellowblog
   never miss a story from yellowblog, when you sign up for medium.
   [31]learn more
   never miss a story from yellowblog
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.myyellowroad.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/66041f734512
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512&source=--------------------------nav_reg&operation=register
   8. https://blog.myyellowroad.com/?source=logo-lo_dgbktw1tbwan---4981ab970712
   9. https://blog.myyellowroad.com/about
  10. https://blog.myyellowroad.com/whats-your-story-ddd43c7095fb
  11. http://www.myyellowroad.com/
  12. https://blog.myyellowroad.com/@yonatan.hadar?source=post_header_lockup
  13. https://blog.myyellowroad.com/@yonatan.hadar
  14. https://medium.com/@yellowroad
  15. https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-42fd0a43b009
  16. https://www.kaggle.com/c/avazu-ctr-prediction
  17. https://www.kaggle.com/wiki/logloss
  18. https://en.wikipedia.org/wiki/curse_of_dimensionality
  19. https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-42fd0a43b009
  20. https://blog.myyellowroad.com/
  21. https://blog.myyellowroad.com/tagged/machine-learning?source=post
  22. https://blog.myyellowroad.com/tagged/artificial-intelligence?source=post
  23. https://blog.myyellowroad.com/tagged/deep-learning?source=post
  24. https://blog.myyellowroad.com/tagged/python?source=post
  25. https://blog.myyellowroad.com/tagged/data-science?source=post
  26. https://blog.myyellowroad.com/@yonatan.hadar?source=footer_card
  27. https://blog.myyellowroad.com/@yonatan.hadar
  28. https://blog.myyellowroad.com/?source=footer_card
  29. https://blog.myyellowroad.com/?source=footer_card
  30. https://blog.myyellowroad.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/66041f734512/share/twitter
  34. https://medium.com/p/66041f734512/share/facebook
  35. https://medium.com/p/66041f734512/share/twitter
  36. https://medium.com/p/66041f734512/share/facebook
