   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

             introduction to id23 and openai gym

   a demonstration of basic id23 problems.

   by [27]justin francis

   july 13, 2017

   image from page 163 of "mazes and labyrinths; a general account of
   their history and developments" (1922). image from page 163 of "mazes
   and labyrinths; a general account of their history and developments"
   (1922). (source: [28]internet archive on flickr)

   [29]check out the session, "building id23
   applications with ray," at the artificial intelligence conference in
   new york, april 15-18, 2019.

   you can find the code used in this post on [30]justin francis' github.

   those interested in the world of machine learning are aware of the
   capabilities of reinforcement-learning-based ai. the past few years
   have seen many breakthroughs using id23 (rl). the
   company [31]deepmind combined deep learning with id23
   to achieve above-human results on a multitude of atari games and, in
   march 2016, defeated go champion le sedol four games to one. though rl
   is currently excelling in many game environments, it is a novel way to
   solve problems that require optimal decisions and efficiency, and will
   surely play a part in machine intelligence to come.

   [32]openai was founded in late 2015 as a non-profit with a [33]mission
   to    build safe artificial general intelligence (agi) and ensure agi's
   benefits are as widely and evenly distributed as possible.    in addition
   to exploring many issues regarding agi, one major contribution that
   openai made to the machine learning world was developing both the
   [34]gym and [35]universe software platforms.

   gym is a collection of environments/problems designed for testing and
   developing id23 algorithms   it saves the user from
   having to create complicated environments. gym is written in python,
   and there are multiple environments such as robot simulations or atari
   games. there is also an online leaderboard for people to compare
   results and code.

   id23, explained simply, is a computational approach
   where an agent interacts with an environment by taking actions in which
   it tries to maximize an accumulated reward. here is a simple graph,
   which i will be referring to often:
   openai gym tutorial id23 figure 1. [36]reinforcement
   learning: an introduction 2nd edition, richard s. sutton and andrew g.
   barto, used with permission.

   an agent in a current state (s[t]) takes an action (a[t]) to which the
   environment reacts and responds, returning a new state(s[t+1]) and
   reward (r[t+1]) to the agent. given the updated state and reward, the
   agent chooses the next action, and the loop repeats until an
   environment is solved or terminated.

   openai   s gym is based upon these fundamentals, so let   s install gym and
   see how it relates to this loop. we   ll get started by installing gym
   using python and the ubuntu terminal. (you can also use mac following
   the instructions on [37]gym   s github.)
sudo apt-get install -y python3-numpy python3-dev python3-pip cmake zlib1g-dev l
ibjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev
swig
cd ~
git clone https://github.com/openai/gym.git
cd gym
sudo pip3 install -e '.[all]'

   next, we can open python3 in our terminal and import gym.
python3
import gym

   first, we need an environment. for our first example, we will load the
   very basic taxi environment.
env = gym.make("taxi-v2")

   to initialize the environment, we must reset it.
env.reset()

   you will notice that resetting the environment will return an integer.
   this number will be our initial state. all possible states in this
   environment are represented by an integer ranging from 0 to 499. we can
   determine the total number of possible states using the following
   command:
env.observation_space.n

   if you would like to visualize the current state, type the following:
env.render()

   in this environment the yellow square represents the taxi, the (   |   )
   represents a wall, the blue letter represents the pick-up location, and
   the purple letter is the drop-off location. the taxi will turn green
   when it has a passenger aboard. while we see colors and shapes that
   represent the environment, the algorithm does not think like us and
   only understands a flattened state, in this case an integer.

   so, now that we have our environment loaded and we know our current
   state, let's explore the actions available to the agent.
env.action_space.n

   this shows us there are a total of six actions available. gym will not
   always tell you what these actions mean, but in this case, the six
   possible actions are: down (0), up (1), right (2), left (3), pick-up
   (4), and drop-off (5).

   for learning   s sake, let   s override the current state to 114.
env.env.s = 114
env.render()

   and, let   s move up.
env.step(1)
env.render()

   you will notice that env.step(1) will return four variables.
(14, -1, false, {'prob': 1.0})

   in the future we will define these variables as so:
state, reward, done, info = env.step(1)

   these four variables are: the new state (s[t+1] = 14), reward (r[t+1] =
   -1), a boolean stating whether the environment is terminated or done,
   and extra info for debugging. every gym environment will return these
   same four variables after an action is taken, as they are the core
   variables of a id23 problem.

   take a look at the rendered environment. what do you expect the
   environment would return if you were to move left? it would, of course,
   give the exact same return as before. the environment always gives a -1
   reward for each step in order for the agent to try and find the
   quickest solution possible. if you were measuring your total
   accumulated reward, constantly running into a wall would heavily
   penalize your final reward. the environment will also give a -10 reward
   every time you incorrectly pick up or drop off a passenger.

   so, how can we solve the environment?

   one surprising way you could solve this environment is to choose
   randomly among the six possible actions. the environment is considered
   solved when you successfully pick up a passenger and drop them off at
   their desired location. upon doing this, you will receive a reward of
   20 and done will equal true. the odds are small, but it   s still
   possible, and given enough random actions you will eventually luck out.
   a core part of evaluating any agent   s performance is to compare it to a
   completely random agent. in a gym environment, you can choose a random
   action using env.action_space.sample(). you can create a loop that will
   do random actions until the environment is solved. we will put a
   counter in there to see how many steps it takes to solve the
   environment.
state = env.reset()
counter = 0
reward = none
while reward != 20:
    state, reward, done, info = env.step(env.action_space.sample())
    counter += 1

print(counter)

   you may luck out and solve the environment fairly quickly, but on
   average, a completely random policy will solve this environment in
   about 2000+ steps, so in order to maximize our reward, we will have to
   have the algorithm remember its actions and their associated rewards.
   in this case, the algorithm   s memory is going to be a q action value
   table.

   to manage this q table, we will use a numpy array. the size of this
   table will be the number of states (500) by the number of possible
   actions (6).
import numpy as np
q = np.zeros([env.observation_space.n, env.action_space.n])

   over multiple episodes of trying to solve the problem, we will be
   updating our q values, slowly improving our algorithm   s efficiency and
   performance. we will also want to track our total accumulated reward
   for each episode, which we will define as g.
g = 0

   similar to most machine learning problems, we will need a learning rate
   as well. i will use my personal favorite of 0.618, also known as the
   mathematical constant phi.
alpha = 0.618

   next, we can implement a very basic id24 algorithm.
for episode in range(1,1001):
    done = false
    g, reward = 0,0
    state = env.reset()
    while done != true:
            action = np.argmax(q[state]) #1
            state2, reward, done, info = env.step(action) #2
            q[state,action] += alpha * (reward + np.max(q[state2]) - q[state,act
ion]) #3
            g += reward
            state = state2
    if episode % 50 == 0:
        print('episode {} total reward: {}'.format(episode,g))

   this code alone will solve the environment. there is a lot going on in
   this code, so i will try and break it down.

   first (#1): the agent starts by choosing an action with the highest q
   value for the current state using argmax. argmax will return the
   index/action with the highest value for that state. initially, our q
   table will be all zeros. but, after every step, the q values for
   state-action pairs will be updated.

   second (#2): the agent then takes action and we store the future state
   as state2 (s[t+1]). this will allow the agent to compare the previous
   state to the new state.

   third (#3): we update the state-action pair (s[t] , a[t]) for q using
   the reward, and the max q value for state2 (s[t+1]). this update is
   done using the action value formula (based upon the bellman equation)
   and allows state-action pairs to be updated in a recursive fashion
   (based on future values). see figure 2 for the value iteration update.
   id24 formula figure 2. id24 formula. source: by gregz448,
   cc0, [38]on wikimedia commons.

   following this update, we update our total reward g and update state
   (s[t]) to be the previous state2 (s[t+1]) so the loop can begin again
   and the next action can be decided.

   after so many episodes, the algorithm will converge and determine the
   optimal action for every state using the q table, ensuring the highest
   possible reward. we now consider the environment problem solved.

   now that we solved a very simple environment, let   s move on to the more
   complicated atari environment   ms. pacman.
env = gym.make("mspacman-v0")
state = env.reset()

   you will notice that env.reset() returns a large array of numbers. to
   be specific, you can enter state.shape to show that our current state
   is represented by a 210x160x3 tensor. this represents the height,
   length, and the three rgb color channels of the atari game or, simply
   put, the pixels. as before, to visualize the environment you can enter:
env.render()

   also, as before, we can determine our possible actions by:
env.action_space.n

   this will show that we have nine possible actions: integers 0-8. it   s
   important to remember that an agent should have no idea what these
   actions mean; its job is to learn which actions will optimize reward.
   but, for our sake, we can:
env.env.get_action_meanings()

   this will show the nine possible actions the agent can chose from,
   represented as taking no action, and the eight possible positions of
   the joystick.

   using our previous strategy, let   s see how good a random agent can
   perform.
state = env.reset()
reward, info, done = none, none, none
while done != true:
    state, reward, done, info = env.step(env.action_space.sample())
    env.render()

   this completely random policy will get a few hundred points, at best,
   and will never solve the first level.

   continuing on, we cannot use our basic q table algorithm because there
   is a total of 33,600 pixels with three rgb values that can have a range
   from 0 to 255. it   s easy to see that things are getting extremely
   complicated; this is where deep learning comes to the rescue. using
   techniques such as convolutional neural networks or a [39]id25, a
   machine learning library is able to take the complex high-dimensional
   array of pixels, make an abstract representation, and translate that
   representation into a optimal action.

   in summary, you now have the basic knowledge to take gym and start
   experimenting with other people's algorithms or maybe even create your
   own. if you would like a copy of the code used in this openai gym
   tutorial to follow along with or edit, you can find the code on my
   [40]github.

   the field of id23 is rapidly expanding with new and
   better methods for solving environments   at this time, the [41]a3c
   method is one of the most popular. id23 will more
   than likely play an important role in the future of ai and continues to
   produce very interesting results.

   [42]learn about ai with these books, videos, and tutorials. this
   collection of ai resources will get you up to speed on the basics, best
   practices, and latest techniques.
   article image: image from page 163 of "mazes and labyrinths; a general
   account of their history and developments" (1922). (source:
   [43]internet archive on flickr).

   share
    1. [44]tweet
    2.
    3.
     __________________________________________________________________

   [45]justin francis

[46]justin francis

   justin francis is currently an undergraduate student at the university
   of alberta in canada. justin is also on the software team for the
   university's engineering club 'autonomous robotic vehicle project'
   (arvp.org) helping implement and experiment with deep learning and
   id23 algorithms.  in the past, he was the founder and
   educator at a non-profit community cooperative bicycle shop and spent
   two years exploring and experiencing the georgia strait on his
   sailboat.
   [47]more
     __________________________________________________________________

   [48]bots landscape.

   [49]ai

[50]infographic: the bot platform ecosystem

   by [51]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [52]vertical forest, milan.

   [53]ai

[54]evolve ai

   by [55]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [56]welcome sign at o'reilly ai conference 2016

   [57]ai

[58]highlights from the o'reilly ai conference in new york 2016

   by [59]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [60]close up of uber's self driving car in pittsburgh.

   [61]ai

[62]how ai is propelling driverless cars, the future of surface transport

   by [63]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [64]our company
     * [65]teach/speak/write
     * [66]careers
     * [67]customer service
     * [68]contact us

site map

     * [69]ideas
     * [70]learning
     * [71]topics
     * [72]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [73]terms of service     [74]privacy policy     [75]editorial independence

   animal

   iframe: [76]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/justin-francis
  28. https://www.flickr.com/photos/126377022@n07/14782276054
  29. https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/74451
  30. https://github.com/wagonhelm/reinforcement-learning-introduction/blob/master/id23 introduction.ipynb
  31. https://deepmind.com/applied/deepmind-for-google/
  32. https://openai.com/
  33. https://www.openai.com/blog/openai-technical-goals/
  34. https://gym.openai.com/
  35. https://universe.openai.com/
  36. http://incompleteideas.net/sutton/book/the-book-2nd.html
  37. https://github.com/openai/gym
  38. https://commons.wikimedia.org/wiki/file:q-l  ring_formel_1.png
  39. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
  40. https://github.com/wagonhelm/reinforcement-learning-introduction/blob/master/id23 introduction.ipynb
  41. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2
  42. https://www.oreilly.com/ideas/learn-about-ai-with-these-books-videos-and-tutorials
  43. https://www.flickr.com/photos/126377022@n07/14782276054
  44. https://twitter.com/share
  45. https://www.oreilly.com/people/justin-francis
  46. https://www.oreilly.com/people/justin-francis
  47. https://www.oreilly.com/people/justin-francis
  48. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  49. https://www.oreilly.com/topics/ai
  50. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  51. https://www.oreilly.com/people/b1d73-jon-bruner
  52. https://www.oreilly.com/ideas/evolve-ai
  53. https://www.oreilly.com/topics/ai
  54. https://www.oreilly.com/ideas/evolve-ai
  55. https://www.oreilly.com/people/14d38-naveen-rao
  56. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  57. https://www.oreilly.com/topics/ai
  58. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  59. https://www.oreilly.com/people/0d2c1-mac-slocum
  60. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  61. https://www.oreilly.com/topics/ai
  62. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  63. https://www.oreilly.com/people/c7521-shahin-farshchi
  64. http://oreilly.com/about/
  65. http://oreilly.com/work-with-us.html
  66. http://oreilly.com/careers/
  67. http://shop.oreilly.com/category/customer-service.do
  68. http://shop.oreilly.com/category/customer-service.do
  69. https://www.oreilly.com/ideas
  70. https://www.oreilly.com/topics/oreilly-learning
  71. https://www.oreilly.com/topics
  72. https://www.oreilly.com/all
  73. http://oreilly.com/terms/
  74. http://oreilly.com/privacy.html
  75. http://www.oreilly.com/about/editorial_independence.html
  76. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  78. https://www.oreilly.com/
  79. https://www.facebook.com/oreilly/
  80. https://twitter.com/oreillymedia
  81. https://www.youtube.com/user/oreillymedia
  82. https://plus.google.com/+oreillymedia
  83. https://www.linkedin.com/company/oreilly-media
  84. https://www.oreilly.com/
