   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    an annotated path to start with machine
   learning comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

an annotated path to start with machine learning

   [25]09/09/201708/17/2018[26]artificial intelligence, [27]deep learning,
   [28]generic, [29]keras, [30]machine learning, [31]neural networks,
   [32]python, [33]scikit-learn, [34]tensorflow, [35]theano[36]2 comments

   do not worry about your difficulties in mathematics. i can assure you mine
are still greater.   

(a. einstein)


   machine learning is becoming more and more widespread and, day after
   day, new computer scientists and engineers begin their long jump into
   this wonderful world. unfortunately, the number of theories,
   algorithms, applications, papers, books, videos and so forth is so huge
   to disorient whoever hasn   t a clear picture of what he wants/needs to
   learn to improve his/her skills. in this short post, i wanted to share
   my experiences, suggesting a feasible path to learn quickly the
   essential concepts and being ready to go deeper the most complex
   topics. of course, this is only a personal proposal: every student can
   choose to dedicate more attention to some topics which are more
   interesting based on his/her experience.

prerequisites

   machine learning is based on mathematics. it   s not an optional,
   theoretical approach: it   s a fundamental pillar that cannot be
   discarded. if you are a computer engineer, working daily with uml, orm,
   design patterns and many other software engineering tools/techniques,
   close your eyes and, for one second, forget almost everything. this
   doesn   t mean that all those concepts aren   t important. they are! but
   machine learning needs a different approach. one of the reasons why
   python has become more and more popular in this field is its
      prototyping speed   . in machine learning, a language that allows you to
   model an algorithm with a few lines of code (without classes,
   interfaces and all other oo infrastructures) is absolutely a must.

   calculus, id203 theory and id202 are necessary
   mathematical skills for almost any algorithm. if you already have a
   good mathematical background, you can skip this section, otherwise,
   it   s a good idea to refresh some important concepts. considering the
   number of theories, i discourage starting with big manuals: even if
   they can also be used when looking for particular concepts, in the
   beginning, it   s better focusing on a simple subset of topics. there are
   many good online resources (like [37]coursera, [38]khan academy or
   [39]udacity, just to name a few) which adopt a pragmatic approach
   suitable for any background. however, my suggestion is to use a brief
   compendium, where the most important concepts are explained and to go
   on by searching and studying new elements whenever they are needed.
   this isn   t a very systematic approach, but the alternative has a
   dramatic drawback: the huge amount of concepts can discourage and
   disorient all the people without a solid academic background.

   an acceptable starting    recipe    can be:
     * id203 theory
          + discrete and continuous random variables
          + main distributions (bernoulli, categorical, binomial, normal,
            exponential, poisson, beta, gamma)
          + moments
          + bayes statistics
          + correlation and covariance
     * id202
          + vectors and matrices
          + determinant of a matrix
          + eigenvectors and eigenvalues
          + id105 (like svd)
     * calculus
          + real functions
          + derivatives, integrals
          + main numerical methods

   there are many free resources on the web, like:
     * grinstead, snell, [40]introduction to id203, swarthmore and
       dartmouth colleges
     * wise, gallagher, [41]an introduction to id202 (with matlab
       examples), columbia
     * heinbockel, [42]introduction to calculus vol. i, old dominion
       university
     * bendersky e., [43]understanding id119
     * bonaccorso g., [44]a brief (and comprehensive) guide to stochastic
       id119 algorithms
     * bonaccorso g., [45]mastering machine learning algorithms, packt,
       2018

   wikipedia is also a very good resource and many formulas, theories and
   theorems are explained in a clear and comprehensible way.

a machine learning path proposal (for very beginners)

feature engineering

   the very first step to jump into machine learning is understanding how
   to measure and improve the quality of datasets. managing categorical
   and missing features, id172 and id84
   ([46]pca, [47]ica, [48]nmf) are fundamental techniques that can
   dramatically improve the performances of every algorithm. it   s also
   useful for studying how to split the datasets into training and test
   sets and how to adopt the [49]cross-validation instead of classical
   test methods.

   a good tutorial on principal component analysis is:
     * shlens j., [50]a tutorial on principal component analysis, ucsd

   if you   re interested in how it   s possible to exploit the [51]hebbian
   learning for pca, i   ve published this articles:
     * [52]ml algorithms addendum: hebbian learning
     * [53]pca with rubner-tavan networks

numpy: the king of python mathematics!

   when working with python, [54]numpy is much more than a library. it   s
   the foundation of almost any machine learning implementation and it   s
   absolutely necessary to know how it works, focusing the attention on
   the concepts of vectorization and [55]broadcasting. through these
   techniques, it   s possible to speed up the learning process of the
   majority of algorithms, exploiting the power of multithreading and
   [56]simd and mimd architectures.

   the official documentation is complete, however, i suggest also these
   resources:
     * vanderplas j., [57]python data science handbook: essential tools
       for working with data, o   reilly
     * langtangen p. h., [58]a primer on scientific programming with
       python, springer

data visualization

   even if it   s not a purely machine learning topic, it   s important to
   know how to visualize the data sets. [59]matplotlib is probably the
   most diffused solution: it   s easy to use and allows plotting different
   kinds of charts. very interesting alternatives are offered by [60]bokeh
   and [61]seaborne. it   s not necessary to have a complete knowledge of
   all packages, but it   s useful to know the strength/weakness points of
   each of them, so to able to pick up the right package when needed.

   a good resource to learn matplotlib details is:
     * mcgreggor d., [62]mastering matplotlib, packt publishing

id75

   id75 is one of the simplest models and can be studied by
   considering it as an optimization problem that can be solved by
   minimizing the mean squared error. this approach is effective but
   limits the possibilities that can be exploited. i suggest also to study
   it as a bayesian problem, where the parameters are represented using
   prior probabilities (gaussian-distributed, for example) and the
   optimization becomes an id113 (id113). even if it
   can seem more complex, this approach offers a new vision that is shared
   by dozens of other more complex models.

   a very useful introduction to bayesian statistics is available on
   coursera:
     * [63]bayesian statistics: from concept to data analysis
     * [64]bayesian statistics: techniques and models

   i also suggest these books:
     * downey b. a., [65]think bayes, o   reilly
     * davidson-pilon c., [66]bayesian methods for hackers, addison-wesley

   path

linear classification

   id28 is normally the best starting point. it   s also a
   good opportunity to study some [67]id205, to understand
   the power of concepts like id178, cross-id178, and [68]mutual
   information (i   ve written also the post: [69]ml algorithms addendum:
   mutual information in classification tasks). categorical cross-id178
   is the most stable and diffused cost function in deep learning
   classification and a simple id28 can show how it can
   speed up the learning process (compared to the mean squared error).
   another important topic is id173 (in particular, ridge, lasso,
   and elasticnet). too many times, it is considered as an    esoteric    way
   to improve the accuracy of a model, but its real meaning is much more
   precise and should be understood with some concrete examples. i also
   suggest starting considering a id28 as a simple neural
   network, visualizing (for 2d examples) how the weight vector moves
   during the learning process.

   i also suggest to include the [70]hyperparameter grid search methods in
   this section. instead of trying different values without a complete
   awareness, grid search allows evaluating the performances of different
   hyperparameters sets. the engineer can, therefore, focus his/her
   attention only the combinations that yield the highest accuracy.

   if you   re interested in some important concepts regarding the fisher
   information, i   ve written this post:
     * [71]ml algorithms addendum: fisher information

support vector machines

   support vector machines offer a different approach to classification
   (both linear and nonlinear). the algorithm is very simple and can be
   learned by every student with a basic knowledge of geometry. however,
   it   s very useful to understand how kernel-id166s work because their real
   power is shown in tasks where linear methods fail.

   some useful free resources:
     * law, [72]a simple introduction to support vector machines, michigan
       state university
     * [73]kernel methods on wikipedia
     * [74]linearly separable? no? for me it is! a brief introduction to
       kernel methods

id90

   another approach to classification and regression is offered by
   [75]id90. in general, they are not the first choice for very
   complex problems, but they offer a completely different approach, which
   can be easily understood even by non-technical people and can be
   visualized during meetings or official presentations.

   a good tutorial (easy) on id90 is:
     * kak a., [76]id90: how to construct them and how to use
       them for classifying new data, purdue university

classification metrics

   evaluating the performance of a classifier can be more difficult than
   expected. the overall accuracy is a good measure, but it is often
   necessary to evaluate the behavior with false positives and false
   negatives. i suggest dedicating some time to studying: [77]precision,
   recall, f-score and [78]roc curve. they can dramatically change the way
   a model is considered acceptable or not. pay attention to recall, which
   measures the impact of false negatives on the accuracy. having a good
   precision, but a bad recall means that your model is generating many
   false negatives (think about this in a medical environment).
   f(beta)-score is a good trade-off between precision and recall.

   the majority of linear classification algorithms, id166, and
   classification metrics are explained in:
     * bonaccorso g., [79]machine learning algorithms (second edition),
       packt, 2018

a quick glimpse into id108

   after having understood the dynamics of a decision tree, it   s useful to
   study methods where sets (ensembles) of trees are trained together to
   improve the overall accuracy. id79s, gradient tree boosting
   and adaboost are powerful algorithms whose complexity is reasonable
   low. it   s interesting comparing the learning process of a simple tree
   and the ones adopted by boosting and id112 methods. scikit-learn
   provides the most common implementations, but if you want to exploit
   the full power of these approaches, i suggest dedicating some time to
   study [80]xgboost, which is a distributed framework that can work both
   with cpus and gpus, speeding up the training process even with very
   large datasets.

   two valid tutorials on id108 are:
     * baskin i., marcou g., [81]varnek a., tutorial on id108,
       facult   de chimie de strasbourg
     * dietterich t. g., [82]ensemble methods in machine learning, oregon
       state university
     * bonaccorso g., [83]mastering machine learning algorithms, packt,
       2018

id91

   when starting with id91 methods, in my opinion, the best thing to
   do is considering the gaussian mixture algorithm (based on em,
   [84]expectation-maximization). even if id116 is quite simpler (and
   must be studied), gaussian mixtures offers a pure bayesian approach,
   which is useful for many other similar tasks. other algorithms that
   must be studied include [85]hierarchical id91, [86]spectral
   id91, and [87]dbscan. it   s also useful to understand the idea of
   instance-based learning, studying the k-nearest neighbors algorithm
   (that can be adopted for both supervised and unsupervised tasks).

   a good introduction to kmeans is: [88]introduction to id116
   id91

   a useful free resource on spectral id91 is:
     * von luxburg u., [89]a tutorial on spectral id91, max-planck
       institute

id91 metrics

   id91 metrics are a little bit more empirical because their
   semantics can change with the context. however, i suggest studying the
   [90]silhouette plots and some ground truth methods (like the adjusted
   [91]rand score). they can provide you with a complete insight into the
   structure of the id91 process, showing all those situations when
   a hyperparameter tuning is probably necessary.

   a very interesting resource on cluster stability is:
     * von luxburg u., [92]cluster stability: an overview, arxiv:1007.1075
     * [93]assessing id91 optimality with instability index

   the majority of id91 algorithms and metrics are exposed in:
     * bonaccorso g., [94]machine learning algorithms (second edition),
       packt, 2018

neural networks for beginners

   neural networks are the basis of deep learning and they should be
   studied in a separate course. however, i think it   s useful to
   understand the concepts of [95]id88, multi-layer id88, and
   the [96]id26 algorithm. scikit-learn offers a very simple
   implementation of [97]neural networks, however, it might be a good idea
   starting the exploration of [98]keras, which is a high-level framework
   based on [99]tensorflow, [100]theano or [101]cntk, that allows modeling
   and training neural networks with a minimum initial effort.

   some good resource to start with neural networks are:
     * hassoun m, [102]fundamentals of id158s, the mit
       press
     * gulli a., pal s., [103]deep learning with keras, packt publishing
     * bonaccorso g., [104]mastering machine learning algorithms, packt,
       2018

   the best deep learning book (advanced) available on the market is
   probably:
     * goodfellow i., bengio y., courville a., [105]deep learning, the mit
       press

   moreover, i suggest to    play    with the [106]tensorflow playground,
   where it   s possible to simulate a complex neural network, to tune its
   parameters and observe the resulting output.

   path image copyright by [107]theritters.

   see also:

[108]eight reasons to study machine learning     giuseppe bonaccorso

     these considerations may be accepted or not, however i hope to show
     some good starting points before beginning to study machine
     learning. [list_items][list_item]old fashioned computing techniques
     are getting more and more useless. industrial revolution is over and
     so all kind of merely-automatic machines.

share:

     * [109]click to share on twitter (opens in new window)
     * [110]click to share on facebook (opens in new window)
     * [111]click to share on linkedin (opens in new window)
     * [112]click to share on pocket (opens in new window)
     * [113]click to share on tumblr (opens in new window)
     * [114]click to share on reddit (opens in new window)
     * [115]click to share on pinterest (opens in new window)
     * [116]click to share on skype (opens in new window)
     * [117]click to share on whatsapp (opens in new window)
     * [118]click to share on telegram (opens in new window)
     * [119]click to email this to a friend (opens in new window)
     * [120]click to print (opens in new window)
     *

you can also be interested in these articles:

   [121]algorithms[122]keras[123]machine
   learning[124]python[125]scikit-learn

post navigation

   [126]ml algorithms addendum: fisher information
   [127]artificial intelligence is a matter of language

2 thoughts on    an annotated path to start with machine learning   

    1. mario
       04/10/2018 at 0:29
       hello giuseppe     and thanks for the invaluable resource.
       i notice some text recommendations are striken off     is that
       because you don   t recommend those books anymore?
       regards
       mario
       [128]reply
          + [129]giuseppe bonaccorso
            04/10/2018 at 13:13
            thank for your comment! i   ve just updated a few broken links
            (nothing changed since the original version, but i   m going to
            add new resources in a next post).
            [130]reply

leave a reply [131]cancel reply

   iframe: [132]jetpack_remote_comment

follow me

     * [133]linkedin
     * [134]twitter
     * [135]facebook
     * [136]github
     * [137]instagram
     * [138]amazon
     * [139]medium
     * [140]rss

search articles

   ____________________ (button)

latest blog posts

     * [141]machine learning algorithms     second edition 08/28/2018
     * [142]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [143]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [144]a book that every data scientist should read 06/22/2018
     * [145]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [146]my tweets

   copyright    2019 [147]giuseppe bonaccorso. all rights reserved.
   [148]privacy policy - [149]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [150]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/machine-learning/deep-learning/
  28. https://www.bonaccorso.eu/category/generic/
  29. https://www.bonaccorso.eu/category/machine-learning/deep-learning/keras/
  30. https://www.bonaccorso.eu/category/machine-learning/
  31. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  32. https://www.bonaccorso.eu/category/software/python/
  33. https://www.bonaccorso.eu/category/machine-learning/scikit-learn/
  34. https://www.bonaccorso.eu/category/machine-learning/deep-learning/tensorflow-deep-learning/
  35. https://www.bonaccorso.eu/category/machine-learning/deep-learning/theano/
  36. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#comments
  37. https://www.coursera.org/
  38. https://www.khanacademy.org/
  39. https://www.udacity.com/
  40. https://www.dartmouth.edu/~chance/teaching_aids/books_articles/id203_book/amsbook.mac.pdf
  41. http://www.stat.columbia.edu/~liam/teaching/4315-spr06/linalg.pdf
  42. http://www.math.odu.edu/~jhh/volume-1.pdf
  43. http://eli.thegreenplace.net/2016/understanding-gradient-descent/
  44. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
  45. https://www.amazon.com/gp/product/1788621115?pf_rd_p=d1f45e03-8b73-4c9a-9beb-4819111bef9a&pf_rd_r=864bt559egpd6rbxk4cb
  46. https://en.wikipedia.org/wiki/principal_component_analysis
  47. https://en.wikipedia.org/wiki/independent_component_analysis
  48. https://en.wikipedia.org/wiki/non-negative_matrix_factorization
  49. https://en.wikipedia.org/wiki/cross-validation_(statistics)
  50. https://www.cs.princeton.edu/picasso/mats/pca-tutorial-intuition_jp.pdf
  51. https://en.wikipedia.org/wiki/hebbian_theory
  52. https://www.bonaccorso.eu/2017/08/21/ml-algorithms-addendum-hebbian-learning/
  53. https://www.bonaccorso.eu/2017/09/25/pca-rubner-tavan-networks/
  54. http://www.numpy.org/
  55. https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html
  56. https://en.wikipedia.org/wiki/simd
  57. https://amzn.to/2xv2sys
  58. https://amzn.to/2yq4v7b
  59. https://matplotlib.org/
  60. https://bokeh.pydata.org/en/latest/
  61. https://seaborn.pydata.org/
  62. https://amzn.to/2yqab0z
  63. https://www.coursera.org/learn/bayesian-statistics/
  64. https://www.coursera.org/learn/mcmc-bayesian-statistics
  65. https://amzn.to/2fsijjs
  66. https://amzn.to/2ftktbl
  67. https://en.wikipedia.org/wiki/information_theory
  68. https://www.bonaccorso.eu/2017/08/18/ml-algorithms-addendum-mutual-information-in-classification-tasks/
  69. https://www.bonaccorso.eu/2017/08/18/ml-algorithms-addendum-mutual-information-in-classification-tasks/
  70. https://en.wikipedia.org/wiki/hyperparameter_(machine_learning)
  71. https://www.bonaccorso.eu/2017/09/02/ml-algorithms-addendum-fisher-information/
  72. https://www.cise.ufl.edu/class/cis4930sp11dtm/notes/intro_id166_new.pdf
  73. https://en.wikipedia.org/wiki/kernel_method
  74. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/
  75. https://en.wikipedia.org/wiki/decision_tree_learning
  76. https://engineering.purdue.edu/kak/tutorials/decisiontreeclassifiers.pdf
  77. https://en.wikipedia.org/wiki/precision_and_recall
  78. https://en.wikipedia.org/wiki/receiver_operating_characteristic
  79. https://www.amazon.com/machine-learning-algorithms-reference-algorithms/dp/1789347998/ref=sr_1_7?s=books&ie=utf8&qid=1534519212&sr=1-7&keywords=machine+learning+algorithms+second
  80. https://github.com/dmlc/xgboost
  81. http://infochim.u-strasbg.fr/new/cs3_2010/tutorial/ensemble/ensemblemodeling.pdf
  82. http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf
  83. https://www.amazon.com/gp/product/1788621115?pf_rd_p=d1f45e03-8b73-4c9a-9beb-4819111bef9a&pf_rd_r=864bt559egpd6rbxk4cb
  84. https://en.wikipedia.org/wiki/expectation   maximization_algorithm
  85. https://en.wikipedia.org/wiki/hierarchical_id91
  86. https://en.wikipedia.org/wiki/spectral_id91
  87. https://en.wikipedia.org/wiki/dbscan
  88. https://www.datascience.com/blog/introduction-to-id116-id91-algorithm-learn-data-science-tutorials
  89. https://www.cs.cmu.edu/~aarti/class/10701/readings/luxburg06_tr.pdf
  90. https://en.wikipedia.org/wiki/silhouette_(id91)
  91. https://en.wikipedia.org/wiki/rand_index
  92. https://arxiv.org/abs/1007.1075
  93. https://www.bonaccorso.eu/2017/08/03/assessing-id91-optimality-instability-index/
  94. https://www.amazon.com/machine-learning-algorithms-reference-algorithms/dp/1789347998/ref=sr_1_7?s=books&ie=utf8&qid=1534519212&sr=1-7&keywords=machine+learning+algorithms+second
  95. https://en.wikipedia.org/wiki/id88
  96. https://en.wikipedia.org/wiki/id26
  97. http://scikit-learn.org/stable/modules/neural_networks_supervised.html
  98. https://keras.io/
  99. https://www.tensorflow.org/
 100. http://www.deeplearning.net/software/theano/
 101. https://www.microsoft.com/en-us/cognitive-toolkit/
 102. https://amzn.to/2fqiujw
 103. https://amzn.to/2yhww1r
 104. https://www.amazon.com/gp/product/1788621115?pf_rd_p=d1f45e03-8b73-4c9a-9beb-4819111bef9a&pf_rd_r=864bt559egpd6rbxk4cb
 105. https://amzn.to/2fqqqlg
 106. http://playground.tensorflow.org/#activation=tanh&batchsize=10&dataset=circle&regdataset=reg-plane&learningrate=0.03&id173rate=0&noise=0&networkshape=4,2&seed=0.91391&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
 107. https://www.flickr.com/photos/theritters/
 108. https://www.bonaccorso.eu/2016/07/08/ten-reasons-study-machine-learning/
 109. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=twitter
 110. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=facebook
 111. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=linkedin
 112. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=pocket
 113. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=tumblr
 114. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=reddit
 115. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=pinterest
 116. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=skype
 117. https://api.whatsapp.com/send?text=an annotated path to start with machine learning https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/
 118. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=telegram
 119. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/?share=email
 120. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#print
 121. https://www.bonaccorso.eu/tag/algorithms/
 122. https://www.bonaccorso.eu/tag/keras/
 123. https://www.bonaccorso.eu/tag/machine-learning/
 124. https://www.bonaccorso.eu/tag/python/
 125. https://www.bonaccorso.eu/tag/scikit-learn/
 126. https://www.bonaccorso.eu/2017/09/02/ml-algorithms-addendum-fisher-information/
 127. https://www.bonaccorso.eu/2017/09/11/artificial-intelligence-is-a-matter-of-language/
 128. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#comment-83
 129. https://www.bonaccorso.eu/
 130. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#comment-84
 131. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#respond
 132. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1292&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=db44e8641c4eb341096bc065fbbdd98c0386ca88#parent=https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/
 133. https://www.linkedin.com/in/giuseppebonaccorso/
 134. https://twitter.com/giuseppeb/
 135. https://www.facebook.com/giuseppe.bonaccorso/
 136. https://github.com/giuseppebonaccorso/
 137. https://www.instagram.com/giuseppebonaccorso/
 138. https://www.amazon.com/author/giuseppebonaccorso
 139. https://medium.com/@giuseppe.bonaccorso
 140. https://www.bonaccorso.eu/feed/
 141. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
 142. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
 143. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
 144. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
 145. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
 146. https://twitter.com/giuseppeb
 147. https://www.bonaccorso.eu/
 148. https://www.iubenda.com/privacy-policy/331721
 149. https://www.iubenda.com/privacy-policy/331721/cookie-policy
 150. https://www.bonaccorso.eu/2017/09/09/an-annotated-path-to-start-with-machine-learning/#cancel

   hidden links:
 152. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
