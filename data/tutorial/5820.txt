   #[1]alternate

   [2]oracle + datascience.com
   [3]oracle + datascience.com
     * [4]technology
     * [5]solutions
          +
     * [6]resources
          +
     * [7]blog
          +
     * [8]innovation

   [9]join our mailing list
   [10]platform overview

[11]platform

   [12]learn how data science can be more impactful
   [13]features

[14]features

   [15]scale with functionality for it, business, & data teams
   [16]technology

[17]technology

   [18]explore the tools and solutions we support
   [19]documentation

[20]documentation

   [21]start working in our data science platform
   new
   [22]demo our latest release
   make data science and machine learning a competitive advantage, no
   matter what industry you serve.
   [23]obic_business_big-data_detailed_rgb@1x

[24]solutions

   [25]build innovative solutions with a collaborative data science
   platform.
     * [26]automotive
     * [27]communications
     * [28]construction & engineering
     * [29]financial services
     * [30]food and beverage
     * [31]healthcare

     * [32]higher education
     * [33]hospitality
     * [34]industrial manufacturing
     * [35]public sector
     * [36]retail
     * [37]utilities

   hone your skills, learn best practices, and stay informed with our data
   science resources.
   [38]obic_business_content-management_detailed_rgb@1x

[39]content library

   [40]learn about real-world data science applications
     * [41]best practices
     * [42]learn data science
     * [43]use cases
     * [44]data science platform

   [45]obic_technology_video_detailed_rgb@1x

[46]video knowledge center

   [47]watch educational lectures and product demos
     * [48]events & presentations
     * [49]platform demos & use cases
     * [50]webinars
     * [51]data science education

   [52]obic_business_document_detailed_rgb@1x

[53]datascience.com blog

   [54]insights on data science and machine learning

recent posts

     * [55]id21 in pytorch, part 1: how to use dataloaders
       and build a fully connected class
     * [56]4 types of data analytics
     * [57]the hidden insights of patent data
     * [58]4 approaches to overcoming label bias in positive and unlabeled
       learning

   [59]obic_business_contract-approval_detailed_rgb@1x-1
   [60]become a contributor
   our mission is to enable every business to successfully perform data
   science at scale.
   [61]company

[62]company

   [63]see how we're changing enterprise data science
     * [64]contact
     * [65]careers

   [66]newsroom

[67]newsroom

   [68]stay up to date on the latest news
     * [69]press releases
     * [70]in the news

   [71]events

[72]events

   [73]get on the guest list for our next event
     * [74]elevate
     * [75]special events
     * [76]conferences
     * [77]webinars

   [78]partners

[79]partners

   [80]learn about our network of industry partners
     * [81]mapr
     * [82]github
     * [83]h2o.ai
     * [84]become a partner

   [85]company contact
   [86]contact
   [87]request demo
   [88]request demo
   [89]join our mailing list

technology

   [90]platform-overview-icon@2x

[91]overview

   [92]obic_business_big-data_detailed_rgb@1x

[93]solutions

     * [94]resources
     * [95]blog
     * [96]company
     * [97]videos

   [98]contact

   [99]learn data science, [100]machine learning

introduction to id116 id91

   author: andrea trevino posted on december 6, 2016
     *
     *
     *

prerequisites

     * experience with the specific topic: novice
     * professional experience: no industry experience

   knowledge of machine learning is not required, but the reader should be
   familiar with basic data analysis (e.g., descriptive analysis) and the
   programming language python. to follow along, download the sample
   dataset [101]here.

introduction to id116 id91

   id116 id91 is a type of unsupervised learning, which is used
   when you have unlabeled data (i.e., data without defined categories or
   groups). the goal of this algorithm is to find groups in the data, with
   the number of groups represented by the variable k. the algorithm works
   iteratively to assign each data point to one of k groups based on the
   features that are provided. data points are clustered based on feature
   similarity. the results of the id116 id91 algorithm are:
    1. the centroids of the k clusters, which can be used to label new
       data
    2. labels for the training data (each data point is assigned to a
       single cluster)

   rather than defining groups before looking at the data, id91
   allows you to find and analyze the groups that have formed organically.
   the "choosing k" section below describes how the number of groups can
   be determined.

   each centroid of a cluster is a collection of feature values which
   define the resulting groups. examining the centroid feature weights can
   be used to qualitatively interpret what kind of group each cluster
   represents.

   this introduction to the id116 id91 algorithm covers:
     * common business cases where id116 is used
     * the steps involved in running the algorithm
     * a python example using delivery fleet data

business uses

   the id116 id91 algorithm is used to find groups which have not
   been explicitly labeled in the data. this can be used to confirm
   business assumptions about what types of groups exist or to identify
   unknown groups in complex data sets. once the algorithm has been run
   and the groups are defined, any new data can be easily assigned to the
   correct group.

   this is a versatile algorithm that can be used for any type of
   grouping. some examples of use cases are:
     * behavioral segmentation:
          + segment by purchase history
          + segment by activities on application, website, or platform
          + define personas based on interests
          + create profiles based on activity monitoring
     * inventory categorization:
          + group inventory by sales activity
          + group inventory by manufacturing metrics
     * sorting sensor measurements:
          + detect activity types in motion sensors
          + group images
          + separate audio
          + identify groups in health monitoring
     * detecting bots or anomalies:
          + separate valid activity groups from bots
          + group valid activity to clean up outlier detection

   in addition, monitoring if a tracked data point switches between groups
   over time can be used to detect meaningful changes in the data.

algorithm

   the   -means id91 algorithm uses iterative refinement to produce a
   final result. the algorithm inputs are the number of clusters    and the
   data set. the data set is a collection of features for each data point.
   the algorithms starts with initial estimates for the    centroids, which
   can either be randomly generated or randomly selected from the data
   set. the algorithm then iterates between two steps:

   1. data assigment step:

   each centroid defines one of the clusters. in this step, each data
   point is assigned to its nearest centroid, based on the squared
   euclidean distance. more formally, if c[i] is the collection of
   centroids in set c, then each data point x is assigned to a cluster
   based on

   $$\underset{c_i \in c}{\arg\min} \; dist(c_i,x)^2$$

   where dist(    ) is the standard (l[2]) euclidean distance. let the set
   of data point assignments for each i^th cluster centroid be s[i].

   2. centroid update step:

   in this step, the centroids are recomputed. this is done by taking the
   mean of all data points assigned to that centroid's cluster.

   $$c_i=\frac{1}{|s_i|}\sum_{x_i \in s_i x_i}$$

   the algorithm iterates between steps one and two until a stopping
   criteria is met (i.e., no data points change clusters, the sum of the
   distances is minimized, or some maximum number of iterations is
   reached).

   this algorithm is guaranteed to converge to a result. the result may be
   a local optimum (i.e. not necessarily the best possible outcome),
   meaning that assessing more than one run of the algorithm with
   randomized starting centroids may give a better outcome.

choosing k

   the algorithm described above finds the clusters and data set labels
   for a particular pre-chosen k. to find the number of clusters in the
   data, the user needs to run the id116 id91 algorithm for a
   range of k values and compare the results. in general, there is no
   method for determining exact value of k, but an accurate estimate can
   be obtained using the following techniques.

   one of the metrics that is commonly used to compare results across
   different values of k is the mean distance between data points and
   their cluster centroid. since increasing the number of clusters will
   always reduce the distance to data points, increasing k will always
   decrease this metric, to the extreme of reaching zero when k is the
   same as the number of data points. thus, this metric cannot be used as
   the sole target. instead, mean distance to the centroid as a function
   of k is plotted and the "elbow point," where the rate of decrease
   sharply shifts, can be used to roughly determine k.

   a number of other techniques exist for validating k, including
   cross-validation, information criteria, the information theoretic jump
   method, the silhouette method, and the g-means algorithm. in addition,
   monitoring the distribution of data points across groups provides
   insight into how the algorithm is splitting the data for each k.

   introduction-to-id116-id91-elbow-point-example.png

example: applying id116 id91 to delivery fleet data

   as an example, we'll show how the id116 algorithm works with a
   [102]sample dataset of delivery fleet driver data. for the sake of
   simplicity, we'll only be looking at two driver features: mean distance
   driven per day and the mean percentage of time a driver was >5 mph over
   the speed limit. in general, this algorithm can be used for any number
   of features, so long as the number of data samples is much greater than
   the number of features.

step 1: clean and transform your data

   for this example, we've already cleaned and completed some simple data
   transformations. a sample of the data as a pandas dataframe is shown
   below.

   screen shot 2016-11-23 at 11.22.10 am.png

   the chart below shows the dataset for 4,000 drivers, with the distance
   feature on the x-axis and speeding feature on the y-axis.

   screen shot 2016-11-23 at 11.26.33 am.png

step 2: choose k and run the algorithm

   start by choosing k=2. for this example, use the python packages
   [103]scikit-learn and [104]numpy for computations as shown below:
import numpy as np
from sklearn.cluster import kmeans

### for the purposes of this example, we store feature data from our
### dataframe `df`, in the `f1` and `f2` arrays. we combine this into
### a feature matrix `x` before entering it into the algorithm.
f1 = df['distance_feature'].values

f2 = df['speeding_feature'].values

x=np.matrix(zip(f1,f2))
kmeans = kmeans(n_clusters=2).fit(x)

   the cluster labels are returned in kmeans.labels_.

step 3: review the results

   the chart below shows the results. visually, you can see that
   the id116 algorithm splits the two groups based on the distance
   feature. each cluster centroid is marked with a star.
     * group 1 centroid = (50, 5.2)
     * group 2 centroid = (180.3, 10.5)

   using domain knowledge of the dataset, we can infer that group 1 is
   urban drivers and group 2 is rural drivers.

   id116-id91-output-5-image.png

step 4: iterate over several values of k

   test how the results look for k=4. to do this, all you need to change
   is the target number of clusters in the kmeans() function.
kmeans = kmeans(n_clusters=4).fit(x)

   the chart below shows the resulting clusters. we see that four distinct
   groups have been identified by the algorithm; now speeding drivers have
   been separated from those who follow speed limits, in addition to the
   rural vs. urban divide. the threshold for speeding is lower with the
   urban driver group than for the rural drivers, likely due to urban
   drivers spending more time in intersections and stop-and-go traffic.

   pasted image at 2016_12_15 09_55 pm (2).png

additional notes and alternatives

feature engineering

   feature engineering is the process of using domain knowledge to choose
   which data metrics to input as features into a machine learning
   algorithm. feature engineering plays a key role in id116 id91;
   using meaningful features that capture the variability of the data is
   essential for the algorithm to find all of the naturally-occurring
   groups.

   categorical data (i.e., category labels such as gender, country,
   browser type) needs to be encoded or separated in a way that can still
   work with the algorithm.

   feature transformations, particularly to represent rates rather than
   measurements, can help to normalize the data. for example, in the
   delivery fleet example above, if total distance driven had been used
   rather than mean distance per day, then drivers would have been grouped
   by how long they had been driving for the company rather than rural vs.
   urban.

alternatives

   a number of [105]alternative id91 algorithms exist including
   dbscan, spectral id91, and modeling with gaussian mixtures. a
   id84 technique, such as principal component
   analysis, can be used to separate groups of patterns in data. you can
   read more about alternatives to id116 [106]in this post.

   one possible outcome is that there are no organic clusters in the data;
   instead, all of the data fall along the continuous feature ranges
   within one single group. in this case, you may need to revisit the data
   features to see if different measurements need to be included or a
   feature transformation would better represent the variability in the
   data. in addition, you may want to impose categories or labels based on
   domain knowledge and modify your analysis approach.

   for more information on id116 id91, visit the [107]scikit learn
   site.
     __________________________________________________________________

   want to keep learning? download our [108]new study from forrester about
   the tools and practices keeping companies on the forefront of data
   science.
     __________________________________________________________________


   andrea trevino
   author
   andrea trevino

   data scientist. loves unravelling data puzzles and will spend hours
   talking about anything related to audio, languages, or food.

enjoyed this post? don't forget to share.

related content

   learn data science

id21 in pytorch, part 1: how to use dataloaders and build a
fully connected class

   [109]learn more

   learn data science

4 types of data analytics

   [110]learn more

   learn data science

4 approaches to overcoming label bias in positive and unlabeled learning

   [111]learn more

   learn data science

fashion retailing forecasting, part 2: a problem with maximum likelihood

   [112]learn more

[113]subscribe to our newsletter    

   oracle + datascience.com


technology

     * [114]about
     * [115]data science
     * [116]data integration platform
     * [117]data analytics
     * [118]big data platform

solutions

     * [119]overview

resources

     * [120]content library
     * [121]video knowledge center
     * [122]blog
     * [123]become a contributor

tools

     * [124]modeling how-tos
     * [125]open source trends
     * [126]query optimization
     * [127]model testing

company

     * [128]about
     * [129]privacy policy
     * [130]terms of use

    copyright    2018, oracle and/or its affiliates. all rights reserved.
       oracle and java are registered trademarks of oracle and/or its
    affiliates. other names may be trademarks of their respective owners.

references

   visible links
   1. https://www.datascience.com/blog/rss.xml
   2. https://www.datascience.com/
   3. https://www.datascience.com/
   4. https://www.datascience.com/technology
   5. https://www.datascience.com/solutions
   6. https://www.datascience.com/resources
   7. https://www.datascience.com/blog
   8. https://www.datascience.com/innovation
   9. https://www.datascience.com/opt-in?hsctatracking=186fb139-cedb-4d48-abb0-2814c743ae69|63852544-ef41-4564-a521-4aaa126ed061
  10. https://www.datascience.com/platform
  11. https://www.datascience.com/platform
  12. https://www.datascience.com/platform
  13. https://www.datascience.com/platform/features
  14. https://www.datascience.com/platform/features
  15. https://www.datascience.com/platform/features
  16. https://www.datascience.com/platform/technology
  17. https://www.datascience.com/platform/technology
  18. https://www.datascience.com/platform/technology
  19. https://docs.datascience.com/en/master/
  20. https://docs.datascience.com/en/master/
  21. https://docs.datascience.com/en/master/
  22. https://www.datascience.com/request-demo
  23. https://www.datascience.com/solutions
  24. https://www.datascience.com/solutions
  25. https://www.datascience.com/solutions
  26. https://www.datascience.com/solutions/automotive
  27. https://www.datascience.com/solutions/communications
  28. https://www.datascience.com/solutions/construction-and-engineering
  29. https://www.datascience.com/solutions/financial-services
  30. https://www.datascience.com/solutions/food-and-beverage
  31. https://www.datascience.com/solutions/healthcare
  32. https://www.datascience.com/solutions/higher-ed
  33. https://www.datascience.com/solutions/hospitality
  34. https://www.datascience.com/solutions/industrial-manufacturing
  35. https://www.datascience.com/solutions/public-sector
  36. https://www.datascience.com/solutions/retail
  37. https://www.datascience.com/solutions/utilities
  38. https://www.datascience.com/resources
  39. https://www.datascience.com/resources
  40. https://www.datascience.com/resources
  41. https://www.datascience.com/resources#.best-practices
  42. https://www.datascience.com/resources#.learn-data-science
  43. https://www.datascience.com/resources#.use-cases
  44. https://www.datascience.com/resources#.data-science-platform
  45. https://datascience.hubs.vidyard.com/
  46. https://datascience.hubs.vidyard.com/
  47. https://datascience.hubs.vidyard.com/
  48. https://datascience.hubs.vidyard.com/categories/events
  49. https://datascience.hubs.vidyard.com/categories/platform-demos-and-use-cases
  50. https://datascience.hubs.vidyard.com/categories/webinars
  51. https://datascience.hubs.vidyard.com/categories/learn-data-science
  52. https://www.datascience.com/blog
  53. https://www.datascience.com/blog
  54. https://www.datascience.com/blog
  55. https://www.datascience.com/blog/transfer-learning-in-pytorch-part-one
  56. https://www.datascience.com/blog/4-types-of-data-analytics
  57. https://www.datascience.com/blog/the-hidden-insights-of-patent-data
  58. https://www.datascience.com/blog/overcoming-label-bias-in-positive-and-unlabeled-learning
  59. https://www.datascience.com/contribute
  60. https://www.datascience.com/contribute
  61. https://www.datascience.com/company
  62. https://www.datascience.com/company
  63. https://www.datascience.com/company
  64. https://www.datascience.com/company#company-contact
  65. https://www.datascience.com/company#company-careers
  66. https://www.datascience.com/newsroom
  67. https://www.datascience.com/newsroom
  68. https://www.datascience.com/newsroom
  69. https://www.datascience.com/newsroom/topic/press-release
  70. https://www.datascience.com/newsroom/topic/news
  71. https://www.datascience.com/events
  72. https://www.datascience.com/events
  73. https://www.datascience.com/events
  74. https://datascience.hubs.vidyard.com/watch/r7vyjyvomu81yfn5n9telc
  75. https://www.datascience.com/events/all/topic/special-events
  76. https://www.datascience.com/events/all/topic/conferences
  77. https://www.datascience.com/events/all/topic/webinars
  78. https://www.datascience.com/partners
  79. https://www.datascience.com/partners
  80. https://www.datascience.com/partners
  81. https://www.datascience.com/newsroom/mapr-partnership-announcement
  82. https://www.datascience.com/newsroom/datascience-github-partnership
  83. https://www.datascience.com/resources/webinars/h2o-artificial-intelligence
  84. https://www.datascience.com/become-a-partner
  85. https://www.datascience.com/company#company-contact
  86. https://www.datascience.com/company#company-contact
  87. https://www.datascience.com/request-demo
  88. https://www.datascience.com/request-demo
  89. https://www.datascience.com/opt-in?hsctatracking=186fb139-cedb-4d48-abb0-2814c743ae69|63852544-ef41-4564-a521-4aaa126ed061&__hstc=183026745.5298088ab707eca61125e21792b2ad47.1539033217735.1539884691142.1539896691576.24&__hssc=183026745.7.1539896691576&__hsfp=2830425072&hsutk=5298088ab707eca61125e21792b2ad47
  90. https://www.datascience.com/platform
  91. https://www.datascience.com/technology
  92. https://www.datascience.com/platform/features
  93. https://www.datascience.com/solutions
  94. https://www.datascience.com/resources
  95. https://www.datascience.com/blog
  96. https://www.oracle.com/corporate/#info
  97. https://datascience.hubs.vidyard.com/
  98. https://www.oracle.com/corporate/contact/global.html
  99. https://www.datascience.com/blog/topic/learn-data-science
 100. https://www.datascience.com/blog/topic/machine-learning
 101. https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/introduction-to-id116-id91/data/data_1024.csv
 102. https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/introduction-to-id116-id91/data/data_1024.csv
 103. http://scikit-learn.org/stable/
 104. http://www.numpy.org/
 105. https://www.datascience.com/blog/id116-alternatives
 106. https://www.datascience.com/blog/id116-alternatives
 107. http://scikit-learn.org/stable/modules/id91.html#id116
 108. https://www.datascience.com/resources/white-papers/forrester-data-science-platforms-create-business-value
 109. https://www.datascience.com/blog/transfer-learning-in-pytorch-part-one
 110. https://www.datascience.com/blog/4-types-of-data-analytics
 111. https://www.datascience.com/blog/overcoming-label-bias-in-positive-and-unlabeled-learning
 112. https://www.datascience.com/blog/fashion-retail-forecasting-maximum-likelihood-problem
 113. https://www.datascience.com/oracle-opt-in
 114. https://www.datascience.com/technology
 115. https://cloud.oracle.com/en_us/ai-platform
 116. https://cloud.oracle.com/en_us/data-integration-platform
 117. https://cloud.oracle.com/en_us/oac
 118. https://cloud.oracle.com/big-data-cloud
 119. https://www.datascience.com/solutions
 120. https://www.datascience.com/resources
 121. https://datascience.hubs.vidyard.com/
 122. https://www.datascience.com/blog
 123. https://www.datascience.com/contribute
 124. https://www.datascience.com/resources/tools/playbooks
 125. https://www.datascience.com/resources/tools/trends
 126. https://www.datascience.com/resources/tools/grunion
 127. https://www.datascience.com/resources/tools/skater
 128. https://www.oracle.com/corporate/#info
 129. https://www.datascience.com/privacy-policy
 130. https://www.datascience.com/terms

   hidden links:
 132. javascript:;
 133. javascript:;
 134. javascript:;
 135. http://www.facebook.com/share.php?u=https://www.datascience.com/blog/id116-id91/
 136. https://twitter.com/share?url=https://www.datascience.com/blog/id116-id91&via=datascience
 137. http://www.linkedin.com/sharearticle?mini=true&url=https://www.datascience.com/blog/id116-id91/&summary=&source=datascience
 138. http://www.facebook.com/share.php?u=https://www.datascience.com/blog/id116-id91/
 139. https://twitter.com/share?url=https://www.datascience.com/blog/id116-id91&text=introduction%20to%20id116%20id91&via=datascience
 140. http://www.linkedin.com/sharearticle?mini=true&url=https://www.datascience.com/blog/id116-id91/&summary=&source=datascience
 141. https://www.linkedin.com/company/6393389?trk=tyah&trkinfo=clickedvertical%3acompany%2cclickedentityid%3a6393389%2cidx%3a3-4-9%2ctarid%3a1472601951147%2ctas%3adatascience
 142. https://twitter.com/datascienceinc
 143. https://www.facebook.com/datascience/?fref=ts
 144. https://www.instagram.com/datascienceinc/
