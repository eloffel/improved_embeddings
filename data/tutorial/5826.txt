   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   proximal policy optimization

proximal policy optimization

   july 20, 2017
   3 minute read

   we   re releasing a new class of id23 algorithms,
   [11]proximal policy optimization (ppo), which perform comparably or
   better than state-of-the-art approaches while being much simpler to
   implement and tune. ppo has become the default id23
   algorithm at openai because of its ease of use and good performance.

   [12]view on github[13]view on arxivppo lets us train ai policies in
   challenging environments, like the [14]roboschool one shown above where
   an agent tries to reach a target (the pink sphere), learning to walk,
   run, turn, use its momentum to recover from minor hits, and how to
   stand up from the ground when it is knocked over.

   [15]id189 are fundamental to recent breakthroughs in
   using deep neural networks for control, from [16]video games, to [17]3d
   locomotion, to [18]go. but getting good results via policy gradient
   methods is challenging because they are sensitive to the choice of
   stepsize     too small, and progress is hopelessly slow; too large and
   the signal is overwhelmed by the noise, or one might see catastrophic
   drops in performance. they also often have very poor sample efficiency,
   taking millions (or billions) of timesteps to learn simple tasks.

   researchers have sought to eliminate these flaws with approaches like
   [19]trpo and [20]acer, by constraining or otherwise optimizing the size
   of a policy update. these methods have their own trade-offs     acer is
   far more complicated than ppo, requiring the addition of code for
   off-policy corrections and a replay buffer, while only doing marginally
   better than ppo on the atari benchmark; trpo     though useful for
   continuous control tasks     isn't easily compatible with algorithms that
   share parameters between a policy and value function or auxiliary
   losses, like those used to solve problems in atari and other domains
   where the visual input is significant.

ppo

   with supervised learning, we can easily implement the cost function,
   run id119 on it, and be very confident that we'll get
   excellent results with relatively little hyperparameter tuning. the
   route to success in id23 isn't as obvious     the
   algorithms have many moving parts that are hard to debug, and they
   require substantial effort in tuning in order to get good results. ppo
   strikes a balance between ease of implementation, sample complexity,
   and ease of tuning, trying to compute an update at each step that
   minimizes the cost function while ensuring the deviation from the
   previous policy is relatively small.

   we've [21]previously detailed a variant of ppo that uses an adaptive
   [22]kl penalty to control the change of the policy at each iteration.
   the new variant uses a novel objective function not typically found in
   other algorithms:

   \[l^{clip}(\theta) = \hat{e}_{t}[ min( r_t(\theta)\hat{a}_t,
   clip(r_t(\theta), 1 - \varepsilon, 1 + \varepsilon) \hat{a}_t ) ]\]
     * \(\theta\) is the policy parameter
     * \(\hat{e}_t\) denotes the empirical expectation over timesteps
     * \(r_{t}\) is the ratio of the id203 under the new and old
       policies, respectively
     * \(\hat{a}_t\)is the estimated advantage at time \(t\)
     * \(\varepsilon\) is a hyperparameter, usually 0.1 or 0.2

   this objective implements a way to do a trust region update which is
   compatible with stochastic id119, and simplifies the
   algorithm by removing the kl penalty and need to make adaptive updates.
   in tests, this algorithm has displayed the best performance on
   continuous control tasks and almost matches acer's performance on
   atari, despite being far simpler to implement.
     __________________________________________________________________

controllable, complicated robots

   agents trained with ppo develop flexible movement policies that let
   them improvise turns and tilts as they head towards a target location.

   we've created interactive agents based on policies trained by ppo     we
   can [23]use the keyboard to set new target positions for a robot in an
   environment within roboschool; though the input sequences are different
   from what the agent was trained on, it manages to generalize.

   we've also used ppo to teach complicated, simulated robots to walk,
   like the 'atlas' model from boston dynamics shown below; the model has
   30 distinct joints, versus 17 for the bipedal robot. [24]other
   researchers have used ppo to train simulated robots to perform
   impressive feats of parkour while running over obstacles.

baselines: ppo, ppo2, acer, and trpo

   this release of [25]baselines includes scalable, parallel
   implementations of ppo and trpo which both use mpi for data passing.
   both use python3 and tensorflow. we're also adding pre-trained versions
   of the policies used to train the above robots to the [26]roboschool
   [27]agent zoo.

   update: we're also releasing a gpu-enabled implementation of ppo,
   called ppo2. this runs approximately 3x faster than the current ppo
   baseline on atari. in addition, we're releasing an implementation of
   actor critic with experience replay (acer), a sample-efficient policy
   gradient algorithm. acer makes use of a replay buffer, enabling it to
   perform more than one gradient update using each piece of sampled
   experience, as well as a q-function approximate trained with the
   retrace algorithm.
     __________________________________________________________________

   we   re looking for people to help build and optimize our reinforcement
   learning algorithm codebase. if you   re excited about rl, benchmarking,
   thorough experimentation, and open source, please [28]apply, and
   mention that you read the baselines ppo post in your application.
     __________________________________________________________________

   cover artwork
   ben barry
     __________________________________________________________________

   authors
   [29]john schulman[30]oleg klimov[31]filip wolski[32]prafulla
   dhariwal[33]alec radford
     __________________________________________________________________

     * [34]about
     * [35]progress
     * [36]resources
     * [37]blog
     * [38]charter
     * [39]jobs
     * [40]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://openai.com/blog/openai-baselines-ppo/#ppo
  12. https://github.com/openai/baselines
  13. https://arxiv.org/abs/1707.06347
  14. https://github.com/openai/roboschool
  15. http://karpathy.github.io/2016/05/31/rl/
  16. https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html
  17. https://arxiv.org/abs/1506.02438
  18. https://www.nature.com/nature/journal/v529/n7587/full/nature16961.html
  19. https://arxiv.org/abs/1502.05477
  20. https://arxiv.org/abs/1611.01224
  21. https://channel9.msdn.com/events/neural-information-processing-systems-conference/neural-information-processing-systems-conference-nips-2016/deep-reinforcement-learning-through-policy-optimization
  22. https://en.wikipedia.org/wiki/kullback   leibler_divergence
  23. https://github.com/openai/roboschool/blob/master/agent_zoo/demo_keyboard_humanoid1.py
  24. https://arxiv.org/abs/1707.02286
  25. https://github.com/openai/baselines
  26. https://blog.openai.com/roboschool/
  27. https://github.com/openai/roboschool/tree/master/agent_zoo
  28. https://jobs.lever.co/openai/5c1b2c12-2d18-42f0-836e-96af2cfca5ef
  29. https://openai.com/blog/authors/john/
  30. https://openai.com/blog/authors/oleg/
  31. https://openai.com/blog/authors/filip/
  32. https://openai.com/blog/authors/prafulla/
  33. https://openai.com/blog/authors/alec/
  34. https://openai.com/about/
  35. https://openai.com/progress/
  36. https://openai.com/resources/
  37. https://openai.com/blog/
  38. https://openai.com/charter/
  39. https://openai.com/jobs/
  40. https://openai.com/press/

   hidden links:
  42. https://openai.com/
  43. https://openai.com/
  44. https://twitter.com/openai
  45. https://www.facebook.com/openai.research
