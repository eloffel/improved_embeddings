   [1]openai gym

   nav
     * [2]home
     * [3]environments
     * [4]documentation
     * close

     * [5]getting started with gym
          + [6]installation
               o [7]building from source
          + [8]environments
          + [9]observations
          + [10]spaces
     * [11]available environments
          + [12]the registry
     * [13]background: why gym? (2016)

getting started with gym

   gym is a toolkit for developing and comparing id23
   algorithms. it makes no assumptions about the structure of your agent,
   and is compatible with any numerical computation library, such as
   tensorflow or theano.

   the [14]gym library is a collection of test problems     environments    
   that you can use to work out your id23 algorithms.
   these environments have a shared interface, allowing you to write
   general algorithms.
     __________________________________________________________________

installation

   to get started, you   ll need to have python 3.5+ installed. simply
   install gym using pip:
pip install gym

   and you   re good to go!

building from source

   if you prefer, you can also clone the gym git repository directly. this
   is particularly useful when you   re working on modifying gym itself or
   adding environments. download and install using:
git clone https://github.com/openai/gym
cd gym
pip install -e .

   you can later run pip install -e .[all] to perform a [15]full
   installation containing all environments. this requires installing
   several more involved dependencies, including cmake and a recent
   [16]pip version.
     __________________________________________________________________

environments

   here   s a bare minimum example of getting something running. this will
   run an instance of the [17]cartpole-v0 environment for 1000 timesteps,
   rendering the environment at each step. you should see a window pop up
   rendering the classic [18]cart-pole problem:
import gym
env = gym.make('cartpole-v0')
env.reset()
for _ in range(1000):
    env.render()
    env.step(env.action_space.sample()) # take a random action
env.close()

   it should look something like this:

   normally, we   ll end the simulation before the cart-pole is allowed to
   go off-screen. more on that later.

   if you   d like to see some other environments in action, try replacing
   cartpole-v0 above with something like [19]mountaincar-v0,
   [20]mspacman-v0 (requires the [21]atari dependency), or [22]hopper-v1
   (requires the [23]mujoco dependencies). environments all descend from
   the [24]env base class.

   note that if you   re missing any dependencies, you should get a helpful
   error message telling you what you   re missing. ([25]let us know if a
   dependency gives you trouble without a clear instruction to fix it.)
   [26]installing a missing dependency is generally pretty simple. you   ll
   also need a [27]mujoco license for hopper-v1.
     __________________________________________________________________

observations

   if we ever want to do better than take random actions at each step,
   it   d probably be good to actually know what our actions are doing to
   the environment.

   the environment   s step function returns exactly what we need. in fact,
   step returns four values. these are:
     * observation (object): an environment-specific object representing
       your observation of the environment. for example, pixel data from a
       camera, joint angles and joint velocities of a robot, or the board
       state in a board game.
     * reward (float): amount of reward achieved by the previous action.
       the scale varies between environments, but the goal is always to
       increase your total reward.
     * done (boolean): whether it   s time to reset the environment again.
       most (but not all) tasks are divided up into well-defined episodes,
       and done being true indicates the episode has terminated. (for
       example, perhaps the pole tipped too far, or you lost your last
       life.)
     * info (dict): diagnostic information useful for debugging. it can
       sometimes be useful for learning (for example, it might contain the
       raw probabilities behind the environment   s last state change).
       however, official evaluations of your agent are not allowed to use
       this for learning.

   this is just an implementation of the classic    agent-environment loop   .
   each timestep, the agent chooses an action, and the environment returns
   an observation and a reward.
   [aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e5
   3.svg]

   the process gets started by calling reset(), which returns an initial
   observation. so a more proper way of writing the previous code would be
   to respect the done flag:
import gym
env = gym.make('cartpole-v0')
for i_episode in range(20):
    observation = env.reset()
    for t in range(100):
        env.render()
        print(observation)
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)
        if done:
            print("episode finished after {} timesteps".format(t+1))
            break
env.close()

   this should give a video and output like the following. you should be
   able to see where the resets happen.
[-0.061586   -0.75893141  0.05793238  1.15547541]
[-0.07676463 -0.95475889  0.08104189  1.46574644]
[-0.0958598  -1.15077434  0.11035682  1.78260485]
[-0.11887529 -0.95705275  0.14600892  1.5261692 ]
[-0.13801635 -0.7639636   0.1765323   1.28239155]
[-0.15329562 -0.57147373  0.20218013  1.04977545]
episode finished after 14 timesteps
[-0.02786724  0.00361763 -0.03938967 -0.01611184]
[-0.02779488 -0.19091794 -0.03971191  0.26388759]
[-0.03161324  0.00474768 -0.03443415 -0.04105167]
     __________________________________________________________________

spaces

   in the examples above, we   ve been sampling random actions from the
   environment   s action space. but what actually are those actions? every
   environment comes with an action_space and an observation_space. these
   attributes are of type [28]space, and they describe the format of valid
   actions and observations:
import gym
env = gym.make('cartpole-v0')
print(env.action_space)
#> discrete(2)
print(env.observation_space)
#> box(4,)

   the [29]discrete space allows a fixed range of non-negative numbers, so
   in this case valid actions are either 0 or 1. the [30]box space
   represents an n-dimensional box, so valid observations will be an array
   of 4 numbers. we can also check the box   s bounds:
print(env.observation_space.high)
#> array([ 2.4       ,         inf,  0.20943951,         inf])
print(env.observation_space.low)
#> array([-2.4       ,        -inf, -0.20943951,        -inf])

   this introspection can be helpful to write generic code that works for
   many different environments. box and discrete are the most common
   spaces. you can sample from a space or check that something belongs to
   it:
from gym import spaces
space = spaces.discrete(8) # set with 8 elements {0, 1, 2, ..., 7}
x = space.sample()
assert space.contains(x)
assert space.n == 8

   for cartpole-v0 one of the actions applies force to the left, and one
   of them applies force to the right. (can you figure out which is
   which?)

   fortunately, the better your learning algorithm, the less you   ll have
   to try to interpret these numbers yourself.
     __________________________________________________________________

available environments

   gym comes with a diverse suite of environments that range from easy to
   difficult and involve many different kinds of data. view the [31]full
   list of environments to get the birds-eye view.
     * [32]classic control and [33]toy text: complete small-scale tasks,
       mostly from the rl literature. they   re here to get you started.
     * [34]algorithmic: perform computations such as adding multi-digit
       numbers and reversing sequences. one might object that these tasks
       are easy for a computer. the challenge is to learn these algorithms
       purely from examples. these tasks have the nice property that it   s
       easy to vary the difficulty by varying the sequence length.
     * [35]atari: play classic atari games. we   ve integrated the
       [36]arcade learning environment (which has had a big impact on
       id23 research) in an [37]easy-to-install form.
     * [38]2d and 3d robots: control a robot in simulation. these tasks
       use the [39]mujoco physics engine, which was designed for fast and
       accurate robot simulation. included are some environments from a
       recent [40]benchmark by uc berkeley researchers (who incidentally
       will be [41]joining us this summer). mujoco is proprietary
       software, but offers [42]free trial licenses.

the registry

   gym   s main purpose is to provide a large collection of environments
   that expose a common interface and are versioned to allow for
   comparisons. to list the environments available in your installation,
   just ask gym.envs.registry:
from gym import envs
print(envs.registry.all())
#> [envspec(doubledunk-v0), envspec(inverteddoublependulum-v0), envspec(beamride
r-v0), envspec(phoenix-ram-v0), envspec(asterix-v0), envspec(timepilot-v0), envs
pec(alien-v0), envspec(robotank-ram-v0), envspec(cartpole-v0), envspec(berzerk-v
0), envspec(berzerk-ram-v0), envspec(gopher-ram-v0), ...

   this will give you a list of [43]envspec objects. these define
   parameters for a particular task, including the number of trials to run
   and the maximum number of steps. for example, [44]envspec(hopper-v1)
   defines an environment where the goal is to get a 2d simulated robot to
   hop; [45]envspec(go9x9-v0) defines a go game on a 9x9 board.

   these environment ids are treated as opaque strings. in order to ensure
   valid comparisons for the future, environments will never be changed in
   a fashion that affects performance, only replaced by newer versions. we
   currently suffix each environment with a v0 so that future replacements
   can naturally be called v1, v2, etc.

   it   s very easy to add your own enviromments to the registry, and thus
   make them available for gym.make(): just [46]register() them at load
   time.

background: why gym? (2016)

   id23 (rl) is the subfield of machine learning
   concerned with decision making and motor control. it studies how an
   agent can learn how to achieve goals in a complex, uncertain
   environment. it   s exciting for two reasons:
     * rl is very general, encompassing all problems that involve making a
       sequence of decisions: for example, controlling a robot   s motors so
       that it   s able to [47]run and [48]jump, making business decisions
       like pricing and inventory management, or playing [49]video games
       and [50]board games. rl can even be applied to supervised learning
       problems with [51]sequential [52]or [53]structured outputs.
     * rl algorithms have started to achieve good results in many
       difficult environments. rl has a long history, but until recent
       advances in deep learning, it required lots of problem-specific
       engineering. deepmind   s [54]atari results, [55]brett from
       [56]pieter abbeel   s group, and [57]alphago all used deep rl
       algorithms which did not make too many assumptions about their
       environment, and thus can be applied in other settings.

   however, rl research is also slowed down by two factors:
     * the need for better benchmarks. in supervised learning, progress
       has been driven by large labeled datasets like [58]id163. in rl,
       the closest equivalent would be a large and diverse collection of
       environments. however, the existing open-source collections of rl
       environments don   t have enough variety, and they are often
       difficult to even set up and use.
     * lack of standardization of environments used in publications.
       subtle differences in the problem definition, such as the reward
       function or the set of actions, can drastically alter a task   s
       difficulty. this issue makes it difficult to reproduce published
       research and compare results from different papers.

   gym is an attempt to fix both problems.

     * [59]environments
     * [60]documentation

   [61]openai

references

   1. https://gym.openai.com/
   2. https://gym.openai.com/
   3. https://gym.openai.com/envs
   4. https://gym.openai.com/docs
   5. https://gym.openai.com/docs/#getting-started-with-gym
   6. https://gym.openai.com/docs/#installation
   7. https://gym.openai.com/docs/#building-from-source
   8. https://gym.openai.com/docs/#environments
   9. https://gym.openai.com/docs/#observations
  10. https://gym.openai.com/docs/#spaces
  11. https://gym.openai.com/docs/#available-environments
  12. https://gym.openai.com/docs/#the-registry
  13. https://gym.openai.com/docs/#background-why-gym-2016
  14. https://github.com/openai/gym
  15. https://github.com/openai/gym#installing-everything
  16. https://github.com/openai/gym#pip-version
  17. https://gym.openai.com/envs/cartpole-v0
  18. https://www.youtube.com/watch?v=j7e6_my3chk
  19. https://gym.openai.com/envs/mountaincar-v0
  20. https://gym.openai.com/envs/mspacman-v0
  21. https://github.com/openai/gym#atari
  22. https://gym.openai.com/envs/hopper-v1
  23. https://github.com/openai/gym#mujoco
  24. https://github.com/openai/gym/blob/master/gym/core.py
  25. https://github.com/openai/gym/issues
  26. https://github.com/openai/gym#environment-specific-installation
  27. https://www.roboti.us/license.html
  28. https://github.com/openai/gym/blob/master/gym/core.py
  29. https://github.com/openai/gym/blob/master/gym/spaces/discrete.py
  30. https://github.com/openai/gym/blob/master/gym/spaces/box.py
  31. https://gym.openai.com/envs
  32. https://gym.openai.com/envs#classic_control
  33. https://gym.openai.com/envs#toy_text
  34. https://gym.openai.com/envs#algorithmic
  35. https://gym.openai.com/envs#atari
  36. http://www.arcadelearningenvironment.org/
  37. https://github.com/openai/gym#atari
  38. https://gym.openai.com/envs#mujoco
  39. http://www.mujoco.org/
  40. http://arxiv.org/abs/1604.06778
  41. https://gym.openai.com/blog/team-plus-plus/
  42. https://www.roboti.us/trybuy.html
  43. https://github.com/openai/gym/blob/master/gym/envs/registration.py
  44. https://gym.openai.com/envs/hopper-v1
  45. https://gym.openai.com/envs/go9x9-v0
  46. https://github.com/openai/gym/blob/master/gym/envs/__init__.py
  47. https://gym.openai.com/envs/humanoid-v0
  48. https://gym.openai.com/envs/hopper-v0
  49. https://gym.openai.com/envs#atari
  50. https://gym.openai.com/envs#board_game
  51. http://arxiv.org/abs/1511.06732
  52. http://arxiv.org/abs/0907.0786
  53. http://arxiv.org/abs/1601.01705
  54. https://deepmind.com/id25.html
  55. http://news.berkeley.edu/2015/05/21/deep-learning-robot-masters-skills-via-trial-and-error/
  56. https://gym.openai.com/blog/welcome-pieter-and-shivon
  57. https://googleblog.blogspot.com/2016/01/alphago-machine-learning-game-go.html
  58. http://www.image-net.org/
  59. https://gym.openai.com/envs
  60. https://gym.openai.com/docs
  61. https://openai.com/
