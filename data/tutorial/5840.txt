   [1]fork me on github

   [loop.svg]

reinforcejs

     * [2]about
     * [3]gridworld: dp
     * [4]gridworld: td
     * [5]puckworld: id25
     * [6]waterworld: id25

gridworld: id145 demo

   (button) policy evaluation (one sweep) (button) policy update (button)
   toggle value iteration (button) reset
   cell reward: (select a cell)
     __________________________________________________________________

   ### setup this is a toy environment called **gridworld** that is often
   used as a toy model in the id23 literature. in this
   particular case: - **state space**: gridworld has 10x10 = 100 distinct
   states. the start state is the top left cell. the gray cells are walls
   and cannot be moved to. - **actions**: the agent can choose from up to
   4 actions to move around. in this example - **environment dynamics**:
   gridworld is deterministic, leading to the same new state given each
   state and action - **rewards**: the agent receives +1 reward when it is
   in the center square (the one that shows r 1.0), and -1 reward in a few
   states (r -1.0 is shown for these). the state with +1.0 reward is the
   goal state and resets the agent back to start. in other words, this is
   a deterministic, finite markov decision process (mdp) and as always the
   goal is to find an agent policy (shown here by arrows) that maximizes
   the future discounted reward. my favorite part is letting value
   iteration converge, then change the cell rewards and watch the policy
   adjust. **interface**. the color of the cells (initially all white)
   shows the current estimate of the value (discounted reward) of that
   state, with the current policy. note that you can select any cell and
   change its reward with the *cell reward* slider. ### dynamic
   programming an interested reader should refer to **richard sutton's
   free online book on id23**, in this particular case
   [chapter
   4](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node40.html).
   briefly, an agent interacts with the environment based on its policy
   \\(\pi(a \mid s)\\). this is a function from states \\(s\\) to an
   action \\(a\\), or more generally to a distribution over the possible
   actions. after every action, the agent also receives a reward \\(r\\)
   from the environment. the interaction between the agent and the
   environment hence takes the form \\(s\_t, a\_t, r\_t, s\_{t+1},
   a\_{t+1}, r\_{t+1}, s\_{t+2}, \ldots \\), indexed by t (time) and our
   goal is to find the policy \\(\pi^\*\\) that maximizes the amount of
   reward over time. in the dp approach, it is helpful to define a **value
   function** \\(v^\pi(s)\\) that gives the expected reward when starting
   from the state \\(s\\) and then interacting with the environment
   according to \\(\pi\\): $$ v^\pi(s) = e \\{ r\_t + \gamma r\_{t+1} +
   \gamma^2 r\_{t+2} + \ldots \mid s\_t = s \\} $$ note that the
   expectation is over both 1. the agent's (in general) stochastic policy
   and 2. the environment dynamics; that is, how the environment evolves
   when the agent takes an action \\(a\\) in state \\(s\\) and what the
   obtained reward is in that case. the constant \\(\gamma\\) is a
   discount factor that gives more weight to earlier rewards than the
   later ones. we can start writing out the expectation. to get the value
   of state \\(s\\) we sum over all the actions the agent would take, then
   over all ways the environment could respond, and so on back and forth.
   when you do this, you'll find that: $$ v^\pi(s) = \sum\_{a} \pi(s,a)
   \sum\_{s'} \mathcal{p}\_{ss'}^{a} \left[ \mathcal{r}\_{ss'}^{a} +
   \gamma v^\pi(s') \right] $$ in the above equation
   \\(\mathcal{p}\_{ss'}^{a} , \mathcal{r}\_{ss'}^{a} \\) are fixed
   constants specific to the environment, and give the id203 of the
   next state \\(s'\\) given that the agent took action \\(a\\) in state
   \\(s\\), and the expected reward for that transition, respectively.
   this equation is called the **bellman equation** and interestingly, it
   describes a recursive relationship that the value function for a given
   policy should satisfy. with our goal of finding the optimal policy
   \\(\pi^\*(s,a)\\) that gets the most value from all states, our
   strategy will be to follow the **policy iteration** scheme: we start
   out with some diffuse initial policy and evaluate the value function of
   every state under that policy by turning the bellman equation into an
   update. the value function effectively diffuses the rewards backwards
   through the environment dynamics and the agent's expected actions, as
   given by its current policy. once we estimate the values of all states
   we will update the policy to be greedy with respect the value function.
   we then re-estimate the value of each state, and continue iterating
   until we converge to the optimal policy (the process can be shown to
   converge). these two steps can be controlled by the buttons in the
   interface: - the **policy evaluation (one sweep)** button turns the
   bellman equation into a synchronous update and performs a single step
   of value function estimation. intuitively, this update is diffusing the
   raw rewards at each state to other nearby states through 1. the the
   dynamics of the environment and 2. the current policy. - the **policy
   update** button iterates over all states and updates the policy at each
   state to take the action that leads to the state with the best value
   (integrating over the next state distribution of the environment for
   each action). - the **value iteration** button starts a timer that
   presses the two buttons in turns. in particular, note that value
   iteration doesn't wait for the value function to be fully estimates,
   but only a single synchronous sweep of bellman update is carried out.
   in full policy iteration there would be many sweeps (until convergence)
   of backups before the policy is updated. ### sketch of the code the
   goal of **policy evaluation** is to update the value of every state by
   diffusing the rewards backwards through the dynamics of the world and
   current policy (this is called a *backup*). the code looks like this:
evaluatepolicy: function() {
  // perform a synchronous update of the value function
  var vnew = zeros(this.ns); // initialize new value function array for each sta
te
  for(var s=0;s < this.ns;s++) {
    var v = 0.0;
    var poss = this.env.allowedactions(s); // fetch all possible actions
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      var prob = this.p[a*this.ns+s]; // id203 of taking action under curr
ent policy
      var ns = this.env.nextstatedistribution(s,a); // look up the next state
      var rs = this.env.reward(s,a,ns); // get reward for s->a->ns transition
      v += prob * (rs + this.gamma * this.v[ns]);
    }
    vnew[s] = v;
  }
  this.v = vnew; // swap
},

   here, we see `this.ns` (number of states), `this.p` which stores the
   current policy, and `this.env`, which is a pointer to the environment
   object. the policy array is one-dimensional in this implementation, but
   stores the id203 of taking any action in any state, so i'm using
   funny indexing (`this.p[a*this.ns+s]`) to not have to deal with 2d
   arrays in javascript. the code implements the synchronous value backup
   for each state: \begin{align} v^\pi(s) & = e\_\pi \\{ r\_t + \\gamma
   r\_{t+1} + \\gamma^2 r\_{t+2} + \\ldots \\mid s\_t = s \\} \\\\ & =
   e\_\pi \\{ r\_t + \\gamma v^\pi(s\_{t+1}) \\mid s\_t = s \\} \\\\ & =
   \sum\_a \pi(s,a) \sum\_{s'} \mathcal{p}\_{ss'}^a \left[
   \mathcal{r}\_{ss'}^a + \\gamma v(s') \right] \end{align} however, note
   that in the code we only took expectation over the policy actions (the
   first sum above), while the second sum above was not evaluated because
   this gridworld is deterministic (i.e. `ns` in the code is just a single
   integer indicating the next state). hence, the entire id203 mass
   is on a single next state and no expectation is needed. once the value
   function was re-estimated, we can perform a **policy update**, making
   the policy greedy with respect to the estimate value in each state.
   this is a very simple process, but the code below is a little bloated
   because we're handling ties between actions carefully and distributing
   the policy probabilities equally over all winning actions:
updatepolicy: function() {
  // update policy to be greedy w.r.t. learned value function
  // iterate over all states...
  for(var s=0;s < this.ns;s++) {
    var poss = this.env.allowedactions(s);
    // compute value of taking each allowed action
    var vmax, nmax;
    var vs = [];
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      // compute the value of taking action a
      var ns = this.env.nextstatedistribution(s,a);
      var rs = this.env.reward(s,a,ns);
      var v = rs + this.gamma * this.v[ns];
      // bookeeping: store it and maintain max
      vs.push(v);
      if(i === 0 || v > vmax) { vmax = v; nmax = 1; }
      else if(v === vmax) { nmax += 1; }
    }
    // update policy smoothly across all argmaxy actions
    for(var i=0,n=poss.length;i < n;i++) {
      var a = poss[i];
      this.p[a*this.ns+s] = (vs[i] === vmax) ? 1.0/nmax : 0.0;
    }
  }
},

   here, we are computing the action value function at each state
   \\(q(s,a)\\), which measures how much expected reward we would get by
   taking the action \\(a\\) in the state \\(s\\). the function has the
   form: \begin{align} q^\pi (s,a) &= e\_\pi \\{ r\_t + \\gamma
   v^\pi(s\_{t+1}) \\mid s\_t = s, a\_t = a \\} \\\\ &= \sum\_{s'}
   \mathcal{p}\_{ss'}^a \left[ \mathcal{r}\_{ss'}^a + \\gamma v^\pi(s')
   \right] \end{align} in our gridworld example, we are looping over all
   states and evaluating the q function for each of the (up to) four
   possible actions. then we update the policy to take the argmaxy actions
   at each state. that is, $$ \pi'(s) = \arg\max_a q(s,a) $$ it can be
   shown (see richard sutton's book) that performing these updates over
   and over again is guaranteed to converge to the optimal policy. in this
   gridworld example, this corresponds to arrows that perfectly guide the
   agent to the terminal state where it gets reward +1. ### reinforcejs
   api use of dp if you'd like to use the reinforcejs id145
   for your mdp, you have to define an environment object `env` that has a
   few methods that the dp agent will need: - `env.getnumstates()` returns
   an integer of total number of states - `env.getmaxnumactions()` returns
   an integer with max number of actions in any state -
   `env.allowedactions(s)` takes an integer `s` and returns a list of
   available actions, which should be integers from zero to
   `maxnumactions` - `env.nextstatedistribution(s,a)`, which is a
   misnomer, since right now the library assumes deterministic mdps that
   have a single unique new state for every (state, action) pair.
   therefore, the function should return a single integer that identifies
   the next state of the world - `env.reward(s,a,ns)`, which returns a
   float for the reward achieved by the agent for the `s`, `a`, `ns`
   transition. in the simplest case, the reward is usually based only the
   state `s`. see the gridworld environment in this demo's source code for
   an example. an example of creating and training the agent will then
   look something like:
// create environment
env = new gridworld();
// create the agent, yay! discount factor 0.9
agent = new rl.dpagent(env, {'gamma':0.9});

// call this function repeatedly until convergence:
agent.learn();

// once trained, get the agent's behavior with:
var action = agent.act(); // returns the index of the chosen action

   ### pros and cons in practice you'll rarely see people use dynamic
   programming to solve id23 problems. there are
   numerous reasons for this, but the two biggest ones are probably that:
   - it's not obvious how one can extend this to continuous actions and
   states - to calculate these updates one must have access to the
   environment model \\(p\_{ss'}^a\\), which is in practice almost never
   available. the best we can usually hope for is to get samples from this
   distribution by having an agent interacting with the environment and
   collecting experience, which we can use to approximately evaluate the
   expectations by sampling. more on this in td methods! however, the nice
   parts are that: - everything is mathematically exact, expressible and
   analyzable. - if your problem is relatively small (maybe a few thousand
   states and up to few tens/hundreds actions), dp methods might be the
   best solution for you because they are the most stable, safest and come
   with simplest convergence guarantees.

references

   1. https://github.com/karpathy/reinforcejs
   2. https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
   3. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html
   4. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html
   5. https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html
   6. https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
