   [1]fork me on github

   [loop.svg]

reinforcejs

     * [2]about
     * [3]gridworld: dp
     * [4]gridworld: td
     * [5]puckworld: id25
     * [6]waterworld: id25

puckworld: deep id24


   // agent parameter spec to play with (this gets eval()'d on 
   var spec = {}_______________________________________________
   spec.update = 'qlearn'; // qlearn | sarsa___________________
   spec.gamma = 0.9; // discount factor, [0, 1)________________
   spec.epsilon = 0.2; // initial epsilon for epsilon-greedy po
   spec.alpha = 0.01; // value function learning rate__________
   spec.experience_add_every = 10; // number of time steps befo
   spec.experience_size = 5000; // size of experience replay me
   spec.learning_steps_per_iteration = 20;_____________________
   spec.tderror_clamp = 1.0; // for robustness_________________
   spec.num_hidden_units = 100 // number of neurons in hidden l
   (button) reinit agent (button) toggle run (button) go fast (button) go
   normal (button) go slow
   exploration epsilon: 0.15
   (button) load a pretrained agent

   mystery text box____________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   experience write pointer:
   latest td error:
   average reward graph (high is good)
   ### setup this demo is a modification of **puckworld**: - the **state
   space** is now large and continuous: the agent observes its own
   location (x,y), velocity (vx,vy), the locations of the green target,
   and the red target (total of 8 numbers). - there are 4 **actions**
   available to the agent: to apply thrusters to the left, right, up and
   down. this gives the agent control over its velocity. - the puckworld
   **dynamics** integrate the velocity of the agent to change its
   position. the green target moves once in a while to a random location.
   the red target always slowly follows the agent. - the **reward**
   awarded to the agent is based on its distance to the green target (low
   is good). however, if the agent is in the vicinity of the red target
   (inside the disk), the agent gets a negative reward proportional to its
   distance to the red target. the optimal strategy for the agent is to
   always go towards the green target (this is the regular puckworld), but
   to also avoid the red target's area of effect. this makes things more
   interesting because the agent has to learn to avoid it. also, sometimes
   it's fun to watch the red target corner the agent. in this case, the
   optimal thing to do is to temporarily pay the price to zoom by quickly
   and away, rather than getting cornered and paying much more reward
   price when that happens. **interface**: the current reward experienced
   by the agent is shown as its color (green = high, red = low). the
   action taken by the agent (the medium-sized circle moving around) is
   shown by the arrow. ### deep id24 we're going to extend the
   temporal difference learning (id24) to continuous state spaces.
   in the previos demo we've estimated the id24 function
   \\(q(s,a)\\) as a lookup table. now, we are going to use a function
   approximator to model \\(q(s,a) = f\_{\theta}(s,a)\\), where
   \\(\theta\\) are some parameters (e.g. weights and biases of a neurons
   in a network). however, as we will see everything else remains exactly
   the same. the first paper that showed impressive results with this
   approach was [playing atari with deep reinforcement
   learning](http://arxiv.org/abs/1312.5602) at nips workshop in 2013, and
   more recently the nature paper [human-level control through deep
   reinforcement
   learning](http://www.nature.com/nature/journal/v518/n7540/full/nature14
   236.html), both by mnih et al. however, more impressive than what we'll
   develop here, their function \\(f\_{\theta,a}\\) was an entire
   convolutional neural network that looked at the raw pixels of atari
   game console. it's hard to get all that to work in js :( recall that in
   id24 we had training data that is a single chain of \\(s\_t,
   a\_t, r\_t, s\_{t+1}, a\_{t+1}, r\_{t+1}, s\_{t+2}, \ldots \\) where
   the states \\(s\\) and the rewards \\(r\\) are samples from the
   environment dynamics, and the actions \\(a\\) are sampled from the
   agent's policy \\(a\_t \sim \pi(a \mid s\_t)\\). the agent's policy in
   id24 is to execute the action that is currently estimated to have
   the highest value (\\( \pi(a \mid s) = \arg\max\_a q(s,a) \\)), or with
   a id203 \\(\epsilon\\) to take a random action to ensure some
   exploration. the id24 update at each time step then had the
   following form: $$ q(s\_t, a\_t) \leftarrow q(s\_t, a\_t) + \alpha
   \left[ r\_t + \gamma \max\_a q(s\_{t+1}, a) - q(s\_t, a\_t) \right] $$
   as mentioned, this equation is describing a regular stochastic gradient
   descent update with learning rate \\(\alpha\\) and the id168 at
   time \\(t\\): $$ l\_t = (r\_t + \gamma \max\_a q(s\_{t+1}, a) - q(s\_t,
   a\_t))^2 $$ here \\(y = r\_t + \gamma \max\_a q(s\_{t+1}, a)\\) is
   thought of as a scalar-valued fixed target, while we backpropagate
   through the neural network that produced the prediction \\(f\_{\theta}
   = q(s\_t,a\_t)\\). note that the loss has a standard l2 norm regression
   form, and that we nudge the parameter vector \\(\theta\\) in a way that
   makes the computed \\(q(s,a)\\) slightly closer to what it should be
   (i.e. to satisfy the bellman equation). this softly encourages the
   constraint that the reward should be properly diffused, in expectation,
   backwards through the environment dynamics and the agent's policy. in
   other words, deep id24 is a 1-dimensional regression problem with
   a vanilla neural network, solved with vanilla stochastic gradient
   descent, except our training data is not fixed but generated by
   interacting with the environment. ### bells and whistles there are
   several bells and whistles that make id24 with function
   approximation tracktable in practical problems: **modeling q(s,a)**.
   first, as mentioned we are using a function approximator to model the q
   function: \\(q(s,a) = f\_{\theta}(s,a)\\). the natural approach to take
   is to perhaps have a neural network that takes as inputs the state
   vector \\(s\\) and an action vector \\(a\\) (e.g. encoded with a 1-of-k
   indicator vector), and output a single number that gives \\(q(s,a)\\).
   however, this approach leads to a practical problem because the policy
   of the agent is to take an action that maximizes q, that is: \\( \pi(a
   \mid s) = \arg\max\_a q(s,a) \\). therefore, with this naive approach,
   when the agent wants to produce an action we'd have to iterate over all
   actions, evaluate q for each one and take the action that gave the
   highest q. a better idea is to instead predict the value \\(q(s,a)\\)
   as a neural network that only takes the state \\(s\\), and produces
   multiple output, each interpreted as the q value of taking that action
   in the given state. this way, deciding what action to take reduces to
   performing a single forward pass of the network and finding the argmax
   action. a diagram may help get the idea across:
   [qsa.jpeg]
   simple example with 3-dimensional state space (blue) and 2 possible
   actions (red). green nodes are neurons of a neural network f. left: a
   naive approach in which it takes multiple forward passes to find the
   argmax action. right: a more efficient approach where the q(s,a)
   computation is effectively shared among the neurons in the network.
   only a single forward pass is required to find the best action to take
   in any given state.
   formulated in this way, the update algorithm becomes: 1. experience a
   \\(s\_t, a\_t, r\_t, s\_{t+1}\\) transition in the environment. 2.
   forward \\(s\_{t+1}\\) to evaluate the (fixed) target \\(y = \max\_a
   f\_{\theta}(s\_{t+1})\\). this quantity is interpreted to be \\(\max\_a
   q(s\_{t+1})\\). 3. forward \\(f\_{\theta}(s\_t)\\) and apply a simple
   l2 regression loss on the dimension \\(a\_t\\) of the output, to be
   \\(y\\). this gradient will have a very simple form where the predicted
   value is simply subtracted from \\(y\\). the other output dimensions
   have zero gradient. 4. backpropagate the gradient and perform a
   parameter update. **experience replay memory**. an important
   contribution of the mnih et al. papers was to suggest an experience
   replay memory. this is loosely inspired by the brain, and in particular
   the way it syncs memory traces in the hippocampus with the cortex. what
   this amounts to is that instead of performing an update and then
   throwing away the experience tuple \\(s\_t, a\_t, r\_t, s\_{t+1}\\), we
   keep it around and effectively build up a training set of experiences.
   then, we don't learn based on the new experience that comes in at time
   \\(t\\), but instead sample random expriences from the replay memory
   and perform an update on each sample. again, this is exactly as if we
   were training a neural net with sgd on a dataset in a regular machine
   learning setup, except here the dataset is a result of agent
   interaction. this feature has the effect of removing correlations in
   the observed state,action,reward sequence and reduces gradual drift and
   forgetting. the algorihm hence becomes: 1. experience \\(s\_t, a\_t,
   r\_t, s\_{t+1}\\) in the environment and add it to the training dataset
   \\(\mathcal{d}\\). if the size of \\(\mathcal{d}\\) is greater than
   some threshold, start replacing old experiences. 2. sample **n**
   experiences from \\(\mathcal{d}\\) at random and update the q function.
   **clamping td error**. another interesting feature is to clamp the td
   error gradient at some fixed maximum value. that is, if the td error
   \\(r\_t + \gamma \max\_a q(s\_{t+1}, a) - q(s\_t, a\_t)\\) is greater
   in magnitude then some threshold `spec.tderror_clamp`, then we cap it
   at that value. this makes the learning more robust to outliers and has
   the interpretation of using huber loss, which is an l2 penalty in a
   small region around the target value and an l1 penalty further away.
   **periodic target q value updates**. the last modification suggested in
   mnih et al. also aims to reduce correlations between updates and the
   immediately undertaken behavior. the idea is to freeze the q network
   once in a while into a frozen, copied network \\(\hat{q}\\) which is
   used to only compute the targets. this target network is once in a
   while updated to the actual current \\(q\\) network. that is, we use
   the target network \\(r\_t + \gamma \max\_a \hat{q}(s\_{t+1},a)\\) to
   compute the target, and \\(q\\) to evaluate the \\(q(s\_t, a\_t)\\)
   part. in terms of the implementation, this requires one additional line
   of code where we every now and then we sync \\(\hat{q} \leftarrow q\\).
   i played around with this idea but at least on my simple toy examples i
   did not find it to give a substantial benefit, so i took it out of
   reinforcejs in the current implementation for sake of brevity. ###
   reinforcejs api use of id25 if you'd like to use reinforcejs id25 in your
   application, define an `env` object that has the following methods: -
   `env.getnumstates()` returns an integer for the dimensionality of the
   state feature space - `env.getmaxnumactions()` returns an integer with
   the number of actions the agent can use this seems kind of silly and
   the name `getnumstates` is a bit of a misnomer because it refers to the
   size of the state feature vector, not the raw number of unique states,
   but i chose this interface so that it is consistent with the tabular td
   code and dp method. similar to the tabular td agent, the io with the
   agent has an extremely simple interface:
// create environment
var env = {};
env.getnumstates = function() { return 8; }
env.getmaxnumactions = function() { return 4; }

// create the agent, yay!
var spec = { alpha: 0.01 } // see full options on top of this page
agent = new rl.id25agent(env, spec);

setinterval(function(){ // start the learning loop
  var action = agent.act(s); // s is an array of length 8
  // execute action in environment and get the reward
  agent.learn(reward); // the agent improves its q,policy,model, etc. reward is
a float
}, 0);

   as for the available parameters in the id25 agent `spec`: - `spec.gamma`
   is the discount rate. when it is zero, the agent will be maximally
   greedy and won't plan ahead at all. it will grab all the reward it can
   get right away. for example, children that fail the marshmallow
   experiment have a very low gamma. this parameter goes up to 1, but
   cannot be greater than or equal to 1 (this would make the discounted
   reward infinite). - `spec.epsilon` controls the epsilon-greedy policy.
   high epsilon (up to 1) will cause the agent to take more random
   actions. it is a good idea to start with a high epsilon (e.g. 0.2 or
   even a bit higher) and decay it over time to be lower (e.g. 0.05). -
   `spec.num_hidden_units`: currently the id25 agent is hardcoded to use a
   neural net with one hidden layer, the size of which is controlled with
   this parameter. for each problems you may get away with smaller
   networks. - `spec.alpha` controls the learning rate. everyone sets this
   by trial and error and that's pretty much the best thing we have. -
   `spec.experience_add_every`: reinforcejs won't add a new experience to
   replay every single frame to try to conserve resources and get more
   variaty. you can turn this off by setting this parameter to 1. default
   = 5 - `spec.experience_size`: size of memory. more difficult problems
   may need bigger memory - `spec.learning_steps_per_iteration`: the more
   the better, but slower. default = 20 - `spec.tderror_clamp`: for
   robustness, clamp the td errror gradient at this value. ### pros and
   cons the very nice property of id25 agents is that the i/o interface is
   quite generic and very simple: the agent accepts state vectors,
   produces discrete actions, and learns from relatively arbitrary agents.
   however, there are several things to keep in mind when applying this
   agent in practice: - if the rewards are very sparse in the environment
   the agent will have trouble learning. right now there is no priority
   sweeping support, but one might imagine oversampling experience that
   have high td errors. it's not clear how this can be done in most
   principled way. similarly, there are no eligibility traces right now
   though this could be added with a few modifications in future versions.
   - the exploration is rather naive, since a random action is taken once
   in a while. if the environment requires longer sequences of precise
   actions to get a reward, the agent might have a lot of difficulty
   finding these by chance, and then also learning from them sufficiently.
   - id25 only supports a set number of discrete actions and it is not
   obvious how one can incorporate (high-dimensional) continuous action
   spaces. speaking of high-dimensional continuous action spaces, this is
   what policy gradient actor critic methods are quite good at! head over
   to the policy gradient (pg) demo. (oops, coming soon...)

references

   1. https://github.com/karpathy/reinforcejs
   2. https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
   3. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html
   4. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html
   5. https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html
   6. https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
