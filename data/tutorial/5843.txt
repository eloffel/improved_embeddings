   [1]fork me on github

   [loop.svg]

reinforcejs

     * [2]about
     * [3]gridworld: dp
     * [4]gridworld: td
     * [5]puckworld: id25
     * [6]waterworld: id25


   // agent parameter spec to play with (this gets eval()'d on 
   var spec = {}_______________________________________________
   spec.update = 'qlearn'; // qlearn | sarsa___________________
   spec.gamma = 0.9; // discount factor, [0, 1)________________
   spec.epsilon = 0.2; // initial epsilon for epsilon-greedy po
   spec.alpha = 0.005; // value function learning rate_________
   spec.experience_add_every = 5; // number of time steps befor
   spec.experience_size = 10000; // size of experience_________
   spec.learning_steps_per_iteration = 5;______________________
   spec.tderror_clamp = 1.0; // for robustness_________________
   spec.num_hidden_units = 100 // number of neurons in hidden l
   (button) reinit agent (button) go very fast (button) go fast (button)
   go normal (button) go slow
   (button) load a pretrained agent
   exploration epsilon: 0.15

   mystery text box____________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ### setup this is another deep id24 demo with a more realistic
   and larger setup: - the **state space** is even larger and continuous:
   the agent has 30 eye sensors pointing in all directions and in each
   direction is observes 5 variables: the range, the type of sensed object
   (green, red), and the velocity of the sensed object. the agent's
   proprioception includes two additional sensors for its own speed in
   both x and y directions. this is a total of 152-dimensional state
   space. - there are 4 **actions** available to the agent: to apply
   thrusters to the left, right, up and down. this gives the agent control
   over its velocity. - the **dynamics** integrate the velocity of the
   agent to change its position. the green and red targets bounce around.
   - the **reward** awarded to the agent is +1 for making contact with any
   red target (these are apples) and -1 for making contact with any green
   target (this is poison). the optimal strategy of the agent is to cruise
   around, run away from green targets and eat red targets. what's
   interesting about this demo is that the state space is so
   high-dimensional, and also that the sensed variables are
   agent-relative. they're not just toy x,y coordinates of some fixed
   number of targets as in previous demo.

references

   1. https://github.com/karpathy/reinforcejs
   2. https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
   3. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html
   4. https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html
   5. https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html
   6. https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html
