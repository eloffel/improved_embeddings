   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

explained simply: how deepmind taught ai to play video games

   [11]go to the profile of aman agarwal
   [12]aman agarwal (button) blockedunblock (button) followfollowing
   aug 27, 2017
   [0*1ygjpwnhuhdzlylq.jpg]
   [13]image credit

   google   s deepmind is one of the world   s foremost ai research teams.
   they   re most famous for creating the alphago player that beat south
   korean go champion lee sedol in 2016.

   the key technology used to create the go playing ai was deep
   id23.

   let   s go back 4 years, to when [14]deepmind first built an ai which
   could play atari games from the 70s. games like breakout, pong and
   space invaders. it was this research that led to alphago, and to
   deepmind being acquired by google.
   [1*04oeirzclxsyqhrfiro39w.jpeg]
   space invaders!

   today we   re going to take that original research paper and break it
   down paragraph-by-paragraph. this will make it more approachable for
   people who only just now started learning about id23.
   and for those who don   t use english as their first language (which
   makes it very difficult to read such papers).

   here   s the original paper if you want to try reading it:

   iframe: [15]/media/71f48cb2d54451e6a64ccd83b22c9aab?postid=9eb5f38c89ee

some quick notes (in case you don   t make it to the bottom of this 20-minute
article)

   the explanations here are by two people:
    1. [16]me, a self driving cars engineer
    2. [17]qiang lu, a phd candidate and researcher at the university of
       denver

   we hope that our work will save you a lot of time and effort if you
   were to study this on your own.

and if you   re more comfortable reading chinese, here is an *unofficial*
[18]translation of this essay.

    1. we love your claps, but love your comments even more. unload
       whatever is on your mind         feelings, suggestions, corrections, or
       criticism         into the comment box!
    2. i intend to write many more articles like this, and am looking for
       more collaborators. if you   d seriously like to contribute, please
       leave a comment.

let   s get started

   [1*9tq-bwyi2et8hfj29-1q9w.png]

   we want to make a robot learn how to play an atari game by itself,
   using id23.

   the robot in our case is a convolutional neural network.

   this is almost real end-to-end deep learning, because our robot gets
   inputs in the same way as a human player         it directly sees the image
   on the screen and the reward/change in points after each move, and that
   is all the information it needs to make a decision.

   and what does the robot output? well, ideally we want the robot to pick
   the action which it thinks promises the most reward in future. but
   instead of directly choosing the action, here we let it assign    values   
   to each of the 18 possible joystick actions. so put simply, the value v
   for any action a represents the robot   s expectation of the future
   reward it will get if it performs that action a.

   so in essence, this neural network is a value function. it takes as
   input the screen state and change in reward, and it outputs the
   different values associated with each possible action. so that you can
   pick the action with the highest value, or pick any other action based
   on how you program the overall player.
   [1*bt6ukj1isn8tmjnrqh-42g.png]

   say you have the game screen, and you want to tell a neural network
   what   s on the screen. one way would be to directly feed the image into
   the neural network; we don   t process the inputs in any other way. the
   other would be to create a summary of what   s happening on the screen in
   a numerical format, and then feed that into the neural network. the
   former is being referred to here as    high dimensional sensory inputs   
   and the latter is    hand crafted feature representations.   
   [1*jvu34f_goniqcn09nnh1ow.png]

   read abstract explanation. then this paragraph is self-explanatory.
   [1*9laplhioqbstdndhvgdlpw.png]

   deep learning methods don   t work easily with id23
   like they do in supervised/unsupervised learning. most dl applications
   have involved huge training datasets with accurate samples and labels.
   or in unsupervised learning, the target cost function is still quite
   convenient to work with.

   but in rl, there   s a catch         as you know, rl involves rewards which
   could be delayed many time steps into the future (for example it takes
   several moves to knock the opponent   s queen in chess, and each of those
   moves doesn   t return the same immediate reward as the final move, even
   if one of those moves might be more important than the final move).

   the rewards could also be noisy         for instance, sometimes the points
   for a particular move are slightly random and not easily predictable!

   moreover, in dl usually the input samples are assumed to be unrelated
   to each other. for example in an image recognition network, the
   training data will have huge numbers of randomly organized and
   unrelated images. but while learning how to play a game, usually the
   strategy of moves doesn   t just depend on the current state of the
   screen but also some previous states and moves. it   s not simple to
   assume no correlation.

   now, wait a second. why is it important for our training data samples
   to not be correlated to each other? say you had 5 samples of animal
   images, and you wanted to learn to classify them into    cat    and    not
   cat   . if one of those images is of a cat, does it affect the likelihood
   of another image also being a cat? no. but in a video game, one frame
   of the screen is definitely related to the next frame. and to the next
   frame. and so on. if it takes 10 frames for a laser beam to destroy
   your spaceship, i   m pretty sure the 9th frame is a pretty good
   indication that the 10th frame is going to be painful. you don   t want
   to treat the two frames just a few milliseconds apart as totally
   separate experiences while learning, because they obviously carry
   valuable information about each other. they   re part of the same
   experience         that of a laser beam striking your spaceship.

   even the training data itself keeps changing in nature as the robot
   learns new strategies, making it harder to train on. what does that
   mean? for example, say you   re a noob chess player. you start out with
   some noob strategies when you play your first chess game, i.e keep
   moving forward, kill the pawn on the first chance you get etc. so as
   you keep learning those behaviors and feel happy taking pawns, those
   moves act like your current training set.

   now one day you try out a different strategy         sacrificing one of your
   own bishops to save your queen and take the other   s rook. bam you
   realize this is so amazing. you   ve added this new trick to your
   training set, which you would never have learned if you had kept just
   practicing your previous noob strategy.

   this is what it means to have a non-stationary data distribution, which
   doesn   t really happen in supervised/unsupervised learning.

   so given these challenges, how do you even train the neural network in
   such a situation?
   [1*sqrbuitd7etrdzdavsetba.png]

   in this paper we show how we overcame the above mentioned problems and
   we directly used raw video/image data. which means we   re awesome.
   one specific trick that is worth mentioning:    experience replay   . this
   solves the challenge of    data correlation    and    non-stationary data
   distributions    (see previous paragraph to understand what these mean).

   we record all our experiences         again using the chess analogy, each
   experience looks like [current board status, move i tried, reward i
   got, new board status]         into a memory. then while training, we pick up
   randomly distributed batches of experiences which aren   t related to
   each other. in each batch, different experiences could be associated
   with different strategies as well         because all the previous
   experiences and strategies are now jumbled together! make sense?

   this makes the training data samples more random and un-correlated, and
   it also makes it feel more stationary to the neural network because
   every new batch is already full of random strategy experiences.
   [1*ymhzl1j1jmdzrr7ajjl_dw.png]

   much of it is self-explanatory. the key here is that the exact same
   neural network architecture and hyperparameters (learning rate, etc)
   were used for each different game. it   s not like we used a bigger
   network for space invaders and a smaller network for ping-pong. we did
   train the networks from scratch for each new game, but the network
   design itself was the same. that   s pretty awesome right?
   [1*b8rw56tn65z3r2ah_hzpkq.png]

   first couple sentences are self-explanatory. by saying that    e    is
   stochastic, it means that the environment is not always predictable
   (which is true in games right? anything could happen at any time).

   it also repeats that the neural network does not get any information
   about the internal state of the game. for example, we don   t tell it
   things like    there   s a monster at this position who is firing at you
   and moving in this direction, your spaceship is present here and moving
   there, etc   . we simply give it the image and let the convolutional
   network figure out by itself where the monster is, and where the player
   is, and who is shooting where etc. this is to make the robot train in a
   more human-like way.
   [1*jutmkob7e8zmhiqxj4a5na.png]

   perceptual aliasing: meaning that two different states/places can be
   perceived as the same. for example, in a building, it is nearly
   impossible to determine a location solely with the visual information,
   because all the corridors may look the same.

   perceptual aliasing is a problem. in atari games the state of the game
   doesn   t change so much in every millisecond, nor is a human being
   capable of making decisions in every millisecond. so when we take video
   input at 60 frames per second, and treat each frame as a separate
   state, then most of the states in our training data will look exactly
   the same! it   s better to keep a longer horizon for what a    state    looks
   like, which has, say, at least 4 to 5 frames (say). multiple
   consecutive frames also contain valuable information about each
   other         for example, a still photograph of two cars a foot away from
   each other is very ambiguous         is one car about to crash into the
   other? or are they about to move away from each other after coming this
   close? you don   t know. but if you take 4 frames from the video and see
   them one after the other, now you know how the cars are moving and can
   guess if they   re going to crash or not. we call this a sequence of
   consecutive frames, and use one sequence as a state.

   moreover, when a human moves the joystick it usually stays in the same
   position for several milliseconds, so that is incorporated into this
   state. the same action is continued in each of the frames. each
   sequence (which includes several frames and the same action between
   them) is an individual state, and this state still fulfils a markov
   decision process (mdp).

   if you   ve read up on rl, you   d know what mdps are and what they mean!
   mdps are the core assumption in rl.
   [1*lae10a_oikdrmehsoloe7g.png]

   now, to understand this part you should really do some background study
   into id23 and id24 first. it   s very important.
   you should understand what the bellman equation does, what discounted
   future rewards are etc. but let me try to give a really simple overview
   of id24.

   remember what i said about    value function    earlier? scroll up to the
   abstract and read it.

   now, let   s say you had a table which had a row for all the possible
   states (s) of the game, and the columns represented all the possible
   joystick moves (a). each cell in the row represents the maximum total
   future value possible if you take that particular action and play your
   best fro then on. this means you now have a    cheat sheet    of what to
   expect from any action at any state! the values of these cells is
   called the q-star value. (q*(s,a)). for any state s, if you take action
   a, the maximum total future value is q*(s,a) as seen in that table.

   in the last line, pi is    policy   . policy is simply the strategy about
   what action to pick when you are in a particular state.
   [1*bmqzzkhhdmrjfxg92ftgdg.png]

   now, if you think about it, say you are in state s1. you see the q*
   value for all possible actions in the table (explained in para3), and
   choose a1 because its q value is the highest. you get an immediate
   reward r1, and the game moves into a different state s2. for s2, the
   max future reward will be if it takes (say) action a2 in the table.

   now, the initial q value q*(s1,a1) is the max value you could get if
   you played optimally from then on, right? this means, that q*(s1, a1)
   should be equal to the sum of the reward r1 and the max future value of
   the next state q*(s2,a2)! does this make sense? but hey we want to
   reduce the influence of the next state, so we multiply it by a number
   gamma which is between 0 and 1. this is called discounting q*(s2,a2).

   therefore q*(s1,a1) = r1 + [gamma x q*(s2,a2)]
   [1*cnit8moospzlxhubjncy4w.png]
   i   ll admit, this one won   t be easy to grasp just by reading my
   explanation. put some time here.

   look at the previous equation again. we   re assuming that for any state,
   and for any future action, we *know* the optimal value function
   already, and can use it to pick the best action at the current state
   (because by iterating over all the possible q values we can literally
   look ahead into the future). but of course, such a q-function doesn   t
   really exist in the real world! the best we can do is *approximate* the
   q function by another function, and update that approx function little
   by little by testing it in the real world again and again. this approx
   function can be a simple linear polynomial, but we can even use
   non-linear functions. so we choose to use a neural network to be our
      approximate q function.   

   now you know why we   re reading this paper in the first place         deepmind
   uses a neural network to approximate a q function, and then they let
   the computer play atari games using the network to help predict the
   best moves. with time, as the computer gets a better and better idea of
   how the rewards work, it can tweak its neural network (by adjusting the
   weights), so that it becomes a better and better approximation of that
      real    q function! and by the time that approximation is good enough,
   voila we realize it can actually make better predictions than humans.
   [1*yghztlf1ekxeb3r3gzpltq.png]

   now, leaving aside some of the mathematical mumbo jumbo above (it   s
   hard for me too!). know that id24 is a *model-free* approach.
   when you say    model-free    rl, it means that your agent doesn   t need to
   explicitly learn the rules or physics of the game. in model-based rl,
   those rules and physics are defined in terms of a    transition matrix   
   which calculates the next state given a current state and action, and a
      reward function    which computes the reward given a current state and
   action. in our case these two things are too complex to calculate, and
   if you think about it, we don   t really need them! in our    model free   
   approach, we simply care about learning the q value function with hit
   and trial, because we assume that a good q function will inherently
   have to follow the rules and physics of the game.

   our approach is also *off-policy*, and not *on-policy*. the difference
   here is more subtle because in this paper they   ve followed a hybrid of
   sorts. assume you   re at state s and have several actions to choose
   from. we have an approximate q value function, so we calculate what
   will be the q value for each of those actions. now, while choosing the
   action, you have two options. the    common sense    option is to simply
   choose the action that has the highest q value right? yes, and this is
   what   s called a    greedy    strategy. you always pick the action which
   seems best to you *right now, given your current understanding of the
   game*         in other words, given your current approximate of the q
   function         which means, given your current strategy. but there lies the
   problem         when you start out, you don   t really have a good q function
   approximator right? and even if you have a somewhat good strategy, you
   still want your ai to check out other possible strategies and see where
   they lead. this is why a    greedy    strategy isn   t always the best when
   you   re learning. while learning, you don   t want to just keep trying
   what you believe will work         you want to try other things which seem
   less likely, just so you can get experience. and that   s the difference
   between on-policy (greedy) and off-policy (not greedy).

   why did i say we use a hybrid of sorts? because we vary the approach
   based on how much we   ve learned. we vary the id203 with which the
   agent will pick the greedy action. how do we vary that? we pick greedy
   actions with a id203 of (1-e), where e is a variable that
   represents how random the choice is. so e=1 means the choice is
   completely random, and e=0 means that we always pick the greedy action.
   makes sense? at first, when the network just begins learning, we pick e
   to be very close to 1, because we want the ai to explore as many
   strategies as possible. as time goes by and the ai learns more and
   more, we reduce the value of e towards 0 so that the ai stays on a
   particular strategy.
   [1*vqn4etgqtaewq0yvtim4xa.png]

   qiang: the backgammon game is the most popular game for scientists to
   test their various artificial intelligence and machine learning
   algorithms. reference [24] used a model-free algorithm to achieve a
   superhuman level of play. model-free means there is not a explicit
   equation between the algorithm   s input (screen images) and output (best
   play strategy found).

   id24, where    q    stands for    quality   , uses a q function which
   represents the maximum discounted future reward when we perform a
   certain action in a certain state. then the optimal policy (play
   strategy) is continually found from that point on. the difference in
   reference [24] is that they are using approximation for the q-value
   using a multi-layer id88 (mlp). in their mlp, one hidden layer
   exists between the output layer and the input layer.
   [1*c97rt7mqfchfnfmpegf8qw.png]

   qiang: the unsuccessful following applications of similar methods on
   other games made people don   t believe td-gammon approach. they owe the
   success of td-gammon on backgammon to the stochasticity of dice rolls.
   [1*rsdh7cuwilxhdogwdbbfbg.png]

   go a few paragraphs back, where we saw what kinds of functions can be
   used to approximate our theoretically perfect q-function. apparently,
   linear functions are better suited to the task than non-linear
   functions like neural networks, because they make the network easier to
      converge    (i.e the weights adjust in such a way that the networks
   makes more accurate, instead of becoming more random).
   [1*bufuydkx_mv3kvwb0yjqlg.png]

   qiang: in recent time, combining deep learning with reinforcement
   becomes research interest again. environment, value function, and
   policy have been estimated by deep learning algorithms. in the
   meantime, the divergence has been partially solved by gradient
   temporal-difference methods. however, as mentioned in the paper, these
   methods can only works with nonlinear function approximator not
   directly with nonlinear function.
   [1*zdmjcnxds7tv0xcltvzquq.png]

   qiang: nfq is the most similar prior work to the approach in this
   paper. main idea of nfq is using rprop (resilient id26) to
   update the parameters of the q-network to optimise the sequence of loss
   functions in equation 2. the disadvantages of nfq is that it introduced
   computational cost which is proportional to the size of data set
   because of batch update.

   this paper uses stochastic gradient updates which is computationally
   efficient. the nfq applied on simple tasks but not on the visual input,
   this paper   s algorithm learns end-to-end. another paper about
   id24 also used low-dimensional state instead of raw visual inputs
   which is the advantage of this paper.
   [1*micrjpbaylwpurgreihrhq.png]

   qiang: this paragraph introduced several applications of the atari 2600
   emulator. the first paper that used the atari 2600 emulator as a
   id23 platform applied standard id23
   algorithms with linear function approximation and generic visual
   features. larger number of features, and project features to
   lower-dimensional space improved the results. and the hyperneat
   evolutionary architecture evolved respectively a neural network for
   each game. and the neural network represents the play strategy, which
   were able to be trained and exploit some design flaws in some games.

   but as mentioned in the paper, this paper   s algorithm learns the
   strategy for seven atari 2600 games without adjustment of the
   architecture. this is the big advantage of the algorithm in this paper.
   [1*qhw9ge_b8g0beoq8dxrvba.png]

   self explanatory?
   [1*1vrgehbsirmo1uxsp7adpw.png]

   td gammon was an on-policy approach, and it directly used the
   experiences (s1, a1, r1, s2) to train the network (without experience
   replay etc).
   [1*oeurz0yamvqs64dtenwa4q.png]

   now we come to the specific improvements made over td gammon. the first
   of these is experience replay, which has already been talked about
   previously. the    phi    function does image preprocessing etc, so the
   state of the game is stored as the final preprocessed form (more on
   this in the next section).
   [1*gx4xejaphuvs-sfcj78voq.png]
   [1*zaawzz8yjysdfhu8k5wb_g.png]

   these are the concrete advantages of using experience replay (this
   paragraph continues on the next page). firstly, just like in regular
   deep learning, where each data sample can be reused multiple times to
   update weights, we can use the same experience multiple times while
   training. this is more efficient use of data.

   second and third are very related. because each state is very closely
   related to the next state (as it is while playing a video game), then
   training the weights with each consecutive state will lead to the
   program only following one single way of playing the game. you predict
   a move based on q function, you make that move, and update the weights
   so that the next time you will again likely move left. but by breaking
   this pattern and drawing randomly from past experiences, you can avoid
   these feedback loops.
   [1*rgxq8x-9wwaqkxyyyystaw.png]

   now, it   s good to draw random samples from experience replay, but
   sometimes in a game there are important transitions that you would like
   the agent to learn about. this is a limitation of the current approach
   in this paper. a suggestion given is to pick important transitions with
   a greater id203 while using experience replay. or something like
   that.

   (beyond this point, everything is based on the theory covered in the
   previous sections so a lot of it is just technical details)
   [1*14-arlnqc5pu9tr8-uy-2a.png]

   most of this is self-explanatory. the state s is preprocessed to
   include 4 consecutive frames, all preprocessed into grayscale and
   resized and cropped to 84x84 squares. i think this is because given
   that the game runs at over 24 frames per second, and humans can   t react
   so fast as to make a move in each single frame, it makes sense to
   consider 4 consecutive frames as being in the same state.
   [1*06iqirsgzn1ttmzvwe65ga.png]

   while making the network architecture, you can either make it a q
   function which takes both s1 and a1 and outputs q-value for this
   combination. but this means that you   d have to run this network for
   each of the 18 possible joystick actions at each step, and compare the
   output of all 18 runs. instead, you can simply have an architecture
   where you use s1 as input and have 18 outputs each corresponding to the
   q-value for a given joystick action. it   s much more efficient to
   compare the q-values in this way!
   [1*q1p6xgnj-vhnnq6mkx4w-q.png]

   self explanatory :)
   [1*4hketidzx3ki7q9oairbra.png]

   oooh. first half is self explanatory. the second half tells about one
   very important thing about this experiment: that the nature of rewards
   being input to the agent was modified. so, *any* positive reward was
   input as +1, negative rewards were input as -1, and no change was input
   as 0. this is of course very different from how real games
   work         rewards are always changing, and some accomplishments have
   higher rewards than others. but it   s impressive that in spite of this,
   the agent performed better than humans in some games!
   [1*mpk23nqcuny7rxciwlv6tg.png]

   we   ve already talked about e-greedy (in section 2), and experience
   replay. this is about the specific details of their implementation.
   [1*eky_9owclk_qn0ro4pgxlw.png]

   more detail about why they use a stack of 4 video frames instead of
   using a single frame for each state.
   [1*ho7tdfnanegjaqg6hfryrq.png]

   this is about the evaluation metric you use while training. usually in
   supervised learning you have something like validation accuracy, but
   here you don   t have any validation set etc to compare with. so what
   other things can we use to check whether our network is training
   towards a point or are the weights just dancing around? id48m, let   s
   think about that. the purpose of this whole paper is to create an ai
   agent which gets a high score on the game, so why not just use the
   total score as our evaluation metric? and we can play several games and
   get the average overall score. well, turns out that using this metric
   doesn   t work well in practice, as it happens to be very noisy.

   let   s think about some other metric? well, another thing we   re doing in
   this experiment is to find a    policy    which the ai will follow to
   ensure the highest score (it   s off policy learning, as explained
   previously). and the q-value at any particular moment represents the
   total reward expected by the ai in future. so if the ai finds a great
   policy, then the q-value for that policy will be higher, right? let   s
   see if we can use the q-value itself as our evaluation metric. and
   voila, it seems to be more stable than just the averaged total reward.
   now, as you can see there   s no theoretical explanation for this, and it
   was just an idea which happened to work. (actually that   s what happens
   in deep learning all the time. some things just work, and other things
   which seem common sense just don   t. another example of this is dropout,
   which is a batshit crazy technique but works amazingly).
   [1*xggbzwnencaq3wgffkvwlg.png]

   this should be self explanatory. it shows how the value function
   changes in different moves of the game.
   [1*ndrzkv19smwuciztn2saxa.png]

   here we compare the paper   s results with prior work in this field.
      sarsa    refers to [s1,a1,r,s2,a2]. it is an on-policy learning
   algorithm (as opposed to our off-policy) approach. it   s not easy to
   understand the difference so easily, [19]here   s a good one.

   and [20]another.

   the rest of this paragraph is quite easy to read.
   [1*obok_by9a85yv_mzcicqvw.png]

   for this paragraph and everything beyond       look and marvel at how much
   better their approach performs!   

   iframe: [21]/media/23564290a225786d2eabc0f1d24c6d9e?postid=9eb5f38c89ee

   i don   t use this to send you links or sell stuff. instead it   s a way
   for me to have discussions with you and learn from your stories and
   ideas.
     __________________________________________________________________

   oh and if you   re someone who needs help explaining your science or
   technology work to non-technical people for any reason (whether it   s
   marketing or education), i could really help you. drop me a message on
   twitter: @mngrwl

     * [22]artificial intelligence
     * [23]technology
     * [24]tech
     * [25]startup
     * [26]programming

   (button)
   (button)
   (button) 3.7k claps
   (button) (button) (button) 24 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of aman agarwal

[28]aman agarwal

   engineer, teacher, environmentalist. learner of foreign languages,
   lover of history, cinema and art.

     (button) follow
   [29]freecodecamp.org

[30]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 3.7k
     * (button)
     *
     *

   [31]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [32]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9eb5f38c89ee
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/explained-simply-how-deepmind-taught-ai-to-play-video-games-9eb5f38c89ee&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/explained-simply-how-deepmind-taught-ai-to-play-video-games-9eb5f38c89ee&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_1a4m2tkv14rw---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@mngrwl?source=post_header_lockup
  12. https://medium.freecodecamp.org/@mngrwl
  13. http://image source: http://truthorial.com/games/pacman-could-have-been-call-fuck-man
  14. https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/
  15. https://medium.freecodecamp.org/media/71f48cb2d54451e6a64ccd83b22c9aab?postid=9eb5f38c89ee
  16. http://aman-agarwal.com/
  17. https://www.linkedin.com/in/luqiang21/
  18. https://kknews.cc/sports/b48mjln.html
  19. https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-id24/
  20. https://stackoverflow.com/a/41420616
  21. https://medium.freecodecamp.org/media/23564290a225786d2eabc0f1d24c6d9e?postid=9eb5f38c89ee
  22. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  23. https://medium.freecodecamp.org/tagged/technology?source=post
  24. https://medium.freecodecamp.org/tagged/tech?source=post
  25. https://medium.freecodecamp.org/tagged/startup?source=post
  26. https://medium.freecodecamp.org/tagged/programming?source=post
  27. https://medium.freecodecamp.org/@mngrwl?source=footer_card
  28. https://medium.freecodecamp.org/@mngrwl
  29. https://medium.freecodecamp.org/?source=footer_card
  30. https://medium.freecodecamp.org/?source=footer_card
  31. https://medium.freecodecamp.org/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/9eb5f38c89ee/share/twitter
  35. https://medium.com/p/9eb5f38c89ee/share/facebook
  36. https://medium.com/p/9eb5f38c89ee/share/twitter
  37. https://medium.com/p/9eb5f38c89ee/share/facebook
