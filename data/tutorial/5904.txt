   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   24 september 2017 / [12]id72

id72 objectives for natural language processing

   id72 objectives for natural language processing

   this post discusses the most common auxiliary tasks used in multi-task
   learning in natural language processing.

   this post has two main parts: in the first part, i will talk about
   artificial tasks that can be used as auxiliary objectives for mtl. in
   the second part, i will focus on common nlp tasks and discuss which
   other nlp tasks have benefited them.

   in a [13]previous blog post, i discussed how id72 (mtl)
   can be used to improve the performance of a model by leveraging a
   related task. id72 consists of two main components: a)
   the architecture used for learning and b) the auxiliary task(s) that
   are trained jointly. both facets still have a lot of room for
   improvement. in addition, id72 has the potential to be a
   key technique on the path to more robust models that learn from limited
   data: training a model to acquire proficiency in performing a wide
   range of nlp tasks would allow us to induce representations, which
   should be useful for transferring knowledge to many other tasks, as
   outlined in [14]this blog post.

   on the way to this goal, we first need to learn more about the
   relationships between our tasks, what we can learn from each, and how
   to combine them most effectively. most of the existing theory in mtl
   has focused on homogeneous tasks, i.e. tasks that are variations of the
   same classification or regression problem, such as classifying
   individual mnist digits. these guarantees, however, do not hold for the
   heterogeneous tasks to which mtl is most often applied in natural
   language processing (nlp) and id161.

   there have been some recent studies looking into when multi-task
   learning between different nlp tasks works but we still do not
   understand very well which tasks are useful. to this end, as
   inspiration, i will give an overview in the following of different
   approaches for id72 for nlp. i will focus on the second
   component of id72; instead of discussing how a model is
   trained, as most architectures only differ in which layers they share,
   i will concentrate on the auxiliary tasks and objectives that are used
   for learning.

   this post has two main parts: in the first part, i will talk about
   artificial tasks that can be used as auxiliary objectives for mtl. in
   the second part, i will focus on common nlp tasks and discuss which
   other nlp tasks have benefited them.

artificial auxiliary objectives

   id72 is all about coming up with ways to add a suitable
   bias to your model. incorporating artificial auxiliary tasks that
   cleverly complement your target task is arguably one of the most
   ingenious and fun ways to do mtl. it is a feature-engineering of sorts:
   instead of engineering the features, you are engineering the auxiliary
   task you optimize. similarly to feature engineering, domain expertise
   is therefore required as we will see in the following:

   language modelling language modelling has been shown to be beneficial
   for many nlp tasks and can be incorporated in various ways. word
   embeddings pre-trained by id97 have been shown to beneficial -- as
   is known, id97 approximates the language modelling objective;
   languages models have been used to pre-train mt and
   sequence-to-sequence models ^[15][1]; contextual language model
   embeddings have also been found useful for many tasks ^[16][2]. in this
   context, we can also treat language modelling as an auxiliary task that
   is learned together with the main task. rei (2017) ^[17][3] shows that
   this improves performance on several sequence labelling tasks.

   conditioning the initial state   the initial state of a recurrent
   neural network is typically initialized to a \(0\) vector. according to
   a [18]lecture by hinton in 2013, it is beneficial to learn the initial
   state just like any other sets of weights. while a learned state will
   be more helpful than a \(0\) vector it will be independent of the
   sequence and thus unable to adapt. weng et al. (2017) ^[19][4] propose
   to add a suitable bias to the initial encoder and decoder states for
   id4 by training it to predict the words in the sentence. in this sense,
   this objective can essentially be seen as a language modelling
   objective for the initial state and might thus be helpful for other
   tasks. similarly, we can think of other task-specific biases that could
   be encoded in the initial state to aid learning: a sentiment model
   might benefit from knowing about the general audience response to a
   movie or whether a user is more likely to be sarcastic while a parser
   might be able to leverage prior knowledge of the domain's tree depth or
   complexity.

   adversarial loss   an auxiliary adversarial loss was first found to be
   useful for id20 ^[20][5], ^[21][6], where it is used to
   learn domain-invariant representations by rendering the model unable to
   distinguish between different domains. this is typically done by adding
   a gradient reversal layer that reverses the sign of the gradient during
   back-propagation, which in turn leads to a maximization rather than a
   minimization of the adversarial loss. it is not to be confused with
   adversarial examples ^[22][7], which significantly increase the model's
   loss typically via small perturbations to its input; adversarial
   training ^[23][8], which trains a model to correctly classify such
   examples; or id3, which are trained to
   generate some output representation. an adversarial loss can be added
   to many tasks in order to learn task-independent representations
   ^[24][9]. it can also be used to ignore certain features of the input
   that have been found to be detrimental to generalization, such as
   data-specific properties that are unlikely to generalize. finally, an
   adversarial auxiliary task might also help to combat bias and ensure
   more privacy by encouraging the model to learn representations, which
   do not contain information that would allow the reconstruction of
   sensitive user attributes.

   predicting data statistics   an auxiliary loss can also be to predict
   certain underlying statistics of the training data. in contrast to the
   adversarial loss, which tries to make the model oblivious to certain
   features, this auxiliary task explicitly encourages the model to
   predict certain data statistics. plank et al. (2016) ^[25][10] predict
   the log frequency of a word as an auxiliary task for language
   modelling. intuitively, this makes the representation predictive of
   frequency, which encourages the model to not share representations
   between common and rare words, which benefits the handling of rare
   tokens. another facet of this auxiliary task is to predict attributes
   of the user, such as their gender, which has been shown to be
   beneficial for predicting mental health conditions ^[26][11] or other
   demographic information ^[27][12]. we can think of other statistics
   that might be beneficial for a model to encode, such as the frequency
   of pos tags, parsing structures, or entities, the preferences of users,
   a sentence's coverage for summarization, or even a user's website usage
   patterns.

   learning the inverse   another auxiliary task that might be useful in
   many circumstances is to learn the inverse of the task together with
   the main task. a popular example of this framework is cyclegan
   ^[28][13], which can [29]generate photos from paintings. an inverse
   auxiliary loss, however, is applicable to many other tasks: mt might be
   the most intuitive, as every translation direction such as
   english->french directly provides data for the inverse direction, as
   xia et al. (2016) ^[30][14] demonstrate. xia et al. (2017) ^[31][15]
   show that this has applications not only to mt, but also to image
   classification (with image generation as its inverse) and sentiment
   classification (paired with sentence generation). for multimodal
   translation, elliott and k  d  r (2017) ^[32][16] jointly learn an
   inverse task by predicting image representations. it is not difficult
   to think of inverse complements for many other tasks: entailment has
   hypothesis generation; video captioning has video generation; speech
   recognition has id133, etc.

   predicting what should be there   for many tasks, where a model has to
   pick up on certain features of the training data, we can focus the
   model's attention on these characteristics by encouraging it explicitly
   to predict them. for id31, for instance, yu and jiang
   (2016) ^[33][17] predict whether the sentence contains a positive or
   negative domain-independent sentiment word, which sensitizes the model
   towards the sentiment of the words in the sentence. for name error
   detection, cheng et al. (2015) ^[34][18] predict if a sentence contains
   a name. we can envision similar auxiliary tasks that might be useful
   for other tasks: predicting whether certain entities occur in a
   sentence might be useful for id36; predicting whether a
   headline contains certain lurid terms might help for clickbait
   detection, while predicting whether an emotion word occurs in the
   sentence might benefit emotion detection. in summary, this auxiliary
   task should be helpful whenever a task includes certain highly
   predictive terms or features.

joint training of existing nlp tasks

   in this second section, we will now look at existing nlp tasks, which
   have been used to improve the performance of a main task. while certain
   tasks such as chunking and semantic tagging have been found to be
   useful for many tasks ^[35][19], the choice whether to use a particular
   auxiliary task largely depends on characteristics of the main task. in
   the following, i will thus highlight different strategies and rationals
   that were used to select auxiliary tasks for many common tasks in nlp:

   id103   recent id72 approaches for
   automatic id103 (asr) typically use additional supervision
   signals that are available in the id103 pipeline as
   auxiliary tasks to train an asr model end-to-end. phonetic recognition
   and frame-level state classification can be used as auxiliary tasks to
   induce helpful intermediate representations. toshniwal et al. (2017)
   ^[36][20] find that positioning the auxiliary loss at an intermediate
   layer improves performance. similarly, ar  k et al. (2017) ^[37][21]
   predict the phoneme duration and frequency profile as auxiliary tasks
   for id133.

   machine translation   the main benefit mtl has brought to machine
   translation (mt) is by jointly training translation models from and to
   different languages: dong et al. (2015) ^[38][22] jointly train the
   decoders; zoph and knight (2016) ^[39][23] jointly train the encoders,
   while johnson et al. (2016) ^[40][24] jointly train both encoders and
   decoders; malaviya et al. (2017) ^[41][25] train one model to translate
   from 1017 languages into english.

   other tasks have also shown to be useful for mt: luong et al. (2015)
   ^[42][26] show gains using parsing and image captioning as auxiliary
   tasks; niehues and cho (2017) ^[43][27] combine id4 with id52
   and ner; wu et al. (2017) ^[44][28] jointly model the target word
   sequence and its dependency tree structure.

   multilingual tasks   similarly to mt, it can often be beneficial to
   jointly train models for different languages: gains have been shown for
   id33 ^[45][29], ^[46][30], id39
   ^[47][31], part-of-speech tagging ^[48][32], document classification
   ^[49][33], discourse segmentation ^[50][34], and sequence tagging
   ^[51][35].

   language grounding   for grounding language in images or videos, it is
   often useful to enable the model to learn causal relationships in the
   data. for video captioning, pasunuru and bansal (2017) ^[52][36]
   jointly learn to predict the next frame in the video and to predict
   entailment, while hermann et al. (2017) ^[53][37] also predict the next
   frame in a video and the words that represent the visual state for
   language learning in a simulated environment.

   id29   for a task where multiple label sets or formalisms
   are available such as for id29, an interesting mtl strategy
   is to learn these formalisms together: to this end, guo et al. (2016)
   ^[54][38] jointly train on multi-typed treebanks; peng et al. (2017)
   ^[55][39] learn three semantic dependency graph formalisms
   simultaneously; fan et al. (2017) ^[56][40] jointly learn different
   alexa-based id29 formalisms; and zhao and huang (2017)
   ^[57][41] jointly train a syntactic and a discourse parser. for more
   shallow id29 such as frame-semantic argument
   identification, swayamdipta et al. (2017) ^[58][42] predict whether an
   id165 is syntactically meaningful, i.e. a syntactic constituent.

   representation learning   for learning general-purpose representations,
   the challenge often is in defining the objective. most existing
   representation learning models have been based on a single loss
   function, such as predicting the next word ^[59][43] or sentence
   ^[60][44] or training on a certain task such as entailment ^[61][45] or
   mt ^[62][46]. rather than learning representations based on a single
   loss, intuitively, representations should become more general as more
   tasks are used to learn them. as an example of this strategy, hashimoto
   et al. (2017) ^[63][47] jointly train a model on multiple nlp tasks,
   while jernite et al. (2017) ^[64][48] propose several discourse-based
   artificial auxiliary tasks for sentence representation learning.

   id53   for id53 (qa) and reading
   comprehension, it is beneficial to learn the different parts of a more
   complex end-to-end model together: choi et al. (2017) ^[65][49] jointly
   learn a sentence selection and answer generation model, while wang et
   al. (2017) ^[66][50] jointly train a ranking and reader model for
   open-domain qa.

   information retrieval   for id36, information related to
   different relations or roles can often be shared. to this end, jiang
   (2009) ^[67][51] jointly learn linear models between different relation
   types; yang and mitchell (2017) ^[68][52] jointly predict semantic role
   labels and relations; katiyar and cardie (2017) ^[69][53] jointly
   extract entities and relations; and liu et al. (2015) ^[70][54] jointly
   train domain classification and web search ranking.

   chunking   chunking has been shown to benefit from being jointly
   trained with low-level tasks such as id52 ^[71][55], ^[72][56],
   ^[73][57].

   miscellaneous   besides the tasks mentioned above, various other tasks
   have been shown to benefit from mtl: balikas and moura (2017) ^[74][58]
   jointly train coarse-grained and fine-grained id31; luo
   et al. (2017) ^[75][59] jointly predict charges and extract articles;
   augenstein and s  gaard (2017) ^[76][60] use several auxiliary tasks for
   keyphrase boundary detection; and isonuma et al. (2017) ^[77][61] pair
   sentence extraction with document classification.

conclusion

   i hope this blog post was able to provide you with some insight with
   regard to which strategies are employed to select auxiliary tasks and
   objectives for id72 in nlp. as i mentioned [78]before,
   id72 can be very broadly defined. i have tried to
   provide as broad of an overview as possible but i still likely have
   omitted many relevant approaches. if you are aware of an approach that
   provides a valuable perspective that is not represented here, please
   let me know in the comments below.
     __________________________________________________________________

    1. ramachandran, p., liu, p. j., & le, q. v. (2016). unsupervised
       pretrainig for sequence to sequence learning. arxiv preprint
       arxiv:1611.02683. [79]      
    2. peters, m. e., ammar, w., bhagavatula, c., & power, r. (2017).
       semi-supervised sequence tagging with bidirectional language
       models. in proceedings of the 55th annual meeting of the
       association for computational linguistics (pp. 1756   1765). [80]      
    3. rei, m. (2017). semi-supervised multitask learning for sequence
       labeling. in proceedings of acl 2017. [81]      
    4. weng, r., huang, s., zheng, z., dai, x., & chen, j. (2017). neural
       machine translation with word predictions. in proceedings of the
       2017 conference on empirical methods in natural language
       processing. [82]      
    5. ganin, y., & lempitsky, v. (2015). unsupervised id20
       by id26. in proceedings of the 32nd international
       conference on machine learning. (vol. 37). [83]      
    6. ganin, y., ustinova, e., ajakan, h., germain, p., larochelle, h.,
       laviolette, f.,     lempitsky, v. (2016). domain-adversarial training
       of neural networks. journal of machine learning research, 17, 1   35.
       [84]http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf
       [85]      
    7. szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d.,
       goodfellow, i., & fergus, r. (2014). intriguing properties of
       neural networks. in iclr 2014. retrieved from
       [86]http://arxiv.org/abs/1312.6199 [87]      
    8. miyato, t., dai, a. m., & goodfellow, i. (2016). virtual
       adversarial training for semi-supervised text classification.
       retrieved from [88]http://arxiv.org/abs/1605.07725 [89]      
    9. liu, p., qiu, x., & huang, x. (2017). adversarial multi-task
       learning for text classification. in acl 2017. retrieved from
       [90]http://arxiv.org/abs/1704.05742 [91]      
   10. plank, b., s  gaard, a., & goldberg, y. (2016). multilingual
       part-of-speech tagging with bidirectional long short-term memory
       models and auxiliary loss. in proceedings of the 54th annual
       meeting of the association for computational linguistics. [92]      
   11. benton, a., mitchell, m., & hovy, d. (2017). id72
       for mental health using social media text. in proceedings of the
       15th conference of the european chapter of the association for
       computational linguistics: volume 1, long papers. retrieved from
       [93]http://m-mitchell.com/publications/multitask-clinical.pdf
       [94]      
   12. roy, d. (2017). twitter demographic classification using deep
       multi-modal id72. in proceedings of the 55th annual
       meeting of the association for computational linguistics (pp.
       478   483). [95]      
   13. zhu, j., park, t., efros, a. a., ai, b., & berkeley, u. c. (2017).
       unpaired image-to-image translation using cycle-consistent
       adversarial networks. [96]      
   14. xia, y., he, d., qin, t., wang, l., yu, n., liu, t.-y., & ma, w.-y.
       (2016). dual learning for machine translation. in advances in
       neural information processing systems 29 (nips 2016) (pp. 1   9).
       retrieved from [97]http://arxiv.org/abs/1611.00179 [98]      
   15. xia, y., qin, t., chen, w., bian, j., yu, n., & liu, t. (2017).
       dual supervised learning. in icml. [99]      
   16. elliott, d., & k  d  r,   . (2017). imagination improves multimodal
       translation. retrieved from [100]http://arxiv.org/abs/1705.04350
       [101]      
   17. yu, j., & jiang, j. (2016). learning sentence embeddings with
       auxiliary tasks for cross-domain sentiment classification.
       proceedings of the 2016 conference on empirical methods in natural
       language processing (emnlp2016), 236   246. retrieved from
       [102]http://www.aclweb.org/anthology/d/d16/d16-1023.pdf [103]      
   18. cheng, h., fang, h., & ostendorf, m. (2015). open-domain name error
       detection using a multi-task id56. in proceedings of the 2015
       conference on empirical methods in natural language processing (pp.
       737   746). [104]      
   19. bingel, j., & s  gaard, a. (2017). identifying beneficial task
       relations for id72 in deep neural networks. in eacl.
       retrieved from [105]http://arxiv.org/abs/1702.08303 [106]      
   20. toshniwal, s., tang, h., lu, l., & livescu, k. (2017). multitask
       learning with low-level auxiliary tasks for encoder-decoder based
       id103. retrieved from
       [107]http://arxiv.org/abs/1704.01631 [108]      
   21. ar  k, s.   ., chrzanowski, m., coates, a., diamos, g., gibiansky,
       a., kang, y.,     shoeybi, m. (2017). deep voice: real-time neural
       text-to-speech. in icml 2017. [109]      
   22. dong, d., wu, h., he, w., yu, d., & wang, h. (2015). multi-task
       learning for multiple language translation. in proceedings of the
       53rd annual meeting of the association for computational
       linguistics and the 7th international joint conference on natural
       language processing (pp. 1723   1732). [110]      
   23. zoph, b., & knight, k. (2016). multi-source neural translation.
       naacl, 30   34. retrieved from [111]http://arxiv.org/abs/1601.00710
       [112]      
   24. johnson, m., schuster, m., le, q. v, krikun, m., wu, y., chen, z.,
           dean, j. (2016). google   s multilingual id4
       system: enabling zero-shot translation. arxiv preprint
       arxiv:1611.0455. [113]      
   25. malaviya, c., neubig, g., & littell, p. (2017). learning language
       representations for typology prediction. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       retrieved from [114]http://arxiv.org/abs/1707.09569 [115]      
   26. luong, m.-t., le, q. v., sutskever, i., vinyals, o., & kaiser, l.
       (2015). multi-task sequence to sequence learning. in arxiv preprint
       arxiv:1511.06114. retrieved from
       [116]http://arxiv.org/abs/1511.06114 [117]      
   27. niehues, j., & cho, e. (2017). exploiting linguistic resources for
       id4 using id72. in wmt 2017.
       retrieved from [118]http://arxiv.org/abs/1708.00993 [119]      
   28. wu, s., zhang, d., yang, n., li, m., & zhou, m. (2017).
       sequence-to-dependency id4. in proceedings
       of the 55th annual meeting of the association for computational
       linguistics (pp. 698   707). [120]      
   29. duong, l., cohn, t., bird, s., & cook, p. (2015). low resource
       id33: cross-lingual parameter sharing in a neural
       network parser. proceedings of the 53rd annual meeting of the
       association for computational linguistics and the 7th international
       joint conference on natural language processing (short papers),
       845   850. [121]      
   30. ammar, w., mulcaire, g., ballesteros, m., dyer, c., & smith, n. a.
       (2016). one parser, many languages. transactions of the association
       for computational linguistics, vol. 4, pp. 431   444, 2016, 4,
       431   444. retrieved from [122]http://arxiv.org/abs/1602.01595
       [123]      
   31. gillick, d., brunk, c., vinyals, o., & subramanya, a. (2016).
       multilingual language processing from bytes. naacl, 1296   1306.
       retrieved from [124]http://arxiv.org/abs/1512.00103 [125]      
   32. fang, m., & cohn, t. (2017). model transfer for tagging
       low-resource languages using a bilingual dictionary. in proceedings
       of the 55th annual meeting of the association for computational
       linguistics (acl 2017). [126]      
   33. nikolaos pappas and andrei popescu-belis (2017). multilingual
       hierarchical attention networks for document classification. in
       proceedings of the eighth international joint conference on natural
       language processing. retrieved from
       [127]https://arxiv.org/pdf/1707.00896.pdf [128]      
   34. braud, c., lacroix, o., & s  gaard, a. (2017). cross-lingual and
       cross-domain discourse segmentation of entire documents. in
       proceedings of the 55th annual meeting of the association for
       computational linguistics. [129]      
   35. yang, z., salakhutdinov, r., & cohen, w. (2016). multi-task
       cross-lingual sequence tagging from scratch. [130]      
   36. pasunuru, r., & bansal, m. (2017). multi-task video captioning with
       video and entailment generation. in proceedings of the 55th annual
       meeting of the association for computational linguistics (acl
       2017). [131]      
   37. hermann, k. m., hill, f., green, s., wang, f., faulkner, r., soyer,
       h.,     phil blunsom. (2017). grounded language learning in a
       simulated 3d world. retrieved from
       [132]https://arxiv.org/pdf/1706.06551.pdf [133]      
   38. guo, j., che, w., wang, h., & liu, t. (2016). exploiting
       multi-typed treebanks for parsing with deep id72.
       retrieved from [134]http://arxiv.org/abs/1606.01161 [135]      
   39. peng, h., thomson, s., smith, n. a., & allen, p. g. (2017). deep
       multitask learning for semantic id33. in acl 2017.
       retrieved from [136]https://arxiv.org/pdf/1704.06855.pdf [137]      
   40. fan, x., monti, e., mathias, l., & dreyer, m. (2017). transfer
       learning for neural id29. acl repl4nlp 2017. retrieved
       from [138]http://arxiv.org/abs/1706.04326 [139]      
   41. zhao, k., & huang, l. (2017). joint syntacto-discourse parsing and
       the syntacto-discourse treebank. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [140]      
   42. swayamdipta, s., thomson, s., dyer, c., & smith, n. a. (2017).
       frame-id29 with softmax-margin segmental id56s and a
       syntactic scaffold. retrieved from
       [141]http://arxiv.org/abs/1706.09528 [142]      
   43. mikolov, t., chen, k., corrado, g., & dean, j. (2013). distributed
       representations of words and phrases and their compositionality.
       nips. [143]      
   44. kiros, r., zhu, y., salakhutdinov, r., zemel, r. s., torralba, a.,
       urtasun, r., & fidler, s. (2015). skip-thought vectors, (786).
       retrieved from [144]http://arxiv.org/abs/1506.06726 [145]      
   45. conneau, a., kiela, d., schwenk, h., barrault, l., & bordes, a.
       (2017). supervised learning of universal sentence representations
       from natural language id136 data. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [146]      
   46. mccann, b., bradbury, j., xiong, c., & socher, r. (2017). learned
       in translation: contextualized word vectors. [147]      
   47. hashimoto, k., xiong, c., tsuruoka, y., & socher, r. (2017). a
       joint many-task model: growing a neural network for multiple nlp
       tasks. in proceedings of the 2017 conference on empirical methods
       in natural language processing. retrieved from
       [148]http://arxiv.org/abs/1611.01587 [149]      
   48. jernite, y., bowman, s. r., & sontag, d. (2017). discourse-based
       objectives for fast unsupervised sentence representation learning.
       retrieved from [150]http://arxiv.org/abs/1705.00557 [151]      
   49. choi, e., hewlett, d., uszkoreit, j., lacoste, a., & berant, j.
       (2017). coarse-to-fine id53 for long documents. in
       proceedings of the 55th annual meeting of the association for
       computational linguistics (pp. 209   220). [152]      
   50. wang, s., yu, m., guo, x., wang, z., klinger, t., & zhang, w.
       (2017). r^3: reinforced reader-ranker for open-domain question
       answering. [153]      
   51. jiang, j. (2009). multi-task id21 for
       weakly-supervised id36. proceedings of the 47th
       annual meeting of the acl and the 4th ijcnlp of the afnlp,
       (august), 1012   1020. [154]https://doi.org/10.3115/1690219.1690288
       [155]      
   52. yang, b., & mitchell, t. (2017). a joint sequential and relational
       model for frame-id29. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [156]      
   53. katiyar, a., & cardie, c. (2017). going out on a limb : joint
       extraction of entity mentions and relations without dependency
       trees. in proceedings of the 55th annual meeting of the association
       for computational linguistics (pp. 917   928). [157]      
   54. liu, x., gao, j., he, x., deng, l., duh, k., & wang, y.-y. (2015).
       representation learning using multi-task deep neural networks for
       semantic classification and information retrieval. naacl-2015,
       912   921. [158]      
   55. collobert, r., & weston, j. (2008). a unified architecture for
       natural language processing. proceedings of the 25th international
       conference on machine learning - icml    08, 20(1), 160   167.
       [159]https://doi.org/10.1145/1390156.1390177 [160]      
   56. s  gaard, a., & goldberg, y. (2016). deep id72 with
       low level tasks supervised at lower layers. proceedings of the 54th
       annual meeting of the association for computational linguistics,
       231   235. [161]      
   57. ruder, s., bingel, j., augenstein, i., & s  gaard, a. (2017). sluice
       networks: learning what to share between loosely related tasks.
       arxiv preprint arxiv:1705.08142. retrieved from
       [162]http://arxiv.org/abs/1705.08142 [163]      
   58. balikas, g., & moura, s. (2017). multitask learning for
       fine-grained twitter id31. in international acm sigir
       conference on research and development in information retrieval
       2017. [164]      
   59. luo, b., feng, y., xu, j., zhang, x., & zhao, d. (2017). learning
       to predict charges for criminal cases with legal basis. in
       proceedings of the 2017 conference on empirical methods in natural
       language processing. retrieved from
       [165]http://arxiv.org/abs/1707.09168 [166]      
   60. augenstein, i., & s  gaard, a. (2017). id72 of
       keyphrase boundary classification. in proceedings of the 55th
       annual meeting of the association for computational linguistics.
       retrieved from [167]http://arxiv.org/abs/1704.00514 [168]      
   61. isonuma, m., fujino, t., mori, j., matsuo, y., & sakata, i. (2017).
       extractive summarization using id72 with document
       classification. in proceedings of the 2017 conference on empirical
       methods in natural language processing (pp. 2091   2100). [169]      

   sebastian ruder

[170]sebastian ruder

   read [171]more posts by this author.
   [172]read more

       sebastian ruder    

[173]id72

     * [174]neural id21 for natural language processing (phd
       thesis)
     * [175]10 exciting ideas of 2018 in nlp
     * [176]a review of the neural history of natural language processing

   [177]see all 5 posts    

   [178]id27s in 2017: trends and future directions

   id27s

id27s in 2017: trends and future directions

   id27s are an integral part of current nlp models, but
   approaches that supersede the original id97 have not been proposed.
   this post focuses on the deficiencies of id27s and how recent
   approaches have tried to resolve them.

     * sebastian ruder
       [179]sebastian ruder

   [180]highlights of emnlp 2017: exciting datasets, return of the
   clusters, and more

   natural language processing

highlights of emnlp 2017: exciting datasets, return of the clusters, and more

   this post discusses highlights of the 2017 conference on empirical
   methods in natural language processing (emnlp 2017). these include
   exciting datasets, new cluster-based methods, distant supervision, data
   selection, character-level models, and many more.

     * sebastian ruder
       [181]sebastian ruder

   [182]sebastian ruder
      
   id72 objectives for natural language processing
   share this
   please enable javascript to view the [183]comments powered by disqus.

   [184]sebastian ruder    2019

   [185]latest posts [186]twitter [187]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/multi-task-learning/index.html
  13. http://ruder.io/multi-task/index.html
  14. http://ruder.io/transfer-learning/index.html
  15. http://ruder.io/multi-task-learning-nlp/index.html#fn1
  16. http://ruder.io/multi-task-learning-nlp/index.html#fn2
  17. http://ruder.io/multi-task-learning-nlp/index.html#fn3
  18. https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf
  19. http://ruder.io/multi-task-learning-nlp/index.html#fn4
  20. http://ruder.io/multi-task-learning-nlp/index.html#fn5
  21. http://ruder.io/multi-task-learning-nlp/index.html#fn6
  22. http://ruder.io/multi-task-learning-nlp/index.html#fn7
  23. http://ruder.io/multi-task-learning-nlp/index.html#fn8
  24. http://ruder.io/multi-task-learning-nlp/index.html#fn9
  25. http://ruder.io/multi-task-learning-nlp/index.html#fn10
  26. http://ruder.io/multi-task-learning-nlp/index.html#fn11
  27. http://ruder.io/multi-task-learning-nlp/index.html#fn12
  28. http://ruder.io/multi-task-learning-nlp/index.html#fn13
  29. https://github.com/junyanz/cyclegan
  30. http://ruder.io/multi-task-learning-nlp/index.html#fn14
  31. http://ruder.io/multi-task-learning-nlp/index.html#fn15
  32. http://ruder.io/multi-task-learning-nlp/index.html#fn16
  33. http://ruder.io/multi-task-learning-nlp/index.html#fn17
  34. http://ruder.io/multi-task-learning-nlp/index.html#fn18
  35. http://ruder.io/multi-task-learning-nlp/index.html#fn19
  36. http://ruder.io/multi-task-learning-nlp/index.html#fn20
  37. http://ruder.io/multi-task-learning-nlp/index.html#fn21
  38. http://ruder.io/multi-task-learning-nlp/index.html#fn22
  39. http://ruder.io/multi-task-learning-nlp/index.html#fn23
  40. http://ruder.io/multi-task-learning-nlp/index.html#fn24
  41. http://ruder.io/multi-task-learning-nlp/index.html#fn25
  42. http://ruder.io/multi-task-learning-nlp/index.html#fn26
  43. http://ruder.io/multi-task-learning-nlp/index.html#fn27
  44. http://ruder.io/multi-task-learning-nlp/index.html#fn28
  45. http://ruder.io/multi-task-learning-nlp/index.html#fn29
  46. http://ruder.io/multi-task-learning-nlp/index.html#fn30
  47. http://ruder.io/multi-task-learning-nlp/index.html#fn31
  48. http://ruder.io/multi-task-learning-nlp/index.html#fn32
  49. http://ruder.io/multi-task-learning-nlp/index.html#fn33
  50. http://ruder.io/multi-task-learning-nlp/index.html#fn34
  51. http://ruder.io/multi-task-learning-nlp/index.html#fn35
  52. http://ruder.io/multi-task-learning-nlp/index.html#fn36
  53. http://ruder.io/multi-task-learning-nlp/index.html#fn37
  54. http://ruder.io/multi-task-learning-nlp/index.html#fn38
  55. http://ruder.io/multi-task-learning-nlp/index.html#fn39
  56. http://ruder.io/multi-task-learning-nlp/index.html#fn40
  57. http://ruder.io/multi-task-learning-nlp/index.html#fn41
  58. http://ruder.io/multi-task-learning-nlp/index.html#fn42
  59. http://ruder.io/multi-task-learning-nlp/index.html#fn43
  60. http://ruder.io/multi-task-learning-nlp/index.html#fn44
  61. http://ruder.io/multi-task-learning-nlp/index.html#fn45
  62. http://ruder.io/multi-task-learning-nlp/index.html#fn46
  63. http://ruder.io/multi-task-learning-nlp/index.html#fn47
  64. http://ruder.io/multi-task-learning-nlp/index.html#fn48
  65. http://ruder.io/multi-task-learning-nlp/index.html#fn49
  66. http://ruder.io/multi-task-learning-nlp/index.html#fn50
  67. http://ruder.io/multi-task-learning-nlp/index.html#fn51
  68. http://ruder.io/multi-task-learning-nlp/index.html#fn52
  69. http://ruder.io/multi-task-learning-nlp/index.html#fn53
  70. http://ruder.io/multi-task-learning-nlp/index.html#fn54
  71. http://ruder.io/multi-task-learning-nlp/index.html#fn55
  72. http://ruder.io/multi-task-learning-nlp/index.html#fn56
  73. http://ruder.io/multi-task-learning-nlp/index.html#fn57
  74. http://ruder.io/multi-task-learning-nlp/index.html#fn58
  75. http://ruder.io/multi-task-learning-nlp/index.html#fn59
  76. http://ruder.io/multi-task-learning-nlp/index.html#fn60
  77. http://ruder.io/multi-task-learning-nlp/index.html#fn61
  78. http://ruder.io/multi-task/index.html
  79. http://ruder.io/multi-task-learning-nlp/index.html#fnref1
  80. http://ruder.io/multi-task-learning-nlp/index.html#fnref2
  81. http://ruder.io/multi-task-learning-nlp/index.html#fnref3
  82. http://ruder.io/multi-task-learning-nlp/index.html#fnref4
  83. http://ruder.io/multi-task-learning-nlp/index.html#fnref5
  84. http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf
  85. http://ruder.io/multi-task-learning-nlp/index.html#fnref6
  86. http://arxiv.org/abs/1312.6199
  87. http://ruder.io/multi-task-learning-nlp/index.html#fnref7
  88. http://arxiv.org/abs/1605.07725
  89. http://ruder.io/multi-task-learning-nlp/index.html#fnref8
  90. http://arxiv.org/abs/1704.05742
  91. http://ruder.io/multi-task-learning-nlp/index.html#fnref9
  92. http://ruder.io/multi-task-learning-nlp/index.html#fnref10
  93. http://m-mitchell.com/publications/multitask-clinical.pdf
  94. http://ruder.io/multi-task-learning-nlp/index.html#fnref11
  95. http://ruder.io/multi-task-learning-nlp/index.html#fnref12
  96. http://ruder.io/multi-task-learning-nlp/index.html#fnref13
  97. http://arxiv.org/abs/1611.00179
  98. http://ruder.io/multi-task-learning-nlp/index.html#fnref14
  99. http://ruder.io/multi-task-learning-nlp/index.html#fnref15
 100. http://arxiv.org/abs/1705.04350
 101. http://ruder.io/multi-task-learning-nlp/index.html#fnref16
 102. http://www.aclweb.org/anthology/d/d16/d16-1023.pdf
 103. http://ruder.io/multi-task-learning-nlp/index.html#fnref17
 104. http://ruder.io/multi-task-learning-nlp/index.html#fnref18
 105. http://arxiv.org/abs/1702.08303
 106. http://ruder.io/multi-task-learning-nlp/index.html#fnref19
 107. http://arxiv.org/abs/1704.01631
 108. http://ruder.io/multi-task-learning-nlp/index.html#fnref20
 109. http://ruder.io/multi-task-learning-nlp/index.html#fnref21
 110. http://ruder.io/multi-task-learning-nlp/index.html#fnref22
 111. http://arxiv.org/abs/1601.00710
 112. http://ruder.io/multi-task-learning-nlp/index.html#fnref23
 113. http://ruder.io/multi-task-learning-nlp/index.html#fnref24
 114. http://arxiv.org/abs/1707.09569
 115. http://ruder.io/multi-task-learning-nlp/index.html#fnref25
 116. http://arxiv.org/abs/1511.06114
 117. http://ruder.io/multi-task-learning-nlp/index.html#fnref26
 118. http://arxiv.org/abs/1708.00993
 119. http://ruder.io/multi-task-learning-nlp/index.html#fnref27
 120. http://ruder.io/multi-task-learning-nlp/index.html#fnref28
 121. http://ruder.io/multi-task-learning-nlp/index.html#fnref29
 122. http://arxiv.org/abs/1602.01595
 123. http://ruder.io/multi-task-learning-nlp/index.html#fnref30
 124. http://arxiv.org/abs/1512.00103
 125. http://ruder.io/multi-task-learning-nlp/index.html#fnref31
 126. http://ruder.io/multi-task-learning-nlp/index.html#fnref32
 127. https://arxiv.org/pdf/1707.00896.pdf
 128. http://ruder.io/multi-task-learning-nlp/index.html#fnref33
 129. http://ruder.io/multi-task-learning-nlp/index.html#fnref34
 130. http://ruder.io/multi-task-learning-nlp/index.html#fnref35
 131. http://ruder.io/multi-task-learning-nlp/index.html#fnref36
 132. https://arxiv.org/pdf/1706.06551.pdf
 133. http://ruder.io/multi-task-learning-nlp/index.html#fnref37
 134. http://arxiv.org/abs/1606.01161
 135. http://ruder.io/multi-task-learning-nlp/index.html#fnref38
 136. https://arxiv.org/pdf/1704.06855.pdf
 137. http://ruder.io/multi-task-learning-nlp/index.html#fnref39
 138. http://arxiv.org/abs/1706.04326
 139. http://ruder.io/multi-task-learning-nlp/index.html#fnref40
 140. http://ruder.io/multi-task-learning-nlp/index.html#fnref41
 141. http://arxiv.org/abs/1706.09528
 142. http://ruder.io/multi-task-learning-nlp/index.html#fnref42
 143. http://ruder.io/multi-task-learning-nlp/index.html#fnref43
 144. http://arxiv.org/abs/1506.06726
 145. http://ruder.io/multi-task-learning-nlp/index.html#fnref44
 146. http://ruder.io/multi-task-learning-nlp/index.html#fnref45
 147. http://ruder.io/multi-task-learning-nlp/index.html#fnref46
 148. http://arxiv.org/abs/1611.01587
 149. http://ruder.io/multi-task-learning-nlp/index.html#fnref47
 150. http://arxiv.org/abs/1705.00557
 151. http://ruder.io/multi-task-learning-nlp/index.html#fnref48
 152. http://ruder.io/multi-task-learning-nlp/index.html#fnref49
 153. http://ruder.io/multi-task-learning-nlp/index.html#fnref50
 154. https://doi.org/10.3115/1690219.1690288
 155. http://ruder.io/multi-task-learning-nlp/index.html#fnref51
 156. http://ruder.io/multi-task-learning-nlp/index.html#fnref52
 157. http://ruder.io/multi-task-learning-nlp/index.html#fnref53
 158. http://ruder.io/multi-task-learning-nlp/index.html#fnref54
 159. https://doi.org/10.1145/1390156.1390177
 160. http://ruder.io/multi-task-learning-nlp/index.html#fnref55
 161. http://ruder.io/multi-task-learning-nlp/index.html#fnref56
 162. http://arxiv.org/abs/1705.08142
 163. http://ruder.io/multi-task-learning-nlp/index.html#fnref57
 164. http://ruder.io/multi-task-learning-nlp/index.html#fnref58
 165. http://arxiv.org/abs/1707.09168
 166. http://ruder.io/multi-task-learning-nlp/index.html#fnref59
 167. http://arxiv.org/abs/1704.00514
 168. http://ruder.io/multi-task-learning-nlp/index.html#fnref60
 169. http://ruder.io/multi-task-learning-nlp/index.html#fnref61
 170. http://ruder.io/author/sebastian/index.html
 171. http://ruder.io/author/sebastian/index.html
 172. http://ruder.io/author/sebastian/index.html
 173. http://ruder.io/tag/multi-task-learning/index.html
 174. http://ruder.io/thesis/index.html
 175. http://ruder.io/10-exciting-ideas-of-2018-in-nlp/index.html
 176. http://ruder.io/a-review-of-the-recent-history-of-nlp/index.html
 177. http://ruder.io/tag/multi-task-learning/index.html
 178. http://ruder.io/index.html
 179. http://ruder.io/author/sebastian/index.html
 180. http://ruder.io/index.html
 181. http://ruder.io/author/sebastian/index.html
 182. http://ruder.io/
 183. https://disqus.com/?ref_noscript
 184. http://ruder.io/
 185. http://ruder.io/
 186. https://twitter.com/seb_ruder
 187. https://ghost.org/

   hidden links:
 189. https://twitter.com/seb_ruder
 190. http://ruder.io/rss/index.rss
 191. http://ruder.io/index.html
 192. http://ruder.io/index.html
 193. https://twitter.com/share?text=multi-task%20learning%20objectives%20for%20natural%20language%20processing&url=http://ruder.io/multi-task-learning-nlp/
 194. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/multi-task-learning-nlp/
