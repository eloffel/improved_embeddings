   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                   visualizing convolutional neural networks

   building convnets from scratch with tensorflow and tensorboard.

   by [27]justin francis

   september 15, 2017

   filter filter (source: [28]pixabay)

   attention readers: we invite you to [29]access the corresponding python
   code and ipython notebooks for this article on github.

   given all of the higher level tools that you can use with tensorflow,
   such as [30]tf.contrib.learn and [31]keras, one can very easily build a
   convolutional neural network with a very small amount of code. but
   often with these higher level applications, you cannot access the
   little inbetween bits of the code, and some of the understanding of
   what   s happening under the surface is lost.

   in this tutorial, i   ll walk you through how to build a convolutional
   neural network from scratch, using just the low-level tensorflow and
   visualizing our graph and network performance using tensorboard. if you
   don't understand some of the basics of a fully connected neural
   network, i highly recommend you first check out [32]not another mnist
   tutorial with tensorflow. throughout this article, i will also break
   down each step of the convolutional neural network to its absolute
   basics so you can fully understand what is happening in each step of
   the graph. by building this model from scratch, you can easily
   visualize different aspects of the graph so that you can see each layer
   of convolutions and use them to make your own id136s. i will only
   highlight major aspects of the code, so if you would like to follow
   this code step-by-step, you can checkout the [33]corresponding jupyter
   notebook on github.

gathering a data set

   getting started, i had to decide which image data set to use. i decided
   to use the university of oxford, visual geometry group   s [34]pet data
   set. i chose this data set for a few reasons: it is very simple and
   well-labeled, it has a decent amount of training data, and it also has
   bounding boxes   to utilize if i want to train a detection model down the
   road. another data set i thought would be excellent for a building a
   first model was the [35]simpsons data set found on kaggle, which has a
   great amount of simple data on which to train.

choosing a model

   next, i had to decide on the model of my convolutional neural network.
   some very popular models are [36]googlenet or [37]vgg16, which both
   have multiple convolutions designed to detect images from the 1000
   class data set id163. i decided on a much simpler four convolutional
   network:
   four convolutional network figure 1. image courtesy of justin francis.

   to break down this model, it starts with a 224x224x3 image, which is is
   convolved to 32 feature maps, based from the previous three channels.
   we than convolve this group of 32 feature maps together into another 32
   features. this is then pooled into a 112x112x32 image, which we
   convolve into 64 feature maps twice followed, with a final pooling of
   56x56x64. each unit of this final pooled layer is then fully connected
   to 512 neurons, and then finally put through a softmax layer based upon
   the number of classes.

processing and building a data set

   first, let   s get started with loading our dependencies, which includes
   a group of helper functions i made called imfunctions for processing
   the image data.
import imfunctions as imf
import tensorflow as tf
import scipy.ndimage
from scipy.misc import imsave
import matplotlib.pyplot as plt
import numpy as np

   we can then download and extract the images using imfunctions.
imf.downloadimages('annotations.tar.gz', 19173078)
imf.downloadimages('images.tar.gz', 791918971)
imf.maybeextract('annotations.tar.gz')
imf.maybeextract('images.tar.gz')

   we can then sort the images into separate folders, including training
   and test folders. the number in the sortimages function represents the
   percentage of test data you would like to separate from the training
   data.
imf.sortimages(0.15)

   we can then build our data set into a numpy array with a corresponding
   one hot vector to represent our class. this will also subtract the
   image mean from all of the training and test imagesa standard practice
   when building a convnet. the function will ask you what classes you
   would like to include   due to my limited amount of gpu ram (3gb), i
   choose a very small data set that tries to differentiate two breeds of
   dogs: a [38]shiba inu from a [39]samoyed.
train_x, train_y, test_x, test_y, classes, classlabels = imf.builddataset()

how convolutions and pooling work

   now that we have a data set to work with, let   s step back a bit and
   look at the absolute basics of how convolutions work. before jumping
   into a color convolutional filter, let's look at a grayscale one to
   make sure everything is clear. let's make a 7x7 filter that applies
   four different feature maps. tensorflow   s conv2d function is fairly
   simple and takes in four variables: input, filter, strides, and
   padding. on the [40]tensorflow site, they describe the conv2d function
   as follows:

     computes a 2-d convolution given 4-d input and filter tensors.

     given an input tensor of shape [batch, in_height, in_width,
     in_channels] and a filter / kernel tensor of shape [filter_height,
     filter_width, in_channels, out_channels].

   since we are working with grayscale, the in_channels will be 1, and
   since we are applying four filters our out_channels will be 4. let   s
   apply the following four filters/kernels to just one of our images, or
   a batch of 1:
   visualized convolutional filters 1 figure 2. image courtesy of justin
   francis.

   and let   s see how this filter affected our grayscaled image input.
gray = np.mean(image,-1)
x = tf.placeholder(tf.float32, shape=(none, 224, 224, 1))
conv = tf.nn.conv2d(x, filters, [1,1,1,1], padding="same")
test = tf.session()
test.run(tf.global_variables_initializer())
filteredimage = test.run(conv, feed_dict={x: gray.reshape(1,224,224,1)})
tf.reset_default_graph()

   this will return a 4d tensor of (1, 224, 224, 4), which we can use to
   visualize the four filters:
   visualized convolutional filters 2 figure 3. image courtesy of justin
   francis.

   it's clear to see that convolutions of filter kernels are very
   powerful. to break it down, our 7x7 kernel is striding in steps of one
   over 49 of the image   s pixels at a time, the value of each pixel is
   then multiplied by each kernel value, and then all of the 49 values are
   added together to make one pixel. if the idea of image filter kernels
   is still not making sense, i highly recommend this [41]website   they do
   an excellent job of visualizing kernels.

   now, in essence, most convolutional neural networks consist of just
   convolutions and poolings. most commonly, a 3x3 kernel filter is used
   for convolutions. particularly, max poolings with a stride of 2x2 and
   kernel size of 2x2 are just an aggressive way to essentially reduce an
   image   s size based upon its maximum pixel values within a kernel. here
   is a basic example of a 2x2 kernel with a stride of 2 in both
   dimensions.
   a 2x2 kernel with a stride of 2 figure 4. image courtesy of justin
   francis.

   now, for both conv2d and max pooling, there are two options to choose
   from for padding:    valid,    which will shrink an input and    same,    which
   will maintain the inputs size by adding zeros around the edges of the
   input. here is an example of a max pool with a 3x3 kernel, with a
   stride of 1x1 to compare the padding options:
   max pool with a 3x3 kernel, with a stride of 1x1 figure 5. image
   courtesy of justin francis.

building the convnet

   now that we   ve got the basics covered, let   s start building our
   convolutional neural network model. we can start with our placeholders.
   x will be our input placeholder, which we will feed our images into,
   and y_ will be the true classes of a set of images.
x = tf.placeholder(tf.float32, shape=(none, 224, 224, 3))
y_ = tf.placeholder(tf.float32, [none, classes])
keeprate1 = tf.placeholder(tf.float32)
keeprate2 = tf.placeholder(tf.float32)

   we will create all parts for each process under one scope. scope   s are
   extremely useful for visualizing the graph in tensorboard down the road
   because they will group everything into one expandable object. we
   create our first set of filters with a kernel size of 3x3, which takes
   in three channels and outputs 32 filters. this means that for every one
   of the 32 filters, there will be 3x3 kernel weights for the r, g, and b
   channels. it   s very important that the weight values for our filter are
   initialized using truncated normal so we have multiple random filters
   that tensorflow will adapt to fit our model.
# convolution 1 - 1
with tf.name_scope('conv1_1'):
    filter1_1 = tf.variable(tf.truncated_normal([3, 3, 3, 32], dtype=tf.float32,
                            stddev=1e-1), name='weights1_1')
    stride = [1,1,1,1]
    conv = tf.nn.conv2d(x, filter1_1, stride, padding='same')
    biases = tf.variable(tf.constant(0.0, shape=[32], dtype=tf.float32),
                         trainable=true, name='biases1_1')
    out = tf.nn.bias_add(conv, biases)
    conv1_1 = tf.nn.relu(out)

   at the end of our first convolution, conv1_1, we finish off by applying
   [42]relu, which acts as a threshold by assigning every negative number
   to zero. we then convolve those 32 features together into another 32
   features. you can see conv2d assigns the input to be the output of the
   first convolutional layer.
# convolution 1 - 2
with tf.name_scope('conv1_2'):
    filter1_2 = tf.variable(tf.truncated_normal([3, 3, 32, 32], dtype=tf.float32
,
                                                stddev=1e-1), name='weights1_2')
    conv = tf.nn.conv2d(conv1_1, filter1_2, [1,1,1,1], padding='same')
    biases = tf.variable(tf.constant(0.0, shape=[32], dtype=tf.float32),
                         trainable=true, name='biases1_2')
    out = tf.nn.bias_add(conv, biases)
    conv1_2 = tf.nn.relu(out)

   we then pool to shrink the image in half.
# pool 1
with tf.name_scope('pool1'):
    pool1_1 = tf.nn.max_pool(conv1_2,
                             ksize=[1, 2, 2, 1],
                             strides=[1, 2, 2, 1],
                             padding='same',
                             name='pool1_1')
    pool1_1_drop = tf.nn.dropout(pool1_1, keeprate1)

   this last part involves using dropout on the pool layer (we will go
   into more detail on that later). we then follow with two more
   convolutions, with 64 features and another pool. notice that the first
   convolution has to convert the previous 32 feature channels into 64.
# convolution 2 - 1
with tf.name_scope('conv2_1'):
    filter2_1 = tf.variable(tf.truncated_normal([3, 3, 32, 64], dtype=tf.float32
,
                                                stddev=1e-1), name='weights2_1')
    conv = tf.nn.conv2d(pool1_1_drop, filter2_1, [1, 1, 1, 1], padding='same')
    biases = tf.variable(tf.constant(0.0, shape=[64], dtype=tf.float32),
                         trainable=true, name='biases2_1')
    out = tf.nn.bias_add(conv, biases)
    conv2_1 = tf.nn.relu(out)

# convolution 2 - 2
with tf.name_scope('conv2_2'):
    filter2_2 = tf.variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32
,
                                                stddev=1e-1), name='weights2_2')
    conv = tf.nn.conv2d(conv2_1, filter2_2, [1, 1, 1, 1], padding='same')
    biases = tf.variable(tf.constant(0.0, shape=[64], dtype=tf.float32),
                         trainable=true, name='biases2_2')
    out = tf.nn.bias_add(conv, biases)
    conv2_2 = tf.nn.relu(out)

# pool 2
with tf.name_scope('pool2'):
    pool2_1 = tf.nn.max_pool(conv2_2,
                             ksize=[1, 2, 2, 1],
                             strides=[1, 2, 2, 1],
                             padding='same',
                             name='pool2_1')
    pool2_1_drop = tf.nn.dropout(pool2_1, keeprate1)

   next, we create a 512 neuron, fully connected layer, which will have a
   weight connection for each pixel of our 56x56x64 pool2_1 layer. that   s
   more than 100 million different weight values! in order to calculate
   our fully connected network, we have to flatten the input into one
   dimension, and then we can multiply it by our weights and add our bias.
#fully connected 1
with tf.name_scope('fc1') as scope:
    shape = int(np.prod(pool2_1_drop.get_shape()[1:]))
    fc1w = tf.variable(tf.truncated_normal([shape, 512], dtype=tf.float32,
                                           stddev=1e-1), name='weights3_1')
    fc1b = tf.variable(tf.constant(1.0, shape=[512], dtype=tf.float32),
                       trainable=true, name='biases3_1')
    pool2_flat = tf.reshape(pool2_1_drop, [-1, shape])
    out = tf.nn.bias_add(tf.matmul(pool2_flat, fc1w), fc1b)
    fc1 = tf.nn.relu(out)
    fc1_drop = tf.nn.dropout(fc1, keeprate2)

   last, we have our softmax with its associated weights and bias, and
   finally our output y.
#fully connected 3 & softmax output
with tf.name_scope('softmax') as scope:
    fc2w = tf.variable(tf.truncated_normal([512, classes], dtype=tf.float32,
                                           stddev=1e-1), name='weights3_2')
    fc2b = tf.variable(tf.constant(1.0, shape=[classes], dtype=tf.float32),
                       trainable=true, name='biases3_2')
    ylogits = tf.nn.bias_add(tf.matmul(fc1_drop, fc2w), fc2b)
    y = tf.nn.softmax(ylogits)

create loss & optimizer

   now, we can start developing the training aspect of our model. first,
   we have to decide the batch size; i couldn   t use more than 10 without
   running out of gpu memory. then we have to decide the number of epochs,
   which is the number of times the algorithm will cycle through all the
   training data in batches, and lastly our learning rate alpha.
numepochs = 400
batchsize = 10
alpha = 1e-5

   we then create some scopes for our cross id178, accuracy checker, and
   back propagation optimizer.
with tf.name_scope('cross_id178'):
    cross_id178 = tf.nn.softmax_cross_id178_with_logits(logits=ylogits, labe
ls=y_)
    loss = tf.reduce_mean(cross_id178)

with tf.name_scope('accuracy'):
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.name_scope('train'):
    train_step = tf.train.adamoptimizer(learning_rate=alpha).minimize(loss)

   we can then create our session and initialize all our variables.
sess = tf.session()
init = tf.global_variables_initializer()
sess.run(init)

create summaries for tensorboard

   now, we also will want to utilize tensorboard so we can visualize how
   well our classifier is doing. we will create two plots: one for our
   training set and one for our test set. we can visualize our graph
   network by using the add_graph function. we will measure our total loss
   and accuracy using summary scalar, and merge our summaries together so
   we only have to call write_op to log our scalars.
writer_1 = tf.summary.filewriter("/tmp/id98/train")
writer_2 = tf.summary.filewriter("/tmp/id98/test")
writer_1.add_graph(sess.graph)
tf.summary.scalar('loss', loss)
tf.summary.scalar('accuracy', accuracy)
tf.summary.histogram("weights1_1", filter1_1)
write_op = tf.summary.merge_all()

train the model

   we then can set up our code for evaluation and training. we don   t want
   to use our summary writer for our loss and accuracy for every time
   step, as this would greatly slow down the classifier. so instead, we
   log every five steps.
steps = int(train_x.shape[0]/batchsize)

for i in range(numepochs):
    acchist = []
    acchist2 = []
    train_x, train_y = imf.shuffle(train_x, train_y)
    for ii in range(steps):
        #calculate our current step
        step = i * steps + ii
        #feed forward batch of train images into graph and log accuracy
        acc = sess.run([accuracy], feed_dict={x: train_x[(ii*batchsize):((ii+1)*
batchsize),:,:,:], y_: train_y[(ii*batchsize):((ii+1)*batchsize)], keeprate1: 1,
 keeprate2: 1})
        acchist.append(acc)

        if step % 5 == 0:
            # get train summary for one batch and add summary to tensorboard
            summary = sess.run(write_op, feed_dict={x: train_x[(ii*batchsize):((
ii+1)*batchsize),:,:,:], y_: train_y[(ii*batchsize):((ii+1)*batchsize)], keeprat
e1: 1, keeprate2: 1})
            writer_1.add_summary(summary, step)
            writer_1.flush()

            # get test summary on random 10 test images and add summary to tenso
rboard
            test_x, test_y = imf.shuffle(test_x, test_y)
            summary = sess.run(write_op, feed_dict={x: test_x[0:10,:,:,:], y_: t
est_y[0:10], keeprate1: 1, keeprate2: 1})
            writer_2.add_summary(summary, step)
            writer_2.flush()

        #back propigate using adam optimizer to update weights and biases.
        sess.run(train_step, feed_dict={x: train_x[(ii*batchsize):((ii+1)*batchs
ize),:,:,:], y_: train_y[(ii*batchsize):((ii+1)*batchsize)], keeprate1: 0.2, kee
prate2: 0.5})

    print('epoch number {} training accuracy: {}'.format(i+1, np.mean(acchist)))

    #feed forward all test images into graph and log accuracy
    for iii in range(int(test_x.shape[0]/batchsize)):
        acc = sess.run(accuracy, feed_dict={x: test_x[(iii*batchsize):((iii+1)*b
atchsize),:,:,:], y_: test_y[(iii*batchsize):((iii+1)*batchsize)], keeprate1: 1,
 keeprate2: 1})
        acchist2.append(acc)
    print("test set accuracy: {}".format(np.mean(acchist2)))

visualize graph

   while this is training, let   s check out the tensorboard results by
   activating tensorboard in the terminal.
tensorboard --logdir="/tmp/id98/"

   we can then direct our web browser to the default tensorboard address
   [43]http://0.0.0.0/6006. let   s first look at our graph model.

   as you can see, by using scopes we are able to visualize a nice clean
   version of our graph.
   visualized graph model figure 6. image courtesy of justin francis.

measure performance

   let   s check out the scalar history for our accuracy and loss.
   training data accuracy and loss figure 7. image courtesy of justin
   francis.

   you may be able to tell that we have a huge problem. for our training
   data, the classifier is getting 100% accuracy and 0 loss, but our test
   data is only achieving 80% at best and still getting a lot of loss.
   this is your obvious sign of overfitting   some classic symptoms include
   not enough training data, or having too many neurons.

   we could create more training data by resizing, scaling, and rotating
   our training data, but a much easier approach is to add dropout to the
   output of our pooling and fully connected layers. this will make every
   training step completely cut, or drop out, a percentage of neurons
   randomly, in a layer. this will force our classifier to only train
   small sets of neurons at a time, rather than the whole set. this allows
   neurons to specialize in specific tasks, rather than all neurons
   generalizing together. dropping out 80% of our convolutional layers and
   50% of our fully connected layers gives some amazing results.
   adding dropout to the training figure 8. image courtesy of justin
   francis

   just by dropping off neurons, we were able to achieve just under 90% on
   our test data   that is almost a 10% increase in performance! one
   drawback is that the classifier took about 6x longer to train.

visualize evolving filters

   for extra fun, for every 50 training steps, i passed an image through a
   filter and made a gif of the filters    weights evolving. it resulted in
   some pretty cool effects and some really good insight on how the
   convolutional network was working. here are two filters from conv1_2:
   visualize evolving filters 1 figure 9. image courtesy of justin
   francis.

   you can see the initial weight initialization shows a lot of the image,
   but as the weights updated over time, they became more focused on
   detecting certain edges. to my surprise, i discovered that the very
   first convolutional kernel, filter1_1, hardly changed at all. it seemed
   that the beginning weight initializations did good enough on their own.
   going further down the network, here is conv2_2   you can see it
   beginning to detect more abstract generalized features.
   visualize evolving filters 2 figure 10. image courtesy of justin
   francis.

   all in all, i was extremely impressed that i was able to train a model
   with almost 90% accuracy using less than 400 training images. i   m sure
   with more training data, and more tweaking of the hyperparameters, i
   could have achieved even better results.

   this concludes how to create a convolutional neural network from
   scratch using tensorflow, and how to gain id136s from tensorboard
   and by visualizing our filters. it   s important to remember that a much
   easier method when making a classifier with little data is to take a
   model and weights that have already been trained on a huge data set
   with multiple gpus, such as [44]googlenet or [45]vgg16, and cut off the
   very last layer and replace them with their own classes. then, all the
   classifier has to do is learn the weights for the very last layer and
   use the pre-existing trained filter weights. so, i hope you got
   something out of this post and go forth, have fun, experiment, learn,
   and visualize!

   this post is a collaboration between o'reilly
   and [46]tensorflow. [47]see our statement of editorial independence.
   article image: filter (source: [48]pixabay).
   tags: [49]all about tensorflow

   share
    1. [50]tweet
    2.
    3.
     __________________________________________________________________

   [51]justin francis

[52]justin francis

   justin francis is currently an undergraduate student at the university
   of alberta in canada. justin is also on the software team for the
   university's engineering club 'autonomous robotic vehicle project'
   (arvp.org) helping implement and experiment with deep learning and
   id23 algorithms.  in the past, he was the founder and
   educator at a non-profit community cooperative bicycle shop and spent
   two years exploring and experiencing the georgia strait on his
   sailboat.
   [53]more
     __________________________________________________________________

   [54]bots landscape.

   [55]ai

[56]infographic: the bot platform ecosystem

   by [57]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [58]vertical forest, milan.

   [59]ai

[60]evolve ai

   by [61]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [62]welcome sign at o'reilly ai conference 2016

   [63]ai

[64]highlights from the o'reilly ai conference in new york 2016

   by [65]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [66]close up of uber's self driving car in pittsburgh.

   [67]ai

[68]how ai is propelling driverless cars, the future of surface transport

   by [69]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [70]our company
     * [71]teach/speak/write
     * [72]careers
     * [73]customer service
     * [74]contact us

site map

     * [75]ideas
     * [76]learning
     * [77]topics
     * [78]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [79]terms of service     [80]privacy policy     [81]editorial independence

   animal

   iframe: [82]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/justin-francis
  28. https://pixabay.com/en/filter-photo-effect-glass-407151/
  29. https://github.com/wagonhelm/visualizing-convnets/blob/master/visualizingconvnets.ipynb
  30. https://www.tensorflow.org/get_started/tflearn
  31. https://keras.io/
  32. https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow
  33. https://github.com/wagonhelm/visualizing-convnets/blob/master/visualizingconvnets.ipynb
  34. http://www.robots.ox.ac.uk/~vgg/data/pets/
  35. https://www.kaggle.com/alexattia/the-simpsons-characters-dataset
  36. http://www.cs.unc.edu/~wliu/papers/googlenet.pdf
  37. https://arxiv.org/pdf/1409.1556.pdf
  38. https://en.wikipedia.org/wiki/shiba_inu
  39. https://en.wikipedia.org/wiki/samoyed_(dog)
  40. https://www.tensorflow.org/
  41. http://setosa.io/ev/image-kernels/
  42. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  43. http://0.0.0.0/6006
  44. http://www.cs.unc.edu/~wliu/papers/googlenet.pdf
  45. https://arxiv.org/pdf/1409.1556.pdf
  46. https://www.tensorflow.org/
  47. http://www.oreilly.com/about/editorial_independence.html
  48. https://pixabay.com/en/filter-photo-effect-glass-407151/
  49. https://www.oreilly.com/tags/all-about-tensorflow
  50. https://twitter.com/share
  51. https://www.oreilly.com/people/justin-francis
  52. https://www.oreilly.com/people/justin-francis
  53. https://www.oreilly.com/people/justin-francis
  54. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  55. https://www.oreilly.com/topics/ai
  56. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  57. https://www.oreilly.com/people/b1d73-jon-bruner
  58. https://www.oreilly.com/ideas/evolve-ai
  59. https://www.oreilly.com/topics/ai
  60. https://www.oreilly.com/ideas/evolve-ai
  61. https://www.oreilly.com/people/14d38-naveen-rao
  62. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  63. https://www.oreilly.com/topics/ai
  64. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  65. https://www.oreilly.com/people/0d2c1-mac-slocum
  66. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  67. https://www.oreilly.com/topics/ai
  68. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  69. https://www.oreilly.com/people/c7521-shahin-farshchi
  70. http://oreilly.com/about/
  71. http://oreilly.com/work-with-us.html
  72. http://oreilly.com/careers/
  73. http://shop.oreilly.com/category/customer-service.do
  74. http://shop.oreilly.com/category/customer-service.do
  75. https://www.oreilly.com/ideas
  76. https://www.oreilly.com/topics/oreilly-learning
  77. https://www.oreilly.com/topics
  78. https://www.oreilly.com/all
  79. http://oreilly.com/terms/
  80. http://oreilly.com/privacy.html
  81. http://www.oreilly.com/about/editorial_independence.html
  82. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  84. https://www.oreilly.com/
  85. https://www.facebook.com/oreilly/
  86. https://twitter.com/oreillymedia
  87. https://www.youtube.com/user/oreillymedia
  88. https://plus.google.com/+oreillymedia
  89. https://www.linkedin.com/company/oreilly-media
  90. https://www.oreilly.com/
