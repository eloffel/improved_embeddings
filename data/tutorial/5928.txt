   #[1]github [2]recent commits to lm-lstm-crf:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]37
     * [35]star [36]578
     * [37]fork [38]166

[39]liyuanlucasliu/[40]lm-lstm-crf

   [41]code [42]issues 11 [43]pull requests 2 [44]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [45]sign up
   empower sequence labeling with task-aware language model
   [46]http://arxiv.org/abs/1709.04109
   [47]ner [48]language-model [49]crf [50]sequence-labeling [51]pytorch
     * [52]77 commits
     * [53]6 branches
     * [54]1 release
     * [55]fetching contributors
     * [56]apache-2.0

    1. [57]python 100.0%

   (button) python
   branch: master (button) new pull request
   [58]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/l
   [59]download zip

downloading...

   want to be notified of new releases in liyuanlucasliu/lm-lstm-crf?
   [60]sign in [61]sign up

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [64]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [65]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [66]permalink
   type         name          latest commit message  commit time
        failed to load latest commit information.
        [67]docs
        [68]model            [69]update predictor.py oct 6, 2018
        [70].gitignore
        [71]license
        [72]readme.md
        [73]eval_w.py
        [74]eval_wc.py
        [75]requirements.txt
        [76]seq_w.py
        [77]seq_wc.py
        [78]train_w.py
        [79]train_wc.py

readme.md

lm-lstm-crf

   [80]documentation status [81]license [82]insight.io

   check our new ner toolkit            
     * id136:
          + [83]lightner: id136 w. models pre-trained / trained w. any
            following tools, efficiently.
     * training:
          + [84]ld-net: train ner models w. efficient contextualized
            representations.
          + [85]vanillaner: train vanilla ner models w. pre-trained
            embedding.
     * distant training:
          + [86]autoner: train ner models w.o. line-by-line annotations
            and get competitive performance.
     __________________________________________________________________

   this project provides high-performance character-aware sequence
   labeling tools, including [87]training, [88]evaluation and
   [89]prediction.

   details about lm-lstm-crf can be accessed [90]here, and the
   implementation is based on the pytorch library.

   important: a serious bug was found on the bioes_to_span function in the
   original implementation, please refer the numbers reported in the
   [91]benchmarks section as the accurate performance.

   the documents would be available [92]here.

quick links

     * [93]model
     * [94]installation
     * [95]data
     * [96]usage
     * [97]benchmarks
     * [98]pretrained model

model notes

                             [99][framework.png]

   as visualized above, we use conditional random field (crf) to capture
   label dependencies, and adopt a hierarchical lstm to leverage both
   char-level and word-level inputs. the char-level structure is further
   guided by a language model, while pre-trained id27s are
   leveraged in word-level. the language model and the sequence labeling
   model are trained at the same time, and both make predictions at
   word-level. id199 are used to transform the output of
   char-level lstm into different semantic spaces, and thus mediating
   these two tasks and allowing language model to empower sequence
   labeling.

installation

   for training, a gpu is strongly recommended for speed. cpu is supported
   but training could be extremely slow.

pytorch

   the code is based on pytorch and supports pytorch 0.4 now . you can
   find installation instructions [100]here.

dependencies

   the code is written in python 3.6. its dependencies are summarized in
   the file requirements.txt. you can install these dependencies like
   this:
pip3 install -r requirements.txt

data

   we mainly focus on the conll 2003 ner dataset, and the code takes its
   original format as input. however, due to the license issue, we are
   restricted to distribute this dataset. you should be able to get it
   [101]here. you may also want to search online (e.g., github), someone
   might release it accidentally.

format

   we assume the corpus is formatted as same as the conll 2003 ner
   dataset. more specifically, empty lines are used as separators between
   sentences, and the separator between documents is a special line as
   below.
-docstart- -x- -x- -x- o

   other lines contains words, labels and other fields. word must be the
   first field, label mush be the last, and these fields are separated by
   space. for example, the first several lines in the wsj portion of the
   ptb id52 corpus should be like the following snippet.
-docstart- -x- -x- -x- o

pierre nnp
vinken nnp
, ,
61 cd
years nns
old jj
, ,
will md
join vb
the dt
board nn
as in
a dt
nonexecutive jj
director nn
nov. nnp
29 cd
. .



usage

   here we provide implementations for two models, one is lm-lstm-crf and
   the other is its variant, lstm-crf, which only contains the word-level
   structure and crf. train_wc.py and eval_wc.py are scripts for
   lm-lstm-crf, while train_w.py and eval_w.py are scripts for lstm-crf.
   the usages of these scripts can be accessed by the parameter -h, i.e.,
python train_wc.py -h
python train_w.py -h
python eval_wc.py -h
python eval_w.py -h

   the default running commands for ner and id52, and np chunking
   are:
     * id39 (ner):

python train_wc.py --train_file ./data/ner/train.txt --dev_file ./data/ner/testa
.txt --test_file ./data/ner/testb.txt --checkpoint ./checkpoint/ner_ --caseless
--fine_tune --high_way --co_train --least_iters 100

     * part-of-speech (pos) tagging:

python train_wc.py --train_file ./data/pos/train.txt --dev_file ./data/pos/testa
.txt --test_file ./data/pos/testb.txt --eva_matrix a --checkpoint ./checkpoint/p
os_ --caseless --fine_tune --high_way --co_train

     * noun phrase (np) chunking:

python train_wc.py --train_file ./data/np/train.txt.iobes --dev_file ./data/np/t
esta.txt.iobes --test_file ./data/np/testb.txt.iobes --checkpoint ./checkpoint/n
p_ --caseless --fine_tune --high_way --co_train --least_iters 100

   for other datasets or tasks, you may wanna try different stopping
   parameters, especially, for smaller dataset, you may want to set
   least_iters to a larger value; and for some tasks, if the speed of loss
   decreasing is too slow, you may want to increase lr.

benchmarks

   here we compare lm-lstm-crf with recent state-of-the-art models on the
   conll 2000 chunking dataset, the conll 2003 ner dataset, and the wsj
   portion of the ptb id52 dataset. all experiments are conducted
   on a gtx 1080 gpu.

   a serious bug was found on the bioes_to_span function in the original
   implementation, please refer the following numbers as the accurate
   performance.

ner

   when models are only trained on the wsj portion of the ptb id52
   dataset, the results are summarized as below.
      model    max(acc) mean(acc) std(acc) time(h)
   lm-lstm-crf 91.35    91.24     0.12     4
   -- highway  90.87    90.79     0.07     4
   -- co-train 91.23    90.95     0.34     2

pos

   when models are only trained on the wsj portion of the ptb id52
   dataset, the results are summarized as below.
            model          max(acc) mean(acc) std(acc) reported(acc) time(h)
   [102]lample et al. 2016 97.51    97.35     0.09     n/a           37
   [103]ma et al. 2016     97.46    97.42     0.04     97.55         21
   lm-lstm-crf             97.59    97.53     0.03                   16

pretrained model

evaluation

   we released pre-trained models on these three tasks. the checkpoint
   file can be downloaded at the following links. notice that the ner
   model and chunking model (coming soon) are trained on both the training
   set and the development set:
   wsj-ptb id52 conll03 ner
   [104]args           [105]args
   [106]model          [107]model

   also, eval_wc.py is provided to load and run these checkpoints. its
   usage can be accessed by command python eval_wc.py -h, and a running
   command example is provided below:
python eval_wc.py --load_arg checkpoint/ner/ner_4_cwlm_lstm_crf.json --load_chec
k_point checkpoint/ner_ner_4_cwlm_lstm_crf.model --gpu 0 --dev_file ./data/ner/t
esta.txt --test_file ./data/ner/testb.txt

prediction

   to annotated raw text, seq_wc.py is provided to annotate un-annotated
   text. its usage can be accessed by command python seq_wc.py -h, and a
   running command example is provided below:
python seq_wc.py --load_arg checkpoint/ner/ner_4_cwlm_lstm_crf.json --load_check
_point checkpoint/ner_ner_4_cwlm_lstm_crf.model --gpu 0 --input_file ./data/ner2
003/test.txt --output_file output.txt

   the input format is similar to conll, but each line is required to only
   contain one field, token. for example, an input file could be:
-docstart-

but
china
saw
their
luck
desert
them
in
the
second
match
of
the
group
,
crashing
to
a
surprise
2-0
defeat
to
newcomers
uzbekistan
.

   and the corresponding output is:
-docstart- -docstart- -docstart-

but <loc> china </loc> saw their luck desert them in the second match of the gro
up , crashing to a surprise 2-0 defeat to newcomers <loc> uzbekistan </loc> .


reference

@inproceedings{2017arxiv170904109l,
  title = "{empower sequence labeling with task-aware neural language model}",
  author = {{liu}, l. and {shang}, j. and {xu}, f. and {ren}, x. and {gui}, h. a
nd {peng}, j. and {han}, j.},
  booktitle={aaai},
  year = 2018,
}

     *    2019 github, inc.
     * [108]terms
     * [109]privacy
     * [110]security
     * [111]status
     * [112]help

     * [113]contact github
     * [114]pricing
     * [115]api
     * [116]training
     * [117]blog
     * [118]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [119]reload to refresh your
   session. you signed out in another tab or window. [120]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/liyuanlucasliu/lm-lstm-crf/commits/master.atom
   3. https://github.com/liyuanlucasliu/lm-lstm-crf#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/liyuanlucasliu/lm-lstm-crf
  32. https://github.com/join
  33. https://github.com/login?return_to=/liyuanlucasliu/lm-lstm-crf
  34. https://github.com/liyuanlucasliu/lm-lstm-crf/watchers
  35. https://github.com/login?return_to=/liyuanlucasliu/lm-lstm-crf
  36. https://github.com/liyuanlucasliu/lm-lstm-crf/stargazers
  37. https://github.com/login?return_to=/liyuanlucasliu/lm-lstm-crf
  38. https://github.com/liyuanlucasliu/lm-lstm-crf/network/members
  39. https://github.com/liyuanlucasliu
  40. https://github.com/liyuanlucasliu/lm-lstm-crf
  41. https://github.com/liyuanlucasliu/lm-lstm-crf
  42. https://github.com/liyuanlucasliu/lm-lstm-crf/issues
  43. https://github.com/liyuanlucasliu/lm-lstm-crf/pulls
  44. https://github.com/liyuanlucasliu/lm-lstm-crf/pulse
  45. https://github.com/join?source=prompt-code
  46. http://arxiv.org/abs/1709.04109
  47. https://github.com/topics/ner
  48. https://github.com/topics/language-model
  49. https://github.com/topics/crf
  50. https://github.com/topics/sequence-labeling
  51. https://github.com/topics/pytorch
  52. https://github.com/liyuanlucasliu/lm-lstm-crf/commits/master
  53. https://github.com/liyuanlucasliu/lm-lstm-crf/branches
  54. https://github.com/liyuanlucasliu/lm-lstm-crf/releases
  55. https://github.com/liyuanlucasliu/lm-lstm-crf/graphs/contributors
  56. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/license
  57. https://github.com/liyuanlucasliu/lm-lstm-crf/search?l=python
  58. https://github.com/liyuanlucasliu/lm-lstm-crf/find/master
  59. https://github.com/liyuanlucasliu/lm-lstm-crf/archive/master.zip
  60. https://github.com/login?return_to=https://github.com/liyuanlucasliu/lm-lstm-crf
  61. https://github.com/join?return_to=/liyuanlucasliu/lm-lstm-crf
  62. https://desktop.github.com/
  63. https://desktop.github.com/
  64. https://developer.apple.com/xcode/
  65. https://visualstudio.github.com/
  66. https://github.com/liyuanlucasliu/lm-lstm-crf/tree/e468974ce2193a5579417f9e253eb6c997932636
  67. https://github.com/liyuanlucasliu/lm-lstm-crf/tree/master/docs
  68. https://github.com/liyuanlucasliu/lm-lstm-crf/tree/master/model
  69. https://github.com/liyuanlucasliu/lm-lstm-crf/commit/bfc32169e02b48e2fc557d6f7a65ed32d19fe5c7
  70. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/.gitignore
  71. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/license
  72. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/readme.md
  73. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/eval_w.py
  74. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/eval_wc.py
  75. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/requirements.txt
  76. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/seq_w.py
  77. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/seq_wc.py
  78. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/train_w.py
  79. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/train_wc.py
  80. http://lm-lstm-crf.readthedocs.io/en/latest/?badge=latest
  81. https://opensource.org/licenses/apache-2.0
  82. https://insight.io/github.com/liyuanlucasliu/lm-lstm-crf
  83. https://github.com/liyuanlucasliu/lightner
  84. https://github.com/liyuanlucasliu/ld-net
  85. https://github.com/liyuanlucasliu/vanilla_ner
  86. https://shangjingbo1226.github.io/autoner/
  87. https://github.com/liyuanlucasliu/lm-lstm-crf#usage
  88. https://github.com/liyuanlucasliu/lm-lstm-crf#evaluation
  89. https://github.com/liyuanlucasliu/lm-lstm-crf#prediction
  90. http://arxiv.org/abs/1709.04109
  91. https://github.com/liyuanlucasliu/lm-lstm-crf#benchmarks
  92. http://lm-lstm-crf.readthedocs.io/en/latest/
  93. https://github.com/liyuanlucasliu/lm-lstm-crf#model-notes
  94. https://github.com/liyuanlucasliu/lm-lstm-crf#installation
  95. https://github.com/liyuanlucasliu/lm-lstm-crf#data
  96. https://github.com/liyuanlucasliu/lm-lstm-crf#usage
  97. https://github.com/liyuanlucasliu/lm-lstm-crf#benchmarks
  98. https://github.com/liyuanlucasliu/lm-lstm-crf#pretrained-model
  99. https://github.com/liyuanlucasliu/lm-lstm-crf/blob/master/docs/framework.png
 100. http://pytorch.org/
 101. http://aclweb.org/anthology/w03-0419
 102. https://github.com/glample/tagger
 103. https://github.com/xuezhemax/lasagnenlp
 104. https://drive.google.com/file/d/0b587sdkqutqmympinfp6b1hkwee/view?usp=sharing
 105. https://drive.google.com/file/d/1tgaq0hu9asibdrqfn5fmdq72pk1i-o74/view?usp=sharing
 106. https://drive.google.com/file/d/0b587sdkqutqmnnr3nnk1whdimg8/view?usp=sharing
 107. https://drive.google.com/file/d/1o9kjzv5echahys3gpgl7epge5fuxyyjr/view?usp=sharing
 108. https://github.com/site/terms
 109. https://github.com/site/privacy
 110. https://github.com/security
 111. https://githubstatus.com/
 112. https://help.github.com/
 113. https://github.com/contact
 114. https://github.com/pricing
 115. https://developer.github.com/
 116. https://training.github.com/
 117. https://github.blog/
 118. https://github.com/about
 119. https://github.com/liyuanlucasliu/lm-lstm-crf
 120. https://github.com/liyuanlucasliu/lm-lstm-crf

   hidden links:
 122. https://github.com/
 123. https://github.com/liyuanlucasliu/lm-lstm-crf
 124. https://github.com/liyuanlucasliu/lm-lstm-crf
 125. https://github.com/liyuanlucasliu/lm-lstm-crf
 126. https://help.github.com/articles/which-remote-url-should-i-use
 127. https://github.com/liyuanlucasliu/lm-lstm-crf#lm-lstm-crf
 128. https://github.com/liyuanlucasliu/lm-lstm-crf#quick-links
 129. https://github.com/liyuanlucasliu/lm-lstm-crf#model-notes
 130. https://github.com/liyuanlucasliu/lm-lstm-crf#installation
 131. https://github.com/liyuanlucasliu/lm-lstm-crf#pytorch
 132. https://github.com/liyuanlucasliu/lm-lstm-crf#dependencies
 133. https://github.com/liyuanlucasliu/lm-lstm-crf#data
 134. https://github.com/liyuanlucasliu/lm-lstm-crf#format
 135. https://github.com/liyuanlucasliu/lm-lstm-crf#usage
 136. https://github.com/liyuanlucasliu/lm-lstm-crf#benchmarks
 137. https://github.com/liyuanlucasliu/lm-lstm-crf#ner
 138. https://github.com/liyuanlucasliu/lm-lstm-crf#pos
 139. https://github.com/liyuanlucasliu/lm-lstm-crf#pretrained-model
 140. https://github.com/liyuanlucasliu/lm-lstm-crf#evaluation
 141. https://github.com/liyuanlucasliu/lm-lstm-crf#prediction
 142. https://github.com/liyuanlucasliu/lm-lstm-crf#reference
 143. https://github.com/
