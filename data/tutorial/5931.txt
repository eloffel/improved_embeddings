   #[1]github [2]recent commits to relation-classification:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]9
     * [35]star [36]91
     * [37]fork [38]27

[39]sshanu/[40]relation-classification

   [41]code [42]issues 5 [43]pull requests 0 [44]projects 0 [45]insights
   branch: master
   (button) create new file [46]find file [47]history
   [48]relation-classification/lstm seq and tree/
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [49]permalink
   type           name           latest commit message commit time
        [50]..
        failed to load latest commit information.
        [51]readme.md
        [52]model3v1.ipynb       [53]update            dec 4, 2017
        [54]model3v2.ipynb
        [55]path_extractor.ipynb

readme.md

relation classification using lstms on sequences and tree structures

   we implemented a architecture based on the paper [56]end-to-end
   id36 using lstms on sequences and tree structures. this
   recurrent neural network based model captures both word sequence and
   dependency tree substructure information by stacking bidirectional
   treestructured lstm-id56s on bidirectional sequential lstm-id56s. this
   allows our model to jointly represent both entities and relations with
   shared parameters in a single model.

   our model allows joint modeling of entities and relations in a single
   model by using both bidirectional sequential (left-to-right and
   right-to-left) and bidirectional tree-structured (bottom-up and
   top-down) lstmid56s.

model

   the model mainly consists of three representation layers: a embeddings
   layer, a word sequence based lstm-id56 layer (sequence layer), and
   finally a dependency subtree based lstm-id56 layer (dependency layer).

   [57]relation classification network

embedding layer

   embedding layer consists of words, part-of-speech (pos) tags,
   dependency relations.

sequence layer

   the sequence layer represents words in a linear sequence using the
   representations from the embedding layer. we represent the word
   sequence in a sentence with bidirectional lstm-id56s. the lstm unit at
   t-th word receives the concatenation of word and pos embeddings as its
   input vector.

                             [58][lstm_seq.jpg]

   we also concatenate the hidden state vectors of the two directions   
   lstm units corresponding to each word (denoted as    ht and    ht) as its
   output vector (st), and pass it to the subsequent layers.

entity detection

   we perform entity detection on top of the sequence layer. we employ a
   two-layered nn with an hidden layer and a softmax output layer for
   entity detection.

dependency layer

   the dependency layer represents a relation between a pair of two target
   words (corresponding to a relation candidate in relation
   classification) in the dependency tree.

   this layer mainly focuses on the shortest path between a pair of target
   words in the dependency tree (i.e., the path between the least common
   node and the two target words).

   we employ bidirectional tree-structured lstmid56s (i.e., bottom-up and
   top-down) to represent a relation candidate by capturing the dependency
   structure around the target word pair. this bidirectional structure
   propagates to each node not only the information from the leaves but
   also information from the root. this is especially important for
   relation classification, which makes use of argument nodes near the
   bottom of the tree, and our top-down lstm-id56 sends information from
   the top of the tree to such near-leaf nodes (unlike in standard
   bottom-up lstm-id56s).

   tree-structured lstm-id56's equations :

                           [59][lstm_tree_eq.jpg]

   while we use one node from shortest dependency path, then the hidden
   and current states of the children of this node in dependency tree are
   taken as previous state in lstm.

   we stack the dependency layers (corresponding to relation candidates)
   on top of the sequence layer to incorporate both word sequence and
   dependency tree structure information into the output. the
   dependency-layer lsmt unit at the t-th word recives as input, the
   concatenation of its corresponding hidden state vectors st in the
   sequence layer, dependency type embedding.

relation classification

   the relation candidate vector is constructed as the concatenation dp =
   [   hpa;    hp1;    hp2], where    hpa is the hidden state vector of the top
   lstm unit in the bottom-up lstm-id56 (representing the lowest common
   ancestor of the target word pair p), and    hp1,    hp2 are the hidden
   state vectors of the two lstm units representing the first and second
   target words in the top-down lstmid56.

   similarly to the entity detection, we employ a two-layered nn with an
   hidden layer and a softmax output layer.

training

   we update the model parameters including weights, biases, and
   embeddings by bptt and adam id119 with gradient clipping,
   l2-id173 (we regularize weights w and u, not the bias terms
   b). we also apply dropout to the embedding layer and to the final
   hidden layers for entity detection and relation classification. we
   employ entity pretraining to improve the model.

data

   semeval-2010 task 8 defines 9 relation types between nominals and a
   tenth type other when two nouns have none of these relations and no
   direction is considered.

experiments

    model   train-accuracy test-accuracy epochs
   model3v1 97.54          66.5          11
   model3v2 99.9           70.69         19
     * learning rate = 0.001
     * learning rate decay = 0.96
     * state size = 100
     * lambda_l2 = 0.0001
     * gradient clipping = 10
     * entity detection pretrained

[60]model3v1

     * bidirectional lstm over whole sentence
     * bottom-up and top-down lstm along shortest dependency path with
       childrens from dependency tree.

[61]model3v2

     * dropout on hidden layers of both entity detection and relation
       classifier of 0.3.

     *    2019 github, inc.
     * [62]terms
     * [63]privacy
     * [64]security
     * [65]status
     * [66]help

     * [67]contact github
     * [68]pricing
     * [69]api
     * [70]training
     * [71]blog
     * [72]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [73]reload to refresh your
   session. you signed out in another tab or window. [74]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/sshanu/relation-classification/commits/master.atom
   3. https://github.com/sshanu/relation-classification/tree/master/lstm seq and tree#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree
  32. https://github.com/join
  33. https://github.com/login?return_to=/sshanu/relation-classification
  34. https://github.com/sshanu/relation-classification/watchers
  35. https://github.com/login?return_to=/sshanu/relation-classification
  36. https://github.com/sshanu/relation-classification/stargazers
  37. https://github.com/login?return_to=/sshanu/relation-classification
  38. https://github.com/sshanu/relation-classification/network/members
  39. https://github.com/sshanu
  40. https://github.com/sshanu/relation-classification
  41. https://github.com/sshanu/relation-classification
  42. https://github.com/sshanu/relation-classification/issues
  43. https://github.com/sshanu/relation-classification/pulls
  44. https://github.com/sshanu/relation-classification/projects
  45. https://github.com/sshanu/relation-classification/pulse
  46. https://github.com/sshanu/relation-classification/find/master
  47. https://github.com/sshanu/relation-classification/commits/master/lstm seq and tree
  48. https://github.com/sshanu/relation-classification
  49. https://github.com/sshanu/relation-classification/tree/580cff2d0727405e415259aef5a69cc79e90385e/lstm seq and tree
  50. https://github.com/sshanu/relation-classification
  51. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/readme.md
  52. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/model3v1.ipynb
  53. https://github.com/sshanu/relation-classification/commit/e53e43fcbe94399617efc031b1e6299851e8f83c
  54. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/model3v2.ipynb
  55. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/path_extractor.ipynb
  56. http://www.aclweb.org/anthology/p/p16/p16-1105.pdf
  57. https://github.com/sshanu/relation-classification/blob/master/img/lstm_tree.jpg
  58. https://github.com/sshanu/relation-classification/blob/master/img/lstm_seq.jpg
  59. https://github.com/sshanu/relation-classification/blob/master/img/lstm_tree_eq.jpg
  60. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/model3v1.ipynb
  61. https://github.com/sshanu/relation-classification/blob/master/lstm seq and tree/model3v2.ipynb
  62. https://github.com/site/terms
  63. https://github.com/site/privacy
  64. https://github.com/security
  65. https://githubstatus.com/
  66. https://help.github.com/
  67. https://github.com/contact
  68. https://github.com/pricing
  69. https://developer.github.com/
  70. https://training.github.com/
  71. https://github.blog/
  72. https://github.com/about
  73. https://github.com/sshanu/relation-classification/tree/master/lstm seq and tree
  74. https://github.com/sshanu/relation-classification/tree/master/lstm seq and tree

   hidden links:
  76. https://github.com/
  77. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree
  78. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree
  79. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree
  80. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#relation-classification-using-lstms-on-sequences-and-tree-structures
  81. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#model
  82. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#embedding-layer
  83. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#sequence-layer
  84. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#entity-detection
  85. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#dependency-layer
  86. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#relation-classification
  87. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#training
  88. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#data
  89. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#experiments
  90. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#model3v1
  91. https://github.com/sshanu/relation-classification/tree/master/lstm%20seq%20and%20tree#model3v2
  92. https://github.com/
