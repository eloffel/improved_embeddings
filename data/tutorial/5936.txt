   #[1]machine learning in action    feed [2]machine learning in action   
   comments feed [3]machine learning in action    topic modelling (part 1):
   creating article corpus from simple wikipedia dump comments feed
   [4]user verification based on keystroke dynamics: python code [5]topic
   modelling (part 2): discovering topics from articles with latent
   dirichlet allocation [6]alternate [7]alternate [8]machine learning in
   action [9]wordpress.com

   [10]skip to content

     * [11]home
     * [12]presentations
     * [13]about
     * [14]contact

     * [15]facebook
     * [16]linkedin
     * [17]twitter
     * [18]instagram
     * [19]search

   search for: ____________________ (button) search

[20]machine learning in action

a perfect hands-on practice for beginners to elevate their ml skills

   [21]natural language processing

topic modelling (part 1): creating article corpus from simple wikipedia dump

   date: [22]august 28, 2017author: [23]abhijeet kumar [24]8 comments

   a huge number of text articles are generated everyday from different
   publishing houses, blogs, media, etc. this leads to one of the major
   tasks in natural language processing i.e. effectively managing,
   searching and categorizing articles depending upon their subjects or
   themes. typically, these id111 tasks will include text
   id91, document similarity and categorization of text.
   comprehensively, we have to find out some ways so that the theme of the
   article can be extracted. in text analytics, this is known as    topic
   modelling   . also, given a topic, our software should be able to find
   out articles which are similar to it. this is known as    document
   similarity   .

   deriving such meaningful information from text documents is the main
   objective of this blog-post series. i will be covering the whole
   application of topic modelling in 3 blog-posts. the purpose of the
   blog-post series is to build the system from scratch and provide an
   insight of implementation of the same to our readers. this particular
   post will be focusing on creating a corpus of [25]simple wikipedia
   articles from dumped simple wiki xml file. once the text data
   (articles) has been retrieved, it can be used by machine learning
   techniques for model training in order to discover topics from the text
   corpus.

   there are mainly two steps in the text data retrieval process from
   simple wikipedia dump:

   1. xml parsing of the wiki dump
   2. cleaning of the articles    text

   the simple wikipedia is an edition of the
   online encyclopedia wikipedia, primarily written in basic english. the
   articles on simple wikipedia are usually shorter than their [26]english
   wikipedia counterparts, presenting only the basic information.
   it contains over 127,000 content pages for people to search, explore or
   even edit. we downloaded the free backup xml file in which all the
   articles are dumped. then a sample of 60,000 simple wikipedia articles
   is randomly selected for building the application. you can download the
   same backup xml file(used in this blog) from [27]here or it can be
   downloaded from [28]index of simple wiki website.

1. xml parsing of wiki dump

   all the information of an article like title, id, time stamp,
   contributor, text content, etc lies in the page tag of xml file. there
   are more than 100,000 such legitimate pages. a typical article in wiki
   dumped xml file looks like this.

   xml_wiki
   the document object model (tree view) represents this xml snippet like
   this:

   tree_xml

   seeing all this, one can observe that we have to get article text from
   the text tag in the xml file, which is one of the children of the
   revision tag (revision itself being a child of the page tag). we will
   use the [29]element tree xml api for parsing the xml file and
   extracting the text portion of the article. the below python code
   traverses down the tree to get the content of the text tag. the
   contents of each article are extracted from the text tag of that
   corresponding page in iterations and can be written in separate text
   files.
import xml.etree.elementtree as et
import codecs
import re

tree = et.parse('simplewiki-20170201-pages-articles-multistream.xml')
root = tree.getroot()
path = 'articles-corpus//'
url  = '{http://www.mediawiki.org/xml/export-0.10/}page'

for i,page in enumerate(root.findall(url)):
    for p in page:
        r_tag = "{http://www.mediawiki.org/xml/export-0.10/}revision"

        if p.tag == r_tag:
            for x in p:
                tag = "{http://www.mediawiki.org/xml/export-0.10/}text"

                if x.tag == tag:

                    text = x.text
                    if not text == none:
                        # extracting the text portion from the article

                        text = text[:text.find("==")]

                        # <em><strong>cleaning of text (described in section 2)<
/strong></em>
                        # printing the article
                        print text
                        print '\n====================================\n'

   also, we are only interested in getting the introductory text about the
   title (like in above sample, the title is    treason   ), not its
   subheading or other contents like responsibilities to protect and
   references. in order to do this, we extract the sub string from
   starting index to the index location before the start of the first
   subheading. it is implemented by the python statement given below:
   text = text[: text.find("==")].

   the created text article for the above sample page looks like this:

   uncleaned

2. cleaning of article text

   data pre-processing (a.k.a data cleaning) is one of the most
   significant step in text analytics. the purpose is to remove any
   unwanted words or characters which are written for human readability,
   but won   t contribute to topic modelling in any way.

   there are mainly two steps that need to be done on word level:

   a) removal of stop words     stop words like    and   ,    if   ,    the   , etc are
   very common in all english sentences and are not very meaningful in
   deciding the theme of the article, so these words have been removed
   from the articles.

   b) lemmatization     it is the process of grouping together the different
   inflected forms of a word so they can be analysed as a single item. for
   example,    include   ,    includes,    and    included    would all be represented
   as    include   . the context of the sentence is also preserved in
   lemmatization as opposed to id30 (another buzz word in id111
   which does not consider the meaning of the sentence).

   the following python code defines a function clean() for cleaning the
   text article passed as an argument to it:
from nltk.corpus import stopwords
from nltk.stem.id138 import id138lemmatizer
import string

stop    = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma   = id138lemmatizer()

# pass the article text as string "doc"
def clean(doc):

  # remove stop words & punctuation, and lemmatize words
  s_free  = " ".join([i for i in doc.lower().split() if i not in stop])
  p_free  = ''.join(ch for ch in s_free if ch not in exclude)
  lemm    = " ".join(lemma.lemmatize(word) for word in p_free.split())
  words   = lemm.split()

  # only take words which are greater than 2 characters
  cleaned = [word for word in words if len(word) > 2]
  return cleaned

   we will plug the above cleaning code in the next blog-post where the
   training code of the id44 (lda) model will be
   shown in order to discover hidden topics from the corpus. as of now, we
   are focusing only on creating the wiki corpus of articles.

   specially for wikipedia articles, one needs to apply several steps to
   clean the article text which includes removal of file attachment, image
   attachments, urls, infobox, xml labels, etc. the following python code
   applies regular expression for matching such patterns and removing
   them. these 30 filters are applied depending on my analysis of the wiki
   text. there may be several other patterns which might have been missed
   here.
# remove text written between double curly braces
article_txt = re.sub(r"{{.*}}","",article_txt)

# remove file attachments
article_txt = re.sub(r"\[\[file:.*\]\]","",article_txt)

# remove image attachments
article_txt = re.sub(r"\[\[image:.*\]\]","",article_txt)

# remove unwanted lines starting from special characters
article_txt = re.sub(r"\n: \'\'.*","",article_txt)
article_txt = re.sub(r"\n!.*","",article_txt)
article_txt = re.sub(r"^:\'\'.*","",article_txt)

# remove non-breaking space symbols
article_txt = re.sub(r"&nbsp","",article_txt)

# remove urls link
article_txt = re.sub(r"http\s+","",article_txt)

# remove digits from text
article_txt = re.sub(r"\d+","",article_txt)

# remove text written between small braces
article_txt = re.sub(r"\(.*\)","",article_txt)

# remove sentence which tells category of article
article_txt = re.sub(r"category:.*","",article_txt)

# remove the sentences inside infobox or taxobox
article_txt = re.sub(r"\| .*","",article_txt)
article_txt = re.sub(r"\n\|.*","",article_txt)
article_txt = re.sub(r"\n \|.*","",article_txt)
article_txt = re.sub(r".* \|\n","",article_txt)
article_txt = re.sub(r".*\|\n","",article_txt)

# remove infobox or taxobox
article_txt = re.sub(r"{{infobox.*","",article_txt)
article_txt = re.sub(r"{{infobox.*","",article_txt)
article_txt = re.sub(r"{{taxobox.*","",article_txt)
article_txt = re.sub(r"{{taxobox.*","",article_txt)
article_txt = re.sub(r"{{ infobox.*","",article_txt)
article_txt = re.sub(r"{{ infobox.*","",article_txt)
article_txt = re.sub(r"{{ taxobox.*","",article_txt)
article_txt = re.sub(r"{{ taxobox.*","",article_txt)

# remove lines starting from *
article_txt = re.sub(r"\* .*","",article_txt)

# remove text written between angle bracket
article_txt = re.sub(r"","",article_txt)

# remove new line character
article_txt = re.sub(r"\n","",article_txt)

# replace all punctuations with space
article_txt = re.sub(r"\!|\"|\#|\$|\%|\&|\'|\(|\)|\*|\+|\,|\-|\.|\/|\:|\;|\|\?|\
@|\[|\\|\]|\^|\_|\`|\{|\||\}|\~"," ",article_txt)

# replace consecutive multiple space with single space
article_txt = re.sub(r" +"," ",article_txt)

# replace non-breaking space with regular space
article_txt = article_txt.replace(u'\xa0', u' ')

# writing the clean text in file
if len(article_txt) > 150 and is_ascii(article_txt) and not article_txt == none
and not article_txt == "":
    outfile = path + str(i+1) +"_article.txt"
    f       = codecs.open(outfile, "w", "utf-8")
    f.write(article_txt)
    f.close()

   the above code snippet of text filters can be plugged to the text
   extracted from the text tag (figure 1). finally, we keep only those
   articles which have length more than 150 characters. also, we check and
   write only those text articles which contain only ascii characters
   (english characters only).

   this completes the first step towards id96, i.e. creating the
   corpus of articles from simple wikipedia. once you follow this blog
   till here, you will be able to create a corpus of around 70,000
   articles in the directory    articles-corpus    used in python program. i
   will be writing about discovering the hidden topics from the corpus
   created in the next blog-post soon. so stay tuned till then !!

   you can get the full python code for parsing, cleaning and creating an
   article corpus (from simple wiki xml dump file) from github
   link [30]here.

   if you liked the post, follow this blog to get updates about upcoming
   articles. also, share it so that it can reach out to the readers who
   can actually gain from this. please feel free to discuss anything
   regarding the post. i would love to hear feedback from you.

   happy machine learning     
   advertisements

sharing is caring

     * [31]click to share on facebook (opens in new window)
     * [32]click to share on whatsapp (opens in new window)
     * [33]click to share on twitter (opens in new window)
     *

like this:

   like loading...

related

   [34]etree api xml parsing[35]hidden topic discovery[36]nltk
   lemmatizer[37]nltk stop words removal[38]pre-processing text[39]python
   implementation[40]python topic modelling[41]regular
   expressions[42]simple wiki article corpus

published by abhijeet kumar

   currently, i am working as a consultant with an it company in the field
   of machine learning and deep learning with experience in speech
   analytics, natural language processing and little bit in image
   analytics. i believe in spreading knowledge from whatever i learn or
   do. love to post python implementations of various machine learning
   applications. hope it helps the students and beginners out here to get
   started with hands on coding in machine learning and deep learning
   areas. [43]view all posts by abhijeet kumar

post navigation

   [44]previous previous post: user verification based on keystroke
   dynamics: python code
   [45]next next post: topic modelling (part 2): discovering topics from
   articles with id44

8 thoughts on    topic modelling (part 1): creating article corpus from simple
wikipedia dump   

   [46]add comment
    1.
   amman habib says:
       [47]march 12, 2018 at 3:48 pm
       thankyou for the article great help .
       [48]likeliked by [49]1 person
       [50]reply
         1.
        [51]abhijeet kumar says:
            [52]march 16, 2018 at 6:53 pm
            glad you liked.
            [53]likelike
            [54]reply
    2.
   astha says:
       [55]december 5, 2018 at 6:31 am
       hi abhijeet, sorry to disturb you again.
       i tried to extract the whole wikipedia dump, which is around
       63gb(after extracting) but my system could not take the load. after
       changing my system configurations (ram:16gb, 120gbssd hard disk,
       processor i5), still can   t run my program . it   s giving
       memoryerror.
       please help me.
       [56]likelike
       [57]reply
         1.
        [58]abhijeet kumar says:
            [59]december 5, 2018 at 10:30 am
            hi astha,
            are you using simple wikipedia dump or wikipedia dump ?
            seems like you are taking wikipedia dump.
            is this a single xml file of 63 gb ?
            can you elaborate your situation ?
            [60]likelike
            [61]reply
              1.
             astha says:
                 [62]december 5, 2018 at 12:46 pm
                 hope i   m able to explain myself well
                 enwiki-latest-pages-articles.xml.bz2(14.4 gb)(it   s a
                 wikipedia dump)
                 after unzipping it   s size is 64.7 gb.
                 so when i exract it by your code it throws memory error.
                 please suggest something that works for the above
                 wikipedia dump.
                 thanks.
                 [63]likelike
                 [64]reply
                   1.
                  [65]abhijeet kumar says:
                      [66]december 5, 2018 at 1:55 pm
                      oh it   s huge.
                      so basically you may need to use some package which
                      can read/parse it serially. that means it should
                      load only a part of file in ram at a time.
                      there would be ways for that. like this
                      [67]http://boscoh.com/programming/reading-xml-serial
                      ly.html
                      but if you are doing it for first time, you can go
                      for simple wikipedia dump oops which will be small.
                      [68]likelike
                      [69]reply
                        1.
                       astha kaushik says:
                           [70]december 6, 2018 at 11:32 am
                           hi abhijeet, thanks for replying
                           i tried on simple wikipedia dump & it worked
                           fine. so, i wanted to try on large wikipedia
                           dump. i followed that link & tried to extract,
                           it   s been more than 24 hours & it   s still
                           running.
                           please tell if it will take this much time
                           [71]likeliked by [72]1 person
                           [73]reply
                             1.
                            [74]abhijeet kumar says:
                                [75]december 6, 2018 at 2:03 pm
                                hi,
                                firstly,
                                i do not know what   s the tree structure of
                                large wiki dump. the xml parser written in
                                this post is for simple wiki. it may not
                                work.
                                secondly,
                                if at all it is working fine then you
                                would be able to see it in the directory
                                where the program would write text
                                articles in separate files.
                                if it is running actually from 24 hours it
                                would have generated lakhs of files.
                                [76]likelike
                                [77]reply

leave a reply [78]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [79]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [80]log out /
   [81]change )
   google photo

   you are commenting using your google account. ( [82]log out /
   [83]change )
   twitter picture

   you are commenting using your twitter account. ( [84]log out /
   [85]change )
   facebook photo

   you are commenting using your facebook account. ( [86]log out /
   [87]change )
   [88]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   advertisements

   search for: ____________________ (button) search

categories

     * [89]applications
          + [90]id161
          + [91]natural language processing
          + [92]speech analytics
     * [93]data analytics
     * [94]deep neural network
     * [95]miscellaneous
          + [96]distance metrics
     * [97]supervised learning
          + [98]classifiers
     * [99]unsupervised learning
          + [100]id91

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 854 other followers

   ____________________

   (button) follow

archives

   archives [select month_______]

visitors

     * 255,347 hits

   advertisements

      2019 [101]machine learning in action

   [102]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [103]cancel reblog post

   iframe: [104]likes-master

   %d bloggers like this:

references

   visible links
   1. https://appliedmachinelearning.blog/feed/
   2. https://appliedmachinelearning.blog/comments/feed/
   3. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/feed/
   4. https://appliedmachinelearning.blog/2017/07/26/user-verification-based-on-keystroke-dynamics-python-code/
   5. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/&for=wpcom-auto-discovery
   8. https://appliedmachinelearning.blog/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#main
  11. https://appliedmachinelearning.blog/
  12. https://appliedmachinelearning.blog/presentations/
  13. https://appliedmachinelearning.blog/about/
  14. https://appliedmachinelearning.blog/contact/
  15. http://www.facebook.com/
  16. http://www.linkedin.com/
  17. http://www.twitter.com/
  18. http://www.instagram.com/
  19. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  20. https://appliedmachinelearning.blog/
  21. https://appliedmachinelearning.blog/category/applications/natural-language-processing/
  22. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  23. https://appliedmachinelearning.blog/author/abhijeetchar/
  24. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comments
  25. https://simple.wikipedia.org/wiki/main_page
  26. https://en.wikipedia.org/wiki/main_page
  27. https://www.dropbox.com/s/d04xx38u8mgdzpf/simplewiki-20170201-pages-articles-multistream.zip?dl=0
  28. https://dumps.wikimedia.org/simplewiki/
  29. https://docs.python.org/2/library/xml.etree.elementtree.html
  30. https://github.com/abhijeet3922/topic-modelling-on-wiki-corpus
  31. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?share=facebook
  32. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?share=jetpack-whatsapp
  33. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?share=twitter
  34. https://appliedmachinelearning.blog/tag/etree-api-xml-parsing/
  35. https://appliedmachinelearning.blog/tag/hidden-topic-discovery/
  36. https://appliedmachinelearning.blog/tag/nltk-lemmatizer/
  37. https://appliedmachinelearning.blog/tag/nltk-stop-words-removal/
  38. https://appliedmachinelearning.blog/tag/pre-processing-text/
  39. https://appliedmachinelearning.blog/tag/python-implementation/
  40. https://appliedmachinelearning.blog/tag/python-topic-modelling/
  41. https://appliedmachinelearning.blog/tag/regular-expressions/
  42. https://appliedmachinelearning.blog/tag/simple-wiki-article-corpus/
  43. https://appliedmachinelearning.blog/author/abhijeetchar/
  44. https://appliedmachinelearning.blog/2017/07/26/user-verification-based-on-keystroke-dynamics-python-code/
  45. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  46. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#respond
  47. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-481
  48. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=481&_wpnonce=c44f80c7b3
  49. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  50. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=481#respond
  51. https://appliedmachinelearning.wordpress.com/
  52. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-491
  53. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=491&_wpnonce=88e3f7cdf4
  54. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=491#respond
  55. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2242
  56. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2242&_wpnonce=46d1b58e8f
  57. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2242#respond
  58. http://appliedmachinelearning.blog/
  59. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2245
  60. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2245&_wpnonce=c3ed70bbc9
  61. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2245#respond
  62. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2253
  63. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2253&_wpnonce=e21bd41ea1
  64. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2253#respond
  65. http://appliedmachinelearning.blog/
  66. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2257
  67. http://boscoh.com/programming/reading-xml-serially.html
  68. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2257&_wpnonce=3a2bf04aac
  69. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2257#respond
  70. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2287
  71. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2287&_wpnonce=b84acbefc5
  72. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  73. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2287#respond
  74. http://appliedmachinelearning.blog/
  75. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-2288
  76. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?like_comment=2288&_wpnonce=9d25f82474
  77. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/?replytocom=2288#respond
  78. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#respond
  79. https://gravatar.com/site/signup/
  80. javascript:highlandercomments.doexternallogout( 'wordpress' );
  81. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  82. javascript:highlandercomments.doexternallogout( 'googleplus' );
  83. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  84. javascript:highlandercomments.doexternallogout( 'twitter' );
  85. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  86. javascript:highlandercomments.doexternallogout( 'facebook' );
  87. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  88. javascript:highlandercomments.cancelexternalwindow();
  89. https://appliedmachinelearning.blog/category/applications/
  90. https://appliedmachinelearning.blog/category/applications/computer-vision/
  91. https://appliedmachinelearning.blog/category/applications/natural-language-processing/
  92. https://appliedmachinelearning.blog/category/applications/speech-analytics/
  93. https://appliedmachinelearning.blog/category/data-analytics/
  94. https://appliedmachinelearning.blog/category/deep-neural-network/
  95. https://appliedmachinelearning.blog/category/miscellaneous/
  96. https://appliedmachinelearning.blog/category/miscellaneous/distance-metrics/
  97. https://appliedmachinelearning.blog/category/supervised-learning/
  98. https://appliedmachinelearning.blog/category/supervised-learning/classifiers/
  99. https://appliedmachinelearning.blog/category/unsupervised-learning/
 100. https://appliedmachinelearning.blog/category/unsupervised-learning/id91/
 101. https://appliedmachinelearning.blog/
 102. https://wordpress.com/?ref=footer_custom_blog
 103. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
 104. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 106. https://appliedmachinelearning.blog/
 107. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-form-guest
 108. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-form-load-service:wordpress.com
 109. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-form-load-service:twitter
 110. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/#comment-form-load-service:facebook
 111. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
