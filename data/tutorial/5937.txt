   #[1]machine learning in action    feed [2]machine learning in action   
   comments feed [3]machine learning in action    topic modelling (part 2):
   discovering topics from articles with id44
   comments feed [4]topic modelling (part 1): creating article corpus from
   simple wikipedia dump [5]topic modelling (part 3): document id91,
   exploration & theme extraction from simplewiki articles [6]alternate
   [7]alternate [8]machine learning in action [9]wordpress.com

   [10]skip to content

     * [11]home
     * [12]presentations
     * [13]about
     * [14]contact

     * [15]facebook
     * [16]linkedin
     * [17]twitter
     * [18]instagram
     * [19]search

   search for: ____________________ (button) search

[20]machine learning in action

a perfect hands-on practice for beginners to elevate their ml skills

   [21]id91, [22]natural language processing, [23]unsupervised
   learning

topic modelling (part 2): discovering topics from articles with latent
dirichlet allocation

   date: [24]september 28, 2017author: [25]abhijeet kumar [26]7 comments

   this blog-post is second in the series of blog-posts covering    topic
   modelling    from simple wikipedia articles. before reading this post, i
   would suggest reading our first article [27]here. in the first step
   towards id96 which entailed creating a corpus of articles
   from simple wikipedia, we were able to create a corpus of around 70,000
   articles in the directory    articles-corpus   .

   look at the above featured image of this blog-post      these are some of
   the topics (word distributions) which are the outcome of the experiment
   undertaken in this post. lets get started with discovering topics from
   the corpus of wiki articles. we will be using an unsupervised machine
   learning technique, id44 (lda), for
   automatically finding the mixture of similar words together, thus
   forming the topic or theme. from such a huge corpus of articles, we do
   not have the information about the categories to which these articles
   belong to or are related. this forms an unsupervised problem where we
   do not know the labels/classes/categories of the data and aim to find
   the groups or the clusters within the population. having said that, i
   am now going to list down the steps which we have to perform in order
   to discover the topics hidden in the 60,000 articles, serving as
   training data:
    1. pre-processing and training corpus creation
    2. building dictionary
    3. feature extraction
    4. lda model training

   later in this blog-post, i will be discussing about the interpretation
   of the results (discovered topics), which is the outcome of the
   training process. for the installations required for this application,
   you can follow this [28]link.

1.  preprocessing & training data preparation.

   as discussed in part-i, we need to remove the stop words from the
   articles because they do not contribute to the theme of the article   s
   content. similarly, id30 or lemmatization is an effective process
   in order to treat various inflected forms of words as a single word as
   they essentially mean the same. i would encourage you to go through the
   previous post (part-1) if the above sentences do not make sense to you.
import os
import random
import codecs
from nltk.corpus import stopwords
from nltk.stem.id138 import id138lemmatizer

# function to remove stop words from sentences & lemmatize words.
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    normalized = " ".join(lemma.lemmatize(word,'v') for word in stop_free.split(
))
    x = normalized.split()
    y = [s for s in x if len(s) > 2]
    return y

# remember this folder contains 72,000 articles extracted in part-1 (previous po
st)
corpus_path = "articles-corpus/"
article_paths = [os.path.join(corpus_path,p) for p in os.listdir(corpus_path)]

# read contents of all the articles in a list "doc_complete"
doc_complete = []
for path in article_paths:
    fp = codecs.open(path,'r','utf-8')
    doc_content = fp.read()
    doc_complete.append(doc_content)

# randomly sample 70000 articles from the corpus created from wiki_parser.py

docs_all = random.sample(doc_complete, 70000)
docs = open("docs_wiki.pkl",'wb')
cpickle.dump(docs_all,docs)

# use 60000 articles for training.
docs_train = docs_all[:60000]

# cleaning all the 60,000 simplewiki articles
stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma = id138lemmatizer()
doc_clean = [clean(doc) for doc in docs_train]

   in the above code, we are reading all the articles in a list and
   creating the training data by choosing 60,000 articles from randomly
   sampled 70,000 articles from that list. the remaining 10,000 articles
   are left for test purpose (document id91/categorization) in
   part-3. further, the articles are cleaned by removing stop words and
   passing each word of corpus through    id138lemmatizer   . as a result,
   we get cleaned articles on which we can build the dictionary and train
   the lda model for topic modelling.

2.  building word dictionary

   in this step, we need to build the vocabulary of the corpus in which
   all the unique words of the article corpus are given ids and their
   frequency counts are also stored. the following python code creates the
   dictionary from the 60,000 randomly sampled cleaned articles. you may
   note that we are using gensim library for building the dictionary. in
   gensim, the words are referred to as    tokens    and the index of each
   word in the dictionary is called    id   .
from gensim import corpora
# creating term dictionary of corpus, where each unique term is assigned an inde
x.
dictionary = corpora.dictionary(doc_clean)

# filter terms which occurs in less than 4 articles & more than 40% of the artic
les
dictionary.filter_extremes(no_below=4, no_above=0.4)

# list of few words which are removed from dictionary as they are content neutra
l
stoplist = set('also use make people know many call include part find become lik
e mean often different \
               usually take wikt come give well get since type list say change s
ee refer actually iii \
               aisne kinds pas ask would way something need things want every st
r'.split())
stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword i
n dictionary.token2id]
dictionary.filter_tokens(stop_ids)

   also, it can be seen that there are 2 additional steps performed after
   creating the dictionary:
    1. all the tokens in the dictionary which either have occurred in less
       than 4 articles or have occurred in more than 40% of the articles
       are removed from the dictionary, as these words will not be
       contributing to the various themes or topics.
    2. after printing the most frequent words of the dictionary, we found
       that few words which are mostly content neutral words are also
       present in the dictionary. these words may lead to modeling of
          word distribution   (topic) which is neutral and do not capture any
       theme or content. we made a list of such words and filtered all
       such words.

   once you have built the dictionary, you may find the most frequent
   words with their respective frequencies like this:
words frequency
[(u'state', 10294), (u'one', 9451), (u'unite', 9213), (u'first', 8511), (u'ameri
can', 8383), (u'name', 6933), (u'play', 6043), (u'new', 5701), (u'bear', 5624),
(u'two', 5614), (u'time', 5523), (u'world', 4949)]
ids
[22871, 579, 19641, 3768, 2573, 18650, 19284, 6702, 24598, 17353, 20208, 4284]

   each word is also given a unique id in the vocabulary (dictionary).

3.  feature extraction (bag of words)

   histograms of words are the features used for text representation. in
   general, we first build the vocabulary of the article corpus and then
   we generate a word count vector for each article, which is nothing but
   the frequencies of all the words in the vocabulary for that particular
   article. most of them will be zero as a single article won   t contain
   all the words in the vocabulary. for example, suppose we have 500 words
   in vocabulary. so, each word count vector will contain the frequencies
   of these 500 vocabulary words in a particular wiki article. suppose
   that the text in an article was    get the work done, work done   . so, a
   fixed length encoding will be generated as
   [0,0,0,0,0,      .0,0,2,0,0,0,      ,0,0,1,0,0,   0,0,1,0,0,      2,0,0,0,0,0]. here,
   all the word counts are placed at 296th, 359th, 415th, 495th index of
   the 500 length word count vector and the rest are zero.
# converting list of documents (corpus) into document term matrix using dictiona
ry prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]

   the above python code uses gensim to convert all the 60,000 articles
   into a document term matrix (word count vector for each document).

4. lda model training

   we have finally arrived at the training phase of id96. latent
   dirichlet allocation is an unsupervised probabilistic model which is
   used to discover latent themes in a document. let   s try to understand
   briefly the working of lda technique.

   lda technique makes the following two assumptions:
   1. articles/documents are produced from a mixture of topics. each
   article belongs to each of the topics to a certain degree (each
   articles is made up of some topic distribution).
   2. each topic is a generative model which generates words of the
   vocabulary with certain probabilities. words frequently occurring
   together will have more id203 (each topic is made of some word
   distribution).

   so, can you guess the input to this algorithm?

   input is the    document-term matrix    which keeps the histograms of words
   (word count) present in each wiki article. the dimensions of the matrix
   is (m,n) ,i.e. number of documents * number of words in vocabulary.
   documents and articles are interchangeable words here. we also provide
   k as an input, which is the number of topics that have to be
   discovered.
   doc_term document-term matrix

   what is the output of the id44 algorithm?

   the output of lda algorithm are 2 smaller matrices     a document to
   topic matrix and a topic to word matrix. document-topic matrix is of
   (m,k) dimensions where m is number of articles andk is number of topics
   in the vocabulary. topic-word matrix is of (k,n) where n is the number
   of words in the vocabulary.
   doc_topic document-topic distributiontopic_word topic-word distribution

   document-topic matrix accounts for the id203 distribution of the
   topics present in the article. similarly, topic-words matrix accounts
   for the id203 distribution of words that they have been generated
   from that topic. both these matrices are initialized randomly and then
   these distributions are improved upon in an iterative process. after
   repeating the previous step a large number of times, you   ll eventually
   reach an approximately steady state where these distributions seem
   logically correct.

   the following python code runs lda algorithm using gensim
   implementation. once the training is completed, the model is dumped
   using cpickle library for future use and all the 50 topics (learned by
   model) are printed.
from gensim.models.ldamodel import ldamodel as lda
# creating the object for lda model using gensim library & training lda model on
 the document term matrix.
ldamodel = lda(doc_term_matrix, num_topics=50,  word = dictionary, passes=50,
iterations=500)

# dump lda model using cpickle for future use
ldafile = open('lda_model_sym_wiki.pkl','wb')
cpickle.dump(ldamodel,ldafile)
ldafile.close()

# print all the 50 topics
for i,topic in enumerate(ldamodel.print_topics(num_topics=50, num_words=10)):
   words = topic[1].split("+")
   print words,"\n"

   this completes the pipeline for topic discovery from 60,000 simple wiki
   articles. you can very easily combine the python code snippets from the
   beginning to the end serially to have full implementation for topic
   modelling.

results

   for the first time, it was really exciting for me to see how the topics
   have been formed as a mixture of similar words from the same domain
   which may speak a lot about the theme of the article. these are some of
   the interesting learned topics resulting from the fitted model:
   topic 3 topic 6  topic 9  topic 10 topic 15  topic 16    topic 35  topic 40
   album   war      word     body     template football    party      black
   song    military study    cause    link     team        political  white
   release church   science  cell     text     play        leader     red
   record  world    language disease  category national    vote       bird
   single  army     work     human    function player      election   blue
   music   battle   latin    medical  add      league      crash      green
   first   fight    research system   name     association politics   ship
   pop     british  theory   doctor   value    japan       bank       fly
   singer  force    spanish  result   set      infobox     orange     brown
   hit     civil    society  test     table    club        democratic wear


topic 41 topic 30  topic 50   topic 1     topic 5   topic 37 topic 35  topic 38
plant    die      band       power      number      english  water    movie
food     age      rock       energy     computer    england  chemical america
grow     bear     group      heat       system      king     contain  star
eat      year     music      engineer   program     kingdom  gas      play
flower   death    members    electric   information british  compound actor
mountain january  label      current    data        royal    ball     television
cells    marry    popular    education  software    great    form     americans
range    years    play       higher     code        queen    units    movies
fruit    cancer   form       technology internet    scotland acid     direct
cook     february instrument board      systems     london   process  actress

   the above table shows 16 of 50 topics after the model is trained, where
   top ten terms are listed for each topic. with lda training, the word
   distribution of the same topic tends to be similar. formally speaking,
   they are highly associated. for example: topic 1 is about music, topic
   6 is about war, topic 9 is related to literature, topic 10 is related
   to medical, topic 16 is about games, topic 35 is about politics, topic
   40 relates to various colors, topic 30 relates to biography of some
   person, topic is about it, topic 35 is related to chemistry and topic
   38 relates to movies.

   there are other topics too which have been generated and can describe
   the theme of the articles. you can check all the 50 topics along with
   their word id203 distribution  from this [29]file.

discussion

   as we analyze the topics discovered by lda model, we see that these
   topics are basically probabilistic word distribution which can very
   well describe a particular theme or content. after experimenting number
   of times with simple wiki articles, i came to a conclusion that words
   in the modeled topics may not be perfectly similar but are definitely
   associated.
   a very few topics generated from unsupervised training are content
   neutral. for example:

   (topic 44, u'0.031*"women" + 0.020*"drug" + 0.019*"blood" + 0.017*"men"
   + 0.015*"sing" + 0.014*"sex" + 0.014*"god" + 0.014*"feel" +
   0.013*"nuclear" + 0.013*"child"')

   (topic 45, u'0.040*"art" + 0.029*"paint" + 0.027*"heart" +
   0.026*"attack" + 0.020*"oil" + 0.018*"business" + 0.018*"street" +
   0.017*"horse" + 0.016*"police" + 0.015*"work"')

   also, there are some topics in which a few words may seem irrelevant to
   the theme/content but if you analyze them properly they are somewhat
   associated. this problem of association of two different context with
   one word is called    word-sense disambiguation   . because of association
   of two context with same word, the topic modeled also contains two
   themes jointly in one topic. for example:

   (topic 29, u'0.079*"die" + 0.060*"age" + 0.059*"bear" + 0.029*"year" +
   0.024*"death" + 0.018*"january" + 0.018*"marry" + 0.016*"years" +
   0.016*"cancer" + 0.014*"february"')

   (topic 30, u'0.102*"south" + 0.050*"new" + 0.040*"west" + 0.036*"north"
   + 0.034*"park" + 0.030*"east" + 0.027*"wales" + 0.026*"division" +
   0.020*"coast" + 0.016*"australia"')

   (topic 39, u'0.041*"black" + 0.040*"white" + 0.034*"red" + 0.029*"bird"
   + 0.027*"blue" + 0.024*"green" + 0.020*"ship" + 0.019*"fly" +
   0.019*"brown" + 0.018*"wear"'

   in topic 29, the word distribution is representation of biography
   theme. usually, in biographies, the articles talk about the birth,
   marraige and death of the person. the word    cancer    has occurred within
   the distribution because it may have been the cause of death in most of
   the articles. also, the months    january    and    february    have occurred
   as they might be present in the biography articles to show the timeline
   of the life of the person. similarly, if you observe topic 30, the
   distribution has captured the directions but as the words    south    or
      east    are associated with    new  south wales    state which is on the
   east coast of australia, therefore related words like
      new   ,   wales   ,   australia   ,   coast    are also present. in topic 39, words
   like    birds    or    wear    are present in the distribution. the probable
   cause of this may be the usage of the colors with birds and wearing in
   many wiki articles.

final thoughts

   hope it was an easy task for our readers to follow the blog-post till
   here. in this post, i have tried to explain the pipeline of the topic
   discovery process, from preparing the training data to the training of
   the lda model. i have also tried to briefly explain the latent
   dirichlet allocation algorithm to provide an idea of what goes into and
   what comes out from the lda model. i would encourage readers to
   implement this series of blog-posts (see [30]part-1), and match their
   outputs with the results shown here (though topics discovered can be
   different at every run).

   there are several factors that you can experiment with in order to get
   even better word distributions forming the topics:

   1. getting more number of articles : you can try increasing the number
   of articles by changing the minimum article length from 150 to 100
   characters in part-1. also see if we can prevent discarding the
   articles which contains few non-ascii characters. more training data
   may lead to better topic-word distribution.

   2. preprocessing : by analyzing the word distributions of topics
   generated, you may find
     * pairs that are always juxtaposed (entities) e.g.    los angeles   
       (topic 20),    new york    (topic 27). these pairs should be combined
       like los_angeles or new_york.
     * words that are not properly lemmatized like (germany, german),
       (chinese, china), (america,americans) etc. lemmatization of nouns
       may help. remember, we did lemmatization of verbs
       lemma.lemmatize(word,'v').

   3. dictionary : the vocabulary of the corpus can be improved by
   removing the content neutral words. iteratively running the whole topic
   discovery process and analyzing the word distributions (topics) can
   help in finding content neutral words from dictionary. some example are
      ing    (topic 10),    per    (topic 43).

   4. parameters of lda : there are two parameters of lda to look upon    
   alpha and beta. understanding the mathematics behind lda model may help
   in tuning these parameters. i would encourage readers to do so.

   the full python implementation of id96 on simple-wiki
   articles dataset can be found on github link [31]here. this completes
   the second step towards id96, i.e. topic discovery from
   training articles.  after this step, now you will be having a [32]dump
   of 70,000 randomly sampled cleaned wiki articles and [33]lda model
   which consists of 50 discovered topics. we will need both of them while
   performing article id91/ categorization in part-3 of this
   blog-post series.

   i will be writing about id91 the test wiki-articles using the
   modeled topics in the next blog-post soon. so stay tuned till then!!

   if you liked the post, follow this blog to get updates about the
   upcoming articles. also, share this article so that it can reach out to
   the readers who can actually gain from this. please feel free to
   discuss anything regarding the post. i would love to hear feedback from
   you.

   happy machine learning     
   advertisements

sharing is caring

     * [34]click to share on facebook (opens in new window)
     * [35]click to share on whatsapp (opens in new window)
     * [36]click to share on twitter (opens in new window)
     *

like this:

   like loading...

related

   [37]bag of words[38]document id91 python[39]gensim lda[40]latent
   dirichlet allocation[41]nltk lemmatization[42]python topic
   modelling[43]topic discovery[44]topic word cloud[45]wikipedia topic
   model[46]word dictionary

published by abhijeet kumar

   currently, i am working as a consultant with an it company in the field
   of machine learning and deep learning with experience in speech
   analytics, natural language processing and little bit in image
   analytics. i believe in spreading knowledge from whatever i learn or
   do. love to post python implementations of various machine learning
   applications. hope it helps the students and beginners out here to get
   started with hands on coding in machine learning and deep learning
   areas. [47]view all posts by abhijeet kumar

post navigation

   [48]previous previous post: topic modelling (part 1): creating article
   corpus from simple wikipedia dump
   [49]next next post: topic modelling (part 3): document id91,
   exploration & theme extraction from simplewiki articles

7 thoughts on    topic modelling (part 2): discovering topics from articles
with id44   

   [50]add comment
    1.
   chaitanya says:
       [51]october 4, 2017 at 4:39 am
       if not lda probabilistic model what other models we can use? can
       you suggest some?
       [52]likelike
       [53]reply
         1.
        [54]ml bot2 says:
            [55]october 4, 2017 at 5:24 am
            yes. there is another technique for modeling topics from
            articles. that is called lsi (latent semantic analysis)
            you can find a usage of that model in gensim experiments here.
            [56]https://radimrehurek.com/gensim/wiki.html
            [57]likelike
            [58]reply
    2. pingback: [59]topic modelling (part 3): document id91,
       exploration & theme extraction from simplewiki articles     machine
       learning in action
    3.
   vagif mallayev says:
       [60]december 2, 2018 at 7:07 pm
       thanks abhijeet for your tutorial!
       i have found in your code on the github this string (line 61):
       words,ids = dictionary.filter_n_most_frequent(50)
       as i understand, that removes 50 most frequent tokens from the
       cleaned dictionary.
       can you explain the purpose for that? are those tokens neutral?
       [61]likelike
       [62]reply
         1.
        [63]abhijeet kumar says:
            [64]december 2, 2018 at 7:17 pm
            hi,
            can you tell me the file name where this line is written ?
            [65]likelike
            [66]reply
              1.
             vagif mallayev says:
                 [67]december 4, 2018 at 7:52 pm
                 it   s here:
                 [68]https://github.com/abhijeet3922/topic-modelling-on-wi
                 ki-corpus/blob/master/wiki_topic_model.py
                 [69]likelike
                 [70]reply
                   1.
                  [71]abhijeet kumar says:
                      [72]december 5, 2018 at 10:41 am
                      hey,
                         words,ids = dictionary.filter_n_most_frequent(50)
                      print words,   \n\n   ,ids   
                      this was just to see the most frequent words in the
                      corpus.
                      that will be wrong to remove frequent 50 words. i
                      think forgot to comment those 2 lines before
                      pushing. thanks for pointing out.
                      [73]likelike
                      [74]reply

leave a reply [75]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [76]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [77]log out /
   [78]change )
   google photo

   you are commenting using your google account. ( [79]log out /
   [80]change )
   twitter picture

   you are commenting using your twitter account. ( [81]log out /
   [82]change )
   facebook photo

   you are commenting using your facebook account. ( [83]log out /
   [84]change )
   [85]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   advertisements

   search for: ____________________ (button) search

categories

     * [86]applications
          + [87]id161
          + [88]natural language processing
          + [89]speech analytics
     * [90]data analytics
     * [91]deep neural network
     * [92]miscellaneous
          + [93]distance metrics
     * [94]supervised learning
          + [95]classifiers
     * [96]unsupervised learning
          + [97]id91

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   join 854 other followers

   ____________________

   (button) follow

archives

   archives [select month_______]

visitors

     * 255,347 hits

   advertisements

      2019 [98]machine learning in action

   [99]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [100]cancel reblog post

   iframe: [101]likes-master

   %d bloggers like this:

references

   visible links
   1. https://appliedmachinelearning.blog/feed/
   2. https://appliedmachinelearning.blog/comments/feed/
   3. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/feed/
   4. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
   5. https://appliedmachinelearning.blog/2017/10/13/topic-modelling-part-3-document-id91-exploration-theme-extraction-from-simplewiki-articles/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/&for=wpcom-auto-discovery
   8. https://appliedmachinelearning.blog/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#main
  11. https://appliedmachinelearning.blog/
  12. https://appliedmachinelearning.blog/presentations/
  13. https://appliedmachinelearning.blog/about/
  14. https://appliedmachinelearning.blog/contact/
  15. http://www.facebook.com/
  16. http://www.linkedin.com/
  17. http://www.twitter.com/
  18. http://www.instagram.com/
  19. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  20. https://appliedmachinelearning.blog/
  21. https://appliedmachinelearning.blog/category/unsupervised-learning/id91/
  22. https://appliedmachinelearning.blog/category/applications/natural-language-processing/
  23. https://appliedmachinelearning.blog/category/unsupervised-learning/
  24. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  25. https://appliedmachinelearning.blog/author/abhijeetchar/
  26. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comments
  27. https://appliedmachinelearning.wordpress.com/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  28. https://github.com/abhijeet3922/topic-modelling-on-wiki-corpus/blob/master/installation
  29. https://www.dropbox.com/s/6th96yql2pas1jh/topics.txt?dl=0
  30. https://appliedmachinelearning.wordpress.com/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  31. https://github.com/abhijeet3922/topic-modelling-on-wiki-corpus
  32. https://www.dropbox.com/s/fc1rr3k9oyhb31h/docs_wiki.pkl?dl=0
  33. https://www.dropbox.com/s/v3ww0l064plvv1a/lda_model_sym_wiki.pkl?dl=0
  34. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?share=facebook
  35. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?share=jetpack-whatsapp
  36. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?share=twitter
  37. https://appliedmachinelearning.blog/tag/bag-of-words/
  38. https://appliedmachinelearning.blog/tag/document-id91-python/
  39. https://appliedmachinelearning.blog/tag/gensim-lda/
  40. https://appliedmachinelearning.blog/tag/latent-dirichlet-allocation/
  41. https://appliedmachinelearning.blog/tag/nltk-lemmatization/
  42. https://appliedmachinelearning.blog/tag/python-topic-modelling/
  43. https://appliedmachinelearning.blog/tag/topic-discovery/
  44. https://appliedmachinelearning.blog/tag/topic-word-cloud/
  45. https://appliedmachinelearning.blog/tag/wikipedia-topic-model/
  46. https://appliedmachinelearning.blog/tag/word-dictionary/
  47. https://appliedmachinelearning.blog/author/abhijeetchar/
  48. https://appliedmachinelearning.blog/2017/08/28/topic-modelling-part-1-creating-article-corpus-from-simple-wikipedia-dump/
  49. https://appliedmachinelearning.blog/2017/10/13/topic-modelling-part-3-document-id91-exploration-theme-extraction-from-simplewiki-articles/
  50. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#respond
  51. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-464
  52. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=464&_wpnonce=a5784730b5
  53. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=464#respond
  54. https://appliedmachinelearning.wordpress.com/
  55. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-465
  56. https://radimrehurek.com/gensim/wiki.html
  57. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=465&_wpnonce=7e23b7f175
  58. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=465#respond
  59. https://appliedmachinelearning.wordpress.com/2017/10/13/topic-modelling-part-3-document-id91-exploration-theme-extraction-from-simplewiki-articles/
  60. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-2210
  61. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=2210&_wpnonce=c433983542
  62. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=2210#respond
  63. http://appliedmachinelearning.blog/
  64. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-2211
  65. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=2211&_wpnonce=11054930c5
  66. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=2211#respond
  67. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-2231
  68. https://github.com/abhijeet3922/topic-modelling-on-wiki-corpus/blob/master/wiki_topic_model.py
  69. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=2231&_wpnonce=00f94fcb12
  70. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=2231#respond
  71. http://appliedmachinelearning.blog/
  72. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-2246
  73. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?like_comment=2246&_wpnonce=d83fee8c97
  74. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/?replytocom=2246#respond
  75. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#respond
  76. https://gravatar.com/site/signup/
  77. javascript:highlandercomments.doexternallogout( 'wordpress' );
  78. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  79. javascript:highlandercomments.doexternallogout( 'googleplus' );
  80. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  81. javascript:highlandercomments.doexternallogout( 'twitter' );
  82. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  83. javascript:highlandercomments.doexternallogout( 'facebook' );
  84. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
  85. javascript:highlandercomments.cancelexternalwindow();
  86. https://appliedmachinelearning.blog/category/applications/
  87. https://appliedmachinelearning.blog/category/applications/computer-vision/
  88. https://appliedmachinelearning.blog/category/applications/natural-language-processing/
  89. https://appliedmachinelearning.blog/category/applications/speech-analytics/
  90. https://appliedmachinelearning.blog/category/data-analytics/
  91. https://appliedmachinelearning.blog/category/deep-neural-network/
  92. https://appliedmachinelearning.blog/category/miscellaneous/
  93. https://appliedmachinelearning.blog/category/miscellaneous/distance-metrics/
  94. https://appliedmachinelearning.blog/category/supervised-learning/
  95. https://appliedmachinelearning.blog/category/supervised-learning/classifiers/
  96. https://appliedmachinelearning.blog/category/unsupervised-learning/
  97. https://appliedmachinelearning.blog/category/unsupervised-learning/id91/
  98. https://appliedmachinelearning.blog/
  99. https://wordpress.com/?ref=footer_custom_blog
 100. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
 101. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 103. https://appliedmachinelearning.blog/
 104. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-form-guest
 105. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-form-load-service:wordpress.com
 106. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-form-load-service:twitter
 107. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/#comment-form-load-service:facebook
 108. https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/
