deepdriving: learning affordance for direct perception in autonomous driving

                                [teaser.jpg]

                                  abstract

      today, there are two major paradigms for vision-based autonomous
    driving systems: mediated perception approaches that parse an entire
    scene to make a driving decision, and behavior reflex approaches that
   directly map an input image to a driving action by a regressor. in this
   paper, we propose a third paradigm: a direct perception based approach
     to estimate the affordance for driving. we propose to map an input
     image to a small number of key perception indicators that directly
      relate to the affordance of a road/traffic state for driving. our
    representation provides a set of compact yet complete descriptions of
   the scene to enable a simple controller to drive autonomously. falling
   in between the two extremes of mediated perception and behavior reflex,
    we argue that our direct perception representation provides the right
         level of abstraction. to demonstrate this, we train a deep
   convolutional neural network (id98) using 12 hours of human driving in a
    video game and show that our model can work well to drive a car in a
      very diverse set of virtual environments. finally, we also train
    another id98 for car distance estimation on the kitti dataset, results
    show that the direct perception approach can generalize well to real
                               driving images.

                                    paper

     * c. chen, a. seff, a. kornhauser and j. xiao.
       [1]deepdriving: learning affordance for direct perception in
       autonomous driving
       proceedings of 15th ieee international conference on computer
       vision (iccv2015)

                     video (download [2]raw video here)

               iframe: [3]//www.youtube.com/embed/5hfvoxv9gii

          see also: new [4]video on making u-turns with deepdriving

                            source code and data

     * [5]deepdrivingcode_v1.zip: source code and setup file. this is the
       old version, please use the new one instead.
     * [6]deepdrivingcode_v2.zip: source code and setup file. please use
       this version.
     * [7]torcs_trainset.zip: the training set is in leveldb format, the
       images are in the integer data field, while the corresponding
       labels are in the float data field.
     * [8]torcs_baseline_testset.zip: a fixed testset to compare with the
       baselines (caltech lane detector and gist).
     * note: our real testing is to let the convnet to drive the car in
       the game. so the testing images are generated on-the-fly.

                               acknowledgement

    this work is partially supported by gift funds from google, intel and
   project x grant to the princeton vision group, and a hardware donation
                                from nvidia.

references

   1. http://deepdriving.cs.princeton.edu/paper.pdf
   2. http://deepdriving.cs.princeton.edu/deepdriving_with_audio.mp4
   3. http://www.youtube.com/embed/5hfvoxv9gii
   4. https://www.youtube.com/watch?v=wihhccq9nqk
   5. http://deepdriving.cs.princeton.edu/./deepdrivingcode_v1.zip
   6. http://deepdriving.cs.princeton.edu/./deepdrivingcode_v2.zip
   7. http://deepdriving.cs.princeton.edu/./torcs_trainset.zip
   8. http://deepdriving.cs.princeton.edu/./torcs_baseline_testset.zip
