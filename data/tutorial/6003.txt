   jan h  nermann
    1. [1]home
    2. [2]blog

    1. [3]notes

                               [4]jan h  nermann

   [qvgxoqaaabz0rvh0y29tbwvudabmyxzjntcumta3ljewmldvkqsaaaaldevydgrhdgu6y3
   jlyxrladiwmtktmdetmddumtc6ndq6mdirmde6mdcizce4aaaajxrfwhrkyxrlom1vzglme
   qaymde5ltaxlta3vde3ojq0ojaykzaxojaw+tmfbaaaaabjru5erkjggg==]

   01. may 2017

                        self-driving cars in the browser

   using id23, the goal of this project was to create a
   fully self-learning agent, that would be able to control a car in a 2d
   bottom-down environment. written solely in javascript.

   filed under
     * id23,
     * simulation,
     * ddpg

   note: this works only in modern browsers, so make sure you are on the
   newest version     

   this is a project i have been working on for quite some time now. these
   cars learned how to drive by themselves. they got feedback on what good
   and what bad actions are based on their current speed as a form of
   reward. powered by a neural network.

   you can drag the mouse to draw obstacles, which the cars must avoid.
   play around with this demo and get excited about machine learning!

   the following is a more detailed description of how this works. you may
   stop reading here and just play with the demo if you're not interested
   in the technical background!

   the demo is loading and is ready in one second... but we need
   javascript to work.

  concepts

   a short introduction to a few id23 concepts.
    1. agent: the agent, in this case, is the driver of the car.
    2. action: at each timestep
       [math: <semantics><mrow><mi>t</mi></mrow><annotation
       encoding="application/x-tex">t</annotation></semantics> :math]
       t the agent has to take an action
       [math:
       <semantics><mrow><msub><mi>a</mi><mi>t</mi></msub></mrow><annotatio
       n encoding="application/x-tex">a_t</annotation></semantics> :math]
       at   . this may for example be steering left, going faster or
       braking.
    3. state: the state
       [math:
       <semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotatio
       n encoding="application/x-tex">s_t</annotation></semantics> :math]
       st    is a vector that describes the environment the agent is
       currently in. it contains the information the agent uses to make a
       reasonable decision on what to do. after each action the state is
       updated to reflect the changes in the environment.
    4. reward: after taking an action in a state, the agent receives a
       reward
       [math:
       <semantics><mrow><msub><mi>r</mi><mi>t</mi></msub></mrow><annotatio
       n encoding="application/x-tex">r_t</annotation></semantics> :math]
       rt   , which describes how good the action he took was. the goal of
       the agent is to maximise this reward.
    5. id23: learning what actions to take in order to
       maximise a given reward function. in math words: learning
       [math:
       <semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mi>  </
       mi><mo>(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>)</mo></mrow><ann
       otation encoding="application/x-tex">a_t =
       \pi(s_t)</annotation></semantics> :math]
       at   =  (st   ) that maximizes the cumulative future reward.

  neural networks

   the agents learn by adjusting the weights of their neural network
   (function approximators). in this case, this involves two neural
   networks: one state to action net (3-layer, 150 neurons), one state +
   action to q-value net (2-layer, 200 neurons). the q-value describes how
   good an action is. by learning the second network, "the value network",
   you can obtain policy gradients, which you can then use to learn the
   first network. the first network, "the actor network", is now your
   decision maker. this algorithm is called "deep deterministic policy
   gradient" or in short ddpg. combining this with state-of-the-art
   techniques, such as prioritised experience replay buffers, relu
   non-linearites and the adam learner, results in the cars you can see
   above. even though this, at first, might seem reasonable, a lot of
   trouble with neural networks these days is the hyper-parameter search.
   there are at least a dozen of parameters you need to tune in order to
   achieve optimal results, which is kind of a drawback. in the future
   this might be overcome by automatic hyper-parameter search, which
   iterates over a set of hyper-parameters and finds the best.

   note however, that the demo you see above doesn't train the neural
   networks, therefore they are not learning (this is done offline).
   [wcaaabrsurbvajxy2bgzgjmywvjz+bgygdg4oti5uhl4xcqbpkehevexcqljkwkzwqzggt
   l5buulzrvvawbhazznxunts1thv0qh0fp38dqynjelaemzptmlswhtayok2sbw2sbceda2s
   7ewq7iaada3gmrhlbypwaaacv0rvh0zgf0ztpjcmvhdguamjaxos0wmy0ynfqxnzo0mzowm
   cswmtowmhrndywaaaaldevydgrhdgu6bw9kawz5adiwmtgtmtetmjbumtm6mze6mzarmde6
   mdc0caqraaaaaelftksuqmcc]
   [ddpg_181120_123128_d9fecbfd28b01c59e415f79dbf3536e6.jpg]

   deep deterministic policy gradients (ddpg).

  sensors

   the state (or the input to the neural nets) of the agent consists of
   two time-steps, the current time-step and the previous time-step. this
   helps the agent make decisions based on how things moved over time. for
   each time-step the agent receives information about its environment.
   this includes 19-distance sensors, which are arranged in different
   angles. you can think of these sensors as beams, that stop when they
   hit an object. the shorter the beam, the higher the input to the agent
   (0     for no hit, 1     for a very short beam). in addition, a time-step
   contains the current speed of the agent. in total, the input to the
   neural networks is 158-dimensional.

   imagine sitting in a room with a computer, looking at 158-numbers on
   the screen and having to press left or right in order to increase some
   kind of number, namely the reward. that is what this agent is doing.
   isn   t that crazy?

  exploration

   a major issue with ddpg is exploration. in regular id25 (deep
   q-networks) you have discrete actions from which you can choose from.
   so you can easily mix up your action-state-space by epsilon-greedily
   randomising actions. in continuous spaces (as the case with ddpg) this
   is not as easy. in this project i used dropout as a way to explore.
   this means dropping some neurons of the last layer of the actor network
   randomly and therefore obtaining some kind of variation in actions.

  multi-agent learning

   in addition to applying dropout to the actor network, i put 4 agents
   into the virtual environment at the same time. all these agents share
   the same value network, but have different actors and therefore have
   different approaches to different states, thus every agent explores
   different areas of the state-action space. all in all this resulted in
   better and faster convergence.

   if you want to hear more on the progress of the project as i add new
   features, i encourage you to follow me on twitter [5]@janhuenermann!
   additionally feel free to share the project in social media, so more
   people can get excited about ai!

    code

   the code for the demo above along with the javascript library neurojs
   i specifically made for this project is available on [6]github. it
   currently has 4.1k stars.

   made with in munich

references

   1. https://janhuenermann.com/
   2. https://janhuenermann.com/blog
   3. https://janhuenermann.com/notes
   4. https://janhuenermann.com/
   5. https://twitter.com/janhuenermann
   6. https://github.com/janhuenermann/neurojs
