   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   june 13, 2017     5 minute read

learning from human preferences

   one step towards building safe ai systems is to remove the need for
   humans to write goal functions, since using a simple proxy for a
   complex goal, or getting the complex goal a bit wrong, can lead to
   [11]undesirable and even dangerous behavior. in [12]collaboration with
   deepmind's safety team, we've developed an algorithm which can infer
   what humans want by being told which of two proposed behaviors is
   better.

   [13]read paper
     __________________________________________________________________

   we present a learning algorithm that uses small amounts of human
   feedback to solve modern rl environments. machine learning systems with
   human feedback [14]have [15]been [16]explored [17]before, but we've
   scaled up the approach to be able to work on much more complicated
   tasks. our algorithm needed 900 bits of feedback from a human evaluator
   to learn to backflip     a seemingly simple task which is simple to judge
   but [18]challenging to specify.
   [humanfeedbackjump.gif] our algorithm learned to backflip using around
   900 individual bits of feedback from the human evaluator.

   the overall training process is a 3-step feedback cycle between the
   human, the agent   s understanding of the goal, and the rl training.
   [diagram-4.png]

   our ai agent starts by acting randomly in the environment.
   periodically, two video clips of its behavior are given to a human, and
   the human decides which of the two clips is closest to fulfilling its
   goal     in this case, a backflip. the ai gradually builds a model of the
   goal of the task by finding the reward function that best explains the
   human   s judgments. it then uses rl to learn how to achieve that goal.
   as its behavior improves, it continues to ask for human feedback on
   trajectory pairs where it's most uncertain about which is better, and
   further refines its understanding of the goal.

   our approach demonstrates promising sample efficiency     as stated
   previously, the backflip video required under 1000 bits of human
   feedback. it took less than an hour of a human evaluator's time, while
   in the background the policy accumulated about 70 hours of overall
   experience (simulated at a much faster rate than real-time.) we will
   continue to work on reducing the amount of feedback a human needs to
   supply. you can see a sped-up version of the training process in the
   following video.

   iframe: [19]https://www.youtube.com/embed/oc7cw3fu3gu

   we've tested our method on a number of tasks in the simulated robotics
   and atari domains (without being given access to the reward function:
   so in atari, without having access to the game score). our agents can
   learn from human feedback to achieve strong and sometimes superhuman
   performance in many of the environments we tested. in the following
   animation you can see agents trained with our technique playing a
   variety of atari games. the horizontal bar on the right hand side of
   each frame represent's each agents prediction about how much a human
   evaluator would approve of their current behavior. these visualizations
   indicate that agents trained with human feedback learn to value oxygen
   in seaquest (left), anticipate rewards in breakout and pong (center),
   or work out how to recover from crashes in enduro (right).
   seaquest.gif
   spaceinvadersbehavior.gif
   pong.gif
   enduro.gif

   note there's no need for the feedback to align with the environment's
   normal reward function: we can, for example, train our agents to
   precisely keep even with other cars in enduro rather than maximizing
   game score by passing them. we also sometimes find that learning from
   feedback does better than id23 with the normal reward
   function, because the human shapes the reward better than whoever wrote
   the environment's reward.
   [enduro.gif]

challenges

   our algorithm's performance is only as good as the human evaluator's
   intuition about what behaviors look correct, so if the human doesn't
   have a good grasp of the task they may not offer as much helpful
   feedback. relatedly, in some domains our system can result in agents
   adopting policies that trick the evaluators. for example, a robot which
   was supposed to grasp items instead positioned its manipulator in
   between the camera and the object so that it only appeared to be
   grasping it, as shown below.

   we addressed this particular problem by adding in visual cues (the
   thick white lines in the above animation) to make it easy for the human
   evaluators to estimate depth.
     __________________________________________________________________

   the research described in this post was done in collaboration with jan
   leike, miljan martic, and shane legg at deepmind. our two organizations
   plan to continue to collaborate on topics that touch on long-term ai
   safety. we think that techniques like this are a step towards safe ai
   systems capable of learning human-centric goals, and can complement and
   extend existing approaches like reinforcement and imitation learning.
   this post is representative of the work done by openai's safety team;
   if you're interested in working on problems like this, please [20]join
   us!
     __________________________________________________________________

   footnote:

   good_backflip.gif
   bad_backflip.gif

   by comparison, we took two hours to write our own reward function (the
   animation in the above right) to get a robot to backflip, and though it
   succeeds it's a lot less elegant than the one trained simply through
   human feedback (top left). we think there are many cases where human
   feedback could let us specify a specific goal more intuitively and
   quickly than is possible by manually hand-crafting the objective.

   you can replicate this backflip in [21]gym with the following reward
   function for hopper:
def reward_fn(a, ob):
    backroll = -ob[7]
    height = ob[0]
    vel_act = a[0] * ob[8] + a[1] * ob[9] + a[2] * ob[10]
    backslide = -ob[5]
    return backroll * (1.0 + .3 * height + .1 * vel_act + .05 * backslide)
     __________________________________________________________________

   authors
   [22]dario amodei[23]paul christiano[24]alex ray
     __________________________________________________________________

     * [25]about
     * [26]progress
     * [27]resources
     * [28]blog
     * [29]charter
     * [30]jobs
     * [31]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1606.06565
  12. https://deepmind.com/blog/learning-through-human-feedback/
  13. https://arxiv.org/abs/1706.03741
  14. https://papers.nips.cc/paper/4805-a-bayesian-approach-for-policy-learning-from-trajectory-preference-queries
  15. https://link.springer.com/chapter/10.1007/978-3-319-02675-6_46
  16. https://arxiv.org/abs/1208.0984
  17. https://hal.inria.fr/hal-00980839
  18. https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/#bflip
  19. https://www.youtube.com/embed/oc7cw3fu3gu
  20. https://openai.com/jobs/
  21. http://github.com/openai/gym
  22. https://openai.com/blog/authors/dario-amodei/
  23. https://openai.com/blog/authors/paul/
  24. https://openai.com/blog/authors/alex-ray/
  25. https://openai.com/about/
  26. https://openai.com/progress/
  27. https://openai.com/resources/
  28. https://openai.com/blog/
  29. https://openai.com/charter/
  30. https://openai.com/jobs/
  31. https://openai.com/press/

   hidden links:
  33. https://openai.com/
  34. https://openai.com/
  35. https://twitter.com/openai
  36. https://www.facebook.com/openai.research
