   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]coastline automation
     * [9]99-line steering model
     * [10]coastline home
     __________________________________________________________________

training a deep learning model to steer a car in 99 lines of code

the magical power of deep learning in 2017.

   [11]go to the profile of matt harvey
   [12]matt harvey (button) blockedunblock (button) followfollowing
   jan 24, 2017

   deep learning in 2017 is magical. we get to apply immensely complex
   algorithms to equally complex problems without having to spend all our
   time writing the algorithms ourselves. instead, thanks to libraries
   like [13]tensorflow and [14]keras, we get to focus on the fun stuff:
   model architecture, parameter tuning and data augmentation.

   today, we   ll explore one such application of deep learning. we   ll use
   the [15]udacity self-driving car nanodegree program simulator to train
   a generalized steering model in under 100 lines of code. (as a point of
   reference, wordpress    get_permalink() method, which retrieves the url
   of a wordpress post, [16]is 124 lines!)

     note: quite a few people have written about their solutions to this
     udacity simulator steering problem on medium. i highly recommend
     vivek yadav   s post, [17]an augmentation based deep neural network
     approach to learn human driving behavior, which presents a wealth of
     information on the problem and the power of data augmentation.

   without further ado, here   s our entire solution: 99 lines including
   comments and white space:

   iframe: [18]/media/43e9577600c28e81a4e0bc352af24edd?postid=ba94e0456e6a

     note: shoutout to [19]whoopska on reddit for the code review!

setup

   the udacity simulator has two modes, training and autonomous, and two
   tracks. the challenge is to train only on track 1 and come up with a
   model that can drive itself around track 2, which has significantly
   different features.

   to do this, we first drive the car around in training mode on track 1
   for five laps to generate our data, doing our best to keep the car in
   the center. from there, the above code takes over. let   s go through
   each part:
   [1*1xig_n243desu7atjsligq.png]

the model

   the model is a convnet that draws inspiration from a slew of sources
   including [20]nvidia, [21]vgg16 and mr. yadav   s post, linked above. we
   stack five convolution layers, which grow in width each layer, with max
   pooling between them for spatial reduction (and memory friendliness).
   then we top it off with a couple dense layers, and use a linear
   activation to output our continuous steering angle. we use    elu   
   activations across the board the adam optimizer, both of which were
   found to help the model converge quickly.

processing the log

   the udacity simulator spits out the data it records into a csv. it
   stores the path to three images for each frame: left image, center
   image and right image. this is where the nvidia influence really comes
   in strong:

   we actually throw out the center images entirely because the left and
   right images hold so much information. for each sample, we add 0.4
   steering angle to the left image, and -0.4 angle for the right image.
   the intuition here is that if our center image (as we   d see when
   actually driving with the model) looks like the right image when we   re
   going straight, then we probably need to veer to the left a little, and
   vice versa.

   the 0.4 value is pretty aggressive and was found through trial and
   error. others who have solved this problem report using 0.2 or 0.25.
   although those values worked great for track 1, generalizing for track
   2 required us to go big or go home, and 0.4 worked best.

processing the images

   we have to pass our images to our neural net as numpy arrays for
   training, so that is taken care of in the image processing function
   (and we normalize the values between -0.5 and 0.5).

   we also do some data augmentation here to help generalize our model,
   which is critical for getting it to work well on track 2. for each
   image, we randomly shift it vertically between -20 and +20%. this
   vertical shift helps the model perform significantly better on both
   tracks. we also randomly apply a darkened area to each image. one of
   the challenges with only training on track 1 is that track 2 has a lot
   of harsh shadows, which really threw off the model   s ability to stay on
   course. applying a random darkened box to each image solved this issue
   effectively.

   last, we double the number of training samples we have by randomly
   performing a horizontal flip on the images, with a corresponding
   inversion of the steering angle. this has the added effect of
   neutralizing steering bias from imbalanced training data.

training & generator

   since we   re dealing with image data, we can   t load it all up into
   memory at the same time, so we use kera   s awesome fit_generator
   function. this function accepts a python generator as an argument,
   which yields the data.

   our generator randomly chooses batch_size samples from our x/y pairs,
   passes the image through our processor, and returns the batch for
   training. we used a batch size of 64, which performed significantly
   better than smaller batch sizes (32) and much larger ones (256/512).
   more exploration should be done here to better understand why.

steering performance

   the model was able to drive itself around track 1 flawlessly after just
   a single epoch of training on 20,224 samples, which takes all of 64
   seconds on my geforce 960m. it   s really quite amazing how quickly the
   model is able to perform on this track.

   iframe: [22]/media/0b06933bd5cb8a00e03693ede6f78cd1?postid=ba94e0456e6a

   the model breezes through track 1. note: this used 0.25 for the
   steering angles, rather than the 0.4 for the angles on track 2. i
   recorded this one before moving to the more aggressive angles.

   generalizing for track 2 took longer, going 8 epochs before
   circumnavigating the whole track. it took a while for the model to
   learn how to deal with shadows and other very dark areas, relative to
   the training data.

   iframe: [23]/media/bec113b88c5341ea03f24988ac77c05e?postid=ba94e0456e6a

   the model solves track 2. before applying a random darkened area to the
   images, the model would fail any time it encountered a shadow. you can
   see it still gets pretty close to the edge at times, but finds its way
   back.

   so there you have it, a generalized steering model in under 100 lines
   of code. now, a few caveats and notes:
     * i   m not generally for attempting to solve problems under some
       arbitrary code limit. the purpose of this post is to show how
       current libraries allow us to focus on the application of machine
       learning, instead of getting caught in the weeds writing
       sophisticated algorithms.
     * although it didn   t take much code, this project did take quite a
       lot of experimenting and research to find good network parameters
       and data augmentation techniques. but that   s the beauty of it: i
       was able to spend all my time there, rather than on the nitty
       gritty.
     * this is only possible because of libraries like [24]tensorflow,
       [25]tflearn and [26]keras. the biggest hat tip to those who spend
       their time developing and open sourcing these projects.
     * i   m not in the udacity self-driving car program, but my medium feed
       is flooded with posts by those who are, so i got inspired. :)

   have suggestions for how i could improve my solution? i   d love to hear
   it. thanks for reading!

     * [27]machine learning
     * [28]deep learning
     * [29]ai
     * [30]udacity
     * [31]id101

   (button)
   (button)
   (button) 256 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of matt harvey

[33]matt harvey

   founder of coastline automation, using ai to make every car
   crash-proof.

     (button) follow
   [34]coastline automation

[35]coastline automation

   practical applications of deep learning and research reports from the
   road.

     * (button)
       (button) 256
     * (button)
     *
     *

   [36]coastline automation
   never miss a story from coastline automation, when you sign up for
   medium. [37]learn more
   never miss a story from coastline automation
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.coast.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ba94e0456e6a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.coast.ai/training-a-deep-learning-model-to-steer-a-car-in-99-lines-of-code-ba94e0456e6a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.coast.ai/training-a-deep-learning-model-to-steer-a-car-in-99-lines-of-code-ba94e0456e6a&source=--------------------------nav_reg&operation=register
   8. https://blog.coast.ai/?source=logo-lo_trgsrnzskj9p---1336970de061
   9. https://blog.coast.ai/training-a-deep-learning-model-to-steer-a-car-in-99-lines-of-code-ba94e0456e6a
  10. https://coast.ai/
  11. https://blog.coast.ai/@harvitronix?source=post_header_lockup
  12. https://blog.coast.ai/@harvitronix
  13. https://tensorflow.org/
  14. https://keras.io/
  15. https://www.udacity.com/drive
  16. https://core.trac.wordpress.org/browser/tags/4.7/src/wp-includes/link-template.php#l118
  17. https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9
  18. https://blog.coast.ai/media/43e9577600c28e81a4e0bc352af24edd?postid=ba94e0456e6a
  19. https://www.reddit.com/r/machinelearning/comments/5qbjz7/p_an_autonomous_vehicle_steering_model_in_99/dcyphps/
  20. https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/
  21. https://arxiv.org/abs/1409.1556
  22. https://blog.coast.ai/media/0b06933bd5cb8a00e03693ede6f78cd1?postid=ba94e0456e6a
  23. https://blog.coast.ai/media/bec113b88c5341ea03f24988ac77c05e?postid=ba94e0456e6a
  24. https://www.tensorflow.org/
  25. http://tflearn.org/
  26. https://keras.io/
  27. https://blog.coast.ai/tagged/machine-learning?source=post
  28. https://blog.coast.ai/tagged/deep-learning?source=post
  29. https://blog.coast.ai/tagged/ai?source=post
  30. https://blog.coast.ai/tagged/udacity?source=post
  31. https://blog.coast.ai/tagged/autonomous-cars?source=post
  32. https://blog.coast.ai/@harvitronix?source=footer_card
  33. https://blog.coast.ai/@harvitronix
  34. https://blog.coast.ai/?source=footer_card
  35. https://blog.coast.ai/?source=footer_card
  36. https://blog.coast.ai/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/ba94e0456e6a/share/twitter
  40. https://medium.com/p/ba94e0456e6a/share/facebook
  41. https://medium.com/p/ba94e0456e6a/share/twitter
  42. https://medium.com/p/ba94e0456e6a/share/facebook
