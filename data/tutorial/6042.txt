   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]the startup
     * [9]top story
     * [10]write for us
     * [11]get smarter at building startups
     __________________________________________________________________

everything about self driving cars explained for non-engineers

   [12]go to the profile of aman agarwal
   [13]aman agarwal (button) blockedunblock (button) followfollowing
   apr 11, 2017

   i promise you won   t have to use either google or a dictionary while
   reading this. in this post i will teach you the core concepts about
   everything from    deep learning    to    id161   . using dead simple
   english.
   [1*9rq7hbmmkrzcoe31ronx6g.jpeg]

   you probably already know what self driving cars are and that they are
   considered the dope shit these days, so if you don   t mind i   m going to
   skip any high school essay-ish introduction. :)

     but i   m not skipping my own introduction: hi i   m [14]aman, i   m an
     engineer, and i have a low tolerance for unnecessarily
        sophisticated    talk. i write essays on medium to make hard things
     simple. simplicity is underrated.
     __________________________________________________________________

how do self-driving cars work?

   also called id101, they work on the combination of 3 cool
   fields of technology. here   s a brief introduction to each of them, and
   then we will go into depth. by the end of this essay, you will know
   enough about all these technologies to be able to hold an intelligent
   conversation with an engineer or investor in these fields. things like
      id158s    won   t sound like magic spells or sci-fi
   film words anymore.

   by the way, i   ve categorized some things into roughly separate
      systems   , but of course in practice these systems are all highly
   interconnected without clearly defined boundaries.

   id161 (ooooooohhhhh sounds so cool): the technology which
   allows the car to    see    its surroundings. these are the eyes and ears
   of the car. this whole system is called perception. basically we use:
   1. good old cameras, which are the most important (simple 2 megapixel
   cameras can work fine),
   2. radars, which are second-most important. they throw radio waves
   around, and like ultrasound you detect the waves that bounce off
   objects and come back,
   3. and lasers, which are cool to have but they are pretty expensive
   nowadays, and they don   t work when its raining or foggy. also called
      lidar   . you can say they   re like radar but give a little better
   picture quality, and lasers can go pretty far so you have greater range
   of view. the lasers are usually placed in a spinning wheel on top of
   the car so they spin around very fast, looking at the environment
   around them. here you can see a lidar placed on top of the google car:
   [1*1cacag4lfh_xqprmbdc9pa.jpeg]
   check out the lidar sensor on top. it   s a spinning laser beam.

   deep learning: it is a technology that allows the car to make driving
   decisions on its own, based on information it gathered through the
   id161 stuff described above. this is what trains the    brain   
   of the car. we will go into detail about this in a minute.

   by combining the two even on a basic level, you can do some interesting
   things. here   s a project i made, detecting lane lines and other cars on
   the road using only a camera feed.

   iframe: [15]/media/fcbc6050e97f8db31adbc0dcc8a3093a?postid=f73997dcb60c

   robotics: you can see everything, and you can think and make decisions.
   but if your brain   s decisions (eg: lift the left leg) can   t reach the
   muscles of the leg, your leg won   t move and you won   t be able to walk.
   similarly, if your car has a    brain    (=a computer with deep learning
   software), the computer needs to connect with your car   s parts to be
   able to control the car. put simply, these connections and related
   functions make up    robotics   . it allows you to take the software
   brain   s decisions and use machinery to actually turn the steering,
   press and release the throttle, brakes etc.

   navigation: even after being given all the above, ultimately you still
   need to figure out    where    you are on the planet and the directions for
   where you want to go. there are several aspects to this, like gps (your
   good old navigation device, which takes location information from
   satellites), and stored maps, etc. you also mix in id161
   data.

   so the car controls its steering and brakes etc based on the decisions
   made by its brain, and these decisions are based on the information
   received through cameras and radars and lasers and the directions it
   receives from the navigation programs. this completes the whole system
   of a self-driving car.
     __________________________________________________________________

   tidbit (feel free to skip)
   self driving cars come in many    levels   , from level 1 to level 5, based
   on how independent the car is and how little human assistance it needs
   while driving. oh and there   s also level 0, which is your good old
   manually operated car.

   level 5 means the car will be 100% self-driving. it will not have a
   steering or brakes, because it   s not meant to be driven by people.
   these cars don   t exist yet, and cutting-edge cars are still at level 3
   and at most level 4.
     __________________________________________________________________

deep learning explained for chimpanzees like myself

   let   s say you were a wildlife safari enthusiast, and i was your
   super-idiot friend. i   m going for safari in africa next week. and you
   give me some advice:    aman, stay away from the fucking elephants.   

   and i ask you back,    what   s an elephant?   

   you will most likely say,    you stupid jerk, elephants are... okay never
   mind, here   s a photograph of an elephant, this is what it looks like.
   stay away from them.   
   [1*7jppemcbx8d7xmfyqthbsg.jpeg]

   and then i go off to safari.

   next week you get to know that i still managed to run into an elephant
   and almost ended up getting trampled. you ask me what happened.

   i reply,    i don   t know, i did see this huge animal but it didn   t look
   anything like the photo you showed me, so i thought it was safe to play
   with and i went ahead and pulled the little wagging thing. here   s the
   photograph of the animal i took before that      
   [1*spy7w7owtnhjymqtwbnh9a.jpeg]

   you:          .

   you:    okay, aman. i   m sorry, my bad. i expected too much from your
   brain. let me give you a    cheat code    which you need to follow when
   you   re on the safari next time. if you see anything that looks
   brown-ish from all angles, seems to have four leathery legs like
   pillars, large flapping ears, and a thick long nose coming out of its
   face like a big tube, and is fat and bigger than you are, then that   s
   an elephant and you need to stay away.   

   next month i go back to safari again (hey, it   s my hypothetical story
   and i can go to safari as many times as i want) and i don   t run into
   any elephants this time, because your    cheat code    works well.

   how did you come up with that    cheat code   ? it   s because you   ve already
   seen an elephant from all different sides, and you picked some features
   of an elephant which remain pretty same regardless of which angle you
   view the elephant from. so you had lots of data about elephants to
   think about, and that helped you to form a mental picture of the most
   obvious signs of an elephant, and gave them to me as a cheat code.
   realize that i don   t have to really    know    exactly what an elephant is,
   i just have a cheat code that helps me recognize an elephant. but that
   cheat code works almost as well as knowing what elephants are!

   but why wasn   t it okay to just show me one photograph (the one you
   showed earlier) and assume it was enough for me to get the idea?
   because i (being an idiot of course) took that photograph as the    holy
   truth            i assumed that every elephant will look *exactly* the same as
   that photograph, and will be a near perfect match.

   deep learning works in a very similar way.

   here   s the basis of deep learning: id158s, which
   i   ll explain now. they are also called deep neural networks.

   first, i assume you know a little bit of math from high school. you
   know what a matrix is, right? and that you can multiply a matrix with
   some other matrix? here   s a refresher:
   [1*45nui4fuh2wpiw_yzmqpaa.gif]

   remember what a matrix is.

   an id158 (ann) is some really fancy stuff but i   ll
   take you on the journey with baby steps. you see, the human brain is
   made up of a network of many cells called    neurons   , which is the
   inspiration for anns. this is what it looks like on paper:
   [1*tobl6xlerkwabswtafay_g.png]

   an ann is a magic box, which takes in an input and gives out an output.
   for example, suppose you wanted a magic ann which takes in a photograph
   and can tell if the photograph is of an elephant or not. you put a
   photo into the box, and out comes an answer    yes    or    no   .

   or, you put in a photograph of the road ahead and you want the ann to
   tell you if you should slow down or speed up. and the answer comes out,
      speed up!   

   how does it happen? how can you create this ann?

   put simply, an ann is like a simple version of a human brain. first you
   train it with data. when you give data to an ann, it creates a    cheat
   code    which helps it make decisions next time. (i   ll give a detailed
   example in a second) this    cheat code    is called    weights    of the ann.
   you saw that    matrix    earlier? you can say that the first matrix [x y]
   matrix is an input, and the second matrix [u w] matrix is the set of
   weights or cheat codes. when you multiply the input with the weights,
   you get an answer.

   the ann can be of many different varieties, and based on how you design
   it, can either give a    yes or no    answer, or, for example, it could
   give a specific number, or a list of different numbers, etc. you can
   choose what kind of output it will give and what kind of input it will
   receive, and how large and complex it will be, which makes it extremely
   versatile. you can build neural networks that take videos as inputs,
   voice samples, images, or text paragraphs, etc. of course, all these
   are converted into numbers on a computer automatically, and there will
   be a huge matrix of    weights    which will be multiplied to that input,
   generating an output which is your answer.

   but i   m sure you still don   t understand completely. how do you    train   
   the ann to give you meaningful outputs?

   let   s say you want to train an ann to recognize elephants.

   so you create a fresh ann which takes a certain type of input and gives
   out a certain type of output. at first, this ann doesn   t really know
   anything about the problem it   s trying to solve. it   s just like me from
   the original elephant example         before you showed me the first
   photograph of an elephant, i had no idea what an elephant even was. if
   you asked me right there and then to look for an elephant in a
   photograph, i   d probably have picked something randomly. so you can say
   that initially, i had some random bullshit cheat code right? but the
   moment you show me one elephant and tell me that it is an elephant,
   suddenly i adjust my random cheat code and now i have some idea of what
   elephants are. my cheat code is not random anymore, and if i saw an
   elephant from the front i   d probably pick the right answer! so i have
   updated my cheat code when i was given more information. and the more
   different photographs of elephants i have, the better my cheat code
   becomes at giving the right answer. i still don   t know    what    an
   elephant is, but i   ve seen enough correlation in different photographs
   to recognize the most special features of an elephant and stay away
   from them.

   similarly, when you first create an ann, you start by giving it a
   random set of weights (=cheat code) to start with. then you show it an
   input image (photograph of an elephant), and also tell it the output
   you expect (answer    yes   ). the beauty of anns is that if you give it an
   example input and its corresponding output, then it will adjust its
   cheat code so that it can repeat that correct answer next time. it will
   change the numbers in that matrix of weights the more data you give to
   it, to be able to match your answers. this is why, you can start with a
   random set of weights and with time the ann will adjust them so that
   they work well as a cheat code! cool right? that   s how the ann
      learns   . the more data you give it, the more it adjusts its weights,
   and the more accurate it becomes.

   as i explained earlier already, how this happens is through matrix
   multiplication. the matrix of weights for this ann will have values
   such that if you multiply it with your input image (the image will be
   converted into numbers), you can get an answer. usually, there   s not
   just one matrix of weights, but a series of matrices which you multiply
   one after the other. but the concept is the same as you see in the
   image below:
   [1*kqtpsss6h4y24m81chdcva.png]
   rough matrix representation (i made this diagram myself) :)

   but that   s not enough. remember that the ann is not smart         it works on
   cheat codes, after all. if you only gave it elephant photographs to
   look at, and keep saying    yes    for every photo, what do you think it
   will do?

   it will be lazy and assume that every photograph is an elephant! it
   will adjust its weights (cheat code) in such a way that regardless of
   the photo it is given, it will always output a    yes   . this is called
      underfitting   , which basically means you made the mistake of thinking
   that your idiot friend is smart. your ann has become biased towards a
   particular answer. to prevent this, you need to also feed it
   counter-examples which have the answer    no   . so you also mix in lots of
   example photos which are not of elephants, but even lions, cows, human
   beings, squirrels, pikachus and bulbasaurs, sea horses and    (sorry, i
   got carried away) and now the ann will be forced to adjust its cheat
   code more carefully. now you   re increasing its accuracy and making it
   more reliable! or, you can also have    overfitting   , when the ann   s
   cheat code is still lazy but it   s now so super smart that it begins to
   memorize the entire training data you give it! this often happens when
   your ann is very large and deep, so in a way it just begins to store
   all the information you give it. it becomes super good at answering on
   your training data but it fails when given a new example that it hasn   t
   encountered before. it   s no longer working like a cheat code, but
   rather it has become like a dictionary of images. you can prevent this
   by reducing the size of your model (by randomly dropping some of the
   weights), or by increasing the variety of data. the former is called
      dropout    (pretty straightforward but a very crazy idea), and the
   latter is called    balancing the dataset   .

   you have also learned your lesson from the safari example earlier, and
   now you will be careful to show it other photographs of elephants from
   many different angles so that it doesn   t make the same mistake that
   your idiot friend made. this is why researchers/engineers often
      augment    their training data sets to include mirror reflections or
   brighter/darker versions of the same images etc, to increase the
   variety of their data. this helps make sure that the ann can become as
   accurate as it can. you want the cheat code to be so super    robust   
   that it works as well as real humans, or better.

   by the end of the training, with enough balanced data and augmentation
   etc, you will have a trained ann (also called trained model) which can
   recognize elephants in images with reasonable accuracy, and also know
   when not to recognize an elephant.

      overfitting   , the word you encountered just two paragraphs ago
   (=network becoming too smart and acting like it has just memorized your
   data), is a very common term in the deep learning lingo and you now
   know what it means. (underfitting is a less troubling problem because
   it   s fairly obvious to diagnose and easy to tackle.) someone might say,
      oh man my model isn   t working so well on new data, i guess it is
   overfitting    and you   ll say,    did you try to augment the training
   data?    and you will instantly own the night. the other person might
   even assume you   re a deep learning engineer yourself! (if they do, tell
   them    oh no, i just follow aman on medium   . when they ask who i am,
   please pretend and say    oh you don   t know? he   s the coolest guy in this
   space!   ).

   the process of adjusting weights (updating the cheat code based on
   data) involves something called id26. while training,
   whenever you show the ann an example input (let   s say a kitten) and
   then tell it the answer (   no it   s not an elephant!   ), it will first try
   to use its current cheat code to come up with its own answer. if the
   answer is right, then it doesn   t need to adjust its cheat code, right?
   the lazy ann will update its cheat code only when it makes a mistake.
   so only if the ann   s answer was wrong, it will have to adjust its
   weights (=cheat code) by a tiny bit and test the weights again. this
   adjustment algorithm is called id26. the    mistake    made by
   the ann sends ripples through the matrix of weights, changing many of
   their values. so you can say that the error propagates back through the
   network. again, this is why you can always create the ann with random
   weights initially, and over time it will adjust them on its own.

   put simply, if you take a cheat code to an exam, and get some of your
   answers wrong, then you will likely upgrade your cheat codes for the
   next exam in that subject to have higher accuracy. the process of
   editing/rewriting your cheat code after every exam, based on its grade,
   is called id26.

   now scroll up to the graph image of a    neural network    i pasted above.
   you see that thing called the    hidden layer   ? well, the ann is made of
   layers of neurons, these neurons carry the weights as values. they are
   called    hidden layers    because you don   t need to know the exact weights
   in the network! you can print the values out, for sure         but it   s
   useless to look at a matrix which can run into millions or billions of
   small numbers. it   s a cheat code, and it updates itself based on the
   inputs you give it, and that   s all you need to know.

   this is why deep learning is often considered a    black box    system. if
   you   ve ever programmed before in any language, you are used to writing
   down explicit instructions for the program for everything. but here,
   you can   t see the underlying    cheat code    the computer created for
   itself to use in place of a written algorithm.

   coming back to self-driving cars, let   s say you took photographs from
   different cameras around the car at a particular millisecond, and took
   all the data collected from the radar and lidar (=lasers) too, and
   combined them together in a list, and used that list as the input for
   your car   s ann. and the output you expect from the network is a long
   list of the steering angle, the throttle/braking value, whether to
   light up the headlights or not, whether to honk or not, etc. these are
   your    labels    to be predicted by the ann.

   the kind of deep learning in which you explicitly train the ann using
   specific data gathered by humans, is called    supervised learning   . for
   each data sample, you have the data and you have the label.

   so here   s an example of how you will    train    the car   s brain. first you
   will simply drive the car normally by yourself, but keep collecting the
   input data from sensors and cameras etc. there is also equipment which
   measures, for each millisecond of data, the steering angle and throttle
   pressure etc that you made while driving. then once you get back home,
   you can start    training    the ann on all that data you collected while
   driving. the ann will update its cheat code to take your computer
   vision input and try to mimic your driving decisions as closely as
   possible. this is known as behavioural cloning, and it is what most car
   companies are doing nowadays         collecting driving data, and making
   their cars    practice   . after every training trip, the car will become
   better and better at making decisions. behavioural cloning is only used
   for small parts of the driving process.

everything else being similar, driving data is the biggest winning factor in
the race to develop self-driving cars.

   now         you know what    deep learning    is, and what it does. you know what
   deep neural networks are (= anns), and how they work (weights = cheat
   codes, id26 = adjusting the weights based on a given example
   test). you also know that you should have a large and balanced dataset
   so that the network doesn   t overfit (get lazy or too smart in setting
   its cheat codes).

   congratulations! i kid you not, this is more of an achievement than you
   think!

id161

   remember this scene from the terminator?
   [1*crhjtdi3wputih0gyg9vtq.jpeg]

   schwarzenegger enters a bar naked, and looks for someone to take
   clothes from. this must be what comes to your mind when you think
      id161   , and you are partially right.

   self driving cars can also see the world like that, but they are still
   nowhere near the sophistication you can see there. the major purpose of
   id161 techniques is to process the camera images to detect
   lane lines, track other vehicles and pedestrians, look for any bumps or
   holes in the road, measure the distance between the car and other
   objects etc.

   there   s no one technique or underlying technology for computer
   vision         rather, often it comes down to simple image processing. you
   are limited only by your imagination and ability to design complex
   algorithms, and if you   re good at geometry that is another plus. for
   example, a popular way of processing the camera images involves looking
   at how quickly the colors change in horizontal and vertical directions.
   that is called a    gradient    and it can be used to find edges. here   s an
   example:
   [1*kdvi0ag1fpaq6fzeu7-yew.jpeg]

   these are all created by using simple techniques using a popular
   library of id161 functions, called [16]opencv. you should
   know about opencv, because it is the most popular library for this
   purpose. every robotics or deep learning guy knows about it.

   taking the above example further, the images you see are simply a
   representation of ones and zeros on a rectangle. the ones are white,
   and the zeros are black! you can select the pixels in any one region of
   the picture and play around with it. this is how i detected lane lines
   and came up with my project output as seen at the beginning of this
   blog post.

   another common technique used in id161 to detect objects is
   called stereo vision. don   t be thrown off by the seemingly complex
   name, stereo vision simply means looking at something with more than
   one eye (you do it all the time). when you see something with two eyes,
   you have a better estimate of how far it is and what shape it has.
   similarly, cars use more than one camera on either side and combine the
   images to see the world more realistically.

   what about adverse conditions like snow, rain, etc when it   s difficult
   to see the road? well, let   s not forget that the other two sensors
   (radar and lasers). even when any two sensors don   t work so well, the
   system is designed such that third one should still be quite reliable
   (but it   s an ongoing field of research and we   re not perfect yet).
   moreover, if the car   s neural networks have already been trained to
   drive in such conditions, the car should have some idea of how to make
   decisions. again, as i said earlier, the foremost priority is about
   collecting enough data. the more you train, the better the car gets.
   period.

   i won   t go into more detail about id161, as it is a very
   broad field. just know that it   s not sci-fi, and there   s nothing
      magical    about it. it   s just about using your imagination to play with
   images. there are other techniques, which you can read about in this
   very short and simple page:
   [17]nova | the great robot race | what robots see (non-flash) | pbs
   (printable) in this slide show, learn about some of the world's largest
   and most ambitious darpa detection experiments.www.pbs.org

robotics and navigation

   actually, i   m not going to spend too much time here. robotics is pretty
   straightforward (you don   t need fancy imagination to understand what
   happens)    you basically need to know about something called an
   actuator.

   an actuator is a device which takes an electrical signal as input, and
   converts it into a physical action. it   s not too complicated, often it
   just has a motor inside it that rotates by a certain angle based on the
   value of the signal it receives. actuators come in all shapes and
   sizes, there will be one in the steering wheel, one for the throttle,
   brakes, gears, the engine etc. you get the idea.

   for navigation, apart from the gps/maps/id161 technologies,
   you should also know about a really cool technique called    dead
   reckoning   . it involves calculating your current position based on your
   speed and distance travelled and knowing the history of all the turns
   you made up until the present moment, etc. you remember the sucky
   sherlock holmes movie with robert downey jr? there   s a scene in which
   he gets kidnapped and taken inside a horse carriage and blindfolded,
   but after they arrive at the destination he magically knows exactly
   where he is. do watch the first few minutes of this scene and you   ll
   understand what    dead reckoning    is. the system works pretty much like
   sherlock holmes.

   iframe: [18]/media/afc31785219bec1728319ee5bfd56420?postid=f73997dcb60c

alright! now you know should have a very good idea about how self-driving
cars work. here is what we have learned:

    1. deep learning         cheat codes, balancing the training data,
       overfitting and other issues, why deep learning is a    black box   
       etc.
    2. id161         camera, radar and lidar sensors, stereo vision,
       how you can process images creatively to extract many different
       kinds of information, there   s a popular programming library called
       opencv.
    3. robotics         what actuators are.
    4. navigation         you learned that apart from the gps and stored maps
       etc, cars use a sherlock holmes -style technology called    dead
       reckoning   .

     so    do you feel more dangerous and awesome now than 20 minutes
     ago? :)

finally, some thoughts on the    ai will take our jobs and then kill us   
discussion

   i want you to notice an interesting thing about what you learned just
   now. you learned that every neural network is designed and trained to
   take a specific kind of input, and then give a specific kind of output.

   even if you take one huge neural network and used it to act as the
   brain of a self driving car, you   ll still need to connect it to inputs
   from all the sensors and cameras around the car, and the gps and maps,
   and also connect the outputs of the neural networks to all the hardware
   and actuators that make the car move. as you learned already, a neural
   network is a sequence of matrices multiplied or added to each other. if
   you don   t connect it to an input or an output, it   s a dead mathematical
   expression just like any other.

   but let   s say you irresponsibly take a poorly trained neural network
   and use it to run a dangerous machine like an eye surgery robot. it   s
   operating on an eye which has some very rare condition that neither the
   doctors nor the neural network has been trained to detect. the robot
   makes an error that permanently damages the eye. now in this
   hypothetical scenario, i   m sure that next day newspapers around the
   world will talk about how ai woke up and decided to kill a person. but
   now that you have read the entire essay above, i hope you can see why
   this is bullshit.

   you could make an argument, that hey - the whole universe, even human
   brains, are essentially made up of mathematical expressions at the
   deepest level, and thus some matrices are more evil than others. so
   maybe a neural network can potentially be evil. maybe while training
   the neural network, you rearranged its weights in such a way that it
   becomes evil.

   even if this could be true, it   s still not a good argument. even if
   that poorly made (or    evil   , as newspapers would call it) ai software
   was simply never installed on a real robot, it could never have made
   any mistakes.

   i personally feel that the use of ai in the real world should be
   regulated, to ensure that it doesn   t harm people. it should be similar
   to the medical industry         people should be free to develop a new drug
   in their lab in test tubes. but if you want to test your new drug on
   animals or human beings, you have to ask the government for permission
   and prove that it is ready to be tested.

   it   s all about how you decide to use a software. engineers should be
   responsible for building software properly, and also for deciding
   whether their software is ready to be used in the real world or not.
     __________________________________________________________________

and that   s it! if you found this helpful, let me know in the comments. i will
feel good about it.

   iframe: [19]/media/f89d74e34b316cf3876aeec18b225a6b?postid=f73997dcb60c

   i don   t use this to send you links or sell stuff. instead it   s a way
   for me to have discussions with you and learn from your stories and
   ideas.
     __________________________________________________________________

   oh and if you   re a scientist or tech company, and need some help in
   explaining your science to non-technical people for marketing, pr or
   training etc, i can help you. drop me a message on twitter: @mngrwl

     * [20]self driving cars
     * [21]tech
     * [22]world
     * [23]programming
     * [24]technology

   (button)
   (button)
   (button) 3.5k claps
   (button) (button) (button) 53 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of aman agarwal

[26]aman agarwal

   engineer, teacher, environmentalist. learner of foreign languages,
   lover of history, cinema and art.

     (button) follow
   [27]the startup

[28]the startup

   medium's largest publication for makers. subscribe to receive our top
   stories here     [29]https://goo.gl/zhclji

     * (button)
       (button) 3.5k
     * (button)
     *
     *

   [30]the startup
   never miss a story from the startup, when you sign up for medium.
   [31]learn more
   never miss a story from the startup
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f73997dcb60c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/swlh/everything-about-self-driving-cars-explained-for-non-engineers-f73997dcb60c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/swlh/everything-about-self-driving-cars-explained-for-non-engineers-f73997dcb60c&source=--------------------------nav_reg&operation=register
   8. https://medium.com/swlh?source=logo-lo_yf87hyniu9gz---f5af2b715248
   9. https://medium.com/swlh/how-we-got-11-3-million-pageviews-without-the-growth-hacking-bullshit-5e0456dcbe3
  10. https://medium.com/swlh/step-1-please-fill-out-the-form-below-c354ca4bad31
  11. https://growthsupply.com/the-startup-newsletter/?utm_source=tn&utm_medium=medium
  12. https://medium.com/@mngrwl?source=post_header_lockup
  13. https://medium.com/@mngrwl
  14. http://aman-agarwal.com/
  15. https://medium.com/media/fcbc6050e97f8db31adbc0dcc8a3093a?postid=f73997dcb60c
  16. http://opencv.org/
  17. http://www.pbs.org/wgbh/nova/darpa/see-nf.html
  18. https://medium.com/media/afc31785219bec1728319ee5bfd56420?postid=f73997dcb60c
  19. https://medium.com/media/f89d74e34b316cf3876aeec18b225a6b?postid=f73997dcb60c
  20. https://medium.com/tag/self-driving-cars?source=post
  21. https://medium.com/tag/tech?source=post
  22. https://medium.com/tag/world?source=post
  23. https://medium.com/tag/programming?source=post
  24. https://medium.com/tag/technology?source=post
  25. https://medium.com/@mngrwl?source=footer_card
  26. https://medium.com/@mngrwl
  27. https://medium.com/swlh?source=footer_card
  28. https://medium.com/swlh?source=footer_card
  29. https://goo.gl/zhclji
  30. https://medium.com/swlh
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. http://www.pbs.org/wgbh/nova/darpa/see-nf.html
  34. https://medium.com/p/f73997dcb60c/share/twitter
  35. https://medium.com/p/f73997dcb60c/share/facebook
  36. https://medium.com/p/f73997dcb60c/share/twitter
  37. https://medium.com/p/f73997dcb60c/share/facebook
