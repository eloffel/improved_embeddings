   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

vehicle detection and tracking

   [16]go to the profile of ivan kazakov
   [17]ivan kazakov (button) blockedunblock (button) followfollowing
   may 14, 2017

     [18]   you can absolutely be super-human with just cameras   . elon musk
     at ted.

   [1*92cbneuuwabyuiw-itgfbq.png]
     __________________________________________________________________

   this is the [19]udacity   s self-driving car engineer nanodegree program
   final project for the 1st term. source code and a more technically
   elaborated writeup are available on [20]github

the goal

   to write a software pipeline to identify vehicles in a video from a
   front-facing camera on a car.

   in my implementation, i used a deep learning approach to image
   recognition. specifically, i leveraged the extraordinary power of
   convolutional neural networks (id98s) to recognize images.

   however, the task at hand is not just to detect a vehicle   s presence,
   but rather to point to its location. it turns out id98s are suitable for
   these type of problems as well. there is a [21]lecture in [22]cs231n
   course dedicated specifically to localization, and the principle i   ve
   employed in my solution basically reflects the idea of a region
   proposal discussed in that lecture and implemented in the architectures
   such as [23]faster r-id98.

   the main idea is that since there is a binary classification problem
   (vehicle/non-vehicle), a model could be constructed in such a way that
   it would have an input size of a small training sample (e.g., 64x64)
   and a single-feature convolutional layer of 1x1 at the top, which
   output could be used as a id203 value for classification.

   having trained this type of a model, the input   s width and height
   dimensions can be expanded arbitrarily, transforming the output layer   s
   dimensions from 1x1 to a map with an aspect ratio approximately
   matching that of a new large input.

   essentially, this would be somewhat equal to:
    1. cutting new large input image into squares of the models    initial
       input size (e.g., 64x64)
    2. detecting the subject in each of those squares
    3. stitching the resulting detections back, preserving the same order
       as the corresponding squares in the source input into a map with
       the aspect ratio of the sides approximately matching that of a new
       large input image.

   [1*kkgzz-tn5rx0imd3akklvq.png]
   consider each of these squares to be processed individually by its own
   dedicated id98, producing a 4x20 detection map

data

   udacity equips students with the great resources for training the
   classifier. [24]vehicles and [25]non-vehicles samples of the [26]kitti
   vision benchmark suite have been used for training.

   the final model had difficulties in detecting the white lexus in the
   project video, so i augmented the dataset with about 200 samples of it.
   additionally, i used the same random image augmentation technique as in
   [27]project 2 for traffic signs classification, yielding about 1500
   images of vehicles from the project video. the total number of
   vehicle   s images used for training, validation, and testing was about
   7500. obviously each sample had been horizontally flipped, additionally
   inflating the dataset by a factor of 2. as a result, i had
   approximately 15000 data points.
   [1*oqgtgfafsrdo-hutyhb0-q.png]
   class ids are vehicles and non-vehicles

   an equal number of non-vehicle images has been added as negative
   examples.
   [1*jzorbyn5thje4dbzxvv7za.png]
   typical vehicle and non-vehicle samples with their corresponding labels

model

   i borrowed the technique of constructing the top of the network from
   [28]the implementation of max ritter, who apparently employed the same
   approach.

   a lot of model architectures with varying complexity have been tested
   to derive a final model.

   i started with a id21 from [29]vgg16 architecture with
   weights trained on [30]id163. vgg is a great and well-tested
   architecture, and id163 weights apparently assume that it should
   have an idea of the vehicle   s features. i added my top single-feature
   binary classifier and fine-tuned the model. as expected, it yielded a
   pretty high test accuracy of about 99,5%. the flip side with the vgg is
   that it is rather complex, making predictions computationally heavy.

   i then tested some custom id98 configurations of varying number of
   layers and shapes, incrementally reducing complexity and evaluating
   test accuracy, and finally arrived at the model with as little as about
   28,000 trainable parameters with test accuracy of still about 99.4%:
epoch 5/5
607/607 [==============================] - 48s - loss: 0.0063 - acc: 0.9923 - va
l_loss: 0.0073 - val_acc: 0.9926
evaluating accuracy on test set.
test accuracy:  [0.0065823850340600764, 0.99373970345963758]

   reducing the model   s complexity to the extreme is beneficial for both
   the computational cost of predictions and overfitting. although the
   dataset may seem not too big, it   s hard to assume that the model of
   28000 parameters may be able to memorize it. additionally, i   ve
   aggressively employed dropout to further mitigate the risk of
   overfitting.

   the model has been implemented and trained using [31]keras with
   [32]tensorflow backend.

   sample predictions results:
   [1*yknxd4ixrf4nv1s8-ecrwa.png]

using trained model for vehicle detection

   original frame from the video stream looks like this:
   [1*1v9u-qzgnswfbcvlbp262g.png]

   it is not strictly original, as it has already been subjected to
   undistortion, but that deserves a story of its [33]own. for the task at
   hand, this is the image to be processed by the vehicle detection
   pipeline.

   the region of interest for the vehicle detection starts at an
   approximately 400th pixel from the top and spans vertically for about
   260 pixels. thus, we have a region of interest with the dimensions of
   260x1280, starting at 400th pixel vertically.
   [1*-inhi1mc-lamry5ujrguqw.png]

   this transforms the dimensionality of the top convolutional layer from
   (?,1,1,1) to (?,25,153,1), where 25 and 153 are the height and width
   dimensions of a miniature map of predictions that, in turn, will
   ultimately be projected to the original high-resolution image.

   the vehicle scanning pipeline consists the following steps:
    1. obtain the region of interest (see above)

   2. produce the detection map using trained id98 model:
   [1*w-ufnexeawdovef0iqkh7g.png]

   3. apply the confidence threshold to generate the binary map:
   [1*90nzf1ep8kc33tg_tiotfw.png]

   the predictions are very polarized, that is, they mostly stick to ones
   and zeros for vehicles and non-vehicle points. therefore, even the
   midpoint of 0.5 for a confidence threshold might be a reliable choice.
   just to be on safe side, i stick with 0.7

   4. label the obtained detection areas with the label() function of the
   scipy.ndimage.measurements package. this step allows outlining the
   boundaries of labels that will, in turn, help to keep each detected
      island    within its feature label's bounds when building the heat map.

     this is also the first approximation of detected vehicles.

   5. project those featured labels of detection points to the coordinate
   space of the original image, transforming each point into a 64x64
   square and keeping those squares within the features    area bounds.

   just to illustrate the result of this points-to-squares transformation
   projected onto the original image:
   [1*rsejq3i-uijmlqjogl4ujq.png]

   6. create the heat map. overlapping squares from the image above are
   essentially building-up the    heat   .
   [1*dnxvy_owjfwcsczqdeakww.png]

   7. label the heat map again, producing the final    islands    for actual
   vehicles    bounding boxes. labeling of this particular heat map creates
   2    islands    of detections. obviously.

   8. save labeled features of the heat map to the list of labels, where
   they would be kept for a certain number of consequent frames.

   9. the final step is getting the actual bounding boxes for the
   vehicles. [34]opencv provides the handy function cv2.grouprectangles().
   as said in [35]docs: "it clusters all the input rectangles using the
   rectangle equivalence criteria that combines rectangles with similar
   sizes and similar locations." exactly what is needed. the function has
   a groupthreshold parameter responsible for "minimum possible number of
   rectangles minus 1". that is, it won't produce any result until the
   history accumulates bounding boxes from at least that number of frames.
   [1*hmkm5e16ijhqkuifkbjmjw.png]

video implementation

   i   ve merged vehicle and [36]lane detections into a single pipeline to
   generate a combined footage with both the lane projection and vehicles
   bounding boxes.

   iframe: [37]/media/a594b016db5f9ffa870cea6049693a83?postid=44b851d70508

reflections

   i thoroughly studied the approach of applying id166 classifier to hog
   features covered it the project lessons, but still pretty confident
   that deep learning approach is way more suitable for the task of
   vehicle detection. in a cs231n lecture that i referred to at the
   beginning, the hogs are viewed only from a historical perspective.
   furthermore, there is a [38]paper which argues that the dpms (those are
   based on hogs) might be considered as a certain type of convolutional
   neural networks.

   it took some time figuring out how to derive a model that would produce
   the detection map of a reliable resolution when expanding it to accept
   the input image of a full-size region of interest.

   even the tiny model that i   ve finally picked takes about 0.75 seconds
   to produce a detection map for 260x1280 input image on a mid-2014 3ghz
   quad-core i7 macbook pro. that is 1.33 frames per second.

acknowledgements

   i   d like to personally thank [39]david silver and [40]oliver cameron
   for the great content they have developed for the udacity   s
   self-driving cars nanodegree program, as well as all the udacity team
   for the exceptional learning experience they provide.

     * [41]machine learning
     * [42]self driving cars
     * [43]autonomous vehicles
     * [44]udacity
     * [45]towards data science

   (button)
   (button)
   (button) 549 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of ivan kazakov

[47]ivan kazakov

     (button) follow
   [48]towards data science

[49]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 549
     * (button)
     *
     *

   [50]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [51]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/44b851d70508
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/vehicle-detection-and-tracking-44b851d70508&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/vehicle-detection-and-tracking-44b851d70508&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vnmxyievsrlv---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ivankazakov?source=post_header_lockup
  17. https://towardsdatascience.com/@ivankazakov
  18. https://youtu.be/ziwlwfaag-8?t=14m48s
  19. https://www.udacity.com/drive
  20. https://github.com/antevis/carnd-project5-vehicle_detection_and_tracking
  21. https://youtu.be/wfg_jmq6_sk?list=pllvh2fwaqhnpj1web-jhmpuueq8mx-xxg
  22. http://cs231n.github.io/
  23. https://arxiv.org/abs/1506.01497
  24. https://s3.amazonaws.com/udacity-sdc/vehicle_tracking/vehicles.zip
  25. https://s3.amazonaws.com/udacity-sdc/vehicle_tracking/non-vehicles.zip
  26. http://www.cvlibs.net/datasets/kitti/
  27. https://github.com/antevis/carnd-project2-traffic-signs-classifier
  28. https://github.com/maxritter/sdc-vehicle-lane-detection
  29. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
  30. http://www.image-net.org/
  31. https://keras.io/
  32. https://www.tensorflow.org/
  33. https://github.com/antevis/carnd-project4-advanced_lane_finding/blob/master/writeup.md#undistorted-image
  34. http://opencv.org/
  35. http://docs.opencv.org/3.0-beta/modules/objdetect/doc/cascade_classification.html?highlight=cv2.grouprectangles#cv2.grouprectangles
  36. https://github.com/antevis/carnd-project4-advanced_lane_finding
  37. https://towardsdatascience.com/media/a594b016db5f9ffa870cea6049693a83?postid=44b851d70508
  38. https://arxiv.org/abs/1409.5403
  39. https://medium.com/@dsilver829
  40. https://medium.com/@olivercameron
  41. https://towardsdatascience.com/tagged/machine-learning?source=post
  42. https://towardsdatascience.com/tagged/self-driving-cars?source=post
  43. https://towardsdatascience.com/tagged/autonomous-vehicles?source=post
  44. https://towardsdatascience.com/tagged/udacity?source=post
  45. https://towardsdatascience.com/tagged/towards-data-science?source=post
  46. https://towardsdatascience.com/@ivankazakov?source=footer_card
  47. https://towardsdatascience.com/@ivankazakov
  48. https://towardsdatascience.com/?source=footer_card
  49. https://towardsdatascience.com/?source=footer_card
  50. https://towardsdatascience.com/
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. https://medium.com/p/44b851d70508/share/twitter
  54. https://medium.com/p/44b851d70508/share/facebook
  55. https://medium.com/p/44b851d70508/share/twitter
  56. https://medium.com/p/44b851d70508/share/facebook
