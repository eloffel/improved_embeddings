   #[1]pyimagesearch    feed [2]pyimagesearch    comments feed
   [3]pyimagesearch    id163: vggnet, resnet, inception, and xception
   with keras comments feed [4]alternate [5]alternate

[6]navigation

   [7]pyimagesearch [8]pyimagesearch be awesome at opencv, python, deep
   learning, and id161

   [9]home

main menu

     * [10]start here
     * [11]practical python and opencv
     * [12]pyimagesearch gurus
     * [13]opencv 3 & 4 tutorials
     * [14]free crash course
     * [15]about
     * [16]contact

   [17]return to content

id163: vggnet, resnet, inception, and xception with keras

   by [18]adrian rosebrock on march 20, 2017 in [19]deep learning,
   [20]machine learning, [21]tutorials
   [22]python file icon click here to download the source code to this
   post

   a few months ago i wrote a [23]tutorial on how to classify images using
   convolutional neural networks (specifically, vgg16) pre-trained on the
   id163 dataset with python and the keras deep learning library.

   the pre-trained networks inside of keras are capable of
   recognizing 1,000 different object categories, similar to objects we
   encounter in our day-to-day lives with high accuracy.

   back then, the pre-trained id163 models were separate from the core
   keras library, requiring us to clone a [24]free-standing github
   repo and then manually copy the code into our projects.

   this solution worked well enough; however, since my original blog post
   was published, the pre-trained networks (vgg16, vgg19, resnet50,
   inception v3, and xception) have been fully integrated into the keras
   core (no need to clone down a separate repo anymore)     these
   implementations can be found inside the [25]applications sub-module.

   because of this, i   ve decided to create a new, updated tutorial that
   demonstrates how to utilize these state-of-the-art networks in your own
   classification projects.

   specifically, we   ll create a special python script that can load any of
   these networks using either a tensorflow or theano backend, and then
   classify your own custom input images.

   to learn more about classifying images with vggnet, resnet, inception,
   and xception, just keep reading.

   looking for the source code to this post?
   [26]jump right to the downloads section.

vggnet, resnet, inception, and xception with keras

   in the first half of this blog post i   ll briefly discuss the vgg,
   resnet, inception, and xception network architectures included in the
   keras library.

   we   ll then create a custom python script using keras that can load
   these pre-trained network architectures from disk and classify your own
   input images.

   finally, we   ll review the results of these classifications on a few
   sample images.

state-of-the-art deep learning image classifiers in keras

   keras ships out-of-the-box with five convolutional neural networks that
   have been pre-trained on the id163 dataset:
    1. vgg16
    2. vgg19
    3. resnet50
    4. inception v3
    5. xception

   let   s start with a overview of the id163 dataset and then move into
   a brief discussion of each network architecture.

what is id163?

   [27]id163 is formally a project aimed at (manually) labeling and
   categorizing images into almost 22,000 separate object categories for
   the purpose of id161 research.

   however, when we hear the term    id163    in the context of deep
   learning and convolutional neural networks, we are likely referring to
   the [28]id163 large scale visual recognition challenge, or ilsvrc
   for short.

   the goal of this image classification challenge is to train a model
   that can correctly classify an input image into 1,000 separate object
   categories.

   models are trained on ~1.2 million training images with another 50,000
   images for validation and 100,000 images for testing.

   these 1,000 image categories represent object classes that we encounter
   in our day-to-day lives, such as species of dogs, cats, various
   household objects, vehicle types, and much more. you can find the full
   list of object categories in the ilsvrc challenge [29]here.

   when it comes to image classification, the id163 challenge is the de
   facto benchmark for id161 classification algorithms     and the
   leaderboard for this challenge has been dominated by convolutional
   neural networks and deep learning techniques since 2012.

   the state-of-the-art pre-trained networks included in the keras core
   library represent some of the highest performing convolutional neural
   networks on the id163 challenge over the past few years. these
   networks also demonstrate a strong ability to generalize to images
   outside the id163 dataset via id21, such as feature
   extraction and fine-tuning.

vgg16 and vgg19

   figure 1: a visualization of the vgg architecture ([30]source).

   the vgg network architecture was introduced by simonyan and zisserman
   in their 2014 paper, [31]very deep convolutional networks for large
   scale image recognition.

   this network is characterized by its simplicity, using only 3  3
   convolutional layers stacked on top of each other in increasing depth.
   reducing volume size is handled by max pooling. two fully-connected
   layers, each with 4,096 nodes are then followed by a softmax classifier
   (above).

   the    16    and    19    stand for the number of weight layers in the network
   (columns d and e in figure 2 below):

   figure 2: table 1 of [32]very deep convolutional networks for large
   scale image recognition, simonyan and zisserman (2014).

   in 2014, 16 and 19 layer networks were considered very deep (although
   we now have the resnet architecture which can be successfully trained
   at depths of 50-200 for id163 and over 1,000 for cifar-10).

   simonyan and zisserman found training vgg16 and vgg19 challenging
   (specifically regarding convergence on the deeper networks), so in
   order to make training easier, they first trained smaller versions of
   vgg with less weight layers (columns a and c) first.

   the smaller networks converged and were then used as initializations
   for the larger, deeper networks     this process is called pre-training.

   while making logical sense, pre-training is a very time consuming,
   tedious task, requiring an entire network to be trained before it can
   serve as an initialization for a deeper network.

   we no longer use pre-training (in most cases) and instead prefer
   xaiver/glorot initialization or msra initialization (sometimes called
   he et al. initialization from the paper, [33]delving deep into
   rectifiers: surpassing human-level performance on id163
   classification). you can read more about the importance of weight
   initialization and the convergence of deep neural networks
   inside [34]all you need is a good init, mishkin and matas (2015).

   unfortunately, there are two major drawbacks with vggnet:
    1. it is painfully slow to train.
    2. the network architecture weights themselves are quite large (in
       terms of disk/bandwidth).

   due to its depth and number of fully-connected nodes, vgg is over 533mb
   for vgg16 and 574mb for vgg19. this makes deploying vgg a tiresome
   task.

   we still use vgg in many deep learning image classification problems;
   however, smaller network architectures are often more desirable (such
   as squeezenet, googlenet, etc.).

resnet

   unlike traditional sequential network architectures such as alexnet,
   overfeat, and vgg, resnet is instead a form of    exotic architecture   
   that relies on micro-architecture modules (also called
      network-in-network architectures   ).

   the term micro-architecture refers to the set of    building blocks    used
   to construct the network. a collection of micro-architecture building
   blocks (along with your standard conv, pool, etc. layers) leads to
   the macro-architecture (i.e,. the end network itself).

   first introduced by he et al. in their 2015 paper, [35]deep residual
   learning for image recognition, the resnet architecture has become a
   seminal work, demonstrating that extremely deep networks can be trained
   using standard sgd (and a reasonable initialization function) through
   the use of residual modules:

   figure 3: the residual module in resnet as originally proposed by he et
   al. in 2015.

   further accuracy can be obtained by updating the residual module to use
   identity mappings, as demonstrated in their 2016 followup
   publication, [36]identity mappings in deep residual networks:

   figure 4: (left) the original residual module. (right) the updated
   residual module using pre-activation.

   that said, keep in mind that the resnet50 (as in 50 weight layers)
   implementation in the keras core is based on the former 2015 paper.

   even though resnet is much deeper than vgg16 and vgg19, the model size
   is actually substantially smaller due to the usage of global average
   pooling rather than fully-connected layers     this reduces the model
   size down to 102mb for resnet50.

inception v3

   the    inception    micro-architecture was first introduced by szegedy et
   al. in their 2014 paper, [37]going deeper with convolutions:

   figure 5: the original inception module used in googlenet.

   the goal of the inception module is to act as a    multi-level feature
   extractor    by computing 1  1, 3  3, and 5  5 convolutions within the same
   module of the network     the output of these filters are then stacked
   along the channel dimension and before being fed into the next layer in
   the network.

   the original incarnation of this architecture was called googlenet, but
   subsequent manifestations have simply been called inception vn where n
   refers to the version number put out by google.

   the inception v3 architecture included in the keras core comes from the
   later publication by szegedy et al., [38]rethinking the inception
   architecture for id161 (2015) which proposes updates to the
   inception module to further boost id163 classification accuracy.

   the weights for inception v3 are smaller than both vgg and resnet,
   coming in at 96mb.

xception

   figure 6: the xception architecture.

   xception was proposed by none other than [39]fran  ois chollet himself,
   the creator and chief maintainer of the keras library.

   xception is an extension of the inception architecture which replaces
   the standard inception modules with depthwise separable convolutions.

   the original publication, xception: deep learning with depthwise
   separable convolutions can be found [40]here.

   xception sports the smallest weight serialization at only 91mb.

what about squeezenet?

   figure 7: the    fire    module in squeezenet, consisting of a    squeeze   
   and an    expand   . ([41]iandola et al., 2016).

   for what it   s worth, the [42]squeezenet architecture can obtain
   alexnet-level accuracy (~57% rank-1 and ~80% rank-5) at only 4.9mb
   through the usage of    fire    modules that    squeeze    and    expand   .

   while leaving a small footprint, squeezenet can also be very tricky to
   train.

   that said, i demonstrate how to train squeezenet from scratch on the
   id163 dataset inside my upcoming book, [43]deep learning for
   id161 with python.

classifying images with vggnet, resnet, inception, and xception with python
and keras

   let   s learn how to classify images with pre-trained convolutional
   neural networks using the keras library.

   open up a new file, name it classify_image.py , and insert the
   following code:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   # import the necessary packages
   from keras.applications import resnet50
   from keras.applications import inceptionv3
   from keras.applications import xception # tensorflow only
   from keras.applications import vgg16
   from keras.applications import vgg19
   from keras.applications import id163_utils
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array
   from keras.preprocessing.image import load_img
   import numpy as np
   import argparse
   import cv2

   lines 2-13 import our required python packages. as you can see, most of
   the packages are part of the keras library.

   specifically, lines 2-6 handle importing the keras implementations of
   resnet50, inception v3, xception, vgg16, and vgg19, respectively.

   please note that the xception network is compatible only with the
   tensorflow backend (the class will throw an error if you try to
   instantiate it with a theano backend).

   line 7 gives us access to the id163_utils  sub-module, a handy set
   of convenience functions that will make pre-processing our input images
   and decoding output classifications easier.

   the remainder of the imports are other helper functions, followed by
   numpy for numerical processing and cv2  for our opencv bindings.

   next, let   s parse our command line arguments:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   15
   16
   17
   18
   19
   20
   21
   # construct the argument parse and parse the arguments
   ap = argparse.argumentparser()
   ap.add_argument("-i", "--image", required=true,
   help="path to the input image")
   ap.add_argument("-model", "--model", type=str, default="vgg16",
   help="name of pre-trained network to use")
   args = vars(ap.parse_args())

   we   ll require only a single command line argument, --image , which is
   the path to our input image that we wish to classify.

   we   ll also accept an optional command line argument, --model , a string
   that specifies which pre-trained convolutional neural network we would
   like to use     this value defaults to vgg16  for the vgg16 network
   architecture.

   given that we accept the name of our pre-trained network via a command
   line argument, we need to define a python dictionary that maps the
   model names (strings) to their actual keras classes:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   # define a dictionary that maps model names to their classes
   # inside keras
   models = {
   "vgg16": vgg16,
   "vgg19": vgg19,
   "inception": inceptionv3,
   "xception": xception, # tensorflow only
   "resnet": resnet50
   }

   # esnure a valid model name was supplied via command line argument
   if args["model"] not in models.keys():
   raise assertionerror("the --model command line argument should "
   "be a key in the `models` dictionary")

   lines 25-31 defines our models  dictionary which maps a model name
   string to the corresponding class.

   if the --model  name is not found inside models , we   ll raise an
   assertionerror  (lines 34-36).

   a convolutional neural network takes an image as an input and then
   returns a set of probabilities corresponding to the class labels as
   output.

   typical input image sizes to a convolutional neural network trained on
   id163 are 224  224, 227  227, 256  256, and 299  299; however, you may
   see other dimensions as well.

   vgg16, vgg19, and resnet all accept 224  224 input images while
   inception v3 and xception require 299  299 pixel inputs, as demonstrated
   by the following code block:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   ____________________________________________________________
   # initialize the input image shape (224x224 pixels) along wi
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)________
   inputshape = (224, 224)_____________________________________
   preprocess = id163_utils.preprocess_input________________
   ____________________________________________________________
   # if we are using the inceptionv3 or xception networks, then
   # need to set the input shape to (299x299) [rather than (224
   # and use a different image processing function_____________
   if args["model"] in ("inception", "xception"):______________
   	inputshape = (299, 299)____________________________________
   	preprocess = preprocess_input______________________________
   38
   39
   40
   41
   42
   43
   44
   45
   46
   47
   48
   49
   # initialize the input image shape (224x224 pixels) along with
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)
   inputshape = (224, 224)
   preprocess = id163_utils.preprocess_input

   # if we are using the inceptionv3 or xception networks, then we
   # need to set the input shape to (299x299) [rather than (224x224)]
   # and use a different image processing function
   if args["model"] in ("inception", "xception"):
   inputshape = (299, 299)
   preprocess = preprocess_input

   here we initialize our inputshape  to be 224  224 pixels. we also
   initialize our preprocess  function to be the standard
   preprocess_input  from keras (which performs mean subtraction).

   however, if we are using inception or xception, we need to set the
   inputshape  to 299  299 pixels, followed by updating preprocess  to use
   a separate pre-processing function that [44]performs a different type
   of scaling.

   the next step is to load our pre-trained network architecture weights
   from disk and instantiate our model:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   ____________________________________________________________
   # initialize the input image shape (224x224 pixels) along wi
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)________
   inputshape = (224, 224)_____________________________________
   preprocess = id163_utils.preprocess_input________________
   ____________________________________________________________
   # if we are using the inceptionv3 or xception networks, then
   # need to set the input shape to (299x299) [rather than (224
   # and use a different image processing function_____________
   if args["model"] in ("inception", "xception"):______________
   	inputshape = (299, 299)____________________________________
   	preprocess = preprocess_input______________________________
   ____________________________________________________________
   # load our the network weights from disk (note: if this is t
   # first time you are running this script for a given network
   # weights will need to be downloaded first -- depending on w
   # network you are using, the weights can be 90-575mb, so be_
   # patient; the weights will be cached and subsequent runs of
   # script will be *much* faster)_____________________________
   print("[info] loading {}...".format(args["model"]))_________
   network = models[args["model"]]_____________________________
   model = network(weights="id163")_________________________
   51
   52
   53
   54
   55
   56
   57
   58
   59
   # load our the network weights from disk (note: if this is the
   # first time you are running this script for a given network, the
   # weights will need to be downloaded first -- depending on which
   # network you are using, the weights can be 90-575mb, so be
   # patient; the weights will be cached and subsequent runs of this
   # script will be *much* faster)
   print("[info] loading {}...".format(args["model"]))
   network = models[args["model"]]
   model = network(weights="id163")

   line 58 uses the models  dictionary along with the --model  command
   line argument to grab the correct network  class.

   the convolutional neural network is then instantiated on line 59 using
   the pre-trained id163 weights;

   note: weights for vgg16 and vgg19 are > 500mb. resnet weights are
   ~100mb, while inception and xception weights are between 90-100mb. if
   this is the first time you are running this script for a given network,
   these weights will be (automatically) downloaded and cached to your
   local disk. depending on your internet speed, this may take awhile.
   however, once the weights are downloaded, they will not need to be
   downloaded again, allowing subsequent runs of classify_image.py  to
   be much faster.

   our network is now loaded and ready to classify an image     we just need
   to prepare this image for classification:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   ____________________________________________________________
   # initialize the input image shape (224x224 pixels) along wi
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)________
   inputshape = (224, 224)_____________________________________
   preprocess = id163_utils.preprocess_input________________
   ____________________________________________________________
   # if we are using the inceptionv3 or xception networks, then
   # need to set the input shape to (299x299) [rather than (224
   # and use a different image processing function_____________
   if args["model"] in ("inception", "xception"):______________
   	inputshape = (299, 299)____________________________________
   	preprocess = preprocess_input______________________________
   ____________________________________________________________
   # load our the network weights from disk (note: if this is t
   # first time you are running this script for a given network
   # weights will need to be downloaded first -- depending on w
   # network you are using, the weights can be 90-575mb, so be_
   # patient; the weights will be cached and subsequent runs of
   # script will be *much* faster)_____________________________
   print("[info] loading {}...".format(args["model"]))_________
   network = models[args["model"]]_____________________________
   model = network(weights="id163")_________________________
   ____________________________________________________________
   # load the input image using the keras helper utility while 
   # the image is resized to `inputshape`, the required input d
   # for the id163 pre-trained network______________________
   print("[info] loading and pre-processing image...")_________
   image = load_img(args["image"], target_size=inputshape)_____
   image = img_to_array(image)_________________________________
   ____________________________________________________________
   # our input image is now represented as a numpy array of sha
   # (inputshape[0], inputshape[1], 3) however we need to expan
   # dimension by making the shape (1, inputshape[0], inputshap
   # so we can pass it through thenetwork______________________
   image = np.expand_dims(image, axis=0)_______________________
   ____________________________________________________________
   # pre-process the image using the appropriate function based
   # model that has been loaded (i.e., mean subtraction, scalin
   image = preprocess(image)___________________________________
   61
   62
   63
   64
   65
   66
   67
   68
   69
   70
   71
   72
   73
   74
   75
   76
   # load the input image using the keras helper utility while ensuring
   # the image is resized to `inputshape`, the required input dimensions
   # for the id163 pre-trained network
   print("[info] loading and pre-processing image...")
   image = load_img(args["image"], target_size=inputshape)
   image = img_to_array(image)

   # our input image is now represented as a numpy array of shape
   # (inputshape[0], inputshape[1], 3) however we need to expand the
   # dimension by making the shape (1, inputshape[0], inputshape[1], 3)
   # so we can pass it through thenetwork
   image = np.expand_dims(image, axis=0)

   # pre-process the image using the appropriate function based on the
   # model that has been loaded (i.e., mean subtraction, scaling, etc.)
   image = preprocess(image)

   line 65 loads our input image from disk using the supplied inputshape
   to resize the width and height of the image.

   line 66 converts the image from a pil/pillow instance to a numpy array.

   our input image is now represented as a numpy array with the shape
   (inputshape[0], inputshape[1], 3) .

   however, we typically train/classify images in batches with
   convolutional neural networks, so we need to add an extra dimension to
   the array via np.expand_dims  on line 72.

   after calling np.expand_dims  the image  has the shape (1,
   inputshape[0], inputshape[1], 3) . forgetting to add this extra
   dimension will result in an error when you call .predict  of the
   model .

   lastly, line 76 calls the appropriate pre-processing function to
   perform mean subtraction/scaling.

   we are now ready to pass our image through the network and obtain the
   output classifications:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   ____________________________________________________________
   # initialize the input image shape (224x224 pixels) along wi
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)________
   inputshape = (224, 224)_____________________________________
   preprocess = id163_utils.preprocess_input________________
   ____________________________________________________________
   # if we are using the inceptionv3 or xception networks, then
   # need to set the input shape to (299x299) [rather than (224
   # and use a different image processing function_____________
   if args["model"] in ("inception", "xception"):______________
   	inputshape = (299, 299)____________________________________
   	preprocess = preprocess_input______________________________
   ____________________________________________________________
   # load our the network weights from disk (note: if this is t
   # first time you are running this script for a given network
   # weights will need to be downloaded first -- depending on w
   # network you are using, the weights can be 90-575mb, so be_
   # patient; the weights will be cached and subsequent runs of
   # script will be *much* faster)_____________________________
   print("[info] loading {}...".format(args["model"]))_________
   network = models[args["model"]]_____________________________
   model = network(weights="id163")_________________________
   ____________________________________________________________
   # load the input image using the keras helper utility while 
   # the image is resized to `inputshape`, the required input d
   # for the id163 pre-trained network______________________
   print("[info] loading and pre-processing image...")_________
   image = load_img(args["image"], target_size=inputshape)_____
   image = img_to_array(image)_________________________________
   ____________________________________________________________
   # our input image is now represented as a numpy array of sha
   # (inputshape[0], inputshape[1], 3) however we need to expan
   # dimension by making the shape (1, inputshape[0], inputshap
   # so we can pass it through thenetwork______________________
   image = np.expand_dims(image, axis=0)_______________________
   ____________________________________________________________
   # pre-process the image using the appropriate function based
   # model that has been loaded (i.e., mean subtraction, scalin
   image = preprocess(image)___________________________________
   ____________________________________________________________
   # classify the image________________________________________
   print("[info] classifying image with '{}'...".format(args["m
   preds = model.predict(image)________________________________
   p = id163_utils.decode_predictions(preds)________________
   ____________________________________________________________
   # loop over the predictions and display the rank-5 predictio
   # probabilities to our terminal_____________________________
   for (i, (id163id, label, prob)) in enumerate(p[0]):______
   	print("{}. {}: {:.2f}%".format(i + 1, label, prob * 100))__
   78
   79
   80
   81
   82
   83
   84
   85
   86
   # classify the image
   print("[info] classifying image with '{}'...".format(args["model"]))
   preds = model.predict(image)
   p = id163_utils.decode_predictions(preds)

   # loop over the predictions and display the rank-5 predictions +
   # probabilities to our terminal
   for (i, (id163id, label, prob)) in enumerate(p[0]):
   print("{}. {}: {:.2f}%".format(i + 1, label, prob * 100))

   a call to .predict  on line 80 returns the predictions from the
   convolutional neural network.

   given these predictions, we pass them into the id163 utility
   function .decode_predictions  to give us a list of id163 class label
   ids,    human-readable    labels, and the id203 associated with the
   labels.

   the top-5 predictions (i.e., the labels with the largest probabilities)
   are then printed to our terminal on lines 85 and 86.

   the last thing we   ll do here before we close out our example is load
   our input image from disk via opencv, draw the #1 prediction on the
   image, and finally display the image to our screen:
   vggnet, resnet, inception, and xception with keras
   python

   # import the necessary packages_____________________________
   from keras.applications import resnet50_____________________
   from keras.applications import inceptionv3__________________
   from keras.applications import xception # tensorflow only___
   from keras.applications import vgg16________________________
   from keras.applications import vgg19________________________
   from keras.applications import id163_utils_______________
   from keras.applications.inception_v3 import preprocess_input
   from keras.preprocessing.image import img_to_array__________
   from keras.preprocessing.image import load_img______________
   import numpy as np__________________________________________
   import argparse_____________________________________________
   import cv2__________________________________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-i", "--image", required=true,_____________
   	help="path to the input image")____________________________
   ap.add_argument("-model", "--model", type=str, default="vgg1
   	help="name of pre-trained network to use")_________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # define a dictionary that maps model names to their classes
   # inside keras______________________________________________
   models = {__________________________________________________
   	"vgg16": vgg16,____________________________________________
   	"vgg19": vgg19,____________________________________________
   	"inception": inceptionv3,__________________________________
   	"xception": xception, # tensorflow only____________________
   	"resnet": resnet50_________________________________________
   }___________________________________________________________
   ____________________________________________________________
   # esnure a valid model name was supplied via command line ar
   if args["model"] not in models.keys():______________________
   	raise assertionerror("the --model command line argument sho
   		"be a key in the `models` dictionary")____________________
   ____________________________________________________________
   # initialize the input image shape (224x224 pixels) along wi
   # the pre-processing function (this might need to be changed
   # based on which model we use to classify our image)________
   inputshape = (224, 224)_____________________________________
   preprocess = id163_utils.preprocess_input________________
   ____________________________________________________________
   # if we are using the inceptionv3 or xception networks, then
   # need to set the input shape to (299x299) [rather than (224
   # and use a different image processing function_____________
   if args["model"] in ("inception", "xception"):______________
   	inputshape = (299, 299)____________________________________
   	preprocess = preprocess_input______________________________
   ____________________________________________________________
   # load our the network weights from disk (note: if this is t
   # first time you are running this script for a given network
   # weights will need to be downloaded first -- depending on w
   # network you are using, the weights can be 90-575mb, so be_
   # patient; the weights will be cached and subsequent runs of
   # script will be *much* faster)_____________________________
   print("[info] loading {}...".format(args["model"]))_________
   network = models[args["model"]]_____________________________
   model = network(weights="id163")_________________________
   ____________________________________________________________
   # load the input image using the keras helper utility while 
   # the image is resized to `inputshape`, the required input d
   # for the id163 pre-trained network______________________
   print("[info] loading and pre-processing image...")_________
   image = load_img(args["image"], target_size=inputshape)_____
   image = img_to_array(image)_________________________________
   ____________________________________________________________
   # our input image is now represented as a numpy array of sha
   # (inputshape[0], inputshape[1], 3) however we need to expan
   # dimension by making the shape (1, inputshape[0], inputshap
   # so we can pass it through thenetwork______________________
   image = np.expand_dims(image, axis=0)_______________________
   ____________________________________________________________
   # pre-process the image using the appropriate function based
   # model that has been loaded (i.e., mean subtraction, scalin
   image = preprocess(image)___________________________________
   ____________________________________________________________
   # classify the image________________________________________
   print("[info] classifying image with '{}'...".format(args["m
   preds = model.predict(image)________________________________
   p = id163_utils.decode_predictions(preds)________________
   ____________________________________________________________
   # loop over the predictions and display the rank-5 predictio
   # probabilities to our terminal_____________________________
   for (i, (id163id, label, prob)) in enumerate(p[0]):______
   	print("{}. {}: {:.2f}%".format(i + 1, label, prob * 100))__
   ____________________________________________________________
   # load the image via opencv, draw the top prediction on the 
   # and display the image to our screen_______________________
   orig = cv2.imread(args["image"])____________________________
   (id163id, label, prob) = p[0][0]_________________________
   cv2.puttext(orig, "label: {}, {:.2f}%".format(label, prob * 
   	(10, 30), cv2.font_hershey_simplex, 0.8, (0, 0, 255), 2)___
   cv2.imshow("classification", orig)__________________________
   cv2.waitkey(0)______________________________________________
   88
   89
   90
   91
   92
   93
   94
   95
   # load the image via opencv, draw the top prediction on the image,
   # and display the image to our screen
   orig = cv2.imread(args["image"])
   (id163id, label, prob) = p[0][0]
   cv2.puttext(orig, "label: {}, {:.2f}%".format(label, prob * 100),
   (10, 30), cv2.font_hershey_simplex, 0.8, (0, 0, 255), 2)
   cv2.imshow("classification", orig)
   cv2.waitkey(0)

   to see our pre-trained id163 networks in action, take a look at the
   next section.

vggnet, resnet, inception, and xception classification results

   all examples in this blog post were gathered using [45]keras >= 2.0 and
   a [46]tensorflow backend. if you are using tensorflow, [47]make sure
   you are using version >= 1.0, otherwise you will run into errors. i   ve
   also tested this script with the theano backend and confirmed that the
   implementation will work with theano as well.

   once you have tensorflow/theano and keras installed, make sure you
   download the source code + example images to this blog post using
   the    downloads    section at the bottom of the tutorial.

   from there, let   s try classifying an image with vgg16:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/soccer_ball.jpg --
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/soccer_ball.jpg --model vgg16

   figure 8: classifying a soccer ball using vgg16 pre-trained on the
   id163 database using keras ([48]source).

   taking a look at the output, we can see vgg16 correctly classified the
   image as    soccer ball    with 93.43% accuracy.

   to use vgg19, we simply need to change the --model  command line
   argument:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/bmw.png --model vg
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/bmw.png --model vgg19

   figure 9: classifying a vehicle as    convertible    using vgg19 and keras
   ([49]source).

   vgg19 is able to correctly classify the the input image
   as    convertible    with a id203 of 91.76%. however, take a look at
   the other top-5 predictions: sports car with 4.98% id203 (which
   the car is), limousine at 1.06% (incorrect, but still reasonable),
   and    car wheel    at 0.75% (also technically correct since there are car
   wheels in the image).

   we can see similar levels of top-5 accuracy in the following example
   where we use the pre-trained resnet architecture:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/clint_eastwood.jpg
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/clint_eastwood.jpg --model
   resnet

   figure 10: using resnet pre-trained on id163 with keras + python
   ([50]source).

   resnet correctly classifies this image of clint eastwood holding a gun
   as    revolver    with 69.79% accuracy. it   s also interesting to
   see    rifle    at 7.74% and    assault rifle    at 5.63% included in the top-5
   predictions as well. given the viewing angle of the revolver and the
   substantial length of the barrel (for a handgun) it   s easy to see how a
   convolutional neural network would also return higher probabilities for
   a rifle as well.

   this next example attempts to classify the species of dog using resnet:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/jemma.png --model 
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/jemma.png --model resnet

   figure 11: classifying dog species using resnet, keras, and python.

   the species of dog is correctly identified as    beagle    with 94.48%
   confidence.

   i then tried classifying the following image of johnny depp from
   the pirates of the caribbean franchise:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/boat.png --model i
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/boat.png --model inception

   figure 12: classifying a ship wreck with resnet pre-trained on id163
   with keras ([51]source).

   while there is indeed a    boat    class in id163, it   s interesting to
   see that the inception network was able to correctly identify the scene
   as a    (ship) wreck    with 96.29% id203. all other predicted
   labels, including    seashore   ,    canoe   ,    paddle   , and    breakwater    are
   all relevant, and in some cases absolutely correct as well.

   for another example of the inception network in action, i took a photo
   of the couch sitting in my office:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/office.png --model
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/office.png --model inception

   figure 13: recognizing various objects in an image with inception v3,
   python, and keras.

   inception correctly predicts there is a    table lamp    in the image with
   69.68% confidence. the other top-5 predictions are also dead-on,
   including a    studio couch   ,    window shade    (far right of the image,
   barely even noticeable),    lampshade   , and    pillow   .

   in the context above, inception wasn   t even used as an object detector,
   but it was still able to classify all parts of the image within its
   top-5 predictions. it   s no wonder that convolutional neural networks
   make for excellent object detectors!

   moving on to xception:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/scotch.png --model
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/scotch.png --model xception

   figure 14: using the xception network architecture to classify an image
   ([52]source).

   here we have an image of scotch barrels, specifically my favorite
   scotch, lagavulin. xception correctly classifies this image
   as    barrels   .

   this last example was classified using vgg16:
   vggnet, resnet, inception, and xception with keras
   shell

   $ python classify_image.py --image images/tv.png --model vgg
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python classify_image.py --image images/tv.png --model vgg16

   figure 15: vgg16 pre-trained on id163 with keras.

   the image itself was captured a few months ago as i was finishing
   up the witcher iii: the wild hunt (easily in my top-3 favorite games of
   all time). the first prediction by vgg16 is    home theatre        a
   reasonable prediction given that there is a    television/monitor    in the
   top-5 predictions as well.

   as you can see from the examples in this blog post, networks
   pre-trained on the id163 dataset are capable of recognizing a
   variety of common day-to-day objects. i hope that you can use this code
   in your own projects!

what now?

   congratulations!

   you can now recognize 1,000 separate object categories from the
   id163 dataset using pre-trained state-of-the-art convolutional
   neural networks.

      but what if you wanted to train your own custom deep learning
   networks from scratch?

   how would you go about it?

   do you know where to start?

   let me help:

   whether this is the first time you   ve worked with machine learning and
   neural networks or you   re already a seasoned deep learning
   practitioner, my new book is engineered from the ground up to help you
   reach deep learning expert status.

   [53][dl4cv_in_post.png]

                        [54]click here to learn more

summary

   in today   s blog post we reviewed the five convolutional neural networks
   pre-trained on the id163 dataset inside the keras library:
    1. vgg16
    2. vgg19
    3. resnet50
    4. inception v3
    5. xception

   i then demonstrated how to use each of these architectures to classify
   your own input images using the keras library and the python
   programming language.

   if you are interested in learning more about deep learning and
   convolutional neural networks (and how to train your own networks from
   scratch), be sure to take a look at my upcoming book, [55]deep learning
   for id161 with python, available for pre-order now.

downloads:

   if you would like to download the code and images used in this post,
   please enter your email address in the form below. not only will you
   get a .zip of the code, i   ll also send you a free 17-page resource
   guide on id161, opencv, and deep learning. inside you'll find
   my hand-picked tutorials, books, courses, and libraries to help you
   master cv and dl! sound good? if so, enter your email address and i   ll
   send you the code immediately!

   email address: ____________________

   download the code!

resource guide (it   s totally free).

   get your free 17-page id161 and deep learning resource guide
   pdf
   enter your email address below to get my free 17-page id161,
   opencv, and deep learning resource guide pdf. inside you'll find my
   hand-picked tutorials, books, courses, and python libraries to help you
   master id161 and deep learning!
   ____________________
   download the guide!

   [56]inception, [57]keras, [58]micro-architecture, [59]resnet, [60]vgg,
   [61]xception
   [62]an interview with davis king, creator of the dlib toolkit
   [63]how to install dlib

93 responses to id163: vggnet, resnet, inception, and xception with keras

    1. aramis march 20, 2017 at 12:24 pm [64]#
       dear adrian;
       thank you for you nice tutorial. i always learn many new points
       from your tutorials which organized and explained very-well. i have
       implemented this code and i could figure out how to use these
       models with keras. i thought now i can use id21 with
       these pre-trained models and train on my own data.
       however, the main problem with my data is that they are medical
       images and gray-scale. i could follow the tutorial which proposed
       by fcohelt but i couldn   t figure out how to change the structure of
       the models to accept 1 channel data.
       i would be glad if you could give some hint for id21
       with pre-trained models for not rgb but gray-scale images.
       regards.
       [65]reply
          + adrian rosebrock march 21, 2017 at 7:20 am [66]#
            these pre-trained networks assume you are using 3 channel
            images     you won   t be able to modify them to use 1 channel
            images unless you train them from scratch. instead, the
            solution is to turn your 1 channel image into a 3 channel
            image:
            image = np.dstack([1chan, 1chan, 1chan])
            from there you can pass the image through the network since
            it   s a 3 channel image (but appears gray).
            i   ll also be discussing id21 in great detail in
            my upcoming book, [67]deep learning for id161 with
            python.
            [68]reply
          + [69]jdk january 15, 2018 at 2:26 am [70]#
            great job,
            [71]reply
    2. parth march 20, 2017 at 12:42 pm [72]#
       hey adrian,
       thanks for the blog.
       i was hoping to do pedestrian/human detection using convolutional
       neural networks. i have tried using hog but it didn   t turn out to
       be super accurate. the problem i am facing with using id98 with
       id163 trained classifiers is that there is no class/label as
          person    or    human    or anything of that sort. what do you suggest i
       do? could i try training it with inria person dataset or something
       similar? if yes, how?
       thanks!
       [73]reply
          + adrian rosebrock march 21, 2017 at 7:18 am [74]#
            i would fine-tune one of the networks on a dataset that is
            representative of the people you want to detect in images. if
            that   s inria, use it for fine-tuning.
            [75]reply
    3. nicho march 20, 2017 at 12:59 pm [76]#
       great
       [77]reply
    4. ashti march 20, 2017 at 5:11 pm [78]#
       hi,
       not related to this post.
       but i have a query wrt to keyframe extraction from videos.
       using python and opencv i have to extract keyframes.
       i tried getting frames for each frame and then subtracting from
       each other and storing unique one which resulted in huge amoutn of
       frames.\
       i need to calculate pixel difference of frames and compare it with
       a threshold value. if pd > threshold store it as keyframe. can you
       please give me an example on how can i calculate threshold of
       images which would be fetch me good amount of keyframes. same would
       be applied for other videos too   
       [79]reply
          + adrian rosebrock march 21, 2017 at 7:13 am [80]#
            hey ashti     i would kindly ask that comments on a particular
            blog post be related to the subject matter of the post
            (otherwise it comes off as a bit rude/presumptive). if you
            want to learn more about comparing images, [81]try this post.
            best of luck with the project.
            [82]reply
    5. miaodx march 21, 2017 at 3:31 am [83]#
       aha, not so easy for me to point out a typo since there are so many
       readers and you   re so careful.
       however, in    vgg16 and vgg19    section,    due to its depth and number
       of fully-connected nodes, vgg is over 533mb for vgg16 and 574mb for
       vgg16. this makes deploying vgg a tiresome task.   
       the latter should be vgg19, i think. ^_^
       [84]reply
          + adrian rosebrock march 21, 2017 at 7:06 am [85]#
            you are correct, thank you for pointing out the typo! it is
            fixed now.
            [86]reply
    6. ruben march 22, 2017 at 4:32 am [87]#
       when i import from keras.applications import resnet50, i have the
       next error:
          
       importerror: cannot import name    globalaveragepooling2d   
       [88]reply
          + adrian rosebrock march 22, 2017 at 8:34 am [89]#
            which version of keras are you running? and which version of
            tensorflow/theano?
            [90]reply
               o ruben march 22, 2017 at 8:59 am [91]#
                 thanks, all is ok with keras-2.0.2 theano -0.9.0
                 [92]reply
                    # adrian rosebrock march 22, 2017 at 9:08 am [93]#
                      congrats on resolving the issue!
                      [94]reply
    7. abraham george march 23, 2017 at 11:57 am [95]#
       i need to take live images and label it how do i do it?
       i cannot pre process the obtained frame ,what should i do?
       [96]reply
          + adrian rosebrock march 25, 2017 at 9:34 am [97]#
            for each frame in your video stream you would pass it through
            the network and obtain the output class labels.
            [98]reply
    8. abraham george march 23, 2017 at 11:59 am [99]#
       what is the difference between parsing an image and reading it
       using imread?
       [100]reply
          + adrian rosebrock march 25, 2017 at 9:34 am [101]#
            i   m not sure what you mean abraham, can you please elaborate
            on your comment?
            [102]reply
    9. sunggu kim march 24, 2017 at 11:01 am [103]#
       thank you for great tutorial.
       i   m always wondering about can i append more class to pre-trained
       network with my data or should i re-train all things?
       if possible we can save huge time and resources.
       will it be possible?
       thanks!
       [104]reply
          + adrian rosebrock march 25, 2017 at 9:22 am [105]#
            the process of changing the output classes of a pre-trained
            network without having to re-train it from scratch is called
            fine-tuning. i   ll be covering fine-tuning in detail inside
            [106]deep learning for id161 with python.
            [107]reply
               o sunggu kim march 26, 2017 at 4:31 am [108]#
                 oh you always have a great answer.
                 i already bought the course from kickstarter.
                 i hope it to be released as soon as possible.
                 thank you.
                 [109]reply
                    # adrian rosebrock march 28, 2017 at 1:09 pm [110]#
                      thank you sunggu kim! i am working on the book and
                      will ensure it will be released as soon as possible.
                      [111]reply
   10. mjb march 28, 2017 at 7:19 pm [112]#
       hi adrian,
       great post as always. i was wondering, how one can test the top 1
       and top 5 error of this pre-trained model across a standardized
       data set say id163 to compare these in a more scientific way.
       any tips?
       [113]reply
          + adrian rosebrock march 31, 2017 at 2:07 pm [114]#
            can you elaborate more on what you mean by comparing the top-1
            and top-5 accuracies? normally for benchmark datasets like
            id163 your rank-1 and rank-5 accuracy on the test set is
            the standardized method to compare algorithms.
            [115]reply
   11. ap march 31, 2017 at 8:23 am [116]#
       thank, excellent !
       as more models emerge having a clean framework to review results
       with is very helpful, thank you and keras. tested with
       keras2/tf1.01 on windows.
       [117]reply
   12. aurora guerra april 11, 2017 at 12:33 pm [118]#
       hi adrian.
       how could you train a neural network for the recognition of leaf
       species?
       i would have to create my own network or use an existing network
       thanks for all post, these are great
       [119]reply
          + adrian rosebrock april 12, 2017 at 1:05 pm [120]#
            hey aurora     i don   t have any blog posts specifically related
            to leaf species classification, but i   ll keep that in mind for
            a future blog post. do you have a link to a leaf dataset you
            are currently working with?
            in the meantime, be sure to take a look at [121]deep learning
            for id161 with python where i   ll be discussing
            training your own deep learning neural networks in detail. a
            book like this would surely help with your project.
            [122]reply
   13. revan april 12, 2017 at 10:08 pm [123]#
       hello sir,
       i   m presently working on image processing project i want to
       know(step 1) how to differentiate human from animals.(step2)if
       captured image is human i want to confirm whether the human in the
       captured image has performed any crime by comparing currently
       captured image with an image that has been already stored in the
       database or cloud.
       so, it will b great if u provide code for step 1 n step2 asap   ..
       [124]reply
          + revan april 12, 2017 at 10:12 pm [125]#
            by the way i   m using raspberry pi3, opencv, python language
            please help me and guide me   
            [126]reply
          + adrian rosebrock april 16, 2017 at 9:07 am [127]#
            differentiating between humans and animals can easily be
            accomplished via a bit of machine learning or deep learning.
            exactly which method you should use is highly dependent on
            your input images/video streams.
            as for crime detection, that sounds more like    activity
            recognition    which is not something i cover on pyimagesearch.
            [128]reply
   14. ravi kishan april 15, 2017 at 4:58 pm [129]#
       hey adrian,
       your tutorial   s are really good. i had an issue which you could
       help me out with :). i want to store the value of the tensor at the
          global pool layer    in resnet50 but am unable to do so.
       would be really nice if you could help me out
       [130]reply
          + adrian rosebrock april 16, 2017 at 8:52 am [131]#
            so if i understand correctly, you want to pass an image
            through the network and then take the raw values from the
            global pool layer prior to the softmax classifier being
            applied?
            [132]reply
   15. kranthi april 16, 2017 at 12:31 am [133]#
       using tensorflow for inception case got attribute error on
       concat_v2.
       [134]reply
          + kranthi april 16, 2017 at 1:19 am [135]#
            hello sir,
            using tensorflow 1.01 version keras >2 version working for
            inception.
            tried with theano 0.90 and keras >2 but not working when i
            tried with inception. you said only xception has to be run on
            tensorflow backend.
            error was typeerror: int() argument must be a string, a
            bytes-like object or a number, not    list   .
            everything else worked as given.
            thanks for great and up to date technology based tutorials.
            [136]reply
               o adrian rosebrock april 16, 2017 at 8:51 am [137]#
                 as i mentioned in the blog post, xception only works for
                 the tensorflow backend. as for inception, this should
                 work without a problem on theano. can you try upgrading
                 your theano version as well?
                 [138]reply
   16. jeff april 21, 2017 at 7:08 pm [139]#
       hi adrian, this is awesome. thank you very much. one question, is
       it posible to train my own model and merge it with an existing one?
       thank you.
       [140]reply
          + adrian rosebrock april 24, 2017 at 9:52 am [141]#
            hey jeff     you can   t really    merge    the models together, but
            what you can do is:
            1. train your own model(s) and create an ensemble from your
            other models (and pre-trained ones) as well. this makes the
            assumption that all networks are trying to predict the same
            class labels.
            2. if you want to predict different class labels from the
            labels in id163, you should try fine-tuning a pre-trained
            network.
            i   m covering both techniques inside [142]deep learning for
            id161 with python.
            [143]reply
   17. shiva april 27, 2017 at 10:23 pm [144]#
       hi adrian,
       my name is shiva, doing postdoctoral research in id161 at
       asu.
       i first found you because of an online search for deep learning
       tutorials.
       i am greatly interested in using deep learning models to perform
       medical image classification, segmentation and cbir.
       my question is:
       i have data with training and validation splits for three classes.
       how do you modify the above codes to accept the training and
       validation splits and print the validation accuracy?
       the reason i am asking is because i could find tutorials on using
       pre-trained models to predict a single image but not in-depth
       analysis on using these very deep models for data classification
       with train and validation splits.
       my experience with deep learning is intermediate level.
       thanks and looking forward.
       shiva
       [145]reply
          + adrian rosebrock april 28, 2017 at 9:26 am [146]#
            hi shiva     i think you might have some confusion regarding
            pre-trained neural networks. once the networks are trained on
            a given number of classes (in this case, 1,000 id163
            classes) you cannot use them to train on new classes (in your
            case, three classes) unless you apply feature extraction or
            fine-tuning.
            fine-tuning will be covered in detail inside [147]deep
            learning for id161 with python. otherwise, i would
            suggest you work through the [148]pyimagesearch gurus course
            so you get get some more experience working with machine
            learning and training models.
            [149]reply
   18. ramesh june 18, 2017 at 1:51 am [150]#
       hi adrian,
       wonderful tutorial. i want to limit the output to a particular set
       of labels only. that is to say, i don   t want all the id163
       labels. am i right in stating that in the previous few comments,
       you were referring to the solution of exact task i want to do when
       you said fine tuning of the pre-trained model is required?
       [151]reply
          + adrian rosebrock june 20, 2017 at 11:09 am [152]#
            there are two ways to do this.
            the first is a bit    hackish   . simply use the pre-trained
            network as is, then ignore the indexes of the labels you are
            not interested in. then, take the label with the largest
            id203 (form the set of labels you care about) and use
            that as your final classification. again, this is a hack and
            only recommended in very specific situations.
            otherwise, i would suggest fine-tuning.
            [153]reply
   19. hesam moshiri july 12, 2017 at 5:00 am [154]#
       hi
       is it possible to fine-tune these existing models for a custom
       dataset?
       [155]reply
          + adrian rosebrock july 12, 2017 at 2:42 pm [156]#
            yes, absolutely. i will be covering fine-tuning in detail
            inside [157]deep learning for id161 with python.
            [158]reply
   20. lucas august 7, 2017 at 6:49 pm [159]#
       hi adrian, i   m interested in implementing the xception and
       inception models to my own image classification problem. however,
       my dataset consists of small images of 25 by 25 pixels, which are
       black and white, so an input_shape of ( 1, 25, 25). do you know if
       it   s possible, or the network fundamentally requires color?
       [160]reply
          + adrian rosebrock august 10, 2017 at 8:58 am [161]#
            i wouldn   t recommend trying to use xception on your images if
            they are (1) grayscale and (2) substantially smaller than the
            images xception was trained on. you can resize your images and
            then convert them to 3 channels by using:
            image = np.dstack([image] * 3)
            however, i wouldn   t expect very good accuracy.
            [162]reply
   21. *_cyrus_rex_* september 1, 2017 at 5:12 pm [163]#
       hi adrian, great tutorial! i would be interested in classify just
       few of all the labels (seashore, lakeshore and alp). how could i go
       through this, maybe modifying the inception v3 model? thanks in
       advance!
       [164]reply
          + adrian rosebrock september 5, 2017 at 9:39 am [165]#
            i would recommend using id21, either via feature
            extraction or fine-tuning. i   m covering both inside [166]deep
            learning for id161 with python.
            [167]reply
   22. somnath banerjee september 17, 2017 at 5:54 pm [168]#
       dear adrian,
       very nice tutorials. i used your sample code to do some simple
       recognition. primarily using the resnet50 model. below is a quick
       summary of my findings. any suggestions on how to take this to more
       accuracy?
       python classify_image.py    image images/burger.jpg    model resnet ==>
       cheeseburger (that was good)
       python classify_image.py    image images/milk.jpg    model resnet ==>
       eggnog (understandable)
       python classify_image.py    image images/fruits.jpg    model resnet ==>
       bellpepper (close)
       python classify_image.py    image images/chicken_biriyani.jpg    model
       resnet ==> plate (needs help with indian / subcontinent food)
       python classify_image.py    image images/pasta.jpg    model resnet ==>
       corn (close)
       [169]reply
          + adrian rosebrock september 18, 2017 at 2:02 pm [170]#
            it really depends on your input images but if you are
            intending to detect a small subset of images consider applying
            id21, specifically fine-tuning or feature
            extraction. i cover both of these techniques in-depth inside
            [171]deep learning for id161 with python.
            [172]reply
   23. [173]rafael september 27, 2017 at 2:08 pm [174]#
       hi adrian,
       always a nice article!
       there is a typo in    to use vgg19, we simply need to change the
          network command line argument:    phrase the change command id
             model    not       network   
       i have a doubt of how to use id21 with different image
       inputs. example: i have 2 different grayscale + depth image and i   d
       like to use existing trained model.
       do you think it is possible?
       or do i have to train a new model?
       [175]reply
          + adrian rosebrock september 28, 2017 at 9:10 am [176]#
            thank you for pointing out the type, rafael.
            as for your question, keep in mind that the id163
            classifiers provided by keras are pre-trained on rgb (3
            channel) images in the id163 dataset. you can explicitly
            construct a 3 channel image from a single channel image via:
            gray = np.dstack([gray] * 3)
            and fine-tune from there; however, keep in mind that the
            filters learned by the neural network assume multi-channel.
            your accuracy likely won   t be as good, but i would give it a
            shot just to obtain a baseline.
            [177]reply
   24. kendall edwards september 29, 2017 at 2:49 pm [178]#
       adrian. great tutorial. is there any way to make this work in a
       jupyter notebook?
       [179]reply
          + adrian rosebrock october 2, 2017 at 10:03 am [180]#
            provided you have jupyter notebooks and opencv installed on
            your system, yes. make sure you replace the command line
            arguments with hard coded paths, though.
            [181]reply
   25. ballu october 14, 2017 at 5:29 am [182]#
       line 59 always exits with killed     
       [183]reply
          + adrian rosebrock october 14, 2017 at 10:31 am [184]#
            what system are you executing the script on? normally the
            vague    killed    message happens due to an incorrect compile or
            the system running out of memory.
            [185]reply
   26. abid november 8, 2017 at 12:27 pm [186]#
       hi, adrian rosebrock, i need to know that how could i found and
       draw the bounding boxes around the detected objects
       [187]reply
          + adrian rosebrock november 9, 2017 at 6:22 am [188]#
            please take a look at [189]this blog post on id164
            with deep learning.
            [190]reply
               o abid november 10, 2017 at 12:55 am [191]#
                 sir can   t we do it using keras
                 [192]reply
                    # adrian rosebrock november 13, 2017 at 2:17 pm [193]#
                      yes, but not easily. i   m covering how to train your
                      own custom object detectors inside my new book,
                      [194]deep learning for id161 with python.
                      [195]reply
   27. [196]samad january 4, 2018 at 3:33 am [197]#
       dear dr.adrian, thank you, i   d like to design a system that capture
       abnormal activity from camera, please guide me.
       [198]reply
   28. xingtao wei january 5, 2018 at 10:54 am [199]#
       nice tutorial. i used keras with tensorflow backend to fine-tuning
       an inceptionv3 model, and i saw the model size tripled after
       fine-tuning. that is, the original inceptionv3 model was about
       98mb, and the size grew to 288mb after fine-tuning. any ideas on
       the reason?
       [200]reply
          + adrian rosebrock january 5, 2018 at 1:23 pm [201]#
            i would suggest checking to see if the optimizer status was
            serialized to the model as well. you can delete it via:
            python

            with h5py.file(model_file, 'a') as f:_______________________
                    if 'optimizer_weights' in f.keys():_________________
                        del f['optimizer_weights']______________________
            ____________________________________________________________

   1

   2

   3

   with h5py.file(model_file, 'a') as f:

           if 'optimizer_weights' in f.keys():

               del f['optimizer_weights']
            [202]reply
   29. niladri february 26, 2018 at 3:28 am [203]#
       hi adrian,
       i am using live video stream to detect the objects which are
       labelled, only issue is that..i want to print the detected objects
       regularly..in a stream and not the output of the detection after
       stopping the script. could you please provide any help or snippet.
       thanks
       [204]reply
          + adrian rosebrock february 26, 2018 at 1:45 pm [205]#
            hey niladri     can you elaborate more on what you mean by    in a
            stream and not the output of detection after stopping the
            script   . i   m not sure what you mean.
            [206]reply
   30. anusha prakash march 2, 2018 at 12:05 am [207]#
       what was the conclusion? which of the models work best? and which
       layer features are best to pass to a classifier?
       [208]reply
          + adrian rosebrock march 2, 2018 at 10:27 am [209]#
            there is no    best model    as it is highly dependent on your
            image classification project. the same goes for the best
            feature extraction layer in a network. if you   re interested in
            learning more about these best practices and which
            models/layers to choose for a given project, i would suggest
            working through [210]deep learning for id161 with
            python.
            [211]reply
   31. zubair march 18, 2018 at 1:34 am [212]#
       hi adrian
       i am zubair nawaz and i want to run this program on video not on
       images. how i can give videos path or anything else that will work
       not images.
       thanks.
       waiting for your quick reply.
       [213]reply
          + adrian rosebrock march 19, 2018 at 5:17 pm [214]#
            you should use the    cv2.videocapture    function to access your
            video file. [215]this blog post will help you get started.
            [216]reply
   32. adesh march 21, 2018 at 8:13 am [217]#
       hey adrian, how can i draw a rectangle around the detected objects
       ?
       [218]reply
          + adrian rosebrock march 22, 2018 at 9:59 am [219]#
            you need a network train for id164 not image
            classification. i would suggest starting with [220]this post.
            [221]reply
   33. tom march 22, 2018 at 7:15 pm [222]#
       which model will be good for painting cross verification, if the
       painting is original or not?
       [223]reply
   34. gagandeep singh april 10, 2018 at 3:04 am [224]#
       hi adrian,
       is it possible to draw bounding box (basically id164)
       while using inception or alexnet? do we have to apply selective
       segmentation or something similar before feeding the image for
       evaluation or is there any other neural network that can identify
       roi first (specially in tensorflow)?
       thanks!
       [225]reply
          + gagandeep singh april 10, 2018 at 3:07 am [226]#
            p.s. i dont want to use ssd or fast rid98 models!
            [227]reply
               o adrian rosebrock april 10, 2018 at 11:57 am [228]#
                 no, unfortunately you cannot use a network trained for
                 image classification directly for id164. i   ll
                 be covering this in more detail in a blog post publishing
                 later this month/early next. there is a hack you can do,
                 however. you can treat an image classifier as an object
                 detector by:
                 1. applying a [229]sliding window + [230]image pyramid
                 2. extracting the roi at each step along the way
                 3. classifying the roi with the network
                 i actually demonstrate exactly how to do this inside the
                 practitioner bundle of [231]deep learning for computer
                 vision with python.
                 [232]reply
   35. oluchi may 6, 2018 at 7:50 pm [233]#
       hello,i am working on license plate detection using deep
       learning,am planing to use vggnet16 pre-trained model for the final
       verification of the license plate bounding box.i have successfully
       extracted my license plate region and want to use id98 to verify for
       the true license plate region from among the candidate region.i
       dont really know how to go about it.
       [234]reply
          + adrian rosebrock may 9, 2018 at 10:12 am [235]#
            i would suggest two approaches using id21:
            1. fine-tuning
            2. feature extraction and training a model on top of the
            features
            i cover both inside [236]deep learning for id161
            with python.
            i hope that helps point you in the right direction or at the
            very least gives you some more terms to go on.
            [237]reply
   36. ashish gupta june 18, 2018 at 9:30 am [238]#
       which is the most accurate architecture on id163 among alexnet,
       resnet, inception, vgg?
       [239]reply
          + adrian rosebrock june 19, 2018 at 8:43 am [240]#
            on id163 specifically? resnet is typically the most
            accurate.
            [241]reply
   37. pj september 13, 2018 at 6:48 am [242]#
       hi,
       i am getting the output like this :
       [info] loading inception   
       [info] loading and pre-processing image   
       [info] classifying image with    inception      
       1. wreck: 96.29%
       2. seashore: 0.42%
       3. canoe: 0.22%
       4. paddle: 0.14%
       5. breakwater: 0.10%
       it matches with your output but then
       cv2.imshow(   classification   ,orig)
       is showing error   ..?
       i have written the command
       cv2.imwrite(   classified_image44.png   ,orig) ,
       it is creating .png file with empty contents.
       i tried the model vgg16,vgg19 also
       [243]reply
          + adrian rosebrock september 14, 2018 at 9:38 am [244]#
            what specifically is the error you are receiving?
            [245]reply
   38. moses waiming wong october 12, 2018 at 2:24 pm [246]#
       xception has one key feature worth mentioning which is residual to
       enable deeper network without vanishing gradient. this is the
       accent of residual network and fchollet   s idea is to combine the
       advantage of inception above id98 with this residual feature and
       making really good results.
       [247]reply
   39. abbas october 25, 2018 at 11:33 pm [248]#
       hi adrain! which id98 architecture is best for features extraction
       from images??inception or xception?
       [249]reply
          + adrian rosebrock october 29, 2018 at 1:47 pm [250]#
            no single network is    best    at feature extraction. some models
            generalize better than others but it   s really dependent on
            your actual project. i would suggest you try multiple models,
            run experiments, and let your empirical results guide your
            decisions.
            [251]reply
   40. abbas october 29, 2018 at 12:47 am [252]#
       great tutorial adrain!! do you have any video tutorials??
       [253]reply
          + adrian rosebrock october 29, 2018 at 1:16 pm [254]#
            i don   t do video very often. i   m a writer and i prefer writing
            rather than creating video.
            [255]reply
               o abbas november 17, 2018 at 12:34 am [256]#
                 do you have any tutorial regarding image captioning using
                 inception model??
                 [257]reply
                    # adrian rosebrock november 19, 2018 at 12:46 pm
                      [258]#
                      sorry, i do not have any tutorials for image
                      captioning as of yet.
                      [259]reply
   41. phyu phyu december 24, 2018 at 9:48 pm [260]#
       hi!
       i want to know about inception score and do you have any tutorials
       for it? it is related to make a decision that the generated image
       is true or false/better or worse. although we can see with human
       eyes, we need to show in words. could you give me an idea if you
       don   t mind?
       [261]reply
          + adrian rosebrock december 27, 2018 at 10:31 am [262]#
            sorry, i   m not sure what you mean by the    inception score    and
            the generated image     are you referring to gans   ?
            [263]reply
   42. johnny january 3, 2019 at 5:24 pm [264]#
       hello adrian! great tutorial.
       i wonder if i can use this this code on a raspberry pi with
       movidius usb stick?
       [265]reply
          + adrian rosebrock january 5, 2019 at 8:42 am [266]#
            you bet. see [267]this tutorial.
            [268]reply
   43. pearline february 27, 2019 at 1:56 am [269]#
       sir,
       that was a great post to learn about id98 models quickly. !!! i have
       a doubt regarding depthwise separable convolution of xception and
       mobilenet. the depthwise separable convolution used by both models
       are same or different, sir
       [270]reply
          + adrian rosebrock february 27, 2019 at 5:28 am [271]#
            only the xception network utilizes depthwise separable
            convolution, the others listed in this post use standard
            convolution.
            [272]reply

leave a reply [273]click here to cancel reply.

   comment
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________

   ______________________________name (required)

   ______________________________email (will not be published) (required)

   ______________________________website

   submit comment

   search...___________ (search)

resource guide (it   s totally free).

   [274]get your free 17-page id161 and deep learning resource
   guide pdf

   get your free 17 page id161, opencv, and deep learning
   resource guide pdf. inside you'll find my hand-picked tutorials, books,
   courses, and libraries to help you master cv and dl.

                           [275]download for free!

deep learning for id161 with python book     out now!

   [276]deep learning with id161 and python kickstarter

   you're interested in deep learning and id161, but you don't
   know how to get started. let me help. [277]my new book will teach you
   all you need to know about deep learning.

   click here to master deep learning

you can detect faces in images & video.

   [278]learn how to detect faces in images and video

   are you interested in detecting faces in images & video? but tired of
   googling for tutorials that never work? then let me help! i guarantee
   that my new book will turn you into a face detection ninja by the end
   of this weekend. [279]click here to give it a shot yourself.

   click here to master face detection

pyimagesearch gurus: now enrolling!

   the pyimagesearch gurus course is now enrolling! inside the course
   you'll learn how to perform:
     * automatic license plate recognition (anpr)
     * deep learning
     * face recognition
     * and much more!

   click the button below to learn more about the course, take a tour, and
   get 10 (free) sample lessons.

   take a tour & get 10 (free) lessons

hello! i   m adrian rosebrock.

   i'm an entrepreneur and ph.d who has launched two successful image
   search engines, [280]id my pill and [281]chic engine. i'm here to share
   my tips, tricks, and hacks i've learned along the way.

learn id161 in a single weekend.

   [282]become an opencv guru

   want to learn id161 & opencv? i can teach you in a single
   weekend. i know. it sounds crazy, but it   s no joke. my new book is your
   guaranteed, quick-start guide to becoming an opencv ninja. so why not
   give it a try? [283]click here to become a id161 ninja.

   click here to become an opencv ninja

subscribe via rss

   [284]pyimagesearch rss feed

   never miss a post! subscribe to the pyimagesearch rss feed and keep up
   to date with my image search engine tutorials, tips, and tricks
     * [285]popular

     * [286]raspbian stretch: install opencv 3 + python on your raspberry
       pi september 4, 2017
     * [287]install guide: raspberry pi 3 + raspbian jessie + opencv 3
       april 18, 2016
     * [288]home surveillance and motion detection with the raspberry pi,
       python, opencv, and dropbox june 1, 2015
     * [289]install opencv and python on your raspberry pi 2 and b+
       february 23, 2015
     * [290]ubuntu 16.04: how to install opencv october 24, 2016
     * [291]real-time id164 with deep learning and opencv
       september 18, 2017
     * [292]basic motion detection and tracking with python and opencv may
       25, 2015

   find me on [293]twitter, [294]facebook, and [295]linkedin.

      2019 pyimagesearch. all rights reserved.

   [tr?id=1465896023527386&ev=pageview&noscript=1]

   [email]
   [email]

references

   1. http://feeds.feedburner.com/pyimagesearch
   2. https://www.pyimagesearch.com/comments/feed/
   3. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/feed/
   4. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/
   5. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/&format=xml
   6. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#navigation
   7. https://www.pyimagesearch.com/
   8. https://www.pyimagesearch.com/
   9. https://www.pyimagesearch.com/
  10. https://www.pyimagesearch.com/start-here-learn-computer-vision-opencv/
  11. https://www.pyimagesearch.com/practical-python-opencv/
  12. https://www.pyimagesearch.com/pyimagesearch-gurus/
  13. https://www.pyimagesearch.com/opencv-tutorials-resources-guides/
  14. https://www.pyimagesearch.com/free-opencv-computer-vision-deep-learning-crash-course/
  15. https://www.pyimagesearch.com/about/
  16. https://www.pyimagesearch.com/contact/
  17. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#top
  18. https://www.pyimagesearch.com/author/adrian/
  19. https://www.pyimagesearch.com/category/deep-learning-2/
  20. https://www.pyimagesearch.com/category/machine-learning-2/
  21. https://www.pyimagesearch.com/category/tutorials/
  22. https://app.monstercampaigns.com/c/tortsem7qkvyuxc4cyfi/
  23. https://www.pyimagesearch.com/2016/08/10/id163-classification-with-python-and-keras/
  24. https://github.com/fchollet/deep-learning-models
  25. https://github.com/fchollet/keras/tree/master/keras/applications
  26. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/
  27. http://image-net.org/
  28. http://www.image-net.org/challenges/lsvrc/
  29. http://image-net.org/challenges/lsvrc/2014/browse-synsets
  30. https://www.cs.toronto.edu/~frossard/post/vgg16/
  31. https://arxiv.org/abs/1409.1556
  32. https://arxiv.org/abs/1409.1556
  33. https://arxiv.org/abs/1502.01852
  34. https://arxiv.org/abs/1511.06422
  35. https://arxiv.org/abs/1512.03385
  36. https://arxiv.org/abs/1603.05027
  37. https://arxiv.org/abs/1409.4842
  38. https://arxiv.org/abs/1512.00567
  39. https://twitter.com/fchollet
  40. https://arxiv.org/abs/1610.02357
  41. https://arxiv.org/abs/1602.07360
  42. https://arxiv.org/abs/1602.07360
  43. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  44. https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py#l389
  45. https://blog.keras.io/introducing-keras-2.html?t=1
  46. https://www.tensorflow.org/install/
  47. https://www.tensorflow.org/install/
  48. https://pl.wiktionary.org/wiki/piedpilko
  49. https://www.pinterest.com/explore/bmw-convertible/
  50. http://i.dailymail.co.uk/i/pix/2009/09/24/article-1215766-0009b05300000258-375_468x286.jpg
  51. https://www.aliexpress.com/item/pirates-of-the-caribbean-boats-ship-boy-nature-tropical-sea-waves-trees-tv-movie-film-poster/32607808033.html
  52. http://www.slrmag.co.uk/lagavulin-rolls-out-the-barrel-to-mark-double-century/
  53. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  54. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  55. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  56. https://www.pyimagesearch.com/tag/inception/
  57. https://www.pyimagesearch.com/tag/keras/
  58. https://www.pyimagesearch.com/tag/micro-architecture/
  59. https://www.pyimagesearch.com/tag/resnet/
  60. https://www.pyimagesearch.com/tag/vgg/
  61. https://www.pyimagesearch.com/tag/xception/
  62. https://www.pyimagesearch.com/2017/03/13/an-interview-with-davis-king-creator-of-the-dlib-toolkit/
  63. https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/
  64. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420811
  65. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420811
  66. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420887
  67. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  68. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420887
  69. http://www.brlnce.com/
  70. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-446501
  71. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-446501
  72. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420812
  73. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420812
  74. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420886
  75. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420886
  76. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420815
  77. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420815
  78. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420829
  79. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420829
  80. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420883
  81. https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/
  82. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420883
  83. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420858
  84. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420858
  85. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420878
  86. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420878
  87. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420959
  88. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420959
  89. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420978
  90. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420978
  91. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420991
  92. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420991
  93. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420994
  94. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-420994
  95. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421070
  96. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421070
  97. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421186
  98. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421186
  99. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421071
 100. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421071
 101. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421185
 102. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421185
 103. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421120
 104. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421120
 105. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421172
 106. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 107. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421172
 108. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421214
 109. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421214
 110. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421404
 111. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421404
 112. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421426
 113. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421426
 114. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421639
 115. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421639
 116. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421603
 117. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-421603
 118. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422544
 119. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422544
 120. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422620
 121. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 122. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422620
 123. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422677
 124. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422677
 125. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422679
 126. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422679
 127. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422947
 128. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422947
 129. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422887
 130. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422887
 131. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422931
 132. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422931
 133. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422906
 134. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422906
 135. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422909
 136. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422909
 137. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422930
 138. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-422930
 139. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423405
 140. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423405
 141. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423591
 142. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 143. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423591
 144. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423940
 145. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423940
 146. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423988
 147. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 148. https://www.pyimagesearch.com/pyimagesearch-gurus/
 149. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-423988
 150. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-427590
 151. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-427590
 152. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-427767
 153. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-427767
 154. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-429618
 155. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-429618
 156. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-429666
 157. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 158. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-429666
 159. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-431874
 160. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-431874
 161. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-432060
 162. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-432060
 163. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-433867
 164. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-433867
 165. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-434147
 166. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 167. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-434147
 168. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435032
 169. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435032
 170. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435111
 171. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 172. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435111
 173. http://na/
 174. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435854
 175. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435854
 176. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435921
 177. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-435921
 178. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-436032
 179. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-436032
 180. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-436257
 181. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-436257
 182. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-437514
 183. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-437514
 184. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-437556
 185. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-437556
 186. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-439997
 187. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-439997
 188. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440069
 189. https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/
 190. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440069
 191. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440152
 192. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440152
 193. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440482
 194. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 195. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-440482
 196. http://-/
 197. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445576
 198. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445576
 199. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445710
 200. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445710
 201. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445715
 202. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-445715
 203. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451317
 204. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451317
 205. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451372
 206. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451372
 207. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451725
 208. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451725
 209. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451764
 210. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 211. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-451764
 212. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453422
 213. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453422
 214. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453579
 215. https://www.pyimagesearch.com/2017/01/09/count-the-total-number-of-frames-in-a-video-with-opencv-and-python/
 216. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453579
 217. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453808
 218. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453808
 219. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453939
 220. https://www.pyimagesearch.com/2017/09/11/object-detection-with-deep-learning-and-opencv/
 221. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453939
 222. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453983
 223. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-453983
 224. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455846
 225. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455846
 226. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455847
 227. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455847
 228. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455931
 229. https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/
 230. https://www.pyimagesearch.com/2015/03/16/image-pyramids-with-python-and-opencv/
 231. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 232. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-455931
 233. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-460185
 234. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-460185
 235. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-460673
 236. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 237. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-460673
 238. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-468369
 239. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-468369
 240. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-468539
 241. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-468539
 242. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-477855
 243. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-477855
 244. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-478083
 245. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-478083
 246. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-482240
 247. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-482240
 248. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-483959
 249. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-483959
 250. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484473
 251. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484473
 252. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484344
 253. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484344
 254. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484439
 255. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-484439
 256. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-487527
 257. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-487527
 258. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-487855
 259. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-487855
 260. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-493498
 261. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-493498
 262. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-493832
 263. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-493832
 264. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-494654
 265. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-494654
 266. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-494810
 267. https://www.pyimagesearch.com/2018/02/19/real-time-object-detection-on-the-raspberry-pi-with-the-movidius-ncs/
 268. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-494810
 269. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-503660
 270. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-503660
 271. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-503694
 272. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#comment-503694
 273. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#respond
 274. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
 275. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
 276. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 277. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 278. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
 279. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
 280. http://www.idmypill.com/
 281. http://www.chicengine.com/
 282. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
 283. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
 284. http://feeds.feedburner.com/pyimagesearch
 285. https://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/#tab-pop
 286. https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/
 287. https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/
 288. https://www.pyimagesearch.com/2015/06/01/home-surveillance-and-motion-detection-with-the-raspberry-pi-python-and-opencv/
 289. https://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/
 290. https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/
 291. https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/
 292. https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/
 293. https://twitter.com/pyimagesearch
 294. https://www.facebook.com/pyimagesearch
 295. http://www.linkedin.com/pub/adrian-rosebrock/2a/873/59b
