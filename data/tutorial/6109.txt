   #[1]pyimagesearch    feed [2]pyimagesearch    comments feed
   [3]pyimagesearch    stochastic id119 (sgd) with python
   comments feed [4]alternate [5]alternate

[6]navigation

   [7]pyimagesearch [8]pyimagesearch be awesome at opencv, python, deep
   learning, and id161

   [9]home

main menu

     * [10]start here
     * [11]practical python and opencv
     * [12]pyimagesearch gurus
     * [13]opencv 3 & 4 tutorials
     * [14]free crash course
     * [15]about
     * [16]contact

   [17]return to content

stochastic id119 (sgd) with python

   by [18]adrian rosebrock on october 17, 2016 in [19]deep learning,
   [20]machine learning, [21]tutorials

   figure 2: the id168 associated with stochastic gradient
   descent. loss continues to decrease as we allow more epochs to pass.

   in last week   s blog post, we discussed [22]id119, a
   first-order optimization algorithm that can be used to learn a set of
   classifier coefficients for [23]parameterized learning.

   however, the    vanilla    implementation of id119 can be
   prohibitively slow to run on large datasets     in fact, it can even be
   considered computationally wasteful.

   instead, we should apply stochastic id119 (sgd), a simple
   modification to the standard id119 algorithm that computes
   the gradient and updates our weight matrix w on small batches of
   training data, rather than the entire training set itself.

   while this leads to    noiser    weight updates, it also allows us to
   take more steps along the gradient (1 step for each batch versus 1 step
   per epoch), ultimately leading to faster convergence and no negative
   affects to loss and classification accuracy.

   to learn more about stochastic id119, keep reading.

   looking for the source code to this post?
   [24]jump right to the downloads section.

stochastic id119 (sgd) with python

   taking a look at [25]last week   s blog post, it should be (at least
   somewhat) obvious that the id119 algorithm will run very
   slowly on large datasets. the reason for this    slowness    is because
   each iteration of id119 requires that we compute a
   prediction for each training point in our training data.

   for image datasets such as id163 where we have over 1.2 million
   training images, this computation can take a long time.

   it also turns out that computing predictions for every training data
   point before taking a step and updating our weight matrix w is
   computationally wasteful (and doesn   t help us in the long run).

   instead, what we should do is batch our updates.

updating our id119 optimization algorithm

   before i discuss stochastic id119 in more detail, let   s
   first look at the original id119 pseudocode and then the
   updated, sgd pseudocode, both inspired by the [26]cs231n course slides.

   below follows the pseudocode for vanilla id119:
   vanilla id119
   python

   while true:_________________________________________________
   	wgradient = evaluate_gradient(loss, data, w)_______________
   	w += -alpha * wgradient____________________________________
   ____________________________________________________________
   1
   2
   3
   while true:
   wgradient = evaluate_gradient(loss, data, w)
   w += -alpha * wgradient

   and here we can see the pseudocode for stochastic id119:
   stochastic id119 (sgd)
   python

   while true:_________________________________________________
   	batch = next_training_batch(data, 256)_____________________
   	wgradient = evaluate_gradient(loss, batch, w)______________
   	w += -alpha * wgradient____________________________________
   1
   2
   3
   4
   while true:
   batch = next_training_batch(data, 256)
   wgradient = evaluate_gradient(loss, batch, w)
   w += -alpha * wgradient

   as you can see, the implementations are quite similar.

   the only difference between vanilla id119 and stochastic
   id119 is the addition of the next_training_batch  function.
   instead of computing our gradient over the entire data  set, we instead
   sample our data, yielding a batch .

   we then evaluate the gradient on this batch  and update our weight
   matrix w.

   note: for an implementation perspective, we also randomize our training
   samples before applying sgd.

batching id119 for machine learning

   after looking at the pseudocode for sgd, you   ll immediately notice an
   introduction of a new parameter: the batch size.

   in a    purist    implementation of sgd, your mini-batch size would be set
   to 1. however, we often uses mini-batches that are > 1. typical values
   include 32, 64, 128, and 256.

   so, why are these common mini-batch size values?

   to start, using batches > 1 helps [27]reduce variance in the parameter
   update, ultimately leading to a more stable convergence.

   secondly, optimized matrix operation libraries are often more efficient
   when the input matrix size is a power of 2.

   in general, the mini-batch size is not a hyperparameter that you should
   worry much about. you basically determine how many training examples
   will fit on your gpu/main memory and then use the nearest power of 2 as
   the batch size.

implementing stochastic id119 (sgd) with python

   we are now ready to update our code from last week   s blog post on
   [28]vanilla id119. since i have already reviewed this code
   in detail earlier, i   ll defer an exhaustive, thorough review of each
   line of code to last week   s post.

   that said, i will still be pointing out the salient, important lines of
   code in this example.

   to get started, open up a new file, name it sgd.py , and insert the
   following code:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   # import the necessary packages
   import matplotlib.pyplot as plt
   from sklearn.datasets.samples_generator import make_blobs
   import numpy as np
   import argparse

   def sigmoid_activation(x):
   # compute and return the sigmoid activation value for a
   # given input value
   return 1.0 / (1 + np.exp(-x))

   def next_batch(x, y, batchsize):
   # loop over our dataset `x` in mini-batches of size `batchsize`
   for i in np.arange(0, x.shape[0], batchsize):
   # yield a tuple of the current batched data and labels
   yield (x[i:i + batchsize], y[i:i + batchsize])

   lines 2-5 start by importing our required python packages. then, line 7
   defines our sigmoid_activation  function used during the training
   process.

   in order to apply stochastic id119, we need a function that
   yields mini-batches of training data     and that is exactly what the
   next_batch  function on lines 12-16 does.

   the next_batch method requires three parameters:
     * x : our training dataset of feature vectors.
     * y : the class labels associated with each of the training data
       points.
     * batchsize : the size of each mini-batch that will be returned.

   lines 14-16 then loop over our training examples, yielding subsets of
   both x  and y  as mini-batches.

   next, let   s parse our command line arguments:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   ap.add_argument("-b", "--batch-size", type=int, default=32,_
   	help="size of sgd mini-batches")___________________________
   args = vars(ap.parse_args())________________________________
   18
   19
   20
   21
   22
   23
   24
   25
   26
   # construct the argument parse and parse the arguments
   ap = argparse.argumentparser()
   ap.add_argument("-e", "--epochs", type=float, default=100,
   help="# of epochs")
   ap.add_argument("-a", "--alpha", type=float, default=0.01,
   help="learning rate")
   ap.add_argument("-b", "--batch-size", type=int, default=32,
   help="size of sgd mini-batches")
   args = vars(ap.parse_args())

   lines 19-26 parse our (optional) command line arguments.

   the --epochs  switch controls the number of epochs, or rather, the
   number of times the training process    sees    each individual training
   example.

   the --alpha  value controls our learning rate in the id119
   algorithm.

   and finally, the --batch-size  indicates the size of each of our
   mini-batches. we   ll default this value to be 32.

   in order to apply stochastic id119, we need a dataset. below
   we generate some data to work with:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   ap.add_argument("-b", "--batch-size", type=int, default=32,_
   	help="size of sgd mini-batches")___________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 400 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=400, n_features=2, centers=2,_
   	cluster_std=2.5, random_state=95)__________________________
   28
   29
   30
   31
   # generate a 2-class classification problem with 400 data points,
   # where each data point is a 2d feature vector
   (x, y) = make_blobs(n_samples=400, n_features=2, centers=2,
   cluster_std=2.5, random_state=95)

   above we generate a 2-class classification problem. we have a total of
   400 data points, each of which are 2d. 200 data points belong to class
   0 and the remaining 200 to class 1.

   our goal is to correctly classify each of these 400 data points into
   their respective classes.

   now let   s perform some initializations:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   ap.add_argument("-b", "--batch-size", type=int, default=32,_
   	help="size of sgd mini-batches")___________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 400 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=400, n_features=2, centers=2,_
   	cluster_std=2.5, random_state=95)__________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   33
   34
   35
   36
   37
   38
   39
   40
   41
   42
   43
   44
   45
   # insert a column of 1's as the first entry in the feature
   # vector -- this is a little trick that allows us to treat
   # the bias as a trainable parameter *within* the weight matrix
   # rather than an entirely separate variable
   x = np.c_[np.ones((x.shape[0])), x]

   # initialize our weight matrix such it has the same number of
   # columns as our input features
   print("[info] starting training...")
   w = np.random.uniform(size=(x.shape[1],))

   # initialize a list to store the loss value for each epoch
   losshistory = []

   for a more through review of this section, [29]please see last week   s
   tutorial.

   below follows our actual stochastic id119 (sgd)
   implementation:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   ap.add_argument("-b", "--batch-size", type=int, default=32,_
   	help="size of sgd mini-batches")___________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 400 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=400, n_features=2, centers=2,_
   	cluster_std=2.5, random_state=95)__________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# initialize the total loss for the epoch__________________
   	epochloss = []_____________________________________________
   ____________________________________________________________
   	# loop over our data in batches____________________________
   	for (batchx, batchy) in next_batch(x, y, args["batch_size"]
   		# take the dot product between our current batch of_______
   		# features and weight matrix `w`, then pass this value____
   		# through the sigmoid activation function_________________
   		preds = sigmoid_activation(batchx.dot(w))_________________
   ____________________________________________________________
   		# now that we have our predictions, we need to determine__
   		# our `error`, which is the difference between our predict
   		# and the true values_____________________________________
   		error = preds - batchy____________________________________
   ____________________________________________________________
   		# given our `error`, we can compute the total loss value o
   		# the batch as the sum of squared loss____________________
   		loss = np.sum(error ** 2)_________________________________
   		epochloss.append(loss)____________________________________
   ____________________________________________________________
   		# the gradient update is therefore the dot product between
   		# the transpose of our current batch and the error on the_
   		# # batch_________________________________________________
   		gradient = batchx.t.dot(error) / batchx.shape[0]__________
   ____________________________________________________________
   		# use the gradient computed on the current batch to take__
   		# a "step" in the correct direction_______________________
   		w += -args["alpha"] * gradient____________________________
   ____________________________________________________________
   	# update our loss history list by taking the average loss__
   	# across all batches_______________________________________
   	losshistory.append(np.average(epochloss))__________________
   47
   48
   49
   50
   51
   52
   53
   54
   55
   56
   57
   58
   59
   60
   61
   62
   63
   64
   65
   66
   67
   68
   69
   70
   71
   72
   73
   74
   75
   76
   77
   78
   79
   80
   # loop over the desired number of epochs
   for epoch in np.arange(0, args["epochs"]):
   # initialize the total loss for the epoch
   epochloss = []

   # loop over our data in batches
   for (batchx, batchy) in next_batch(x, y, args["batch_size"]):
   # take the dot product between our current batch of
   # features and weight matrix `w`, then pass this value
   # through the sigmoid activation function
   preds = sigmoid_activation(batchx.dot(w))

   # now that we have our predictions, we need to determine
   # our `error`, which is the difference between our predictions
   # and the true values
   error = preds - batchy

   # given our `error`, we can compute the total loss value on
   # the batch as the sum of squared loss
   loss = np.sum(error ** 2)
   epochloss.append(loss)

   # the gradient update is therefore the dot product between
   # the transpose of our current batch and the error on the
   # # batch
   gradient = batchx.t.dot(error) / batchx.shape[0]

   # use the gradient computed on the current batch to take
   # a "step" in the correct direction
   w += -args["alpha"] * gradient

   # update our loss history list by taking the average loss
   # across all batches
   losshistory.append(np.average(epochloss))

   on line 48 we start looping over the desired number of epochs.

   we then initialize an epochloss  list to store the loss value for each
   of the mini-batch gradient updates. as we   ll see later in this code
   block, the epochloss  list will be used to compute the average loss
   over all mini-batch updates for an entire epoch.

   line 53 is the    core    of the stochastic id119 algorithm and
   is what separates it from the vanilla id119 algorithm     we
   loop over our training samples in mini-batches.

   for each of these mini-batches, we take the data, compute the dot
   product between it and the weight matrix, and then pass the results
   through the sigmoid activation function to obtain our predictions.

   line 62 computes the error  between the these predictions, allowing us
   to minimize the [30]least squares loss on line 66.

   line 72 evaluates the gradient for the current batch. once we have the
   gradient , we can update the weight matrix w  on line 76 by taking a
   step, scaled by our learning rate --alpha .

   again, for a more thorough, detailed review of the id119
   algorithm, [31]please refer to last week   s tutorial.

   our last code block handles visualizing our data points along with the
   decision boundary learned by the stochastic id119 algorithm:
   stochastic id119 (sgd) with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   def next_batch(x, y, batchsize):____________________________
   	# loop over our dataset `x` in mini-batches of size `batchs
   	for i in np.arange(0, x.shape[0], batchsize):______________
   		# yield a tuple of the current batched data and labels____
   		yield (x[i:i + batchsize], y[i:i + batchsize])____________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   ap.add_argument("-b", "--batch-size", type=int, default=32,_
   	help="size of sgd mini-batches")___________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 400 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=400, n_features=2, centers=2,_
   	cluster_std=2.5, random_state=95)__________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# initialize the total loss for the epoch__________________
   	epochloss = []_____________________________________________
   ____________________________________________________________
   	# loop over our data in batches____________________________
   	for (batchx, batchy) in next_batch(x, y, args["batch_size"]
   		# take the dot product between our current batch of_______
   		# features and weight matrix `w`, then pass this value____
   		# through the sigmoid activation function_________________
   		preds = sigmoid_activation(batchx.dot(w))_________________
   ____________________________________________________________
   		# now that we have our predictions, we need to determine__
   		# our `error`, which is the difference between our predict
   		# and the true values_____________________________________
   		error = preds - batchy____________________________________
   ____________________________________________________________
   		# given our `error`, we can compute the total loss value o
   		# the batch as the sum of squared loss____________________
   		loss = np.sum(error ** 2)_________________________________
   		epochloss.append(loss)____________________________________
   ____________________________________________________________
   		# the gradient update is therefore the dot product between
   		# the transpose of our current batch and the error on the_
   		# # batch_________________________________________________
   		gradient = batchx.t.dot(error) / batchx.shape[0]__________
   ____________________________________________________________
   		# use the gradient computed on the current batch to take__
   		# a "step" in the correct direction_______________________
   		w += -args["alpha"] * gradient____________________________
   ____________________________________________________________
   	# update our loss history list by taking the average loss__
   	# across all batches_______________________________________
   	losshistory.append(np.average(epochloss))__________________
   ____________________________________________________________
   # compute the line of best fit by setting the sigmoid functi
   # to 0 and solving for x2 in terms of x1____________________
   y = (-w[0] - (w[1] * x)) / w[2]_____________________________
   ____________________________________________________________
   # plot the original data along with our line of best fit____
   plt.figure()________________________________________________
   plt.scatter(x[:, 1], x[:, 2], marker="o", c=y)______________
   plt.plot(x, y, "r-")________________________________________
   ____________________________________________________________
   # construct a figure that plots the loss over time__________
   fig = plt.figure()__________________________________________
   plt.plot(np.arange(0, args["epochs"]), losshistory)_________
   fig.suptitle("training loss")_______________________________
   plt.xlabel("epoch #")_______________________________________
   plt.ylabel("loss")__________________________________________
   plt.show()__________________________________________________
   82
   83
   84
   85
   86
   87
   88
   89
   90
   91
   92
   93
   94
   95
   96
   97
   # compute the line of best fit by setting the sigmoid function
   # to 0 and solving for x2 in terms of x1
   y = (-w[0] - (w[1] * x)) / w[2]

   # plot the original data along with our line of best fit
   plt.figure()
   plt.scatter(x[:, 1], x[:, 2], marker="o", c=y)
   plt.plot(x, y, "r-")

   # construct a figure that plots the loss over time
   fig = plt.figure()
   plt.plot(np.arange(0, args["epochs"]), losshistory)
   fig.suptitle("training loss")
   plt.xlabel("epoch #")
   plt.ylabel("loss")
   plt.show()

visualizing stochastic id119 (sgd)

   to execute the code associated with this blog post, be sure to download
   the code using the    downloads    section at the bottom of this tutorial.

   from there, you can execute the following command:
   stochastic id119 (sgd) with python
   python

   $ python sgd.py_____________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python sgd.py

   you should then see the following plot displayed to your screen:
   figure 1: learning the classification decision boundary using
   stochastic id119.

   figure 1: learning the classification decision boundary using
   stochastic id119.

   as the plot demonstrates, we are able to learn a weight matrix w that
   correctly classifies each of the data points.

   i have also included a plot that visualizes loss decreasing in further
   iterations of the stochastic id119 algorithm:
   figure 2: the id168 associated with stochastic gradient
   descent. loss continues to decrease as we allow more epochs to pass.

   figure 2: the id168 associated with stochastic gradient
   descent. loss continues to decrease as we allow more epochs to pass.

summary

   in today   s blog post, we learned about stochastic id119
   (sgd), an extremely common extension to the vanilla id119
   algorithm. in fact, in nearly all situations, you   ll see sgd used
   instead of the original id119 version.

   sgd is also very common when training your own neural networks and deep
   learning classifiers. if you recall, a couple of weeks ago we used sgd
   to [32]train a simple neural network. we also used sgd when training
   [33]lenet, a common convolutional neural network.

   over the next couple of weeks i   ll be discussing some id161
   topics, but then we   ll pick up a thorough discussion of id26
   along with the various types of layers in convolutional neural networks
   come early november.

   be sure to use the form below to sign up for the pyimagesearch
   newsletter to be notified when future blog posts are published!

downloads:

   if you would like to download the code and images used in this post,
   please enter your email address in the form below. not only will you
   get a .zip of the code, i   ll also send you a free 17-page resource
   guide on id161, opencv, and deep learning. inside you'll find
   my hand-picked tutorials, books, courses, and libraries to help you
   master cv and dl! sound good? if so, enter your email address and i   ll
   send you the code immediately!

   email address: ____________________

   download the code!

resource guide (it   s totally free).

   get your free 17-page id161 and deep learning resource guide
   pdf
   enter your email address below to get my free 17-page id161,
   opencv, and deep learning resource guide pdf. inside you'll find my
   hand-picked tutorials, books, courses, and python libraries to help you
   master id161 and deep learning!
   ____________________
   download the guide!

   [34]classification, [35]deep learning, [36]id119,
   [37]machine learning, [38]neural nets, [39]sgd
   [40]id119 with python
   [41]ubuntu 16.04: how to install opencv

17 responses to stochastic id119 (sgd) with python

    1. wajih october 17, 2016 at 1:15 pm [42]#
       another excellent explaination of a very important topic in deep
       learning    this blog of yours is a must read!!
       [43]reply
          + adrian rosebrock october 17, 2016 at 1:59 pm [44]#
            thanks wajih! i   m glad you are enjoying the blog posts and
            tutorials. i   ll be sure to continue to do more deep learning
            tutorials in the future.
            [45]reply
               o wajih october 18, 2016 at 5:08 am [46]#
                 thats is really cool. looking forward for more important
                 stuff on deep learning!
                 [47]reply
                    # adrian rosebrock october 20, 2016 at 8:57 am [48]#
                      there will certainly be plenty more posts on deep
                      learning     
                      [49]reply
    2. i008 october 19, 2016 at 4:31 am [50]#
       line 84:
       y = (-w[0]     (w[1] * x)) / w[2]
       is not exactly clear to me, could someone please elaborate on that?
       [51]reply
          + hashim july 12, 2017 at 7:15 am [52]#
            you can think of it like this:
            in order to draw the decision boundary, you need to draw only
            the points (x,y) which lie right over the boundary.
            according to the sigmoid function, the boundary is the value
            0.5. so, in order to obtain a 0.5, you need to provide a zero
            value as input to the sigmoid (that is, a zero value as output
            from the scoring function).
            thus, if the scoring function equals zero:
            0 = w0 + w1*x + w2*y ==> y = (-w0     w1*x)/w2
            you can use any x   s coordinates you want, and you   ll get the
            proper y   s coordinates to draw the boundary
            [53]reply
    3. foobar april 13, 2017 at 12:21 pm [54]#
       great stuff, thanks!
       method name is not next_method btw
       [55]reply
          + adrian rosebrock april 16, 2017 at 9:02 am [56]#
            thanks! i have updated the blog post to fix this typo.
            [57]reply
    4. [58]i262666 january 4, 2018 at 12:33 am [59]#
       great blog and very clear explanation! however, the overall logic
       is quite strange. we loop over all epochs and inside each epoch we
       process all data points in each mini-batch and adjusting the
       weights for every mini-batch. and because we use the gradient
       descent, we move for each mini-batch toward the minimal of the loss
       function. and tracking the id168 for each epoch shows on
       the displayed plot that the error values are decreasing.
       so far, all is logical. but how you get the final weights for the
       trained network which best satisfies the targets for every data
       inputs. after training the network you star testing it against the
       test data-set. what weights we would use for testing the trained
       network?
       [60]reply
          + adrian rosebrock january 5, 2018 at 1:40 pm [61]#
            the weights you use to test the network would be the weights
            you obtained from training it. after the network has been
            trained the weights    freeze    and do not change. if you   re
            interested in learning more about id119 and how it
            works in the grand scheme of neural networks, take a look at
            my book, [62]deep learning for id161 with python.
            [63]reply
    5. misha singla april 13, 2018 at 7:33 am [64]#
       really helpful..but what are the changes required if hinge loss is
       considered instead of least squared error?
       [65]reply
    6. arron.niu august 3, 2018 at 5:12 am [66]#
       very thanks. i have pratised the vanilla id119 and the
       sgd according to the two posts. but i training 4000 and 40000
       samples. but the execute time of sgd is slowly than the vanilla
       id119. why?
       [67]reply
    7. [68]georgem november 9, 2018 at 11:40 am [69]#
       great post! and thank you for your work on making this article very
       well structured and informative.
       one question: i understand you split data in mini-batches and you
       computed gradients on each mini-batch. why is it called stochastic
       gradient decent? because you don   t have any stochasticity, you   re
       just summing all the gradients in a mini-batch.
       maybe my understanding is wrong.
       thank you!
       [70]reply
          + adrian rosebrock november 10, 2018 at 9:57 am [71]#
            true sgd is called    stochastic    because it randomly samples a
            single data point and then updates the weights. in practice
            that   s highly inefficient so we use mini-batch gradient
            descent which has taken the name of sgd.
            [72]reply
               o [73]georgem november 13, 2018 at 2:23 pm [74]#
                 thanks for your prompt answer adrian!
                    in practice that   s highly inefficient    -> inefficient
                 you mean that time of convergence might be high? true sgd
                 is faster than regular true gd right? true gd computes
                 the gradient for each example in the data so it takes
                 longer.
                 if we only do mini-batch, with no randomness is that
                 mini-batch gd?
                 and if we randomly shuffle the data at each epoch to have
                 different data in our mini-batches is called mini-batch
                 sdg?
                 sorry, i hope i   m not being annoying. i   ve been looking
                 everywhere online to clarify this. the deeper i dig, the
                 more confused i get lol.
                 thank you!
                 [75]reply
                    # adrian rosebrock november 13, 2018 at 4:10 pm [76]#
                      1. it   s inefficient in time to convergence.
                      2. but it also tends to be highly inefficient for
                      large datasets. you   ll be performing too many
                      non-consecutive i/o operations which slows the whole
                      operation down.
                      if you find yourself getting confused i would
                      suggest working through [77]deep learning for
                      id161 with python where i discuss these
                      concepts in far more detail.
                      [78]reply
                         @ [79]georgem november 15, 2018 at 9:05 am [80]#
                           ok, thank you adrian!

leave a reply [81]click here to cancel reply.

   comment
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________

   ______________________________name (required)

   ______________________________email (will not be published) (required)

   ______________________________website

   submit comment

   search...___________ (search)

resource guide (it   s totally free).

   [82]get your free 17-page id161 and deep learning resource
   guide pdf

   get your free 17 page id161, opencv, and deep learning
   resource guide pdf. inside you'll find my hand-picked tutorials, books,
   courses, and libraries to help you master cv and dl.

                           [83]download for free!

deep learning for id161 with python book     out now!

   [84]deep learning with id161 and python kickstarter

   you're interested in deep learning and id161, but you don't
   know how to get started. let me help. [85]my new book will teach you
   all you need to know about deep learning.

   click here to master deep learning

you can detect faces in images & video.

   [86]learn how to detect faces in images and video

   are you interested in detecting faces in images & video? but tired of
   googling for tutorials that never work? then let me help! i guarantee
   that my new book will turn you into a face detection ninja by the end
   of this weekend. [87]click here to give it a shot yourself.

   click here to master face detection

pyimagesearch gurus: now enrolling!

   the pyimagesearch gurus course is now enrolling! inside the course
   you'll learn how to perform:
     * automatic license plate recognition (anpr)
     * deep learning
     * face recognition
     * and much more!

   click the button below to learn more about the course, take a tour, and
   get 10 (free) sample lessons.

   take a tour & get 10 (free) lessons

hello! i   m adrian rosebrock.

   i'm an entrepreneur and ph.d who has launched two successful image
   search engines, [88]id my pill and [89]chic engine. i'm here to share
   my tips, tricks, and hacks i've learned along the way.

learn id161 in a single weekend.

   [90]become an opencv guru

   want to learn id161 & opencv? i can teach you in a single
   weekend. i know. it sounds crazy, but it   s no joke. my new book is your
   guaranteed, quick-start guide to becoming an opencv ninja. so why not
   give it a try? [91]click here to become a id161 ninja.

   click here to become an opencv ninja

subscribe via rss

   [92]pyimagesearch rss feed

   never miss a post! subscribe to the pyimagesearch rss feed and keep up
   to date with my image search engine tutorials, tips, and tricks
     * [93]popular

     * [94]raspbian stretch: install opencv 3 + python on your raspberry
       pi september 4, 2017
     * [95]install guide: raspberry pi 3 + raspbian jessie + opencv 3
       april 18, 2016
     * [96]home surveillance and motion detection with the raspberry pi,
       python, opencv, and dropbox june 1, 2015
     * [97]install opencv and python on your raspberry pi 2 and b+
       february 23, 2015
     * [98]ubuntu 16.04: how to install opencv october 24, 2016
     * [99]real-time id164 with deep learning and opencv
       september 18, 2017
     * [100]basic motion detection and tracking with python and opencv may
       25, 2015

   find me on [101]twitter, [102]facebook, and [103]linkedin.

      2019 pyimagesearch. all rights reserved.

   [tr?id=1465896023527386&ev=pageview&noscript=1]

   [email]
   [email]

references

   1. http://feeds.feedburner.com/pyimagesearch
   2. https://www.pyimagesearch.com/comments/feed/
   3. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/feed/
   4. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/
   5. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/&format=xml
   6. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#navigation
   7. https://www.pyimagesearch.com/
   8. https://www.pyimagesearch.com/
   9. https://www.pyimagesearch.com/
  10. https://www.pyimagesearch.com/start-here-learn-computer-vision-opencv/
  11. https://www.pyimagesearch.com/practical-python-opencv/
  12. https://www.pyimagesearch.com/pyimagesearch-gurus/
  13. https://www.pyimagesearch.com/opencv-tutorials-resources-guides/
  14. https://www.pyimagesearch.com/free-opencv-computer-vision-deep-learning-crash-course/
  15. https://www.pyimagesearch.com/about/
  16. https://www.pyimagesearch.com/contact/
  17. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#top
  18. https://www.pyimagesearch.com/author/adrian/
  19. https://www.pyimagesearch.com/category/deep-learning-2/
  20. https://www.pyimagesearch.com/category/machine-learning-2/
  21. https://www.pyimagesearch.com/category/tutorials/
  22. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  23. https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/
  24. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/
  25. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  26. http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf
  27. http://ufldl.stanford.edu/tutorial/supervised/optimizationstochasticgradientdescent/
  28. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  29. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  30. http://www.anc.ed.ac.uk/rbf/intro/node9.html
  31. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  32. https://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/
  33. https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/
  34. https://www.pyimagesearch.com/tag/classification/
  35. https://www.pyimagesearch.com/tag/deep-learning/
  36. https://www.pyimagesearch.com/tag/gradient-descent/
  37. https://www.pyimagesearch.com/tag/machine-learning/
  38. https://www.pyimagesearch.com/tag/neural-nets/
  39. https://www.pyimagesearch.com/tag/sgd/
  40. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  41. https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/
  42. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408367
  43. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408367
  44. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408371
  45. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408371
  46. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408423
  47. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408423
  48. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408579
  49. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408579
  50. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408494
  51. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-408494
  52. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-429625
  53. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-429625
  54. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-422719
  55. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-422719
  56. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-422941
  57. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-422941
  58. http://www.devtechnologies.net/
  59. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-445560
  60. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-445560
  61. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-445732
  62. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  63. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-445732
  64. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-456483
  65. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-456483
  66. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-472900
  67. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-472900
  68. http://www.georgemihaila.com/
  69. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486310
  70. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486310
  71. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486437
  72. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486437
  73. http://georgemihaila.com/
  74. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486926
  75. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486926
  76. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486945
  77. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  78. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-486945
  79. http://www.georgemihaila.com/
  80. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#comment-487264
  81. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#respond
  82. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
  83. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
  84. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  85. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
  86. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
  87. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
  88. http://www.idmypill.com/
  89. http://www.chicengine.com/
  90. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
  91. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
  92. http://feeds.feedburner.com/pyimagesearch
  93. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/#tab-pop
  94. https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/
  95. https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/
  96. https://www.pyimagesearch.com/2015/06/01/home-surveillance-and-motion-detection-with-the-raspberry-pi-python-and-opencv/
  97. https://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/
  98. https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/
  99. https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/
 100. https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/
 101. https://twitter.com/pyimagesearch
 102. https://www.facebook.com/pyimagesearch
 103. http://www.linkedin.com/pub/adrian-rosebrock/2a/873/59b
