   #[1]pyimagesearch    feed [2]pyimagesearch    comments feed
   [3]pyimagesearch    id119 with python comments feed
   [4]alternate [5]alternate

[6]navigation

   [7]pyimagesearch [8]pyimagesearch be awesome at opencv, python, deep
   learning, and id161

   [9]home

main menu

     * [10]start here
     * [11]practical python and opencv
     * [12]pyimagesearch gurus
     * [13]opencv 3 & 4 tutorials
     * [14]free crash course
     * [15]about
     * [16]contact

   [17]return to content

id119 with python

   by [18]adrian rosebrock on october 10, 2016 in [19]deep learning,
   [20]machine learning, [21]tutorials

   gradient_descent_header

   every relationship has its building blocks. love. trust. mutual
   respect.

   yesterday, i asked my girlfriend of 7.5 years to marry me. she said
   yes.

   it was quite literally the happiest day of my life. i feel like the
   luckiest guy in the world, not only because i have her, but also
   because this incredible pyimagesearch community has been so supportive
   over the past 3 years. thank you for being on this journey with me.

   and just like love and marriage have a set of building blocks, so do
   machine learning and neural network classifiers.

   over the past few weeks we opened our discussion of machine learning
   and neural networks with an [22]introduction to linear classification
   that discussed the concept of parameterized learning, and how this type
   of learning enables us to define a scoring function that maps our input
   data to output class labels.

   this scoring function is defined in terms of parameters; specifically,
   our weight matrix w and our bias vector b. our scoring function accepts
   these parameters as inputs and returns a predicted class label for each
   input data point x_{i} .

   from there, we discussed two common id168s: [23]multi-class id166
   loss and [24]cross-id178 loss (commonly referred to in the same
   breath as    softmax classifiers   ). id168s, at the most basic
   level, are used to quantify how    good    or    bad    a given predictor
   (i.e., a set of parameters) are at classifying the input data points in
   our dataset.

   given these building blocks, we can now move on to arguably the most
   important aspect of machine learning, neural networks, and deep
   learning     optimization.

   throughout this discussion we   ve learned that high classification
   accuracy is dependent on finding a set of weights w such that our data
   points are correctly classified. given w, can compute our output class
   labels via our scoring function. and finally, we can determine how
   good/poor our classifications are given some w via our id168.

   but how do we go about finding and obtaining a weight matrix w that
   obtains high classification accuracy?

   do we randomly initialize w, evaluate, and repeat over and over
   again, hoping that at some point we land on a w that obtains reasonable
   classification accuracy?

   well we could     and it some cases that might work just fine.

   but in most situations, we instead need to define an optimization
   algorithm that allows us to iteratively improve our weight matrix w.

   in today   s blog post, we   ll be looking at arguably the most common
   algorithm used to find optimal values of w     id119.

   looking for the source code to this post?
   [25]jump right to the downloads section.

id119 with python

   the id119 algorithm comes in two flavors:
    1. the standard    vanilla    implementation.
    2. the optimized    stochastic    version that is more commonly used.

   today well be reviewing the basic vanilla implementation to form a
   baseline for our understanding. then next week i   ll be discussing the
   stochastic version of id119.

id119 is an optimization algorithm

   the id119 method is an iterative optimization algorithm that
   operates over a loss landscape.

   we can visualize our loss landscape as a bowl, similar to the one you
   may eat cereal or soup out of:
   figure 1: a plot of our loss landscape. we typically see this landscape
   depicted as a "bowl". our goal is to move towards the basin of this
   bowl where this is minimal loss.

   figure 1: a plot of our loss landscape. we typically see this landscape
   depicted as a    bowl   . our goal is to move towards the basin of this
   bowl where this is minimal loss.

   the surface of our bowl is called our loss landscape, which is
   essentially a plot of our id168.

   the difference between our loss landscape and your cereal bowl is that
   your cereal bowl only exists in three dimensions, while your loss
   landscape exists in many dimensions, perhaps tens, hundreds, or even
   thousands of dimensions.

   each position along the surface of the bowl corresponds to a
   particular loss value given our set of parameters, w (weight matrix)
   and b (bias vector).

   our goal is to try different values of w and b, evaluate their loss,
   and then take a step towards more optimal values that will (ideally)
   have lower loss.

   iteratively repeating this process will allow us to navigate our loss
   landscape, following the gradient of the id168 (the bowl), and
   find a set of parameters that have minimum loss and high classification
   accuracy.

the    gradient    in id119

   to make our explanation of id119 a little more intuitive,
   let   s pretend that we have a robot     let   s name him chad:
   figure 2: introducing our robot, chad, who will help us understand the
   concept of id119.

   figure 2: introducing our robot, chad, who will help us understand the
   concept of id119.

   we place chad on a random position in our bowl (i.e., the loss
   landscape):
   figure 3: chad is placed on a random position on the loss landscape.
   however, chad has only one sensor -- the loss value at the exact
   position he is standing at. how is he going to get to the bottom of the
   basin?

   figure 3: chad is placed on a random position on the loss landscape.
   however, chad has only one sensor     the loss value at the exact
   position he is standing at. using this sensor (and this sensor alone),
   how is he going to get to the bottom of the basin?

   it   s now chad   s job to navigate to the bottom of the basin (where
   there is minimum loss).

   seems easy enough, right? all chad has to do is orient himself such
   that he   s facing    downhill    and then ride the slope until he reaches
   the bottom of the basin.

   but we have a problem: chad isn   t a very smart robot.

   chad only has one sensor     this sensor allows him to take his weight
   matrix w and compute a id168 l.

   therefore, chad is able to compute his (relative) position on the loss
   landscape, but he has absolutely no idea in which direction he should
   take a step to move himself closer to the bottom of the basin.

   what is chad to do?

   the answer is to apply id119.

   all we need to do is follow the slope of the gradient w. we can compute
   the gradient of w across all dimensions using the following equation:
   \frac{df(x)}{dx} = \lim_{h\to 0} \frac{f(x + h) - f(x)}{h}

   in > 1 dimensions, our gradient becomes a vector of partial
   derivatives.

   the problem with this equation is that:
    1. it   s an approximation to the gradient.
    2. it   s very slow.

   in practice, we use the analytic gradient instead. this method is
   exact, fast, but extremely challenging to implement due to partial
   derivatives and multivariable calculus. you can read more about the
   numeric and analytic gradients [26]here.

   for the sake of this discussion, simply try to internalize what
   id119 is doing: attempting to optimize our parameters for
   low loss and high classification accuracy.

pseudocode for id119

   below i have included some python-like pseudocode of the standard,
   vanilla id119 algorithm, inspired by the [27]cs231n slides:
   id119 with python
   python

   while true:_________________________________________________
   	wgradient = evaluate_gradient(loss, data, w)_______________
   	w += -alpha * wgradient____________________________________
   ____________________________________________________________
   1
   2
   3
   while true:
   wgradient = evaluate_gradient(loss, data, w)
   w += -alpha * wgradient

   this pseudocode is essentially what all variations of id119
   are built off of.

   we start off on line 1 by looping until some condition is met. normally
   this condition is either:
    1. a specified number of epochs has passed (meaning our learning
       algorithm has    seen    each of the training data points n times).
    2. our loss has become sufficiently low or training
       accuracy satisfactorily high.
    3. loss has not improved in m subsequent epochs.

   line 2 then calls a function named evaluate_gradient . this function
   requires three parameters:
     * loss : a function used to compute the loss over our current
       parameters w and input data .
     * data : our training data where each training sample is represented
       by a feature vector.
     * w : this is actually our weight matrix that we are optimizing over.
       our goal is to apply id119 to find a w that yields
       minimal loss.

   the evaluate_gradient  function returns a vector that is k-dimensional,
   where k is the number of dimensions in our feature vector. the
   wgradient  variable is actually our gradient, where we have a gradient
   entry for each dimension.

   we then apply the actual id119 on line 3.

   we multiply our wgradient  by alpha , which is our learning rate. the
   learning rate controls the size of our step.

   in practice, you   ll spend a lot of time finding an optimal learning
   rate alpha      it is by far the most important parameter in your model.

   if alpha  is too large, we   ll end up spending all our time bouncing
   around our loss landscape and never actually    descending    to the bottom
   of our basin (unless our random bouncing takes us there by pure luck).

   conversely, if alpha  is too small, then it will take many (perhaps
   prohibitively many) iterations to reach the bottom of the basin.

   because of this, alpha  will cause you many headaches     and you   ll
   spend a considerable amount of your time trying to find an optimal
   value for your classifier and dataset.

implementing id119 with python

   now that we know the basics of id119, let   s implement
   id119 in python and use it to classify some data.

   open up a new file, name it gradient_descent.py , and insert the
   following code:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   # import the necessary packages
   import matplotlib.pyplot as plt
   from sklearn.datasets.samples_generator import make_blobs
   import numpy as np
   import argparse

   def sigmoid_activation(x):
   # compute and return the sigmoid activation value for a
   # given input value
   return 1.0 / (1 + np.exp(-x))

   lines 2-5 import our required python packages.

   we then define the sigmoid_activation  function on line 7. when
   plotted, this function will resemble an    s   -shaped curve:
   figure 4: a plot of the sigmoid activation function.

   figure 4: a plot of the sigmoid activation function. notice how y=0.5
   when x=0.

   we call this an [28]activation function because the function will
      activate    and fire    on    (output value >= 0.5) or    off    (output vale <
   0.5) based on the inputs x .

   while there are other (better) alternatives to the sigmoid activation
   function, it makes for an excellent    starting point    in our discussion
   of machine learning, neural networks, and deep learning.

   i   ll also be discussing id180 in more detail in a future
   blog post, so for the time being, simply keep in mind that this is a
   non-linear activation function that we can use to    threshold    our
   predictions.

   next, let   s parse our command line arguments:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   12
   13
   14
   15
   16
   17
   18
   # construct the argument parse and parse the arguments
   ap = argparse.argumentparser()
   ap.add_argument("-e", "--epochs", type=float, default=100,
   help="# of epochs")
   ap.add_argument("-a", "--alpha", type=float, default=0.01,
   help="learning rate")
   args = vars(ap.parse_args())

   we can provide two (optional) command line arguments to our script:
     * --epochs : the number of epochs that we   ll use when training our
       classifier using id119.
     * --alpha : the learning rate for id119. we typically
       see 0.1, 0.01, and 0.001 as initial learning rate values, but
       again, you   ll want to [29]tune this hyperparameter for your own
       classification problems.

   now that our command line arguments are parsed, let   s generate some
   data to classify:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 250 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,_
   	cluster_std=1.05, random_state=20)_________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   20
   21
   22
   23
   24
   25
   26
   27
   28
   29
   30
   31
   32
   33
   34
   35
   36
   37
   # generate a 2-class classification problem with 250 data points,
   # where each data point is a 2d feature vector
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,
   cluster_std=1.05, random_state=20)

   # insert a column of 1's as the first entry in the feature
   # vector -- this is a little trick that allows us to treat
   # the bias as a trainable parameter *within* the weight matrix
   # rather than an entirely separate variable
   x = np.c_[np.ones((x.shape[0])), x]

   # initialize our weight matrix such it has the same number of
   # columns as our input features
   print("[info] starting training...")
   w = np.random.uniform(size=(x.shape[1],))

   # initialize a list to store the loss value for each epoch
   losshistory = []

   on line 22 we make a call to make_blobs  which generates 250 data
   points. these data points are 2d, implying that the    feature vectors   
   are of length 2.

   furthermore, 125 of these data points belong to class 0 and the other
   125 to class 1. our goal is to train a classifier that correctly
   predicts each data point as being class 0 or class 1.

   line 29 applies a neat little trick that allows us to skip explicitly
   keeping track of our bias vector b. to accomplish this, we insert a
   brand new column of 1   s as the first entry in our feature vector. this
   addition of a column containing a constant value across all feature
   vectors allows us to treat our bias as a trainable parameter that
   is within the weight matrix w rather than an entirely separate
   variable. you can learn more about this trick [30]here and [31]here.

   line 34 (randomly) initializes our weight matrix such that it has the
   same number of dimensions as our input features.

   it   s also common to see both zero and one weight initialization, but i
   tend to prefer random initialization better. weight initialization
   methods will be discussed in further detail inside future neural
   network and deep learning blog posts.

   finally, line 37 initializes a list to keep track of our loss after
   each epoch. at the end of our python script, we   ll plot the loss which
   should ideally decrease over time.

   all of our variables are now initialized, so we can move on to the
   actual training and id119 procedure:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 250 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,_
   	cluster_std=1.05, random_state=20)_________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# take the dot product between our features `x` and the____
   	# weight matrix `w`, then pass this value through the______
   	# sigmoid activation function, thereby giving us our_______
   	# predictions on the dataset_______________________________
   	preds = sigmoid_activation(x.dot(w))_______________________
   ____________________________________________________________
   	# now that we have our predictions, we need to determine___
   	# our `error`, which is the difference between our predicti
   	# and the true values______________________________________
   	error = preds - y__________________________________________
   ____________________________________________________________
   	# given our `error`, we can compute the total loss value as
   	# the sum of squared loss -- ideally, our loss should______
   	# decrease as we continue training_________________________
   	loss = np.sum(error ** 2)__________________________________
   	losshistory.append(loss)___________________________________
   	print("[info] epoch #{}, loss={:.7f}".format(epoch + 1, los
   39
   40
   41
   42
   43
   44
   45
   46
   47
   48
   49
   50
   51
   52
   53
   54
   55
   56
   57
   # loop over the desired number of epochs
   for epoch in np.arange(0, args["epochs"]):
   # take the dot product between our features `x` and the
   # weight matrix `w`, then pass this value through the
   # sigmoid activation function, thereby giving us our
   # predictions on the dataset
   preds = sigmoid_activation(x.dot(w))

   # now that we have our predictions, we need to determine
   # our `error`, which is the difference between our predictions
   # and the true values
   error = preds - y

   # given our `error`, we can compute the total loss value as
   # the sum of squared loss -- ideally, our loss should
   # decrease as we continue training
   loss = np.sum(error ** 2)
   losshistory.append(loss)
   print("[info] epoch #{}, loss={:.7f}".format(epoch + 1, loss))

   on line 40 we start looping over the supplied number of --epochs . by
   default, we   ll allow our training procedure to    see    each of the
   training points a total of 100 times (thus, 100 epochs).

   line 45 takes the dot product between our entire training data x  and
   our weight matrix w . we take the output of this dot product and feed
   the values through the sigmoid activation function, giving us our
   predictions.

   given our predictions, the next step is to determine the    error    of the
   predictions, or more simply, the difference between our predictions and
   the true values (line 50).

   line 55 computes the [32]least squares error over our predictions (our
   loss value). the goal of this training procedure is thus to minimize
   the least squares error.

   now that we have our error , we can compute the gradient  and then use
   it to update our weight matrix w :
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 250 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,_
   	cluster_std=1.05, random_state=20)_________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# take the dot product between our features `x` and the____
   	# weight matrix `w`, then pass this value through the______
   	# sigmoid activation function, thereby giving us our_______
   	# predictions on the dataset_______________________________
   	preds = sigmoid_activation(x.dot(w))_______________________
   ____________________________________________________________
   	# now that we have our predictions, we need to determine___
   	# our `error`, which is the difference between our predicti
   	# and the true values______________________________________
   	error = preds - y__________________________________________
   ____________________________________________________________
   	# given our `error`, we can compute the total loss value as
   	# the sum of squared loss -- ideally, our loss should______
   	# decrease as we continue training_________________________
   	loss = np.sum(error ** 2)__________________________________
   	losshistory.append(loss)___________________________________
   	print("[info] epoch #{}, loss={:.7f}".format(epoch + 1, los
   ____________________________________________________________
   	# the gradient update is therefore the dot product between_
   	# the transpose of `x` and our error, scaled by the total__
   	# number of data points in `x`_____________________________
   	gradient = x.t.dot(error) / x.shape[0]_____________________
   ____________________________________________________________
   	# in the update stage, all we need to do is nudge our weigh
   	# matrix in the negative direction of the gradient (hence t
   	# term "id119" by taking a small step towards a_
   	# set of "more optimal" parameters_________________________
   	w += -args["alpha"] * gradient_____________________________
   59
   60
   61
   62
   63
   64
   65
   66
   67
   68
   # the gradient update is therefore the dot product between
   # the transpose of `x` and our error, scaled by the total
   # number of data points in `x`
   gradient = x.t.dot(error) / x.shape[0]

   # in the update stage, all we need to do is nudge our weight
   # matrix in the negative direction of the gradient (hence the
   # term "id119" by taking a small step towards a
   # set of "more optimal" parameters
   w += -args["alpha"] * gradient

   line 62 handles computing the actual gradient, which is the dot product
   between our data points x  and the error .

   line 68 is the most critical step in our algorithm and where the actual
   id119 takes place. here we update our weight matrix w  by
   taking a --step  in the negative direction of the gradient, thereby
   allowing us to move towards the bottom of the basin of the loss
   landscape (hence the term, id119).

   after updating our weight matrix, we keep looping until the desired
   number of epochs has been met     id119 is thus an iterative
   algorithm.

   to actually demonstrate how we can use our weight matrix w as a
   classifier, take a look at the following code block:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 250 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,_
   	cluster_std=1.05, random_state=20)_________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# take the dot product between our features `x` and the____
   	# weight matrix `w`, then pass this value through the______
   	# sigmoid activation function, thereby giving us our_______
   	# predictions on the dataset_______________________________
   	preds = sigmoid_activation(x.dot(w))_______________________
   ____________________________________________________________
   	# now that we have our predictions, we need to determine___
   	# our `error`, which is the difference between our predicti
   	# and the true values______________________________________
   	error = preds - y__________________________________________
   ____________________________________________________________
   	# given our `error`, we can compute the total loss value as
   	# the sum of squared loss -- ideally, our loss should______
   	# decrease as we continue training_________________________
   	loss = np.sum(error ** 2)__________________________________
   	losshistory.append(loss)___________________________________
   	print("[info] epoch #{}, loss={:.7f}".format(epoch + 1, los
   ____________________________________________________________
   	# the gradient update is therefore the dot product between_
   	# the transpose of `x` and our error, scaled by the total__
   	# number of data points in `x`_____________________________
   	gradient = x.t.dot(error) / x.shape[0]_____________________
   ____________________________________________________________
   	# in the update stage, all we need to do is nudge our weigh
   	# matrix in the opposite direction of the gradient (hence t
   	# term "id119" by taking a small step towards a_
   	# set of "more optimal" parameters_________________________
   	w += -args["alpha"] * gradient_____________________________
   ____________________________________________________________
   # to demonstrate how to use our weight matrix as a classifie
   # let's look over our a sample of training examples_________
   for i in np.random.choice(250, 10):_________________________
   	# compute the prediction by taking the dot product of the__
   	# current feature vector with the weight matrix w, then____
   	# passing it through the sigmoid activation function_______
   	activation = sigmoid_activation(x[i].dot(w))_______________
   ____________________________________________________________
   	# the sigmoid function is defined over the range y=[0, 1],_
   	# so we can use 0.5 as our threshold -- if `activation` is_
   	# below 0.5, it's class `0`; otherwise it's class `1`______
   	label = 0 if activation < 0.5 else 1_______________________
   ____________________________________________________________
   	# show our output classification___________________________
   	print("activation={:.4f}; predicted_label={}, true_label={}
   		activation, label, y[i]))_________________________________
   70
   71
   72
   73
   74
   75
   76
   77
   78
   79
   80
   81
   82
   83
   84
   85
   # to demonstrate how to use our weight matrix as a classifier,
   # let's look over our a sample of training examples
   for i in np.random.choice(250, 10):
   # compute the prediction by taking the dot product of the
   # current feature vector with the weight matrix w, then
   # passing it through the sigmoid activation function
   activation = sigmoid_activation(x[i].dot(w))

   # the sigmoid function is defined over the range y=[0, 1],
   # so we can use 0.5 as our threshold -- if `activation` is
   # below 0.5, it's class `0`; otherwise it's class `1`
   label = 0 if activation < 0.5 else 1

   # show our output classification
   print("activation={:.4f}; predicted_label={}, true_label={}".format(
   activation, label, y[i]))

   we start on on line 72 by looping over a sample of our training data.

   for each training point x[i]  we compute the dot product between x[i]
   and the weight matrix w , then feed the value through our activation
   function.

   on line 81, we compute the actual output class label. if the
   activation  is < 0.5, then the output is class 0; otherwise, the output
   is class 1.

   our last code block is used to plot our training data along with
   the decision boundary that is used to determine if a given data point
   is class 0 or class 1:
   id119 with python
   python

   # import the necessary packages_____________________________
   import matplotlib.pyplot as plt_____________________________
   from sklearn.datasets.samples_generator import make_blobs___
   import numpy as np__________________________________________
   import argparse_____________________________________________
   ____________________________________________________________
   def sigmoid_activation(x):__________________________________
   	# compute and return the sigmoid activation value for a____
   	# given input value________________________________________
   	return 1.0 / (1 + np.exp(-x))______________________________
   ____________________________________________________________
   # construct the argument parse and parse the arguments______
   ap = argparse.argumentparser()______________________________
   ap.add_argument("-e", "--epochs", type=float, default=100,__
   	help="# of epochs")________________________________________
   ap.add_argument("-a", "--alpha", type=float, default=0.01,__
   	help="learning rate")______________________________________
   args = vars(ap.parse_args())________________________________
   ____________________________________________________________
   # generate a 2-class classification problem with 250 data po
   # where each data point is a 2d feature vector______________
   (x, y) = make_blobs(n_samples=250, n_features=2, centers=2,_
   	cluster_std=1.05, random_state=20)_________________________
   ____________________________________________________________
   # insert a column of 1's as the first entry in the feature__
   # vector -- this is a little trick that allows us to treat__
   # the bias as a trainable parameter *within* the weight matr
   # rather than an entirely separate variable_________________
   x = np.c_[np.ones((x.shape[0])), x]_________________________
   ____________________________________________________________
   # initialize our weight matrix such it has the same number o
   # columns as our input features_____________________________
   print("[info] starting training...")________________________
   w = np.random.uniform(size=(x.shape[1],))___________________
   ____________________________________________________________
   # initialize a list to store the loss value for each epoch__
   losshistory = []____________________________________________
   ____________________________________________________________
   # loop over the desired number of epochs____________________
   for epoch in np.arange(0, args["epochs"]):__________________
   	# take the dot product between our features `x` and the____
   	# weight matrix `w`, then pass this value through the______
   	# sigmoid activation function, thereby giving us our_______
   	# predictions on the dataset_______________________________
   	preds = sigmoid_activation(x.dot(w))_______________________
   ____________________________________________________________
   	# now that we have our predictions, we need to determine___
   	# our `error`, which is the difference between our predicti
   	# and the true values______________________________________
   	error = preds - y__________________________________________
   ____________________________________________________________
   	# given our `error`, we can compute the total loss value as
   	# the sum of squared loss -- ideally, our loss should______
   	# decrease as we continue training_________________________
   	loss = np.sum(error ** 2)__________________________________
   	losshistory.append(loss)___________________________________
   	print("[info] epoch #{}, loss={:.7f}".format(epoch + 1, los
   ____________________________________________________________
   	# the gradient update is therefore the dot product between_
   	# the transpose of `x` and our error, scaled by the total__
   	# number of data points in `x`_____________________________
   	gradient = x.t.dot(error) / x.shape[0]_____________________
   ____________________________________________________________
   	# in the update stage, all we need to do is nudge our weigh
   	# matrix in the opposite direction of the gradient (hence t
   	# term "id119" by taking a small step towards a_
   	# set of "more optimal" parameters_________________________
   	w += -args["alpha"] * gradient_____________________________
   ____________________________________________________________
   # to demonstrate how to use our weight matrix as a classifie
   # let's look over our a sample of training examples_________
   for i in np.random.choice(250, 10):_________________________
   	# compute the prediction by taking the dot product of the__
   	# current feature vector with the weight matrix w, then____
   	# passing it through the sigmoid activation function_______
   	activation = sigmoid_activation(x[i].dot(w))_______________
   ____________________________________________________________
   	# the sigmoid function is defined over the range y=[0, 1],_
   	# so we can use 0.5 as our threshold -- if `activation` is_
   	# below 0.5, it's class `0`; otherwise it's class `1`______
   	label = 0 if activation < 0.5 else 1_______________________
   ____________________________________________________________
   	# show our output classification___________________________
   	print("activation={:.4f}; predicted_label={}, true_label={}
   		activation, label, y[i]))_________________________________
   ____________________________________________________________
   # compute the line of best fit by setting the sigmoid functi
   # to 0 and solving for x2 in terms of x1____________________
   y = (-w[0] - (w[1] * x)) / w[2]_____________________________
   ____________________________________________________________
   # plot the original data along with our line of best fit____
   plt.figure()________________________________________________
   plt.scatter(x[:, 1], x[:, 2], marker="o", c=y)______________
   plt.plot(x, y, "r-")________________________________________
   ____________________________________________________________
   # construct a figure that plots the loss over time__________
   fig = plt.figure()__________________________________________
   plt.plot(np.arange(0, args["epochs"]), losshistory)_________
   fig.suptitle("training loss")_______________________________
   plt.xlabel("epoch #")_______________________________________
   plt.ylabel("loss")__________________________________________
   plt.show()__________________________________________________
   87
   88
   89
   90
   91
   92
   93
   94
   95
   96
   97
   98
   99
   100
   101
   102
   # compute the line of best fit by setting the sigmoid function
   # to 0 and solving for x2 in terms of x1
   y = (-w[0] - (w[1] * x)) / w[2]

   # plot the original data along with our line of best fit
   plt.figure()
   plt.scatter(x[:, 1], x[:, 2], marker="o", c=y)
   plt.plot(x, y, "r-")

   # construct a figure that plots the loss over time
   fig = plt.figure()
   plt.plot(np.arange(0, args["epochs"]), losshistory)
   fig.suptitle("training loss")
   plt.xlabel("epoch #")
   plt.ylabel("loss")
   plt.show()

visualizing id119

   to test our id119 classifier, be sure to download the source
   code using the    downloads    section at the bottom of this tutorial.

   from there, execute the following command:
   id119 with python
   shell

   $ python gradient_descent.py________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   $ python gradient_descent.py

   examining the output, you   ll notice that our classifier runs for a
   total of 100 epochs with the loss decreasing and classification
   accuracy increasing after each epoch:
   figure 5: when applying id119, our loss decreases and
   classification accuracy increases after each epoch.

   figure 5: when applying id119, our loss decreases and
   classification accuracy increases after each epoch.

   to visualize this better, take a look at the plot below which
   demonstrates how our loss over time has decreased dramatically:
   figure 6: plotting loss over time using id119. notice how
   loss sharply drops.

   figure 6: plotting loss over time using id119. notice how
   loss sharply drops and then levels out towards later epochs.

   we can then see a plot of our training data points along with the
   decision boundary learned by our id119 classifier:
   figure 7: plotting the decision boundary learned by our gradient
   descent classifier.

   figure 7: plotting the decision boundary learned by our gradient
   descent classifier.

   notice how the decision boundary learned by our id119
   classifier neatly divides data points of the two classes.

   we then then manually investigate the classifications made by our
   id119 model. in each case, we are able to correctly predict
   the class:
   figure 8: making predictions using our id119 classifier.

   figure 8: making predictions using our id119 classifier.

   to visualize and demonstrate id119 in action, i have created
   the following animation which shows the decision boundary being
      learned    after each epoch:
   figure 8: an animation depicting how the decision boundary is learned
   via id119.

   figure 8: an animation depicting how the decision boundary is learned
   via id119.

   as you can see, our decision boundary starts off widely inaccurate due
   to the random initialization. but as time passes, we are able to apply
   id119, update our weight matrix w, and eventually learn an
   accurate model.

want to learn more about id119?

   in next week   s blog post, i   ll be discussing a slight modification to
   id119 called stochastic id119 (sgd).

   in the meantime, if you want to learn more about id119, you
   should absolutely refer to andrew ng   s id119 lesson in the
   [33]coursera machine learning course.

   i would also recommend andrej karpathy   s [34]excellent slides from the
   cs231n course.

summary

   in this blog post we learned about id119, a first-order
   optimization algorithm that can be used to learn a set of parameters
   that will (ideally) obtain low loss and high classification accuracy on
   a given problem.

   i then demonstrated how to implement a basic id119 algorithm
   using python. using this implementation, we were able to
   actually visualize how id119 can be used to learn and
   optimize our weight matrix w.

   in next week   s blog post, i   ll be discussing a modification to the
   vanilla id119 implementation called stochastic gradient
   descent (sgd). the sgd flavor of id119 is more commonly used
   than the one we introduced today, but i   ll save a more thorough
   discussion for next week.

   see you then!

   before you go, be sure to use the form below to sign up for the
   pyimagesearch newsletter     you   ll then be notified when future blog
   posts are published.

downloads:

   if you would like to download the code and images used in this post,
   please enter your email address in the form below. not only will you
   get a .zip of the code, i   ll also send you a free 17-page resource
   guide on id161, opencv, and deep learning. inside you'll find
   my hand-picked tutorials, books, courses, and libraries to help you
   master cv and dl! sound good? if so, enter your email address and i   ll
   send you the code immediately!

   email address: ____________________

   download the code!

resource guide (it   s totally free).

   get your free 17-page id161 and deep learning resource guide
   pdf
   enter your email address below to get my free 17-page id161,
   opencv, and deep learning resource guide pdf. inside you'll find my
   hand-picked tutorials, books, courses, and python libraries to help you
   master id161 and deep learning!
   ____________________
   download the guide!

   [35]classification, [36]deep learning, [37]id119,
   [38]machine learning, [39]neural nets
   [40]bubble sheet multiple choice scanner and test grader using omr,
   python and opencv
   [41]stochastic id119 (sgd) with python

38 responses to id119 with python

    1. joe minichino october 10, 2016 at 11:26 am [42]#
       congratulations adrian. and yes, the article is also very
       interesting.
       [43]reply
          + adrian rosebrock october 11, 2016 at 12:59 pm [44]#
            thank you very much joe!
            [45]reply
    2. [46]jason october 10, 2016 at 11:31 am [47]#
       didn   t have time to read this but congratulations! i hope your
       experience of marriage is as fulfilling and wonderful as mine has
       been.
       [48]reply
          + adrian rosebrock october 11, 2016 at 12:59 pm [49]#
            thank you jason!     
            [50]reply
    3. [51]sujit pal october 10, 2016 at 11:40 am [52]#
       nice article. just wanted to point out a typo. in your formula for
       d(f(x))/dx it should be limit of h tends to 0 not infinity.
       [53]reply
          + adrian rosebrock october 11, 2016 at 12:59 pm [54]#
            thank you for pointing this out sujit. i have updated the
            post.
            [55]reply
    4. max kostka october 10, 2016 at 12:17 pm [56]#
       thanks for the informative post and the link to the slides, and i
       can totally recommend the machine learning course by andrew ng at
       coursera, too. it   s a real good introduction to different machine
       learning techniques. the hands on approach with the homework is
       worth every minute spent on it.
       [57]reply
    5. johnny october 10, 2016 at 12:44 pm [58]#
       fascinating! i love id119 studies like this one. i love
       adrian   s teaching style. and mostly, love that you shared the news
       with us! congratulations you two, many blessings and prayers coming
       at you!
       [59]reply
          + adrian rosebrock october 11, 2016 at 12:56 pm [60]#
            thank you johnny!     
            [61]reply
    6. ranjeet singh october 11, 2016 at 2:19 am [62]#
       great tutorial
       kidnly put some id26 tutorials too.
       [63]reply
          + adrian rosebrock october 11, 2016 at 12:52 pm [64]#
            i will certainly be doing id26 tutorials, likely
            2-3 of them. right now it looks like i should be able to
            publish them in november/december following my current
            schedule.
            [65]reply
    7. [66]wajih october 11, 2016 at 4:10 am [67]#
       this explanation is so beautiful, simple and elegant   
       [68]reply
          + adrian rosebrock october 11, 2016 at 12:51 pm [69]#
            thank you wajih!
            [70]reply
    8. abkul october 12, 2016 at 2:43 am [71]#
       thanks for the crystal clear explanation of the topic.
       congratulations for quiting bachelors club.
       [72]reply
          + adrian rosebrock october 13, 2016 at 9:19 am [73]#
            thank you abkul!
            [74]reply
    9. moeen october 13, 2016 at 5:13 pm [75]#
       hey adrian- big congrats. i wish you the best!
       [76]reply
          + adrian rosebrock october 15, 2016 at 9:57 am [77]#
            thanks moeen!
            [78]reply
   10. arasch u lagies october 14, 2016 at 1:20 am [79]#
       congratulations adrian.
       [80]reply
          + adrian rosebrock october 15, 2016 at 9:56 am [81]#
            thank you arasch!
            [82]reply
   11. vantruong57 october 16, 2016 at 10:51 am [83]#
       congratulations adrian.
       [84]reply
          + adrian rosebrock october 17, 2016 at 4:07 pm [85]#
            thank you!
            [86]reply
   12. srinivasan babu october 21, 2016 at 9:46 am [87]#
       nice article.
       thanks lot
       [88]reply
          + adrian rosebrock october 23, 2016 at 10:19 am [89]#
            thank you srinivasan!
            [90]reply
   13. abkul october 31, 2016 at 2:18 am [91]#
       kindly do tutorials on    feature selection    in relation to deep
       learning
       [92]reply
          + adrian rosebrock november 1, 2016 at 9:04 am [93]#
            i   ll certainly be doing more deep learning tutorials in the
            future. be sure to keep an eye on the pyimagesearch blog.
            [94]reply
   14. zauron november 2, 2016 at 6:49 pm [95]#
       nice article! it   s my fault but i don   t understand how you
       calculate the decision boundary:
       y = (-w[0]     (w[1] * x)) / w[2]
       could you elaborate a little more or give me some reference?
       thanks in advance
       [96]reply
          + [97]dpereira december 1, 2016 at 8:38 am [98]#
            hi zauron,
            you can think of it like this:
            in order to draw the decision boundary, you need to draw only
            the points (x,y) which lie right over the boundary.
            according to the sigmoid function, the boundary is the value
            0.5. so, in order to obtain a 0.5, you need to provide a zero
            value as input to the sigmoid (that is, a zero value as output
            from the scoring function).
            thus, if the scoring function equals zero:
            0 = w0 + w1*x + w2*y ==> y = (-w0     w1*x)/w2
            you can use any x   s coordinates you want, and you   ll get the
            proper y   s coordinates to draw the boundary
            [99]reply
   15. chahrazad january 23, 2017 at 2:45 am [100]#
       hello, it is a bit confusiong to me how the gradient was computed :
       gradient = x.t.dot(error) / x.shape[0] ,
       shouldn   t this computation be true if the id168 was derived
       using id113 not the squared error ?
       [101]reply
   16. foobar april 12, 2017 at 5:42 am [102]#
       great stuff, thanks!
          in the meantime, if you want to learn more about id119,
       you should absolutely refer to andrew ng   s id119 lesson
       in the coursera machine learning course.
       i would also recommend andrej karpathy   s excellent slides from the
       cs231n course.   
       both links are dead.
       [103]reply
          + adrian rosebrock april 12, 2017 at 12:58 pm [104]#
            thank you for letting me know! i have updated both links so
            they are now working.
            [105]reply
   17. tri dao june 14, 2017 at 1:42 am [106]#
       i think the gradient is for logistic loss, not the squared loss
       you   re using.
       [107]reply
   18. niki july 13, 2017 at 5:06 pm [108]#
       thank you, an interesting tutorial! i   m a little bit confused
       though. when calculating the gradient, we try to minimize the loss
       function, which means we need to take the derivative of the loss
       function. the id168 is the sum of the square of the errors,
       with error being defined as the actual label minus the predicted
       label. here the predicted labels are calculated using sigmoid
       function. this means the gradient will include the gradient of the
       sigmoid function, but here i see the gradient of a linear predictor
       function.
       could you elaborate more on what has been done. thank you!
       [109]reply
   19. niki july 13, 2017 at 7:21 pm [110]#
       my bad! i got error for w in the gradient formula.
       [111]reply
          + adrian rosebrock july 14, 2017 at 7:24 am [112]#
            congrats on figuring it out niki, nice job     
            [113]reply
          + kiro september 20, 2017 at 4:33 am [114]#
            i think you are right. there is a mistake. shouldn   t it be
            like that:
            gradient = x.t.dot(error) * preds*(1-preds)
            where preds*(1-preds) is the derivative of the sigmoid
            function?
            thanks!
            [115]reply
   20. ben august 17, 2017 at 10:27 am [116]#
       congratulations on your pending nuptial.
       [117]reply
          + adrian rosebrock august 17, 2017 at 10:31 am [118]#
            thanks ben!
            [119]reply

trackbacks/pingbacks

    1. [120]stochastic id119 (sgd) with python - pyimagesearch
       - october 17, 2016
       [   ] last week   s blog post, we discussed id119, a
       first-order optimization algorithm that can be used to learn a set
       of classifier coefficients [   ]

leave a reply [121]click here to cancel reply.

   comment
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________
   __________________________________________________

   ______________________________name (required)

   ______________________________email (will not be published) (required)

   ______________________________website

   submit comment

   search...___________ (search)

resource guide (it   s totally free).

   [122]get your free 17-page id161 and deep learning resource
   guide pdf

   get your free 17 page id161, opencv, and deep learning
   resource guide pdf. inside you'll find my hand-picked tutorials, books,
   courses, and libraries to help you master cv and dl.

                           [123]download for free!

deep learning for id161 with python book     out now!

   [124]deep learning with id161 and python kickstarter

   you're interested in deep learning and id161, but you don't
   know how to get started. let me help. [125]my new book will teach you
   all you need to know about deep learning.

   click here to master deep learning

you can detect faces in images & video.

   [126]learn how to detect faces in images and video

   are you interested in detecting faces in images & video? but tired of
   googling for tutorials that never work? then let me help! i guarantee
   that my new book will turn you into a face detection ninja by the end
   of this weekend. [127]click here to give it a shot yourself.

   click here to master face detection

pyimagesearch gurus: now enrolling!

   the pyimagesearch gurus course is now enrolling! inside the course
   you'll learn how to perform:
     * automatic license plate recognition (anpr)
     * deep learning
     * face recognition
     * and much more!

   click the button below to learn more about the course, take a tour, and
   get 10 (free) sample lessons.

   take a tour & get 10 (free) lessons

hello! i   m adrian rosebrock.

   i'm an entrepreneur and ph.d who has launched two successful image
   search engines, [128]id my pill and [129]chic engine. i'm here to share
   my tips, tricks, and hacks i've learned along the way.

learn id161 in a single weekend.

   [130]become an opencv guru

   want to learn id161 & opencv? i can teach you in a single
   weekend. i know. it sounds crazy, but it   s no joke. my new book is your
   guaranteed, quick-start guide to becoming an opencv ninja. so why not
   give it a try? [131]click here to become a id161 ninja.

   click here to become an opencv ninja

subscribe via rss

   [132]pyimagesearch rss feed

   never miss a post! subscribe to the pyimagesearch rss feed and keep up
   to date with my image search engine tutorials, tips, and tricks
     * [133]popular

     * [134]raspbian stretch: install opencv 3 + python on your raspberry
       pi september 4, 2017
     * [135]install guide: raspberry pi 3 + raspbian jessie + opencv 3
       april 18, 2016
     * [136]home surveillance and motion detection with the raspberry pi,
       python, opencv, and dropbox june 1, 2015
     * [137]install opencv and python on your raspberry pi 2 and b+
       february 23, 2015
     * [138]ubuntu 16.04: how to install opencv october 24, 2016
     * [139]real-time id164 with deep learning and opencv
       september 18, 2017
     * [140]basic motion detection and tracking with python and opencv may
       25, 2015

   find me on [141]twitter, [142]facebook, and [143]linkedin.

      2019 pyimagesearch. all rights reserved.

   [tr?id=1465896023527386&ev=pageview&noscript=1]

   [email]
   [email]

references

   1. http://feeds.feedburner.com/pyimagesearch
   2. https://www.pyimagesearch.com/comments/feed/
   3. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/feed/
   4. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
   5. https://www.pyimagesearch.com/wp-json/oembed/1.0/embed?url=https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/&format=xml
   6. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#navigation
   7. https://www.pyimagesearch.com/
   8. https://www.pyimagesearch.com/
   9. https://www.pyimagesearch.com/
  10. https://www.pyimagesearch.com/start-here-learn-computer-vision-opencv/
  11. https://www.pyimagesearch.com/practical-python-opencv/
  12. https://www.pyimagesearch.com/pyimagesearch-gurus/
  13. https://www.pyimagesearch.com/opencv-tutorials-resources-guides/
  14. https://www.pyimagesearch.com/free-opencv-computer-vision-deep-learning-crash-course/
  15. https://www.pyimagesearch.com/about/
  16. https://www.pyimagesearch.com/contact/
  17. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#top
  18. https://www.pyimagesearch.com/author/adrian/
  19. https://www.pyimagesearch.com/category/deep-learning-2/
  20. https://www.pyimagesearch.com/category/machine-learning-2/
  21. https://www.pyimagesearch.com/category/tutorials/
  22. https://www.pyimagesearch.com/2016/08/22/an-intro-to-linear-classification-with-python/
  23. https://www.pyimagesearch.com/2016/09/05/multi-class-id166-loss/
  24. https://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/
  25. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/
  26. https://www.youtube.com/watch?v=ruuw4-inuxm
  27. http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf
  28. https://en.wikipedia.org/wiki/activation_function
  29. https://www.pyimagesearch.com/2016/08/15/how-to-tune-hyperparameters-with-python-and-scikit-learn/
  30. http://book.caltech.edu/bookforum/showthread.php?t=845
  31. http://stackoverflow.com/questions/3775032/how-to-update-the-bias-in-neural-network-id26
  32. http://www.anc.ed.ac.uk/rbf/intro/node9.html
  33. https://www.youtube.com/watch?v=ufnu3vhv5ca
  34. http://cs231n.stanford.edu/slides/2016/winter1516_lecture3.pdf
  35. https://www.pyimagesearch.com/tag/classification/
  36. https://www.pyimagesearch.com/tag/deep-learning/
  37. https://www.pyimagesearch.com/tag/gradient-descent/
  38. https://www.pyimagesearch.com/tag/machine-learning/
  39. https://www.pyimagesearch.com/tag/neural-nets/
  40. https://www.pyimagesearch.com/2016/10/03/bubble-sheet-multiple-choice-scanner-and-test-grader-using-omr-python-and-opencv/
  41. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/
  42. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407880
  43. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407880
  44. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407989
  45. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407989
  46. http://jasonscotthoffman.com/
  47. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407881
  48. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407881
  49. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407988
  50. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407988
  51. http://sujitpal.blogspot.com/
  52. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407882
  53. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407882
  54. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407986
  55. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407986
  56. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407885
  57. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407885
  58. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407886
  59. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407886
  60. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407985
  61. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407985
  62. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407927
  63. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407927
  64. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407979
  65. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407979
  66. http://github/
  67. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407932
  68. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407932
  69. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407978
  70. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-407978
  71. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408035
  72. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408035
  73. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408101
  74. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408101
  75. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408132
  76. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408132
  77. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408231
  78. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408231
  79. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408158
  80. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408158
  81. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408229
  82. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408229
  83. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408305
  84. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408305
  85. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408387
  86. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408387
  87. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408653
  88. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408653
  89. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408818
  90. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-408818
  91. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409440
  92. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409440
  93. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409544
  94. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409544
  95. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409700
  96. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-409700
  97. https://www.gradiant.org/?lang=en
  98. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-412386
  99. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-412386
 100. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-416275
 101. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-416275
 102. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-422586
 103. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-422586
 104. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-422614
 105. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-422614
 106. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-427313
 107. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-427313
 108. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429790
 109. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429790
 110. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429804
 111. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429804
 112. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429841
 113. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-429841
 114. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-435223
 115. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-435223
 116. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-432600
 117. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-432600
 118. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-432603
 119. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#comment-432603
 120. https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/
 121. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#respond
 122. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
 123. https://app.monstercampaigns.com/c/mdoijtrmex7bpm0rp2hn/
 124. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 125. https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/
 126. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
 127. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-face-detection
 128. http://www.idmypill.com/
 129. http://www.chicengine.com/
 130. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
 131. https://www.pyimagesearch.com/practical-python-opencv/?src=sidebar-single-weekend
 132. http://feeds.feedburner.com/pyimagesearch
 133. https://www.pyimagesearch.com/2016/10/10/gradient-descent-with-python/#tab-pop
 134. https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/
 135. https://www.pyimagesearch.com/2016/04/18/install-guide-raspberry-pi-3-raspbian-jessie-opencv-3/
 136. https://www.pyimagesearch.com/2015/06/01/home-surveillance-and-motion-detection-with-the-raspberry-pi-python-and-opencv/
 137. https://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/
 138. https://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/
 139. https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/
 140. https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/
 141. https://twitter.com/pyimagesearch
 142. https://www.facebook.com/pyimagesearch
 143. http://www.linkedin.com/pub/adrian-rosebrock/2a/873/59b
