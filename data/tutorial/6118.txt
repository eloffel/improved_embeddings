   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

pytorch tutorial distilled

migrating from tensorflow to pytorch

   [16]go to the profile of illarion khlestov
   [17]illarion khlestov (button) blockedunblock (button) followfollowing
   sep 25, 2017
   [1*aqngmfybistlrf9k7d9cng.jpeg]

   when i first started study pytorch, i drop it after a few days. it was
   hard for me to get core concepts of this framework comparing with the
   tensorflow. that   s why i   ve put it on my    knowledge bookshelf    and
   forgot about it. but not so far ago a new version of pytorch was
   released. so i   ve decided to give it a chance again. after a while, i
   understood that this framework is really easy to use and it makes me
   happy to code in pytorch. in this post, i will try to explain core
   concepts of it clearly so that you will be motivated at least give it a
   try right now, not after a few years or more. we will cover some basic
   principles and some advanced stuff as learning rate schedulers, custom
   layers and more.

resources

   first that you should know about pytorch it that [18]documentation and
   [19]tutorials are stored separately. also sometimes they may don   t meet
   each other, because of fast development and version changes. so fill
   free to investigate [20]source code. it   s very clear and
   straightforward. and it   s better to mention that there are exist
   awesome [21]pytorch forums, where you may ask any appropriate question,
   and you will get an answer relatively fast. this place seems to be even
   more popular than stackoverflow for the pytorch users.

pytorch as numpy

   so let   s dive into pytorch itself. the main building block of the
   pytorch is the tensors. really, they are very similar to the [22]numpy
   ones. tensors support a lot of the same api, so sometimes you may use
   pytorch just as a drop-in replacement of the numpy. you may ask what
   the reason is. the principal goal is that pytorch can utilize gpu so
   that you can transfer your id174 or any other computation
   hungry stuff to machine learning workhorse. and it   s very easy to
   convert tensors from numpy to pytorch and vice versa. let   s check some
   examples in code:

   iframe: [23]/media/caf8def11adef8f02d682c26b30f288b?postid=95ce8781a89c

from the tensors to the variables

   tensors are an awesome part of the pytorch. but mainly all we want is
   to build some neural networks. what is about id26? of
   course, we can manually implement it, but what is the reason?
   thankfully automatic differentiation exists. to support it pytorch
   [24]provides variables to you. variables are wrappers above tensors.
   with them, we can build our computational graph, and compute gradients
   automatically later on. every variable instance has two
   attributes: .data that contain initial tensor itself and .grad that
   will contain gradients for the corresponding tensor.

   iframe: [25]/media/214f557a06e55da09ba5bd2f2740b7cb?postid=95ce8781a89c

   you may note that we have manually computed and applied our gradients.
   it   s so tedious. do we have some optimizer? of course!

   iframe: [26]/media/1b9b56e531553ae43d9af79942cc1462?postid=95ce8781a89c

   now all our variables will be updated automatically. but the main point
   that you should get from the last snippet: we still should manually
   zero gradients before calculating new ones. this is one of the core
   concepts of the pytorch. sometimes it may be not very obvious why we
   should do this, but on the other hand, we have full control over our
   gradients, when and how we want to apply them.

static vs. dynamic computational graphs

   next main difference between pytorch and tensorflow is their approach
   to the graph representation. tensorflow [27]uses a static graph, that
   means that we define it once and after execute that graph over and over
   again. in pytorch each forward pass defines a new computational graph.
   in the beginning, the distinction between those approaches not so huge.
   but dynamic graphs became very handful when you want to debug your code
   or define some conditional statements. you can use your favorite
   debugger as it is! compare next two definitions of the while loop
   statements - the first one in tensorflow and the second one in pytorch:

   iframe: [28]/media/2e9029abd7bef658d7519be4e8176351?postid=95ce8781a89c

   iframe: [29]/media/ca2584bf9ad710e712c2e227ed13f462?postid=95ce8781a89c

   it seems to me that second solution much easier than first one. and
   what do you think about it?

models definition

   ok, now we see that it   s easy to build some if/else/while complex
   statements in pytorch. but let   s revert to the usual models. the
   framework provides out of the box layers constructors very similar to
   [30]keras ones:

     the nn package defines a set of modules, which are roughly
     equivalent to neural network layers. a module receives input
     variables and computes output variables, but may also hold internal
     state such as variables containing learnable parameters. the nn
     package also defines a set of useful id168s that are
     commonly used when training neural networks.

   iframe: [31]/media/e26dadacc9034bc873a236617a196a5f?postid=95ce8781a89c

   also if we want to build more complex models, we may subclass provided
   nn.module class. and of course, these two approaches can be mixed with
   each other.

   iframe: [32]/media/76edc5e2f5e6d498bbab08aa2b21062d?postid=95ce8781a89c

   at the __init__ method we should define all layers that will be used
   later. in the forward method, we should propose steps how we want to
   use already defined layers. backward pass, as usual, will be computed
   automatically.

self-defined layers

   but what if we want to define some custom model with nonstandard
   backprop? here is one example         xnor networks:
   [1*cjzifgglap9xgkg8mlrysq.png]

   i will not dive into details, more about this type of networks you may
   read in the [33]initial paper. all relevant to our issue is that
   id26 should be applied only to weights that less than 1 and
   greater than -1. in pytorch it [34]can be implemented quite easy:

   iframe: [35]/media/48bf0fc8fecfe815810a138441674709?postid=95ce8781a89c

   so as you may see, we should only define exactly two methods: one for
   forward and one for backward pass. if we need access to some variables
   from the forward pass we may store them in the ctx variable. note: in
   previous api forward/backward methods were not static and we stored
   required variables as self.save_for_backward(input) and access them as
   input, _ = self.saved_tensors.

train model with cuda

   if was discussed earlier how we might pass one tensor to the cuda. but
   if we want to pass the whole model, it   s ok to call .cuda() method from
   the model itself, and wrap each input variable to the .cuda() and it
   will be enough. after all computations, we should get results back
   with .cpu() method.

   iframe: [36]/media/a0754eaf7543b84f3a14bfabf1ada845?postid=95ce8781a89c

   also, pytorch supports direct devices allocation at the source code:

   iframe: [37]/media/a4d012ac9617d0a72a7ddd7b8f02e1bd?postid=95ce8781a89c

   because sometimes we want to run the same model on the cpu and the gpu
   without code modification i propose some kind of wrapper:

   iframe: [38]/media/e7a51f45014201aef5f5a02c86ed1460?postid=95ce8781a89c

weight initialization

   in tensorflow weights initialization mainly are made during tensor
   declaration. pytorch offers another approach         at first, tensor should
   be declared, and on the next step weights for this tensor should be
   changed. weights can be initialized as direct access to the tensor
   attribute, as a call to the bunch of methods inside torch.nn.init
   package. this decision can be not very straightforward, but it becomes
   useful when you want to initialize all layers of some type with same
   initialization.

   iframe: [39]/media/0152c521ff9c9df1ed546564f8dd7431?postid=95ce8781a89c

excluding subgraphs from backward

   sometimes when you want to retrain some layers of your model or prepare
   it for the production mode, it   s great when you can disable autograd
   mechanics for some layers. for this purposes, [40]pytorch provides two
   flags: requires_grad and volatile. first one will disable gradients for
   current layer, but child nodes still can calculate some. the second one
   will disable autograd for current layer and for all child nodes.

   iframe: [41]/media/dfdac4bb40bb2190da57603515b22f73?postid=95ce8781a89c

training process

   there are also exists some other bells and whistles in pytorch. for
   example, you may use [42]learning rate scheduler that will adjust your
   learning rate based on some rules. or you may enable/disable batch norm
   layers and dropouts with single train flag. if you want it   s easy to
   change random seed separately for cpu and gpu.

   iframe: [43]/media/a0e831526a0d22e2988647ad3f8ea1fc?postid=95ce8781a89c

   also, you may print info about your model, or save/load it with few
   lines of code. if your model was initialized with [44]ordereddict or
   class-based model string representation will contain names of the
   layers.

   iframe: [45]/media/af9f29a3d4938c4255a98764ccfdd6c2?postid=95ce8781a89c

   as per pytorch documentation saving model with state_dict() method is
   [46]more preferable.

logging

   logging of the training process is a pretty important part.
   unfortunately, pytorch has no any tools like tensorboard. so you may
   use usual text logs with [47]python logging module or try some of the
   third party libraries:
     * [48]a simple logger for experiments
     * [49]a language-agnostic interface to tensorboard
     * [50]log tensorboard events without touching tensorflow
     * [51]tensorboard for pytorch
     * [52]facebook visualization library wisdom

data handling

   you may remember [53]data loaders proposed in tensorflow or even tried
   to implement some of them. for me, it took about 4 hours or more to get
   some idea how all pipeline should work.
   [1*s00vu2hiejnz35zlj2kqfw.gif]
   image source: tensorflow docs

   initially, i thought to add here some code, but i think such gif will
   be enough to explain basic idea how all things happen.

   pytorch developers decided do not reinvent the wheel. they just use
   multiprocessing. to create your own custom data loader, it   s enough to
   inherit your class from torch.utils.data.dataset and change some
   methods:

   iframe: [54]/media/7499f54f158531418c5d8fbc27c01f22?postid=95ce8781a89c

   the two things you should know. first         image dimensions are different
   from tensorflow. they are [batch_size x channels x height x width]. but
   this transformation can be made without you interaction by
   preprocessing step torchvision.transforms.totensor(). there are also a
   lot of useful utils in the [55]transforms package.

   the second important thing that you may use pinned memory on gpu. for
   this, you just need to place additional flag async=true to a cuda()
   call and get pinned batches from dataloader with flag pin_memory=true.
   more about this feature [56]discussed here.

final architecture overview

   now you know about models, optimizers and a lot of other stuff. what is
   the right way to merge all of them? i propose to split your models and
   all wrappers on such building blocks:
   [1*a-cwynur2lqdehuf1_gdcw.png]

   and here is some pseudo code for clarity:

   iframe: [57]/media/528d40b05ef5e7aab5007d10cf57018b?postid=95ce8781a89c

conclusion

   i hope with this post you   ve understood main points of pytorch:
     * it can be used as drop-in replacement of numpy
     * it   s really fast for prototyping
     * it   s easy to debug and use conditional flows
     * there are lots of great tools out of the box

   pytorch is the fast-growing framework with an awesome community. and i
   think that today is the best day to try it out!

     * [58]machine learning
     * [59]pytorch
     * [60]tutorial
     * [61]tensorflow
     * [62]towards data science

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [63]go to the profile of illarion khlestov

[64]illarion khlestov

   machine learning researcher

     (button) follow
   [65]towards data science

[66]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [67]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [68]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/95ce8781a89c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/pytorch-tutorial-distilled-95ce8781a89c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/pytorch-tutorial-distilled-95ce8781a89c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_qwcuiuqpfmvl---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@illarionkhlestov?source=post_header_lockup
  17. https://towardsdatascience.com/@illarionkhlestov
  18. http://pytorch.org/docs/master/
  19. http://pytorch.org/tutorials/
  20. http://pytorch.org/tutorials/
  21. https://discuss.pytorch.org/
  22. https://docs.scipy.org/doc/numpy-dev/user/quickstart.html
  23. https://towardsdatascience.com/media/caf8def11adef8f02d682c26b30f288b?postid=95ce8781a89c
  24. http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html
  25. https://towardsdatascience.com/media/214f557a06e55da09ba5bd2f2740b7cb?postid=95ce8781a89c
  26. https://towardsdatascience.com/media/1b9b56e531553ae43d9af79942cc1462?postid=95ce8781a89c
  27. https://www.tensorflow.org/programmers_guide/graphs
  28. https://towardsdatascience.com/media/2e9029abd7bef658d7519be4e8176351?postid=95ce8781a89c
  29. https://towardsdatascience.com/media/ca2584bf9ad710e712c2e227ed13f462?postid=95ce8781a89c
  30. https://keras.io/
  31. https://towardsdatascience.com/media/e26dadacc9034bc873a236617a196a5f?postid=95ce8781a89c
  32. https://towardsdatascience.com/media/76edc5e2f5e6d498bbab08aa2b21062d?postid=95ce8781a89c
  33. https://arxiv.org/abs/1603.05279
  34. http://pytorch.org/docs/master/notes/extending.html
  35. https://towardsdatascience.com/media/48bf0fc8fecfe815810a138441674709?postid=95ce8781a89c
  36. https://towardsdatascience.com/media/a0754eaf7543b84f3a14bfabf1ada845?postid=95ce8781a89c
  37. https://towardsdatascience.com/media/a4d012ac9617d0a72a7ddd7b8f02e1bd?postid=95ce8781a89c
  38. https://towardsdatascience.com/media/e7a51f45014201aef5f5a02c86ed1460?postid=95ce8781a89c
  39. https://towardsdatascience.com/media/0152c521ff9c9df1ed546564f8dd7431?postid=95ce8781a89c
  40. http://pytorch.org/docs/master/notes/autograd.html
  41. https://towardsdatascience.com/media/dfdac4bb40bb2190da57603515b22f73?postid=95ce8781a89c
  42. http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate
  43. https://towardsdatascience.com/media/a0e831526a0d22e2988647ad3f8ea1fc?postid=95ce8781a89c
  44. https://docs.python.org/3/library/collections.html
  45. https://towardsdatascience.com/media/af9f29a3d4938c4255a98764ccfdd6c2?postid=95ce8781a89c
  46. http://pytorch.org/docs/master/notes/serialization.html
  47. https://docs.python.org/3/library/logging.html
  48. https://github.com/oval-group/logger
  49. https://github.com/torrvision/crayon
  50. https://github.com/teamhg-memex/tensorboard_logger
  51. https://github.com/lanpa/tensorboard-pytorch
  52. https://github.com/facebookresearch/visdom
  53. https://www.tensorflow.org/api_guides/python/reading_data
  54. https://towardsdatascience.com/media/7499f54f158531418c5d8fbc27c01f22?postid=95ce8781a89c
  55. http://pytorch.org/docs/master/torchvision/transforms.html
  56. http://pytorch.org/docs/master/notes/cuda.html#use-pinned-memory-buffers
  57. https://towardsdatascience.com/media/528d40b05ef5e7aab5007d10cf57018b?postid=95ce8781a89c
  58. https://towardsdatascience.com/tagged/machine-learning?source=post
  59. https://towardsdatascience.com/tagged/pytorch?source=post
  60. https://towardsdatascience.com/tagged/tutorial?source=post
  61. https://towardsdatascience.com/tagged/tensorflow?source=post
  62. https://towardsdatascience.com/tagged/towards-data-science?source=post
  63. https://towardsdatascience.com/@illarionkhlestov?source=footer_card
  64. https://towardsdatascience.com/@illarionkhlestov
  65. https://towardsdatascience.com/?source=footer_card
  66. https://towardsdatascience.com/?source=footer_card
  67. https://towardsdatascience.com/
  68. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  70. https://medium.com/p/95ce8781a89c/share/twitter
  71. https://medium.com/p/95ce8781a89c/share/facebook
  72. https://medium.com/p/95ce8781a89c/share/twitter
  73. https://medium.com/p/95ce8781a89c/share/facebook
