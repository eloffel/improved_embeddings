   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

using scrapy to build your own dataset

   [16]go to the profile of michael galarnyk
   [17]michael galarnyk (button) blockedunblock (button) followfollowing
   sep 26, 2017

   iframe: [18]/media/f97dea00c18e00285d104f29c72d97a8?postid=64ea2d7d4673

   web scraping (scrapy) using python

   when i first started working in industry, one of the things i quickly
   realized is sometimes you have to gather, organize, and clean your own
   data. for this tutorial, we will gather data from a crowdfunding
   website called [19]fundrazr. like many websites, the site has its own
   structure, form, and has tons of accessible useful data, but it is hard
   to get data from the site as it doesn   t have a structured api. in
   result, we will web scrape the site to get that unstructured website
   data and put into an ordered form to build our own dataset.

   in order to scrape the website, we will use [20]scrapy. in short,
   scrapy is a framework built to build web scrapers more easily and
   relieve the pain of maintaining them. basically, it allows you to focus
   on the data extraction using css selectors and choosing xpath
   expressions and less on the intricate internals of how spiders are
   supposed to work. this blog post goes a little beyond the great
   [21]official tutorial from the scrapy documentation in the hopes that
   if you need to scrape something a bit harder, you can do it on your
   own. with that, lets get started. if you get lost, i recommend opening
   the [22]video in a separate tab.

getting started (prerequisites)

   if you already have anaconda and google chrome (or firefox), skip to
   creating a new scrapy project.

   1. install anaconda (python) on your operating system. you can either
   download anaconda from the official site and install on your own or you
   can follow these anaconda installation tutorials below.

   iframe: [23]/media/29eef4d7b0f617a3b4458c7e4f3aa083?postid=64ea2d7d4673

   installing anaconda

   2. install scrapy (anaconda comes with it, but just in case). you can
   also install on your terminal (mac/linux) or command line (windows).
   you can type the following:
conda install -c conda-forge scrapy

   3. make sure you have google chrome or firefox. in this tutorial, i am
   using google chrome. if you don   t have google chrome, you can install
   it here using this [24]link.

creating a new scrapy project

   1.open a terminal (mac/linux) or command line (windows). navigate to a
   desired folder (see the image below if you need help) and type
scrapy startproject fundrazr

   [1*nstovwlionixptqm_yox9w.png]
   scrapy startproject fundrazr

   this makes a fundrazr directory with the following contents:
   [1*zedpgq0cl7ipjrywcxwybw.png]
   fundrazr project directory

finding good start urls using inspect on google chrome (or firefox)

   in the spider framework, start_urls is a list of urls where the spider
   will begin to crawl from, when no particular urls are specified. we
   will use each element in the start_urls list as a means to get
   individual campaign links.

   1. the image below shows that based on the category you choose, you get
   a different start url. the highlighted part in black are the possible
   categories of fundrazrs to scrape.
   [1*9epsvj6nmessns2ssvuxva.png]
   finding a good first start_url

   for this tutorial, the first in the list start_urls is:

   [25]https://fundrazr.com/find?category=health

   2. this part is about getting additional elements to put in the
   start_urls list. we are finding out how to go to the next page so we
   can get additional urls to put in start_urls.
   [1*zzb7x0mx3jj5_gvbjzfkeg.png]
   getting additional elements to put in start_urls list by inspecting
   next button

   the second start url is:
   [26]https://fundrazr.com/find?category=health&page=2

   the code below will be used in the code for the spider later in the
   tutorial. all it does is make a list of start_urls. the variable npages
   is just how many additional pages (after the first page) we want to get
   campaign links from.

   iframe: [27]/media/0269c9c34e179b37792dbdb06103bd8e?postid=64ea2d7d4673

   code to generate additional start urls based on current structure of
   website

scrapy shell for finding individual campaign links

   the best way to learn how to extract data with scrapy is using the
   scrapy shell. we will use xpaths which can be used to select elements
   from html documents.

   the first thing we will try and get the xpaths for are the individual
   campaign links. first we do inspect to see roughly where the campaigns
   are in the html.
   [1*n5mwpew1dsd-f_fqu0xmtq.png]
   finding links to individual campaigns

   we will use xpath to extract the part enclosed in the red rectangle
   below.
   [1*xoziitj5ls-waicejn03fq.png]
   the enclosed part is a partial url we will isolate

   in terminal type (mac/linux):
scrapy shell '[28]https://fundrazr.com/find?category=health'

   in command line type (windows):
scrapy shell    [29]https://fundrazr.com/find?category=health"

   type the following into scrapy shell (to help understand the code,
   please see the video):
response.xpath("//h2[contains([30]@class, 'title headline-font')]/a[contains([31
]@class, 'campaign-link')]//@href").extract()

   [1*gsfvvmuq85f1xg5l7mhyaa.png]
   there is a good chance you will get different partial urls as websites
   update over time

   the code below is for getting all the campaign links for a given start
   url (more on this later in the first spider section)

   iframe: [32]/media/5cd4bbc85b77e0c13cac1fdc418545c7?postid=64ea2d7d4673

   exit scrapy shell by typing exit().
   [1*62kspbart2im732mqnn4wg.png]
   exit scrapy shell

inspecting individual campaigns

   while we should previously worked on understanding the structure of
   where individual campaigns links are, this section goes over where
   things are on individual campaigns.
    1. next we go to an individual campaign page (see link below) to
       scrape (i should note that some of these campaigns are difficult to
       view)

   [33]https://fundrazr.com/savemyarm

   2. using the same inspect process as before, we inspect the title on
   the page
   [1*os8g6prp2zc3im8-rgcbwq.png]
   inspect campaign title

   3. now we are going to use scrapy shell again, but this time with an
   individual campaign. we do this because we want to find out how
   individual campaigns are formatted (including finding out how to
   extract the title from the webpage).

   in terminal type (mac/linux):
scrapy shell '[34]https://fundrazr.com/savemyarm'

   in command line type (windows):
scrapy shell    [35]https://fundrazr.com/savemyarm"

   the code to get the campaign title is
response.xpath("//div[contains([36]@id, 'campaign-title')]/descendant::text()").
extract()[0]

   [1*h6eftqceklegtcrq7cgdyq.png]

   4. we can do the same for the other parts of the page.

   amount raised:
response.xpath("//span[contains([37]@class,'stat')]/span[contains([38]@class, 'a
mount-raised')]/descendant::text()").extract()

   goal:
response.xpath("//div[contains([39]@class, 'stats-primary with-goal')]//span[con
tains([40]@class, 'stats-label hidden-phone')]/text()").extract()

   currency type:
response.xpath("//div[contains([41]@class, 'stats-primary with-goal')]/@title").
extract()

   campaign end date:
response.xpath("//div[contains([42]@id, 'campaign-stats')]//span[contains([43]@c
lass,'stats-label hidden-phone')]/span[[44]@class='nowrap']/text()").extract()

   number of contributors:
response.xpath("//div[contains([45]@class, 'stats-secondary with-goal')]//span[c
ontains([46]@class, 'donation-count stat')]/text()").extract()

   story:
response.xpath("//div[contains([47]@id, 'full-story')]/descendant::text()").extr
act()

   url:
response.xpath("//meta[[48]@property='og:url']/@content").extract()

   5. exit scrapy shell by typing:
exit()

items

   the main goal in scraping is to extract structured data from
   unstructured sources, typically, web pages. scrapy spiders can return
   the extracted data as python dicts. while convenient and familiar,
   python dicts lack structure: it is easy to make a typo in a field name
   or return inconsistent data, especially in a larger project with many
   spiders (almost word for word copied from the great scrapy official
   documentation!).
   [1*0xu-5axl8h9adzfkxhgylq.png]
   file we will be modifying

   the code for items.py is [49]here.

   save it under the fundrazr/fundrazr directory (overwrite the original
   items.py file).

   the item class (basically how we store our data before outputting it)
   used in this tutorial looks like this.
   [1*ittpe8dwr8cxipzymjohoa.png]
   items.py code

the spider

   spiders are classes that you define and that scrapy uses to scrape
   information from a website (or a group of websites). the code for our
   spider is below.
   [1*hhpzav8pvzopv2yhdudaoq.png]
   fundrazr scrapy code

   download the code [50]here.

   save it in a file named fundrazr_scrape.py under the fundrazr/spiders
   directory.

   the current project should now have the following contents:
   [1*vi0lowohvkaifkzuau5uoa.png]
   file we will be creating/adding

running the spider

    1. go to the fundrazr/fundrazr directory and type:

scrapy crawl my_scraper -o monthday_year.csv

   [1*84jjg6fgmokfcjxnivkjtq.png]
   scrapy crawl my_scraper -o monthday_year.csv

   2. the data should be outputted in the fundrazr/fundrazr directory.
   [1*v7eepnbptxkhwyjg3lkz4q.png]
   data output location

our data

    1. the data outputted in this tutorial should look roughly like the
       image below. the individual campaigns scraped will vary as the
       website is constantly updated. also it is possible there will be
       spaces between each individual campaign as excel is interpreting
       the csv file.

   [1*kbi0qls60c3ertljngzdeq.png]
   the data should roughly be in this format.

   2. if you want to download a larger file (it was made by changing
   npages = 2 to npages = 450 and adding download_delay = 2), you can
   download a bigger file with roughly 6000 campaigns scraped by
   downloading the file from my [51]github. the file is called
   minimorningscrape.csv (it is a large file).
   [1*bve-wshga0srq5ofykam2g.png]
   roughly 6000 campaigns scraped

closing thoughts

   creating a dataset can be a lot of work and is often an overlooked part
   of learning data science. one thing we didn   t go over is that while we
   scraped a lot of data, we still haven   t cleaned the data enough to do
   analysis. that is for another blog post though. if you have any
   questions or thoughts on the tutorial, feel free to reach out in the
   comments below, through [52]youtube, or through [53]twitter.

     * [54]python
     * [55]data science
     * [56]web scraping
     * [57]scrapy
     * [58]towards data science

   (button)
   (button)
   (button) 2.2k claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [59]go to the profile of michael galarnyk

[60]michael galarnyk

   data scientist at scripps research institute

     (button) follow
   [61]towards data science

[62]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.2k
     * (button)
     *
     *

   [63]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [64]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/64ea2d7d4673
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_b1c3jorxygzu---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@galarnykmichael?source=post_header_lockup
  17. https://towardsdatascience.com/@galarnykmichael
  18. https://towardsdatascience.com/media/f97dea00c18e00285d104f29c72d97a8?postid=64ea2d7d4673
  19. https://fundrazr.com/
  20. https://scrapy.org/
  21. https://doc.scrapy.org/en/latest/intro/tutorial.html
  22. https://www.youtube.com/watch?v=o_j3otxw2_e
  23. https://towardsdatascience.com/media/29eef4d7b0f617a3b4458c7e4f3aa083?postid=64ea2d7d4673
  24. https://support.google.com/chrome/answer/95346?co=genie.platform=desktop&hl=en
  25. https://fundrazr.com/find?category=health
  26. https://fundrazr.com/find?category=health&page=2
  27. https://towardsdatascience.com/media/0269c9c34e179b37792dbdb06103bd8e?postid=64ea2d7d4673
  28. https://fundrazr.com/find?category=health'
  29. https://fundrazr.com/find?category=health
  30. http://twitter.com/class
  31. http://twitter.com/class
  32. https://towardsdatascience.com/media/5cd4bbc85b77e0c13cac1fdc418545c7?postid=64ea2d7d4673
  33. https://fundrazr.com/savemyarm
  34. https://fundrazr.com/savemyarm'
  35. https://fundrazr.com/savemyarm
  36. http://twitter.com/id
  37. http://twitter.com/class
  38. http://twitter.com/class
  39. http://twitter.com/class
  40. http://twitter.com/class
  41. http://twitter.com/class
  42. http://twitter.com/id
  43. http://twitter.com/class
  44. http://twitter.com/class
  45. http://twitter.com/class
  46. http://twitter.com/class
  47. http://twitter.com/id
  48. http://twitter.com/property
  49. https://github.com/mgalarnyk/python_tutorials/raw/master/scrapy/fundrazr/fundrazr/items.py
  50. https://raw.githubusercontent.com/mgalarnyk/python_tutorials/master/scrapy/fundrazr/fundrazr/spiders/fundrazr_scrape.py
  51. https://github.com/mgalarnyk/python_tutorials/tree/master/scrapy/fundrazr/fundrazr
  52. https://www.youtube.com/watch?v=o_j3otxw2_e
  53. https://twitter.com/galarnykmichael
  54. https://towardsdatascience.com/tagged/python?source=post
  55. https://towardsdatascience.com/tagged/data-science?source=post
  56. https://towardsdatascience.com/tagged/web-scraping?source=post
  57. https://towardsdatascience.com/tagged/scrapy?source=post
  58. https://towardsdatascience.com/tagged/towards-data-science?source=post
  59. https://towardsdatascience.com/@galarnykmichael?source=footer_card
  60. https://towardsdatascience.com/@galarnykmichael
  61. https://towardsdatascience.com/?source=footer_card
  62. https://towardsdatascience.com/?source=footer_card
  63. https://towardsdatascience.com/
  64. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  66. https://medium.com/p/64ea2d7d4673/share/twitter
  67. https://medium.com/p/64ea2d7d4673/share/facebook
  68. https://medium.com/p/64ea2d7d4673/share/twitter
  69. https://medium.com/p/64ea2d7d4673/share/facebook
