   #[1]github [2]recent commits to
   self-driving-car-3d-simulator-with-id98:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]16
     * [35]star [36]118
     * [37]fork [38]49

[39]sagar448/[40]self-driving-car-3d-simulator-with-id98

   [41]code [42]issues 1 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   implementing a self driving car using a 3d driving simulator. id98 will
   be used for training
   [47]self-driving-car [48]autonomous [49]id98 [50]python [51]keras
   [52]opencv
     * [53]71 commits
     * [54]1 branch
     * [55]0 releases
     * [56]fetching contributors

    1. [57]python 100.0%

   (button) python
   branch: master (button) new pull request
   [58]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/s
   [59]download zip

downloading...

   want to be notified of new releases in
   sagar448/self-driving-car-3d-simulator-with-id98?
   [60]sign in [61]sign up

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [64]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [65]download the github extension for visual studio
   and try again.

   (button) go back
   [66]@sagar448
   [67]sagar448 [68]update readme.md (button)    
fixed the reward error. reward was being accumulated, reward needs to be a stabl
e 1.

   latest commit [69]24e2ba8 oct 7, 2017
   [70]permalink
   type             name             latest commit message   commit time
        failed to load latest commit information.
        [71]src
        [72]readme.md
        [73]selfdrivingcarwithnn.py [74]last minute changes sep 28, 2017

readme.md

self-driving-car-3d-simulator-with-id98

   [75][3d%20car%20simulator.png]
   [76][68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b69
   70656469612f636f6d6d6f6e732f7468756d622f302f30612f507974686f6e2e7376672
   f3230303070782d507974686f6e2e7376672e706e67]

   [77][687474703a2f2f67657474686564726966742e636f6d2f77702d636f6e74656e74
   2f75706c6f6164732f323031352f30362f57686974652d53706163652e706e67]

introduction

                self driving car after 50 epochs of training
                     [78][selfdrivingafter50epochs.gif]

   some point in our life as programmers we all wonder how a self driving
   car is actually programmed. i went through the same phase and so here
   it is, a very simple digital self driving car controlled using python
   with a reinforcement id24 algorithm as well as a convolutional
   neural network.

   you can essentially apply this to any game, the algorithm can be
   adapted and the reward rules can be changed to allow for different
   outcomes. as we go through the code i will explain step by step what
   each line does and once you've mastered it you can go ahead fork the
   code and do as you wish.
note: you need to have sufficient knowledge about reinforcment learning before p
rogressing, this tutorial
only explains the code it does not go into the theoretical details
the links below help explain the theoretical details as well as other details i
had problems with:

https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-d
iscount-factor-in-reinforcement-learning
http://mnemstudio.org/path-finding-id24-tutorial.htm
https://yanpanlau.github.io/2016/07/10/flappybird-keras.html
https://keon.io/deep-id24/
https://en.wikipedia.org/wiki/id24
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow
-part-0-id24-with-tables-and-neural-networks-d195264329d0

   we will be using keras to make the magic happen with tensorflow
   backend. assuming you are familiar with keras and tensorflow and have
   them installed we can start!
note: check my other gits for brief explanation on keras and other simple algori
thms such as the id98
and id56 if you are unfamiliar with keras/tensorflow!

my setup

   in order to detect lanes, we need to send game frames to the algorithm
   for processing. i used a library called [79]mss(multiplescreenshot), it
   allows the users to take quick screenshots with almost minimal effect
   in fps. unfortunately, it takes the screen shot of the entire screen if
   coordinates aren't specified, therefore in order to get game frames,
   the game needs to be properly positioned.

   the picture below depicts my environment.

           layout as displayed on my screen [80][environment.png]

   [81][687474703a2f2f67657474686564726966742e636f6d2f77702d636f6e74656e74
   2f75706c6f6164732f323031352f30362f57686974652d53706163652e706e67]
   you can set it up anyway you want but make sure to change the
   coordinates of the screenshot module so it only covers the game area.

   before we start the implementation it's a good idea to have the code
   open on the side as the comments have details you wouldn't want to
   miss.

implementation

imports

1     import cv2
2     import mss
3     import numpy as np
4     from keras.models import sequential
5     from keras.layers import dense, flatten
6     from keras.optimizers import sgd
7     from keras.layers.convolutional import conv2d
8     import pyautogui as p
9     import random
10    import time

   we start by importing a couple libraries. in order, we import opencv,
   our mss library, numpy for computation, keras for our id98, pyautogui to
   control our keyboard, random and finally time for delay purposes.

detecting lanes

1     #function calculates the lanes
2     def calculatelanes(orgimage):
3         errors = false
4         #since our game has yellow lanes, we can detect a specific color
5         #keep that color, and get rid of everything else to make it easier
6         #to detect the yellow lanes
7         #so we convert our image to the hsl color scheme
8         hslimg = cv2.cvtcolor(orgimage, cv2.color_bgr2hls)
9         #the lower and upper arrays define boundaries of the bgr color space
10        #bgr because opencv represents images in numpy in reverse order
11        #so for our yellow color we say that our pixels color that are yellow
will be
12        # r>= 100, b >= 0, g>=10 (lower limit), r<=255, b<=255, g<=40
13        lower = np.uint8([ 10,   0, 100])
14        upper = np.uint8([ 40, 255, 255])
15        #inrange basically finds the color we want in the hlsimg with the lowe
r and upper
16        #boundaries(the ranges)
17        yellow_mask = cv2.inrange(hslimg, lower, upper)
18        #we then apply this mask to our original image, and this returns an im
age showing
19        #only the pixels that fall in the range of that mask
20        yellowimg = cv2.bitwise_and(orgimage, orgimage, mask=yellow_mask)
21        #convert the original image to gray
22        grayimg = cv2.cvtcolor(yellowimg, cv2.color_bgr2gray)
23        #apply blurring
24        #the 5x5 is the gaussianblur kernel convolved with image
25        #the 0 is the sigmax and sigmay standard deviation usually taken as 0
26        blurredimg = cv2.gaussianblur(grayimg, (5, 5), 0)
27        #detect edges in the image
28        #700 is the max val, any edges above the intensity gradient of 700 are
 edges
29        #200 is the lowest intensity gradient, anything below is not an edge
30        imagewithedges = cv2.canny(blurredimg, threshold1=200, threshold2=700)
31        #these are the points of our trapezoid/hexagon that we crop out
32        points = np.array([[0, 310],[0, 300], [220, 210], [380, 210], [600, 30
0], [600, 310]])
33        #now we calculate the region of interest
35        #we first create a mask (a blank black array same size as our image)
36        mask = np.zeros_like(imagewithedges)
37        #then we fill the mask underneath the points(inside the polygon) we de
fined
38        #with color white (255)(this is the part of the image we want to proce
ss)
39        cv2.fillpoly(mask, [points], 255)
40        #this bitwise and function basically combines the two images
41        #the coloured bit where the pixels had a value 255 is kept while the
42        #top bit is removed (which is 0)
43        croppedimg = cv2.bitwise_and(blurredimg, mask)

   line 2 our parameter being our screen shot (orgimage)

   line 3 we initialise our variable errors to false indicating that
   currently we have no errors produced

   line 8 the lanes in the game are yellow and so we convert our image to
   the hsl color space in order to enhance our lanes. they were not very
   clear in the rgb space, therefore hsl was used.

   line 13&14 we define our upper and lower limit of the color space.
   although the boundaries are given in terms of rgb it is actually in
   hsl. the comments are in rgb to make it easier to understand. those
   limits represent the region where the color yellow falls within.
   therefore, we use those limits so we can seek out a similar color.

   line 17 now we apply the limits to our hsl image. it seeks out the
   color yellow and sets the rest of the pixels of the image to 0. we have
   essentially created a mask. an area where relevant pixels keep their
   values and the ones not needed are set to 0.

   line 20 the bitwise_and function basically takes a look at the pixel
   values and if the pixel value in the mask and the pixel value in the
   image have the same value they are kept, if they are different then it
   is set to 0. we are left with a image with only yellow region visible.

                                yellow image
                             [82][yellowimg.png]

   line 22 now we can convert our image to grayscale. we do this in order
   to make the edge detection more accurate. the canny edge detection
   function used later on essentially measures the magnitude of pixel
   intensity changes. therefore if we have colors that are similar to each
   other there isn't a big change in pixel intensity and it might not be
   considered an edge. grayscale images are also less computation heavy.

   line 26 we now apply a gaussian blur. we do this in order to get rid of
   rough edges. some realistic games or even in real life there are cracks
   on the road that might be considered something of interest so in order
   to get rid of the "noisy edges" we apply a blur.

   line 30 now we finally apply the edge detection function. we have
   thresholds that identify what can and cannot be considered an edge.

   line 32 we don't want all the edges detected in the image. we only want
   those that concern the lanes. so we create a region of interest, a
   specific set of coordinates.

   line 36 we create an empty black mask with the same space dimension as
   our image.

   line 39 anything around the polygon defined by our roi is filled with
   black while the inside is filled with the color white (255).

   line 43 finally we take our blurred image and we apply our mask to it.
   so the white region of our mask is replaced with our image while the
   rest is black (not used).

                                 croppedimg
                            [83][croppedimg.png]

   great now we've managed to narrow down our edges to the region that we
   are interested in. thats most of the processing done. we now want to
   get the appropriate lines and combine them into lanes. the next half of
   this function does exactly that.
1         #basically the accumulation of the most imperfect edges with the minim
um
2         #length being defined by 180
3         #thickness of the lines is 5
4         lines = cv2.houghlinesp(croppedimg, 1, np.pi/180, 180, np.array([]), 1
80, 5)
5         #now we need to find the slope, intercept and length of each of our de
tected lines
6         left_lines = []
7         length_left = []
8         right_lines = []
9         length_right = []
10        #we may not always detect a line that is why we do try/except statemen
t
11        try:
12            for line in lines:
13                #coordinates of a single line
14                for x1, y1, x2, y2 in line:
15                    #we dont want a vertical line or a horizontal line
16                    if x1==x2 or y1==y2:
17                        continue
18                    #slope formula
19                    slope = (y2-y1)/(x2-x1)
20                    #intercept
21                    intercept = y1 - slope*x1
22                    #length
23                    length = np.sqrt((y2-y1)**2+(x2-x1)**2)
24                    #y is reversed in images therefore a negative slope is a l
eft line not right
25                    if slope<0:
26                        left_lines.append((slope, intercept))
27                        length_left.append((length))
28                    else:
29                        right_lines.append((slope, intercept))
30                        length_right.append((length))
31          #now we have collected our similar lines into right and left lists
32          #now we can convert them into lanes by dot product all the similar l
ines with lengths
33          #the longer lines are weighted more therefore affect the lanes more
34          #then we normalise them by dividing by sum of the lengths(sort of li
ke averaginng)
35          left_lane  = np.dot(length_left,  left_lines) /np.sum(length_left)
if len(length_left) >0 else none
36          right_lane = np.dot(length_right, right_lines)/np.sum(length_right)
if len(length_right)>0 else none
37          #now we have the right lane and the left lane through averaging and
dot product
38          #now we need to convert them back into coordinates for pixel points
39          #having an equation of a line (assume infinite) we can select arbitr
ary points and find
40          #the x or y value accordingly.
41          #so we select arbitrary points for y1 = croppedimg.shape[0]
42          #and for y2 = y1*0.6, we need this in order to draw our lines (conve
rting to pixel coordinates)
43          #we all need them to be int so cv2.line can use them
44          leftx1 = int((croppedimg.shape[0] - left_lane[1])/left_lane[0])
45          leftx2 = int(((croppedimg.shape[0]*0.6) - left_lane[1])/left_lane[0]
)
46          rightx1 = int((croppedimg.shape[0] - right_lane[1])/right_lane[0])
47          rightx2 = int(((croppedimg.shape[0]*0.6) - right_lane[1])/right_lane
[0])
48          left_lane = ((leftx1, int(croppedimg.shape[0])), (leftx2, int(croppe
dimg.shape[0]*0.6)))
49          right_lane = ((rightx1, int(croppedimg.shape[0])), (rightx2, int(cro
ppedimg.shape[0]*0.6)))
50          #now we can draw them on the image
51          #we first create an empty array like our original image
52          #then we draw the lines on the empty image and finally combine with
our original image
53          emptimg = np.zeros_like(orgimage)
54          #[255, 0, 0,]is the color, 20 is the thickness
55          #the star allows us to input a tuple (it processes as integer points
)
56          cv2.line(emptimg, *left_lane, [255, 0, 0], 20)
57          cv2.line(emptimg, *right_lane, [255, 0, 0], 20)
58          #finally we combine the two images
59          #it calculates the weighted sum of two arrays
60          #1.0 is the weight of our original image, we don't want to amplify i
t
61          #0.95 is the weight of our lines, and 0.0 is the scalar added to the
 sum
62          #be very significant in the image, just enough so we can see it and
not obstruct anything else
63          finalimg = cv2.addweighted(orgimage, 1.0, emptimg, 0.95, 0.0)
64      except:
65          errors = true
66          print("nothing detected")
67          #if we dont detect anything or to avoid errors we simply return the
original image
68          return orgimage, errors
69      #if all goes well, we return the image with the detected lanes
70      return finalimg, errors

   line 6-line 9 we initiate empty lists for our data. left and right
   lines and their corresponding lengths.

   line 4 the function houghlines is quite difficult to understand. in
   layman term it is simply detecting lines in our region and returning
   coordinates. we set a threshold of 180 the length, and the thickness
   being 5. to find out more about hough transformation, go to this
   [84]link

   line 12-line 30 looping over all the detected lines we take one line at
   a time and we calculate the intercept and the slope. we omit horizontal
   and vertical lines as our lanes will never be straight in that
   perspective. finally, depending on the slope we append our lines
   accordingly to the right and left lanes.

   line 35-line 36 we want to combine all our lines into lanes. we compute
   the dot product of the lines and their respective lengths. longer lines
   have a heavier effect and so the slopes and intercepts of those line
   will be more dominant. finally divide by the lengths to essentially
   normalise the values(so it can be mapped to the image)

   line 44-line 47 we have the lanes, but in order to draw them we need
   coordinates. to draw any line (assuming infinite) any arbitrary point
   can be used. using the arbitrary y values i calculate the x values.

   line 48-line49 now we just group those points accordingly to the right
   and left lanes.

   line 53 we want to draw the line on top of our image. but in order to
   do that we need to have an overlay image. so here, we create an empty
   image with the same space dimensions as our original.

   line 56-line 57 then we draw our lines on our empty image. the color
   used is blue as the format is bgr and not rgb.

   line 63 finally we combine the two images. this is done by calculating
   the weighted sum of the two arrays of images. in our empty image most
   of the pixels are set to 0 and so only the lane pixels will be
   effected.

   line 65-line 68 if any errors or a lane wasn't detected then we simply
   just output our original image.

   line 70 if all goes well, we output our final processed image.

                                  finalimg
                             [85][finalimg.png]

   now we can go ahead and explore the next part of the code. the next
   part explains how to format our processed images so it could be
   accepted by our keras id98.
1     #processes the images and returns the required data
2     def getframes():
3         #we initialise the mss screenshot library
4         sct = mss.mss()
5         #this essentially takes a screenshot of the square from the coordinate
s
6         #you can adjust these to your liking,
7         game = {'top': 122, 'left': 0, 'width': 512, 'height': 286}
8         #this converts the screenshot into a numpy array
9         gameimg = np.array(sct.grab(game))
10        #we want to resize the array so we can easily display it
11        gameimg = cv2.resize(gameimg, (600, 400))
12        #we pass the array into our calculatelanes function
13        #it returns our detected lanes image as well as if any errors were pro
duced
14        img, errors = calculatelanes(gameimg)
15        #you can show the render if you want with the lanes detections
16        cv2.imshow('window', img)
17        #to further process the image we convert it to a grayscale
18        img = cv2.cvtcolor(cv2.resize(img, (84, 84)), cv2.color_bgr2gray)
19        #in order for keras to accept data we reshape it into the specific for
mat
20        #i want to use an image thats 84x84
21        img = img.reshape(1, 84, 84)
22        #in order to give the algorithm the feel of the "velocity" we stack th
e 4 images
23        input_img = np.stack((img, img, img, img), axis = 3)
24        #this is required for opencv as a failsafe for stopping render
25        #by pressing q, you can stop render
26        if cv2.waitkey(25) & 0xff == ord('q'):
27            cv2.destroyallwindows()
28        #if all goes well we return the input_img and the errors
29        return input_img, errors

   line 2 this function essentially processes our screenshots using the
   lane detection function and then formats the image data so we can then
   use it with our id98.

   line 4 we initialise our screenshot library here.

   line 7 game stores the dimensions of our screenshots. it represents the
   area of the screen we took the screenshot of.

   line 9 we convert it to a numpy array for further processing.

   line 11 i resized the image so when we display it, it can fit on the
   screen. note, if you change the size of the screen you will need to
   edit the coordinates of the roi mask in the lane detection function in
   order to account for the size increase or decrease.

   line 14 we now call the calculatelane() function passing the resized
   game screenshot as a paramter. it returns either the original image
   back to us or it returns our image with detected lanes.

   line 16 you can choose to render your detection but it will slow down
   the process quite a bit.

   line 18 we can now start formatting our image for our id98. our first
   step is to resize it to a suitable size for the id98 to process as well
   as to convert it to grayscale.

   line 21 since keras needs a specific dimension we reshape our image to
   1x84x84. the 1 is essentially the batch number.

   line 23 the id98 needs to make logical decisions. therefore, without any
   sense of velocity the id98 cannot perform. in order to provide the id98
   with some sense of velocity we stack our images. thus, our dimension of
   our input is now (1, 84, 84, 4).

   line 26-line 27 if you've ever used opencv and decided to display your
   image/video then you know to always put this at the end or the
   image/video will not display.

   line 29 finally, we return our input and errors (errors for the
   calculatelanes function)

   that takes care of all the image processing. we can now go ahead and
   start taking a look at function that controls our car in game.
#this function makes the car accelerate
def straight():
    p.keydown("up")
    p.keyup("up")

#we can turn right with this
def right():
    p.keydown("right")
    p.keyup("right")

#turn left with this
def left():
    p.keydown("left")
    p.keyup("left")

   function straight() key down function presses the specific key on our
   keyboard. keyup is important as keydown holds the key, so we need to
   release it. this function is responsible for accelerating our car.

   function right() turns our car to the right.

   function left() turns our car to the left.

   we are now ready to start building our id98 model. our model will be
   quite similar to the other id98 models in the past, we will try to map
   our image data to the actions.
1     #for now we make the car accelerate, turn right and turn left
2     moves = 3
3     #learning rate (discount rate)
4     learningrate = 0.9
5     #this is the exploration rate (epsilon)
6     #its better at first to let the model try everything
7     epsilon = 1.0
8     #we don't want our model to stop exploring so we set a minimum epsilon
9     epsilon_min = 0.01
10    #we also dont want our model to explore all the time therefore we want it
11    #to decay
12    epsilon_decay = 0.995
13    #number of times we want to train the algorithm (the number of games)
14    epochs = 100
15    #we want to store our data for our replay so our model can remember the pa
st experiences
16    memory = []
17    #the max amount of stuff we want to remember
18    max_memory = 500
19
20    #lets start defining our model
21    model = sequential()
22    #we will be using a id98 with 32 filters, 3x3 kernel and the input shape wi
ll be
23    #84x84 with 4 grayscale images stacked on top
24    #padding will be set as same(padding with 0) and we will use the rectified
 activation function
25    model.add(conv2d(32, (3, 3), input_shape=(84, 84, 4), padding='same',
26                     activation='relu'))
27    #this time we will use 64 filters with a 3x3 kernel, with the same act fun
ction
28    #but the padding will change
29    model.add(conv2d(64, (3, 3), activation='relu', padding='valid'))
30    model.add(conv2d(64, (3, 3), activation='relu', padding='valid'))
31    #we flatten our data in order to feed it through the dense(output) layer
32    model.add(flatten())
33    model.add(dense(512, activation='relu'))
34    #we have 3 outputs, forward, left, right
35    model.add(dense(3, activation='linear'))
36    #we will be using the mean squared error
37    model.compile(loss='mean_squared_error',
38                  optimizer=sgd())

   line 2 as shown in the code above, our car can do 3 things. accelerate,
   turn right and turn left. thus we set our moves variable to 3.

   line 4 this is our discount rate. we want our immediate reward to be
   worth more than our future reward therefore we discount the future
   reward in order make the current reward stand out. this is because our
   model is uncertain what the next step may be. (more on this later)

   line 7 this is the exploration rate. we want our algorithm to start off
   by trying different actions.

   line 9 we don't want our model to ever stop trying random actions so we
   set our minimum exploration rate.

   line 12 this is the rate at which our exploration factor decays.

   line 14 this is the number of games we want to play in total.

   line 16 all the games ever played go in here. we want our model to
   learn from its mistakes.

   line 18 we don't want to store too many games as it becomes computation
   heavy.

   now we can start building our actual id98 model.

   line 21 we initialise our machine learning algorithm.

   line 25 this is our first convolutional layer. we want to output 32
   filters with a 3x3 kernel and our input shape will be 84x84x4. we set
   our activation function to rectified linear unit.

   line 29-line 30 we add another two convolutional layers for better
   accuracy.

   line 32-line 33 we flatten our data so we can put it through a hidden
   layer of a simple neural network.

   line 35 this is the final output layer with 3 nodes. it calculates the
   id203 of our 3 actions.

   line 37 configuration for our loss and optimisation function.

   finally, we've reached the last step of the tutorial, our id24
   algorithm. the brain and heart of the algorithm. this algorithm decides
   the actions to take and essentially trains our car to be a better
   driver.
1      #loop over the number of epochs (essentially the number of games)
2      for i in range(epochs):
3         #time.sleep(5)
4         #we set the game_over to false as the game is just starting
5         game_over = false
6         #we start of by getting initial frames and errors
7         input_img, errors = getframes()
8         #we set the errors to false to begin with
9         errors = false
10        #we set the reward to 0
11        reward = 0
12        #while the game is not over we loop
13        while game_over==false:
14            #np.random.rand() returns a number between 0 and 1
15            #we check if its smaller that our exploration factor
16            if np.random.rand() <= epsilon:
17                #if the random number is smaller than our exploration factor
18                #we select a random action from our 3 actions
19                action = np.random.randint(0, moves, size=1)[0]
20            else:
21                #if it's not smaller than we predict an output by inputting ou
r
22                #4 stacked images
23                #ouput is the id203 of our 3 directions
24                output = model.predict(input_img)
25                #action is the index of the highest id203 and therefore
26                #indicates which turn to take
27                action = np.argmax(output[0])
28            #if our action == 0 then we go straight
29            if int(action) == 0:
30                straight()
31            #if our action == 1 then we go right
32            elif int(action) == 1:
33                right()
34            #else we go left
35            else:
36                left()

   line 2 we loop over the amount of games we want to play. in this case i
   have set the epochs to 100.

   line 3 originally i had left the time.sleep in the program as this
   allowed me to prepare for the start of the algorithm but it also slows
   down the learning stage therefore it is commented out.

   line 5 the ai is about to start playing the game so we originally set
   the game_over to false. we will need it later.

   line 7 we start by getting the initial "state" of the algorithm. we
   will need this to predict our corresponding action.

   line 9 despite the error in line 7 we set our errors to false as errors
   at the start do not matter, the algorithm will be performing a random
   action to begin with.

   line 11 initialise our rewards variable to 0.

   line 13 looping for one game, while the game isn't false

   line 16-line 19 we start off by checking if our exploration is bigger
   than a random number between 0 and 1. at the begining it will be and so
   we select a random action from our 3 actions.

   line 20-line 27 once our exploration rate is low enough, we can start
   predicting our actions. output stores a numpy array of size 3 produced
   by the prediction from our input image. action stores the index of the
   maximum id203.

   line 29-line 36 based on our predicted or random action we select one
   of the functions to run that controls our car.

   halfway through! from here we can now actually start studying the bulk
   of the id24 algorithm!
1             #once we've performed our action we get the next frame
2             #we also check weather to reward the algorithm or not
3             input_next_img, errors = getframes()
4             #if we detect lanes and therefore no errors occur we reward the al
gorithm
5             if errors == false:
6                 reward = 1
7             #else if there we detect no lanes and so there is an error we
8             #say its game over
9             else:
10                reward = 0
11                game_over = true
12            #game over or not we want to keep record of the steps the algo too
k
13            #we first check if the total memory length is bigger than the max
memory
14            if len(memory) >= max_memory:
15                #if more memory then needed we delete the first ever element w
e added
16                del memory[0]
17            #we append it to our memory list
18            memory.append((input_img, action, reward, input_next_img, game_ove
r))
19            #next we set our input_img to our latest data
20            input_img = input_next_img
21            if game_over:
22                print("game: {}/{}, total reward: {}".format(i, epochs, reward
))
23        #once the game is over we want to train our algo with the data we just
 collected
24        #we check if our memory length is bigger than our batch size
25        if len(memory) > 32:
26        #if so then we set the batch_size to 32
27            batch_size = 32
28        else:
29        #else we set our batch size to whatever is in the memory
30            batch_size = len(memory)
31        #we are taking a random sample of 32 so not to overfit our algo
32        batch = random.sample(memory, batch_size)
33        #we itereate over every memory we've stored in that memory batch of 32
34        for input_img, action, reward, input_next_img, game_over in batch:
35            #if in that memory our game was over then we set the target_reward
 equal to reward
36            target_reward = reward
37            #if our game was not over
38            if game_over == false:
39            #this essentially is the bellman equation
40            #expected long-term reward for a given action is equal to the
41            #immediate reward from the current action combined with the expect
ed
42            #reward from the best future action taken at the following state.
43            #the model isn't certain that for that specific action it will get
 the best reward
44            #it's based on id203 of the action, if the id203 of th
at action is in the
45            #negatives then our future reward is going to be further decreased
 by our learning rate
46            #this is just the model being cautious, as to not set an impossibl
e reward target
47            #if the reward is impossible then the algorithm might not converge
48            #converge as in a stable condition where it can play the game with
out messing up
49                target_reward = reward + learningrate * \
50                np.amax(model.predict(input_next_img)[0])
51            #so from above we essentially know what is going to happen(input_n
ext_img)
52            #assuming the game wasn't over, the algorithm did well.
53            #so we want the algorithm to perform the same, essentially we
54            #persuade the algorithm to do what it did to get that reward
55            #so we make the algorithm predict from the previous frame(input_im
g)
56            #but we alter its prediction according to the action that got the
highest
57            #reward and...
58            desired_target = model.predict(input_img)
59            #we set that as the target_reward...
60            desired_target[0][action] = target_reward
61            #so to make the algo perform the same, we associate the input_img
with the
62            #target we want and we fit it
63            model.fit(input_img, desired_target, epochs=1, verbose=0)
64        #finally we check if our exploration factor is bigger than our minimum
 exploration
65        #if so we decrease it by the decay to reduce exploration, we do this e
very game
66        if epsilon > epsilon_min:
67            epsilon *= epsilon_decay

   line 3 after the action we get our next frame, and errors if any.

   line 5-line 6 after the action has been performed and we have the next
   frame with calculated lanes and it does not return any errors then we
   set the reward to 1.

   line 9-line 11 if it does return errors then we say that the game is
   over. i have set it up like that so the algorithm can learn to drive
   within lanes. the error is associated with either the lanes not being
   detected or simply because the car was not within any lanes to detect.
   the latter being more probable and thus provides reason for the
   specific guidelines. i also set the reward to 0 as the algorithm fails
   to achieve its goal

   line 14-line 16 regardless the status of the game_over variable, we
   want to record the gameplay that happened. this enables the algorithm
   to learn from it's mistakes. so in this piece of code, we check whether
   the memory is full or not, if so we delete the very first item
   appended.

   line 18 we append to the memory array.

   line 20 we set our next set of frames to our current set of frames.
   essentially progressing our variable input_img to the next undecided
   action frame.

   line 21-line 22 if game was over we print out our statistics.

   line 25-line 30 this simply put is the setup for our replay section of
   the q-algorithm. we want to select random sample of batches to train
   our algorithm with. our default batch size is 32, but at the begining
   there wouldn't be enough to sample 32 batches. therefore, we train the
   algorithm with the whole memory array.

   line 34-line 36 iterating over our memory, we begin by setting our
   target reward to our reward in the first sample memory.

   line 38-line 50 in that memory if our game wasn't over than that means
   our algorithm performed well. so we want to persuade our algorithm to
   do the same thing in the future. therefore we set our future reward
   (target reward) to the current reward from the current action combined
   with the expected reward from the best future action taken at the
   following state. we multiply by our learningrate to avoid converging
   problems. we are essentially increasing the id203 of our desired
   action.

   line 58 here we ask the algorithm again what it might predict for the
   previous state.

   line 60 we manipulate the prediction, we take the prediction and insert
   our own id203 of our corresponding action. simply telling the
   algorithm that for a situation like this we want this action to be
   performed.

   line 63 we feed the manipulations and the results into our model to
   train it for a single epoch.

   line 66-line 67 finally, after everything is done, we decrease our
   exploration rate by multiplying our epsilon with our epsilon decay
   rate.

conclusion

   well thats it for the self driving car! you can definitely make your
   algorithm more complex by adding different directions, making your
   convolutional layers deeper etc. you can even apply this to another car
   game, create your own guidelines and own methods of rewards! if there
   are any question please don't hesitate to contact me, i am happy to
   help. i am open to feedback and different ways in which i could improve
   this, maybe you have a better way of doing this. other than any
   questions, if you find a mistake while reading through this please let
   me know! happy coding!

     *    2019 github, inc.
     * [86]terms
     * [87]privacy
     * [88]security
     * [89]status
     * [90]help

     * [91]contact github
     * [92]pricing
     * [93]api
     * [94]training
     * [95]blog
     * [96]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [97]reload to refresh your
   session. you signed out in another tab or window. [98]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commits/master.atom
   3. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/sagar448/self-driving-car-3d-simulator-with-id98
  32. https://github.com/join
  33. https://github.com/login?return_to=/sagar448/self-driving-car-3d-simulator-with-id98
  34. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/watchers
  35. https://github.com/login?return_to=/sagar448/self-driving-car-3d-simulator-with-id98
  36. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/stargazers
  37. https://github.com/login?return_to=/sagar448/self-driving-car-3d-simulator-with-id98
  38. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/network/members
  39. https://github.com/sagar448
  40. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
  41. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
  42. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/issues
  43. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/pulls
  44. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/projects
  45. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/self-driving-car
  48. https://github.com/topics/autonomous
  49. https://github.com/topics/id98
  50. https://github.com/topics/python
  51. https://github.com/topics/keras
  52. https://github.com/topics/opencv
  53. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commits/master
  54. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/branches
  55. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/releases
  56. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/graphs/contributors
  57. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/search?l=python
  58. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/find/master
  59. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/archive/master.zip
  60. https://github.com/login?return_to=https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
  61. https://github.com/join?return_to=/sagar448/self-driving-car-3d-simulator-with-id98
  62. https://desktop.github.com/
  63. https://desktop.github.com/
  64. https://developer.apple.com/xcode/
  65. https://visualstudio.github.com/
  66. https://github.com/sagar448
  67. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commits?author=sagar448
  68. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commit/24e2ba8c2ef012e812a2f8cf5ec3b753bbdd1272
  69. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commit/24e2ba8c2ef012e812a2f8cf5ec3b753bbdd1272
  70. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/tree/24e2ba8c2ef012e812a2f8cf5ec3b753bbdd1272
  71. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/tree/master/src
  72. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/readme.md
  73. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/selfdrivingcarwithnn.py
  74. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/commit/23367fd6aa83ae8d551bcbb05053ec9f01a1697f
  75. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/3d car simulator.png
  76. https://camo.githubusercontent.com/bc6681b6a28bdf795f4c425de83086b6c736f8d8/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f302f30612f507974686f6e2e7376672f3230303070782d507974686f6e2e7376672e706e67
  77. https://camo.githubusercontent.com/818b0676eda5cc87d486c3512844e57f4532dda6/687474703a2f2f67657474686564726966742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031352f30362f57686974652d53706163652e706e67
  78. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/selfdrivingafter50epochs.gif
  79. https://pypi.python.org/pypi/mss/
  80. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/environment.png
  81. https://camo.githubusercontent.com/818b0676eda5cc87d486c3512844e57f4532dda6/687474703a2f2f67657474686564726966742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031352f30362f57686974652d53706163652e706e67
  82. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/yellowimg.png
  83. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/croppedimg.png
  84. http://www.swarthmore.edu/natsci/mzucker1/opencv-2.4.10-docs/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html
  85. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98/blob/master/src/finalimg.png
  86. https://github.com/site/terms
  87. https://github.com/site/privacy
  88. https://github.com/security
  89. https://githubstatus.com/
  90. https://help.github.com/
  91. https://github.com/contact
  92. https://github.com/pricing
  93. https://developer.github.com/
  94. https://training.github.com/
  95. https://github.blog/
  96. https://github.com/about
  97. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
  98. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98

   hidden links:
 100. https://github.com/
 101. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
 102. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
 103. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98
 104. https://help.github.com/articles/which-remote-url-should-i-use
 105. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#self-driving-car-3d-simulator-with-id98
 106. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#introduction
 107. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#my-setup
 108. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#implementation
 109. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#imports
 110. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#detecting-lanes
 111. https://github.com/sagar448/self-driving-car-3d-simulator-with-id98#conclusion
 112. https://github.com/
