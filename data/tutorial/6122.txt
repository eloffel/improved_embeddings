   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

solving the schr  dinger equation with deep learning

new insights into a familiar problem

   [14]go to the profile of steven steinke
   [15]steven steinke (button) blockedunblock (button) followfollowing
   sep 27, 2017

   [16]the code used below is on github.

   in this project, we   ll be solving a problem familiar to any physics
   undergrad         using the schr  dinger equation to find the quantum ground
   state of a particle in a 1-dimensional box with a potential. however,
   we   re going to tackle this old standby with a new method: deep
   learning. specifically, we   ll use the tensorflow package to set up a
   neural network and then train it on random potential functions and
   their numerically calculated solutions.

   why reinvent the wheel (ground state)? sure, it   s fun to see a new tool
   added to the physics problem-solving toolkit, and i needed the practice
   with tensorflow. but there   s a far more compelling answer. we know
   basically everything there is to know about this topic already. the
   neural network, however, doesn   t know any physics. crudely speaking, it
   just finds patterns. suppose we examine the relative strength of
   connections between input neurons and output. the structure therein
   could give us some insight into how the universe    thinks    about this
   problem. later, we can apply deep learning to a physics problem where
   the underlying theory is unknown. by looking at the innards of that
   neural network, we might learn something new about fundamental physical
   principles that would otherwise remain obscured from our view. therein
   lies the true power of this approach: peering into the mind of the
   universe itself. furthermore, there is a widespread hypothesis that
   quantum mechanics, and perhaps all of physics, is derived from
   information-theoretic principles. from this perspective, a pure
   information-processing system like a neural network should be a
   powerful way to crack tough problems wide open. while there are a few
   very recent papers applying deep learning to quantum systems, as far as
   i can tell, no one is really looking into how deep learning solves
   quantum systems.

   now, on to brass tacks. we need training data for the network. we
   provide this by generating thousands (here, 9600) random potential
   functions and then solving them using conventional methods. the physics
   neophyte can think of these potential graphs as representing hills. the
   higher the hill, the more energy it takes for a particle to get to the
   top. in classical physics, a particle can   t climb a hill requiring more
   energy than the particle has, so if it started on one side, you   d never
   find it on the other . on the other hand, in quantum physics, a
   phenomenon called    tunneling    allows a particle to    climb    a hill
   higher than would be possible classically         though this phenomenon is
   limited to extremely small distances. the higher the hill, the lower
   the tunneling id203. the edges of the graph are treated as having
   infinite height, so the particle cannot escape the region of the graph.

   the random procedure generates three types of potentials: flat steps of
   random height and width, piecewise linear graphs zigging and zagging up
   and down, and random fourier series (graphs built from sine waves). as
   the procedure progresses, the graphs get choppier and choppier.
   [1*1kicgvmisoil_r8t_lfgvw.png]
   early random step potential (green), linear potential (yellow), and
   fourier potential (blue)
   [1*gd9tiuj3narsyue7rqjima.png]
   later random step (blue), linear (yellow), and fourier potential
   (green). they look almost the same now.

   once the potentials have been generated, the code numerically finds the
   quantum ground state         the state of lowest possible energy. in
   classical physics, that would simply be a particle at rest at the
   bottom of the deepest valley. in quantum mechanics, however, there is
   an uncertainty relation forbidding a particle from having a definite
   position (i.e., being at the base of a particular hill) or a definite
   momentum (i.e., being exactly at rest). there are deeper, esoteric
   explanations for why this is the case, but we   ll leave those for a more
   specialized blog.

   the net result is that the quantum state is a wavefunction         a
   id203 wave showing where the particle is most likely to be found
   (if, say, it were probed by photons or another particle). typically, it
   will avoid regions where the potential graph is high, and, because the
   sides of the box have infinite potential, the graph must go to zero at
   the edges.

   for a particle of unit mass in a box of unit length, the energy is
   given by
   [1*z0id19qiutdfh2iz0el9lua.png]

   where   (x) is the wavefunction and v(x) is the potential function. the
   first term of the integral represents the contribution of the
   particle   s kinetic energy to the total, and the second is the
   contribution of its potential energy. if v(x) = 0, the minimum energy
   is     /2, or about 4.9 units.

   by discretizing x into a set of values {x0,x1,   xn} (here, n = 128) and
   treating each   (xi) as an independent variable, we can use tensorflow   s
   id119 algorithm to find a series of gradual adjustments to
   the wavefunction that decreases its energy a little bit every step.
   starting from a simple sine wave, over many iterations (20k), a close
   approximation to the true ground state is reached.
   [1*jly72tlupq1bt9cy-l34pg.png]
   the gradual evolution of the ground state solution in a random
   potential (blue). the earlier iterations are more sinusoidal; the
   solution eventually becomes more localized around cell 30.

   this set of data         thousands of graphs of potential energy functions
   and their corresponding numerically evaluated ground states         is
   divvied up into training data (9600 potentials and ground states) and
   validation data (2400 pairs) to feed the next step in the program.
   [1*jp1dr2raxindneyystkimg.png]
   ground states for a large collection of different potentials (not
   pictured)

   we set up the simplest possible network with reasonable a shot of
   solving this problem: 2 hidden layers and an output layer, all
   activated by the [17]softplus function, with each layer having n
   (actually, n-1, or 127, since the boundaries are fixed at 0) nodes.

   schematically, the neural network is set up like this:
   [1*jdz52sfexcyilrndcwp2ea.png]
   neural network setup, except the real one has a lot more nodes.

   we can think of the input layer as an n-dimensional vector. each value
   of the potential function is weighted and then added to the input going
   into a neuron. this is repeated for every neuron in the hidden layer.
   these weights         every line in the diagram above         all have different
   values. thus, the weights comprise an n  n tensor. each neuron gets a
   bias         a value added to the input independent of what   s coming in from
   the potential function         thus, another n-dimensional vector. next, the
   softplus function, s(y) = ln(1+exp(y)), is applied to the value in each
   node to introduce nonlinearity to the solution. this output becomes the
   input fed into the next hidden layer, which in turn is fed to the
   output layer, producing a predicted ground state for a given input
   potential. mathematically, it can be expressed simply:
   [1*zqn29egwpi7eohmttxs80w.png]

   let   s be really clear here for a second. at each step of the algorithm,
   for each of the 9600 v   s, the same weights wi and biases bi are applied
   to generate 9600 predicted      s. these are then compared to the    actual   
        s we solved for previously, to compute the cost function:
   [1*hvjlrzp_xlz1sbdmolh4cq.png]

   the w   s and b   s are the variables we will update to minimize the cost
   as the id119 algorithm proceeds. because there are so many
   of these (3 layers of 127  127 tensors and 3 vectors of 127 biases =
   48768 independent variables), it   s possible that the solution that
   minimizes the cost will be overfit         that is, it will be specially
   tailored to the unique set of 9600 pairs we fed in to begin with. in
   this case, there are actually 127 predicted values (every x value of
   the ground state) in each pair of training data, yielding a ratio of
   25:1 predictions to independent weights. this makes overfitting
   somewhat less likely. as a further check against overfitting, we feed
   the validation data through the network without minimizing its cost. if
   the validation data generate a comparable cost to the training data,
   this serves as an independent check that our network actually solves
   the general problem of the 1-d sch  dinger equation, rather than solving
   the specific problem of matching the set of random potentials we fed it
   to their solutions.

   the one additional subtlety here is that the algorithm updates the
   weights and biases faster at first and then more slowly as the
   algorithm proceeds, but other than that, we   ve got the whole procedure
   mapped out. now, let   s talk about the results.

   after 100,000 iterations, the average training cost is 0.003 and the
   validation cost is 0.009, corresponding to the network   s prediction
   differing from the rms values of the actual graphs by 0.05 and 0.09,
   respectively. these values might not be very meaningful, so let   s take
   a look at some plots. i went through a few dozen each of the training
   and validation plots and selected good, average, and the worst examples
   of each. the potential is in yellow (not to the same scale), the actual
   solution is green, and the network   s prediction is in blue.
   [1*tmc7ni40btyg2qbmbn-1ca.png]
   good match between training ground state and network prediction
   [1*r_pgysypy49ph1mjldy_og.png]
   average match between training ground state and network prediction
   [1*lpyk1riq9sh2qt8o8gjcmq.png]
   the worst match i found between training ground state and network
   prediction
   [1*e9hmdm0xcwuj8b99thtm7w.png]
   good match between validation ground state and network prediction
   [1*obqdcvkbdl1uacrudq8fda.png]
   average match between validation ground state and network prediction
   [1*jn-cwb7npedbhoixu1wgxw.png]
   the worst match i found between validation ground state and network
   prediction

   speaking with scientific precision, these plots look pretty sweet. even
   the bad matches approximate the correct features of the actual ground
   state. this is encouraging! a later version of this network is likely
   to produce even better results. we   ll summarize some methods to improve
   the next generation neural network shortly.

   but first, let   s look at the innards of the network. because the
   initial weights of the network are randomly assigned, there   s no real
   correlation between the spatial locations of a particular input and the
   nodes that it feeds. therefore, before plotting the weights, we
   rearrange the hidden layers so that the highest weight from a given
   input is associated with the closest neuron (where possible). after
   sorting both layers (and moving the weights and biases to the
   appropriate locations         this manipulation is possible because the
   weights are [18]tensors rather than matrices), we plot the weights and
   biases in greyscale. each row represents the weight of a single input;
   each column contains the weights feeding a single output node. the
   biases are single values for each output node; the images are stretched
   vertically for easier visualization.
   [1*xa-zslkikuu_xkgulqly1a.png]
   weights from the potential function to the first hidden layer
   [1*hpmwurlrp0mcqsyosqsyug.png]
   biases for the first hidden layer
   [1*gel3lgywpxu_oge4ytdloa.png]
   weights from the first hidden layer to the second
   [1*ezmoiihde8e-salolfgw2q.png]
   biases for the second hidden layer
   [1*feyomp8roogtdr9plwktzg.png]
   weights from the second hidden layer to the output (predicted
   ground state)
   [1*bfjd_a5li63uim4kkwwwhq.png]
   biases for the output

   the weights for the first layer are the easiest to interpret. the rows
   near the top and bottom are mostly grey, corresponding to little
   importance of the potential function near the walls. this makes sense
   physically         the ground state goes to zero at the edges anyway,
   regardless of what the potential is doing there. some columns have
   distinct light and dark bands         this means the network is performing
   periodic sampling on the potential. in other words, it has decided
   (some components of) the fourier transform of the potential is worth
   investigating.

   the second and third layers are a bit more difficult to understand at a
   glance. better analysis tools could be applied here. still, some
   structure is visible in both layers. the biases aren   t particularly
   illuminating. you could call them schr  dinger barcodes, if you like.

   this exercise was primarily intended as a proof of principle, and it
   worked almost unreasonably well. possible improvements include:
    1. more hidden layers
    2. more nodes per layer
    3. pre-processing the potential: computing the fourier transform,
       binning together nearby potential values, picking out the minimum
       potential value, etc         essentially, applying techniques from
       convolutional neural networks
    4. use a more sensitive tool than the naked eye to search for patterns
       within the weights
    5. observe how the patterns found in #4 change as 1   3 are varied

   improvements 1   3 will provide more accurate solutions for the ground
   state. however, this problem is already solvable by more precise
   techniques. on the other hand, 4 and 5 are critical for the purpose of
   gaining deeper physics knowledge from the neural network. this is where
   the most energy should be expended going forward         several quanta, at
   least.

   iframe: [19]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=f9f6950a7c0e

   [20][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [21][1*v-ppfkswhbvlwwamsvhhwg.png]
   [22][1*wt2auqisieaozxj-i7brdq.png]

     * [23]neural networks
     * [24]quantum physics
     * [25]quantum mechanics
     * [26]deep learning
     * [27]artificial intelligence

   (button)
   (button)
   (button) 211 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of steven steinke

[29]steven steinke

   quantum physicist by training. deep learning enthusiast. lover of
   learning and teaching.

     (button) follow
   [30]becoming human: artificial intelligence magazine

[31]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 211
     * (button)
     *
     *

   [32]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [33]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f9f6950a7c0e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/solving-schr%c3%b6dingers-equation-with-deep-learning-f9f6950a7c0e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/solving-schr%c3%b6dingers-equation-with-deep-learning-f9f6950a7c0e&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_zbdjnne6momx---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@quantumsteinke?source=post_header_lockup
  15. https://becominghuman.ai/@quantumsteinke
  16. https://github.com/sksteinke/schroedinger-deeplearning
  17. https://www.tensorflow.org/api_docs/python/tf/nn/softplus
  18. https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c
  19. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=f9f6950a7c0e
  20. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  21. https://upscri.be/8f5f8b
  22. https://becominghuman.ai/write-for-us-48270209de63
  23. https://becominghuman.ai/tagged/neural-networks?source=post
  24. https://becominghuman.ai/tagged/quantum-physics?source=post
  25. https://becominghuman.ai/tagged/quantum-mechanics?source=post
  26. https://becominghuman.ai/tagged/deep-learning?source=post
  27. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  28. https://becominghuman.ai/@quantumsteinke?source=footer_card
  29. https://becominghuman.ai/@quantumsteinke
  30. https://becominghuman.ai/?source=footer_card
  31. https://becominghuman.ai/?source=footer_card
  32. https://becominghuman.ai/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/f9f6950a7c0e/share/twitter
  36. https://medium.com/p/f9f6950a7c0e/share/facebook
  37. https://medium.com/p/f9f6950a7c0e/share/twitter
  38. https://medium.com/p/f9f6950a7c0e/share/facebook
