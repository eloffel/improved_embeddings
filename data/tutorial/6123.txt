   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    linearly separable? no? for me it is! a brief
   introduction to kernel methods comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

linearly separable? no? for me it is! a brief introduction to kernel methods

   [25]09/28/201709/30/2017[26]artificial intelligence, [27]generic,
   [28]machine learning, [29]machine learning algorithms addenda,
   [30]python, [31]scikit-learn[32]2 comments

   this is a crash-introduction to kernel methods and the best thing to do
   is starting with a very simple question? is this bidimensional set
   linearly separable?

   of course, the answer is yes, it is. why? a dataset defined in a
   subspace           ^n is linearly separable if there exists a
   (n-1)-dimensional hypersurface that is able to separate all points
   belonging to a class from the others. let   s consider the problem from
   another viewpoint, supposing, for simplicity, to work in 2d.

   we have defined an hypothetical separating line and we have also set an
   arbitrary point o as an origin. let   s now draw the vector w, orthogonal
   to the line and pointing in one of the two sub-spaces. let   s now
   consider the inner product between w and a random point x[0]:  how can
   we decide if it   s on the side pointed by w? simple, the inner product
   is proportional to the cosine of the angle between the two vectors. if
   such an angle is between -  /2 and   /2, the cosine is non-negative,
   otherwise is non-positive:

   it   s to prove that all angles    between w and green dots are always
   bounded between -  /2 and   /2, therefore, all inner products are
   positive (there no points on the line, where the cosine is 0). at the
   same time, all    angles are bounded between   /2 and 3  /2 where the
   cosine is negative.

   now it should be obvious, that w is the weight vector of a    pure   
   linear classifier (like a id88) and the corresponding decision
   rule is simply:

   the way to determine w changes according to the algorithm and so the
   decision rule, which, however is always a function strictly related to
   the angle between w and the sample.

   let   s now consider another famous example:

   is this dataset linearly separable? you   re free to try, but the answer
   is no. for every line you draw, there will be always red and green
   points on the same side, so that the accuracy can never overcome a
   fixed threshold (quite low). now, let   s suppose to project the original
   dataset into a m-dimensional feature space (m can also be equal to n):

   after the transformation, the original half-moon dataset, becomes
   linearly separable! however, there   s still a problem: we need a
   classifier working in the original space, where we must compute an
   inner product of transformed points. let   s analyze the problem using
   the id166 (support vector machines), where the kernels are very diffused.
   a [33]id166 is an algorithm that tries to maximize the separation between
   high-density blocks. i   m not covering all details (which can be found
   in every machine learning book), but i need to define the expression
   for the weights:

   it   s not important if you don   t know how they are derived. it   s simpler
   to understand the final result: if you reconsider the image shown
   before where we had drawn a separating line, all green points have y=1,
   therefore w is the weighted sum of all samples. the values of   [i] are
   obtained through the optimization process, however, at the end, we
   should observe a w similar to what we have arbitrarily drawn.

   if we need to move on the feature space, every x, must be filtered
   by   (x), therefore, we get:

   and the corresponding decision function becomes:

   as the sum is extended to all samples, we need to compute n inner
   products of transformed samples. this isn   t a properly fast solution!
   however, there are particular functions called kernels, which have a
   very nice property:

   in other words, the kernel, evaluated at x[i], x[j] is equal to the
   inner product of the two points projected by a function   (x). pay
   attention:    is another function because, if we start from the
   projection, we couldn   t be able to find a kernel, while if we start
   from the kernel, there is also a famous mathematical result (the
   [34]mercer   s theorem) that guarantees, given a continuous, symmetric a
   (semi-)positive definite kernel, the existence of an implicit
   transformation   (x)  under some (easily met) conditions.

   it   s not difficult to understand, that using a kernel, the decision
   function becomes:

   which is much easier to evaluate.

   from a theoretical viewpoint we could try to find a kernel for a
   specific transformation, but in the real-life, only some kernels
   resulted to be really useful in many different contexts. for example,
   the radial basis function:

   which is excellent whenever we need to remap the original space into a
   radial one. or the polynomial kernel:

   which determines all the polynomial term-features (like x^2 or 2
   x[i]x[j]). there are also other kernels, like sigmoid or tanh, but it   s
   always a good idea to start with a linear classifier, if the accuracy
   is too low, it   s possible to try a rbf or polynomial kernel, then other
   variants, and, if nothing works, it   s probably necessary to change
   model!

   kernels are not limited to classifiers: they can be employed in pca
   (kernel pca allows to extract principal components from non-linear
   datasets and instance-based-learning algorithms. the principle is
   always the same (and it   s the kernel silver bullet: it represents the
   inner product of non-linear projections that can remap the original
   dataset in a higher dimensional space where it   s easier to find a
   linear solution).

   see also:

[35]assessing id91 optimality with instability index     giuseppe
bonaccorso

     many id91 algorithms need to define the number of desired
     clusters before fitting the model. this requirement can appear as a
     contradiction in an unsupervised scenario, however, in many
     real-word scenarios, the data scientist has often already an idea
     about a reasonable range of clusters.

share:

     * [36]click to share on twitter (opens in new window)
     * [37]click to share on facebook (opens in new window)
     * [38]click to share on linkedin (opens in new window)
     * [39]click to share on pocket (opens in new window)
     * [40]click to share on tumblr (opens in new window)
     * [41]click to share on reddit (opens in new window)
     * [42]click to share on pinterest (opens in new window)
     * [43]click to share on skype (opens in new window)
     * [44]click to share on whatsapp (opens in new window)
     * [45]click to share on telegram (opens in new window)
     * [46]click to email this to a friend (opens in new window)
     * [47]click to print (opens in new window)
     *

you can also be interested in these articles:

   [48]kernel[49]linear[50]machine
   learning[51]non-linear[52]pca[53]scikit-learn[54]id166

post navigation

   [55]pca with rubner-tavan networks
   [56]a virtual jacques lacan discusses about artificial intelligence

2 thoughts on    linearly separable? no? for me it is! a brief introduction to
kernel methods   

    1. a m
       09/28/2017 at 23:41
          if such an angle is between -  /2 and   /2, the cosine is
       non-negative, otherwise is non-positive   
       honestly, why do that? what is wrong with saying,    the cosine is
       positive, otherwise it is negative.    this tendency to sacrifice
       clarity for unnecessary complication is so bewildering to me in
       articles like this. your post is great and very helpful, aiming to
       be a simple introduction to a very complex topic for many. be
       confident that you are explaining it well. you don   t need to sound
       like other pretentious and write ridiculous things like that.
       (really, your post was great otherwise!)
       [57]reply
          + [58]giuseppe bonaccorso
            09/29/2017 at 9:35
            i agree with you when there   s an extra (useless complexity),
            but, non-negative doesn   t mean positive as well as
            non-positive doesn   t mean negative. non-negative/positive
            means >= 0 (or <= 0). if i had written positive and negative,
            what about the points lying on the line (where the cosine is
            null)? unfortunately, it can seem conter-intuitive, but this
            is the correct mathematical terminology.
            thank you for the comment!
            [59]reply

leave a reply [60]cancel reply

   iframe: [61]jetpack_remote_comment

follow me

     * [62]linkedin
     * [63]twitter
     * [64]facebook
     * [65]github
     * [66]instagram
     * [67]amazon
     * [68]medium
     * [69]rss

search articles

   ____________________ (button)

latest blog posts

     * [70]machine learning algorithms     second edition 08/28/2018
     * [71]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [72]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [73]a book that every data scientist should read 06/22/2018
     * [74]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [75]my tweets

   copyright    2019 [76]giuseppe bonaccorso. all rights reserved.
   [77]privacy policy - [78]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [79]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/generic/
  28. https://www.bonaccorso.eu/category/machine-learning/
  29. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
  30. https://www.bonaccorso.eu/category/software/python/
  31. https://www.bonaccorso.eu/category/machine-learning/scikit-learn/
  32. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#comments
  33. https://en.wikipedia.org/wiki/support_vector_machine
  34. https://en.wikipedia.org/wiki/mercer's_theorem
  35. https://www.bonaccorso.eu/2017/08/03/assessing-id91-optimality-instability-index/
  36. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=twitter
  37. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=facebook
  38. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=linkedin
  39. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=pocket
  40. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=tumblr
  41. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=reddit
  42. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=pinterest
  43. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=skype
  44. https://api.whatsapp.com/send?text=linearly separable? no? for me it is! a brief introduction to kernel methods https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/
  45. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=telegram
  46. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/?share=email
  47. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#print
  48. https://www.bonaccorso.eu/tag/kernel/
  49. https://www.bonaccorso.eu/tag/linear/
  50. https://www.bonaccorso.eu/tag/machine-learning/
  51. https://www.bonaccorso.eu/tag/non-linear/
  52. https://www.bonaccorso.eu/tag/pca/
  53. https://www.bonaccorso.eu/tag/scikit-learn/
  54. https://www.bonaccorso.eu/tag/id166/
  55. https://www.bonaccorso.eu/2017/09/25/pca-rubner-tavan-networks/
  56. https://www.bonaccorso.eu/2017/10/01/virtual-jacques-lacan-discusses-artificial-intelligence/
  57. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#comment-37
  58. https://www.bonaccorso.eu/
  59. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#comment-39
  60. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#respond
  61. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1488&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=00c832cfe283b39b180fa7d811795938b6517de4#parent=https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/
  62. https://www.linkedin.com/in/giuseppebonaccorso/
  63. https://twitter.com/giuseppeb/
  64. https://www.facebook.com/giuseppe.bonaccorso/
  65. https://github.com/giuseppebonaccorso/
  66. https://www.instagram.com/giuseppebonaccorso/
  67. https://www.amazon.com/author/giuseppebonaccorso
  68. https://medium.com/@giuseppe.bonaccorso
  69. https://www.bonaccorso.eu/feed/
  70. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  71. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  72. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  73. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  74. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  75. https://twitter.com/giuseppeb
  76. https://www.bonaccorso.eu/
  77. https://www.iubenda.com/privacy-policy/331721
  78. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  79. https://www.bonaccorso.eu/2017/09/28/linearly-separable-no-for-me-it-is-a-brief-introduction-to-kernel-methods/#cancel

   hidden links:
  81. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
