    #[1]john sullivan feed

     * [2]john sullivan
     * [3]about
     * [4]data science

   neural network from scratch: id88 linear classifier

   john sullivan

john sullivan

   data science portfolio
   (button) follow
     * boston, ma
     * [5]email
     * [6]linkedin
     * [7]github

neural network from scratch: id88 linear classifier

   13 minute read

   python code: [8]neural network from scratch

   the single-layer [9]id88 is the simplest of the artificial neural
   networks (anns). it was developed by american psychologist [10]frank
   rosenblatt in the 1950s.

   like id28, the id88 is a linear classifier used
   for binary predictions. this means that in order for it to work, the
   data must be [11]linearly separable.

   png

   although the id88 is only applicable to linearly separable data,
   the more detailed [12]multilayered id88 can be applied to more
   complicated nonlinear datasets. this includes applications in areas
   such as id103, image processing, and financial predictions
   just to name a few.

how it works

   the id88 works by    learning    a series of weights, corresponding
   to the input features. these input features are vectors of the
   available data. for example, if we were trying to classify whether an
   animal is a cat or dog, might be weight, might be height, and might be
   length. each pair of weights and input features is multiplied together,
   and then the results are summed. if the summation is above a certain
   threshold, we predict one class, otherwise the prediction belongs to a
   different class. for example, we could set the threshold at . if the
   summation is greater than , the prediction is a (dog), otherwise it   s a
   (cat).

   the final step is to check if our predictions were classified
   correctly. if they were not, then the weights are updated using a
   learning rate. this process continues for a certain number of
   iterations, known as    epochs.    the goal is to determine the weights
   that produce a linear decision boundary that correctly classifies the
   predictions.

id88 algorithm

   the id88 is pretty straightforward. here   s the basics:
    1. initialize the weight vector , set a threshold for the activation
       function, number of time steps for computation, and a learning rate
       .
    2. calculate the output at the first iteration for the first training
       sample for the features:
       \begin{align} \hat{y}^1_1 &=\begin{cases} 1 & \text{if }f \gt z\\ 0
       & \text{otherwise}. \end{cases} \end{align}
    3. update the weights
    4. increment the time-step to . if the final time-step hasn   t been
       reached, go back to step 2., repeating the process for the next
       training sample .

   it should be noted that this isn   t exactly identical to rosenblatt   s
   original id88, which used the for activation.

   \begin{align} sgn(f) =\begin{cases} 1 & \text{if }f \gt 0\\ -1 &
   \text{if }f \lt 0. \end{cases} \end{align}

   the form we   ll be implementing allows us some flexibility in choosing a
   threshold for our activation function. it also allows us to implement
   and for the outputs, which is typical for binary classification.

stochastic id119

   it   s interesting to note that the id88 is identical to stochastic
   id119 (sgd) on the following sum-of-squared error (sse) loss
   function where is the true output, is the predicted output, and is the
   weight:

   we start by approximating the gradient at a single point using sgd:

   as with the id88, is the learning rate, is a training sample, and
   is a given iteration. we can reshape this by taking the partial
   derivative of the id168 at a particular training sample with
   respect to .

   now we can plug this back into the original sgd equation.

   if we plug in for the first iteration and for the training sample, then
   the form is identical to the id88 in the previous section.

   depending on the id168 used, sgd can take on many other forms.
   for additional details about sgd using other id168s and
   variations, [13]here is a good resource.

step-by-step example

   a good way to understand exactly how the id88 works is to walk
   through a simple example. i   m going to use a [14]nand gate model for my
   example, which has a very small linearly separable dataset. given the
   two features and , here   s what the outputs are for the nand gate:
   0 0 1
   0 1 1
   1 0 1
   1 1 0

   if i break the features and output into column vectors, here   s what
   they look like:

   i   m also going to introduce a dummy feature of ones, which will be used
   to calculate the [15]bias term in the model. the bias is an additional
   term that allows for flexibility in fitting the model. it allows all
   input features to be , while still being able to fit the model.

   now i   m going to start working through the algorithm outlined above,
   step-by-step.
    1. i   m going to start by initializing the weight vector to zeros.
       i   m going to set the threshold , the number of time steps to run
       the algorithm , and the learning rate . next i   m going to hop in to
       step 2. where the iterations begin.
    2. calculate the output at the first iteration for the first training
       sample .
       now i   ll make the actual prediction using my activation function.
       \begin{align} \hat{y}^1_1 &=\begin{cases} 1 & \text{if }f \gt z\\ 0
       & \text{otherwise}. \end{cases} \end{align}
       \begin{align} \hat{y}^1_1 = 0 \end{align}
    3. update the weights
    4. increment the time step to , therefore . since the time step is not
       equal to 50, i jump back to step 2. and keep going.

   since there are four training samples, after going through four
   iterations i can evaluate the sse id168 to see if the model has
   converged. looping through all of the training samples is also known as
   an    epoch.    since my dataset has four training samples, there   s four
   iterations in one epoch. i won   t write out all of the iterations, but
   you can find a spreadsheet where i did this by hand [16]here. based on
   my settings for threshold and learning rate, you should find that the
   model has converged after sixteen iterations, or four epochs, with
   these final weights:

python implementation from scratch

   in this section i   m going to implement the nand model in python. this
   makes the id88 accessible to larger more complex datasets that a
   spreadsheet may not be able to handle.

   i   m going to be taking advantage of the [17]numpy library to simplify
   some of the math operations, and the [18]matplotlib library for
   plotting.
    import numpy as np
    import matplotlib.pyplot as plt

   next i   ll enter in my feature data and the outputs.
    # nand gate features
    # note: x0 is a dummy variable for the bias term
    #     x0  x1  x2
    x = [[1., 0., 0.],
         [1., 0., 1.],
         [1., 1., 0.],
         [1., 1., 1.]]

    # desired outputs
    y = [1.,
         1.,
         1.,
         0.]

   here is the full implementation of the id88 function, where is
   the threshold, is the learning rate, and is the number of iterations.
    # training the id88
    #
    # x:   feature data
    # y:   outputs
    # z:   threshold
    # eta: learning rate
    # t:   number of iterations

    def id88_train(x, y, z, eta, t):

        # initializing parameters for the id88
        w = np.zeros(len(x[0]))        # weights
        n = 0

        # initializing additional parameters to compute sse
        yhat_vec = np.ones(len(y))     # vector for predictions
        errors = np.ones(len(y))       # vector for errors (actual - predictions
)
        j = []                         # vector for the sse cost function

        while n < t:
            for i in xrange(0, len(x)):

                # summation step
                f = np.dot(x[i], w)

                # activation function
                if f > z:
                    yhat = 1.
                else:
                    yhat = 0.
                yhat_vec[i] = yhat

                # updating the weights
                for j in xrange(0, len(w)):
                    w[j] = w[j] + eta*(y[i]-yhat)*x[i][j]

                n += 1

            # computing the sum-of-squared errors
            for i in xrange(0,len(y)):
               errors[i] = (y[i]-yhat_vec[i])**2
            j.append(0.5*np.sum(errors))

        # function returns the weight vector, and sum-of-squared errors
        return w, j

    z = 0.0     # threshold
    eta = 0.1   # learning rate
    t = 50      # number of iterations

    print "the weights are:"
    print id88_train(x, y, z, eta, t)[0], "\n"

    print "the sum-of-squared erros are:"
    print id88_train(x, y, z, eta, t)[1]

    the weights are:
    [ 0.2 -0.2 -0.1]

    the sum-of-squared erros are:
    [1.0, 1.5, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

   the weights are identical to those from the spreadsheet calculations.
   another useful tool is to plot the error as a function of the epochs.
    j = id88_train(x, y, z, eta, t)[1]     # pulling out the sum-of-square
d errors from the tuple
    epoch = np.linspace(1,len(j),len(j))

    %matplotlib inline
    plt.plot(epoch, j)
    plt.xlabel('epoch')
    plt.ylabel('sum-of-squared error')
    plt.title('id88 convergence')

   png

   i can see from the results that by the fourth epoch the results have
   converged. this is also consistent with the results from the
   spreadsheet.

   i set the threshold and weights arbitrarily, so these are things that i
   can investigate. if i change the learning rate, say to , the weights
   and number of epochs to convergence change.
    eta = 0.5     # new learning rate
    z = 0.0

    print "the weights are:"
    print id88_train(x, y, z, eta, t)[0], "\n"

    j = id88_train(x, y, z, eta, t)[1]
    epoch = np.linspace(1,len(j),len(j))
    print "the sum-of-squared erros are:"
    print j

    %matplotlib inline
    plt.plot(epoch, j)
    plt.xlabel('epoch')
    plt.ylabel('sum-of-squared error')
    plt.title('id88 convergence')

    the weights are:
    [ 1.5 -1.  -0.5]

    the sum-of-squared erros are:
    [1.0, 1.5, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

   png

   i can also investigate the threshold. here is what the results look
   like using the original learning rate of , and changing the threshold
   to .
    eta = 0.1
    z = 0.5     # new threshold

    print "the weights are:"
    print id88_train(x, y, z, eta, t)[0], "\n"

    j = id88_train(x, y, z, eta, t)[1]
    epoch = np.linspace(1,len(j),len(j))
    print "the sum-of-squared erros are:"
    print j

    %matplotlib inline
    plt.plot(epoch, j)
    plt.xlabel('epoch')
    plt.ylabel('sum-of-squared error')
    plt.title('id88 convergence')

    the weights are:
    [ 0.8 -0.2 -0.1]

    the sum-of-squared erros are:
    [1.5, 1.5, 1.5, 1.0, 1.5, 1.5, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]

   png

   this model performed slightly worse than the previous two, so maybe i
   don   t want to use these parameters.

   it   s important to keep in mind that i   m only using a training set in
   this example. i haven   t actually made predictions on a test set hidden
   from the model. training and making predictions on the same dataset
   isn   t a good idea, as it can lead to overfitting. in the next section
   i   ll work through another example where i split the data into a
   training set and a test set.

a more detailed example

   in this section i   m going to use a larger data set to train/test my
   id88. i   ll also compare my results with an implementation from
   [19]scikit-learn as a validation of my model.

synthetic dataset

   in order to test my id88 for comparison with the scikit-learn
   implementation, i   m going to first create a dataset. since i   ll be
   plotting the intercept, i   m going to put in a dummy feature in the
   first column, which will be ones. this way the model will learn the
   weights for the features, as well as the bias term for the intercept.
    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline

    # setting the random seed to reproduce results
    np.random.seed(5)

    # number of observations
    obs = 1000

    # generating synthetic data from multivariate normal distribution
    class_zeros = np.random.multivariate_normal([0,0], [[1.,.95],[.95,1.]], obs)
    class_ones = np.random.multivariate_normal([1,5], [[1.,.85],[.85,1.]], obs)

    # generating a column of ones as a dummy feature to create an intercept
    intercept = np.ones((2*obs,1))

    # vertically stacking the two classes
    features = np.vstack((class_zeros, class_ones)).astype(np.float32)

    # putting in the dummy feature column
    features = np.hstack((intercept, features))

    # creating the labels for the two classes
    label_zeros = np.zeros((obs,1))
    label_ones = np.ones((obs,1))

    # stacking the labels, and then adding them to the dataset
    labels = np.vstack((label_zeros,label_ones))
    dataset = np.hstack((features,labels))

    # scatter plot to visualize the two classes (red=1, blue=0)
    plt.scatter(features[:,1], features[:,2], c = labels)

   png

splitting the dataset

   i didn   t split the data in the nand example into training and test sets
   because i was just illustrating a simple example of the id88
   algorithm. in this example i   m going to randomly sample 70% of the
   dataset for the training set, and predictions will be made on the
   remaining 30%. splitting the dataset into training and test sets is
   good practice to try and avoid overfitting.
    # shuffling the data to make the sampling random
    np.random.shuffle(dataset)

    # splitting the data into train/test sets
    train = dataset[0:int(0.7*(obs*2))]
    test = dataset[int(0.7*(obs*2)):(obs*2)]

training the model

   the next step is to train the model to determine the weights.
    # training the id88
    #
    # inputs
    # x:   feature data
    # y:   outputs
    # z:   threshold
    # eta: learning rate
    # t:   number of iterations

    # reshaping the data for the function
    x_train = train[:,0:3]
    y_train = train[:,3]

    x_test = test[:,0:3]
    y_test = test[:,3]

    def id88_train(x, y, z, eta, t):

        # initializing parameters for the id88
        w = np.zeros(len(x[0]))        # initial weights
        n = 0

        # initializing additional parameters to compute sum-of-squared errors
        yhat_vec = np.ones(len(y))     # vector for predictions
        errors = np.ones(len(y))       # vector for errors (actual - predictions
)
        j = []                         # vector for the sse cost function

        while n < t:
            for i in xrange(0, len(x)):

                # summation step
                f = np.dot(x[i], w)

                # activation function
                if f >= z:
                    yhat = 1.
                else:
                    yhat = 0.
                yhat_vec[i] = yhat

                # updating the weights
                for j in xrange(0, len(w)):
                    w[j] = w[j] + eta*(y[i]-yhat)*x[i][j]

            n += 1

            # computing the sum-of-squared errors
            for i in xrange(0,len(y)):
               errors[i] = (y[i]-yhat_vec[i])**2
            j.append(0.5*np.sum(errors))

        return w, j


    z = 0.0     # threshold
    eta = 0.1   # learning rate
    t = 5       # number of iterations

    id88_train(x_train, y_train, z, eta, t)

    w = id88_train(x_train, y_train, z, eta, t)[0]
    j = id88_train(x_train, y_train, z, eta, t)[1]
    epoch = np.linspace(1,len(j),len(j))

    print "the weights are:"
    print w
    print "the sum-of-squared errors are:"
    print j

    the weights are:
    [-0.7        -0.43283606  0.42203522]
    the sum-of-squared errors are:
    [5.5, 0.0, 0.0, 0.0, 0.0]

   a plot of the model   s convergence is also useful.
    # plotting sse as a function of epoch
    j = id88_train(x_train, y_train, z, eta, t)[1]
    epoch = np.linspace(1,len(j),len(j))

    plt.figure(1)
    plt.plot(epoch, j)
    plt.xlabel('epoch')
    plt.ylabel('sum-of-squared error')
    plt.title('id88 convergence')

   png

testing the model

   next, we   ll be testing the model.
    from sklearn.metrics import accuracy_score

    w = id88_train(x_train, y_train, z, eta, t)[0]

    def id88_test(x, w, z, eta, t):
        y_pred = []
        for i in xrange(0, len(x-1)):
            f = np.dot(x[i], w)

                # activation function
            if f > z:
                yhat = 1
            else:
                yhat = 0
            y_pred.append(yhat)
        return y_pred

    y_pred = id88_test(x_test, w, z, eta, t)

    print accuracy_score(y_test, y_pred)

    1.0

   we can see that the model performed perfectly. since this is a pretty
   simple, linearly separable dataset, this isn   t surprising. lets take a
   look at what the decision boundary looks like for this model.
    # plot the decision boundary
    # 0 = w0x0 + w1x1 + w2x2
    # x2 = (-w0x0-w1x1)/w2

    min = np.min(x_test[:,1])
    max = np.max(x_test[:,1])
    x1 = np.linspace(min,max,100)

    def x2(x1, w):
        w0 = w[0]
        w1 = w[1]
        w2 = w[2]
        x2 = []
        for i in xrange(0, len(x1-1)):
            x2_temp = (-w0-w1*x1[i])/w2
            x2.append(x2_temp)
        return x2

    x_2 = np.asarray(x2(x1,w))

    plt.scatter(features[:,1], features[:,2], c = labels)
    plt.plot(x1, x_2)

   png

   the plot of the decision boundary confirms that the model has clearly
   separated the two classes.

scikit-learn model

   based on the decision boundary, it looks like the model is working.
   another good check is to verify it with a trusted implementation from
   [20]scikit-learn. to compare the models, i   ll take a look at the
   weights for each model.
    from sklearn.linear_model import id88

    # training the sklearn id88
    clf = id88(random_state=none, eta0= 0.1, shuffle=false, fit_intercept=
false)
    clf.fit(x_train, y_train)
    y_predict = clf.predict(x_test)

    print "sklearn weights:"
    print clf.coef_[0]

    print "my id88 weights:"
    print w

    sklearn weights:
    [-0.7        -0.43283606  0.42203522]
    my id88 weights:
    [-0.7        -0.43283606  0.42203522]

   the scikit-learn implementation yielded identical weights to my model.
   this isn   t surprising given the clear separability of the two datasets.

   i did have to manipulate a few details in the scikit-learn model
   though. i turned off the random state and the shuffle option so that
   the scikit-learn id88 would use the same random seed that i set
   for my model. i also set the learning rate to the same number as my
   id88. finally, i turned of the fit_intercept option. since i
   included the dummy column of ones in the dataset, i   m automatically
   fitting the intercept, so i don   t need this option turned on.

conclusion

   in this article i built a id88 model from scratch in python.
   compared to the scikit-learn implementation, my model yielded identical
   results. in a real life problem would i use my model over something
   from scikit-learn? probably not. my model is not nearly as optimized or
   robust as the scikit-learn implementation. the real value in building a
   model from scratch is gaining a deeper understanding of how an
   algorithm actually works. understanding the underlying assumptions and
   limitations is extremely important when applying a machine learning
   model.

   tags: [21]machine learning, [22]mathematics

   updated: august 16, 2017

share on

   [23]twitter [24]facebook [25]google+ [26]linkedin

   [27]previous [28]next

you may also enjoy

[29]churn prediction: id28 and id79

   7 minute read

   churn prediction, r, id28, id79, auc,
   cross-validation

[30]exploratory data analysis with r: customer churn

   12 minute read

   exploratory data analysis, data wrangling, ggplot2, dplyr

[31]stochastic calculus with python: simulating stock price dynamics

   12 minute read

   quantitative finance, stochastic calculus, geometric brownian motion

     * follow:
     * [32]github
     * [33]feed

      2018 john sullivan. powered by [34]jekyll & [35]minimal mistakes.

references

   1. https://jtsulliv.github.io/feed.xml
   2. https://jtsulliv.github.io/
   3. https://jtsulliv.github.io/about/
   4. https://jtsulliv.github.io/datascience/
   5. mailto:jtsulliv@gmail.com
   6. https://www.linkedin.com/in/jonathan-sullivan-b6761727
   7. https://github.com/jtsulliv
   8. https://github.com/jtsulliv/ml-from-scratch/tree/master/neural-networks
   9. https://en.wikipedia.org/wiki/id88
  10. https://en.wikipedia.org/wiki/frank_rosenblatt
  11. https://en.wikipedia.org/wiki/linear_separability
  12. https://en.wikipedia.org/wiki/multilayer_id88
  13. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  14. https://en.wikipedia.org/wiki/nand_gate
  15. https://stats.stackexchange.com/questions/185911/why-are-bias-nodes-used-in-neural-networks
  16. https://github.com/jtsulliv/ml-from-scratch/tree/master/neural-networks
  17. http://www.numpy.org/
  18. https://matplotlib.org/
  19. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.id88.html
  20. http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.id88.html
  21. https://jtsulliv.github.io/tags/#machine-learning
  22. https://jtsulliv.github.io/tags/#mathematics
  23. https://twitter.com/intent/tweet?text=https://jtsulliv.github.io/id88/
  24. https://www.facebook.com/sharer/sharer.php?u=https://jtsulliv.github.io/id88/
  25. https://plus.google.com/share?url=https://jtsulliv.github.io/id88/
  26. https://www.linkedin.com/sharearticle?mini=true&url=https://jtsulliv.github.io/id88/
  27. https://jtsulliv.github.io/id88/
  28. https://jtsulliv.github.io/stock-movement/
  29. https://jtsulliv.github.io/churn-prediction/
  30. https://jtsulliv.github.io/churn-eda/
  31. https://jtsulliv.github.io/stock-movement/
  32. http://github.com/jtsulliv
  33. https://jtsulliv.github.io/feed.xml
  34. http://jekyllrb.com/
  35. https://mademistakes.com/work/minimal-mistakes-jekyll-theme/
