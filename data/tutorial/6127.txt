   [1][visionlablogo.png] [2][stanfordlogo.jpg]

a hierarchical approach for generating descriptive image paragraphs

abstract

   recent progress on image captioning has made it possible to generate
   novel sentences describing images in natural language, but compressing
   an image into a single sentence can describe visual content in only
   coarse detail. while one new captioning approach, dense captioning, can
   potentially describe images in finer levels of detail by captioning
   many regions within an image, it in turn is unable to produce a
   coherent story for an image. in this paper we overcome these
   limitations by generating entire paragraphs for describing images,
   which can tell detailed, unified stories. we develop a model that
   decomposes both images and paragraphs into their constituent parts,
   detecting semantic regions in images and using a hierarchical recurrent
   neural network to reason about language. linguistic analysis confirms
   the complexity of the paragraph generation task, and thorough
   experiments on a new dataset of image and paragraph pairs demonstrate
   the effectiveness of our approach.

   [3][paper.jpg]
   [photo_smaller.jpg]
   jonathan krause
   [me-v2.jpg]
   justin johnson
   [ranjay.png]
   ranjay krishna
   [feifei2.jpeg]
   fei-fei li
   to appear in cvpr 2017

dataset

   download the [4]dataset here.

   download the [5]training, [6]val and [7]test splits here.

paper

   download the [8]paper here.

bibtex

@inproceedings{krause2016paragraphs,
  title={a hierarchical approach for generating descriptive image paragraphs},
  author={krause, jonathan and johnson, justin and krishna, ranjay and fei-fei,
li},
  booktitle={id161 and patterm recognition (cvpr)},
  year={2017}
}

dataset

   [interconnected_images.png]
   with this paper, we are releasing a new dataset to allow researchers to
   benchmark their progress in generating paragraphs that tell a story
   about an image. the dataset contains 19,561 images from the [9]visual
   genome dataset. each image contains one paragraph. the
   training/val/test sets contains 14,575/2487/2489 images. we show in our
   paper that the paragraphs are more diverse than their corresponding
   sentences descriptions with more verbs, co-references and adjectives.
   since all the images are also part of the visual genome dataset, each
   image also contains 50 region descriptions (short phrases describing
   parts of an image), 35 objects, 26 attributes and 21 relationships and
   17 question-answer pairs.

model

   [model.png]
   we first decompose the input image by detecting objects and other
   regions of interest, then aggregate features across these regions to
   produce a pooled representation richly expressing the image semantics.
   this feature vector is taken as input by a hierarchical recurrent
   neural network composed of two levels: a sentence id56 and a word id56.
   the sentence id56 receives the image features, decides how many
   sentences to generate in the resulting paragraph, and produces an input
   topic vector for each sentence. given this topic vector, the word id56
   generates the words of a single sentence.

example results

   [examples.png]
   we compare our approach with numerous baselines, showcasing the
   benefits of hierarchical modeling for generating descriptive
   paragraphs. read our paper to learn more about our baselines
   (sentence-concat and template methods). we compare and contrast our
   methods with these baselines in terms of diversity, co-references
   (pronouns), verbs, sentence lengths and vocabulary size.

focused paragraph generation

   [regions2para.png]
   as an exploratory experiment in order to highlight the interpretability
   of our model, we investigate generating paragraphs from particular
   regions in the image. we show that the model generates paragraphs
   describing the detected regions without much mention of objects or
   scenery outside of the detections. taking the top-right image as an
   example, despite a few linguistic mistakes, the paragraph generated
   mentions the batter, catcher, dirt, and grass, which all appear in the
   top detected regions, but does not pay heed to the pitcher or the
   umpire in the background.

references

   visible links
   1. http://vision.stanford.edu/
   2. http://stanford.edu/
   3. https://arxiv.org/pdf/1611.06607.pdf
   4. http://visualgenome.org/static/data/dataset/paragraphs_v1.json.zip
   5. https://cs.stanford.edu/people/ranjaykrishna/im2p/train_split.json
   6. https://cs.stanford.edu/people/ranjaykrishna/im2p/val_split.json
   7. https://cs.stanford.edu/people/ranjaykrishna/im2p/test_split.json
   8. https://arxiv.org/pdf/1611.06607.pdf
   9. http://visualgenome.org/

   hidden links:
  11. http://ai.stanford.edu/~jkrause/
  12. http://cs.stanford.edu/people/jcjohns/
  13. http://ranjaykrishna.com/
  14. http://vision.stanford.edu/index.html
