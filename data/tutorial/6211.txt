   #[1]github [2]recent commits to germanwordembeddings:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]7
     * [35]star [36]128
     * [37]fork [38]28

[39]devmount/[40]germanwordembeddings

   [41]code [42]issues 0 [43]pull requests 0 [44]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [45]sign up
   toolkit to obtain and preprocess german corpora, train models using
   id97 (gensim) and evaluate them with generated testsets
   [46]https://devmount.github.io/germanword   
   [47]neural-network [48]id97 [49]word-embeddings [50]model
   [51]training [52]evaluation [53]deep-learning [54]deep-neural-networks
   [55]nlp [56]natural-language-processing [57]gensim [58]german-language
     * [59]137 commits
     * [60]2 branches
     * [61]0 releases
     * [62]fetching contributors
     * [63]mit

    1. [64]jupyter notebook 65.1%
    2. [65]python 34.1%
    3. [66]shell 0.8%

   (button) jupyter notebook python shell
   branch: master (button) new pull request
   [67]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/d
   [68]download zip

downloading...

   want to be notified of new releases in devmount/germanwordembeddings?
   [69]sign in [70]sign up

launching github desktop...

   if nothing happens, [71]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [72]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [73]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [74]download the github extension for visual studio
   and try again.

   (button) go back
   [75]@devmount
   [76]devmount [77]update readme.md
   latest commit [78]c79f6d1 jan 23, 2019
   [79]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [80]code [81]added: ipython notebook for evaluation jul 13, 2015
   [82]data [83]improved evaluation may 29, 2015
   [84]result
   [85]src [86]added: visualize script jun 23, 2015
   [87].gitignore [88]update readme jul 14, 2017
   [89]license
   [90]readme.md
   [91]wikiextractor.py
   [92]evaluation.py [93]updated: support binary files apr 18, 2018
   [94]preprocessing.py
   [95]requirements.txt [96]add script to visualize embedding with
   tensorboard jul 17, 2017
   [97]tfvisualize.py
   [98]training.py
   [99]visualize.py
   [100]vocabulary.py
   [101]id97_german.sh [102]improved: shell script aug 31, 2015

readme.md

[103]germanwordembeddings

   [104]license [105]downloads

   there has been a lot of research about the training of id27s
   on english corpora. this toolkit applies deep learning via
   [106]gensims's id97 on german corpora to train and evaluate german
   language models. an overview about the project, evaluation results and
   [107]download links can be found on the [108]project's website or
   directly in this repository.

   this project is released under the [109]mit license.
    1. [110]get started
    2. [111]obtaining corpora
    3. [112]preprocessing
    4. [113]training models
    5. [114]vocabulary
    6. [115]evaluation
    7. [116]download

get started

   make sure you have python 3 installed, as well as the following
   libraries:
pip install gensim nltk matplotlib numpy scipy scikit-learn

   now you can download [117]id97_german.sh and execute it in your
   shell to automatically download this toolkit and the corresponding
   corpus files and do the model training and evaluation. be aware that
   this could take a huge amount of time!

   you can also clone this repository and use my already [118]trained
   model to play around with the evaluation and visualization.

   if you just want to see how the different python scripts work, have a
   look into the [119]code directory to see jupyter notebook script output
   examples.

obtaining corpora

   there are multiple possibilities for obtaining huge german corpora that
   are publicly available and free to use:

german wikipedia

wget http://download.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles.xm
l.bz2

id151

   shuffled german news of the years 2007 to 2013:
for i in 2007 2008 2009 2010 2011 2012 2013; do
  wget http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.$i.de.sh
uffled.gz
done

   models trained with this toolkit are based on the german wikipedia and
   german news of 2013.

preprocessing

   this tool preprocesses the raw wikipedia xml corpus with the
   wikipediaextractor (a python script from giuseppe attardi to filter a
   wikipedia xml dump, licensed under gplv3) and some shell instructions
   to filter all xml tags and quotations:
wget http://medialab.di.unipi.it/project/semawiki/tools/wikiextractor.py
python wikiextractor.py -c -b 25m -o extracted dewiki-latest-pages-articles.xml.
bz2
find extracted -name '*bz2' \! -exec bzip2 -k -c -d {} \; > dewiki.xml
sed -i 's/<[^>]*>//g' dewiki.xml
sed -i 's|["'\''            ]||g' dewiki.xml
rm -rf extracted

   the german news already contain one sentence per line and don't have
   any xml syntax overhead. only quotation should to be removed:
for i in 2007 2008 2009 2010 2011 2012 2013; do
  gzip -d news.$i.de.shuffled.gz
  sed -i 's|["'\''            ]||g' news.$i.de.shuffled
done

   afterwards, the [120]preprocessing.py script can be called on these
   corpus files with the following options:
   flag default description
   -h, --help - show a help message and exit
   -p, --punctuation false filter punctuation tokens
   -s, --stopwords false filter stop word tokens
   -u, --umlauts false replace german umlauts with their respective
   digraphs
   -b, --bigram false detect and process common bigram phrases
   -t [ ], --threads [ ] number_of_processors number of worker threads
   --batch_size [ ] 32 batch size for sentence processing

   example usage:
python preprocessing.py dewiki.xml corpus/dewiki.corpus -psub
for file in *.shuffled; do python preprocessing.py $file corpus/$file.corpus -ps
ub; done

training models

   models are trained with the help of the [121]training.py script with
   the following options:
   flag default description
   -h, --help - show this help message and exit
   -s [ ], --size [ ] 100 dimension of word vectors
   -w [ ], --window [ ] 5 size of the sliding window
   -m [ ], --mincount [ ] 5 minimum number of occurences of a word to be
   considered
   -t [ ], --threads [ ] number_of_processors number of worker threads to
   train the model
   -g [ ], --sg [ ] 1 training algorithm: skip-gram (1), otherwise cbow
   (0)
   -i [ ], --hs [ ] 1 use of hierachical sampling for training
   -n [ ], --negative [ ] 0 use of negative sampling for training (usually
   between 5-20)
   -o [ ], --cbowmean [ ] 0 for cbow training algorithm: use sum (0) or
   mean (1) to merge context vectors

   example usage:
python training.py corpus/ my.model -s 200 -w 5

   mind that the first parameter is a directory and that every contained
   file will be taken as a corpus file for training.

   if the time needed to train the model should be measured and stored
   into the results file, this would be a possible command:
{ time python training.py corpus/ my.model -s 200 -w 5; } 2>> my.model.result

vocabulary

   to compute the vocabulary of a given corpus, the [122]vocabulary.py
   script can be used:
python vocabulary.py my.model my.model.vocab

evaluation

   to create test sets and evaluate trained models, the [123]evaluation.py
   script can be used. it's possible to evaluate both syntactic and
   semantic features of a trained model. for a successful creation of
   testsets, the following source files should be created before starting
   the script (see the configuration part in the script for more
   information).

syntactic test set

   with the syntactic test, features like singular, plural, 3rd person,
   past tense, comparative or superlative can be evaluated. therefore
   there are 3 source files: adjectives, nouns and verbs. every file
   contains a unique word with its conjugations per line, divided bei a
   dash. these combination patterns can be entered in the pattern_syn
   constant in the script configuration. the script now combinates each
   word with 5 random other words according to the given pattern, to
   create appropriate analogy questions. once the data file with the
   questions is created, it can be evaluated. normally the evaluation can
   be done by [124]gensim's id97 accuracy function, but to get a more
   specific evaluation result (correct matches, top n matches and
   coverage), this project uses it's own accuracy functions
   (test_mostsimilar_groups() and test_mostsimilar() in
   [125]evaluation.py).

   the given source files of this project contains 100 unique nouns with 2
   patterns, 100 unique adjectives with 6 patterns and 100 unique verbs
   with 12 patterns, resulting in 10k analogy questions. here are some
   examples for possible source files:

adjectives.txt

   possible pattern: basic-comparative-superlative

   example content:
gut-besser-beste
laut-lauter-lauteste

   see [126]src/adjectives.txt

nouns.txt

   possible pattern: singular-plural

   example content:
bild-bilder
name-namen

   see [127]src/nouns.txt

verbs.txt

   possible pattern:
   basic-1stpersonsingularpresent-2ndpersonpluralpresent-3rdpersonsingular
   past-3rdpersonpluralpast

   example content:
finden-finde-findet-fand-fanden
suchen-suche-sucht-suchte-suchten

   see [128]src/verbs.txt

semantic test set

   with the semantic test, features concering word meanings can be
   evaluated. therefore there are 3 source files: opposite, best match and
   doesn't match. the given source files result in a total of 950 semantic
   questions.

opposite.txt

   this file contains opposite words, following the pattern of
   oneword-oppositeword per line, to evaluate the models' ability to find
   opposites.. the script combinates each pair with 10 random other pairs,
   to build analogy questions. the given opposite source file of this
   project includes 30 unique pairs, resulting in 300 analogy questions.

   example content:
sommer-winter
tag-nacht

   see [129]src/opposite.txt

bestmatch.txt

   this file contains groups of content similar word pairs, to evaluate
   the models ability to find thematic relevant analogies. the script
   combines each pair with all other pairs of the same group to build
   analogy questions. the given bestmatch source file of this project
   includes 7 groups with a total of 77 unique pairs, resulting in 540
   analogy questions.

   example content:
: politik
elisabeth-k  nigin
charles-prinz
: technik
android-google
ios-apple
windows-microsoft

   see [130]src/bestmatch.txt

doesntfit.txt

   this file contains 3 words (per line) with similar content divided by
   space and a set of words that do not fit, divided by dash, like
   fittingword1 fittingword2 fittingword3
   notfittingword1-notfittingword2-...-notfittingwordn. this tests the
   models' ability to find the least fitting word in a set of 4 words. the
   script combines each matching triple with every not matching word of
   the list divided by dash, to build doesntfit questions. the available
   doesntfit source file of this project includes 11 triples, each with 10
   words that do not fit, resulting in 110 questions.

   example content:
hase hund katze baum-besitzer-elefant-essen-haus-mensch-tier-tierheim-wiese-zoo
august april september jahr-monat-tag-stunde-minute-zeit-kalender-woche-quartal-
uhr

   see [131]src/doesntfit.txt

   those options for the script execution are possible:
   flag description
   -h, --help show a help message and exit
   -c, --create if set, create testsets before evaluating
   -u, --umlauts if set, create additional testsets with transformed
   umlauts and/or use them instead

   example usage:
python evaluation.py my.model -u

   note: only files with the filetypes .bin, .model or without any suffix
   are treated as binary files.

download

   the optimized german language model, that was trained with this toolkit
   based on the german wikipedia (15th may 2015) and german news articles
   from 2013 (15th may 2015) can be downloaded here:

   [132]german.model [704 mb]

   the germanwordembeddings tool and the pretrained language model are
   completely free to use. if you enjoy it, please consider [133]donating
   via paypal for further development.     

     *    2019 github, inc.
     * [134]terms
     * [135]privacy
     * [136]security
     * [137]status
     * [138]help

     * [139]contact github
     * [140]pricing
     * [141]api
     * [142]training
     * [143]blog
     * [144]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [145]reload to refresh your
   session. you signed out in another tab or window. [146]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/devmount/germanwordembeddings/commits/master.atom
   3. https://github.com/devmount/germanwordembeddings#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/devmount/germanwordembeddings
  32. https://github.com/join
  33. https://github.com/login?return_to=/devmount/germanwordembeddings
  34. https://github.com/devmount/germanwordembeddings/watchers
  35. https://github.com/login?return_to=/devmount/germanwordembeddings
  36. https://github.com/devmount/germanwordembeddings/stargazers
  37. https://github.com/login?return_to=/devmount/germanwordembeddings
  38. https://github.com/devmount/germanwordembeddings/network/members
  39. https://github.com/devmount
  40. https://github.com/devmount/germanwordembeddings
  41. https://github.com/devmount/germanwordembeddings
  42. https://github.com/devmount/germanwordembeddings/issues
  43. https://github.com/devmount/germanwordembeddings/pulls
  44. https://github.com/devmount/germanwordembeddings/pulse
  45. https://github.com/join?source=prompt-code
  46. https://devmount.github.io/germanwordembeddings/
  47. https://github.com/topics/neural-network
  48. https://github.com/topics/id97
  49. https://github.com/topics/word-embeddings
  50. https://github.com/topics/model
  51. https://github.com/topics/training
  52. https://github.com/topics/evaluation
  53. https://github.com/topics/deep-learning
  54. https://github.com/topics/deep-neural-networks
  55. https://github.com/topics/nlp
  56. https://github.com/topics/natural-language-processing
  57. https://github.com/topics/gensim
  58. https://github.com/topics/german-language
  59. https://github.com/devmount/germanwordembeddings/commits/master
  60. https://github.com/devmount/germanwordembeddings/branches
  61. https://github.com/devmount/germanwordembeddings/releases
  62. https://github.com/devmount/germanwordembeddings/graphs/contributors
  63. https://github.com/devmount/germanwordembeddings/blob/master/license
  64. https://github.com/devmount/germanwordembeddings/search?l=jupyter-notebook
  65. https://github.com/devmount/germanwordembeddings/search?l=python
  66. https://github.com/devmount/germanwordembeddings/search?l=shell
  67. https://github.com/devmount/germanwordembeddings/find/master
  68. https://github.com/devmount/germanwordembeddings/archive/master.zip
  69. https://github.com/login?return_to=https://github.com/devmount/germanwordembeddings
  70. https://github.com/join?return_to=/devmount/germanwordembeddings
  71. https://desktop.github.com/
  72. https://desktop.github.com/
  73. https://developer.apple.com/xcode/
  74. https://visualstudio.github.com/
  75. https://github.com/devmount
  76. https://github.com/devmount/germanwordembeddings/commits?author=devmount
  77. https://github.com/devmount/germanwordembeddings/commit/c79f6d1f367e3d15a9c3365681b8af100ac3f78d
  78. https://github.com/devmount/germanwordembeddings/commit/c79f6d1f367e3d15a9c3365681b8af100ac3f78d
  79. https://github.com/devmount/germanwordembeddings/tree/c79f6d1f367e3d15a9c3365681b8af100ac3f78d
  80. https://github.com/devmount/germanwordembeddings/tree/master/code
  81. https://github.com/devmount/germanwordembeddings/commit/9640de3c7517f039c0e764890d918ca1d06ad6ce
  82. https://github.com/devmount/germanwordembeddings/tree/master/data
  83. https://github.com/devmount/germanwordembeddings/commit/7b281bd4d7ca7fe7b22de1275b536528ce052990
  84. https://github.com/devmount/germanwordembeddings/tree/master/result
  85. https://github.com/devmount/germanwordembeddings/tree/master/src
  86. https://github.com/devmount/germanwordembeddings/commit/00cafab172a343dd879dcf300002d563fa271834
  87. https://github.com/devmount/germanwordembeddings/blob/master/.gitignore
  88. https://github.com/devmount/germanwordembeddings/commit/4a2c8f93fbeda315c7ace3e2f808373346a5f304
  89. https://github.com/devmount/germanwordembeddings/blob/master/license
  90. https://github.com/devmount/germanwordembeddings/blob/master/readme.md
  91. https://github.com/devmount/germanwordembeddings/blob/master/wikiextractor.py
  92. https://github.com/devmount/germanwordembeddings/blob/master/evaluation.py
  93. https://github.com/devmount/germanwordembeddings/commit/d6840af80726602db9b679d4bd85f936ce2921d0
  94. https://github.com/devmount/germanwordembeddings/blob/master/preprocessing.py
  95. https://github.com/devmount/germanwordembeddings/blob/master/requirements.txt
  96. https://github.com/devmount/germanwordembeddings/commit/d497bf3cfe62812aa0a62c15171dda9a4e935f2b
  97. https://github.com/devmount/germanwordembeddings/blob/master/tfvisualize.py
  98. https://github.com/devmount/germanwordembeddings/blob/master/training.py
  99. https://github.com/devmount/germanwordembeddings/blob/master/visualize.py
 100. https://github.com/devmount/germanwordembeddings/blob/master/vocabulary.py
 101. https://github.com/devmount/germanwordembeddings/blob/master/id97_german.sh
 102. https://github.com/devmount/germanwordembeddings/commit/c2b603a07d968146995ee9dde54a25fd0aa8586a
 103. https://devmount.github.io/germanwordembeddings/
 104. https://github.com/devmount/germanwordembeddings/blob/master/license
 105. https://devmount.github.io/germanwordembeddings/#download
 106. https://radimrehurek.com/gensim/models/id97.html
 107. https://devmount.github.io/germanwordembeddings/#download
 108. https://devmount.github.io/germanwordembeddings/
 109. https://github.com/devmount/germanwordembeddings/blob/master/mit.md
 110. https://github.com/devmount/germanwordembeddings#getstarted
 111. https://github.com/devmount/germanwordembeddings#obtention
 112. https://github.com/devmount/germanwordembeddings#preprocessing
 113. https://github.com/devmount/germanwordembeddings#training
 114. https://github.com/devmount/germanwordembeddings#vocabulary
 115. https://github.com/devmount/germanwordembeddings#evaluation
 116. https://github.com/devmount/germanwordembeddings#download
 117. https://github.com/devmount/germanwordembeddings/blob/master/id97_german.sh
 118. http://cloud.devmount.de/d2bc5672c523b086
 119. https://github.com/devmount/germanwordembeddings/blob/master/code
 120. https://github.com/devmount/germanwordembeddings/blob/master/preprocessing.py
 121. https://github.com/devmount/germanwordembeddings/blob/master/training.py
 122. https://github.com/devmount/germanwordembeddings/blob/master/vocabulary.py
 123. https://github.com/devmount/germanwordembeddings/blob/master/evaluation.py
 124. http://radimrehurek.com/gensim/models/id97.html#gensim.models.id97.id97.accuracy
 125. https://github.com/devmount/germanwordembeddings/blob/master/evaluation.py
 126. https://github.com/devmount/germanwordembeddings/blob/master/src/adjectives.txt
 127. https://github.com/devmount/germanwordembeddings/blob/master/src/nouns.txt
 128. https://github.com/devmount/germanwordembeddings/blob/master/src/verbs.txt
 129. https://github.com/devmount/germanwordembeddings/blob/master/src/opposite.txt
 130. https://github.com/devmount/germanwordembeddings/blob/master/src/bestmatch.txt
 131. https://github.com/devmount/germanwordembeddings/blob/master/src/doesntfit.txt
 132. http://cloud.devmount.de/d2bc5672c523b086
 133. https://paypal.me/devmount
 134. https://github.com/site/terms
 135. https://github.com/site/privacy
 136. https://github.com/security
 137. https://githubstatus.com/
 138. https://help.github.com/
 139. https://github.com/contact
 140. https://github.com/pricing
 141. https://developer.github.com/
 142. https://training.github.com/
 143. https://github.blog/
 144. https://github.com/about
 145. https://github.com/devmount/germanwordembeddings
 146. https://github.com/devmount/germanwordembeddings

   hidden links:
 148. https://github.com/
 149. https://github.com/devmount/germanwordembeddings
 150. https://github.com/devmount/germanwordembeddings
 151. https://github.com/devmount/germanwordembeddings
 152. https://help.github.com/articles/which-remote-url-should-i-use
 153. https://github.com/devmount/germanwordembeddings#germanwordembeddings
 154. https://github.com/devmount/germanwordembeddings#get-started-
 155. https://github.com/devmount/germanwordembeddings#obtaining-corpora-
 156. https://github.com/devmount/germanwordembeddings#german-wikipedia
 157. https://github.com/devmount/germanwordembeddings#statistical-machine-translation
 158. https://github.com/devmount/germanwordembeddings#preprocessing-
 159. https://github.com/devmount/germanwordembeddings#training-models-
 160. https://github.com/devmount/germanwordembeddings#vocabulary-
 161. https://github.com/devmount/germanwordembeddings#evaluation-
 162. https://github.com/devmount/germanwordembeddings#syntactic-test-set
 163. https://github.com/devmount/germanwordembeddings#adjectivestxt
 164. https://github.com/devmount/germanwordembeddings#nounstxt
 165. https://github.com/devmount/germanwordembeddings#verbstxt
 166. https://github.com/devmount/germanwordembeddings#semantic-test-set
 167. https://github.com/devmount/germanwordembeddings#oppositetxt
 168. https://github.com/devmount/germanwordembeddings#bestmatchtxt
 169. https://github.com/devmount/germanwordembeddings#doesntfittxt
 170. https://github.com/devmount/germanwordembeddings#download
 171. https://github.com/
