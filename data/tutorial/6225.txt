   #[1]github [2]recent commits to lftm:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]11
     * [35]star [36]126
     * [37]fork [38]40

[39]datquocnguyen/[40]lftm

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   improving topic models lda and dmm (one-topic-per-document model for
   short texts) with id27s (tacl 2015)
   [47]topic-modeling [48]gibbs-sampling [49]word-embeddings
   [50]short-text
     * [51]19 commits
     * [52]1 branch
     * [53]0 releases
     * [54]fetching contributors
     * [55]view license

    1. [56]java 100.0%

   (button) java
   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/d
   [58]download zip

downloading...

   want to be notified of new releases in datquocnguyen/lftm?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   [65]@datquocnguyen
   [66]datquocnguyen [67]update readme.md
   latest commit [68]2d074e5 may 8, 2017
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]jar
   [71]lib
   [72]src
   [73]test [74]id136 of topic distribution on unseen corpus nov 11,
   2016
   [75]license.txt
   [76]readme.md [77]update readme.md may 8, 2017
   [78]build.xml

readme.md

lf-lda and lf-dmm latent feature topic models

   the implementations of the lf-lda and lf-dmm latent feature topic
   models, as described in my tacl paper:

   dat quoc nguyen, richard billingsley, lan du and mark johnson.
   [79]improving topic models with latent feature word representations.
   transactions of the association for computational linguistics, vol. 3,
   pp. 299-313, 2015. [80][.bib] [81][datasets]
   [82][example_20newsgroups_20topics_top50words]

   the implementations of the lda and dmm topic models are available at
   [83]http://jldadmm.sourceforge.net/

usage

   this section describes the usage of the implementations in command line
   or terminal, using the pre-compiled lftm.jar file.

   here, it is expected that java 1.7+ is already set to run in command
   line or terminal (for example: adding java to the path environment
   variable in windows os).

   the pre-compiled lftm.jar file and source codes are in the jar and src
   folders, respectively. users can recompile the source codes by simply
   running ant (it is also expected that ant is already installed). in
   addition, the users can find input examples in the test folder.

file format of input topic-modeling corpus

   similar to the corpus.txt file in the test folder, each line in the
   input topic-modeling corpus represents a document. here, a document is
   a sequence words/tokens separated by white space characters. the users
   should preprocess the input topic-modeling corpus before training the
   topic models, for example: down-casing, removing non-alphabetic
   characters and stop-words, removing words shorter than 3 characters and
   words appearing less than a certain times.

format of input word-vector file

   similar to the wordvectors.txt file in the test folder, each line in
   the input word-vector file starts with a word type which is followed by
   a vector representation.

   to obtain the vector representations of words, the users can use the
   pre-trained word vectors learned from large external corpora or the
   word vectors which are trained on the input topic-modeling corpus.

   in case of using the pre-trained word vectors learned from the large
   external corpora, the users have to remove words in the input
   topic-modeling corpus, in which these words are not found in the input
   word-vector file.

   some sets of the pre-trained word vectors can be found at:

   [84]id97: https://code.google.com/p/id97/

   [85]glove: http://nlp.stanford.edu/projects/glove/

   if the input topic-modeling corpus is too domain-specific, the domain
   of the external corpus (from which the word vectors are derived) should
   not be too different to that of the input topic-modeling corpus. for
   example, when applying to the biomedical domain, the users may use
   id97 or glove to learn 50 or 100-dimensional word vectors on the
   large external medline corpus instead of using the pre-trained id97
   or glove word vectors.

training lf-lda and lf-dmm

   $ java [-xmx2g] -jar jar/lftm.jar    model <lflda_or_lfdmm> -corpus
   <input_corpus_file_path> -vectors <input_vector_file_path> [-ntopics
   <int>] [-alpha <double>] [-beta <double>] [-lambda <double>] [-initers
   <int>] [-niters <int>] [-twords <int>] [-name <string>] [-sstep <int>]

   where hyper-parameters in [ ] are optional.
     * -model: specify the topic model.
     * -corpus: specify the path to the input training corpus file.
     * -vectors: specify the path to the file containing word vectors.
     * -ntopics <int>: specify the number of topics. the default value is
       20.
     * -alpha <double>: specify the hyper-parameter alpha. following [1,
       2], the default value is 0.1.
     * -beta <double>: specify the hyper-parameter beta. the default value
       is 0.01. following [2], you might also want to try beta value of
       0.1 for short texts.
     * -lambda <double>: specify the mixture weight lambda (0.0 < lambda
       <= 1.0). set the mixture weight lambda to be 1.0 to obtain the best
       topic coherence. in case of document id91/classification
       evaluation, fine-tune this parameter to obtain the highest results
       if you have time; otherwise try both values 0.6 and 1.0 (i would
       suggest to set lambda 0.6 for normal text corpora and 1.0 for short
       text corpora if you don't have time to try both 0.6 and 1.0).
     * -initers <int>: specify the number of initial sampling iterations
       to separate the counts for the latent feature component and the
       dirichlet multinomial component. the default value is 2000.
     * -niters <int>: specify the number of sampling iterations for the
       latent feature topic models. the default value is 200.
     * -twords <int>: specify the number of the most probable topical
       words. the default value is 20.
     * -name <string>: specify a name to the id96 experiment.
       the default value is    model   .
     * -sstep <int>: specify a step to save the sampling output (-sstep
       value < -niters value). the default value is 0 (i.e. only saving
       the output from the last sample).

   note that (topic vectors are learned in parallel, so) running lftm code
   with multiple cpu/core machine to obtain a significantly faster
   training process, e.g. using a multi-core computer, or set number of
   cpus requested for a remote job to be equal to number of topics.

   examples:

   $ java -jar jar/lftm.jar -model lflda -corpus test/corpus.txt -vectors
   test/wordvectors.txt -ntopics 4 -alpha 0.1 -beta 0.01 -lambda 0.6
   -initers 500 -niters 50 -name testlflda

   basically, with this command we run 500 lda sampling iterations (i.e.,
   -initers 500) for initialization and then run 50 lf-lda sampling
   iterations (i.e., -niters 50). the output files are saved in the same
   folder as the input training corpus file, in this case in the test
   folder. we have output files of testlflda.theta, testlflda.phi,
   testlflda.topwords, testlflda.topicassignments and testlflda.paras,
   referring to the document-to-topic distributions, topic-to-word
   distributions, top topical words, topic assignments and model
   hyper-parameters, respectively. similarly, we perform:

   $ java -jar jar/lftm.jar -model lfdmm -corpus test/corpus.txt -vectors
   test/wordvectors.txt -ntopics 4 -alpha 0.1 -beta 0.1 -lambda 1.0
   -initers 500 -niters 50 -name testlfdmm

   we have output files of testlfdmm.theta, testlfdmm.phi,
   testlfdmm.topwords, testlfdmm.topicassignments and testlfdmm.paras.

   in the lf-lda and lf-dmm latent feature topic models, a word is
   generated by the latent feature topic-to-word component or by the
   topic-to-word dirichlet multinomial component. in practical
   implementation, instead of using a binary selection variable to record
   that, i simply add a value of the number of topics to the actual topic
   assignment value. for example with 20 topics, the output topic
   assignment 3 23 4 4 24 3 23 3 23 3 23 for a document means that the
   first word in the document is generated from topic 3 by the latent
   feature topic-to-word component. the second word is also generated from
   the topic 23 - 20 = 3, but by the topic-to-word dirichlet multinomial
   component. it is similar for the remaining words in the document.

document id91 evaluation

   here, we treat each topic as a cluster, and we assign every document
   the topic with the highest id203 given the document. to get the
   id91 scores of purity and normalized mutual information, we
   perform:

   $ java    jar jar/lftm.jar    model eval    label <golden_label_file_path>
   -dir <directory_path> -prob <document-topic-prob/suffix>
     *    label: specify the path to the ground truth label file. each line
       in this label file contains the golden label of the corresponding
       document in the input training corpus. see the corpus.label and
       corpus.txt files in the test folder.
     * -dir: specify the path to the directory containing
       document-to-topic distribution files.
     * -prob: specify a document-to-topic distribution file or a group of
       document-to-topic distribution files in the specified directory.

   examples:

   the command $ java -jar jar/lftm.jar -model eval -label
   test/corpus.label -dir test -prob testlflda.theta will produce the
   id91 score for the testlflda.theta file.

   the command $ java -jar jar/lftm.jar -model eval -label
   test/corpus.label -dir test -prob testlfdmm.theta will produce the
   id91 score for testlfdmm.theta file.

   the command $ java -jar jar/lftm.jar -model eval -label
   test/corpus.label -dir test -prob theta will produce the id91
   scores for all the document-to-topic distribution files having names
   ended by theta. in this case, the distribution files are
   testlflda.theta and testlfdmm.theta. it also provides the mean and
   standard deviation of the id91 scores.

id136 of topic distribution on unseen corpus

   to infer topics on an unseen/new corpus using a pre-trained
   lf-lda/lf-dmm topic model, we perform:

   $ java -jar jar/lftm.jar -model <lfldainf_or_lfdmminf> -paras
   <hyperparameter_file_path> -corpus <unseen_corpus_file_path> [-initers
   <int>] [-niters <int>] [-twords <int>] [-name <string>] [-sstep <int>]
     * -paras: specify the path to the hyper-parameter file produced by
       the pre-trained lf-lda/lf-dmm topic model.

   examples:

   $ java -jar jar/lftm.jar -model lfldainf -paras test/testlflda.paras
   -corpus test/corpus_test.txt -initers 500 -niters 50 -name testlfldainf

   $ java -jar jar/lftm.jar -model lfdmminf -paras test/testlfdmm.paras
   -corpus test/corpus_test.txt -initers 500 -niters 50 -name testlfdmminf

acknowledgments

   the lf-lda and lf-dmm implementations used utilities including the
   lbfgs implementation from [86]mallet toolkit, the random number
   generator in [87]java version of mersennetwister, the parallel.java
   utility from [88]mines java toolkit and the [89]java command line
   arguments parser. i would like to thank the authors of the mentioned
   utilities for sharing the codes.

references

   [1] yue lu, qiaozhu mei, and chengxiang zhai. 2011. investigating task
   performance of probabilistic topic models: an empirical study of plsa
   and lda. information retrieval, 14:178   203.

   [2] jianhua yin and jianyong wang. 2014. a dirichlet multinomial
   mixture model-based approach for short text id91. in proceedings
   of the 20th acm sigkdd international conference on knowledge discovery
   and data mining, pages 233   242.

     *    2019 github, inc.
     * [90]terms
     * [91]privacy
     * [92]security
     * [93]status
     * [94]help

     * [95]contact github
     * [96]pricing
     * [97]api
     * [98]training
     * [99]blog
     * [100]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [101]reload to refresh your
   session. you signed out in another tab or window. [102]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/datquocnguyen/lftm/commits/master.atom
   3. https://github.com/datquocnguyen/lftm#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/datquocnguyen/lftm
  32. https://github.com/join
  33. https://github.com/login?return_to=/datquocnguyen/lftm
  34. https://github.com/datquocnguyen/lftm/watchers
  35. https://github.com/login?return_to=/datquocnguyen/lftm
  36. https://github.com/datquocnguyen/lftm/stargazers
  37. https://github.com/login?return_to=/datquocnguyen/lftm
  38. https://github.com/datquocnguyen/lftm/network/members
  39. https://github.com/datquocnguyen
  40. https://github.com/datquocnguyen/lftm
  41. https://github.com/datquocnguyen/lftm
  42. https://github.com/datquocnguyen/lftm/issues
  43. https://github.com/datquocnguyen/lftm/pulls
  44. https://github.com/datquocnguyen/lftm/projects
  45. https://github.com/datquocnguyen/lftm/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/topic-modeling
  48. https://github.com/topics/gibbs-sampling
  49. https://github.com/topics/word-embeddings
  50. https://github.com/topics/short-text
  51. https://github.com/datquocnguyen/lftm/commits/master
  52. https://github.com/datquocnguyen/lftm/branches
  53. https://github.com/datquocnguyen/lftm/releases
  54. https://github.com/datquocnguyen/lftm/graphs/contributors
  55. https://github.com/datquocnguyen/lftm/blob/master/license.txt
  56. https://github.com/datquocnguyen/lftm/search?l=java
  57. https://github.com/datquocnguyen/lftm/find/master
  58. https://github.com/datquocnguyen/lftm/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/datquocnguyen/lftm
  60. https://github.com/join?return_to=/datquocnguyen/lftm
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/datquocnguyen
  66. https://github.com/datquocnguyen/lftm/commits?author=datquocnguyen
  67. https://github.com/datquocnguyen/lftm/commit/2d074e52c6b9760bd3ca4d2f1238e33f0f7e09fb
  68. https://github.com/datquocnguyen/lftm/commit/2d074e52c6b9760bd3ca4d2f1238e33f0f7e09fb
  69. https://github.com/datquocnguyen/lftm/tree/2d074e52c6b9760bd3ca4d2f1238e33f0f7e09fb
  70. https://github.com/datquocnguyen/lftm/tree/master/jar
  71. https://github.com/datquocnguyen/lftm/tree/master/lib
  72. https://github.com/datquocnguyen/lftm/tree/master/src
  73. https://github.com/datquocnguyen/lftm/tree/master/test
  74. https://github.com/datquocnguyen/lftm/commit/5d5ceaae6c46edbe9a0705ca20f2650bab18201f
  75. https://github.com/datquocnguyen/lftm/blob/master/license.txt
  76. https://github.com/datquocnguyen/lftm/blob/master/readme.md
  77. https://github.com/datquocnguyen/lftm/commit/2d074e52c6b9760bd3ca4d2f1238e33f0f7e09fb
  78. https://github.com/datquocnguyen/lftm/blob/master/build.xml
  79. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/582/158
  80. http://web.science.mq.edu.au/~id25guyen/papers/tacl.bib
  81. http://web.science.mq.edu.au/~id25guyen/papers/tacl-datasets.zip
  82. http://web.science.mq.edu.au/~id25guyen/papers/tacl_topwords_n20_20topics.zip
  83. http://jldadmm.sourceforge.net/
  84. https://code.google.com/p/id97/
  85. http://nlp.stanford.edu/projects/glove/
  86. http://mallet.cs.umass.edu/
  87. http://cs.gmu.edu/~sean/research/
  88. http://dhale.github.io/jtk/api/edu/mines/jtk/util/parallel.html
  89. http://args4j.kohsuke.org/sample.html
  90. https://github.com/site/terms
  91. https://github.com/site/privacy
  92. https://github.com/security
  93. https://githubstatus.com/
  94. https://help.github.com/
  95. https://github.com/contact
  96. https://github.com/pricing
  97. https://developer.github.com/
  98. https://training.github.com/
  99. https://github.blog/
 100. https://github.com/about
 101. https://github.com/datquocnguyen/lftm
 102. https://github.com/datquocnguyen/lftm

   hidden links:
 104. https://github.com/
 105. https://github.com/datquocnguyen/lftm
 106. https://github.com/datquocnguyen/lftm
 107. https://github.com/datquocnguyen/lftm
 108. https://help.github.com/articles/which-remote-url-should-i-use
 109. https://github.com/datquocnguyen/lftm#lf-lda-and-lf-dmm-latent-feature-topic-models
 110. https://github.com/datquocnguyen/lftm#usage
 111. https://github.com/datquocnguyen/lftm#file-format-of-input-topic-modeling-corpus
 112. https://github.com/datquocnguyen/lftm#format-of-input-word-vector-file
 113. https://github.com/datquocnguyen/lftm#training-lf-lda-and-lf-dmm
 114. https://github.com/datquocnguyen/lftm#document-id91-evaluation
 115. https://github.com/datquocnguyen/lftm#id136-of-topic-distribution-on-unseen-corpus
 116. https://github.com/datquocnguyen/lftm#acknowledgments
 117. https://github.com/datquocnguyen/lftm#references
 118. https://github.com/
