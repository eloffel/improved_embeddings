   #[1]github [2]recent commits to tf-id195:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]26
     * [35]star [36]352
     * [37]fork [38]105

[39]jayparks/[40]tf-id195

   [41]code [42]issues 13 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   sequence to sequence learning using tensorflow.
   [47]tensorflow [48]id195 [49]sequence-to-sequence
   [50]neural-machine-translation [51]id4 [52]encoder-decoder
   [53]machine-learning [54]deep-learning [55]neural-network
   [56]natural-language-processing [57]nlp [58]beam-search
   [59]natural-language-understanding
     * [60]55 commits
     * [61]1 branch
     * [62]0 releases
     * [63]fetching contributors

    1. [64]python 51.1%
    2. [65]perl 18.7%
    3. [66]jupyter notebook 13.6%
    4. [67]emacs lisp 11.2%
    5. [68]shell 1.3%
    6. [69]smalltalk 1.2%
    7. other 2.9%

   (button) python perl jupyter notebook emacs lisp shell smalltalk other
   branch: master (button) new pull request
   [70]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/j
   [71]download zip

downloading...

   want to be notified of new releases in jayparks/tf-id195?
   [72]sign in [73]sign up

launching github desktop...

   if nothing happens, [74]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [75]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [76]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [77]download the github extension for visual studio
   and try again.

   (button) go back
   [78]@jayparks
   [79]jayparks [80]merge pull request [81]#15 [82]from germey/master
   (button)    
create model before init filewriter

   latest commit [83]e55d88e apr 17, 2018
   [84]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [85]data [86]add sample files aug 8, 2017
   [87].gitignore [88]now supports batch beamsearch decoding jun 5, 2017
   [89]readme.md
   [90]decode.ipynb
   [91]decode.py
   [92]id195_model.py
   [93]train.ipynb
   [94]train.py

readme.md

tf-id195

sequence to sequence (id195) learning using tensorflow.

   the core building blocks are id56 encoder-decoder architectures and
   attention mechanism.

   the package was largely implemented using the latest (1.2)
   tf.contrib.id195 modules
     * attentionwrapper
     * decoder
     * basicdecoder
     * beamsearchdecoder

   the package supports
     * multi-layer gru/lstm
     * residual connection
     * dropout
     * attention and input_feeding
     * beamsearch decoding
     * write n-best list

dependencies

     * numpy >= 1.11.1
     * tensorflow >= 1.2

history

     * june 5, 2017: major update
     * june 6, 2017: supports batch beamsearch decoding
     * june 11, 2017: separted training / decoding
     * june 22, 2017: supports tf.1.2 (contrib.id56 -> python.ops.id56_cell)

usage instructions

data preparation

   to preprocess raw parallel data of sample_data.src and sample_data.trg,
   simply run
cd data/
./preprocess.sh src trg sample_data ${max_seq_len}

   running the above code performs widely used preprocessing steps for
   machine translation (mt).
     * normalizing punctuation
     * tokenizing
     * bytepair encoding (# merge = 30000) (sennrich et al., 2016)
     * cleaning sequences of length over ${max_seq_len}
     * shuffling
     * building dictionaries

training

   to train a id195 model,
$ python train.py   --cell_type 'lstm' \
                    --attention_type 'luong' \
                    --hidden_units 1024 \
                    --depth 2 \
                    --embedding_size 500 \
                    --num_encoder_symbols 30000 \
                    --num_decoder_symbols 30000 ...

decoding

   to run the trained model for decoding,
$ python decode.py  --beam_width 5 \
                    --decode_batch_size 30 \
                    --model_path $path_to_a_model_checkpoint (e.g. model/transla
te.ckpt-100) \
                    --max_decode_step 300 \
                    --write_n_best false
                    --decode_input $path_to_decode_input
                    --decode_output $path_to_decode_output

   if --beam_width=1, greedy decoding is performed at each time-step.

arguments

   data params
     * --source_vocabulary : path to source vocabulary
     * --target_vocabulary : path to target vocabulary
     * --source_train_data : path to source training data
     * --target_train_data : path to target training data
     * --source_valid_data : path to source validation data
     * --target_valid_data : path to target validation data

   network params
     * --cell_type : id56 cell to use for encoder and decoder (default:
       lstm)
     * --attention_type : attention mechanism (bahdanau, luong), (default:
       bahdanau)
     * --depth : number of hidden units for each layer in the model
       (default: 2)
     * --embedding_size : embedding dimensions of encoder and decoder
       inputs (default: 500)
     * --num_encoder_symbols : source vocabulary size to use (default:
       30000)
     * --num_decoder_symbols : target vocabulary size to use (default:
       30000)
     * --use_residual : use residual connection between layers (default:
       true)
     * --attn_input_feeding : use input feeding method in attentional
       decoder (luong et al., 2015) (default: true)
     * --use_dropout : use dropout in id56 cell output (default: true)
     * --dropout_rate : dropout id203 for cell outputs (0.0: no
       dropout) (default: 0.3)

   training params
     * --learning_rate : number of hidden units for each layer in the
       model (default: 0.0002)
     * --max_gradient_norm : clip gradients to this norm (default 1.0)
     * --batch_size : batch size
     * --max_epochs : maximum training epochs
     * --max_load_batches : maximum number of batches to prefetch at one
       time.
     * --max_seq_length : maximum sequence length
     * --display_freq : display training status every this iteration
     * --save_freq : save model checkpoint every this iteration
     * --valid_freq : evaluate the model every this iteration: valid_data
       needed
     * --optimizer : optimizer for training: (adadelta, adam, rmsprop)
       (default: adam)
     * --model_dir : path to save model checkpoints
     * --model_name : file name used for model checkpoints
     * --shuffle_each_epoch : shuffle training dataset for each epoch
       (default: true)
     * --sort_by_length : sort pre-fetched minibatches by their target
       sequence lengths (default: true)

   decoding params
     * --beam_width : beam width used in beamsearch (default: 1)
     * --decode_batch_size : batch size used in decoding
     * --max_decode_step : maximum time step limit in decoding (default:
       500)
     * --write_n_best : write beamsearch n-best list (n=beam_width)
       (default: false)
     * --decode_input : input file path to decode
     * --decode_output : output file path of decoding output

   runtime params
     * --allow_soft_placement : allow device soft placement
     * --log_device_placement : log placement of ops on devices

acknowledgements

   the implementation is based on following projects:
     * [95]nematus: theano implementation of id4.
       major reference of this project
     * [96]subword-id4: included subword-unit scripts to preprocess input
       data
     * [97]moses: included preprocessing scripts to preprocess input data
     * [98]tf.id195_legacy legacy tensorflow id195 tutorial
     * [99]tf_tutorial_plus: nice tutorials for tf.contrib.id195 api

   for any comments and feedbacks, please email me at
   [100]pjh0308@gmail.com or open an issue here.

     *    2019 github, inc.
     * [101]terms
     * [102]privacy
     * [103]security
     * [104]status
     * [105]help

     * [106]contact github
     * [107]pricing
     * [108]api
     * [109]training
     * [110]blog
     * [111]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [112]reload to refresh your
   session. you signed out in another tab or window. [113]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/jayparks/tf-id195/commits/master.atom
   3. https://github.com/jayparks/tf-id195#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/jayparks/tf-id195
  32. https://github.com/join
  33. https://github.com/login?return_to=/jayparks/tf-id195
  34. https://github.com/jayparks/tf-id195/watchers
  35. https://github.com/login?return_to=/jayparks/tf-id195
  36. https://github.com/jayparks/tf-id195/stargazers
  37. https://github.com/login?return_to=/jayparks/tf-id195
  38. https://github.com/jayparks/tf-id195/network/members
  39. https://github.com/jayparks
  40. https://github.com/jayparks/tf-id195
  41. https://github.com/jayparks/tf-id195
  42. https://github.com/jayparks/tf-id195/issues
  43. https://github.com/jayparks/tf-id195/pulls
  44. https://github.com/jayparks/tf-id195/projects
  45. https://github.com/jayparks/tf-id195/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/tensorflow
  48. https://github.com/topics/id195
  49. https://github.com/topics/sequence-to-sequence
  50. https://github.com/topics/neural-machine-translation
  51. https://github.com/topics/id4
  52. https://github.com/topics/encoder-decoder
  53. https://github.com/topics/machine-learning
  54. https://github.com/topics/deep-learning
  55. https://github.com/topics/neural-network
  56. https://github.com/topics/natural-language-processing
  57. https://github.com/topics/nlp
  58. https://github.com/topics/beam-search
  59. https://github.com/topics/natural-language-understanding
  60. https://github.com/jayparks/tf-id195/commits/master
  61. https://github.com/jayparks/tf-id195/branches
  62. https://github.com/jayparks/tf-id195/releases
  63. https://github.com/jayparks/tf-id195/graphs/contributors
  64. https://github.com/jayparks/tf-id195/search?l=python
  65. https://github.com/jayparks/tf-id195/search?l=perl
  66. https://github.com/jayparks/tf-id195/search?l=jupyter-notebook
  67. https://github.com/jayparks/tf-id195/search?l=emacs-lisp
  68. https://github.com/jayparks/tf-id195/search?l=shell
  69. https://github.com/jayparks/tf-id195/search?l=smalltalk
  70. https://github.com/jayparks/tf-id195/find/master
  71. https://github.com/jayparks/tf-id195/archive/master.zip
  72. https://github.com/login?return_to=https://github.com/jayparks/tf-id195
  73. https://github.com/join?return_to=/jayparks/tf-id195
  74. https://desktop.github.com/
  75. https://desktop.github.com/
  76. https://developer.apple.com/xcode/
  77. https://visualstudio.github.com/
  78. https://github.com/jayparks
  79. https://github.com/jayparks/tf-id195/commits?author=jayparks
  80. https://github.com/jayparks/tf-id195/commit/e55d88ec21090c127d24da16b9e2b6b9aa894821
  81. https://github.com/jayparks/tf-id195/pull/15
  82. https://github.com/jayparks/tf-id195/commit/e55d88ec21090c127d24da16b9e2b6b9aa894821
  83. https://github.com/jayparks/tf-id195/commit/e55d88ec21090c127d24da16b9e2b6b9aa894821
  84. https://github.com/jayparks/tf-id195/tree/e55d88ec21090c127d24da16b9e2b6b9aa894821
  85. https://github.com/jayparks/tf-id195/tree/master/data
  86. https://github.com/jayparks/tf-id195/commit/c09d4521cc5f2857c9ff9583fc221c6ca3ad0fed
  87. https://github.com/jayparks/tf-id195/blob/master/.gitignore
  88. https://github.com/jayparks/tf-id195/commit/d278e9a15052376b298ee1de71505b8449791e84
  89. https://github.com/jayparks/tf-id195/blob/master/readme.md
  90. https://github.com/jayparks/tf-id195/blob/master/decode.ipynb
  91. https://github.com/jayparks/tf-id195/blob/master/decode.py
  92. https://github.com/jayparks/tf-id195/blob/master/id195_model.py
  93. https://github.com/jayparks/tf-id195/blob/master/train.ipynb
  94. https://github.com/jayparks/tf-id195/blob/master/train.py
  95. https://github.com/rsennrich/nematus/
  96. https://github.com/rsennrich/subword-id4/
  97. https://github.com/moses-smt/mosesdecoder
  98. https://github.com/tensorflow/models/tree/master/tutorials/id56/translate
  99. https://github.com/j-min/tf_tutorial_plus
 100. mailto:pjh0308@gmail.com
 101. https://github.com/site/terms
 102. https://github.com/site/privacy
 103. https://github.com/security
 104. https://githubstatus.com/
 105. https://help.github.com/
 106. https://github.com/contact
 107. https://github.com/pricing
 108. https://developer.github.com/
 109. https://training.github.com/
 110. https://github.blog/
 111. https://github.com/about
 112. https://github.com/jayparks/tf-id195
 113. https://github.com/jayparks/tf-id195

   hidden links:
 115. https://github.com/
 116. https://github.com/jayparks/tf-id195
 117. https://github.com/jayparks/tf-id195
 118. https://github.com/jayparks/tf-id195
 119. https://help.github.com/articles/which-remote-url-should-i-use
 120. https://github.com/jayparks/tf-id195#tf-id195
 121. https://github.com/jayparks/tf-id195#sequence-to-sequence-seq2seid24-using-tensorflow
 122. https://github.com/jayparks/tf-id195#dependencies
 123. https://github.com/jayparks/tf-id195#history
 124. https://github.com/jayparks/tf-id195#usage-instructions
 125. https://github.com/jayparks/tf-id195#data-preparation
 126. https://github.com/jayparks/tf-id195#training
 127. https://github.com/jayparks/tf-id195#decoding
 128. https://github.com/jayparks/tf-id195#arguments
 129. https://github.com/jayparks/tf-id195#acknowledgements
 130. https://github.com/
