   #[1]github [2]recent commits to thinc:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]58
     * [35]star [36]949
     * [37]fork [38]106

[39]explosion/[40]thinc

   [41]code [42]issues 13 [43]pull requests 1 [44]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [45]sign up
        spacy's machine learning library for nlp in python
   [46]machine-learning [47]deep-learning [48]artificial-intelligence
   [49]ai [50]python [51]nlp [52]natural-language-processing [53]spacy
   [54]machine-learning-library
     * [55]2,753 commits
     * [56]21 branches
     * [57]43 releases
     * [58]fetching contributors
     * [59]mit

    1. [60]python 48.0%
    2. [61]c 33.1%
    3. [62]c++ 17.6%
    4. [63]cuda 1.2%
    5. other 0.1%

   (button) python c c++ cuda other
   branch: master (button) new pull request
   [64]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [65]download zip

downloading...

   want to be notified of new releases in explosion/thinc?
   [66]sign in [67]sign up

launching github desktop...

   if nothing happens, [68]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [69]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [70]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [71]download the github extension for visual studio
   and try again.

   (button) go back
   [72]@honnibal
   [73]honnibal [74]set version to v7.0.4
   latest commit [75]90129be mar 19, 2019
   [76]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [77].buildkite
   [78]bin [79]update push-tag script mar 14, 2019
   [80]examples [81]tidy up and auto-format dec 2, 2018
   [82]fabfile
   [83]include [84]fix accelerate framework usage for osx mar 25, 2018
   [85]thinc [86]set version to v7.0.4 mar 19, 2019
   [87].build.cmd
   [88].coveragerc
   [89].flake8
   [90].gitignore
   [91]license [92]update license dec 23, 2016
   [93]manifest.in
   [94]readme.md
   [95]azure-pipelines.yml
   [96]examples-requirements.txt
   [97]requirements.txt
   [98]setup.py

readme.md

   [99][68747470733a2f2f6578706c6f73696f6e2e61692f6173736574732f696d672f6c
   6f676f2e737667]

thinc: practical machine learning for nlp in python

   thinc is the machine learning library powering [100]spacy. it features
   a battle-tested linear model designed for large sparse learning
   problems, and a flexible neural network model under development for
   [101]spacy v2.0.

   thinc is a practical toolkit for implementing models that follow the
   [102]"embed, encode, attend, predict" architecture. it's designed to be
   easy to install, efficient for cpu usage and optimised for nlp and deep
   learning with text     in particular, hierarchically structured input and
   variable-length sequences.

        version 6.12 out now! [103]read the release notes here.

   [104]azure pipelines [105]current release version [106]pypi version
   [107]conda version [108]python wheels [109]follow us on twitter

what's where (as of v6.9.0)

   module description
   thinc.v2v.model base class.
   thinc.v2v layers transforming vectors to vectors.
   thinc.i2v layers embedding ids to vectors.
   thinc.t2v layers pooling tensors to vectors.
   thinc.t2t layers transforming tensors to tensors (e.g. id98, lstm).
   thinc.api higher-order functions, for building networks. will be
   renamed.
   thinc.extra datasets and utilities.
   thinc.neural.ops container classes for mathematical operations. will be
   reorganized.
   thinc.linear.avgtron legacy efficient averaged id88
   implementation.

development status

   thinc's deep learning functionality is still under active development:
   apis are unstable, and we're not yet ready to provide usage support.
   however, if you're already quite familiar with neural networks, there's
   a lot here you might find interesting. thinc's conceptual model is
   quite different from tensorflow's. thinc also implements some novel
   features, such as a small dsl for concisely wiring up models, embedding
   tables that support pre-computation and the hashing trick, dynamic
   batch sizes, a concatenation-based approach to variable-length
   sequences, and support for model averaging for the adam solver (which
   performs very well).

no computational graph     just higher order functions

   the central problem for a neural network implementation is this: during
   the forward pass, you compute results that will later be useful during
   the backward pass. how do you keep track of this arbitrary state, while
   making sure that layers can be cleanly composed?

   most libraries solve this problem by having you declare the forward
   computations, which are then compiled into a graph somewhere behind the
   scenes. thinc doesn't have a "computational graph". instead, we just
   use the stack, because we put the state from the forward pass into
   callbacks.

   all nodes in the network have a simple signature:
f(inputs) -> {outputs, f(d_outputs)->d_inputs}

   to make this less abstract, here's a relu activation, following this
   signature:
def relu(inputs):
    mask = inputs > 0
    def backprop_relu(d_outputs, optimizer):
        return d_outputs * mask
    return inputs * mask, backprop_relu

   when you call the relu function, you get back an output variable, and a
   callback. this lets you calculate a gradient using the output, and then
   pass it into the callback to perform the backward pass.

   this signature makes it easy to build a complex network out of smaller
   pieces, using arbitrary higher-order functions you can write yourself.
   to make this clearer, we need a function for a weights layer. usually
   this will be implemented as a class     but let's continue using
   closures, to keep things concise, and to keep the simplicity of the
   interface explicit.

   the main complication for the weights layer is that we now have a
   side-effect to manage: we would like to update the weights. there are a
   few ways to handle this. in thinc we currently pass a callable into the
   backward pass. (i'm not convinced this is best.)
import numpy

def create_linear_layer(n_out, n_in):
    w = numpy.zeros((n_out, n_in))
    b = numpy.zeros((n_out, 1))

    def forward(x):
        y = w @ x + b
        def backward(dy, optimizer):
            dx = w.t @ dy
            dw = numpy.einsum('ik,jk->ij', dy, x)
            db = dy.sum(axis=0)

            optimizer(w, dw)
            optimizer(b, db)

            return dx
        return y, backward
    return forward

   if we call wb = create_linear_layer(5, 4), the variable wb will be the
   forward() function, implemented inside the body of
   create_linear_layer(). the wb instance will have access to the w and b
   variable defined in its outer scope. if we invoke create_linear_layer()
   again, we get a new instance, with its own internal state.

   the wb instance and the relu function have exactly the same signature.
   this makes it easy to write higher order functions to compose them. the
   most obvious thing to do is chain them together:
def chain(*layers):
    def forward(x):
        backprops = []
        y = x
        for layer in layers:
            y, backprop = layer(y)
            backprops.append(backprop)
        def backward(dy, optimizer):
            for backprop in reversed(backprops):
                dy = backprop(dy, optimizer)
            return dy
        return y, backward
    return forward

   we could now chain our linear layer together with the relu activation,
   to create a simple feed-forward network:
wb1 = create_linear_layer(10, 5)
wb2 = create_linear_layer(3, 10)

model = chain(wb1, relu, wb2)

x = numpy.random.uniform(size=(5, 4))

y, bp_y = model(x)

dy = y - truth
dx = bp_y(dy, optimizer)

   this conceptual model makes thinc very flexible. the trade-off is that
   thinc is less convenient and efficient at workloads that fit exactly
   into what [110]tensorflow etc. are designed for. if your graph really
   is static, and your inputs are homogenous in size and shape, [111]keras
   will likely be faster and simpler. but if you want to pass normal
   python objects through your network, or handle sequences and recursions
   of arbitrary length or complexity, you might find thinc's design a
   better fit for your problem.

quickstart

   thinc should install cleanly with both [112]pip and [113]conda, for
   pythons 2.7+ and 3.5+, on linux, macos / osx and windows. its only
   system dependency is a compiler tool-chain (e.g. build-essential) and
   the python development headers (e.g. python-dev).
pip install thinc

   for gpu support, we're grateful to use the work of chainer's cupy
   module, which provides a numpy-compatible interface for gpu arrays.
   however, installing chainer when no gpu is available currently causes
   an error. we therefore do not list chainer as an explicit dependency    
   so building thinc for gpu requires some extra steps:
export cuda_home=/usr/local/cuda-8.0 # or wherever your cuda is
export path=$path:$cuda_home/bin
pip install chainer
python -c "import cupy; assert cupy" # check it installed
pip install thinc_gpu_ops thinc # or `thinc[cuda]`
python -c "import thinc_gpu_ops" # check the gpu ops were built

   the rest of this section describes how to build thinc from source. if
   you have [114]fabric installed, you can use the shortcut:
git clone https://github.com/explosion/thinc
cd thinc
fab clean env make test

   you can then run the examples as follows:
fab eg.mnist
fab eg.basic_tagger
fab eg.id98_tagger

   otherwise, you can build and test explicitly with:
git clone https://github.com/explosion/thinc
cd thinc

virtualenv .env
source .env/bin/activate

pip install -r requirements.txt
python setup.py build_ext --inplace
py.test thinc/

   and then run the examples as follows:
python examples/mnist.py
python examples/basic_tagger.py
python examples/id98_tagger.py

usage

   the neural network api is still subject to change, even within minor
   versions. you can get a feel for the current api by checking out the
   examples. here are a few quick highlights.

1. shape id136

   models can be created with some dimensions unspecified. missing
   dimensions are inferred when pre-trained weights are loaded or when
   training begins. this eliminates a common source of programmer error:
# invalid network     shape mismatch
model = chain(relu(512, 748), relu(512, 784), softmax(10))

# leave the dimensions unspecified, and you can't be wrong.
model = chain(relu(512), relu(512), softmax())

2. operator overloading

   the model.define_operators() classmethod allows you to bind arbitrary
   binary functions to python operators, for use in any model instance.
   the method can (and should) be used as a context-manager, so that the
   overloading is limited to the immediate block. this allows concise and
   expressive model definition:
with model.define_operators({'>>': chain}):
    model = relu(512) >> relu(512) >> softmax()

   the overloading is cleaned up at the end of the block. a fairly
   arbitrary zoo of functions are currently implemented. some of the most
   useful:
     * chain(model1, model2): compose two models f(x) and g(x) into a
       single model computing g(f(x)).
     * clone(model1, int): create n copies of a model, each with distinct
       weights, and chain them together.
     * concatenate(model1, model2): given two models with output
       dimensions (n,) and (m,), construct a model with output dimensions
       (m+n,).
     * add(model1, model2): add(f(x), g(x)) = f(x)+g(x)
     * make_tuple(model1, model2): construct tuples of the outputs of two
       models, at the batch level. the backward pass expects to receive a
       tuple of gradients, which are routed through the appropriate model,
       and summed.

   putting these things together, here's the sort of tagging model that
   thinc is designed to make easy.
with model.define_operators({'>>': chain, '**': clone, '|': concatenate}):
    model = (
        add_eol_markers('eol')
        >> flatten
        >> memoize(
            charlstm(char_width)
            | (normalize >> str2int >> embed(word_width)))
        >> extractwindow(nw=2)
        >> batchnorm(relu(hidden_width)) ** 3
        >> softmax()
    )

   not all of these pieces are implemented yet, but hopefully this shows
   where we're going. the memoize function will be particularly important:
   in any batch of text, the common words will be very common. it's
   therefore important to evaluate models such as the charlstm once per
   word type per minibatch, rather than once per token.

3. callback-based id26

   most neural network libraries use a computational graph abstraction.
   this takes the execution away from you, so that gradients can be
   computed automatically. thinc follows a style more like the autograd
   library, but with larger operations. usage is as follows:
def explicit_sgd_update(x, y):
    sgd = lambda weights, gradient: weights - gradient * 0.001
    yh, finish_update = model.begin_update(x, drop=0.2)
    finish_update(y-yh, sgd)

   separating the id26 into three parts like this has many
   advantages. the interface to all models is completely uniform     there
   is no distinction between the top-level model you use as a predictor
   and the internal models for the layers. we also make concurrency
   simple, by making the begin_update() step a pure function, and
   separating the accumulation of the gradient from the action of the
   optimizer.

4. class annotations

   to keep the class hierarchy shallow, thinc uses class decorators to
   reuse code for layer definitions. specifically, the following
   decorators are available:
     * describe.attributes(): allows attributes to be specified by keyword
       argument. used especially for dimensions and parameters.
     * describe.on_init(): allows callbacks to be specified, which will be
       called at the end of the __init__.py.
     * describe.on_data(): allows callbacks to be specified, which will be
       called on model.begin_training().

     changelog

   version date description
   [115]v7.0.3 2019-03-15 fix pruning in id125
   [116]v7.0.2 2019-02-23 fix regression in linear model class
   [117]v7.0.1 2019-02-16 fix import errors
   [118]v7.0.0 2019-02-15 overhaul package dependencies
   [119]v6.12.1 2018-11-30 fix msgpack pin
   [120]v6.12.0 2018-10-15 wheels and separate gpu ops
   [121]v6.10.3 2018-07-21 python 3.7 support and dependency updates
   [122]v6.11.2 2018-05-21 improve gpu installation
   [123]v6.11.1 2018-05-20 support direct linkage to blas libraries
   v6.11.0 2018-03-16 n/a
   [124]v6.10.2 2017-12-06 efficiency improvements and bug fixes
   [125]v6.10.1 2017-11-15 fix gpu install and minor memory leak
   [126]v6.10.0 2017-10-28 cpu efficiency improvements, refactoring
   [127]v6.9.0 2017-10-03 reorganize layers, bug fix to layer
   id172
   [128]v6.8.2 2017-09-26 fix packaging of gpu_ops
   [129]v6.8.1 2017-08-23 fix windows support
   [130]v6.8.0 2017-07-25 selu layer, attention, improved gpu/cpu
   compatibility
   [131]v6.7.3 2017-06-05 fix convolution on gpu
   [132]v6.7.2 2017-06-02 bug fixes to serialization
   [133]v6.7.1 2017-06-02 improve serialization
   [134]v6.7.0 2017-06-01 fixes to serialization, hash embeddings and
   flatten ops
   [135]v6.6.0 2017-05-14 improved gpu usage and examples
   v6.5.2 2017-03-20 n/a
   [136]v6.5.1 2017-03-20 improved linear class and windows fix
   [137]v6.5.0 2017-03-11 supervised similarity, fancier embedding and
   improvements to linear model
   v6.4.0 2017-02-15 n/a
   [138]v6.3.0 2017-01-25 efficiency improvements, argument checking and
   error messaging
   [139]v6.2.0 2017-01-15 improve api and introduce overloaded operators
   [140]v6.1.3 2017-01-10 more neural network functions and training
   continuation
   v6.1.2 2017-01-09 n/a
   v6.1.1 2017-01-09 n/a
   v6.1.0 2017-01-09 n/a
   [141]v6.0.0 2016-12-31 add thinc.neural for nlp-oriented deep learning

     *    2019 github, inc.
     * [142]terms
     * [143]privacy
     * [144]security
     * [145]status
     * [146]help

     * [147]contact github
     * [148]pricing
     * [149]api
     * [150]training
     * [151]blog
     * [152]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [153]reload to refresh your
   session. you signed out in another tab or window. [154]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/explosion/thinc/commits/master.atom
   3. https://github.com/explosion/thinc#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/explosion/thinc
  32. https://github.com/join
  33. https://github.com/login?return_to=/explosion/thinc
  34. https://github.com/explosion/thinc/watchers
  35. https://github.com/login?return_to=/explosion/thinc
  36. https://github.com/explosion/thinc/stargazers
  37. https://github.com/login?return_to=/explosion/thinc
  38. https://github.com/explosion/thinc/network/members
  39. https://github.com/explosion
  40. https://github.com/explosion/thinc
  41. https://github.com/explosion/thinc
  42. https://github.com/explosion/thinc/issues
  43. https://github.com/explosion/thinc/pulls
  44. https://github.com/explosion/thinc/pulse
  45. https://github.com/join?source=prompt-code
  46. https://github.com/topics/machine-learning
  47. https://github.com/topics/deep-learning
  48. https://github.com/topics/artificial-intelligence
  49. https://github.com/topics/ai
  50. https://github.com/topics/python
  51. https://github.com/topics/nlp
  52. https://github.com/topics/natural-language-processing
  53. https://github.com/topics/spacy
  54. https://github.com/topics/machine-learning-library
  55. https://github.com/explosion/thinc/commits/master
  56. https://github.com/explosion/thinc/branches
  57. https://github.com/explosion/thinc/releases
  58. https://github.com/explosion/thinc/graphs/contributors
  59. https://github.com/explosion/thinc/blob/master/license
  60. https://github.com/explosion/thinc/search?l=python
  61. https://github.com/explosion/thinc/search?l=c
  62. https://github.com/explosion/thinc/search?l=c++
  63. https://github.com/explosion/thinc/search?l=cuda
  64. https://github.com/explosion/thinc/find/master
  65. https://github.com/explosion/thinc/archive/master.zip
  66. https://github.com/login?return_to=https://github.com/explosion/thinc
  67. https://github.com/join?return_to=/explosion/thinc
  68. https://desktop.github.com/
  69. https://desktop.github.com/
  70. https://developer.apple.com/xcode/
  71. https://visualstudio.github.com/
  72. https://github.com/honnibal
  73. https://github.com/explosion/thinc/commits?author=honnibal
  74. https://github.com/explosion/thinc/commit/90129be5f0d6c665344245a7c37dbe1b8afceea2
  75. https://github.com/explosion/thinc/commit/90129be5f0d6c665344245a7c37dbe1b8afceea2
  76. https://github.com/explosion/thinc/tree/90129be5f0d6c665344245a7c37dbe1b8afceea2
  77. https://github.com/explosion/thinc/tree/master/.buildkite
  78. https://github.com/explosion/thinc/tree/master/bin
  79. https://github.com/explosion/thinc/commit/58e4087e93b93c5b270b15c265929916c7c54265
  80. https://github.com/explosion/thinc/tree/master/examples
  81. https://github.com/explosion/thinc/commit/760024b1e451ae7c41a7bda6b50a07aadd942708
  82. https://github.com/explosion/thinc/tree/master/fabfile
  83. https://github.com/explosion/thinc/tree/master/include
  84. https://github.com/explosion/thinc/commit/d5f4b821e665317f2749f0c456e949efcb8e0592
  85. https://github.com/explosion/thinc/tree/master/thinc
  86. https://github.com/explosion/thinc/commit/90129be5f0d6c665344245a7c37dbe1b8afceea2
  87. https://github.com/explosion/thinc/blob/master/.build.cmd
  88. https://github.com/explosion/thinc/blob/master/.coveragerc
  89. https://github.com/explosion/thinc/blob/master/.flake8
  90. https://github.com/explosion/thinc/blob/master/.gitignore
  91. https://github.com/explosion/thinc/blob/master/license
  92. https://github.com/explosion/thinc/commit/e0a28e3ba5eff604235dfb144e2c349b668b6690
  93. https://github.com/explosion/thinc/blob/master/manifest.in
  94. https://github.com/explosion/thinc/blob/master/readme.md
  95. https://github.com/explosion/thinc/blob/master/azure-pipelines.yml
  96. https://github.com/explosion/thinc/blob/master/examples-requirements.txt
  97. https://github.com/explosion/thinc/blob/master/requirements.txt
  98. https://github.com/explosion/thinc/blob/master/setup.py
  99. https://explosion.ai/
 100. https://spacy.io/
 101. https://spacy.io/usage/v2
 102. https://explosion.ai/blog/deep-learning-formula-nlp
 103. https://github.com/explosion/thinc/releases/
 104. https://dev.azure.com/explosion-ai/public/_build?definitionid=7
 105. https://github.com/explosion/thinc/releases
 106. https://pypi.python.org/pypi/thinc
 107. https://anaconda.org/conda-forge/thinc
 108. https://github.com/explosion/wheelwright/releases
 109. https://twitter.com/explosion_ai
 110. https://www.tensorflow.org/
 111. https://keras.io/
 112. http://pypi.python.org/pypi/thinc
 113. https://anaconda.org/conda-forge/thinc
 114. http://www.fabfile.org/
 115. https://github.com/explosion/thinc/releases/tag/v7.0.3
 116. https://github.com/explosion/thinc/releases/tag/v7.0.2
 117. https://github.com/explosion/thinc/releases/tag/v7.0.1
 118. https://github.com/explosion/thinc/releases/tag/v7.0.0
 119. https://github.com/explosion/thinc/releases/tag/v6.12.1
 120. https://github.com/explosion/thinc/releases/tag/v6.12.0
 121. https://github.com/explosion/thinc/releases/tag/v6.10.3
 122. https://github.com/explosion/thinc/releases/tag/v6.11.2
 123. https://github.com/explosion/thinc/releases/tag/v6.11.1
 124. https://github.com/explosion/thinc/releases/tag/v6.10.2
 125. https://github.com/explosion/thinc/releases/tag/v6.10.1
 126. https://github.com/explosion/thinc/releases/tag/v6.10.0
 127. https://github.com/explosion/thinc/releases/tag/v6.9.0
 128. https://github.com/explosion/thinc/releases/tag/v6.8.2
 129. https://github.com/explosion/thinc/releases/tag/v6.8.1
 130. https://github.com/explosion/thinc/releases/tag/v6.8.0
 131. https://github.com/explosion/thinc/releases/tag/v6.7.3
 132. https://github.com/explosion/thinc/releases/tag/v6.7.2
 133. https://github.com/explosion/thinc/releases/tag/v6.7.1
 134. https://github.com/explosion/thinc/releases/tag/v6.7.0
 135. https://github.com/explosion/thinc/releases/tag/v6.6.0
 136. https://github.com/explosion/thinc/releases/tag/v6.5.1
 137. https://github.com/explosion/thinc/releases/tag/v6.5.0
 138. https://github.com/explosion/thinc/releases/tag/v6.3.0
 139. https://github.com/explosion/thinc/releases/tag/v6.2.0
 140. https://github.com/explosion/thinc/releases/tag/v6.1.3
 141. https://github.com/explosion/thinc/releases/tag/v6.0.0
 142. https://github.com/site/terms
 143. https://github.com/site/privacy
 144. https://github.com/security
 145. https://githubstatus.com/
 146. https://help.github.com/
 147. https://github.com/contact
 148. https://github.com/pricing
 149. https://developer.github.com/
 150. https://training.github.com/
 151. https://github.blog/
 152. https://github.com/about
 153. https://github.com/explosion/thinc
 154. https://github.com/explosion/thinc

   hidden links:
 156. https://github.com/
 157. https://github.com/explosion/thinc
 158. https://github.com/explosion/thinc
 159. https://github.com/explosion/thinc
 160. https://help.github.com/articles/which-remote-url-should-i-use
 161. https://github.com/explosion/thinc#thinc-practical-machine-learning-for-nlp-in-python
 162. https://github.com/explosion/thinc#whats-where-as-of-v690
 163. https://github.com/explosion/thinc#development-status
 164. https://github.com/explosion/thinc#no-computational-graph--just-higher-order-functions
 165. https://github.com/explosion/thinc#quickstart
 166. https://github.com/explosion/thinc#usage
 167. https://github.com/explosion/thinc#1-shape-id136
 168. https://github.com/explosion/thinc#2-operator-overloading
 169. https://github.com/explosion/thinc#3-callback-based-id26
 170. https://github.com/explosion/thinc#4-class-annotations
 171. https://github.com/explosion/thinc#-changelog
 172. https://github.com/
