   #[1]github [2]recent commits to
   attention-is-all-you-need-pytorch:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]74
     * [35]star [36]2,343
     * [37]fork [38]582

[39]jadore801120/[40]attention-is-all-you-need-pytorch

   [41]code [42]issues 37 [43]pull requests 9 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   a pytorch implementation of the transformer model in "attention is all
   you need".
   [47]attention [48]deep-learning [49]attention-is-all-you-need
   [50]pytorch [51]nlp [52]natural-language-processing
     * [53]154 commits
     * [54]1 branch
     * [55]0 releases
     * [56]fetching contributors
     * [57]mit

    1. [58]python 100.0%

   (button) python
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/j
   [60]download zip

downloading...

   want to be notified of new releases in
   jadore801120/attention-is-all-you-need-pytorch?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   [67]@jadore801120
   [68]jadore801120 [69]fix translation gpu error, thx to
   [70]@dksehdals216
   latest commit [71]20f355e aug 29, 2018
   [72]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [73]transformer [74]fix translation gpu error, thx to [75]@dksehdals216
   aug 29, 2018
   [76].gitignore
   [77]license
   [78]readme.md
   [79]dataset.py
   [80]preprocess.py [81]0.4 aug 25, 2018
   [82]train.py [83]merge branch 'master' of
   [84]https://github.com/jadore801120/attention-is    aug 26, 2018
   [85]translate.py [86]fix translation gpu error, thx to
   [87]@dksehdals216 aug 29, 2018

readme.md

attention is all you need: a pytorch implementation

   this is a pytorch implementation of the transformer model in
   "[88]attention is all you need" (ashish vaswani, noam shazeer, niki
   parmar, jakob uszkoreit, llion jones, aidan n. gomez, lukasz kaiser,
   illia polosukhin, arxiv, 2017).

   a novel sequence to sequence framework utilizes the self-attention
   mechanism, instead of convolution operation or recurrent structure, and
   achieve the state-of-the-art performance on wmt 2014 english-to-german
   translation task. (2017/06/12)

     the official tensorflow implementation can be found in:
     [89]tensorflow/tensor2tensor.

     to learn more about self-attention mechanism, you could read "[90]a
     structured self-attentive sentence embedding".

       [91][687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67]

   the project support training and translation with trained model now.

   note that this project is still a work in progress.

   if there is any suggestion or error, feel free to fire an issue to let
   me know. :)

requirement

     * python 3.4+
     * pytorch 0.4.1+
     * tqdm
     * numpy

usage

some useful tools:

   the example below uses the moses tokenizer
   ([92]http://www.statmt.org/moses/) to prepare the data and the moses
   id7 script for evaluation.
wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/tok
enizer/tokenizer.perl
wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/sha
re/nonbreaking_prefixes/nonbreaking_prefix.de
wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/sha
re/nonbreaking_prefixes/nonbreaking_prefix.en
sed -i "s/$realbin\/..\/share\/nonbreaking_prefixes//" tokenizer.perl
wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/gen
eric/multi-id7.perl

wmt'16 multimodal translation: multi30k (de-en)

   an example of training for the wmt'16 multimodal translation task
   ([93]http://www.statmt.org/wmt16/multimodal-task.html).

0) download the data.

mkdir -p data/multi30k
wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz &&  tar -xf
 training.tar.gz -c data/multi30k && rm training.tar.gz
wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz && tar -x
f validation.tar.gz -c data/multi30k && rm validation.tar.gz
wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz &&
tar -xf mmt16_task1_test.tar.gz -c data/multi30k && rm mmt16_task1_test.tar.gz

1) preprocess the data.

for l in en de; do for f in data/multi30k/*.$l; do if [[ "$f" != *"test"* ]]; th
en sed -i "$ d" $f; fi;  done; done
for l in en de; do for f in data/multi30k/*.$l; do perl tokenizer.perl -a -no-es
cape -l $l -q  < $f > $f.atok; done; done
python preprocess.py -train_src data/multi30k/train.en.atok -train_tgt data/mult
i30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k
/val.de.atok -save_data data/multi30k.atok.low.pt

2) train the model

python train.py -data data/multi30k.atok.low.pt -save_model trained -save_mode b
est -proj_share_weight -label_smoothing

     if your source and target language share one common vocabulary, use
     the -embs_share_weight flag to enable the model to share
     source/target id27.

3) test the model

python translate.py -model trained.chkpt -vocab data/multi30k.atok.low.pt -src d
ata/multi30k/test.en.atok -no_cuda
     __________________________________________________________________

performance

training

      [94][68747470733a2f2f696d6775722e636f6d2f724b65503162622e706e67]
      [95][68747470733a2f2f696d6775722e636f6d2f396a65335836552e706e67]
     * parameter settings:
          + default parameter and optimizer settings
          + label smoothing
          + target embedding / pre-softmax linear layer weight sharing.
     * elapse per epoch (on nvidia titan x):
          + training set: 0.888 minutes
          + validation set: 0.011 minutes

testing

     * coming soon.
     __________________________________________________________________

todo

     * evaluation on the generated text.
     * attention weight plot.
     __________________________________________________________________

acknowledgement

     * the project structure, some scripts and the dataset preprocessing
       steps are heavily borrowed from [96]openid4/openid4-py.
     * thanks for the suggestions from @srush, @iamalbert and @zijianzhao.

     *    2019 github, inc.
     * [97]terms
     * [98]privacy
     * [99]security
     * [100]status
     * [101]help

     * [102]contact github
     * [103]pricing
     * [104]api
     * [105]training
     * [106]blog
     * [107]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [108]reload to refresh your
   session. you signed out in another tab or window. [109]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commits/master.atom
   3. https://github.com/jadore801120/attention-is-all-you-need-pytorch#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/jadore801120/attention-is-all-you-need-pytorch
  32. https://github.com/join
  33. https://github.com/login?return_to=/jadore801120/attention-is-all-you-need-pytorch
  34. https://github.com/jadore801120/attention-is-all-you-need-pytorch/watchers
  35. https://github.com/login?return_to=/jadore801120/attention-is-all-you-need-pytorch
  36. https://github.com/jadore801120/attention-is-all-you-need-pytorch/stargazers
  37. https://github.com/login?return_to=/jadore801120/attention-is-all-you-need-pytorch
  38. https://github.com/jadore801120/attention-is-all-you-need-pytorch/network/members
  39. https://github.com/jadore801120
  40. https://github.com/jadore801120/attention-is-all-you-need-pytorch
  41. https://github.com/jadore801120/attention-is-all-you-need-pytorch
  42. https://github.com/jadore801120/attention-is-all-you-need-pytorch/issues
  43. https://github.com/jadore801120/attention-is-all-you-need-pytorch/pulls
  44. https://github.com/jadore801120/attention-is-all-you-need-pytorch/projects
  45. https://github.com/jadore801120/attention-is-all-you-need-pytorch/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/attention
  48. https://github.com/topics/deep-learning
  49. https://github.com/topics/attention-is-all-you-need
  50. https://github.com/topics/pytorch
  51. https://github.com/topics/nlp
  52. https://github.com/topics/natural-language-processing
  53. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commits/master
  54. https://github.com/jadore801120/attention-is-all-you-need-pytorch/branches
  55. https://github.com/jadore801120/attention-is-all-you-need-pytorch/releases
  56. https://github.com/jadore801120/attention-is-all-you-need-pytorch/graphs/contributors
  57. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/license
  58. https://github.com/jadore801120/attention-is-all-you-need-pytorch/search?l=python
  59. https://github.com/jadore801120/attention-is-all-you-need-pytorch/find/master
  60. https://github.com/jadore801120/attention-is-all-you-need-pytorch/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/jadore801120/attention-is-all-you-need-pytorch
  62. https://github.com/join?return_to=/jadore801120/attention-is-all-you-need-pytorch
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/jadore801120
  68. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commits?author=jadore801120
  69. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/20f355eb655bad40195ae302b9d8036716be9a23
  70. https://github.com/dksehdals216
  71. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/20f355eb655bad40195ae302b9d8036716be9a23
  72. https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/20f355eb655bad40195ae302b9d8036716be9a23
  73. https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master/transformer
  74. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/20f355eb655bad40195ae302b9d8036716be9a23
  75. https://github.com/dksehdals216
  76. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/.gitignore
  77. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/license
  78. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/readme.md
  79. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/dataset.py
  80. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/preprocess.py
  81. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/b588f42b7a9480f95ac1e33ba62b31d7aa28f4ab
  82. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/train.py
  83. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/bdbd16d58f28096938b3fb3421f52619ced20520
  84. https://github.com/jadore801120/attention-is-all-you-need-pytorch
  85. https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/translate.py
  86. https://github.com/jadore801120/attention-is-all-you-need-pytorch/commit/20f355eb655bad40195ae302b9d8036716be9a23
  87. https://github.com/dksehdals216
  88. https://arxiv.org/abs/1706.03762
  89. https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
  90. https://arxiv.org/abs/1703.03130
  91. https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67
  92. http://www.statmt.org/moses/
  93. http://www.statmt.org/wmt16/multimodal-task.html
  94. https://camo.githubusercontent.com/4e87efa1ae7626d32f0a484ab94de18e115bafbf/68747470733a2f2f696d6775722e636f6d2f724b65503162622e706e67
  95. https://camo.githubusercontent.com/09a2d12d526119d3034b432cc0a770fbaac74fc5/68747470733a2f2f696d6775722e636f6d2f396a65335836552e706e67
  96. https://github.com/openid4/openid4-py
  97. https://github.com/site/terms
  98. https://github.com/site/privacy
  99. https://github.com/security
 100. https://githubstatus.com/
 101. https://help.github.com/
 102. https://github.com/contact
 103. https://github.com/pricing
 104. https://developer.github.com/
 105. https://training.github.com/
 106. https://github.blog/
 107. https://github.com/about
 108. https://github.com/jadore801120/attention-is-all-you-need-pytorch
 109. https://github.com/jadore801120/attention-is-all-you-need-pytorch

   hidden links:
 111. https://github.com/
 112. https://github.com/jadore801120/attention-is-all-you-need-pytorch
 113. https://github.com/jadore801120/attention-is-all-you-need-pytorch
 114. https://github.com/jadore801120/attention-is-all-you-need-pytorch
 115. https://help.github.com/articles/which-remote-url-should-i-use
 116. https://github.com/jadore801120/attention-is-all-you-need-pytorch#attention-is-all-you-need-a-pytorch-implementation
 117. https://github.com/jadore801120/attention-is-all-you-need-pytorch#requirement
 118. https://github.com/jadore801120/attention-is-all-you-need-pytorch#usage
 119. https://github.com/jadore801120/attention-is-all-you-need-pytorch#some-useful-tools
 120. https://github.com/jadore801120/attention-is-all-you-need-pytorch#wmt16-multimodal-translation-multi30k-de-en
 121. https://github.com/jadore801120/attention-is-all-you-need-pytorch#0-download-the-data
 122. https://github.com/jadore801120/attention-is-all-you-need-pytorch#1-preprocess-the-data
 123. https://github.com/jadore801120/attention-is-all-you-need-pytorch#2-train-the-model
 124. https://github.com/jadore801120/attention-is-all-you-need-pytorch#3-test-the-model
 125. https://github.com/jadore801120/attention-is-all-you-need-pytorch#performance
 126. https://github.com/jadore801120/attention-is-all-you-need-pytorch#training
 127. https://github.com/jadore801120/attention-is-all-you-need-pytorch#testing
 128. https://github.com/jadore801120/attention-is-all-you-need-pytorch#todo
 129. https://github.com/jadore801120/attention-is-all-you-need-pytorch#acknowledgement
 130. https://github.com/
