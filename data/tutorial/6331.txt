   #[1]github [2]recent commits to text_classification:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]258
     * [35]star [36]4,258
     * [37]fork [38]1,650

[39]brightmart/[40]text_classification

   [41]code [42]issues 20 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   all kinds of text classification models and more with deep learning
   [47]classification [48]nlp [49]fasttext [50]textid98 [51]textid56
   [52]tensorflow [53]multi-label [54]multi-class [55]attention-mechanism
   [56]text-classification [57]convolutional-neural-networks
   [58]sentence-classification [59]memory-networks
     * [60]301 commits
     * [61]1 branch
     * [62]0 releases
     * [63]fetching contributors
     * [64]mit

    1. [65]python 94.3%
    2. [66]jupyter notebook 5.7%

   (button) python jupyter notebook
   branch: master (button) new pull request
   [67]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/b
   [68]download zip

downloading...

   want to be notified of new releases in brightmart/text_classification?
   [69]sign in [70]sign up

launching github desktop...

   if nothing happens, [71]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [72]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [73]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [74]download the github extension for visual studio
   and try again.

   (button) go back
   [75]@brightmart
   [76]brightmart [77]update readme.md
   latest commit [78]0134a06 feb 8, 2019
   [79]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [80]a00_bert [81]update nov 24, 2018
   [82]a00_boosting [83]import numpy as np for lines 22 and 49 oct 31,
   2018
   [84]a01_fasttext [85]update nov 19, 2018
   [86]a02_textid98
   [87]a03_textid56 [88]using the last hidden state for fc may 10, 2018
   [89]a04_textrid98 [90]change structure jul 12, 2017
   [91]a05_hierarchicalattentionnetwork [92]comment out test() under
   p1_hierarchicalattention_model_transformer.py dec 20, 2018
   [93]a06_id195withattention [94]change structure jul 12, 2017
   [95]a07_transformer
   [96]a08_entitynetwork [97]add bi-directional encoder for story and
   query jul 12, 2017
   [98]a09_dynamicmemorynet [99]delete add feb 24, 2018
   [100]aa1_data_util [101]update nov 18, 2018
   [102]aa2_classificationtflearn [103]change structure jul 12, 2017
   [104]aa3_id98sentenceclassificationtflearn [105]change structure jul 12,
   2017
   [106]aa4_textid98_with_rid98 [107]change structure jul 12, 2017
   [108]aa5_bilstmtextrelation [109]change structure jul 12, 2017
   [110]aa6_twoid98textrelation [111]change structure jul 12, 2017
   [112]data
   [113]images [114]update nov 16, 2018
   [115].travis.yml
   [116]license.md
   [117]readme.md
   [118]a08_predict_ensemble.py [119]ensemble for prediction sep 6, 2017
   [120]multi-label-classification.pdf
   [121]pre-processing.ipynb
   [122]sample_data.zip

readme.md

text classification

   the purpose of this repository is to explore text classification
   methods in nlp with deep learning.

update:

   try bert model for multi-label classification, please check session
   #models detail, 3) bert.

   [123]large amount of chinese corpus for nlp available!

   google's bert achieved new state of art result on more than 10 tasks in
   nlp using pre-train in language model then

   fine-tuning. [124]pre-train texid98: idea from bert for language
   understanding with running code and data set

introduction

   it has all kinds of baseline models for text classification.

   it also support for multi-label classification where multi labels
   associate with an sentence or document.

   although many of these models are simple, and may not get you to top
   level of the task. but some of these models are very

   classic, so they may be good to serve as baseline models. each model
   has a test function under model class. you can run

   it to performance toy task first. the model is independent from data
   set.

   [125]check here for formal report of large scale multi-label text
   classification with deep learning

   several models here can also be used for modelling id53
   (with or without context), or to do sequences generating.

   we explore two id195 model(id195 with
   attention,transformer-attention is all you need) to do text
   classification.

   and these two models can also be used for sequences generating and
   other tasks. if your task is a multi-label classification,

   you can cast the problem to sequences generating.

   we implement two memory network. one is dynamic memory network.
   previously it reached state of art in question

   answering, id31 and sequence generating tasks. it is so
   called one model to do several different tasks,

   and reach high performance. it has four modules. the key component is
   episodic memory module. it use gate mechanism to

   performance attention, and use gated-gru to update episode memory, then
   it has another gru( in a vertical direction) to

   performance hidden state update. it has ability to do transitive
   id136.

   the second memory network we implemented is recurrent entity network:
   tracking state of the world. it has blocks of

   key-value pairs as memory, run in parallel, which achieve new state of
   art. it can be used for modelling question

   answering with contexts(or history). for example, you can let the model
   to read some sentences(as context), and ask a

   question(as query), then ask the model to predict an answer; if you
   feed story same as query, then it can do

   classification task.

models:

    1. fasttext
    2. textid98
    3. bert:pre-training of deep bidirectional transformers for language
       understanding
    4. textid56
    5. rid98
    6. hierarchical attention network
    7. id195 with attention
    8. transformer("attend is all you need")
    9. dynamic memory network
   10. entitynetwork:tracking state of the world
   11. ensemble models
   12. boosting:
       for a single model, stack identical models together. each layer is
       a model. the result will be based on logits added together. the
       only connection between layers are label's weights. the front
       layer's prediction error rate of each label will become weight for
       the next layers. those labels with high error rate will have big
       weight. so later layer's will pay more attention to those
       mis-predicted labels, and try to fix previous mistake of former
       layer. as a result, we will get a much strong model. check
       a00_boosting/boosting.py

   and other models:
    1. bilstmtextrelation;
    2. twoid98textrelation;
    3. bilstmtextrelationtwoid56

performance

   (mulit-label label prediction task,ask to prediction top5, 3 million
   training data,full score:0.5)
   model fasttext textid98 textid56 rid98 hierattenet id195attn entitynet
   dynamicmemory transformer
   score 0.362 0.405 0.358 0.395 0.398 0.322 0.400 0.392 0.322
   training 10m 2h 10h 2h 2h 3h 3h 5h 7h
     __________________________________________________________________

   bert model achieves 0.368 after first 9 epoch from validation set.

   ensemble of textid98,entitynet,dynamicmemory: 0.411

   ensemble entitynet,dynamicmemory: 0.403
     __________________________________________________________________

   notice:

   m stand for minutes; h stand for hours;

   hierattenet means hierarchical attention networkk;

   id195attn means id195 with attention;

   dynamicmemory means dynamicmemorynetwork;

   transformer stand for model from 'attention is all you need'.

usage:

    1. model is in xxx_model.py
    2. run python xxx_train.py to train the model
    3. run python xxx_predict.py to do id136(test).

   each model has a test method under the model class. you can run the
   test method first to check whether the model can work properly.
     __________________________________________________________________

environment:

   python 2.7+ tensorflow 1.8

   (tensorflow 1.1 to 1.13 should also works; most of models should also
   work fine in other tensorflow version, since we

   use very few features bond to certain version.

   if you use python3, it will be fine as long as you change print/try
   catch function in case you meet any error.

   textid98 model is already transfomed to python 3.6

sample data: [126]cached file

   to help you run this repository, currently we re-generate
   training/validation/test data and vocabulary/labels, and saved

   them as cache file using h5py. we suggest you to download it from above
   link.

   it contain everything you need to run this repository: data is
   pre-processed, you can start to train the model in a minute.

   it's a zip file about 1.8g, contains 3 million training data. although
   after unzip it's quite big, but with the help of

   hdf5, it only need a normal size of memory of computer(e.g.8 g or less)
   during training.

   we use jupyter notebook: [127]pre-processing.ipynb to pre-process data.
   you can have a better understanding of this task and

   data by taking a look of it. you can also generate data by yourself in
   the way your want, just change few lines of code

   using this jupyter notebook.

   if you want to try a model now, you can dowload cached file from above,
   then go to folder 'a02_textid98', run
 python  p7_textid98_train.py

   it will use data from cached files to train the model, and print loss
   and f1 score periodically.

   old sample data source: if you need some sample data and id27
   per-trained on id97, you can find it in closed issues, such as:
   [128]issue 3.

   you can also find some sample data at folder "data". it contains two
   files:'sample_single_label.txt', contains 50k data

   with single label; 'sample_multiple_label.txt', contains 20k data with
   multiple labels. input and label of is separate by " label".

   if you want to know more detail about data set of text classification
   or task these models can be used, one of choose is below:

   [129]https://biendata.com/competition/zhihu/

road map

   one way you can use this repository:

   step 1: you can read through this article. you will get a general idea
   of various classic models used to do text classification.

   step 2: pre-process data and/or download cached file.
  a. take a look a look of jupyter notebook('pre-processing.ipynb'), where you c
an familiar with this text

       classification task and data set. you will also know how we pre-process d
ata and generate training/validation/test

       set. there are a list of things you can try at the end of this jupyter.

   b. download zip file that contains cached files, so you will have all necessa
ry data, and can start to train models.

   step 3: run some of models list here, and change some codes and
   configurations as you want, to get a good performance.
  record performances, and things you done that works, and things that are not.

  for example, you can take this sequence to explore:

  1) fasttext---> 2)textid98---> 3)transformer---> 4)bert

   additionally, write your article about this topic, you can follow
   paper's style to write. you may need to read some papers
   on the way, many of these papers list in the # reference at the end of this a
rticle; or join  a machine learning

   competition, and apply it with what you've learned.

use your own data:

   replace data in 'data/sample_multiple_label.txt', and make sure format
   as below:

   'word1 word2 word3 __label__l1 __label__l2 __label__l3'

   where part1: 'word1 word2 word3' is input(x), part2: '__label__l1
   __label__l2 __label__l3'

   representing there are three labels: [l1,l2,l3]. between part1 and
   part2 there should be a empty string: ' '.

   for example: each line (multiple labels) like:

   'w5466 w138990 w1638 w4301 w6 w470 w202 c1834 c1400 c134 c57 c73 c699
   c317 c184 __label__5626661657638885119 __label__4921793805334628695
   __label__8904735555009151318'

   where '5626661657638885119','4921793805334628695'      8904735555009151318   
   are three labels associate with this input string 'w5466 w138990...c699
   c317 c184'

   notice:

   some util function is in data_util.py; check load_data_multilabel() of
   data_util for how process input and labels from raw data.

   there is a function to load and assign pretrained id27 to the
   model,where id27 is pretrained in id97 or fasttext.

pretrain work embedding:

   if id97.load not works, you may load pretrained id27,
   especially for chinese id27 use following lines:

   import gensim

   from gensim.models import keyedvectors

   id97_model = keyedvectors.load_id97_format(id97_model_path,
   binary=true, unicode_errors='ignore') #

   or you can turn off use pretrain id27 flag to false to
   disable loading id27.

models detail:

1.fasttext:

   implmentation of [130]bag of tricks for efficient text classification

   after embed each word in the sentence, this word representations are
   then averaged into a text representation, which is in turn fed to a
   linear classifier.it use softmax function to compute the id203
   distribution over the predefined classes. then cross id178 is used to
   compute loss. bag of word representation does not consider word order.
   in order to take account of word order, id165 features is used to
   capture some partial information about the local word order; when the
   number of classes is large, computing the linear classifier is
   computational expensive. so it usehierarchical softmax to speed
   training process.
    1. use bi-gram and/or tri-gram
    2. use nce loss to speed us softmax computation(not use hierarchy
       softmax as original paper)

   result: performance is as good as paper, speed also very fast.

   check: p5_fasttextb_model.py

[131]alt text

2.textid98:

   implementation of [132]convolutional neural networks for sentence
   classification

   structure:embedding--->conv--->max pooling--->fully connected
   layer-------->softmax

   check: p7_textid98_model.py

   in order to get very good result with textid98, you also need to read
   carefully about this paper [133]a sensitivity analysis of (and
   practitioners' guide to) convolutional neural networks for sentence
   classification: it give you some insights of things that can affect
   performance. although you need to change some settings according to
   your specific task.

   convolutional neural network is main building box for solve problems of
   id161. now we will show how id98 can be used for nlp, in in
   particular, text classification. sentence length will be different from
   one to another. so we will use pad to get fixed length, n. for each
   token in the sentence, we will use id27 to get a fixed
   dimension vector, d. so our input is a 2-dimension matrix:(n,d). this
   is similar with image for id98.

   firstly, we will do convolutional operation to our input. it is a
   element-wise multiply between filter and part of input. we use k number
   of filters, each filter size is a 2-dimension matrix (f,d). now the
   output will be k number of lists. each list has a length of n-f+1. each
   element is a scalar. notice that the second dimension will be always
   the dimension of id27. we are using different size of filters
   to get rich features from text inputs. and this is something similar
   with id165 features.

   secondly, we will do max pooling for the output of convolutional
   operation. for k number of lists, we will get k number of scalars.

   thirdly, we will concatenate scalars to form final features. it is a
   fixed-size vector. and it is independent from the size of filters we
   use.

   finally, we will use linear layer to project these features to
   per-defined labels.

   [134]alt text
     __________________________________________________________________

3.bert:

pre-training of deep bidirectional transformers for language understanding

   bert currently achieve state of art results on more than 10 nlp tasks.
   the key ideas behind this model is that we can

   pre-train the model by using one kind of language model with huge
   amount of raw data, where you can find it easily.

   as most of parameters of the model is pre-trained, only last layer for
   classifier need to be need for different tasks.

   as a result, this model is generic and very powerful. you can just
   fine-tuning based on the pre-trained model within

   a short period of time.

   however, this model is quite big. with sequence length 128, you may
   only able to train with a batch size of 32; for long

   document such as sequence length 512, it can only train a batch size 4
   for a normal gpu(with 11g); and very few people

   can pre-train this model from scratch, as it takes many days or weeks
   to train, and a normal gpu's memory is too small

   for this model.

   specially, the backbone model is transformer, where you can find it in
   attention is all you need. it use two kind of

   tasks to pre-train the model.

masked languge model

   generally speaking, given a sentence, some percentage of words are
   masked, you will need to predict the masked words

   based on this masked sentence. masked words are chosed randomly.

   we feed the input through a deep transformer encoder and then use the
   final hidden states corresponding to the masked

   positions to predict what word was masked, exactly like we would train
   a language model.
source_file each line is a sequence of token, can be a sentence.

input sequence  : the man went to [mask] store with [mask] dog
target sequence :                  the                his

next sentence prediction

   many language understanding task, like id53, id136,
   need understand relationship

   between sentence. however, language model is only able to understand
   without a sentence. next sentence

   prediction is a sample task to help model understand better in these
   kinds of task.

   50% of chance the second sentence is tbe next sentence of the first
   one, 50% of not the next one.

   given two sentence, the model is asked to predict whether the second
   sentence is real next sentence of

   the first one.
input : [cls] the man went to the store [sep] he bought a gallon of milk [sep]
label : isnext

input = [cls] the man heading to the store [sep] penguin [mask] are flight ##les
s birds [sep]
label = notnext

   [135][bert_1.jpeg]

   [136][bert_2.jpeg]

how to use bert?

   basically, you can download pre-trained model, can just fine-tuning on
   your task with your own data.

   for classification task, you can add processor to define the format you
   want to let input and labels from source data.

use bert for multi-label classification?

   run the following command under folder a00_bert:
  python  train_bert_multi-label.py

   it achieve 0.368 after 9 epoch. or you can run multi-label
   classification with downloadable data using bert from

   [137]sentiment_analysis_fine_grain with bert

use bert for online prediction

   you can use session and feed style to restore model and feed data, then
   get logits to make a online prediction.

   [138]online prediction with bert

   originally, it train or evaluate model based on file, not for online.

how to get better model for bert?

   firstly, you can use pre-trained model download from google. run a few
   epoch on you dataset, and find a suitable

   sequence length.

   secondly, you can pre-train the base model in your own data as long as
   you can find a dataset that is related to

   your task, then fine-tuning on your specific task.

   thirdly, you can change id168 and last layer to better suit for
   your task.

   additionally, you can add define some pre-trained tasks that will help
   the model understand your task much better.

   as experienced we got from experiments, pre-trained task is independent
   from model and pre-train is not limit to

   the tasks above.
     __________________________________________________________________

4.textid56

   structure v1:embedding--->bi-directional lstm--->concat
   output--->average----->softmax layer

   check: p8_textid56_model.py

   [139]alt text

   structure v2:embedding-->bi-directional lstm---->dropout-->concat
   ouput--->lstm--->droput-->fc layer-->softmax layer

   check: p8_textid56_model_multilayer.py

   [140]alt text
     __________________________________________________________________

5.bilstmtextrelation

   structure same as textid56. but input is special designed.
   e.g.input:"how much is the computer? eos price of laptop". where 'eos'
   is a special token spilted question1 and question2.

   check:p9_bilstmtextrelation_model.py
     __________________________________________________________________

6.twoid98textrelation

   structure: first use two different convolutional to extract feature of
   two sentences. then concat two features. use linear transform layer to
   out projection to target label, then softmax.

   check: p9_twoid98textrelation_model.py
     __________________________________________________________________

7.bilstmtextrelationtwoid56

   structure: one bi-directional lstm for one sentence(get output1),
   another bi-directional lstm for another sentence(get output2). then:
   softmax(output1moutput2)

   check:p9_bilstmtextrelationtwoid56_model.py

   for more detail you can go to: deep learning for chatbots, part 2    
   implementing a retrieval-based model in tensorflow
     __________________________________________________________________

8.rid98:

   recurrent convolutional neural network for text classification

   implementation of [141]recurrent convolutional neural network for text
   classification

   structure:1)recurrent structure (convolutional layer) 2)max pooling 3)
   fully connected layer+softmax

   it learn represenation of each word in the sentence or document with
   left side context and right side context:

   representation current
   word=[left_side_context_vector,current_word_embedding,right_side_contex
   t_vecotor].

   for left side context, it use a recurrent structure, a no-linearity
   transfrom of previous word and left side previous context; similarly to
   right side context.

   check: p71_textrid98_model.py

   [142]alt text
     __________________________________________________________________

9.hierarchical attention network:

   implementation of [143]hierarchical attention networks for document
   classification

   structure:
    1. embedding
    2. word encoder: word level bi-directional gru to get rich
       representation of words
    3. word attention:word level attention to get important information in
       a sentence
    4. sentence encoder: sentence level bi-directional gru to get rich
       representation of sentences
    5. sentence attetion: sentence level attention to get important
       sentence among sentences
    6. fc+softmax

   [144]alt text

   in nlp, text classification can be done for single sentence, but it can
   also be used for multiple sentences. we may call it document
   classification. words are form to sentence. and sentence are form to
   document. in this circumstance, there may exists a intrinsic structure.
   so how can we model this kinds of task? does all parts of document are
   equally relevant? and how we determine which part are more important
   than another?

   it has two unique features:

   1)it has a hierarchical structure that reflect the hierarchical
   structure of documents;

   2)it has two levels of attention mechanisms used at the word and
   sentence-level. it enable the model to capture important information in
   different levels.

   word encoder: for each words in a sentence, it is embedded into word
   vector in distribution vector space. it use a bidirectional gru to
   encode the sentence. by concatenate vector from two direction, it now
   can form a representation of the sentence, which also capture
   contextual information.

   word attention: same words are more important than another for the
   sentence. so attention mechanism is used. it first use one layer mlp to
   get uit hidden representation of the sentence, then measure the
   importance of the word as the similarity of uit with a word level
   context vector uw and get a normalized importance through a softmax
   function.

   sentence encoder: for sentence vectors, bidirectional gru is used to
   encode it. similarly to word encoder.

   sentence attention: sentence level vector is used to measure importance
   among sentences. similarly to word attention.

   input of data:

   generally speaking, input of this model should have serveral sentences
   instead of sinle sentence. shape is:[none,sentence_lenght]. where none
   means the batch_size.

   in my training data, for each example, i have four parts. each part has
   same length. i concat four parts to form one single sentence. the model
   will split the sentence into four parts, to form a tensor with
   shape:[none,num_sentence,sentence_length]. where num_sentence is number
   of sentences(equal to 4, in my setting).

   check:p1_hierarchicalattention_model.py

   for attentive attention you can check [145]attentive attention
     __________________________________________________________________

10.id195 with attention

   implementation id195 with attention derived from [146]neural machine
   translation by jointly learning to align and translate

   i.structure:

   1)embedding 2)bi-gru too get rich representation from source
   sentences(forward & backward). 3)decoder with attention.

   [147]alt text

   ii.input of data:

   there are two kinds of three kinds of inputs:1)encoder inputs, which is
   a sentence; 2)decoder inputs, it is labels list with fixed
   length;3)target labels, it is also a list of labels.

   for example, labels is:"l1 l2 l3 l4", then decoder inputs will
   be:[_go,l1,l2,l2,l3,_pad]; target label will
   be:[l1,l2,l3,l3,_end,_pad]. length is fixed to 6, any exceed labels
   will be trancated, will pad if label is not enough to fill.

   iii.attention mechanism:
    1. transfer encoder input list and hidden state of decoder
    2. calculate similiarity of hidden state with each encoder input, to
       get possibility distribution for each encoder input.
    3. weighted sum of encoder input based on possibility distribution.
       go though id56 cell using this weight sum together with decoder
       input to get new hidden state

   iv.how vanilla encoder decoder works:

   the source sentence will be encoded using id56 as fixed size vector
   ("thought vector"). then during decoder:
    1. when it is training, another id56 will be used to try to get a word
       by using this "thought vector" as init state, and take input from
       decoder input at each timestamp. decoder start from special token
       "_go". after one step is performanced, new hidden state will be get
       and together with new input, we can continue this process until we
       reach to a special token "_end". we can calculate loss by compute
       cross id178 loss of logits and target label. logits is get
       through a projection layer for the hidden state(for output of
       decoder step(in gru we can just use hidden states from decoder as
       output).
    2. when it is testing, there is no label. so we should feed the output
       we get from previous timestamp, and continue the process util we
       reached "_end" token.

   v.notices:
    1. here i use two kinds of vocabularies. one is from words,used by
       encoder; another is for labels,used by decoder
    2. for vocabulary of lables, i insert three special
       token:"_go","_end","_pad"; "_unk" is not used, since all labels is
       pre-defined.
     __________________________________________________________________

11.transformer("attention is all you need")

   status: it was able to do task classification. and able to generate
   reverse order of its sequences in toy task. you can check it by running
   test function in the model. check: a2_train_classification.py(train) or
   a2_transformer_classification.py(model)

   we do it in parallell style.layer id172,residual connection,
   and mask are also used in the model.

   for every building blocks, we include a test function in the each file
   below, and we've test each small piece successfully.

   sequence to sequence with attention is a typical model to solve
   sequence generation problem, such as translate, dialogue system. most
   of time, it use id56 as buidling block to do these tasks. util recently,
   people also apply convolutional neural network for sequence to sequence
   problem. transformer, however, it perform these tasks solely on
   attention mechansim. it is fast and achieve new state-of-art result.

   [148]alt text

   it also has two main parts: encoder and decoder. below is desc from
   paper:

   encoder:

   6 layers.each layers has two sub-layers. the first is multi-head
   self-attention mechanism; the second is position-wise fully connected
   feed-forward network. for each sublayer. use layernorm(x+sublayer(x)).
   all dimension=512.

   decoder:
    1. the decoder is composed of a stack of n= 6 identical layers.
    2. in addition to the two sub-layers in each encoder layer, the
       decoder inserts a third sub-layer, which performs multi-head
       attention over the output of the encoder stack.
    3. similar to the encoder, we employ residual connections around each
       of the sub-layers, followed by layer id172. we also modify
       the self-attention sub-layer in the decoder stack to prevent
       positions from attending to subsequent positions. this masking,
       combined with fact that the output embeddings are offset by one
       position, ensures that the predictions for position i can depend
       only on the known outputs at positions less than i.

   main take away from this model:
    1. multi-head self attention: use self attention, linear transform
       multi-times to get projection of key-values, then do ordinary
       attention; 2) some tricks to improve performance(residual
       connection,position encoding, poistion feed forward, label smooth,
       mask to ignore things we want to ignore).

   use this model to do task classification:

   here we only use encode part for task classification, removed resdiual
   connection, used only 1 layer.no need to use mask. we use multi-head
   attention and postionwise feed forward to extract features of input
   sentence, then use linear layer to project it to get logits.

   for detail of the model, please check: a2_transformer_classification.py
     __________________________________________________________________

12.recurrent entity network

   input:1. story: it is multi-sentences, as context. 2.query: a sentence,
   which is a question, 3. ansewr: a single label.

   model structure:
    1. input encoding: use bag of word to encode story(context) and
       query(question); take account of position by using position mask
       by using bi-directional id56 to encode story and query, performance
       boost from 0.392 to 0.398, increase 1.5%.
    2. dynamic memory:

   a. compute gate by using 'similiarity' of keys,values with input of
   story.

   b. get candidate hidden state by transform each key,value and input.

   c. combine gate and candidate hidden state to update current hidden
   state.
    3. output moudle( use attention mechanism): a. to get possibility
       distribution by computing 'similarity' of query and hidden state

   b. get weighted sum of hidden state using possibility distribution.

   c. non-linearity transform of query and hidden state to get predict
   label.

   [149]alt text

   main take away from this model:
    1. use blocks of keys and values, which is independent from each
       other. so it can be run in parallel.
    2. modelling context and question together. use memory to track state
       of world; and use non-linearity transform of hidden state and
       question(query) to make a prediction.
    3. simple model can also achieve very good performance. simple encode
       as use bag of word.

   for detail of the model, please check: a3_entity_network.py

   under this model, it has a test function, which ask this model to count
   numbers both for story(context) and query(question). but weights of
   story is smaller than query.
     __________________________________________________________________

13.dynamic memory network

   outlook of model:

   1.input module: encode raw texts into vector representation

   2.question module: encode question into vector representation

   3.episodic memory module: with inputs,it chooses which parts of inputs
   to focus on through the attention mechanism, taking into account of
   question and previous memory====>it poduce a 'memory' vecotr.

   4.answer module:generate an answer from the final memory vector.

   [150]alt text

   detail:

   1.input module:

   a.single sentence: use gru to get hidden state b.list of sentences: use
   gru to get the hidden states for each sentence. e.g. [hidden states
   1,hidden states 2, hidden states...,hidden state n]

   2.question module: use gru to get hidden state

   3.episodic memory module:

   use an attention mechanism and recurrent network to updates its memory.

   a. gate as attention mechanism:
 two-layer feed forward nueral network.input is candidate fact c,previous memory
 m and question q. features get by take: element-wise,matmul and absolute distan
ce of q with c, and q with m.

   b.memory update mechanism: take candidate sentence, gate and previous
   hidden state, it use gated-gru to update hidden state. like:
   h=f(c,h_previous,g). the final hidden state is the input for answer
   module.

   c.need for multiple episodes===>transitive id136.

   e.g. ask where is the football? it will attend to sentence of "john put
   down the football"), then in second pass, it need to attend location of
   john.

   4.answer module: take the final epsoidic memory, question, it update
   hidden state of answer module.

todo

   1.character-level convolutional networks for text classification

   2.convolutional neural networks for text categorization:shallow
   word-level vs. deep character-level

   3.very deep convolutional networks for text classification

   4.adversarial training methods for semi-supervised text classification

   5.ensemble models

conclusion:

   during the process of doing large scale of multi-label classification,
   serveral lessons has been learned, and some list as below:
    1. what is most important thing to reach a high accuracy? it depend
       the task you are doing. from the task we conducted here, we believe
       that ensemble models based on models trained from multiple features
       including word, character for title and description can help to
       reach very high accuarcy; however, in some cases,as just alphago
       zero demonstrated, algorithm is more important then data or
       computational power, in fact alphago zero did not use any humam
       data.
    2. is there a ceiling for any specific model or algorithm? the answer
       is yes. lots of different models were used here, we found many
       models have similiar performances, even though there are quite
       different in structure. in some extent, the difference of
       performance is not so big.
    3. is case study of error useful? i think it is quite useful
       especially when you have done many different things, but reached a
       limit. for example, by doing case study, you can find labels that
       models can make correct prediction, and where they make mistakes.
       and to imporove performance by increasing weights of these wrong
       predicted labels or finding potential errors from data.
    4. how can we become expert in a specific of machine learning? in my
       opinion,join a machine learning competation or begin a task with
       lots of data, then read papers and implement some, is a good
       starting point. so we will have some really experience and ideas of
       handling specific task, and know the challenges of it. but what's
       more important is that we should not only follow ideas from papers,
       but to explore some new ideas we think may help to slove the
       problem. for example, by changing structures of classic models or
       even invent some new structures, we may able to tackle the problem
       in a much better way as it may more suitable for task we are doing.

reference:

   1.bag of tricks for efficient text classification

   2.convolutional neural networks for sentence classification

   3.a sensitivity analysis of (and practitioners' guide to) convolutional
   neural networks for sentence classification

   4.deep learning for chatbots, part 2     implementing a retrieval-based
   model in tensorflow, from [151]www.wildml.com

   5.recurrent convolutional neural network for text classification

   6.hierarchical attention networks for document classification

   7.id4 by jointly learning to align and translate

   8.attention is all you need

   9.ask me anything:dynamic memory networks for natural language
   processing

   10.tracking the state of world with recurrent entity networks

   11.ensemble selection from libraries of models

   12.[152]bert:pre-training of deep bidirectional transformers for
   language understanding

   13.[153]google-research/bert
     __________________________________________________________________

   to be continued. for any problem, concat [154]brightmart@hotmail.com

     *    2019 github, inc.
     * [155]terms
     * [156]privacy
     * [157]security
     * [158]status
     * [159]help

     * [160]contact github
     * [161]pricing
     * [162]api
     * [163]training
     * [164]blog
     * [165]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [166]reload to refresh your
   session. you signed out in another tab or window. [167]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/brightmart/text_classification/commits/master.atom
   3. https://github.com/brightmart/text_classification#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/brightmart/text_classification
  32. https://github.com/join
  33. https://github.com/login?return_to=/brightmart/text_classification
  34. https://github.com/brightmart/text_classification/watchers
  35. https://github.com/login?return_to=/brightmart/text_classification
  36. https://github.com/brightmart/text_classification/stargazers
  37. https://github.com/login?return_to=/brightmart/text_classification
  38. https://github.com/brightmart/text_classification/network/members
  39. https://github.com/brightmart
  40. https://github.com/brightmart/text_classification
  41. https://github.com/brightmart/text_classification
  42. https://github.com/brightmart/text_classification/issues
  43. https://github.com/brightmart/text_classification/pulls
  44. https://github.com/brightmart/text_classification/projects
  45. https://github.com/brightmart/text_classification/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/classification
  48. https://github.com/topics/nlp
  49. https://github.com/topics/fasttext
  50. https://github.com/topics/textid98
  51. https://github.com/topics/textid56
  52. https://github.com/topics/tensorflow
  53. https://github.com/topics/multi-label
  54. https://github.com/topics/multi-class
  55. https://github.com/topics/attention-mechanism
  56. https://github.com/topics/text-classification
  57. https://github.com/topics/convolutional-neural-networks
  58. https://github.com/topics/sentence-classification
  59. https://github.com/topics/memory-networks
  60. https://github.com/brightmart/text_classification/commits/master
  61. https://github.com/brightmart/text_classification/branches
  62. https://github.com/brightmart/text_classification/releases
  63. https://github.com/brightmart/text_classification/graphs/contributors
  64. https://github.com/brightmart/text_classification/blob/master/license.md
  65. https://github.com/brightmart/text_classification/search?l=python
  66. https://github.com/brightmart/text_classification/search?l=jupyter-notebook
  67. https://github.com/brightmart/text_classification/find/master
  68. https://github.com/brightmart/text_classification/archive/master.zip
  69. https://github.com/login?return_to=https://github.com/brightmart/text_classification
  70. https://github.com/join?return_to=/brightmart/text_classification
  71. https://desktop.github.com/
  72. https://desktop.github.com/
  73. https://developer.apple.com/xcode/
  74. https://visualstudio.github.com/
  75. https://github.com/brightmart
  76. https://github.com/brightmart/text_classification/commits?author=brightmart
  77. https://github.com/brightmart/text_classification/commit/0134a06606667d087fa9927a3ad5958c77c0b7f5
  78. https://github.com/brightmart/text_classification/commit/0134a06606667d087fa9927a3ad5958c77c0b7f5
  79. https://github.com/brightmart/text_classification/tree/0134a06606667d087fa9927a3ad5958c77c0b7f5
  80. https://github.com/brightmart/text_classification/tree/master/a00_bert
  81. https://github.com/brightmart/text_classification/commit/69d9c8764701d7c247bb1ccac3f3f2cc7204403d
  82. https://github.com/brightmart/text_classification/tree/master/a00_boosting
  83. https://github.com/brightmart/text_classification/commit/bcadf261e7443c921abd5a74687146833e8986fe
  84. https://github.com/brightmart/text_classification/tree/master/a01_fasttext
  85. https://github.com/brightmart/text_classification/commit/3c404c2022becae47bc460dc2046007f7a20973e
  86. https://github.com/brightmart/text_classification/tree/master/a02_textid98
  87. https://github.com/brightmart/text_classification/tree/master/a03_textid56
  88. https://github.com/brightmart/text_classification/commit/d64e2290fbfe46d7252dba27e3cfd6fc573f6813
  89. https://github.com/brightmart/text_classification/tree/master/a04_textrid98
  90. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
  91. https://github.com/brightmart/text_classification/tree/master/a05_hierarchicalattentionnetwork
  92. https://github.com/brightmart/text_classification/commit/bbb33206eedac63078cb1355c12e1e96c3138343
  93. https://github.com/brightmart/text_classification/tree/master/a06_id195withattention
  94. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
  95. https://github.com/brightmart/text_classification/tree/master/a07_transformer
  96. https://github.com/brightmart/text_classification/tree/master/a08_entitynetwork
  97. https://github.com/brightmart/text_classification/commit/1583c3304f0f071569424d36c206b875a705a026
  98. https://github.com/brightmart/text_classification/tree/master/a09_dynamicmemorynet
  99. https://github.com/brightmart/text_classification/commit/dd30adae5e09a763f812b517f8cc7a4d9429ecd4
 100. https://github.com/brightmart/text_classification/tree/master/aa1_data_util
 101. https://github.com/brightmart/text_classification/commit/361cc3e3d9b1c807d8b6eb4cc18dd9878171d312
 102. https://github.com/brightmart/text_classification/tree/master/aa2_classificationtflearn
 103. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
 104. https://github.com/brightmart/text_classification/tree/master/aa3_id98sentenceclassificationtflearn
 105. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
 106. https://github.com/brightmart/text_classification/tree/master/aa4_textid98_with_rid98
 107. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
 108. https://github.com/brightmart/text_classification/tree/master/aa5_bilstmtextrelation
 109. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
 110. https://github.com/brightmart/text_classification/tree/master/aa6_twoid98textrelation
 111. https://github.com/brightmart/text_classification/commit/1299e649edb3d5d3ba8e1a5e95247a47322b2e18
 112. https://github.com/brightmart/text_classification/tree/master/data
 113. https://github.com/brightmart/text_classification/tree/master/images
 114. https://github.com/brightmart/text_classification/commit/a91bd3d077ef297998b5922c2291b84cadb9495d
 115. https://github.com/brightmart/text_classification/blob/master/.travis.yml
 116. https://github.com/brightmart/text_classification/blob/master/license.md
 117. https://github.com/brightmart/text_classification/blob/master/readme.md
 118. https://github.com/brightmart/text_classification/blob/master/a08_predict_ensemble.py
 119. https://github.com/brightmart/text_classification/commit/59659c6ab8f2d8b4bcfa275b70e14ab103eae339
 120. https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf
 121. https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb
 122. https://github.com/brightmart/text_classification/blob/master/sample_data.zip
 123. https://github.com/brightmart/nlp_chinese_corpus
 124. https://github.com/brightmart/bert_language_understanding
 125. https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf
 126. https://pan.baidu.com/s/1ywzf2eapxq15-r2hhk2m-q
 127. https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb
 128. https://github.com/brightmart/text_classification/issues/3
 129. https://biendata.com/competition/zhihu/
 130. https://arxiv.org/abs/1607.01759
 131. https://github.com/brightmart/text_classification/blob/master/images/fasttext.jpg
 132. http://www.aclweb.org/anthology/d14-1181
 133. https://arxiv.org/abs/1510.03820
 134. https://github.com/brightmart/text_classification/blob/master/images/textid98.jpg
 135. https://github.com/brightmart/text_classification/blob/master/images/bert_1.jpeg
 136. https://github.com/brightmart/text_classification/blob/master/images/bert_2.jpeg
 137. https://github.com/brightmart/sentiment_analysis_fine_grain
 138. https://github.com/brightmart/sentiment_analysis_fine_grain
 139. https://github.com/brightmart/text_classification/blob/master/images/bi-directionalid56.jpg
 140. https://github.com/brightmart/text_classification/blob/master/images/emojifier-v2.png
 141. https://scholar.google.com.hk/scholar?q=recurrent+convolutional+neural+networks+for+text+classification&hl=zh-cn&as_sdt=0&as_vis=1&oi=scholart&sa=x&ved=0ahukewjpx82cvqtuahwhspqkhubdbdyqgqmiitaa
 142. https://github.com/brightmart/text_classification/blob/master/images/rid98.jpg
 143. https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf
 144. https://github.com/brightmart/text_classification/blob/master/images/han.jpg
 145. https://github.com/brightmart/text_classification/issues/55
 146. https://arxiv.org/pdf/1409.0473.pdf
 147. https://github.com/brightmart/text_classification/blob/master/images/id195attention.jpg
 148. https://github.com/brightmart/text_classification/blob/master/images/attention_is_all_you_need.jpg
 149. https://github.com/brightmart/text_classification/blob/master/images/entitynet.jpg
 150. https://github.com/brightmart/text_classification/blob/master/images/dmn.jpg
 151. http://www.wildml.com/
 152. https://arxiv.org/abs/1810.04805
 153. https://github.com/google-research/bert
 154. mailto:brightmart@hotmail.com
 155. https://github.com/site/terms
 156. https://github.com/site/privacy
 157. https://github.com/security
 158. https://githubstatus.com/
 159. https://help.github.com/
 160. https://github.com/contact
 161. https://github.com/pricing
 162. https://developer.github.com/
 163. https://training.github.com/
 164. https://github.blog/
 165. https://github.com/about
 166. https://github.com/brightmart/text_classification
 167. https://github.com/brightmart/text_classification

   hidden links:
 169. https://github.com/
 170. https://github.com/brightmart/text_classification
 171. https://github.com/brightmart/text_classification
 172. https://github.com/brightmart/text_classification
 173. https://help.github.com/articles/which-remote-url-should-i-use
 174. https://github.com/brightmart/text_classification#text-classification
 175. https://github.com/brightmart/text_classification#update
 176. https://github.com/brightmart/text_classification#introduction
 177. https://github.com/brightmart/text_classification#models
 178. https://github.com/brightmart/text_classification#performance
 179. https://github.com/brightmart/text_classification#usage
 180. https://github.com/brightmart/text_classification#environment
 181. https://github.com/brightmart/text_classification#sample-data-cached-file-
 182. https://github.com/brightmart/text_classification#road-map
 183. https://github.com/brightmart/text_classification#use-your-own-data
 184. https://github.com/brightmart/text_classification#pretrain-work-embedding
 185. https://github.com/brightmart/text_classification#models-detail
 186. https://github.com/brightmart/text_classification#1fasttext
 187. https://github.com/brightmart/text_classification
 188. https://github.com/brightmart/text_classification#2textid98
 189. https://github.com/brightmart/text_classification#3bert
 190. https://github.com/brightmart/text_classification#pre-training-of-deep-bidirectional-transformers-for-language-understanding
 191. https://github.com/brightmart/text_classification#masked-languge-model
 192. https://github.com/brightmart/text_classification#next-sentence-prediction
 193. https://github.com/brightmart/text_classification#how-to-use-bert
 194. https://github.com/brightmart/text_classification#use-bert-for-multi-label-classification
 195. https://github.com/brightmart/text_classification#use-bert-for-online-prediction
 196. https://github.com/brightmart/text_classification#how-to-get-better-model-for-bert
 197. https://github.com/brightmart/text_classification#4textid56
 198. https://github.com/brightmart/text_classification#5bilstmtextrelation
 199. https://github.com/brightmart/text_classification#6twoid98textrelation
 200. https://github.com/brightmart/text_classification#7bilstmtextrelationtwoid56
 201. https://github.com/brightmart/text_classification#8rid98
 202. https://github.com/brightmart/text_classification#9hierarchical-attention-network
 203. https://github.com/brightmart/text_classification#10id195-with-attention
 204. https://github.com/brightmart/text_classification#11transformerattention-is-all-you-need
 205. https://github.com/brightmart/text_classification#12recurrent-entity-network
 206. https://github.com/brightmart/text_classification#13dynamic-memory-network
 207. https://github.com/brightmart/text_classification#todo
 208. https://github.com/brightmart/text_classification#conclusion
 209. https://github.com/brightmart/text_classification#reference
 210. https://github.com/
