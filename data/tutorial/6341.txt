   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

word vectors for non-nlp data and research people

   [16]go to the profile of conor mcdonald
   [17]conor mcdonald (button) blockedunblock (button) followfollowing
   sep 7, 2017

   word vectors represent a significant leap forward in advancing our
   ability to analyse relationships across words, sentences and documents.
   in doing so, they advance technology by providing machines much more
   information about words than has previously been possible using
   traditional representations of words. it is word vectors that make
   technologies such as id103 and machine translation
   possible. there are many excellent explanations of word vectors, but in
   this one i want to make the concept accessible to data and research
   people who aren   t very familiar with natural language processing
   (nlp)         for a primer on foundational nlp concepts check out my post
   here:
   [18]https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/.

what are word vectors?

   word vectors are simply vectors of numbers that represent the meaning
   of a word. for now, that   s not very clear but, we   ll come back to it in
   a bit. it is useful, first of all to consider why word vectors are
   considered such a leap forward from traditional representations of
   words.

   traditional approaches to nlp, such as one-hot encoding and
   bag-of-words models (i.e. using dummy variables to represent the
   presence or absence of a word in an observation (e.g. a sentence)),
   whilst useful for some machine learning (ml) tasks, do not capture
   information about a word   s meaning or context. this means that
   potential relationships, such as contextual closeness, are not captured
   across collections of words. for example, a one-hot encoding cannot
   capture simple relationships, such as determining that the words    dog   
   and    cat    both refer to animals that are often discussed in the context
   of household pets. such encodings often provide sufficient baselines
   for simple nlp tasks (for example, email spam classifiers), but lack
   the sophistication for more complex tasks such as translation and
   id103. in essence, traditional approaches to nlp, such as
   one-hot encodings, do not capture syntactic (structure) and semantic
   (meaning) relationships across collections of words and, therefore,
   represent language in a very naive way.

   in contrast, word vectors represent words as multidimensional
   continuous floating point numbers where semantically similar words are
   mapped to proximate points in geometric space. in simpler terms, a word
   vector is a row of real valued numbers (as opposed to dummy numbers)
   where each point captures a dimension of the word   s meaning and where
   semantically similar words have similar vectors. this means that words
   such as wheel and engine should have similar word vectors to the word
   car (because of the similarity of their meanings), whereas the word
   banana should be quite distant. put differently, words that are used in
   a similar context will be mapped to a proximate vector space (we will
   get to how these word vectors are created below). the beauty of
   representing words as vectors is that they lend themselves to
   mathematical operators. for example, we can add and subtract
   vectors         the canonical example here is showing that by using word
   vectors we can determine that:

   king - man + woman = queen

   in other words, we can subtract one meaning from the word vector for
   king (i.e. maleness), add another meaning (femaleness), and show that
   this new word vector (king         man + woman) maps most closely to the word
   vector for queen.

   the numbers in the word vector represent the word   s distributed weight
   across dimensions. in a simplified sense each dimension represents a
   meaning and the word   s numerical weight on that dimension captures the
   closeness of its association with and to that meaning. thus, the
   semantics of the word are embedded across the dimensions of the vector.

a simplified representation of word vectors

   [0*6qefxri6bxni26qs.png]

   in the figure we are imagining that each dimension captures a clearly
   defined meaning. for example, if you imagine that the first dimension
   represents the meaning or concept of    animal   , then each word   s weight
   on that dimension represents how closely it relates to that concept.

   this is quite a large simplification of word vectors as the dimensions
   do not hold such clearly defined meanings, but it is a useful and
   intuitive way to wrap your head around concept of word vector
   dimensions.

   we can use the python nlp library spacy (check out my introduction
   here:
   [19]https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/,
   it has also been recently ported to r
   [20]https://github.com/kbenoit/spacyr) to quickly access some
   pre-trained 300 dimensional word vectors. we create a list of words,
   apply spacy   s parser, extract the vector for each word, stack them
   together and then extract two-principal components for visualisation
   purposes.

   [21]https://gist.github.com/conormm/ca0cdf78fa7a91fdacf500ff4dff0645

   here we simply extract vectors for different animals and words that
   might be used to describe some of them. as mentioned in the beginning
   word vectors are amazingly powerful because they allow us (and
   machines) to identify similarities across different words by
   representing them in a continuous vector space. you can see here how
   the vectors for animals like    lion   ,    tiger   ,    cheetah    and    elephant   
   are very close together. this is likely because they are often
   discussed in similar contexts, for example these animals are big, wild
   and potentially dangerous         indeed, the descriptive word    wild    maps
   quite closely to this group of animals.
   [0*swfrgngpuuqxbsnm.png]

   similar words are mapped together in the vector space. notice how close
   cat and dog are to pet, how clustered elephant, lion and tiger are, and
   how descriptive words also cluster together.

   what is also interesting here is how closely the words    wild   ,    zoo   
   and    domesticated    map to one another. it makes sense given that they
   are words that are frequently used to describe animals, but highlights
   the amazing power of word vectors!

where do word vectors come from?

   an excellent question at this point is where do these dimensions and
   weights come from?! there are two common ways through which word
   vectors are generated:
    1. counts of word / context co-occurrences
    2. predictions of context given word (skip-gram neural network models,
       e.g. id97)

   *note: below i describe a high-level id97 approach to generating
   word vectors, but a good overview of the count / co-occurence approach
   can be found here (https://medium.com/ai-society/jkljlj-7d6e699895c4).

   both approaches to generating word vectors build on firth   s (1957)
   distributional hypothesis which states:

   you shall know a word by the company it keeps   

   put differently, words that share similar contexts tend to have similar
   meanings. the context of a word in a practical sense refers to its
   surrounding word(s) and word vectors are (typically) generated by
   predicting the id203 of a context given a word. put differently,
   the weights that comprise a word vector are learned by making
   predictions on the id203 that other words are contextually close
   to a given word. this is akin to attempting to fill in the blanks
   around some given input word. for example, given the input sequence,
      the fluffy dog barked as it chased a cat   , the two-window (two-words
   preceding and proceeding the focal word) context for the words    dog   
   and    barked    would look like:
   [0*63bfztlahthsv-lo.png]

   i don   t wish to delve into the mathematical details how neural networks
   learn id27s too much, as people much more qualified to do so
   have explicated this already. in particular these posts have been
   helpful to me when trying to understand how word vectors are learned:
    1. deep learning, nlp, and representations
       ([22]http://colah.github.io/posts/2014-07-nlp-id56s-representations/
       )
    2. the amazing power of word vectors
       ([23]https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-
       vectors/)
    3. id97 tutorial         the skip-gram model
       ([24]http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-g
       ram-model/)

   it is useful, however, to touch on the workings of the id97 model
   given its popularity and usefulness. a id97 model is simply a
   neural network with a single hidden layer that is designed to
   reconstruct the context of words by estimating the id203 that a
   word is    close    to another word given as input.

   the model is trained on word, context pairings for every word in the
   corpus, i.e.:

   (dog, the)

   (dog, fluffy)

   (dog, barked)

   (dog, as)

   note that this is technically a supervised learning process, but you do
   not need labelled data         the labels (the targets / dependent variables)
   are generated from the words that form the context of a focal word.
   thus, using the window function the model learns the context in which
   words are used. in this simple example the model will learn that fluffy
   and barked are used in the context (as defined by the window length) of
   the word dog.

   one of the fascinating things about word vectors created by id97
   models is that they are the side effects of a predictive task, not its
   output. in other words, a word vector is not predicted, (it is context
   probabilities that are predicted), the word vector is a learned
   representation of the input that is used on the predictive task         i.e.
   predicting a word given a context. the word vector is the model   s
   attempt to learn a good numerical representation of the word in order
   to minimize the loss (error) of it   s predictions. as the model
   iterates, it adjusts its neurons    weights in an attempt to minimize the
   error of it   s predictions and in doing so, it gradually refines its
   representation of the word. in doing so, the word   s    meaning    becomes
   embedded in the weight learned by each neuron in the hidden layer of
   the network.

   a id97 model, therefore, accepts as input a single word
   (represented as a one-hot encoding amongst all words in the corpus) and
   the model attempts to predict the id203 that a randomly chosen
   word in the corpus is at a nearby position to the input word. this
   means that for every input word there are n output probabilities, where
   n is equal to the total size of the corpus. the magic here is that the
   training process includes only the word   s context, not all words in the
   corpus. this means in our simple example above, given the word    dog    as
   input,    barked    will have a higher id203 estimate than    cat   
   because it is closer in context         i.e. it is learned in the training
   process. put differently, the model attempts to predict the id203
   that other words in the corpus belong to the context of the input word.
   therefore, given the sentence above (   the fluffy dog barked as it
   chased a cat   ) as input a run of the model would look like this:
   [0*q4-vhteomwngkanl.png]

   note: this conceptual nn is a close friend of the diagram in chris
   mccormick   s blog post linked to above

   the value in going through this process is to extract the weights that
   have been learned by the neurons of the model   s hidden layer. it is
   these weights that form the word vector, i.e. if you have a 300 neuron
   hidden layer you will create a 300-dimension word vector for each word
   in the corpus. the output of this process, therefore, is a word-vector
   mapping of size n-input words * n-hidden layer neurons.

   thank you for reading, hope you learned something new :)

     * [25]machine learning
     * [26]nlp
     * [27]deep learning
     * [28]data science
     * [29]towards data science

   (button)
   (button)
   (button) 685 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of conor mcdonald

[31]conor mcdonald

   senior data scientist @ amazon, phd, economics (university of leeds).
   [32]@conormacd

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 685
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8d689c692353
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/word-vectors-for-non-nlp-data-and-research-people-8d689c692353&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/word-vectors-for-non-nlp-data-and-research-people-8d689c692353&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_wzwxpg9nniqv---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@conrmcdonald?source=post_header_lockup
  17. https://towardsdatascience.com/@conrmcdonald
  18. https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/
  19. https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/,
  20. https://github.com/kbenoit/spacyr)
  21. https://gist.github.com/conormm/ca0cdf78fa7a91fdacf500ff4dff0645
  22. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  23. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  24. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/tagged/nlp?source=post
  27. https://towardsdatascience.com/tagged/deep-learning?source=post
  28. https://towardsdatascience.com/tagged/data-science?source=post
  29. https://towardsdatascience.com/tagged/towards-data-science?source=post
  30. https://towardsdatascience.com/@conrmcdonald?source=footer_card
  31. https://towardsdatascience.com/@conrmcdonald
  32. http://twitter.com/conormacd
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/8d689c692353/share/twitter
  39. https://medium.com/p/8d689c692353/share/facebook
  40. https://medium.com/p/8d689c692353/share/twitter
  41. https://medium.com/p/8d689c692353/share/facebook
