   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

convolutional attention model for natural language id136

   [16]go to the profile of marek galovi  
   [17]marek galovi   (button) blockedunblock (button) followfollowing
   jun 1, 2017

   in this article i   d like to show you a model i used for the [18]quora
   question pairs competition. first, i   ll describe a decomposable
   attention model for natural language id136 ([19]parikh et al.,
   2016) and then extend it with a convolutional layer to improve loss and
   classification accuracy. for the purpose of this article i   ll use the
   [20]stanford nli corpus for comparison.

introduction

   natural language id136 (nli) and paraphrase detection are one of
   the main research topics in natural language processing. natural
   language id136 refers to a problem of determining entailment and
   contradiction between two statements and paraphrase detection focuses
   on determining sentence duplicity. papers published on this topic use
   recurrent neural networks [21]wang & jiang, 2016, [22]wang et al.,
   2017, convolutional neural networks [23]mou et al., 2016 or
   feed-forward neural networks with soft-alignment and attention
   [24]parikh et al., 2016.

model architecture

   inputs to the model are two phrases represented as a sequence of word
   embedding vectors a = {a1,...,am} and b = {b1,...,bn}. the goal is to
   estimate a id203 that the two phrases are in entailment or
   contradiction to each other. during model training, we   ll also use a
   variable y which is a ground truth and auxiliary inputs a_len and b_len
   to help us construct masking matrices. the figure below depicts the
   core model architecture which is composed of three layers: attention,
   comparison and aggregation.
   [1*zbytgtbubqmwyr8m1cavjg.png]
   a high-level overview of the model architecture. (image credit: parikh
   et al., 2017)

attention

   attention layer uses a variant of neural attention proposed in
   [25]bahdanau et al., 2016. it   s implemented using a feed-forward neural
   network f that is applied to both questions separately (bahdanau   s work
   uses internal states of a recurrent neural network). outputs of the
   neural network are then normalized using a softmax function and
   soft-aligned to the second sentence. the equations for this layer look
   like this:
   [1*rm0zogyjnt5-p23-q_t2kg.png]
   where m = number of words in a, n = number of words in b

   for training i use pre-trained 300 dimensional glove id27s.
   output size of f(a) is mx200, output size of f(b) is nx200 which means
   that the attention weights matrix e will have shape mxn. beta and alpha
   are then a dot product between the softmax normalized attention weights
   and a corresponding sentence matrix.

comparison

   next step is to compare soft-aligned sentence matrices. similarly to
   the previous step, a feed-forward neural network g is used and inputs
   to the network are concatenated sentence matrices [a, beta] and [b,
   alpha] respectively.
   [1*v6ok4hgtskahtuucmrln5a.png]
   soft-aligned matrix comparison

aggregation

   the last part of the core model architecture is the aggregation layer.
   all this layer does is a column-wise sum over the output of the
   comparison network so that we obtain a fixed-size representation of
   every sentence.
   [1*zclzbe47lbqkq7xrxenrmg.png]
   comparison matrix aggregation

   finally, aggregated sentence representations are concatenated and fed
   into a dense classification network. output layer of the classifier is
   softmax normalized so that we obtain a id203 distribution over
   target classes.

convolutional layer

   extension of the decomposable attention model for nli is based on the
   fact that the same word can have different meanings given it   s context.
   similarly to how us humans understand words in their context, machine
   learning model can learn these higher-level word representations by
   using either recurrent neural network (computationally more expensive)
   or convolutional layers (less expensive but also less efficient).
   [1*bjfetesx8qyzkxyspm8fwq.png]
   two convolutional layers with filter size 3 and stride 1

   the figure above shows how the higher level representations are built
   using the convolutional layer. the example sentence    we have seen a
   beautiful crane in the zoo    contains the word crane which can be a bird
   but it can be a construction device too depending on the context of the
   sentence.

model training

   sentences are first tokenized using nltk.word_tokenize function and
   then embedding index lookup is performed. if a token cannot be found
   the dictionary then it   s omitted and no imputation is performed.
   embedding indices are then padded with zero vectors up to the length of
   the longest sentence in a given batch. convolutional layer then appends
   zero vector before the first token and after the last token so that
   after applying convolutions valid sentence length remains unchanged.
   [1*_fudenwznkw1ai4tvsjrlg.png]
   high level overview of a tensorflow model graph

   attention weights matrix e has all padding values replaced with -np.inf
   so that after applying softmax id172 we obtain a id203
   distribution over the valid portion of the sentence and all padding
   values are 0. finally, comparison, aggregation and classification
   layers are applied as described in the model architecture section of
   this article and cross-id178 loss is used to train the model.

results

   first i implemented the original decomposable attention model for nli
   in tensorflow to get a baseline score. although original paper reports
   86.3% test accuracy i wasn   t able to get it above 83.8% (this might be
   caused by the fact that i   ve used adam optimizer instead of adagrad and
   trained only for 100 epochs). after adding convolutional layer the
   model reached classification accuracy 84.5%. i also changed optimizer
   to adagrad as it worked better with the convolutional architecture.

   at the time of writing this article, the current state-of-the-art model
   for natural language id136 is a bilateral multi-perspective
   matching model ([26]wang et al., 2017) that reaches classification
   accuracy 88.8%. the reason i   ve chosen the decomposable attention model
   for nli for this article is that the architecture of the model is much
   simpler therefore it takes less time to run training/id136 and i
   can experiment with it more.

conclusion

   as we can see convolutional layer gave us slight improvement in the
   classification accuracy. this extension may give larger improvement on
   datasets that contain more words with context conditioned meaning as
   well as it may give little to no improvement on datasets with no such
   words. better scores may also be obtained by training models for more
   training epochs with multiple hyper-parameter combinations.
     __________________________________________________________________

   full tensorflow model code can be found [27]here. if you liked this
   article feel free to recommend it to others. also, if you have any
   suggestions what i   ve done wrong or what could   ve been done differently
   feel free to leave them in the comment section below.

     * [28]machine learning
     * [29]neural networks
     * [30]nlp
     * [31]attention
     * [32]towards data science

   (button)
   (button)
   (button) 274 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of marek galovi  

[34]marek galovi  

   ctu prague, former data scientist [35]@shopify. i like data, machine
   learning and algorithms. sometimes i also write about it.

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 274
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a754834c0d83
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/convolutional-attention-model-for-natural-language-id136-a754834c0d83&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/convolutional-attention-model-for-natural-language-id136-a754834c0d83&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ei2lyfuyxq12---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@marekgalovic?source=post_header_lockup
  17. https://towardsdatascience.com/@marekgalovic
  18. https://www.kaggle.com/c/quora-question-pairs
  19. https://arxiv.org/pdf/1606.01933.pdf
  20. https://nlp.stanford.edu/projects/snli/
  21. https://arxiv.org/pdf/1611.01747.pdf
  22. https://arxiv.org/pdf/1702.03814.pdf
  23. https://arxiv.org/abs/1512.08422
  24. https://arxiv.org/pdf/1606.01933v1.pdf
  25. https://arxiv.org/pdf/1409.0473.pdf
  26. https://arxiv.org/pdf/1702.03814.pdf
  27. https://gist.github.com/marekgalovic/a1a4073b917ae1b18dc7413436794dca
  28. https://towardsdatascience.com/tagged/machine-learning?source=post
  29. https://towardsdatascience.com/tagged/neural-networks?source=post
  30. https://towardsdatascience.com/tagged/nlp?source=post
  31. https://towardsdatascience.com/tagged/attention?source=post
  32. https://towardsdatascience.com/tagged/towards-data-science?source=post
  33. https://towardsdatascience.com/@marekgalovic?source=footer_card
  34. https://towardsdatascience.com/@marekgalovic
  35. http://twitter.com/shopify
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/a754834c0d83/share/twitter
  42. https://medium.com/p/a754834c0d83/share/facebook
  43. https://medium.com/p/a754834c0d83/share/twitter
  44. https://medium.com/p/a754834c0d83/share/facebook
