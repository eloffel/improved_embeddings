   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

id97 (part 1)

   [14]go to the profile of mukul malik
   [15]mukul malik (button) blockedunblock (button) followfollowing
   oct 15, 2016
   [1*0rog7_linnzvvm7bfb8tka.jpeg]

     id97; the steroids for natural language processing

   let   s start with the basics.

word vectors

   q) what are word vectors?

   ans) representation of words with numbers.

   q) why word vectors?

   ans) i   ll sum it up with three main reasons:

   1. computer cannot do computations on strings.

   2. strings don   t hold much explicit information themselves.

   3. words vectors are usually dense vector representations.

   q) so what is explicit information?

   ans) yes, the word itself doesn   t say much about what it represents in
   real life. example:

   the string    cat    just tells us it has three alphabets    c   ,    a    and    t   .

   it has no information about the animal it represents or the count or
   the context in which it is being used.

   q) dense vector representation?

   ans) short answer (for now), these vectors can hold enormous
   information compared to their size.

   q) types of word vectors?

   a) there are two main categories:
     * frequency based : use stats to compute id203 of a word
       co-occurring with respect to it   s neighbouring words.
     * prediction based : use predictive analysis to make a weighted guess
       of a word co-occurring with respect to it   s neighbouring words.

   predictions can be of two types:
     * semantic : try to guess a word w.r.t. context of text
     * syntactic : try to guess a word w.r.t. syntax of text

   q) difference between syntax and context based vectors?

   ans) let   s look at an example:

   consider the following sentence    newton does not like apples.   
     * semantic vectors are concerned with    who or what are the entities,
       a text is about?   . in this case    newton    and    apples   
     * syntactic vectors are concerned with    what about those entities is
       being said?   . in this case    not    and    like   
     __________________________________________________________________

id97

   is one of the most widely used form of word vector representation.
   first coined by google in [16]mikolov et el.

   it has two variants:
    1. cbow (continuous bag of words) : this model tries to predict a word
       on bases of it   s neighbours.

   2. skipgram : this models tries to predict the neighbours of a word.

     statistically it has the effect that cbow smoothes over a lot of the
     distributional information (by treating an entire context as one
     observation). for the most part, this turns out to be a useful thing
     for smaller datasets. however, skip-gram treats each context-target
     pair as a new observation, and this tends to do better when we have
     larger datasets.

     - tensorflow

   in simpler words, cbow tends to find the id203 of a word
   occurring in a neighbourhood (context). so it generalises over all the
   different contexts in which a word can be used.

   whereas skipgram tends to learn the different contexts separately. so
   skipgram needs enough data w.r.t. each context. hence skipgram requires
   more data to train, also skipgram (given enough data) contains more
   knowledge about the context.

   note : these techniques do not need tagged dataset (though a tagged
   dataset can be used to include additional information as we   ll see
   later). so any large text corpus is effectively a dataset. as the tag
   to be predicted are the words already present in the text.

   we will focus on skipgram as large enough datasets (wikipedia, reddit,
   stackoverflow etc.) are available for download.
     __________________________________________________________________

skipgram

   first we decide what context are we looking for in terms of what will
   be our target words (to be predicted), source words (on bases of which
   we predict) and how far are we looking for context (size of window).

   example:

   considering window size to be 3

type 1

   considering the middle word as the source word. the next and previous
   words as the target words.
   [1*iqb2yiu3gymmkfwog-i23q.jpeg]
   fig no. 1
   [1*bjxcwnljwryz6m45ebxtyw.jpeg]
   fig no. 2

type 2

   considering the first word as the source word. the following two words
   as the target words.
   [1*f25mnjgsg3meyoabhmwxgw.jpeg]
   fig no. 3
   [1*zwzrteze_t_mp88qszutaw.jpeg]
   fig no. 4

   in both the types, source word is surrounded by words which are
   relevant to a context of that source word. like    messi    will usually be
   surrounded by words related to    football   . so after seeing a few
   examples, word vector of    messi    will start incorporating context
   related to    football   ,    goals   ,    matches    etc.

   in case of    apple   , its word vector would do the same but for both the
   company and the fruit (see fig no. 6).
   [1*j-86sti2fbnas895wpxzsw.jpeg]
   fig no. 5 id97   s neural network

   w1(s) and w2(s) contain information about the words. the information in
   w1 and w2 are combined/averaged to obtain the id97 representations.

   say the size of w(s) was 400, the id97 representation of    apple   
   would look something like
array([
-2.56660223e-01,  -7.96796158e-02,  -2.04517767e-02,
-7.34366626e-02,   3.26843783e-02,  -1.90244913e-02,
7.93217495e-02,    4.07200940e-02,  -1.74737453e-01,
.....
1.86899990e-01,    -4.33036387e-02,  -2.66942739e-01,
-1.00671440e-01],   dtype=float32)

   now a simple sentence like    apple fell on newton    containing 4 words,
   with help of id97, can be converted into 4*400 (1600) numbers;
   each[1] containing explicit information. so now we also know that the
   text is talking about a person, science, fruit etc.

   [1] : hence dense vector representation
     __________________________________________________________________

visualisation

   visualising id97 directly is currently impossible for mankind
   (because of high dimensionality like 400). instead we use
   id84 techniques like [17]multidimensional scaling ,
   [18]sammon   s mapping, [19]nearest neighbor graph etc.

   the most widely algorithm is [20]t-distributed stochastic neighbour
   embedding (id167). christopher olah has an amazing [21]blog about
   id84.

   the end result of id167 on id97 looks something like
   [1*gb46k5avuvaiujle_zrj3q.jpeg]
   fig no. 6 multiple contexts of apple

   this figure shows that apple lies between companies (ibm, microsoft)
   and fruits (mango).

   that   s because the id97 representation of apple contains
   information about both the company apple and the fruit apple.

   distance between
     * apple and mango : 0.505
     * apple and ibm : 0.554
     * mango and ibm : 0.902

   and
   [1*wcyt-jsdj6hd-okji2e5ia.jpeg]
   fig no. 7 combining contexts of multiple words

   this figure shows that by combining directions of two vectors    state   
   and    america   , the resultant vector    dakota    is relevant to original
   vectors.

   so effectively
     * state + america = dakota
     * state + germany = bavaria

   other examples are :
     * german + airlines = lufthansa
     * king + woman         man = queen
     __________________________________________________________________

implementations

   gensim and tensorflow, both have pretty impressive implementations of
   id97.

   this is excellent [22]blog of [23]gensim   s implementation and
   [24]tensorflow has a [25]tutorial.
     __________________________________________________________________

issues

   by default, id97 model has one representation per word. a vector
   can try to accumulate all contexts but that just ends up generalising
   all the contexts to at least some extent, hence precision of each
   context is compromised. this is especially a problem for words which
   have very different contexts. this might lead to one context, over
   powering others.

   like : there will be only one id97 representation for    apple    the
   company and    apple    the fruit.

   example 1:

      maiden    can be used for a woman, a band (iron maiden), in sports etc.

   when you try to find most similar words to    maiden   
[(u'odi_debut', 0.43079674243927),
 (u'racecourse_debut', 0.42960068583488464),
 .....
 (u'marathon_debut', 0.40903717279434204),
 (u'one_day_debut', 0.40729495882987976),
 (u'test_match_debut', 0.4013477563858032)]

   it is clearly visible that the context related to    sports    has
   overpowered others.

   even combining    iron    and    maiden   , doesn   t resolve the issue as now
   context of    iron    overpowers.
[(u'steel', 0.5581518411636353),
 (u'copper', 0.5266575217247009),
 .....
 (u'bar_iron', 0.49549400806427)]

   example 2

   a word could be used as a verb and a noun but with completely different
   meanings. like the word    iron   . as a verb it is usually used to smooth
   things with an electric iron but at a noun it is mostly used to denote
   the metal.

   when we find the nearest neighbours of    iron   
[(u'steel', 0.5581518411636353),
 (u'copper', 0.5266575217247009),
 .....
 (u'bar_iron', 0.49549400806427)]

   there is negligible reference to verb counterpart.
     __________________________________________________________________

variants

variant 1 : compound noun based id97

   by replacing nouns (like    iron    and    maiden   ) by compound nouns (like
      iron_maiden   ) in training set.

        iron maiden is an amazing band   

   becomes

        iron_maiden is an amazing band   

   so the context of the compound noun stands out and is remarkably
   accurate!

   result:

   most relevant words w.r.t    iron_maiden    are:
[(u'judas_priest', 0.8176089525222778),
 (u'black_sabbath', 0.7859792709350586),
 (u'megadeth', 0.7748109102249146),
 (u'metallica', 0.7701393961906433),
 .....

   that   s hardcore, literally!

   here is a python code for converting nouns into compound nouns (with
   adj-noun pairing as well), in order to create the training set for
   training id97.
   [1*fl0ofzvk7r-hnecewq9r0a.jpeg]
   [1*4_ugkn1hl9ozpfcsxybzyq.jpeg]
   fig no. 8

   this python code is for converting nouns into compound (only noun-noun
   paring).
   [1*6vazagaunue9tjbgahhrwa.jpeg]
   fig no. 9

variant 2 : sense2vec

   (note : non ner implementation of sense2vec)

   taking the above mentioned variant one step further by adding part of
   speech (p.o.s) tags to the training set.

   example:

        i iron my shirt with class   

   becomes

        i/prp iron/vbp my/prp$ shirt/nn with/in class/nn ./.   

   or

        i/noun iron/verb my/adj shirt/noun with/adp class/noun ./punct   

   result:

   now most relevant words w.r.t    iron/verb    are:
[(u'ironing/verb', 0.818801760673523),
 (u'polish/verb', 0.794084906578064),
 (u'smooth/verb', 0.7590495347976685),
 .....

   (refer    example 2    of    issues    section for comparison)

   below is visualisation of sense2vec
   [1*rqudp89tm0crhnh74eilog.jpeg]
   fig no. 10

   below is the python code for preparing training dataset for sense2vec
   [1*62tr97bfbao5j4j0xmt82q.jpeg]
   fig no. 11

   these codes are available at [26]github.
     __________________________________________________________________

conclusion

     * id97 can hold enormous information compared to their size!
     * they can learn both semantics and syntax
     * the one problem is generalisation over multiple contexts but that
       too can be tackled with additional modification of training text
     * they are computation friendly as they are all array of numbers
     * the relationship between vectors can be discovered with just linear
       algebra
     __________________________________________________________________

   next : id97 (part 2) use cases

   prev : [27]natural language processing (nlp)

     * [28]machine learning
     * [29]data science
     * [30]nlp
     * [31]id97

   (button)
   (button)
   (button) 366 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of mukul malik

[33]mukul malik

   nlu & metacognition researcher | cosmology enthusiast

     (button) follow
   [34]hacker noon

[35]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 366
     * (button)
     *
     *

   [36]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [37]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fe2ec6514d70
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/id97-part-1-fe2ec6514d70&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/id97-part-1-fe2ec6514d70&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_1fvtydqsdmlq---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@mukulmalik?source=post_header_lockup
  15. https://hackernoon.com/@mukulmalik
  16. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  17. http://en.wikipedia.org/wiki/multidimensional_scaling
  18. https://en.wikipedia.org/wiki/sammon_mapping
  19. http://en.wikipedia.org/wiki/nearest_neighbor_graph
  20. http://jmlr.csail.mit.edu/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  21. http://colah.github.io/posts/2014-10-visualizing-mnist/
  22. http://textminingonline.com/training-id97-model-on-english-wikipedia-by-gensim
  23. https://radimrehurek.com/gensim/index.html
  24. https://www.tensorflow.org/
  25. https://www.tensorflow.org/versions/r0.11/tutorials/id97/index.html
  26. https://github.com/mukulmalik18/preprocessing
  27. https://medium.com/@mukulmalik/natural-language-processing-nlp-933cb7162932#.280sgl482
  28. https://hackernoon.com/tagged/machine-learning?source=post
  29. https://hackernoon.com/tagged/data-science?source=post
  30. https://hackernoon.com/tagged/nlp?source=post
  31. https://hackernoon.com/tagged/id97?source=post
  32. https://hackernoon.com/@mukulmalik?source=footer_card
  33. https://hackernoon.com/@mukulmalik
  34. https://hackernoon.com/?source=footer_card
  35. https://hackernoon.com/?source=footer_card
  36. https://hackernoon.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/fe2ec6514d70/share/twitter
  40. https://medium.com/p/fe2ec6514d70/share/facebook
  41. https://medium.com/p/fe2ec6514d70/share/twitter
  42. https://medium.com/p/fe2ec6514d70/share/facebook
