   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

the history of deep learning         explored through 6 code snippets

   [11]go to the profile of emil wallner
   [12]emil wallner (button) blockedunblock (button) followfollowing
   oct 4, 2017
   [1*1xweumpria1eovvmbzfeba.jpeg]
   source: google press image for deep mind

   in this article, we   ll explore six snippets of code that made deep
   learning what it is today. we   ll cover the inventors and the background
   to their breakthroughs. each story includes simple code samples on
   [13]floydhub and [14]github to play around with.

   if this is your first encounter with deep learning, i   d suggest reading
   my [15]deep learning 101 for developers.

   to run the code examples on floydhub, install the [16]floydcommand line
   tool. then clone the [17]code examples i   ve provided to your local
   machine.

   note: if you are new to floydhub, you might want to first read the
   [18]getting started with floydhub section in my earlier post.

   initiate the cli in the example project folder on your local machine.
   now you can spin up the project on floydhub with the following command:
floyd run --data emilwallner/datasets/mnist/1:mnist --tensorboard --mode jupyter

the method of least squares

   deep learning started with a snippet of math.

   i   ve translated it into python:
# y = mx + b
# m is slope, b is y-intercept
def compute_error_for_line_given_points(b, m, coordinates):
    totalerror = 0
    for i in range(0, len(coordinates)):
        x = coordinates[i][0]
        y = coordinates[i][1]
        totalerror += (y - (m * x + b)) ** 2
    return totalerror / float(len(coordinates))
# example
compute_error_for_line_given_points(1, 2, [[3,6],[6,9],[12,18]])

   this was first published by adrien-marie legendre in 1805. he was a
   parisian mathematician who was also known for measuring the meter.

   he had a particular obsession with predicting the future location of
   comets. he had the locations of a couple of past comets. he was
   relentless as he used them in his search for a method to calculate
   their trajectory.

   it really was one of those spaghetti-on-the-wall moments. he tried
   several methods, then one version finally stuck with him.

   legendre   s process started by guessing the future location of a comet.
   then he squared the errors he made, and finally remade his guess to
   reduce the sum of the squared errors. this was the seed for linear
   regression.

   play with the above code in the jupyter notebook i   ve provided to get a
   feel for it. m is the coefficient and b in the constant for your
   prediction, and the coordinates are the locations of the comet. the
   goal is to find a combination of m and b where the error is as small as
   possible.
   [1*3p73sxws21_h6-iigbn3aq.png]

   this is the core of deep learning:
     * take an input and a desired output
     * then search for the correlation between the two

id119

   legendre   s method of manually trying to reduce the error rate was
   time-consuming. peter debye was a nobel prize winner from the
   netherlands. he formalized a [19]solution for this process a century
   later in 1909.

   let   s imagine that legendre had one parameter to worry about         we   ll
   call it x. the y axis represents the error value for each value of x.
   legendre was searching for where x results in the lowest error.

   in this graphical representation, we can see that the value of x that
   minimizes the error y is when x = 1.1.
   [1*jegf_uu78bj8pdyrwzirgq.png]

   peter debye noticed that the slope to the left of the minimum is
   negative, while it   s positive on the other side. thus, if you know the
   value of the slope at any given x value, you can guide y towards its
   minimum.

   this led to the method of id119. the principle is used in
   almost every deep learning model.

   to play with this, let   s assume that the error function is error = x   
   -2x  -2. to know the slope of any given x value we take its derivative,
   which is 5x    - 6x        :
   [1*9o7dxvooxqfqzsv18oqh8w.png]

   watch [20]khan academy   s video if you need to brush up your knowledge
   on derivatives.

   debye   s math translated into python:
current_x = 0.5 # the algorithm starts at x=0.5
learning_rate = 0.01 # step size multiplier
num_iterations = 60 # the number of times to train the function
#the derivative of the error function (x**4 = the power of 4 or x^4)
def slope_at_given_x_value(x):
   return 5 * x**4 - 6 * x**2
# move x to the right or left depending on the slope of the error function
for i in range(num_iterations):
   previous_x = current_x
   current_x += -learning_rate * slope_at_given_x_value(previous_x)
   print(previous_x)
print("the local minimum occurs at %f" % current_x)

   the trick here is the learning_rate. by going in the opposite direction
   of the slope it approaches the minimum. additionally, the closer it
   gets to the minimum, the smaller the slope gets. this reduces each step
   as the slope approaches zero.

   num_iterations is your estimated time of iterations before you reach
   the minimum. play with the parameters it to get an intuition for
   id119.

id75

   combining the method of least square and id119 you get
   id75. in the 1950s and 1960s, a group of experimental
   economists implemented versions of these ideas on early computers. the
   logic was implemented on physical punch cards         truly handmade software
   programs. it took several days to prepare these punch cards and up to
   24 hours to run one regression analysis through the computer.

   here   s a id75 example translated into python so that you
   don   t have to do it in punch cards:
#price of wheat/kg and the average price of bread
wheat_and_bread = [[0.5,5],[0.6,5.5],[0.8,6],[1.1,6.8],[1.4,7]]
def step_gradient(b_current, m_current, points, learningrate):
    b_gradient = 0
    m_gradient = 0
    n = float(len(points))
    for i in range(0, len(points)):
        x = points[i][0]
        y = points[i][1]
        b_gradient += -(2/n) * (y - ((m_current * x) + b_current))
        m_gradient += -(2/n) * x * (y - ((m_current * x) + b_current))
    new_b = b_current - (learningrate * b_gradient)
    new_m = m_current - (learningrate * m_gradient)
    return [new_b, new_m]
def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_i
terations):
    b = starting_b
    m = starting_m
    for i in range(num_iterations):
        b, m = step_gradient(b, m, points, learning_rate)
    return [b, m]
gradient_descent_runner(wheat_and_bread, 1, 1, 0.01, 100)

   this should not introduce anything new. however, it can be a bit of a
   mind boggle to merge the error function with id119. run the
   code and play around with this [21]id75 simulator.

the id88

   enter frank rosenblatt         the guy who dissected rat brains during the
   day and searched for signs of extraterrestrial life at night. in 1958,
   he hit the front page of new york times:    [22]new navy device learns by
   doing    with a machine that mimics a [23]neuron.

   if you showed rosenblatt   s machine 50 sets of two images, one with a
   mark to the left and the other on the right, it could make the
   distinction without being pre-programmed. the public got carried away
   with the possibilities of a true learning machine.
   [1*mrtaivhv7l2hkaaraprhfw.png]

   for every training cycle, you start with input data to the left.
   initial random weights are added to all the input data. they are then
   summed up. if the sum is negative, it   s translated into 0, otherwise,
   it   s mapped into a 1.

   if the prediction is correct, then nothing happens to the weights in
   that cycle. if it   s wrong, you multiply the error with a learning rate.
   this adjusts the weights accordingly.

   let   s run the [24]id88 with the classic or logic.
   [1*mxji3qyemqoqkyyhvo1flw.png]

   the id88 machine translated into python:
from random import choice
from numpy import array, dot, random
1_or_0 = lambda x: 0 if x < 0 else 1
training_data = [ (array([0,0,1]), 0),
                    (array([0,1,1]), 1),
                    (array([1,0,1]), 1),
                    (array([1,1,1]), 1), ]
weights = random.rand(3)
errors = []
learning_rate = 0.2
num_iterations = 100
for i in range(num_iterations):
    input, truth = choice(training_data)
    result = dot(weights, input)
    error = truth - 1_or_0(result)
    errors.append(error)
    weights += learning_rate * error * input

for x, _ in training_data:
    result = dot(x, w)
    print("{}: {} -> {}".format(input[:2], result, 1_or_0(result)))

   in 1969, marvin minsky and seymour papert destroyed the [25]idea. at
   the time, minsky and papert ran the ai lab at mit. they wrote a book
   proving that the id88 could only solve linear problems. they also
   debunked claims about the multi-layer id88. sadly, frank
   rosenblatt died in a boat accident two years later.

   in 1970 a finnish master student, discovered the [26]theory to solve
   non-linear problems with multi-layered id88s. because of the
   mainstream criticism of the id88, the funding of ai dried up for
   more than a decade. this was known as the first ai winter.

   the power of minsky and papert   s critique was the xor problem. the
   logic is the same as the or logic with one exception         when you have
   two true statements (1 & 1), you return false (0).
   [1*re6xwuu9ya6dwfkij8imwq.png]

   in the or logic, it   s possible to divide the true combination from the
   false ones. but as you can see, you can   t divide the xor logic with one
   linear function.

id158s

   by 1986, several experiments proved that neural networks could solve
   complex nonlinear problems. at the time, computers were 10,000 times
   faster compared to when the theory was developed. this is how
   [27]rumelhart introduced the legendary paper:

     we describe a new learning procedure, back-propagation, for networks
     of neuron-like units. the procedure repeatedly adjusts the weights
     of the connections in the network so as to minimize a measure of the
     difference between the actual output vector of the net and the
     desired output vector. as a result of the weight adjustments,
     internal    hidden    units which are not part of the input or output
     come to represent important features of the task domain, and the
     regularities in the task are captured by the interactions of these
     units. the ability to create useful new features distinguishes
     back-propagation from earlier, simpler methods such as the
     id88-convergence procedure            nature 323, 533   536 (09 october
     1986)

   to understand the core of this paper, we   ll code the implementation by
   deepmind   s andrew trask. this is not a random snippet of code. it   s
   been used in andrew karpathy   s deep learning course at stanford and
   siraj raval   s udacity course. it solves the xor problem, thawing the
   first ai winter.
   [1*zpkoid122jm8ijvefqm9i5hg.png]

   before we dig into the code, play with [28]this simulator for one to
   two hours to grasp the core logic. then read [29]trask   s blog post.

   note that the added parameter [1] in the x_xor data are [30]bias
   neurons.

   they have the same behavior as a constant in a linear function:
import numpy as np
x_xor = np.array([[0,0,1], [0,1,1], [1,0,1],[1,1,1]])
y_truth = np.array([[0],[1],[1],[0]])
np.random.seed(1)
syn_0 = 2*np.random.random((3,4)) - 1
syn_1 = 2*np.random.random((4,1)) - 1
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output
def sigmoid_output_to_derivative(output):
    return output*(1-output)
for j in range(60000):
    layer_1 = sigmoid(np.dot(x_xor, syn_0))
    layer_2 = sigmoid(np.dot(layer_1, syn_1))
    error = layer_2 - y_truth
    layer_2_delta = error * sigmoid_output_to_derivative(layer_2)
    layer_1_error = layer_2_delta.dot(syn_1.t)
    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)
    syn_1 -= layer_1.t.dot(layer_2_delta)
    syn_0 -= x_xor.t.dot(layer_1_delta)

print("output after training: \n", layer_2)

   id26, id127, and id119 combined
   can be hard to wrap your mind around. the visualizations of this
   process is often a simplification of what   s going on behind the hood.
   focus on understanding the logic behind it, but don   t worry too much of
   having a mental picture of it.

   also, look at andrew karpathy   s [31]lecture on id26, play
   with [32]these visualizations, and read michael nielsen   s [33]chapter
   on it.

deep neural networks

   deep neural networks are neural networks with more than one layer
   between the input and output layer. the notion was [34]introduced by
   rina dechter in 1986. but it didn   t gain mainstream attention until
   [35]2012. this was soon after ibm watson   s [36]jeopardy victory and
   google   s [37]cat recognizer.

   the core structure of deep neural network have stayed the same. but
   they are now applied to several different problems. there have also
   been a lot of improvement in id173.

   in 1963, it was a set of math functions to simplify [38]noisy earth
   data. they are now used in neural networks to improve their ability to
   [39]generalize.

   a large share of the innovation is due to computing power. this
   improved researcher   s innovation cycles         what took a supercomputer one
   year to calculate in the mid-eighties takes half a second with today   s
   gpu technology.

   the reduced cost in computing and the development of deep learning
   libraries have now made it accessible to the general public. let   s look
   at an example of a common deep learning stack, starting from the bottom
   layer:
     * gpu > nvidia tesla k80. the hardware commonly used for graphics
       processing. compared to cpus, they are on average 50   200 times
       faster for deep learning.
     * cuda > low level programming language for the gpus
     * cudnn > nvidia   s library to optimize cuda
     * tensorflow > google   s deep learning framework on top of cudnn
     * tflearn > a front-end framework for tensorflow

   let   s have a look at the [40]mnist image classification of digits, the
      hello world    of deep learning.
   [0*nqqyzeimmqal4rmf.png]

   implemented in tflearn:
from __future__ import division, print_function, absolute_import
import tflearn
from tflearn.layers.core import dropout, fully_connected
from tensorflow.examples.tutorials.mnist import input_data
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.id172 import local_response_id172
from tflearn.layers.estimator import regression
# data loading and preprocessing
mnist = input_data.read_data_sets("/data/", one_hot=true)
x, y, testx, testy = mnist.train.images, mnist.train.labels, mnist.test.images,
mnist.test.labels
x = x.reshape([-1, 28, 28, 1])
testx = testx.reshape([-1, 28, 28, 1])
# building convolutional network
network = tflearn.input_data(shape=[none, 28, 28, 1], name='input')
network = conv_2d(network, 32, 3, activation='relu', regularizer="l2")
network = max_pool_2d(network, 2)
network = local_response_id172(network)
network = conv_2d(network, 64, 3, activation='relu', regularizer="l2")
network = max_pool_2d(network, 2)
network = local_response_id172(network)
network = fully_connected(network, 128, activation='tanh')
network = dropout(network, 0.8)
network = fully_connected(network, 256, activation='tanh')
network = dropout(network, 0.8)
network = fully_connected(network, 10, activation='softmax')
network = regression(network, optimizer='adam', learning_rate=0.01,
                        loss='categorical_crossid178', name='target')
# training
model = tflearn.dnn(network, tensorboard_verbose=0)
model.fit({'input': x}, {'target': y}, n_epoch=20,
            validation_set=({'input': testx}, {'target': testy}),
            snapshot_step=100, show_metric=true, run_id='convnet_mnist')

   there are plenty of great articles explaining the mnist problem:
   [41]here and [42]here.

let   s sum it up

   as you see in the tflearn example, the main logic of deep learning is
   still similar to rosenblatt   s id88. instead of using a binary
   heaviside step function, today   s networks mostly use relu (rectifier
   linear unit) activations.

   in the last layer of the convolutional neural network, loss equals
   categorical_crossid178. this is an evolution of legendre   s least
   square, a logistical regression for multiple categories. the optimizer
   adamoriginates from the work of debye    id119.

   tikhonov   s id173 notion is widely implemented in the form of
   dropout layers and id173 functions, l1/l2.

   if you want a better intuition for neural networks and how to implement
   them, read my previous post: [43]deep learning 101 for coders.

   thanks to [44]ignacio tonoli de maussion, brian young, [45]paal rgd,
   [46]tomas mo  ka, and [47]charlie harrington for reading drafts of this.
   code sources are included in the jupyter notebooks.

about emil wallner

   this is a part of a multi-part blog series as i learn deep learning.
   i   ve spent a decade exploring human learning. i worked for oxford   s
   business school, invested in education startups, and built an education
   technology business. last year, i enrolled at [48]ecole 42 to apply my
   knowledge of human learning to machine learning.

   you can follow along my learning journey on [49]twitter. if you have
   any questions/suggestions please leave a comment below or ping me on
   [50]medium.

   this was first published as a community post on [51]floydhub   s blog.

     * [52]machine learning
     * [53]data science
     * [54]artificial intelligence
     * [55]tech
     * [56]startup

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [57]go to the profile of emil wallner

[58]emil wallner

   i study cs at 42 paris, write, and experiment with deep learning.

     (button) follow
   [59]freecodecamp.org

[60]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [61]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [62]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d0a0e8545202
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/the-history-of-deep-learning-explored-through-6-code-snippets-d0a0e8545202&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/the-history-of-deep-learning-explored-through-6-code-snippets-d0a0e8545202&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_2xsydaznqrnf---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@emilwallner?source=post_header_lockup
  12. https://medium.freecodecamp.org/@emilwallner
  13. https://www.floydhub.com/emilwallner/projects/deep-learning-from-scratch/
  14. https://github.com/emilwallner/deep-learning-from-scratch
  15. https://medium.freecodecamp.org/deep-learning-for-developers-tools-you-can-use-to-code-neural-networks-on-day-1-34c4435ae6b
  16. https://www.youtube.com/watch?v=bylq9kgjtdq&t=167s
  17. https://github.com/emilwallner/deep-learning-from-scratch
  18. http://blog.floydhub.com/my-first-weekend-of-deep-learning
  19. https://www.abebooks.de/erstausgabe/n  herungsformeln-zylinderfunktionen-gro  e-werte-arguments-unbeschr  nkt/5088409685/bd
  20. https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules/ab-basic-diff-rules/v/derivative-properties-example
  21. https://www.mladdict.com/linear-regression-simulator
  22. http://query.nytimes.com/mem/archive-free/pdf?res=9d01e4d8173de53bbc4053dfb1668383649ede
  23. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf
  24. https://en.wikipedia.org/wiki/id88
  25. https://mitpress.mit.edu/books/id88s
  26. http://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf
  27. http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html?foxtrotcallback=true
  28. https://www.mladdict.com/neural-network-simulator
  29. http://iamtrask.github.io/2015/07/12/basic-python-network/
  30. https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  31. https://www.youtube.com/watch?v=i94ovyb6noo
  32. http://www.benfrederickson.com/numerical-optimization/
  33. http://neuralnetworksanddeeplearning.com/chap2.html
  34. http://www.aaai.org/papers/aaai/1986/aaai86-029.pdf
  35. https://trends.google.com/trends/explore?date=all&q=deep learning
  36. http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?pagewanted=all&mcubz=0
  37. https://www.youtube.com/watch?v=tk4qlwtye_s
  38. https://en.wikipedia.org/wiki/tikhonov_id173
  39. https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/
  40. https://www.tensorflow.org/get_started/mnist/beginners
  41. https://www.youtube.com/watch?v=nmd7wjziczc
  42. https://www.oreilly.com/learning/not-another-mnist-tutorial-with-tensorflow
  43. https://medium.freecodecamp.org/deep-learning-for-developers-tools-you-can-use-to-code-neural-networks-on-day-1-34c4435ae6b
  44. https://medium.com/@ignaciotonoli
  45. https://medium.com/u/9fac61fc0c7a
  46. https://medium.com/@tomasmoska
  47. https://medium.com/@whatrocks
  48. https://twitter.com/paulg/status/847844863727087616
  49. https://twitter.com/emilwallner
  50. https://medium.com/@emilwallner
  51. https://blog.floydhub.com/coding-the-history-of-deep-learning
  52. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  53. https://medium.freecodecamp.org/tagged/data-science?source=post
  54. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  55. https://medium.freecodecamp.org/tagged/tech?source=post
  56. https://medium.freecodecamp.org/tagged/startup?source=post
  57. https://medium.freecodecamp.org/@emilwallner?source=footer_card
  58. https://medium.freecodecamp.org/@emilwallner
  59. https://medium.freecodecamp.org/?source=footer_card
  60. https://medium.freecodecamp.org/?source=footer_card
  61. https://medium.freecodecamp.org/
  62. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  64. https://medium.com/p/d0a0e8545202/share/twitter
  65. https://medium.com/p/d0a0e8545202/share/facebook
  66. https://medium.com/p/d0a0e8545202/share/twitter
  67. https://medium.com/p/d0a0e8545202/share/facebook
