   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

applied deep learning - part 1: id158s

   [16]go to the profile of arden dertat
   [17]arden dertat (button) blockedunblock (button) followfollowing
   aug 8, 2017

overview

   welcome to the applied deep learning tutorial series. we will do a
   detailed analysis of several deep learning techniques starting with
   id158s (ann), in particular feedforward neural
   networks. what separates this tutorial from the rest you can find
   online is that we   ll take a hands-on approach with plenty of code
   examples and visualization. i won   t go into too much math and theory
   behind these models to keep the focus on application.

   we will use the [18]keras deep learning framework, which is a high
   level api on top of tensorflow. keras is becoming super popular
   recently because of its simplicity. it   s very easy to build complex
   models and iterate rapidly. i also used barebone tensorflow, and
   actually struggled quite a bit. after trying out keras i   m not going
   back.

   here   s the table of contents. first an overview of ann and the
   intuition behind these deep models. then we will start simple with
   id28, mainly to get familiar with keras. then we will
   train deep neural nets and demonstrate how they outperform linear
   models. we will compare the models on both binary and multiclass
   classification datasets.
    1. [19]ann overview
       1.1) introduction
       1.2) intuition
       1.3) reasoning
    2. [20]id28
       2.1) linearly separable data
       2.2) complex data - moons
       2.3) complex data - circles
    3. [21]id158s (ann)
       3.1) complex data - moons
       3.2) complex data - circles
       3.3) complex data - sine wave
    4. [22]multiclass classification
       4.1) softmax regression
       4.2) deep ann
    5. [23]conclusion

   the code for this article is available [24]here as a jupyter notebook,
   feel free to download and try it out yourself.

   i think you   ll learn a lot from this article. you don   t need to have
   prior knowledge of deep learning, only some basic familiarity with
   general machine learning. so let   s begin   

1. ann overview

1.1) introduction

   id158s (ann) are multi-layer fully-connected neural
   nets that look like the figure below. they consist of an input layer,
   multiple hidden layers, and an output layer. every node in one layer is
   connected to every other node in the next layer. we make the network
   deeper by increasing the number of hidden layers.
   [1*gh5ps4r_a5drl5ebd_gnrg@2x.png]
   figure 1

   if we zoom in to one of the hidden or output nodes, what we will
   encounter is the figure below.
   [1*fcefcrucafymcr0gmfq0qa@2x.png]
   figure 2

   a given node takes the weighted sum of its inputs, and passes it
   through a non-linear activation function. this is the output of the
   node, which then becomes the input of another node in the next layer.
   the signal flows from left to right, and the final output is calculated
   by performing this procedure for all the nodes. training this deep
   neural networid116 learning the weights associated with all the
   edges.

   the equation for a given node looks as follows. the weighted sum of its
   inputs passed through a non-linear activation function. it can be
   represented as a vector dot product, where n is the number of inputs
   for the node.
   [1*xbgesr4ikhql0zg_qnqu-q@2x.png]

   i omitted the bias term for simplicity. bias is an input to all the
   nodes and always has the value 1. it allows to shift the result of the
   activation function to the left or right. it also helps the model to
   train when all the input features are 0. if this sounds complicated
   right now you can safely ignore the bias terms. for completeness, the
   above equation looks as follows with the bias included.
   [1*hwj2oagtw6b8wi0cvkme4w@2x.png]

   so far we have described the forward pass, meaning given an input and
   weights how the output is computed. after the training is complete, we
   only run the forward pass to make the predictions. but we first need to
   train our model to actually learn the weights, and the training
   procedure works as follows:
     * randomly initialize the weights for all the nodes. there are smart
       initialization methods which we will explore in another article.
     * for every training example, perform a forward pass using the
       current weights, and calculate the output of each node going from
       left to right. the final output is the value of the last node.
     * compare the final output with the actual target in the training
       data, and measure the error using a id168.
     * perform a backwards pass from right to left and propagate the error
       to every individual node using id26. calculate each
       weight   s contribution to the error, and adjust the weights
       accordingly using id119. propagate the error gradients
       back starting from the last layer.

   id26 with id119 is literally the    magic    behind
   the deep learning models. it   s a rather long topic and involves some
   calculus, so we won   t go into the specifics in this applied deep
   learning series. for a detailed explanation of id119 refer
   [25]here. a basic overview of id26 is available [26]here.
   for a detailed mathematical treatment refer [27]here and [28]here. and
   for more advanced optimization algorithms refer [29]here.

   in the standard ml world this feed forward architecture is known as the
   multilayer id88. the difference between the ann and id88 is
   that ann uses a non-linear activation function such as sigmoid but the
   id88 uses the step function. and that non-linearity gives the ann
   its great power.

1.2) intuition

   there   s a lot going on already, even with the basic forward pass. now
   let   s simplify this, and understand the intuition behind it.

     essentially what each layer of the ann does is a non-linear
     transformation of the input from one vector space to another.

   let   s use the ann in figure 1 above as an example. we have a
   3-dimensional input corresponding to a vector in 3d space. we then pass
   it through two hidden layers with 4 nodes each. and the final output is
   a 1d vector or a scalar.

   so if we visualize this as a sequence of vector transformations, we
   first map the 3d input to a 4d vector space, then we perform another
   transformation to a new 4d space, and the final transformation reduces
   it to 1d. this is just a chain of id127s. the forward
   pass performs these matrix dot products and applies the activation
   function element-wise to the result. the figure below only shows the
   weight matrices being used (not the activations).
   [1*4u9ofyoxiwqhzofgy-4w-w.png]
   figure 3

   the input vector x has 1 row and 3 columns. to transform it into a 4d
   space, we need to multiply it with a 3x4 matrix. then to another 4d
   space, we multiply with a 4x4 matrix. and finally to reduce it to a 1d
   space, we use a 4x1 matrix.

   notice how the dimensions of the matrices represent the input and
   output dimensions of a layer. the connection between a layer with 3
   nodes and 4 nodes is a id127 using a 3x4 matrix.

   these matrices represent the weights that define the ann. to make a
   prediction using the ann on a given input, we only need to know these
   weights and the activation function (and the biases), nothing more. we
   train the ann via id26 to    learn    these weights.

   if we put everything together it looks like the figure below.
   [1*scev0vy3n-bqbvlv2pjdoq.png]
   figure 4

   a fully connected layer between 3 nodes and 4 nodes is just a matrix
   multiplication of the 1x3 input vector (yellow nodes) with the 3x4
   weight matrix w1. the result of this dot product is a 1x4 vector
   represented as the blue nodes. we then multiply this 1x4 vector with a
   4x4 matrix w2, resulting in a 1x4 vector, the green nodes. and finally
   a using a 4x1 matrix w3 we get the output.

   we have omitted the activation function in the above figures for
   simplicity. in reality after every id127, we apply the
   activation function to each element of the resulting matrix. more
   formally
   [1*bcvn_zuqzqjwenmdxxkinw@2x.png]
   equation 2

   the output of the id127s go through the activation
   function f. in case of the sigmoid function, this means taking the
   sigmoid of each element in the matrix. we can see the chain of matrix
   multiplications more clearly in the equations.

1.3) reasoning

   so far we talked about what deep models are and how they work, but why
   do we need to go deep in the first place?

   we saw that a layer of ann just performs a non-linear transformation of
   its inputs from one vector space to another. if we take a
   classification problem as an example, we want to separate out the
   classes by drawing a decision boundary. the input data in its given
   form is not separable. by performing non-linear transformations at each
   layer, we are able to project the input to a new vector space, and draw
   a complex decision boundary to separate the classes.

   let   s visualize what we just described with a concrete example. given
   the following data we can see that it isn   t linearly separable.
   [1*41rdsity5x4i10ldhceeza@2x.png]

   so we project it to a higher dimensional space by performing a
   non-linear transformation, and then it becomes linearly separable. the
   green hyperplane is the decision boundary.
   [1*nb1gixylcgb9yivwem6byg.png]

   this is equivalent to drawing a complex decision boundary in the
   original input space.
   [1*gibadh6pw3cvrvplfqdunw@2x.png]

     so the main benefit of having a deeper model is being able to do
     more non-linear transformations of the input and drawing a more
     complex decision boundary.

   as a summary, anns are very flexible yet powerful deep learning models.
   they are universal function approximators, meaning they can model any
   complex function. there has been an incredible surge on their
   popularity recently due to a couple of reasons: clever tricks which
   made training these models possible, huge increase in computational
   power especially gpus and distributed training, and vast amount of
   training data. all these combined enabled deep learning to gain
   significant traction.

   this was a brief introduction, there are tons of great tutorials online
   which cover deep neural nets. for reference, i highly recommend
   [30]this paper. it   s a fantastic overview of deep learning and section
   4 covers ann. another great reference is [31]this book which is
   available online.

2. id28

   despite its name, id28 (lr) is a binary classification
   algorithm. it   s the most popular technique for 0/1 classification. on a
   2 dimensional (2d) data lr will try to draw a straight line to separate
   the classes, that   s where the term linear model comes from. lr works
   with any number of dimensions though, not just two. for 3d data it   ll
   try to draw a 2d plane to separate the classes. this generalizes to n
   dimensional data and n-1 dimensional hyperplane separator. if you have
   a supervised binary classification problem, given an input data with
   multiple columns and a binary 0/1 outcome, lr is the first method to
   try. in this section we will focus on 2d data since it   s easier to
   visualize, and in another tutorial we will focus on multidimensional
   input.

1.1) linearly separable data

   first let   s start with an easy example. 2d linearly separable data. we
   are using the scikit-learn make_classification method to generate our
   data and using a helper function to visualize it.

   iframe: [32]/media/d3bec9ab78a6f2d7edea15412b88537a?postid=d7834f67a4f6

   [1*wb4gg6jdftlrpawe83ucuq@2x.png]

   there is a logisticregression classifier available in scikit-learn, i
   won   t go into too much detail here since our goal is to learn building
   models with keras. but here   s how to train an lr model, using the fit
   function just like any other model in scikit-learn. we see the linear
   decision boundary as the green line.

   iframe: [33]/media/ea11c990ed535233e06f436daafdd737?postid=d7834f67a4f6

   [1*4g0gsu92rphn-co9pv1p5a@2x.png]

   as we can see the data is linearly separable. we will now train the
   same id28 model with keras to predict the class
   membership of every input point. to keep things simple for now, we
   won   t perform the standard practices of separating out the data to
   training and test sets, or performing k-fold cross-validation.

   keras has great [34]documentation, check it out for a more detailed
   description of its api. here   s the code for training the model, let   s
   go over it step by step below.

   iframe: [35]/media/2874ae82da05ec8e28d0e72c8bd13af2?postid=d7834f67a4f6

   we will use the sequential model api available [36]here. the sequential
   model allows us to build deep neural networks by stacking layers one on
   top of another. since we   re now building a simple id28
   model, we will have the input nodes directly connected to output node,
   without any hidden layers. note that the lr model has the form y=f(xw)
   where f is the sigmoid function. having a single output layer being
   directly connected to the input reflects this function.

   quick clarification to disambiguate the terms being used. in neural
   networks literature, it   s common to talk about input nodes and output
   nodes. this may sound strange at first glance, what   s an input    node   
   per se? when we say input nodes, we   re talking about the features of a
   given training example. in our case we have 2 features, the x and y
   coordinates of the points we plotted above, so we have 2 input nodes.
   you can simply think of it as a vector of 2 numbers. what about the
   output node then? the output of the id28 model is a
   single number, the id203 of an input data point belonging to
   class 1. in other words p(class=1). the id203 of an input point
   belonging to class 0 is then p(class=0)=1   p(class=1). so you can simply
   think of the output node as a vector with a single number (or simply a
   scalar) between 0 and 1.

   in keras we don   t add layers corresponding to input nodes, we only do
   for hidden and output nodes. in our current model, we don   t have any
   hidden layers, the input nodes are directly connected to the output
   node. this means our neural network definition in keras will just have
   one layer with one node, corresponding to the output node.
model = sequential()
model.add(dense(units=1, input_shape=(2,), activation='sigmoid'))

   the dense function in keras constructs a fully connected neural network
   layer, automatically initializing the weights as biases. it   s a super
   useful function that you will see being used everywhere. the function
   arguments are defined as follows:
     * units: the first argument, representing number of nodes in this
       layer. since we   re constructing the output layer, and we said it
       has only one node, this value is 1.
     * input_shape: the first layer in keras models need to specify the
       input dimensions. the subsequent layers (which we don   t have here
       but we will in later sections) don   t need to specify this argument
       because keras can infer the dimensions automatically. in this case
       our input dimensionality is 2, the x and y coordinates. the
       input_shape parameter expects a vector, so in our case it   s a tuple
       with one number.
     * activation: the activation function of a id28 model
       is the logistic function, or alternatively called the sigmoid. we
       will explore different id180, where to use them and
       why in another tutorial.

model.compile(optimizer='adam', loss='binary_crossid178', metrics=['accuracy']
)

   we then compile the model with the compile function. this creates the
   neural network model by specifying the details of the learning process.
   the model hasn   t been trained yet. right now we   re just declaring the
   optimizer to use and the id168 to minimize. the arguments for
   the compile function are defined as follows:
     * optimizer: which optimizer to use in order to minimize the loss
       function. there are a lot of different optimizers, most of them
       based on id119. we will explore different optimizers in
       another tutorial. for now we will use the adam optimizer, which is
       the one people prefer to use by default.
     * loss: the id168 to minimize. since we   re building a binary
       0/1 classifier, the id168 to minimize is
       binary_crossid178. we will see other examples of id168s
       in later sections.
     * metrics: which metric to report statistics on, for classification
       problems we set this as accuracy.

history = model.fit(x=x, y=y, verbose=0, epochs=50)

   now comes the fun part of actually training the model using the fit
   function. the arguments are as follows:
     * x: the input data, we defined it as x above. it contains the x and
       y coordinates of the input points
     * y: not to be confused with the y coordinate of the input points. in
       all ml tutorials y refers to the labels, in our case the class
       we   re trying to predict: 0 or 1.
     * verbose: prints out the loss and accuracy, set it to 1 to see the
       output.
     * epochs: number of times to go over the entire training data. when
       training models we pass through the training data not just once but
       multiple times.

plot_loss_accuracy(history)

   the output of the fit method is the loss and accuracy at every epoch.
   we then plot it using our custom function, and see that the loss goes
   down to almost 0 over time, and the accuracy goes up to almost 1.
   great! we have successfully trained our first neural network model with
   keras. i know this was a long explanation, but i wanted to explain what
   we   re doing in detail the first time. once you understand what   s going
   on and practice a couple of times, all this becomes second nature.
   [1*rkxn-0ruqo3reekeok8tvg@2x.png]

   below is a plot of the decision boundary. the various shades of blue
   and red represent the id203 of a hypothetical point in that area
   belonging to class 1 or 0. the top left area is classified as class 1,
   with the color blue. the bottom right area is classified as class 0,
   colored as red. and there is a transition around the decision boundary.
   this is a cool way to visualize the decision boundary the model is
   learning.

   iframe: [37]/media/7567b6d9a4315d4dd3d07e04997c254f?postid=d7834f67a4f6

   [1*ml2ks3sco3sx2fkowtfpzq@2x.png]

   the classification report shows the precision and recall of our model.
   we get close to 100% accuracy. the value shown in the report should be
   0.997 but it got rounded up to 1.0.

   iframe: [38]/media/2eefb1d3beacfa10200a4edbcd429091?postid=d7834f67a4f6

   [1*skpvy9u_my-kxrmrbbdtyw@2x.png]

   the confusion matrix shows us how many classes were correctly
   classified vs misclassified. the numbers on the diagonal axis represent
   the number of correctly classified points, the rest are the
   misclassified ones. this particular matrix is not very interesting
   because the model only misclassifies 3 points. we can see one of the
   misclassified points at the top right part of the confusion matrix, the
   true value is class 0 but the predicted value is class 1.

   iframe: [39]/media/624cdb3ea28cd6b21b5e59d19854ab99?postid=d7834f67a4f6

   [1*94hixu2jylbkys5chmwvyg@2x.png]

2.2) complex data - moons

   the previous dataset was linearly separable, so it was trivial for our
   id28 model to separate the classes. here is a more
   complex dataset which isn   t linearly separable. the simple logistic
   regression model won   t be able to clearly distinguish between the
   classes. we   re using the make_moons method of scikit-learn to generate
   the data.

   iframe: [40]/media/6bbf472327bddae02d54afd8c469525b?postid=d7834f67a4f6

   [1*3oxojsetps2enswgh6tiyq@2x.png]

   let   s build another id28 model with the same parameters
   as we did before. on this dataset we get 86% accuracy.

   iframe: [41]/media/743b881b1b0f57837ad467c0816d476d?postid=d7834f67a4f6

   [1*v7uc4ermxsertib_5rm7jg@2x.png]

   the current decision boundary doesn   t look as clean as the one before.
   the model tried to separate out the classes from the middle, but there
   are a lot of misclassified points. we need a more complex classifier
   with a non-linear decision boundary, and we will see an example of that
   soon.
   [1*_wqzllp9uwkugxhnpzwmza@2x.png]

   precision of the model is 86%. it looks good on paper but we should
   easily be able to get 100% with a more complex model. you can imagine a
   curved decision boundary that will separate out the classes, and a
   complex model should be able to approximate that.

   the classification report and the confusion matrix looks as follows.
   [1*4zddbwg95hkz1yt-zumqcg@2x.png]

2.3 complex data - circles

   let   s look at one final example where the liner model will fail. this
   time using the make_circles function in scikit-learn.

   iframe: [42]/media/a5cafd00de9d1cad2bced9672458d66c?postid=d7834f67a4f6

   [1*uxfm-gfajnx3kr1xkt2h7g@2x.png]

   building the model with same parameters.

   iframe: [43]/media/d0f03a26b0c7ab05cf603d68ef62053b?postid=d7834f67a4f6

   [1*h2zlrgfvpygqw2fa-shp8a@2x.png]

   the decision boundary again passes from the middle of the data, but now
   we have much more misclassified points.
   [1*fwnda-xdqkfmhzez0rpzzw@2x.png]

   the accuracy is around 50%, shown below. no matter where the model
   draws the line, it will misclassify half of the points, due to the
   nature of the dataset.

   the confusion matrix we see here is an example one belonging to a poor
   classifier. ideally we prefer confusion matrices to look like the ones
   we saw above. high numbers along the diagonals meaning that the
   classifier was right, and low numbers everywhere else where the
   classifier was wrong. in our visualization, the color blue represents
   large numbers and yellow represents the smaller ones. so we would
   prefer to see blue on the diagonals and yellow everywhere else. blue
   color everywhere is a bad sign meaning that our classifier is confused.
   [1*k2qhw4g4zsc0k3-bonvdia@2x.png]

   the most naive method which always predicts 1 no matter what the input
   is would get a 50% accuracy. our model also got 50% accuracy, so it   s
   not useful at all.

3. id158s (ann)

   now we will train a deep id158s (ann) to better
   classify the datasets which the id28 model struggled,
   moons and circles. we will also classify an even harder dataset of sine
   wave to demonstrate that ann can form really complex decision
   boundaries.

3.1) complex data - moons

   while building keras models for id28 above, we performed
   the following steps:
     * step 1: define a sequential model.
     * step 2: add a dense layer with sigmoid activation function. this
       was the only layer we needed.
     * step 3: compile the model with an optimizer and id168.
     * step 4: fit the model to the dataset.
     * step 5: analyze the results: plotting loss/accuracy curves,
       plotting the decision boundary, looking at the classification
       report, and understanding the confusion matrix.

   while building a deep neural network, we only need to change step 2
   such that, we will add several dense layers one after another. the
   output of one layer becomes the input of the next. keras again does
   most of the heavy lifting by initializing the weights and biases, and
   connecting the output of one layer to the input of the next. we only
   need to specify how many nodes we want in a given layer, and the
   activation function. it   s as simple as that.

   iframe: [44]/media/3382aa6711fb7db55d7a22052e77d813?postid=d7834f67a4f6

   we first add a layer with 4 nodes and tanh activation function. tanh is
   a commonly used activation function, and we   ll learn more about it in
   another tutorial. we then add another layer with 2 nodes again using
   tanh activation. we finally add the last layer with 1 node and sigmoid
   activation. this is the final layer that we also used in the logistic
   regression model.

   this is not a very deep ann, it only has 3 layers: 2 hidden layers, and
   the output layer. but notice a couple of patterns:
     * output layer still uses the sigmoid activation function since we   re
       working on a binary classification problem.
     * hidden layers use the tanh activation function. if we added more
       hidden layers, they would also use tanh activation. we have a
       couple of options for id180: sigmoid, tanh, relu,
       and variants of relu. in another article we   ll explore the pros and
       cons of each one. we will also demonstrate why using sigmoid
       activation in hidden layers is a bad idea. for now it   s safe to use
       tanh.
     * we have fewer number of nodes in each subsequent layer. it   s common
       to have less nodes as we stack layers on top of one another, sort
       of a triangular shape.

   we didn   t build a very deep ann here because it wasn   t necessary. we
   already achieve 100% accuracy with this configuration.
   [1*df_zs5jvpa16ykk6vlvpwq@2x.png]

   the ann is able to come up with a perfect separator to distinguish the
   classes.
   [1*mzsmi_1vbhvig3f7f5a7oq@2x.png]

   100% precision, nothing misclassified.
   [1*70gndqtixf1gd_ks5uv9za@2x.png]

3.2) complex data - circles

   now let   s look at the circles dataset, where the lr model achieved only
   50% accuracy. the model is the same as above, we only change the input
   to the fit function using the current dataset. and we again achieve
   100% accuracy.

   iframe: [45]/media/073ffa0820af3c73e126613827258b12?postid=d7834f67a4f6

   [1*rurgfss3od2xjf28oh8a0w@2x.png]

   similarly the decision boundary looks just like the one we would draw
   by hand ourselves. the ann was able to figure out an optimal separator.
   [1*0_yafsax8qiy7zqibuymtg@2x.png]

   just like above we get 100% accuracy.
   [1*u_wisjjtttc1is0amn9q6a@2x.png]

4.3) complex data - sine wave

   let   s try to classify one final toy dataset. in the previous sections,
   the classes were separable by one continuous decision boundary. the
   boundary had a complex shape, it wasn   t linear, but still one
   continuous decision boundary was enough. ann can draw arbitrary number
   of complex decision boundaries, and we will demonstrate that.

   let   s create a sinusoidal dataset looking like the sine function, every
   up and down belonging to an alternating class. as we can see in the
   figure, a single decision boundary won   t be able to separate out the
   classes. we will need a series of non-linear separators.

   iframe: [46]/media/d358b567e1814ab5f9ca44ce8df37914?postid=d7834f67a4f6

   [1*xqumlylmxan0o4dkceg4ea@2x.png]

   now we need a more complex model for accurate classification. so we
   have 3 hidden layers, and an output layer. the number of nodes per
   layer has also increased to improve the learning capacity of the model.
   choosing the right number of hidden layers and nodes per layer is more
   of an art than science, usually decided by trial and error.

   iframe: [47]/media/a250f8de1b22d34453342b1505c017a0?postid=d7834f67a4f6

   [1*vl-vl7auc7vb4iby2t8fya@2x.png]

   the ann was able to model a pretty complex set of decision boundaries.
   [1*gw4w4ncte8wrlsqqqyqina@2x.png]

   precision is 99%, we only have 14 misclassified points out of 2400.
   pretty good.
   [1*gok87nlrvd4yek1zuhqktw@2x.png]

4. multiclass classification

   in the previous sections we worked on binary classification. now we
   will take a look at a multi-class classification problem, where the
   number of classes is more than 2. we will pick 3 classes for
   demonstration, but our approach generalizes to any number of classes.

   here   s how our dataset looks like, spiral data with 3 classes, using
   the make_multiclass method in scikit-learn.

   iframe: [48]/media/6ced20b143a087cb6abc305515142d81?postid=d7834f67a4f6

   [1*cfkgup89x6srvb857m0i_g@2x.png]

4.1) softmax regression

   as we saw above, id28 (lr) is a classification method
   for 2 classes. it works with binary labels 0/1. softmax regression (sr)
   is a generalization of lr where we can have more than 2 classes. in our
   current dataset we have 3 classes, represented as 0/1/2.

   building the model for sr is very similar to lr, for reference here   s
   how we built our id28 model.

   iframe: [49]/media/f2041b59a4901fe8256a29a4243dd9f0?postid=d7834f67a4f6

   and here   s how how we will build the softmax regression model.

   iframe: [50]/media/f49f68984730ff18ff073f6420e27db0?postid=d7834f67a4f6

   there are a couple of differences, let   s go over them one by one:
     * number of nodes in the dense layer: lr uses 1 node, where sr has 3
       nodes. since we have 3 classes it makes sense for sr to be using 3
       nodes. then the question is, why does lr uses only 1 node, it has 2
       classes so it appears like we should have used 2 nodes instead. the
       answer is, because we can achieve the same result with using only 1
       node. as we saw above, lr models the id203 of an example
       belonging to class one: p(class=1). and we can calculate class 0
       id203 by: 1   p(class=1). but when we have more than 2 classes,
       we need individual nodes for each class. because knowing the
       id203 of one class doesn   t let us infer the id203 of
       the other classes.
     * activation function: lr used sigmoid activation function, sr uses
       softmax. softmax scales the values of the output nodes such that
       they represent probabilities and sum up to 1. so in our case
       p(class=0)+p(class=1)+p(class=2)=1. it doesn   t do it in a naive way
       by dividing individual probabilities by the sum though, it uses the
       exponential function. so higher values get emphasized more and
       lower values get squashed more. we will talk in detail what softmax
       does in another tutorial. for now you can simply think of it as a
       id172 function which lets us interpret the output values as
       probabilities.
     * id168: in a binary classification problem like lr, the loss
       function is binary_crossid178. in the multiclass case, the loss
       function is categorical_crossid178. categorical crossid178 is
       the generalization of binary crossid178 to more than 2 classes.
       going into the theory behind id168s is beyond the scope of
       this tutorial. but for now only knowing this property is enough.
     * fit function: lr used the vector y directly in the fit function,
       which has just one column with 0/1 values. when we   re doing sr the
       labels need to be in one-hot representation. in our case y_cat is a
       matrix with 3 columns, where all the values are 0 except for the
       one that represents our class, and that is 1.

   it took some time to talk about all the differences between lr and sr,
   and it looks like there   s a lot to digest. but again after some
   practice this will become a habit, and you won   t even need to think
   about any of this.

   after all this theory let   s take a step back and remember that lr is a
   linear classifier. sr is also a linear classifier, but for multiple
   classes. so the    power    of the model hasn   t changed, it   s still a
   linear model. we just generalized lr to apply it to a multiclass
   problem.

   training the model gives us an accuracy of around 50%. the most naive
   method which always predicts class 1 no matter what the input is would
   have an accuracy of 33%. the sr model is not much of an improvement
   over it. which is expected because the dataset is not linearly
   separable.
   [1*xirokykoh0n2ycu2_q6jka@2x.png]

   looking at the decision boundary confirms that we still have a linear
   classifier. the lines look jagged due to floating point rounding but in
   reality they   re straight.
   [1*woy7xvz5h25_v4h4c3rjua@2x.png]

   here   s the precision and recall corresponding to the 3 classes. and the
   confusion matrix is all over the place. clearly this is not an optimal
   classifier.
   [1*nupea9z40glloquvif-ktw@2x.png]

4.2) deep ann

   now let   s build a deep ann for multiclass classification. remember that
   the changes going from lr to ann was minimal. we only needed to add
   more dense layers. we will do the same again. adding a couple of dense
   layers with tanh activation function.

   iframe: [51]/media/2f62fe2ac839c35dc0f86f09119ac889?postid=d7834f67a4f6

   note that the output layer still has 3 nodes, and uses the softmax
   activation. the id168 also didn   t change, still
   categorical_crossid178. these won   t change going from a linear model
   to a deep ann, since the problem definition hasn   t changed. we   re still
   working on multiclass classification. but now using a more powerful
   model, and that power comes from adding more layers to our neural net.

   we achieve 99% accuracy in just a couple of epochs.
   [1*hch6xrgyv7gibxkaccuong@2x.png]

   the decision boundary is non-linear.
   [1*fwqt18guealkt34_hidhaq@2x.png]

   we got almost 100% accuracy. we totally misclassified 5 points out of
   1500.
   [1*d0g6affcvkiyaxqznp8mfg@2x.png]

5) conclusion

   thanks for spending time reading this article, i know this was a rather
   lengthy tutorial on id158s and keras. i wanted to
   be as detailed as possible, while still keeping the article length
   manageable. i hope you enjoyed it.

   there was a common theme in this article such that we first introduced
   the task, we then approached it using a simple method and observed the
   limitations. afterwards we used a more complex deep model to improve on
   it and got much better results. i think the ordering is important. no
   complex method becomes successful unless it has evolved from a simpler
   model.

   the entire code for this article is available [52]here if you want to
   hack on it yourself. if you have any feedback feel free to reach out to
   me on [53]twitter.

     * [54]machine learning
     * [55]deep learning
     * [56]neural networks
     * [57]data science
     * [58]towards data science

   (button)
   (button)
   (button) 3.8k claps
   (button) (button) (button) 26 (button) (button)

     (button) blockedunblock (button) followfollowing
   [59]go to the profile of arden dertat

[60]arden dertat

   ml engineer @ pinterest. photography and travel enthusiast.

     (button) follow
   [61]towards data science

[62]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.8k
     * (button)
     *
     *

   [63]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [64]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d7834f67a4f6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_hn1ss6pqt6cc---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ardendertat?source=post_header_lockup
  17. https://towardsdatascience.com/@ardendertat
  18. https://keras.io/
  19. https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6?gi=cb5914a7da5f#04e7
  20. https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6?gi=cb5914a7da5f#fe06
  21. https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6?gi=cb5914a7da5f#106c
  22. https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6?gi=cb5914a7da5f#860e
  23. https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6?gi=cb5914a7da5f#0f8f
  24. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 1 - id158s.ipynb
  25. https://iamtrask.github.io/2015/07/27/python-network-part2/
  26. https://ml.berkeley.edu/blog/2017/02/04/tutorial-3/
  27. http://cs231n.github.io/optimization-2/
  28. http://neuralnetworksanddeeplearning.com/chap2.html
  29. http://ruder.io/optimizing-gradient-descent/index.html
  30. http://u.cs.biu.ac.il/~yogo/nnlp.pdf
  31. http://neuralnetworksanddeeplearning.com/chap1.html
  32. https://towardsdatascience.com/media/d3bec9ab78a6f2d7edea15412b88537a?postid=d7834f67a4f6
  33. https://towardsdatascience.com/media/ea11c990ed535233e06f436daafdd737?postid=d7834f67a4f6
  34. https://keras.io/
  35. https://towardsdatascience.com/media/2874ae82da05ec8e28d0e72c8bd13af2?postid=d7834f67a4f6
  36. https://keras.io/getting-started/sequential-model-guide/
  37. https://towardsdatascience.com/media/7567b6d9a4315d4dd3d07e04997c254f?postid=d7834f67a4f6
  38. https://towardsdatascience.com/media/2eefb1d3beacfa10200a4edbcd429091?postid=d7834f67a4f6
  39. https://towardsdatascience.com/media/624cdb3ea28cd6b21b5e59d19854ab99?postid=d7834f67a4f6
  40. https://towardsdatascience.com/media/6bbf472327bddae02d54afd8c469525b?postid=d7834f67a4f6
  41. https://towardsdatascience.com/media/743b881b1b0f57837ad467c0816d476d?postid=d7834f67a4f6
  42. https://towardsdatascience.com/media/a5cafd00de9d1cad2bced9672458d66c?postid=d7834f67a4f6
  43. https://towardsdatascience.com/media/d0f03a26b0c7ab05cf603d68ef62053b?postid=d7834f67a4f6
  44. https://towardsdatascience.com/media/3382aa6711fb7db55d7a22052e77d813?postid=d7834f67a4f6
  45. https://towardsdatascience.com/media/073ffa0820af3c73e126613827258b12?postid=d7834f67a4f6
  46. https://towardsdatascience.com/media/d358b567e1814ab5f9ca44ce8df37914?postid=d7834f67a4f6
  47. https://towardsdatascience.com/media/a250f8de1b22d34453342b1505c017a0?postid=d7834f67a4f6
  48. https://towardsdatascience.com/media/6ced20b143a087cb6abc305515142d81?postid=d7834f67a4f6
  49. https://towardsdatascience.com/media/f2041b59a4901fe8256a29a4243dd9f0?postid=d7834f67a4f6
  50. https://towardsdatascience.com/media/f49f68984730ff18ff073f6420e27db0?postid=d7834f67a4f6
  51. https://towardsdatascience.com/media/2f62fe2ac839c35dc0f86f09119ac889?postid=d7834f67a4f6
  52. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 1 - id158s.ipynb
  53. https://twitter.com/ardendertat
  54. https://towardsdatascience.com/tagged/machine-learning?source=post
  55. https://towardsdatascience.com/tagged/deep-learning?source=post
  56. https://towardsdatascience.com/tagged/neural-networks?source=post
  57. https://towardsdatascience.com/tagged/data-science?source=post
  58. https://towardsdatascience.com/tagged/towards-data-science?source=post
  59. https://towardsdatascience.com/@ardendertat?source=footer_card
  60. https://towardsdatascience.com/@ardendertat
  61. https://towardsdatascience.com/?source=footer_card
  62. https://towardsdatascience.com/?source=footer_card
  63. https://towardsdatascience.com/
  64. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  66. https://medium.com/p/d7834f67a4f6/share/twitter
  67. https://medium.com/p/d7834f67a4f6/share/facebook
  68. https://medium.com/p/d7834f67a4f6/share/twitter
  69. https://medium.com/p/d7834f67a4f6/share/facebook
