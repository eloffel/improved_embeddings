   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

applied deep learning - part 2: real world case studies

   [16]go to the profile of arden dertat
   [17]arden dertat (button) blockedunblock (button) followfollowing
   sep 1, 2017

overview

   welcome to part 2 of applied deep learning series. [18]part 1 was a
   hands-on introduction to id158s, covering both the
   theory and application with a lot of code examples and visualization.
   now comes the cool part, end-to-end application of deep learning to
   real-world datasets. we will cover the 3 most commonly encountered
   problems as case studies: binary classification, multiclass
   classification and regression.
    1. [19]case study: binary classification
       1.1) data visualization & preprocessing
       1.2) id28 model
       1.3) ann model
       1.4) visualization of deep ann
    2. [20]case study: multiclass classification
       2.1) data visualization & preprocessing
       2.2) softmax regression model
       2.3) ann model
       2.4) cross validation
    3. [21]case study: regression
       3.1) data visualization & preprocessing
       3.2) id75 model
       3.3) ann model
    4. [22]conclusion

   the code for this article is available [23]here as a jupyter notebook,
   feel free to download and try it out yourself.

1. case study: binary classification

   we will be using the human resources analytics [24]dataset on kaggle.
   we   re trying to predict whether an employee will leave based on various
   features such as number of projects they worked on, time spent at the
   company, last performance review, salary etc. the dataset has around
   15,000 rows and 9 columns. the column we   re trying to predict is called
      left   . it   s a binary column with 0/1 values. the label 1 means that
   the employee has left.
   [1*cg6blhwp6lvbjdnw_ol0tq.png]

1.1) data visualization & preprocessing

   first things first, let   s perform some data visualization and
   preprocessing before jumping straight into building the model. this
   part is crucial, since we need to know what type of features we are
   dealing with. for every ml task, we at least need to answer the
   following questions:
     * what type of features do we have: real valued, categorical, or
       both?
     * do any of the features need id172?
     * do we have null values?
     * what is the label distribution, are the classes imbalanced?
     * is there a correlation between the features?

   the jupyter notebook contains the detailed analysis. in summary, there
   are both real and categorical features. there are no null values, but
   some features need id172. 76% percent of the examples are
   labeled as 0, meaning the employee didn   t leave.

   let   s check the correlation of the features with the labels (the column
   named    left   ). we will use the seaborn package for the correlation
   plot.

   iframe: [25]/media/a2329a3202ccab890150f4d8678b68c8?postid=1bb4b142a585

   [1*ht9slhatm1niun4mjp71ua@2x.png]

   in this plot, positive values represent correlation and negative values
   represent inverse correlation with the label. of course    left    has
   perfect correlation with itself, you can ignore that. other than that
   only one feature has a strong signal, which is the
      satisfaction_level   , inversely correlated with whether the employee
   has left. which makes sense.

   now let   s look at the pairwise correlation of all features with one
   another.

   iframe: [26]/media/5f2eac36fb7fd31e2ccc3b24007aa0b2?postid=1bb4b142a585

   [1*iy3c6ahezhuza04048yhcw@2x.png]

   we see that    average_monthly_hours    is positively correlated with
      number_project   , which again makes sense. the more projects a person
   is involved with, the more hours of work they need to put in.

   now let   s look at the distribution of feature values. by inspecting the
   histograms of features we can see which ones need id172. what   s
   the motivation behind this? what does id172 mean and why is it
   needed? most ml algorithms perform better if the real valued features
   are scaled to be in a predefined range, for example [0, 1]. this is
   particularly important for deep neural networks. if the input features
   consist of large values, deep nets really struggle to learn. the reason
   is that as the data flows through the layers, with all the
   multiplications and additions, it gets large very quickly and this
   negatively affects the optimization process by saturating
   non-linearities. we will see the detailed demonstration of this in
   another article, for now we need to pay attention to feature values to
   be small numbers.

   looking at feature histograms, we need to normalize 3 of the features:
   average_monthly_hours, number_project, and time_spend_company. all
   other features are within [0, 1] so we can leave them alone.
   [1*yii2yyfkoukhqncimrgcfa@2x.png]

   scikit-learn has several id172 methods, what we will use is
   standardscaler. it individually scales the features such that they have
   zero mean and unit variance, so they all belong to a standard normal(0,
   1) distribution. note that this doesn   t change the ordering of the
   feature values, it just changes the scale. it   s a simple yet extremely
   important trick.

   the data we loaded is in a pandas dataframe. pandas is an extremely
   popular package to deal with tabular data, especially in an interactive
   environment like jupyter notebooks. dataframe is the most commonly used
   data structure of pandas that acts as a container for our data, and
   exposes several built-in functions to make our life easier (check out
   the notebook for more details). in the code snippet below, df is the
   dataframe for our data.

   iframe: [27]/media/7e780c909d64b099d0b1527be11134e2?postid=1bb4b142a585

   scikit-learn api is very well designed and contains 4 very commonly
   used methods. predictors are ml models like id28, and
   transformers are data manipulators like standard scaler.
     * fit: for predictors performs training on the given input. for
       transformers computes the statistics like mean and standard
       deviation of the input to be used later.
     * transform: for transformers manipulates the input data using the
       stats learned by the fit function. we run the transform method
       after fit since there   s a dependency. predictors don   t support this
       method.
     * fit_transform: performs fit + transform in a single call
       efficiently. for transformers, computes the stats of the input and
       performs the transformation. it   s very a commonly used method with
       transformers. for predictors, trains the model and performs
       prediction on the given input.
     * predict: as its name suggests, for predictors performs the
       prediction task using the model trained with the fit method. very
       commonly used with predictors. transformers don   t support this
       method.

   now that we have scaled the real-valued features to be in a desirable
   range, let   s deal with the categorical features. we need to convert
   categorical data to one-hot representation. for example the salary
   column contains 3 unique string values: low, medium and high. after
   one-hot conversion we will have 3 new binary columns: salary_low,
   salary_medium and salary_high. for a given example, only one of them
   will have the value 1, the others will be 0. we will then drop the
   original salary column because we don   t need it anymore.

   the one-hot conversion is performed by the get_dummies of pandas. we
   could have also used the onehotencoder in scikit-learn, they both get
   the job done. since our data is already is in a pandas dataframe,
   get_dummies is easier. it also automatically perform the renaming of
   the features.

   iframe: [28]/media/4a07f2974054bc3a8ae7fd13021ab4d0?postid=1bb4b142a585

   [1*fmdw4hbu3ilcybcgnxuhmq@2x.png]

   now comes the final part of creating the training and test data. the
   model will perform learning on the training set and be evaluated on the
   held-out test set. scikit-learn has a convenient train_test_split
   function. we only need to specify the fraction of the test set, in our
   case 30%. but first we convert our data from pandas dataframe to numpy
   array using the values attribute of the dataframe.

   iframe: [29]/media/199d7a192a809f7d4a23a750b7ecc656?postid=1bb4b142a585

1.2) id28 model

   now that we   re done with the id174 and train/test set
   generation, here comes the fun part, training the model. we first start
   with a simple model, id28 (lr). we will then train a
   deep ann and compare the results to lr.

   after the first article, building the model should be very familiar.

   iframe: [30]/media/84b45efb60bed5ccb2b332640e6717b4?postid=1bb4b142a585

   we get 79% training accuracy. this is actually pretty bad, because
   above we saw that 76% of the labels were 0. so the most naive
   classifier which always outputs 0 regardless of the input would get 76%
   accuracy, and we   re not doing much better than that. this means our
   data is not linearly separable, just like the examples we saw in the
   first article, and we need a more complex model.
   [1*jibpr-u7ubytfsx_kru-2a@2x.png]

   above chart depicts the training loss and accuracy. but more
   importantly we   re interested in the metrics of the test set. metrics in
   the training set might be misleading since the model is already trained
   on it, we want to check how the model performs on an held-out test set.
   test accuracy is 78%, slightly lower than training accuracy. test
   accuracy of ml models are almost always less than training, because the
   test data is unseen to the model during the training process. looking
   at the classification report, we see that only 60% of the examples
   belonging to class 1 are classified correctly. pretty bad performance.
   the confusion matrix also doesn   t look promising showing a lot of
   misclassified examples.
   [1*zsxiljqjbpae-pzp3ud-hg@2x.png]

1.3) ann model

   now let   s build a deep neural network for binary classification. this
   model will be much more powerful, and will be able to model non-linear
   relationships.

   iframe: [31]/media/e8e238c1367127525d6a6186e835d0a1?postid=1bb4b142a585

   the model building process is again very familiar. we have 2 hidden
   layers with 64 and 16 nodes with tanh activation function. the output
   layer uses the sigmoid activation since it   s a binary classification
   problem. we use the adam optimizer with learning rate set to 0.01.

   this time we achieve 97.7% training accuracy, pretty good.
   [1*rtsspetdvjdhk5juxriqoq@2x.png]

   let   s compare the lr and ann models. the ann model is much superior,
   having a lower loss and a higher accuracy.
   [1*btakm5pvbdkcjdbnsdevqa@2x.png]

   and for completeness here   s the classification report and confusion
   matrix of the ann model on the test set. we achieve 97% accuracy,
   compared to 78% of the lr model. we still misclassify 147 examples out
   of 4500.
   [1*ohm6r0iesnqcsie1exxz6g@2x.png]

   we can further improve the performance of the ann by doing the
   following:
     * train the model for longer (increase the number of epochs).
     * hyperparamter tuning: change the learning rate, use a different
       optimizer than adam (rmsprop for example), use another activation
       function than tanh (can be relu).
     * increase the number of nodes per layer: instead of 64   16   1 we can
       do 128   64   1.
     * increase the number of layers: we can do 128   64   32   16   1.

   one important caveat though, as we make the model more powerful, the
   training loss will likely decrease and accuracy will increase. but we
   will run into the risk of overfitting. meaning the complex model will
   perform worse on the test set compared to a simpler model, even though
   the training metrics of the complex model is better. we will talk more
   about overfitting in another article, but this is very important to
   keep in mind. that   s why we don   t go crazy with number of layers and
   nodes per layer. the simplest model that gets the job done is
   sufficient.

1.4) visualization of deep ann

   in the previous article we learned that each layer of the ann performs
   a non-linear transformation of the input from one vector space to
   another. by doing this we are projecting our input data to a new space
   where the classes are separable from each other via a complex decision
   boundary.

   let   s visually demonstrate this. our input data after the initial data
   preprocessing we did above is 20 dimensional. for visualization
   purposes let   s project it to 2d. remember that having k nodes in a
   layer means that this layer transforms its input such that the output
   is a k-dimensional vector. the ann we trained above had two hidden
   layers with 64 and 16 nodes. then we need a new layer with 2 nodes in
   order to project our data to a 2d space. so we add this new layer just
   before the output node. the rest is completely untouched.

   iframe: [32]/media/dfacc3a686ff1c1b36a8a5bd4cd74746?postid=1bb4b142a585

   here   s the resulting projection of our input data from 20d to 2d space.
   the decision boundary corresponds to the last layer of the ann. the ann
   was able to separate out the classes pretty well, with some
   misclassifications. a lot of data points overlap in 2d so we can   t see
   them all, for reference the model misclassifies around 160 points out
   of 4500 (96% accuracy). we aren   t concerned about accuracy with this
   model anyway, we are interested in the projection of a high-dimensional
   input to 2d. this is a neat little trick to visually demonstrate the
   result of the projections performed by the ann.
   [1*qxou_wmksf7iaztkj0qj1w@2x.png]

   a more principled visualization approach would be using id167, which is
   a id84 technique for visualizing high-dimensional
   data. details available [33]here.

2. case study: multiclass classification

   we will now perform multiclass classification on the famous [34]iris
   dataset. it contains 3 classes of flowers with 50 examples each. there
   are a total of 4 features. so it   s pretty small, but very popular. the
   data looks as follows.
   [1*rj5oed4ak6t3nnqmj1ujbq@2x.png]

2.1) data visualization & preprocessing

   this part is easier now since we only have 4 real-valued features.
   again we normalize the features to be between [0, 1]. the feature
   values are small and we could get away without any id172, but
   it   s a good habit to do so, and there   s no harm doing it anyway.

   pairplot is a cool visualization technique if we have a small number of
   features. we can see the pairwise distribution of features colored by
   the class (the column    label    in the dataset). we use the seaborn
   package, pretty simple for an informative plot.

   iframe: [35]/media/2caeab0271f04c7ba87358e1b078c475?postid=1bb4b142a585

   [1*d_olkiexayh3zvde4ikscq@2x.png]

   we can see that the    setosa    class is easily separable but    versicolor   
   and    virginica    are more intermingled.

2.2) softmax regression model

   we will now train a softmax regression (sr) model to predict the
   labels. the previous article contains a detailed explanation, and
   building the model is again very similar to above. the main difference
   is that we   re using softmax activation and categorical_crossid178 as
   the loss.

   iframe: [36]/media/225a47b2d5f5b3cd6d55ae95289469aa?postid=1bb4b142a585

   the sr model already achieves 97% training accuracy and a minimal loss,
   pretty good already.
   [1*-wsv9b8e8pzagvd5sjj24a@2x.png]

2.3) ann model

   now let   s build our ann model. we add 2 hidden layers with 32 and 16
   nodes. notice that we also change the activation function of these
   layers to relu instead of tanh. we will explore various activation
   functions and their differences in another tutorial, but relu is
   arguably the most popular one. then why we haven   t been using it? well,
   only because the decision boundary plots looked prettier with tanh
   activation. seriously, no other reason.

   iframe: [37]/media/53523a76f3dcf9ff11c7a599411f1f4b?postid=1bb4b142a585

   this time we get 100% traning accuracy.
   [1*nnotfolv_dyzxh8g6mreqa@2x.png]

   and for the record, test accuracy of both sr and ann models are 100%.
   this is a pretty small dataset, so these results are not surprising.
   not all problems need a deep neural net to get good results. for this
   problem it   s probably an overkill, since a linear model works just as
   fine.

2.4) cross validation

   with a small sample size like our current situation, it   s especially
   important to perform cross validation to get a better estimate on
   accuracy. with k-fold cross validation we split the dataset into k
   disjoint parts, use k-1 parts for training and the other 1 part for
   testing. this way every example appears in both training and test sets.
   we then average out the model   s performance in all k runs and get a
   better low-variance estimation of the model accuracy.
   [1*z6nothj9rllt_sghgxa3rg@2x.png]

   usually while training deep learning models we don   t perform k-fold
   cross validation. because the training takes a long time, and training
   the model k times from scratch is not feasible. but since our dataset
   is small it   s a good candidate to try on.

   here   s the plot for 5-fold cross validation accuracy for both models.
   the deep model is doing slightly better, has a higher accuracy and
   lower variance. in the figure the accuracies sometimes seem to be above
   100% but that   s an artifact of smoothing the curves. max accuracy we
   get is 100%.
   [1*ptjyvjz-oolf4cixxcibvg@2x.png]

3. case study: regression

   we   ll now work on a regression problem, predicting a real-valued output
   instead of discreet class memberships. we will be using the [38]house
   sales dataset from king county, wa on kaggle. there are around 21,000
   rows with 20 features. the value we   re trying to predict is a floating
   point number labeled as    price   .

3.1) data visualization & preprocessing

   first let   s take a look at feature distributions
   [1*ougf1y-xgtwmp66fmtwpea@2x.png]

   you know the drill by now, we need to do feature id172 and
   categorization. for example squarefoot related features definitely need
   to be normalized since the values range in thousands, and features like
   zipcode need to be categorized.

   we also have a new type of preprocessing to do, bucketization. for
   example the feature which contains the year the house was built
   (yr_built), ranges from 1900 to 2015. we can certainly categorize it
   with every year belonging to a distinct category, but then it would be
   pretty sparse. we would get more signal if we bucketized this feature
   without losing much information. for example if we use 10 year buckets,
   years between [1950, 1959] would be collapsed together. it would
   probably be sufficient to know that this house was built in 1950s
   instead of 1958 exactly.

   other features that would benefit from bucketizing are latitude and
   longitude of the house. the exact coordinate doesn   t matter that much,
   we can round the coordinate to the nearest kilometer. this way the
   feature values will be more dense and informative. there   s no hard and
   set rule to which ranges to use in bucketization, they   re mostly
   decided by trial and error.

   one final transformation we need to do is for the price of the house,
   the value we   re trying to predict. currently its value ranges from $75k
   to $7.7m. a model trying to predict in such a large scale and variance
   would be very unstable. so we normalize that as well. feel free to
   check the code for the details.

   after all the transformations we go from 20 to 165 features. let   s
   check the correlation of each feature with price.
   [1*ychunn2iqbmt850elvcltq@2x.png]

   the most correlated feature is the square footage, which is expected,
   bigger houses are usually more expensive. looking at the list the
   features make sense. some zipcodes have high correlation with price,
   for example 98039 which corresponds to medina, that   s where bill gates
   lives and it   s one of the most expensive neighborhoods in the country.
   there   s another zipcode 98004 which is more correlated corresponding to
   bellevue. there are a lot of high-rises and tech offices there, which
   has been driving up the prices a lot lately. i used to live in that
   neighborhood, but then it got too boring and expensive so i moved :)

3.2) id75 model

   this is the first time we   re building a regression model. just like
   id28 is the simplest model we try first in a
   classification problem, id75 is the one we start with in a
   regression problem.

   remember that the equation for id28 is y=f(xw) where f
   is the sigmoid function. id75 is simply y=xw, that   s it no
   activation function. i   ve again omitted the bias terms for simplicity.
   with the biases they become y=f(xw+b) and y=xw+b respectively.

   as a reminder here is how we built the id28 (lr) model

   iframe: [39]/media/84b45efb60bed5ccb2b332640e6717b4?postid=1bb4b142a585

   and here   s the code for the id75 model (linr)

   iframe: [40]/media/15b9720d07cf8efc13e12492a9521e09?postid=1bb4b142a585

   there are 3 main differences:
     * lr uses a sigmoid activation function, where linr has no
       activation.
     * lr uses binary_crossid178 id168, where linr uses
       mean_squared_error.
     * lr also reports the accuracy, but accuracy is not an applicable
       metric to a regression problem, since the output is a floating
       point number instead of a class membership.

   the most important change is the id168 mean_squared_error
   (mse). mse is a standard id168 used for regression. the formula
   is pretty simple:
   [1*5b-_uik9ulfzx5sv-abmia@2x.png]

   where y is the true value,    is the predicted and n is the number of
   examples.

   we   re also passing a new validation_split argument to the fit function.
   it specifies the fraction of training data to use as held-out
   validation set during training, in our case it   s 20%. using a
   validation set we can see whether we   re overfitting during training.
   don   t confuse the validation set with the test set though. test set is
   entirely separate and it doesn   t get exposed to the model at all during
   training.

   the loss decreases in the first couple of epochs and then stabilizes.
   we   re likely underfitting meaning our model doesn   t have enough
   capacity, and we need a more compex model.
   [1* t2pcpjsijetfxj2jflw@2x.png]

   and these are the top 10 features with the highest weights. categorical
   zipcode features are highly dominant.
   [1*7ysmgg1ccz1ittcnmabpig@2x.png]

3.3) ann model

   and finally let   s build an ann for regression. in the previous
   examples, going from a linear model to a deep model just involved
   adding new layers with non-linear id180. it will be the
   same this time as well.

   iframe: [41]/media/613840e0e86221d2c8bf9c397cc011a7?postid=1bb4b142a585

   we added new layers with relu activation. the loss plot looks
   interesting now. the training error loss still seems to be decreasing,
   but the validation error starts increasing after the 5th epoch. we   re
   clearly overfitting. the ann is memorizing the training data, and this
   is reducing its ability to generalized on the validation set.
   [1*bgtqvqs1m4ofyfurupiavg@2x.png]

   the solution? there are a couple of methods for tackling overfitting in
   deep neural nets. most of the methods rely on constraining the capacity
   of the model. this can be achieved for example by restricting weights,
   sharing weights or even stopping training before the training loss
   plateaus. we will use the last one in the interest of brevity and
   discuss the rest in another article. this means we will simply halt
   training as soon as the validation loss stops improving. this is called
   early stopping, and it   s pretty easy to implement with keras. we just
   need to modify the call to the fit function as follows. if there   s no
   improvement for validation loss for 2 epochs, we stop training.

   iframe: [42]/media/f966eab5531af2231af46f98fc8aa944?postid=1bb4b142a585

   the validation loss stopped improving at the 5th epoch, the model
   waited for 2 more epochs and finished training at epoch 7.
   [1*zovheahuk6prv9czewmmfa@2x.png]

   the training loss with the linr model was 0.158, the loss for ann is
   0.086. we got 83% improvement over the linear model, pretty good. and
   comparing the loss on the test set, linr model got 0.191 compared to
   0.127 of ann which is 32% improvement. the figure below compares the
   training loss of linr vs ann.
   [1*s2bgol-ms49tcn1usjpx2w@2x.png]

   let   s make a final comparison, dollar value difference between the
   model predicted price and actual price. the most naive model which
   always predicts the average price of the training set ($540k), is off
   by $229k on the test set, pretty bad. id75 model is off by
   $87k vs $68k for the deep ann . the ann is 21% better than linr.
   [1*7astu0ablltjpe4bkydjlq@2x.png]

4) conclusion

   i hope this was an informative article. i tried to demonstrate a
   step-by-step application of deep learning on 3 common machine learning
   problems with real-life datasets.

   id174 and visualization was covered in detail. even though
   they   re not the fun part of solving an ml problem and mostly
   overlooked, they   re extremely important. just like part 1 we first
   tackled the problem with a simple model, and then used a deep neural
   net to get better results.

   the entire code for this article is available [43]here if you want to
   hack on it yourself. if you have any feedback feel free to reach out to
   me on [44]twitter.

     * [45]machine learning
     * [46]deep learning
     * [47]neural networks
     * [48]data science
     * [49]towards data science

   (button)
   (button)
   (button) 1.3k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [50]go to the profile of arden dertat

[51]arden dertat

   ml engineer @ pinterest. photography and travel enthusiast.

     (button) follow
   [52]towards data science

[53]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.3k
     * (button)
     *
     *

   [54]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [55]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1bb4b142a585
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_eieocrbfmy9k---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ardendertat?source=post_header_lockup
  17. https://towardsdatascience.com/@ardendertat
  18. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6
  19. https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?gi=28c0e15f3eb1#581e
  20. https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?gi=28c0e15f3eb1#3b7d
  21. https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?gi=28c0e15f3eb1#cf02
  22. https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585?gi=28c0e15f3eb1#af92
  23. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 2 - case studies.ipynb
  24. https://www.kaggle.com/ludobenistant/hr-analytics
  25. https://towardsdatascience.com/media/a2329a3202ccab890150f4d8678b68c8?postid=1bb4b142a585
  26. https://towardsdatascience.com/media/5f2eac36fb7fd31e2ccc3b24007aa0b2?postid=1bb4b142a585
  27. https://towardsdatascience.com/media/7e780c909d64b099d0b1527be11134e2?postid=1bb4b142a585
  28. https://towardsdatascience.com/media/4a07f2974054bc3a8ae7fd13021ab4d0?postid=1bb4b142a585
  29. https://towardsdatascience.com/media/199d7a192a809f7d4a23a750b7ecc656?postid=1bb4b142a585
  30. https://towardsdatascience.com/media/84b45efb60bed5ccb2b332640e6717b4?postid=1bb4b142a585
  31. https://towardsdatascience.com/media/e8e238c1367127525d6a6186e835d0a1?postid=1bb4b142a585
  32. https://towardsdatascience.com/media/dfacc3a686ff1c1b36a8a5bd4cd74746?postid=1bb4b142a585
  33. https://lvdmaaten.github.io/tsne/
  34. https://archive.ics.uci.edu/ml/datasets/iris
  35. https://towardsdatascience.com/media/2caeab0271f04c7ba87358e1b078c475?postid=1bb4b142a585
  36. https://towardsdatascience.com/media/225a47b2d5f5b3cd6d55ae95289469aa?postid=1bb4b142a585
  37. https://towardsdatascience.com/media/53523a76f3dcf9ff11c7a599411f1f4b?postid=1bb4b142a585
  38. https://www.kaggle.com/harlfoxem/housesalesprediction
  39. https://towardsdatascience.com/media/84b45efb60bed5ccb2b332640e6717b4?postid=1bb4b142a585
  40. https://towardsdatascience.com/media/15b9720d07cf8efc13e12492a9521e09?postid=1bb4b142a585
  41. https://towardsdatascience.com/media/613840e0e86221d2c8bf9c397cc011a7?postid=1bb4b142a585
  42. https://towardsdatascience.com/media/f966eab5531af2231af46f98fc8aa944?postid=1bb4b142a585
  43. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 2 - case studies.ipynb
  44. https://twitter.com/ardendertat
  45. https://towardsdatascience.com/tagged/machine-learning?source=post
  46. https://towardsdatascience.com/tagged/deep-learning?source=post
  47. https://towardsdatascience.com/tagged/neural-networks?source=post
  48. https://towardsdatascience.com/tagged/data-science?source=post
  49. https://towardsdatascience.com/tagged/towards-data-science?source=post
  50. https://towardsdatascience.com/@ardendertat?source=footer_card
  51. https://towardsdatascience.com/@ardendertat
  52. https://towardsdatascience.com/?source=footer_card
  53. https://towardsdatascience.com/?source=footer_card
  54. https://towardsdatascience.com/
  55. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  57. https://medium.com/p/1bb4b142a585/share/twitter
  58. https://medium.com/p/1bb4b142a585/share/facebook
  59. https://medium.com/p/1bb4b142a585/share/twitter
  60. https://medium.com/p/1bb4b142a585/share/facebook
