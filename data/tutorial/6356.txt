   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

applied deep learning - part 3: autoencoders

   [16]go to the profile of arden dertat
   [17]arden dertat (button) blockedunblock (button) followfollowing
   oct 3, 2017

overview

   welcome to part 3 of applied deep learning series. [18]part 1 was a
   hands-on introduction to id158s, covering both the
   theory and application with a lot of code examples and visualization.
   in [19]part 2 we applied deep learning to real-world datasets, covering
   the 3 most commonly encountered problems as case studies: binary
   classification, multiclass classification and regression.

   now we will start diving into specific deep learning architectures,
   starting with the simplest: autoencoders.
    1. [20]introduction
    2. [21]architecture
    3. [22]implementation
    4. [23]denoising autoencoders
    5. [24]sparse autoencoders
    6. [25]use cases
    7. [26]conclusion

   the code for this article is available [27]here as a jupyter notebook,
   feel free to download and try it out yourself.

1. introduction

   autoencoders are a specific type of feedforward neural networks where
   the input is the same as the output. they compress the input into a
   lower-dimensional code and then reconstruct the output from this
   representation. the code is a compact    summary    or    compression    of the
   input, also called the latent-space representation.

   an autoencoder consists of 3 components: encoder, code and decoder. the
   encoder compresses the input and produces the code, the decoder then
   reconstructs the input only using this code.
   [1*mmrdq4g3qvqnc7ijskm9pg@2x.png]

   to build an autoencoder we need 3 things: an encoding method, decoding
   method, and a id168 to compare the output with the target. we
   will explore these in the next section.

   autoencoders are mainly a id84 (or compression)
   algorithm with a couple of important properties:
     * data-specific: autoencoders are only able to meaningfully compress
       data similar to what they have been trained on. since they learn
       features specific for the given training data, they are different
       than a standard data compression algorithm like gzip. so we can   t
       expect an autoencoder trained on handwritten digits to compress
       landscape photos.
     * lossy: the output of the autoencoder will not be exactly the same
       as the input, it will be a close but degraded representation. if
       you want lossless compression they are not the way to go.
     * unsupervised: to train an autoencoder we don   t need to do anything
       fancy, just throw the raw input data at it. autoencoders are
       considered an unsupervised learning technique since they don   t need
       explicit labels to train on. but to be more precise they are
       self-supervised because they generate their own labels from the
       training data.

2. architecture

   let   s explore the details of the encoder, code and decoder. both the
   encoder and decoder are fully-connected feedforward neural networks,
   essentially the anns we covered in [28]part 1. code is a single layer
   of an ann with the dimensionality of our choice. the number of nodes in
   the code layer (code size) is a hyperparameter that we set before
   training the autoencoder.
   [1*44edeuzbesmg_tcakri3kw@2x.png]

   this is a more detailed visualization of an autoencoder. first the
   input passes through the encoder, which is a fully-connected ann, to
   produce the code. the decoder, which has the similar ann structure,
   then produces the output only using the code. the goal is to get an
   output identical with the input. note that the decoder architecture is
   the mirror image of the encoder. this is not a requirement but it   s
   typically the case. the only requirement is the dimensionality of the
   input and output needs to be the same. anything in the middle can be
   played with.

   there are 4 hyperparameters that we need to set before training an
   autoencoder:
     * code size: number of nodes in the middle layer. smaller size
       results in more compression.
     * number of layers: the autoencoder can be as deep as we like. in the
       figure above we have 2 layers in both the encoder and decoder,
       without considering the input and output.
     * number of nodes per layer: the autoencoder architecture we   re
       working on is called a stacked autoencoder since the layers are
       stacked one after another. usually stacked autoencoders look like a
          sandwitch   . the number of nodes per layer decreases with each
       subsequent layer of the encoder, and increases back in the decoder.
       also the decoder is symmetric to the encoder in terms of layer
       structure. as noted above this is not necessary and we have total
       control over these parameters.
     * id168: we either use mean squared error (mse) or binary
       crossid178. if the input values are in the range [0, 1] then we
       typically use crossid178, otherwise we use the mean squared
       error. for more details check out this [29]video.

   autoencoders are trained the same way as anns via id26.
   check out the [30]introduction of part 1 for more details on how neural
   networks are trained, it directly applies to the autoencoders.

3. implementation

   now let   s implement an autoencoder for the following architecture, 1
   hidden layer in the encoder and decoder.
   [1*zevdcg1lp7xvrtsht0b5-q@2x.png]

   we will use the extremely popular mnist [31]dataset as input. it
   contains black-and-white images of handwritten digits.
   [1*bffsomazm3phvmirbayoiq@2x.png]

   they   re of size 28x28 and we use them as a vector of 784 numbers
   between [0, 1]. check the [32]jupyter notebook for the details.

   we will now implement the autoencoder with keras. the hyperparameters
   are: 128 nodes in the hidden layer, code size is 32, and binary
   crossid178 is the id168.

   iframe: [33]/media/d69772240e50a0f5200e2005031be66a?postid=1c083af4d798

   this is very similar to the anns we worked on, but now we   re using the
   keras functional api. refer to [34]this guide for details, but here   s a
   quick comparison. before we used to add layers using the sequential api
   as follows:

   model.add(dense(16, activation='relu'))
   model.add(dense(8, activation='relu'))

   with the functional api we do this:

   layer_1 = dense(16, activation='relu')(input)
   layer_2 = dense(8, activation='relu')(layer_1)

   it   s more verbose but a more flexible way to define complex models. we
   can easily grab parts of our model, for example only the decoder, and
   work with that. the output of dense method is a callable layer, using
   the functional api we provide it with the input and store the output.
   the output of a layer becomes the input of the next layer. with the
   sequential api the add method implicitly handled this for us.

   note that all the layers use the relu activation function, as it   s the
   standard with deep neural networks. the last layer uses the sigmoid
   activation because we need the outputs to be between [0, 1]. the input
   is also in the same range.

   also note the call to fit function, before with anns we used to do:

   model.fit(x_train, y_train)

   but now we do:

   model.fit(x_train, x_train)

   remember that the targets of the autoencoder are the same as the input.
   that   s why we supply the training data as the target.

visualization

   now let   s visualize how well our autoencoder reconstructs its input.

   iframe: [35]/media/6efdae8141ef10b33516736ded4fa9b1?postid=1c083af4d798

   we run the autoencoder on the test set simply by using the predict
   function of keras. for every image in the test set, we get the output
   of the autoencoder. we expect the output to be very similar to the
   input.
   [1*v1wsyntxwwxbnujz53hxuw@2x.png]

   they are indeed pretty similar, but not exactly the same. we can notice
   it more clearly in the last digit    4   . since this was a simple task our
   autoencoder performed pretty well.

advice

   we have total control over the architecture of the autoencoder. we can
   make it very powerful by increasing the number of layers, nodes per
   layer and most importantly the code size. increasing these
   hyperparameters will let the autoencoder to learn more complex codings.
   but we should be careful to not make it too powerful. otherwise the
   autoencoder will simply learn to copy its inputs to the output, without
   learning any meaningful representation. it will just mimic the identity
   function. the autoencoder will reconstruct the training data perfectly,
   but it will be overfitting without being able to generalize to new
   instances, which is not what we want.

   this is why we prefer a    sandwitch    architecture, and deliberately keep
   the code size small. since the coding layer has a lower dimensionality
   than the input data, the autoencoder is said to be undercomplete. it
   won   t be able to directly copy its inputs to the output, and will be
   forced to learn intelligent features. if the input data has a pattern,
   for example the digit    1    usually contains a somewhat straight line and
   the digit    0    is circular, it will learn this fact and encode it in a
   more compact form. if the input data was completely random without any
   internal correlation or dependency, then an undercomplete autoencoder
   won   t be able to recover it perfectly. but luckily in the real-world
   there is a lot of dependency.

4. denoising autoencoders

   keeping the code layer small forced our autoencoder to learn an
   intelligent representation of the data. there is another way to force
   the autoencoder to learn useful features, which is adding random noise
   to its inputs and making it recover the original noise-free data. this
   way the autoencoder can   t simply copy the input to its output because
   the input also contains random noise. we are asking it to subtract the
   noise and produce the underlying meaningful data. this is called a
   denoising autoencoder.
   [1*kpezovx1bvu6v72b0voz9w@2x.png]

   the top row contains the original images. we add random gaussian noise
   to them and the noisy data becomes the input to the autoencoder. the
   autoencoder doesn   t see the original image at all. but then we expect
   the autoencoder to regenerate the noise-free original image.
   [1*sxwrp9i23om0up4seze1qq@2x.png]

   there is only one small difference between the implementation of
   denoising autoencoder and the regular one. the architecture doesn   t
   change at all, only the fit function. we trained the regular
   autoencoder as follows:

   autoencoder.fit(x_train, x_train)

   denoising autoencoder is trained as:

   autoencoder.fit(x_train_noisy, x_train)

   simple as that, everything else is exactly the same. the input to the
   autoencoder is the noisy image, and the expected target is the original
   noise-free one.

visualization

   now let   s visualize whether we are able to recover the noise-free
   images.
   [1*hfzos8xmcgjrgptw78pflg@2x.png]

   looks pretty good. the bottom row is the autoencoder output. we can do
   better by using more complex autoencoder architecture, such as
   convolutional autoencoders. we will cover convolutions in the upcoming
   article.

5. sparse autoencoders

   we introduced two ways to force the autoencoder to learn useful
   features: keeping the code size small and denoising autoencoders. the
   third method is using id173. we can regularize the autoencoder
   by using a sparsity constraint such that only a fraction of the nodes
   would have nonzero values, called active nodes.

   in particular, we add a penalty term to the id168 such that
   only a fraction of the nodes become active. this forces the autoencoder
   to represent each input as a combination of small number of nodes, and
   demands it to discover interesting structure in the data. this method
   works even if the code size is large, since only a small subset of the
   nodes will be active at any time.

   it   s pretty easy to do this in keras with just one parameter. as a
   reminder, previously we created the code layer as follows:
code = dense(code_size, activation='relu')(input_img)

   we now add another parameter called activity_regularizer by specifying
   the id173 strength. this is typically a value in the range
   [0.001, 0.000001]. here we chose 10e-6.
code = dense(code_size, activation='relu', activity_regularizer=l1(10e-6))(input
_img)

   the final loss of the sparse model is 0.01 higher than the standard
   one, due to the added id173 term.

   let   s demonstrate the encodings generated by the regularized model are
   indeed sparse. if we look at the histogram of code values for the
   images in the test set, the distribution is as follows:
   [1*h0bohvh6n2isem37vy3hxg@2x.png]

   the mean for the standard model is 6.6 but for the regularized model
   it   s 0.8, a pretty big reduction. we can see that a large chunk of code
   values in the regularized model are indeed 0, which is what we wanted.
   the variance of the regularized model is also fairly low.

6. use cases

   now we might ask the following questions. how good are autoencoders at
   compressing the input? and are they a commonly used deep learning
   technique?

   unfortunately autoencoders are not widely used in real-world
   applications. as a compression method, they don   t perform better than
   its alternatives, for example jpeg does photo compression better than
   an autoencoder. and the fact that autoencoders are data-specific makes
   them impractical as a general technique. they have 3 common use cases
   though:
     * data denoising: we have seen an example of this on images.
     * id84: visualizing high-dimensional data is
       challenging. id167 is the most commonly used method but struggles
       with large number of dimensions (typically above 32). so
       autoencoders are used as a preprocessing step to reduce the
       dimensionality, and this compressed representation is used by id167
       to visualize the data in 2d space. for great articles on id167
       refer [36]here and [37]here.
     * id5 (vae): this is a more modern and complex
       use-case of autoencoders and we will cover them in another article.
       but as a quick summary, vae learns the parameters of the
       id203 distribution modeling the input data, instead of
       learning an arbitrary function in the case of vanilla autoencoders.
       by sampling points from this distribution we can also use the vae
       as a generative model. [38]here is a good reference.

7. conclusion

   autoencoders are a very useful id84 technique. they
   are very popular as a teaching material in introductory deep learning
   courses, most likely due to their simplicity. in this article we
   covered them in detail and i hope you enjoyed it.

   the entire code for this article is available [39]here if you want to
   hack on it yourself. if you have any feedback feel free to reach out to
   me on [40]twitter.

     * [41]machine learning
     * [42]deep learning
     * [43]neural networks
     * [44]data science
     * [45]towards data science

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of arden dertat

[47]arden dertat

   ml engineer @ pinterest. photography and travel enthusiast.

     (button) follow
   [48]towards data science

[49]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2k
     * (button)
     *
     *

   [50]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [51]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1c083af4d798
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ldjmzygtkhud---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ardendertat?source=post_header_lockup
  17. https://towardsdatascience.com/@ardendertat
  18. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6
  19. https://medium.com/towards-data-science/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585
  20. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#f686
  21. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#3f72
  22. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#4214
  23. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#28b1
  24. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#a265
  25. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#ad4f
  26. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798?gi=80dfd55e9077#8e50
  27. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 3 - autoencoders.ipynb
  28. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6
  29. https://www.youtube.com/watch?v=xtu79zs4xky
  30. https://medium.com/towards-data-science/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6#04e7
  31. https://www.tensorflow.org/get_started/mnist/beginners
  32. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 3 - autoencoders.ipynb
  33. https://towardsdatascience.com/media/d69772240e50a0f5200e2005031be66a?postid=1c083af4d798
  34. https://keras.io/getting-started/functional-api-guide/
  35. https://towardsdatascience.com/media/6efdae8141ef10b33516736ded4fa9b1?postid=1c083af4d798
  36. https://distill.pub/2016/misread-tsne/
  37. http://colah.github.io/posts/2014-10-visualizing-mnist/
  38. http://kvfrans.com/variational-autoencoders-explained/
  39. https://github.com/ardendertat/applied-deep-learning-with-keras/blob/master/notebooks/part 3 - autoencoders.ipynb
  40. https://twitter.com/ardendertat
  41. https://towardsdatascience.com/tagged/machine-learning?source=post
  42. https://towardsdatascience.com/tagged/deep-learning?source=post
  43. https://towardsdatascience.com/tagged/neural-networks?source=post
  44. https://towardsdatascience.com/tagged/data-science?source=post
  45. https://towardsdatascience.com/tagged/towards-data-science?source=post
  46. https://towardsdatascience.com/@ardendertat?source=footer_card
  47. https://towardsdatascience.com/@ardendertat
  48. https://towardsdatascience.com/?source=footer_card
  49. https://towardsdatascience.com/?source=footer_card
  50. https://towardsdatascience.com/
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. https://medium.com/p/1c083af4d798/share/twitter
  54. https://medium.com/p/1c083af4d798/share/facebook
  55. https://medium.com/p/1c083af4d798/share/twitter
  56. https://medium.com/p/1c083af4d798/share/facebook
