   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]activewizards: machine learning company
   [7]activewizards: machine learning company
   [8]sign in[9]get started
     __________________________________________________________________

top 15 python libraries for data science in 2017

   [10]go to the profile of igor bobriakov
   [11]igor bobriakov (button) blockedunblock (button) followfollowing
   may 9, 2017

   as python has gained a lot of traction in the recent years in data
   science industry, i wanted to outline some of its most useful libraries
   for data scientists and engineers, based on recent experience.

   and, since all of the libraries are open sourced, we have added
   commits, contributors count and other metrics from github, which could
   be served as a proxy metrics for library popularity.

core libraries.

1. numpy (commits: 15980, contributors: 522)

   when starting to deal with the scientific task in python, one
   inevitably comes for help to python   s scipy stack, which is a
   collection of software specifically designed for scientific computing
   in python (do not confuse with scipy library, which is part of this
   stack, and the community around this stack). this way we want to start
   with a look at it. however, the stack is pretty vast, there is more
   than a dozen of libraries in it, and we want to put a focal point on
   the core packages (particularly the most essential ones).

   the most fundamental package, around which the scientific computation
   stack is built, is numpy (stands for numerical python). it provides an
   abundance of useful features for operations on n-arrays and matrices in
   python. the library provides vectorization of mathematical operations
   on the numpy array type, which ameliorates performance and accordingly
   speeds up the execution.

2. scipy (commits: 17213, contributors: 489)

   scipy is a library of software for engineering and science. again you
   need to understand the difference between scipy stack and scipy
   library. scipy contains modules for id202, optimization,
   integration, and statistics. the main functionality of scipy library is
   built upon numpy, and its arrays thus make substantial use of numpy. it
   provides efficient numerical routines as numerical integration,
   optimization, and many others via its specific submodules. the
   functions in all submodules of scipy are well documented         another coin
   in its pot.

3. pandas (commits: 15089, contributors: 762)

   pandas is a python package designed to do work with    labeled    and
      relational    data simple and intuitive. pandas is a perfect tool for
   data wrangling. it designed for quick and easy data manipulation,
   aggregation, and visualization.

   there are two main data structures in the library:

      series            one-dimensional
   [0*pwbw0odjjw49kxmt.png]

      data frames   , two-dimensional
   [0*dddyh8gijzang4do.png]

   for example, when you want to receive a new dataframe from these two
   types of structures, as a result you will receive such df by appending
   a single row to a dataframe by passing a series:
   [0*2ofqrfs5brqj0dur.png]

   here is just a small list of things that you can do with pandas:
     * easily delete and add columns from dataframe
     * convert data structures to dataframe objects
     * handle missing data, represents as nans
     * powerful grouping by functionality

google trends history

   [0*9pg2ijupxsfzh-j0.png]

   trends.google.com

github pull requests history

   [0*sr_u9bwfzncimkp4.png]

   datascience.com/trends

visualization.

4.matplotlib (commits: 21754, contributors: 588)

   another scipy stack core package and another python library that is
   tailored for the generation of simple and powerful visualizations with
   ease is matplotlib. it is a top-notch piece of software which is making
   python (with some help of numpy, scipy, and pandas) a cognizant
   competitor to such scientific tools as matlab or mathematica.

   however, the library is pretty low-level, meaning that you will need to
   write more code to reach the advanced levels of visualizations and you
   will generally put more effort, than if using more high-level tools,
   but the overall effort is worth a shot.

   with a bit of effort you can make just about any visualizations:
     * line plots;
     * scatter plots;
     * bar charts and histograms;
     * pie charts;
     * stem plots;
     * contour plots;
     * quiver plots;
     * spectrograms.

   there are also facilities for creating labels, grids, legends, and many
   other formatting entities with matplotlib. basically, everything is
   customizable.

   the library is supported by different platforms and makes use of
   different gui kits for the depiction of resulting visualizations.
   varying ides (like ipython) support functionality of matplotlib.

   there are also some additional libraries that can make visualization
   even easier.
   [0*-eylhnps-dfks_vb.png]

5. seaborn (commits: 1699, contributors: 71)

   seaborn is mostly focused on the visualization of statistical models;
   such visualizations include heat maps, those that summarize the data
   but still depict the overall distributions. seaborn is based on
   matplotlib and highly dependent on that.
   [0*y4wgipejw7r9mqga.png]

6. bokeh (commits: 15724, contributors: 223)

   another great visualization library is bokeh, which is aimed at
   interactive visualizations. in contrast to the previous library, this
   one is independent of matplotlib. the main focus of bokeh, as we
   already mentioned, is interactivity and it makes its presentation via
   modern browsers in the style of data-driven documents (d3.js).
   [0*e8wgkdvp0fldblar.jpg]

7. plotly (commits: 2486, contributors: 33)

   finally, a word about plotly. it is rather a web-based toolbox for
   building visualizations, exposing apis to some programming languages
   (python among them). there is a number of robust, out-of-box graphics
   on the plot.ly website. in order to use plotly, you will need to set up
   your api key. the graphics will be processed server side and will be
   posted on the internet, but there is a way to avoid it.
   [0*2rltuq4747zq_df1.jpg]

google trends history

   [0*i2ywlszncb6x1zzd.png]

   trends.google.com

github pull requests history

   [0*diunakuyqihvrpvu.png]

   datascience.com/trends

machine learning.

8. scikit-learn (commits: 21793, contributors: 842)

   scikits are additional packages of scipy stack designed for specific
   functionalities like image processing and machine learning
   facilitation. in the regard of the latter, one of the most prominent of
   these packages is scikit-learn. the package is built on the top of
   scipy and makes heavy use of its math operations.

   the scikit-learn exposes a concise and consistent interface to the
   common machine learning algorithms, making it simple to bring ml into
   production systems. the library combines quality code and good
   documentation, ease of use and high performance and is de-facto
   industry standard for machine learning with python.

deep learning         keras / tensorflow / theano

   in the regard of deep learning, one of the most prominent and
   convenient libraries for python in this field is keras, which can
   function either on top of tensorflow or theano. let   s reveal some
   details about all of them.

9.theano. (commits: 25870, contributors: 300)

   firstly, let   s talk about theano.

   theano is a python package that defines multi-dimensional arrays
   similar to numpy, along with math operations and expressions. the
   library is compiled, making it run efficiently on all architectures.
   originally developed by the machine learning group of universit   de
   montr  al, it is primarily used for the needs of machine learning.

   the important thing to note is that theano tightly integrates with
   numpy on low-level of its operations. the library also optimizes the
   use of gpu and cpu, making the performance of data-intensive
   computation even faster.

   efficiency and stability tweaks allow for much more precise results
   with even very small values, for example, computation of log(1+x) will
   give cognizant results for even smallest values of x.

10. tensorflow. (commits: 16785, contributors: 795)

   [0*vhngcioczwu9sp6w.png]

   coming from developers at google, it is an open-source library of data
   flow graphs computations, which are sharpened for machine learning. it
   was designed to meet the high-demand requirements of google environment
   for training neural networks and is a successor of distbelief, a
   machine learning system, based on neural networks. however, tensorflow
   isn   t strictly for scientific use in border   s of google         it is general
   enough to use it in a variety of real-world application.

   the key feature of tensorflow is their multi-layered nodes system that
   enables quick training of id158s on large datasets.
   this powers google   s voice recognition and object identification from
   pictures.

11. keras. (commits: 3519, contributors: 428)

   and finally, let   s look at the keras. it is an open-source library for
   building neural networks at a high-level of the interface, and it is
   written in python. it is minimalistic and straightforward with
   high-level of extensibility. it uses theano or tensorflow as its
   backends, but microsoft makes its efforts now to integrate cntk
   (microsoft   s cognitive toolkit) as a new back-end.

   the minimalistic approach in design aimed at fast and easy
   experimentation through the building of compact systems.

   keras is really eased to get started with and keep going with quick
   prototyping. it is written in pure python and high-level in its nature.
   it is highly modular and extendable. notwithstanding its ease,
   simplicity, and high-level orientation, keras is still deep and
   powerful enough for serious modeling.

   the general idea of keras is based on layers, and everything else is
   built around them. data is prepared in tensors, the first layer is
   responsible for input of tensors, the last layer is responsible for
   output, and the model is built in between.

google trends history

   [0*psw27kmxr7fajjxo.png]

   trends.google.com

github pull requests history

   [0*b_c4xgbgwsylrqjy.png]

   datascience.com/trends

natural language processing.

12. nltk (commits: 12449, contributors: 196)

   the name of this suite of libraries stands for natural language toolkit
   and, as the name implies, it used for common tasks of symbolic and
   statistical natural language processing. nltk was intended to
   facilitate teaching and research of nlp and the related fields
   (linguistics, cognitive science artificial intelligence, etc.) and it
   is being used with a focus on this today.

   the functionality of nltk allows a lot of operations such as text
   tagging, classification, and tokenizing, name entities identification,
   building corpus tree that reveals inter and intra-sentence
   dependencies, id30, semantic reasoning. all of the building blocks
   allow for building complex research systems for different tasks, for
   example, sentiment analytics, id54.

13. gensim (commits: 2878, contributors: 179)

   it is an open-source library for python that implements tools for work
   with vector space modeling and id96. the library designed to
   be efficient with large texts, not only in-memory processing is
   possible. the efficiency is achieved by the using of numpy data
   structures and scipy operations extensively. it is both efficient and
   easy to use.

   gensim is intended for use with raw and unstructured digital texts.
   gensim implements algorithms such as hierarchical dirichlet processes
   (hdp), latent semantic analysis (lsa) and id44
   (lda), as well as tf-idf, random projections, id97 and document2vec
   facilitate examination of texts for recurring patterns of words in the
   set of documents (often referred as a corpus). all of the algorithms
   are unsupervised         no need for any arguments, the only input is corpus.

google trends history

   [0*lnenuxgqdock6hmp.png]

   trends.google.com

github pull requests history

   [0*xcnkaskg_lziitvy.png]

   datascience.com/trends

data mining. statistics.

14. scrapy (commits: 6325, contributors: 243)

   scrapy is a library for making crawling programs, also known as spider
   bots, for retrieval of the structured data, such as contact info or
   urls, from the web.

   it is open-source and written in python. it was originally designed
   strictly for scraping, as its name indicate, but it has evolved in the
   full-fledged framework with the ability to gather data from apis and
   act as general-purpose crawlers.

   the library follows famous don   t repeat yourself in the interface
   design         it prompts its users to write the general, universal code that
   is going to be reusable, thus making building and scaling large
   crawlers.

   the architecture of scrapy is built around spider class, which
   encapsulates the set of instruction that is followed by the crawler.

15. statsmodels (commits: 8960, contributors: 119)

   as you have probably guessed from the name, statsmodels is a library
   for python that enables its users to conduct data exploration via the
   use of various methods of estimation of statistical models and
   performing statistical assertions and analysis.

   among many useful features are descriptive and result statistics via
   the use of id75 models, generalized linear models,
   discrete choice models, robust linear models, time series analysis
   models, various estimators.

   the library also provides extensive plotting functions that are
   designed specifically for the use in statistical analysis and tweaked
   for good performance with big data sets of statistical data.

conclusions.

   these are the libraries that are considered to be the top of the list
   by many data scientists and engineers and worth looking at them as well
   as at least familiarizing yourself with them.

   and here are the detailed stats of github activities for each of those
   libraries:
   [0*jyzhkzohxdgwcfsq.png]

   source: [12]google spreadsheet

   of course, this is not the fully exhaustive list and there are many
   other libraries and frameworks that are also worthy and deserve proper
   attention for particular tasks. a great example is different packages
   of scikit that focus on specific domains, like scikit-image for working
   with images.

   so, if you have another useful library in mind, please let our readers
   know in the comments section.

   thank you very much for your attention.

   short version of article available here:
   [13]https://activewizards.com/blog/top-15-libraries-for-data-science-in
   -python/

     * [14]machine learning
     * [15]data science
     * [16]python

   (button)
   (button)
   (button) 5.7k claps
   (button) (button) (button) 21 (button) (button)

     (button) blockedunblock (button) followfollowing
   [17]go to the profile of igor bobriakov

[18]igor bobriakov

   activewizards.com | ai and data science for startups

     (button) follow
   [19]activewizards: machine learning company

[20]activewizards: machine learning company

   helping organizations to implement ai and data science initiatives

     * (button)
       (button) 5.7k
     * (button)
     *
     *

   [21]activewizards: machine learning company
   never miss a story from activewizards: machine learning company, when
   you sign up for medium. [22]learn more
   never miss a story from activewizards: machine learning company
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ab61b4f9b4a7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/activewizards-machine-learning-company?source=avatar-lo_zqslt8sft1ok-5148f39764c8
   7. https://medium.com/activewizards-machine-learning-company?source=logo-lo_zqslt8sft1ok---5148f39764c8
   8. https://medium.com/m/signin?redirect=https://medium.com/activewizards-machine-learning-company/top-15-python-libraries-for-data-science-in-in-2017-ab61b4f9b4a7&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/activewizards-machine-learning-company/top-15-python-libraries-for-data-science-in-in-2017-ab61b4f9b4a7&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@ibobriakov?source=post_header_lockup
  11. https://medium.com/@ibobriakov
  12. https://docs.google.com/spreadsheets/d/1wlotllj65qmblumc3f2mop_4upifbyj6idzwop0cxrq
  13. https://activewizards.com/blog/top-15-libraries-for-data-science-in-python/
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/data-science?source=post
  16. https://medium.com/tag/python?source=post
  17. https://medium.com/@ibobriakov?source=footer_card
  18. https://medium.com/@ibobriakov
  19. https://medium.com/activewizards-machine-learning-company?source=footer_card
  20. https://medium.com/activewizards-machine-learning-company?source=footer_card
  21. https://medium.com/activewizards-machine-learning-company
  22. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  24. https://medium.com/p/ab61b4f9b4a7/share/twitter
  25. https://medium.com/p/ab61b4f9b4a7/share/facebook
  26. https://medium.com/p/ab61b4f9b4a7/share/twitter
  27. https://medium.com/p/ab61b4f9b4a7/share/facebook
