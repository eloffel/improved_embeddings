   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

   [1*n4r6rnrgradj8177dlneta.png]

everything you need to know about neural networks

   [14]go to the profile of mate labs
   [15]mate labs (button) blockedunblock (button) followfollowing
   nov 1, 2017

   courtesy: [16]kailash ahirwar (co-founder & cto, [17]mate labs)

intro:

   understanding what artificial intelligence is and learning how machine
   learning and deep learning power it, are overwhelming experiences. we
   are a group of self-taught engineers who have gone through that
   experience and are sharing, our understanding (through blogs) of it and
   what has helped us along the way, in simplified form, so that anyone
   who is new to this field can easily start making sense of the
   technicalities of this technology.

   moreover, during this mission of ours we have created a [18]platform
   for anyone to be able to[19] build machine learning & deep learning
   models without writing even a single line of code.
     __________________________________________________________________

   [1*v1mnubnpa7thniuchjnpua.png]

   neuron(node)         it is the basic unit of a neural network. it gets
   certain number of inputs and a bias value. when a signal(value)
   arrives, it gets multiplied by a weight value. if a neuron has 4
   inputs, it has 4 weight values which can be adjusted during training
   time.
   [1*6aaau20l9f2qfygik8jssg.png]
   [1*h2gfatdntbwfkcr4kuc5mw.png]
   operations at one neuron of a neural network
     __________________________________________________________________

   [1*5zquuvtbstjygqh_qdb2ha.png]

   connections         it connects one neuron in one layer to another neuron in
   other layer or the same layer. a connection always has a weight value
   associated with it. goal of the training is to update this weight value
   to decrease the loss(error).
   [1*0nktek20-qnalkwoa8dlna.png]

   bias(offset)         it is an extra input to neurons and it is always 1, and
   has it   s own connection weight. this makes sure that even when all the
   inputs are none (all 0   s) there   s gonna be an activation in the neuron.
     __________________________________________________________________

   activation function(transfer function)         id180 are used
   to introduce non-linearity to neural networks. it squashes the values
   in a smaller range viz. a sigmoid activation function squashes values
   between a range 0 to 1. there are many id180 used in
   deep learning industry and relu, selu and tanh are preferred over
   sigmoid activation function. [20]in this article i have explained the
   different id180 available.
   [1*p_hyqatyi8pbt2kel6sioq.png]
   id180
   source         [21]http://prog3.com/sbdm/blog/cyh_24/article/details/50593400
     __________________________________________________________________

   [1*yl2a2dbdq5754h_ktdj8mq.png]
   basic neural network layout

   input layer         this is the first layer in the neural network. it takes
   input signals(values) and passes them on to the next layer. it doesn   t
   apply any operations on the input signals(values) & has no weights and
   biases values associated. in our network we have 4 input signals x1,
   x2, x3, x4.

   hidden layers         hidden layers have neurons(nodes) which apply different
   transformations to the input data. one hidden layer is a collection of
   neurons stacked vertically(representation). in our image given below we
   have 5 hidden layers. in our network, first hidden layer has 4
   neurons(nodes), 2nd has 5 neurons, 3rd has 6 neurons, 4th has 4 and 5th
   has 3 neurons. last hidden layer passes on values to the output layer.
   all the neurons in a hidden layer are connected to each and every
   neuron in the next layer, hence we have a fully connected hidden
   layers.

   output layer         this layer is the last layer in the network & receives
   input from the last hidden layer. with this layer we can get desired
   number of values and in a desired range. in this network we have 3
   neurons in the output layer and it outputs y1, y2, y3.
     __________________________________________________________________

   input shape         it is the shape of the input matrix we pass to the input
   layer. our network   s input layer has 4 neurons and it expects 4 values
   of 1 sample. desired input shape for our network is (1, 4, 1) if we
   feed it one sample at a time. if we feed 100 samples input shape will
   be (100, 4, 1). different libraries expect shapes in different formats.

   weights(parameters)         a weight represent the strength of the connection
   between units. if the weight from node 1 to node 2 has greater
   magnitude, it means that neuron 1 has greater influence over neuron 2.
   a weight brings down the importance of the input value. weights near
   zero means changing this input will not change the output. negative
   weights mean increasing this input will decrease the output. a weight
   decides how much influence the input will have on the output.
     __________________________________________________________________

   [0*s-fqtnzuxibfpobg.png]
   forward propagation

   forward propagation         forward propagation is a process of feeding input
   values to the neural network and getting an output which we call
   predicted value. sometimes we refer forward propagation as id136.
   when we feed the input values to the neural network   s first layer, it
   goes without any operations. second layer takes values from first layer
   and applies multiplication, addition and activation operations and
   passes this value to the next layer. same process repeats for
   subsequent layers and finally we get an output value from the last
   layer.
   [0*koocmnuitbhkq5xx.png]
   backward propagation

   back-propagation         after forward propagation we get an output value
   which is the predicted value. to calculate error we compare the
   predicted value with the actual output value. we use a id168
   (mentioned below) to calculate the error value. then we calculate the
   derivative of the error value with respect to each and every weight in
   the neural network. back-propagation uses chain rule of differential
   calculus. in chain rule first we calculate the derivatives of error
   value with respect to the weight values of the last layer. we call
   these derivatives, gradients and use these gradient values to calculate
   the gradients of the second last layer. we repeat this process until we
   get gradients for each and every weight in our neural network. then we
   subtract this gradient value from the weight value to reduce the error
   value. in this way we move closer (descent) to the local minima(means
   minimum loss).
   [0*kmfz18bmx52li0ff.gif]

   learning rate         when we train neural networks we usually use gradient
   descent to optimize the weights. at each iteration we use
   back-propagation to calculate the derivative of the id168 with
   respect to each weight and subtract it from that weight. learning rate
   determines how quickly or how slowly you want to update your
   weight(parameter) values. learning rate should be high enough so that
   it won   t take ages to converge, and it should be low enough so that it
   finds the local minima.
     __________________________________________________________________

   [0*84ijcxboamf_ey5w.png]
   precision and recall

   accuracy         accuracy refers to the closeness of a measured value to a
   standard or known value.

   precision         precision refers to the closeness of two or more
   measurements to each other. it is the repeatability or reproducibility
   of the measurement.

   recall(sensitivity)         recall refers to the fraction of relevant
   instances that have been retrieved over the total amount of relevant
   instances
   [1*5zbjooy8r5dm4wf818ntea.jpeg]

   confusion matrix         as wikipedia says:

     in the field of [22]machine learning and specifically the problem of
     [23]statistical classification, a confusion matrix, also known as an
     error matrix, is a specific table layout that allows visualization
     of the performance of an algorithm, typically a [24]supervised
     learning one (in [25]unsupervised learning it is usually called a
     matching matrix). each row of the matrix represents the instances in
     a predicted class while each column represents the instances in an
     actual class (or vice versa). the name stems from the fact that it
     makes it easy to see if the system is confusing two classes (i.e.
     commonly mislabelling one as another).

   [1*y_dqqgbzl0w9vmfdqaxqta.png]
   confusion matrix
     __________________________________________________________________

   convergence         convergence is when as the iterations proceed the output
   gets closer and closer to a specific value.

   id173         it is used to overcome the over-fitting problem. in
   id173 we penalise our loss term by adding a l1 (lasso) or an
   l2(ridge) norm on the weight vector w (it is the vector of the learned
   parameters in the given algorithm).

   l(id168) +   n(w)         here    is your id173 term and n(w)
   is l1 or l2 norm

   normalisation         data id172 is the process of rescaling one or
   more attributes to the range of 0 to 1. id172 is a good
   technique to use when you do not know the distribution of your data or
   when you know the distribution is not gaussian (a bell curve). it is
   good to speed up the learning process.
     __________________________________________________________________

   fully connected layers         when activations of all nodes in one layer
   goes to each and every node in the next layer. when all the nodes in
   the lth layer connect to all the nodes in the (l+1)th layer we call
   these layers fully connected layers.
   [1*3qcxynldqrm8bubz-1sh8q.png]
   fully connected layers
     __________________________________________________________________

   id168/cost function         the id168 computes the error for
   a single training example. the cost function is the average of the loss
   functions of the entire training set.
     *    mse   : for mean squared error.
     *    binary_crossid178   : for binary logarithmic loss (logloss).
     *    categorical_crossid178   : for multi-class logarithmic loss
       (logloss).

   model optimizers         the optimizer is a search technique, which is used
   to update weights in the model.
     * sgd: stochastic id119, with support for momentum.
     * rmsprop: adaptive learning rate optimization method proposed by
       geoff hinton.
     * adam: adaptive moment estimation (adam) that also uses adaptive
       learning rates.
     __________________________________________________________________

   performance metrics         performance metrics are used to measure the
   performance of the neural network. accuracy, loss, validation accuracy,
   validation loss, mean absolute error, precision, recall and f1 score
   are some performance metrics.

   batch size         the number of training examples in one forward/backward
   pass. the higher the batch size, the more memory space you   ll need.

   training epochs         it is the number of times that the model is exposed
   to the training dataset.

   one epoch = one forward pass and one backward pass of all the training
   examples.
     __________________________________________________________________

   [1*krj3buv6d1vvgri74dw9rq.gif]

about

     at [26]mate labs we have built [27]mateverse, a machine learning
     platform, where you can build customized ml models in minutes
     without writing a single line of code. [28]our platform enables
     everyone to easily build and train machine learning models, without
     writing a single line of code.

let   s join hands.

     share your thoughts with us on [29]twitter.

     tell us if you have some new suggestion. our ears and eyes are
     always open for something really exciting.
     __________________________________________________________________

   iframe: [30]/media/7ccc375a11ef548386b2ce5b1a6488ba?postid=8988c3ee4491

     * [31]machine learning
     * [32]artificial intelligence
     * [33]deep learning
     * [34]data science
     * [35]technology

   (button)
   (button)
   (button) 4k claps
   (button) (button) (button) 11 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of mate labs

[37]mate labs

   we   re trying to enable machine learning and deep learning to one and
   all. irrespective of whether a user knows how to code or not.

     (button) follow
   [38]hacker noon

[39]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 4k
     * (button)
     *
     *

   [40]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [41]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8988c3ee4491
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_oaftlu2ipkot---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@matelabs_ai?source=post_header_lockup
  15. https://hackernoon.com/@matelabs_ai
  16. https://medium.com/@kailashahirwar
  17. http://www.matelabs.in/
  18. http://www.matelabs.in/
  19. https://medium.com/towards-data-science/how-to-train-a-machine-learning-model-in-5-minutes-c599fa20e7d5
  20. https://medium.com/towards-data-science/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046
  21. http://prog3.com/sbdm/blog/cyh_24/article/details/50593400
  22. https://en.wikipedia.org/wiki/machine_learning
  23. https://en.wikipedia.org/wiki/statistical_classification
  24. https://en.wikipedia.org/wiki/supervised_learning
  25. https://en.wikipedia.org/wiki/unsupervised_learning
  26. http://matelabs.in/
  27. https://www.mateverse.com/
  28. https://www.mateverse.com/
  29. https://twitter.com/matelabs_ai
  30. https://hackernoon.com/media/7ccc375a11ef548386b2ce5b1a6488ba?postid=8988c3ee4491
  31. https://hackernoon.com/tagged/machine-learning?source=post
  32. https://hackernoon.com/tagged/artificial-intelligence?source=post
  33. https://hackernoon.com/tagged/deep-learning?source=post
  34. https://hackernoon.com/tagged/data-science?source=post
  35. https://hackernoon.com/tagged/technology?source=post
  36. https://hackernoon.com/@matelabs_ai?source=footer_card
  37. https://hackernoon.com/@matelabs_ai
  38. https://hackernoon.com/?source=footer_card
  39. https://hackernoon.com/?source=footer_card
  40. https://hackernoon.com/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/8988c3ee4491/share/twitter
  44. https://medium.com/p/8988c3ee4491/share/facebook
  45. https://medium.com/p/8988c3ee4491/share/twitter
  46. https://medium.com/p/8988c3ee4491/share/facebook
