   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

   [1*jcpmjty2a0x1ryy-zqe2va.png]

what is a capsnet or capsule network?

   [14]go to the profile of debarko de     
   [15]debarko de      (button) blockedunblock (button) followfollowing
   oct 31, 2017

   what is a capsule network? what is a capsule? is capsnet better than a
   convolutional neural network (id98)? in this article i will talk about
   all the above questions about capsnet or capsule network released by
   hinton.

     note: this article is not about pharmaceutical capsules. it is about
     capsules in neural networks or machine learning world.

   there is an expectation from you as a reader. you need to be aware of
   id98s. if not, i would like you to go through [16]this article on
   [17]hackernoon. next i will run through a small recap of relevant
   points of id98. that way you can easily grab on to the comparison done
   below. so without further ado lets dive in.

   id98 are essentially a system where we stack a lot of neurons together.
   these networks have been proven to be exceptionally great at handling
   image classification problems. it would be hard to have a neural
   network map out all the pixels of an image since it   s computationally
   really expensive. so convolutional is a method which helps you simplify
   the computation to a great extent without losing the essence of the
   data. convolution is basically a lot of id127 and
   summation of those results.
   [1*qd_t1j_0dqgzsn8x-h89pw.jpeg]
   image 1.0: convolutional neural network

   after an image is fed to the network, a set of kernels or filters scan
   it and perform the convolution operation. this leads to creation of
   feature maps inside the network. these features next pass via
   activation layer and pooling layers in succession and then based on the
   number of layers in the network this continues. activation layers are
   required to induce a sense of [18]non linearity in the network (eg:
   [19]relu). pooling (eg: max pooling) helps in reducing the training
   time. the idea of pooling is that it creates    summaries    of each
   sub-region. it also gives you a little bit of positional and
   translational invariance in id164. at the end of the network
   it will pass via a classifier like softmax classifier which will give
   us a class. training happens based on back propagation of error matched
   against some labelled data. non linearity also helps in solving the
   vanishing gradient in this step.

what is the problem with id98s?

   id98s perform exceptionally great when they are classifying images which
   are very close to the data set. if the images have rotation, tilt or
   any other different orientation then id98s have poor performance. this
   problem was solved by adding different variations of the same image
   during training. in id98 each layer understands an image at a much more
   granular level. lets understand this with an example. if you are trying
   to classify ships and horses. the innermost layer or the 1st layer
   understands the small curves and edges. the 2nd layer might understand
   the straight lines or the smaller shapes, like the mast of a ship or
   the curvature of the entire tail. higher up layers start understanding
   more complex shapes like the entire tail or the ship hull. final layers
   try to see a more holistic picture like the entire ship or the entire
   horse. we use pooling after each layer to make it compute in reasonable
   time frames. but in essence it also loses out the positional data.
   [1*0rgb8eql5j27ujkt--yb_q.png]
   image 2.0: disfiguration transformation

   pooling helps in creating the positional invariance. otherwise id98s
   would fit only for images or data which are very close to the training
   set. this invariance also leads to triggering false positive for images
   which have the components of a ship but not in the correct order. so
   the system can trigger the right to match with the left in the above
   image. you as an observer clearly see the difference. the pooling layer
   also adds this sort of invariance.
   [1*k7cuf8v3bd k7e4shghw.png]
   image 2.1 proportional transformation

   this was never the intention of pooling layer. what the pooling was
   supposed to do is to introduce positional, orientational, proportional
   invariances. but the method we use to get this uses is very crude. in
   reality it adds all sorts of positional invariance. thus leading to the
   dilemma of detecting right ship in image 2.0 as a correct ship. what we
   needed was not invariance but equivariance. invariance makes a id98
   tolerant to small changes in the viewpoint. equivariance makes a id98
   understand the rotation or proportion change and adapt itself
   accordingly so that the spatial positioning inside an image is not
   lost. a ship will still be a smaller ship but the id98 will reduce its
   size to detect that. this leads us to the recent advancement of capsule
   networks.
     __________________________________________________________________

what is a capsule network?

   every few days there is an advancement in the field of neural networks.
   some brilliant minds are working on this field. you can pretty much
   assume every paper on this topic is almost ground breaking or path
   changing. sara sabour, nicholas frost and geoffrey hinton released a
   paper titled    [20]dynamic routing between capsules    4 days back. now
   when one of the godfathers of deep learning    [21]geoffrey hinton    is
   releasing a paper it is bound to be ground breaking. the entire deep
   learning community is going crazy on this paper as you read this
   article. so this paper talks about capsules, capsnet and a run on
   mnist. mnist is a database of tagged handwritten digit images. results
   are showing a significant increase in performance in case of overlapped
   digits. the paper compares to the current state-of-the-art id98s. in
   this paper the authors project that human brain have modules called
      capsules   . these capsules are particularly good at handling different
   types of visual stimulus and encoding things like pose (position, size,
   orientation), deformation, velocity, albedo, hue, texture etc. the
   brain must have a mechanism for    routing    low level visual information
   to what it believes is the best capsule for handling it.
   [1*p1y-baf1wv9-etdqcserha.png]
   image 3.0: capsnet architecture

   capsule is a nested set of neural layers. so in a regular neural
   network you keep on adding more layers. in capsnet you would add more
   layers inside a single layer. or in other words nest a neural layer
   inside another. the state of the neurons inside a capsule capture the
   above properties of one entity inside an image. a capsule outputs a
   vector to represent the existence of the entity. the orientation of the
   vector represents the properties of the entity. the vector is sent to
   all possible parents in the neural network. for each possible parent a
   capsule can find a prediction vector. prediction vector is calculated
   based on multiplying it   s own weight and a weight matrix. whichever
   parent has the largest scalar prediction vector product, increases the
   capsule bond. rest of the parents decrease their bond. this routing by
   agreement method is superior than the current mechanism like
   max-pooling. max pooling routes based on the strongest feature detected
   in the lower layer. apart from dynamic routing, capsnet talks about
   adding squashing to a capsule. squashing is a non-linearity. so instead
   of adding squashing to each layer like how you do in id98, you add the
   squashing to a nested set of layers. so the squashing function gets
   applied to the vector output of each capsule.
   [1*ccxgejhlsxyui4paujqxpw.png]
   image 3.1: novel squashing function

   the paper introduces a new squashing function. you can see it in image
   3.1. relu or similar non linearity functions work well with single
   neurons. but the paper found that this squashing function works best
   with capsules. this tries to squash the length of output vector of a
   capsule. it squashes to 0 if it is a small vector and tries to limit
   the output vector to 1 if the vector is long. the dynamic routing adds
   some extra computation cost. but it definitely gives added advantage.

   now we need to realise that this paper is almost brand new and the
   concept of capsules is not throughly tested. it works on mnist data but
   it still needs to be proven against much larger dataset across a
   variety of classes. there are already (within 4 days) updates on this
   paper who raise the following concerns:

     1. it uses the length of the pose vector to represent the
     id203 that the entity represented by a capsule is present. to
     keep the length less than 1 requires an unprincipled non-linearity
     that prevents there from being any sensible objective function that
     is minimized by the iterative routing procedure.

     2. it uses the cosine of the angle between two pose vectors to
     measure their agreement for routing. unlike the log variance of a
     gaussian cluster, the cosine is not good at distinguishing between
     quite good agreement and very good agreement.

     3. it uses a vector of length n rather than a matrix with n elements
     to represent a pose, so its transformation matrices have n 2
     parameters rather than just n.

   the current implementation of capsules has scope for improvement. but
   we should also keep in mind that the hinton paper in the first place
   only says:

     the aim of this paper is not to explore this whole space but to
     simply show that one fairly straightforward implementation works
     well and that dynamic routing helps.
     __________________________________________________________________

   so that   s a lot of theory. lets have some fun and build a capsnet. i
   will take you through some code to setup a basic capsnet for mnist
   data. i will comment inside the code so you can follow through line by
   line and get an understanding of how it works. i will take you through
   two important pieces in the code. rest you can go to the repo, fork it
   and start working on it:

   iframe: [22]/media/a6a6fd093feada41922956aa8673ebc2?postid=2bfbe48769cc

   the above is the entire capsule layer. this is now stacked to created a
   capsule network. code for capsnet is below:

   iframe: [23]/media/f16430ad59165a58c4e2cb62cb4c2b55?postid=2bfbe48769cc

   the entire code along with the training and evaluation module is
   present [24]here. it   s under [25]apache 2.0 license. you can use it
   freely. i want to give credits for the code to [26]naturomics.

summary

   so we went through what is a capsnet and how they are built. we tried
   to understand that capsules are nothing but nested neural layers on a
   high level. we also looked at how a capsnet delivers rotational and
   other invariances. it does that being equivariant to the spatial setup
   of the each entity inside an image. i am sure there are still questions
   to be answered. capsules and their best implementation is probably the
   biggest question. but this post is an initial push in trying to throw
   some light on the topic. if you have any queries please do share them.
   i will answer them to the best of my knowledge.

     [27]siraj raval and his talks greatly influence this article. share
     this article on [28]twitter. do follow me on [29]twitter for future
     updates. if you liked this article, please hit the      button to
     support it. this will help other medium users find it. [30]share
     this article on twitter so that others can read it.

     * [31]machine learning
     * [32]capsnet
     * [33]deep learning
     * [34]capsule
     * [35]artificial intelligence

   (button)
   (button)
   (button) 6.4k claps
   (button) (button) (button) 24 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of debarko de     

[37]debarko de     

   computer science || engg manager [38]@practo, ex [39]@hashcube || #ai,
   #machinelearning, #neuralnetworks, #javascript || certified open water
   diver     , camera love     

     (button) follow
   [40]hacker noon

[41]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 6.4k
     * (button)
     *
     *

   [42]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [43]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2bfbe48769cc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_77nmeffndsry---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@debarko?source=post_header_lockup
  15. https://hackernoon.com/@debarko
  16. https://hackernoon.com/supervised-deep-learning-in-image-classification-for-noobs-part-1-9f831b6d430d
  17. https://hackernoon.com/
  18. https://stackoverflow.com/a/9783865/2235170
  19. https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-and-softmax-activation-functions
  20. https://arxiv.org/abs/1710.09829
  21. https://en.wikipedia.org/wiki/geoffrey_hinton
  22. https://hackernoon.com/media/a6a6fd093feada41922956aa8673ebc2?postid=2bfbe48769cc
  23. https://hackernoon.com/media/f16430ad59165a58c4e2cb62cb4c2b55?postid=2bfbe48769cc
  24. https://github.com/debarko/capsnet-tensorflow
  25. https://github.com/debarko/capsnet-tensorflow/blob/master/license
  26. https://github.com/naturomics
  27. https://twitter.com/sirajraval
  28. http://bit.ly/2z1z6ru
  29. http://twitter.com/debarko
  30. http://bit.ly/2z1z6ru
  31. https://hackernoon.com/tagged/machine-learning?source=post
  32. https://hackernoon.com/tagged/capsnet?source=post
  33. https://hackernoon.com/tagged/deep-learning?source=post
  34. https://hackernoon.com/tagged/capsule?source=post
  35. https://hackernoon.com/tagged/artificial-intelligence?source=post
  36. https://hackernoon.com/@debarko?source=footer_card
  37. https://hackernoon.com/@debarko
  38. http://twitter.com/practo
  39. http://twitter.com/hashcube
  40. https://hackernoon.com/?source=footer_card
  41. https://hackernoon.com/?source=footer_card
  42. https://hackernoon.com/
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://medium.com/p/2bfbe48769cc/share/twitter
  46. https://medium.com/p/2bfbe48769cc/share/facebook
  47. https://medium.com/p/2bfbe48769cc/share/twitter
  48. https://medium.com/p/2bfbe48769cc/share/facebook
