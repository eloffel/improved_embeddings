   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

probabilistic id114 tutorial         part 1

basic terminology and the problem setting

   [16]go to the profile of prasoon goyal
   [17]prasoon goyal (button) blockedunblock (button) followfollowing
   nov 2, 2017

   a lot of common problems in machine learning involve classification of
   isolated data points that are independent of each other. for instance,
   given an image, predict whether it contains a cat or a dog, or given an
   image of a handwritten character, predict which digit out of 0 through
   9 it is.

   it turns out that a lot of problems do not fit into the above
   framework. for example, given a sentence,    i like machine learning,   
   tag each word with its part-of-speech (a noun, a pronoun, a verb, an
   adjective, etc.). as even this simple example demonstrates, this task
   cannot be solved by treating each word independently            learning    could
   be a noun or a verb depending on its context. this task is important
   for a lot of more complex tasks on text, such as translating from one
   language to another, text to speech, etc.

   it is not obvious how you would use a standard classification model to
   handle these problems. a powerful framework which can be used to learn
   such models with dependency is probabilistic id114 (pgm).
   for this post, the [18]statsbot team asked a data scientist, prasoon
   goyal, to make a tutorial on this framework to us.
   [1*eeq4i5dq2ft-f3thei31oa.jpeg]

   before talking about how to apply a probabilistic graphical model to a
   machine learning problem, we need to understand the pgm framework.
   formally, a probabilistic graphical model (or graphical model for
   short) consists of a graph structure. each node of the graph is
   associated with a random variable, and the edges in the graph are used
   to encode relations between the random variables.

   depending on whether the graph is directed or undirected, we can
   classify graphical modes into two categories         id110s and
   markov networks.

id110s: directed id114

   a canonical example of id110s is the so-called    student
   network,    which looks like this:
   [0*ws_1f7zfdbqph4y8.]

   this graph describes a setting for a student enrolled in a university
   class. there are 5 random variables in the graph:

     difficulty (of the class): takes values 0 (low difficulty) and 1
     (high difficulty)

     intelligence (of the student): takes values 0 (not intelligent) and
     1 (intelligent)

     grade (the student gets in the class): takes values 1 (good grade),
     2 (average grade), and 3 (bad grade)

     sat (student   s score in the sat exam): takes values 0 (low score)
     and 1 (high score)

     letter (quality of recommendation letter the student gets from the
     professor after completing the course): takes values 0 (not a good
     letter) and 1 (a good letter)

   the edges in the graph encode dependencies in the graph.
     * the    grade    of the student depends on the    difficulty    of the class
       and the    intelligence    of the student.
     * the    grade,    in turn, determines whether the student gets a good
       letter of recommendation from the professor.
     * also, the    intelligence    of the student influences their    sat   
       score, in addition to influencing the    grade.   

   note that the direction of arrows tells us about cause-effect
   relationships            intelligence    affects the    sat    score, but the    sat   
   score does not influence the    intelligence.   

   finally, let   s look at the tables associated with each of the nodes.
   formally, these are called id155 distributions
   (cpds).

  onditional id203 distributions

   the cpds for    difficulty    and    intelligence    are fairly simple, because
   these variables do not depend on any of the other variables. the tables
   basically encode the probabilities of these variables, taking 0 or 1 as
   values. as you might have noticed, the values in each of the tables
   must sum to 1.

   next, let   s look at the cpd for    sat.    each row corresponds to the
   values that its parent (   intelligence   ) can take, and each column
   corresponds to the values that    sat    can take. each cell has the
   id155 p(sat=s | intelligence=i), that is, given that
   the value of    intelligence    is i, what is the id203 of the value
   of    sat    being s.

   for instance, we can see that p(sat=s   | intelligence = i  ) is 0.8,
   that is, if the intelligence of the student is high, then the
   id203 of the sat score being high as well is 0.8. on the other
   hand, p(sat=s    | intelligence = i  ), which encodes the fact that if the
   intelligence of the student is high, then the id203 of the sat
   score being low is 0.2.

     note that the sum of values in each row is 1. that makes sense
     because given that intelligence=i  , the sat score can be either s   
     or s  , so the two probabilities must add up to 1. similarly, the cpd
     for    letter    encodes the conditional probabilities p(letter=l |
     grade=g). because    grade    can take three values, we have three rows
     in this table.

   the cpd for    grade    is easy to understand with the above knowledge.
   because it has two parents, the conditional probabilities will be of
   the form p(grade=g | difficulty=d, sat=s), that is, what is the
   id203 of    grade    being g, given that the value of    difficulty    is
   d and that of    sat    is s. each row now corresponds to a pair of values
   of    difficulty    and    intelligence.    again, the row values add up to 1.

   an essential requirement for id110s is that the graph must
   be a directed acyclic graph (dag).

markov networks: undirected id114

   here   s a simple example of a markov network:
   [0*fv20ddoj695rahc1.]

   in the interest of brevity, we will look at this abstract graph, where
   nodes a, b, etc. do not directly correspond to real-world quantities,
   as above. again, the edges represent the interaction between variables.
   we see that while a and b directly influence each other, a and c do
   not. note that markov networks do not need to be acyclic, as was the
   case with id110s.

potential functions

   just as we had cpds for id110s, we have tables to
   incorporate relations between nodes in markov networks. however, there
   are two crucial differences between these tables and cpds.

   first, the values do not need to sum to one, that is, the table does
   not define a id203 distribution. it only tells us that
   configurations with higher values are more likely. second, there is no
   conditioning. it is proportional to the joint distribution of all the
   variables involved, as opposed to conditional distributions in cpds.

   the resulting tables are called    factors    or    potential functions,    and
   denoted using the greek symbol     . so, for example, we can use the
   following potential function to capture the relation between three
   variables a, b, and c, where c is the    soft    xor of a and b, that is, c
   is likely to be 1 if a and b are different, and c is likely to be 0 if
   a and b are identical:
   [0*y0towpwzm5upyi5y.]

   in general, you will have a potential function defined for each maximal
   clique in the graph.

   the graph structure and the tables succinctly capture the joint
   id203 distribution over the random variables.

   a question you may have at this point is, why do we need directed as
   well as undirected graphs? the reason is that for some problems, it is
   more natural to represent them as directed graphs, such as the student
   network above, where it is easy to describe the causal relationship
   between variables         the intelligence of the student affects the sat
   score, but the sat score does not affect the intelligence of the
   student (although it may tell us about the intelligence of the
   student).

   for other kinds of problems, say, images, you may want to represent
   each pixel as a node, and we know that neighboring pixels affect each
   other, but there is no cause-effect relationship between the pixels;
   instead, the interaction is symmetric. so, we use undirected graphical
   models in such cases.

the problem setting

   we   ve been discussing graphs and random variables and tables so far,
   and you may be thinking what the point of all this is? what are we
   actually trying to do? where is machine learning in all this         data,
   training, prediction, etc.? this section will address these questions.

   let   s go back to the student network. suppose we have the graph
   structure with us, which we can create from our knowledge of the world
   (referred to as    domain knowledge    in machine learning). but we don   t
   have cpd tables, only their sizes. we do have some data though         for
   ten different classes in a university, we have a measure of their
   difficulty.

   also, we have data about each student in each of the courses         their
   intelligence, what their sat score was, what grade they got, and
   whether they got a good letter from the professor. from this data, we
   can estimate the parameters of the cpds. for instance, the data might
   show that students with high intelligence often get good sat scores,
   and we might be able to learn from it that p(sat=s1 | intelligence =
   i1)is high. this is the learning phase. we will soon see how we do this
   parameter estimation in bayesian and markov networks.

   now, for a new data point, you will observe some of the variables, but
   not all. for example, in the graph below, you know about the difficulty
   of a course and a student   s sat score, and you want to estimate the
   id203 of the student getting a good grade. (by now, you already
   have values in the tables from the learning phase.)
   [0*a1nt-ji0tcenhi4e.]

   while we don   t have a cpd that gives us that information directly, we
   can see that a high sat score from the student would suggest that the
   student is likely intelligent, and consequently, the id203 of a
   good grade is high if the difficulty of the course is low, as shown
   using the red arrows in the above image. we may also want to estimate
   the id203 of multiple variables simultaneously, like what is the
   id203 of the student getting a good grade and a good letter?

   the variables with known values are called    observed variables,    while
   those whose values are unobserved are called    hidden variables    or
      latent variables.    conventionally, observed variables are denoted
   using grey nodes, while latent variables are denoted using white nodes,
   as in the above image. we may be interested in finding the values of
   some or all of the latent variables.

   answering these questions is analogous to prediction in other areas of
   machine learning         in id114, this process is called
      id136.   

   while we used id110s to describe the terminology above, it
   applies as is to markov networks as well. before we get into the
   algorithms for learning and id136, let   s formalize the idea that we
   just looked at         given the value of some node, which other nodes can we
   get information about?

conditional independences

   the graph structures that we   ve been talking about so far actually
   capture important information about the variables. specifically, they
   define a set of conditional independences between the variables, that
   is, statements of the form            if a is observed, then b is independent
   of c.    let   s look at some examples.

   in the student network, let   s say you know that a student had a high
   sat score. what can you say about her grade? as we saw earlier, a high
   sat score suggests that the student is intelligent, and therefore, you
   would expect a good grade. what if the student has a low sat score? in
   this case, you would not expect a good grade.

   now, let   s say that you also know that the student is intelligent, in
   addition to her sat score. if the sat score was high, then you would
   expect a good grade. what if the sat score was low? you would still
   expect a good grade because you know that the student is intelligent,
   and you would assume that she just didn   t perform well enough on the
   sat. therefore, knowing the sat score doesn   t tell us anything if we
   see the intelligence of the student. to put this as a conditional
   independence statement, we would say            if intelligence is observed,
   then sat and grade are independent.   

     we got this conditional independence information from the way these
     nodes were connected in the graph. if they were connected
     differently, we would get different conditional independence
     information.

   let   s see this with another example.

   let   s say you know that the student is intelligent. what can you say
   about the difficulty of the course? nothing, right? now, what if i tell
   you that the student got a bad grade on the course? this would suggest
   that the course was hard because we know that an intelligent student
   got a bad grade. therefore we can write our conditional independence
   statement as follows            if grade is unobserved, then intelligence and
   difficulty are independent.   

   because these statements capture an independence between two nodes
   subject to a condition, they are called conditional independences. note
   that the two examples have opposite semantics         in the first one, the
   independence holds if the connecting node is observed; in the second
   one, the independence holds if the connecting node is unobserved. this
   difference is because of the way the nodes are connected, that is, the
   directions of arrows.

   we will not go into all possible cases in the interest of brevity, but
   it is straightforward to come up with these using intuition as above.

   in markov networks, we can use a similar intuition, but because there
   are no directional edges (arrows), the conditional independence
   statements are relatively simple         if there is no path between nodes a
   and b such that all nodes on the path are unobserved, then a and b are
   independent. said differently, if there is at least one path from a to
   b where all intermediate nodes are unobserved, then a and b are not
   independent.

   we will look at the details of how parameter estimation and id136
   are done in the second part of this blog post. for now, let   s look at
   an application of id110s, where we can use the idea of
   conditional independence we just learned.

application: monty hall problem

   you must have seen some version of this in a tv game show:
   [0*yprx4wmnc4ouml6p.]
   [19]illustration source

   the host shows you three closed doors, with a car behind one of the
   doors and something invaluable behind the others. you get to pick a
   door. then, the host opens one of the remaining doors, and shows that
   it does not contain the car. now, you have an option to switch the
   door, from the one you picked initially to the one that the host left
   unopened. do you switch?

   intuitively, it appears that the host did not divulge any information.
   it turns out that this intuition is not entirely correct. let   s use the
   new tool in our arsenal         id114         to understand this.

   let   s start by defining some variables:
     * d : the door with the car.
     * f : your first choice.
     * h : the door opened by the host.
     * i : is f = d?

   d, f, and h take values 1, 2, or 3 and i takes values 0 or 1. d and i
   are unobserved, while f is observed. until the host opens one of the
   doors, h is unobserved. therefore, we get the following bayesian
   network for our problem:
   [0*otsryvuykguqskcv.]

   note the directions of arrows         d and f are independent, i clearly
   depends on d and f, and the door picked by the host h also depends on d
   and f. so far, you don   t know anything about d. (this is similar to the
   structure in the student network, where knowing the intelligence of the
   student does not tell you anything about the difficulty of the course.)

   now, the host picks a door h and opens it. so, h is now observed.
   [0*nqtam9ihkqpn_yt4.]

   observing h does not tell us anything about i, that is, whether we have
   picked the right door. that is what our intuition tells us. however, it
   does tell us something about d! (again, drawing analogy with the
   student network, if you know that the student is intelligent, and the
   grade is low, it tells you something about the difficulty of the
   course.)

   let   s see this using numbers. the cpd tables for the variables are as
   follows (this is when no variables have been observed.):
   [0*rjyacxbve7brxmzt.]

   the tables for d and f are straightforward         the door with the car
   could be any door with equal id203, and we pick one of the doors
   with equal id203. the table for i simply says that i=1 when d and
   f are identical, and i=0 when d and f are different. the table for h
   says that if d and f are equal, then the host picks one door from the
   other two with equal id203, while if d and f are different, then
   the host picks the third door.

   now, let   s assume that we have picked a door, that is, f is now
   observed, say f=1. what are the conditional probabilities of i and d,
   given f?
   [0*4hou3c12unt_ca5o.]
   [0*tenlh3gu_rjqumtc.]

   using these equations, we get the following probabilities:
   [0*nxarlkhcwjxevcsv.]

   these numbers make sense         so far, the id203 that we have picked
   the correct door is     and the car could still be behind any door with
   equal id203.

   now, the host opens one of the doors other than f, so we observe h.
   assume h=2. let   s compute the new conditional probabilities of i and d
   given both f and h.
   [1*wnjwljjox5vhofd-c2n1bq.png]
   [1*-vl5lozei8v_h1zaj3tk_w.png]

   using the above equations, we get the following probabilities:
   [0*wjtemlfb_egqklfc.]

   therefore, we don   t know anything additional about i         our first choice
   is correct still with id203    , and this is what our intuition
   tells us. however, we now know that the car is behind door 3 with
   id203    , instead of    .

   so, if we switch, we get the car with id203    ; if we don   t, we
   get the car with id203    .

   we could have gotten the same answer without using id114
   too, but id114 give us a framework that scales well to
   larger problems.

conclusions

   in this pgm tutorial, we looked at some basic terminology in graphical
   models, including id110s, markov networks, conditional
   id203 distributions, potential functions, and conditional
   independences. we also looked at an application of id114 on
   the monty hall problem.

   in the second part of this blog post, we will look at some algorithms
   for parameter estimation and id136, and another application. so,
   stay tuned!

   iframe: [20]/media/02231cd5403151a2a463e36cc3b88462?postid=e4f1d72af189

you   d also like:

   [21]probabilistic id114 tutorial         part 2
   parameter estimation and id136 algorithmsblog.statsbot.co
   [22]sql queries for funnel analysis
   a template for building sql funnel queriesblog.statsbot.co
   [23]how to reduce churn rate by handling stripe failed payments
   how we automated dunning managementblog.statsbot.co

     * [24]machine learning
     * [25]data science
     * [26]bayesian statistics
     * [27]id114
     * [28]classifcation models

   (button)
   (button)
   (button) 988 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of prasoon goyal

[30]prasoon goyal

   phd candidate at ut austin. for more content on machine learning by me,
   check my quora profile
   ([31]https://www.quora.com/profile/prasoon-goyal).

     (button) follow
   [32]stats and bots

[33]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 988
     * (button)
     *
     *

   [34]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [35]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e4f1d72af189
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/probabilistic-graphical-models-tutorial-and-solutions-e4f1d72af189&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/probabilistic-graphical-models-tutorial-and-solutions-e4f1d72af189&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_42xq0ttanyc6---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@prasoongoyal13?source=post_header_lockup
  17. https://blog.statsbot.co/@prasoongoyal13
  18. https://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=pgm
  19. http://www.sciencemadesimple.co.uk/curriculum-blogs/maths-blogs/the-monty-hall-problem
  20. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=e4f1d72af189
  21. https://blog.statsbot.co/probabilistic-graphical-models-tutorial-d855ba0107d1
  22. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
  23. https://blog.statsbot.co/how-to-reduce-churn-rate-by-27-by-handling-stripe-failed-payments-e75892f8956c
  24. https://blog.statsbot.co/tagged/machine-learning?source=post
  25. https://blog.statsbot.co/tagged/data-science?source=post
  26. https://blog.statsbot.co/tagged/bayesian-statistics?source=post
  27. https://blog.statsbot.co/tagged/graphical-model?source=post
  28. https://blog.statsbot.co/tagged/classifcation-models?source=post
  29. https://blog.statsbot.co/@prasoongoyal13?source=footer_card
  30. https://blog.statsbot.co/@prasoongoyal13
  31. https://www.quora.com/profile/prasoon-goyal
  32. https://blog.statsbot.co/?source=footer_card
  33. https://blog.statsbot.co/?source=footer_card
  34. https://blog.statsbot.co/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://blog.statsbot.co/probabilistic-graphical-models-tutorial-d855ba0107d1
  38. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
  39. https://blog.statsbot.co/how-to-reduce-churn-rate-by-27-by-handling-stripe-failed-payments-e75892f8956c
  40. https://medium.com/p/e4f1d72af189/share/twitter
  41. https://medium.com/p/e4f1d72af189/share/facebook
  42. https://medium.com/p/e4f1d72af189/share/twitter
  43. https://medium.com/p/e4f1d72af189/share/facebook
