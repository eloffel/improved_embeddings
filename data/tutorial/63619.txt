   [tr?id=190747804793608&ev=pageview &noscript=1]

     * [1]

physics
     * [2]

mathematics
     * [3]

biology
     * [4]

computer science
     * [5]

all articles

     * (button)
     * (button)
     * (button)
     * (button)

how to win at deep learning

   (button)

share

     (button)
     *

comments
     * (button)

read later

[6]insights puzzle

how to win at deep learning

by [7]pradeep mutalik

   october 9, 2017
   what happens when you increase the number of layers in an artificial
   neural network?
   (button)
   math scuba diving math scuba diving

   [8]dan page for quanta magazine
   [deeplearning_2600.jpg]
   [mutalik_pradeep.jpg]

pradeep mutalik

   puzzle columnist
     __________________________________________________________________

   october 9, 2017
     __________________________________________________________________

   [9]view pdf/print mode
   [10]artificial intelligence[11]computer science[12]deep
   learning[13]insights puzzle[14]machine learning
   [15]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [16]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

      deep learning    is the new buzzword in the field of artificial
   intelligence. as natalie wolchover reported in a [17]recent quanta
   magazine article,       deep neural networks    have learned to converse,
   drive cars, [18]beat video games and [19]go champions, dream, paint
   pictures and help make scientific discoveries.    with such successes,
   one would expect deep learning to be a revolutionary new technique. but
   one would be quite wrong. the basis of deep learning stretches back
   more than half a century to the dawn of ai and the creation of both
   id158s having layers of connected neuronlike units
   and the    back propagation algorithm        a technique of applying error
   corrections to the strengths of the connections between neurons on
   different layers. over the decades, the popularity of these two
   innovations has fluctuated in tandem, in response not just to advances
   and failures, but also to support or disparagement from major figures
   in the field. back propagation was invented in the 1960s, around the
   same time that [20]frank rosenblatt   s    id88    learning algorithm
   called attention to the promise of id158s. back
   propagation was first applied to these networks in the 1970s, but the
   field suffered after [21]marvin minsky and seymour papert   s criticism
   of one-layer id88s. it made a comeback in the 1980s and 1990s
   after [22]david rumelhart, geoffrey hinton and ronald williams once
   again combined the two ideas, then lost favor in the 2000s when it fell
   short of expectations. finally, deep learning began conquering the
   world in the 2010s with the string of successes described above.

   what changed? only brute computing power, which made it possible for
   back-propagation-using id158s to have far more
   layers than before (hence the    deep    in    deep learning   ). this, in
   turn, allowed deep learning machines to train on massive amounts of
   data. it also allowed networks to be trained on a layer by layer basis,
   using [23]a procedure first suggested by hinton.

   can merely increasing the number of layers in a network produce such a
   big qualitative difference? let   s see if we can demonstrate it as we
   explore a simple neural network in this month   s puzzle questions.

problem 1

     we   re going to create a simple network that converts binary numbers
     to decimal numbers. imagine a network with just two layers     an
     input layer consisting of three units and an output layer with seven
     units. each unit in the first layer connects to each unit in the
     second, as shown in the figure below.

     as you can see, there are 21 connections. every unit in the input
     layer, at a given point in time, is either off or on, having a value
     of 0 or 1. every connection from the input layer to the output layer
     has an associated weight that, in id158s, is a
     real number between 0 and 1. just to be contrary, and to make the
     network a touch more similar to an actual neural network, let us
     allow the weight to be a real number between     1 and 1 (the negative
     sign signifying an inhibitory neuron). the product of the input   s
     value and the connection   s weight is conveyed to an output unit as
     shown below. the output unit adds up all the numbers it gets from
     its connections to obtain a single number, as shown in the figure
     below using arbitrary input values and connection weights for a
     single output unit. based on this number, the output number decides
     what its state is going to be. if it is more than a certain
     threshold, the unit   s value becomes 1, and if not, then its value
     becomes 0. we can call a unit that has a value of 1 a    firing    or
        activated    unit and a unit with a value of 0 a    quiescent    unit.

     our three input units from top to bottom can have the values 001,
     010, 011, 100, 101, 110 or 111, which readers may recognize as the
     binary numbers from 1 through 7.

     now the question: is it possible to adjust the connection weights
     and the thresholds of the seven output units such that every binary
     number input results in the firing of only one appropriate output
     unit, with all the others being quiescent? the position of the
     activated output unit should reflect the binary input value. thus,
     if the original input is 001, the leftmost output unit (or
     bottommost when viewed on a phone, as shown in the top figure) alone
     should fire, whereas if the original input is 101, the output unit
     that is fifth from the left (or from the bottom on a phone) alone
     should fire, and so on.

     if you think the above result is not possible, try to adjust the
     weights to get as close as you can. can you think of a simple
     adjustment, using additional connections but not additional units,
     that can improve your answer?

problem 2

     now let   s bring in the learning aspect. assume that the network is
     in an initial state where every connection weight is 0.5 and every
     output threshold is 1. the network is then presented with the entire
     set of all seven input values in serial order (10 times over, if
     required) and allowed to adjust its weights and thresholds based on
     the error difference between the observed value and the desired
     value. as in problem 1, the connection weights must remain between
        1 and 1. can you create a global, general purpose learning rule
     that adjusts the connections and thresholds such that the optimum
     condition of problem 1 can be reached or approached? for example, a
     learning rule could say something like    increase the connection
     weight by 0.2 within the permitted limits, and decrease its
     threshold by 0.1, if the output unit remains quiescent in a
     situation where it should have fired.    you can be as creative as you
     like with the rule, provided the specified limits are followed. note
     that the rule must be the same for all the units in a given layer
     and must be general purpose: it should allow the network to perform
     better for different sets of inputs and outputs as well.

problem 3

     finally, let   s add an extra layer of five units between the input
     and output layers. analyzing the resulting three-layer network will
     probably require computer help, such as using a spreadsheet or a
     simple simulation program. following the same rules as above, with
     extra rules for the hidden layer, how does the three-layer network
     perform on problems 1 and 2 compared to the two-layer network?

   that   s it for now. happy puzzling. hopefully, this exercise will
   slightly alter the connection weights and thresholds of some of your
   neurons in constructive ways!

   editor   s note: the reader who submits the most interesting, creative or
   insightful solution (as judged by the columnist) in the comments
   section will receive a quanta magazine t-shirt. and if you   d like to
   suggest a favorite puzzle for a future insights column, submit it as a
   comment below, clearly marked    new puzzle suggestion.    (it will not
   appear online, so solutions to the puzzle above should be submitted
   separately.)

   note that we may hold comments for the first day or two to allow for
   independent contributions by readers.

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [24](button) subscribe now
   [25]most recent newsletter
   [template]
   [mutalik_pradeep.jpg]

pradeep mutalik

   puzzle columnist
     __________________________________________________________________

   october 9, 2017
     __________________________________________________________________

   [26]view pdf/print mode
   [27]artificial intelligence[28]computer science[29]deep
   learning[30]insights puzzle[31]machine learning
   [32]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [33]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [34](button) subscribe now
   [35]most recent newsletter

the quanta newsletter

   get highlights of the most important news delivered to your email inbox
   ____________________
   (button) subscribe

[36]most recent newsletter

comment on this article

   quanta magazine moderates comments to facilitate an informed,
   substantive, civil conversation. abusive, profane, self-promotional,
   misleading, incoherent or off-topic comments will be rejected.
   moderators are staffed during regular business hours (new york time)
   and can only accept comments written in english.
   (button) show comments

next article

one-way salesman finds fast path home
     __________________________________________________________________

     * [37]about quanta
     * [38]archive
     * [39]contact us
     * [40]terms & conditions
     * [41]privacy policy
     * [42]simons foundation

   all rights reserved    2019

references

   visible links
   1. https://www.quantamagazine.org/physics/
   2. https://www.quantamagazine.org/mathematics/
   3. https://www.quantamagazine.org/biology/
   4. https://www.quantamagazine.org/computer-science/
   5. https://www.quantamagazine.org/archive/
   6. https://www.quantamagazine.org/tag/insights-puzzle/
   7. https://www.quantamagazine.org/authors/pradeep/
   8. https://www.danpage.net/
   9. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  10. https://www.quantamagazine.org/tag/artificial-intelligence/
  11. https://www.quantamagazine.org/tag/computer-science/
  12. https://www.quantamagazine.org/tag/deep-learning/
  13. https://www.quantamagazine.org/tag/insights-puzzle/
  14. https://www.quantamagazine.org/tag/machine-learning/
  15. https://www.amazon.com/dp/026253634x/
  16. https://www.amazon.com/dp/026253634x/
  17. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921
  18. https://www.quantamagazine.org/clever-machines-learn-how-to-be-curious-20170919/
  19. https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/
  20. http://lcn.epfl.ch/tutorial/english/id88/html/learning.html
  21. https://mitpress.mit.edu/books/id88s
  22. https://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html
  23. http://www.scholarpedia.org/article/deep_belief_networks
  24. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#newsletter
  25. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  26. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  27. https://www.quantamagazine.org/tag/artificial-intelligence/
  28. https://www.quantamagazine.org/tag/computer-science/
  29. https://www.quantamagazine.org/tag/deep-learning/
  30. https://www.quantamagazine.org/tag/insights-puzzle/
  31. https://www.quantamagazine.org/tag/machine-learning/
  32. https://www.amazon.com/dp/026253634x/
  33. https://www.amazon.com/dp/026253634x/
  34. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#newsletter
  35. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  36. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  37. https://www.quantamagazine.org/about/
  38. https://www.quantamagazine.org/archive
  39. https://www.quantamagazine.org/contact-us/
  40. https://www.quantamagazine.org/terms-conditions/
  41. https://www.quantamagazine.org/privacy-policy/
  42. http://www.simonsfoundation.org/

   hidden links:
  44. https://www.quantamagazine.org/
  45. https://www.quantamagazine.org/
  46. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comments
  47. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  48. https://twitter.com/share?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&text=how%20to%20win%20at%20deep%20learning&via=quantamagazine
  49. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#0
  50. mailto:?subject=how%20to%20win%20at%20deep%20learning&body=what%20happens%20when%20you%20increase%20the%20number%20of%20layers%20in%20an%20artificial%20neural%20network?%0a%0ahttps://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  51. https://getpocket.com/save?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&title=how%20to%20win%20at%20deep%20learning
  52. http://digg.com/submit?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  53. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&t=how%20to%20win%20at%20deep%20learning
  54. https://share.flipboard.com/bookmarklet/popout?v=how%20to%20win%20at%20deep%20learning&url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  55. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comments
  56. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comments
  57. https://www.quantamagazine.org/authors/pradeep/
  58. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  59. https://twitter.com/share?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&text=how%20to%20win%20at%20deep%20learning&via=quantamagazine
  60. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#0
  61. mailto:?subject=how%20to%20win%20at%20deep%20learning&body=what%20happens%20when%20you%20increase%20the%20number%20of%20layers%20in%20an%20artificial%20neural%20network?%0a%0ahttps://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  62. https://getpocket.com/save?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&title=how%20to%20win%20at%20deep%20learning
  63. http://digg.com/submit?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  64. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&t=how%20to%20win%20at%20deep%20learning
  65. https://share.flipboard.com/bookmarklet/popout?v=how%20to%20win%20at%20deep%20learning&url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  66. https://www.quantamagazine.org/authors/pradeep/
  67. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  68. https://twitter.com/share?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&text=how%20to%20win%20at%20deep%20learning&via=quantamagazine
  69. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#0
  70. mailto:?subject=how%20to%20win%20at%20deep%20learning&body=what%20happens%20when%20you%20increase%20the%20number%20of%20layers%20in%20an%20artificial%20neural%20network?%0a%0ahttps://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  71. https://getpocket.com/save?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&title=how%20to%20win%20at%20deep%20learning
  72. http://digg.com/submit?url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  73. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/&t=how%20to%20win%20at%20deep%20learning
  74. https://share.flipboard.com/bookmarklet/popout?v=how%20to%20win%20at%20deep%20learning&url=https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  75. https://www.quantamagazine.org/one-way-salesman-finds-fast-path-home-20171005/
  76. https://www.quantamagazine.org/
  77. https://www.facebook.com/quantanews
  78. https://twitter.com/quantamagazine
  79. http://youtube.com/c/quantamagazineorgnews
  80. https://instagram.com/quantamag
  81. http://www.dogstudio.be/
