   [tr?id=190747804793608&ev=pageview &noscript=1]

     * [1]

physics
     * [2]

mathematics
     * [3]

biology
     * [4]

computer science
     * [5]

all articles

     * (button)
     * (button)
     * (button)
     * (button)

solution:    how to win at deep learning   

   (button)

share

     (button)
     *

comments
     * (button)

read later

[6]insights puzzle

solution:    how to win at deep learning   

by [7]pradeep mutalik

   november 3, 2017
   when equipped with hidden layers, deep neural networks can accomplish
   nonlinear feats that are difficult even with sophisticated mathematics.
   (button)
   [javascript]
   [mutalik_pradeep.jpg]

pradeep mutalik

   puzzle columnist
     __________________________________________________________________

   november 3, 2017
     __________________________________________________________________

   [8]view pdf/print mode
   [9]artificial intelligence[10]computer science[11]deep
   learning[12]insights puzzle[13]machine learning
   [14]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [15]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

   deep learning promises to make computers far better at tasks like
   facial recognition, image understanding and machine translation. our
   [16]october puzzle asked you to play with simple artificial neural
   networks to explore the phenomenon of deep learning and gain some
   insight into how it has [17]achieved such spectacular successes in the
   field of artificial intelligence.

problem 1

     we   re going to create a simple network that converts binary numbers
     to decimal numbers. imagine a network with just two layers: an input
     layer consisting of three units and an output layer with seven
     units. each unit in the first layer connects to each unit in the
     second, as shown in the figure below.

     as you can see, there are 21 connections. every unit in the input
     layer, at a given point in time, is either off or on, having a value
     of 0 or 1. every connection from the input layer to the output layer
     has an associated weight that, in id158s, is a
     real number between 0 and 1. just to be contrary, and to make the
     network a touch more similar to an actual neural network, let us
     allow the weight to be a real number between    1 and 1 (the negative
     sign signifying an inhibitory neuron). the product of the input   s
     value and the connection   s weight is conveyed to an output unit as
     shown below. the output unit adds up all the numbers it gets from
     its connections to obtain a single number, as shown in the figure
     below using arbitrary input values and connection weights for a
     single output unit. based on this number, the output number decides
     what its state is going to be. if it exceeds a certain threshold,
     the unit   s value becomes 1, and if not, its value becomes 0. we can
     call a unit that has a value of 1 a    firing    or    activated    unit and
     a unit with a value of 0 a    quiescent    unit.

     our three input units from left to right can have the values 001,
     010, 011, 100, 101, 110 or 111, which readers may recognize as the
     binary numbers from 1 through 7.

     now the question: is it possible to adjust the connection weights
     and the thresholds of the seven output units so that every binary
     number input results in the firing of only one appropriate output
     unit, with all the others being quiescent? the position of the
     activated output unit should reflect the binary input value. thus,
     if the original input is 001, the leftmost output unit (or
     bottommost when viewed on a phone, as shown in the top figure) alone
     should fire, whereas if the original input is 101, the output unit
     that is fifth from the left (or from the bottom on a phone) alone
     should fire, and so on.

     if you think the above result is not possible, try to adjust the
     weights to get as close as you can. can you think of a simple
     adjustment, using additional connections but not additional units,
     that can improve your answer?

   as many readers showed, it is certainly possible to do this     in fact,
   it is quite simple. here are some of the solutions that readers found.

   [18]nightrider, [19]irhum shafkat, [20]john smith, [21]tom nichols,
   [22]jr and [23]hongzhou lin essentially found the same elegant
   solution: assign a unit weight (1 or 0.5) to a connection if the input
   unit contributes to an output unit and use its negative value (-1 or
   -0.5), if not. under this scheme, the input-output weight matrix is as
   follows:
   1  -1 1  -1 1  -1 1
   -1 1  1  -1 -1 1  1
   -1 -1 -1 1  1  1  1

   the rows are the three input units (001, 010 and 100), and the columns
   are the seven output units (1 through 7). the threshold of an output
   unit is simply the number of inputs required to make the output unit
   fire: it is 1 for units 1, 2 and 4; 2 for units 3, 5 and 6; and 3 for
   unit 7. this problem was simple enough that such solutions can be found
   by trial and error. for more complicated problems, the formal
   techniques of matrix mathematics are invaluable in neural network
   computing, and were demonstrated by all of the above commenters in very
   sophisticated ways.

   a simple and elegant solution was proposed by [24]russ abbott. it is
   similar to the above, but uses a weight of 0.3 for a required input,
   and -1 for a wrong input. here too, the output units have thresholds
   dependent on the number of required units     in this case, 0.3, 0.6 and
   0.9. a slightly more difficult problem would have been to require the
   thresholds to be the same for all the units, as they are in the brain.
   in this case you would have to adjust the weights themselves based on
   the number of input units. here   s a solution matrix of this sort.
   1    -0.5 0.5 -0.5 0.5 -1  0.33
   -0.5 1    0.5 -0.5 -1  0.5 0.33
   -0.5 -0.5 -1  1    0.5 0.5 0.33

   in this case, the threshold for all the output units can be set at 2/3.

problem 2

     now let   s bring in the learning aspect. assume that the network is
     in an initial state where every connection weight is 0.5 and every
     output threshold is 1. the network is then presented with the entire
     set of all seven input values in serial order (10 times over, if
     required) and allowed to adjust its weights and thresholds based on
     the error difference between the observed value and the desired
     value. as in problem 1, the connection weights must remain between
        1 and 1. can you create a global, general-purpose learning rule
     that adjusts the connections and thresholds so that the optimum
     condition of problem 1 can be reached or approached? for example, a
     learning rule could say something like    increase the connection
     weight by 0.2 within the permitted limits, and decrease its
     threshold by 0.1, if the output unit remains quiescent in a
     situation where it should have fired.    you can be as creative as you
     like with the rule, provided that the specified limits are followed.
     note that the rule must be the same for all the units in a given
     layer and must be general-purpose: it should allow the network to
     perform better for different sets of inputs and outputs as well.

   the general method for solving such problems is called    gradient
   descent.    think of the network   s current state as being at the top of a
   mountain, from which it descends one step at a time in the direction of
   the desired result: the ground. the idea is simple: you start the
   network with some set of random weights. you give the network an input,
   and compare the ideal, correct output with the network   s current
   output. if it is correct, do nothing. if it is wrong, you find the
   difference between the ideal output, multiply it by a very small number
   known as the learning rate (denoted by the greek letter   ,    eta   ) and
   add the product to the weights going into that unit in the direction it
   needs to go. the specific values that can be used in the current
   problem and the general rule are explained formally by [25]tom nichols,
   [26]jr and [27]hongzhou lin. generally, there is no guarantee that
   networks will converge to the best possible solution: the network may
   get stuck in what is known as a local minimum (think of this as the
   bottom of a valley in the foothills, rather than the ground). for most
   simple problems, networks generally converge to very good solutions, if
   not the perfect ones. for more complex problems, finding the best rules
   may not be so easy. a [28]recent article in the new york times titled
      tech giants are paying huge salaries for scarce a.i. talent    describes
   the process of creating rules for deep learning:    the basic concepts of
   deep learning are not hard to grasp, requiring little more than
   high-school-level math. but real expertise requires more significant
   math and an intuitive talent that some call    a dark art.      

   in our case, this very simple network can very easily solve the problem
   posed. in fact, we do not need to use a network with hidden layers at
   all. what then is the limitation of such simple networks? as i
   described in the puzzle column, the field suffered after [29]marvin
   minsky and seymour papert   s criticism of simple networks for a very
   specific task, which gives us an insight about how such networks work.
   this insight, which was described by [30]tom nichols in the context of
   this problem, is that single-layer neural networks can be viewed
   geometrically. think of neural networks as machines that can classify
   groups of objects, which can be pictured as being plotted on an xy
   graph based on numbers that give a measure of two of their properties.

   this diagram shows two groups of objects that need to be separated. for
   many such problems, the groups are    linearly separable        a simple
   network without hidden layers can do the job by properly placing a
   single straight line through the graph. (for problems such as ours,
   which have multiple inputs and outputs, the number of dimensions
   increases far beyond the two shown in an xy graph: squares become
      hypercubes    and the line becomes a    hyperplane    as [31]nichols
   described. however, the principle of linear separability remains.
   [32]nightrider described a linear-programming procedure that can yield
   the optimal such separating hyperplane in the general case). many
   problems can be solved by drawing a single separating line or
   hyperplane, and therefore by networks like ours. but there are other
   problems, like the one in the figure above illustrating the famous
      [33]xor problem,    which cannot be solved by simple networks without
   hidden layers, as minsky and papert showed. no single straight line can
   separate the two groups shown in this figure.

   astute readers will notice, however, that two straight lines can
   separate the groups. an additional unit in an extra,    hidden    layer in
   a network can be thought of as giving you the ability to add an
   additional line. these lines can then be added together or combined in
   other ways by using more hidden layers.

   in the diagram above, one group of objects is completely enclosed by
   the other group. but a set of four straight lines can separate them,
   and therefore so can a deep network with hidden layers. you can see how
   this makes deep networks so powerful: with a bunch of straight lines,
   you can extract highly mixed groups of any shape, regular or irregular.
   thus deep neural networks can do nonlinear feats, which are difficult
   with even sophisticated mathematics.

problem 3

     finally, let   s add an extra layer of five units between the input
     and output layers. analyzing the resulting three-layer network will
     probably require computer help, like using a spreadsheet or a simple
     simulation program. following the same rules as above, with extra
     rules for the hidden layer, how does the three-layer network perform
     on problems 1 and 2 compared with the two-layer network?

   as several commenters noted, an extra layer is not necessary for this
   problem     the seven outputs are linearly separable. there are
   technically interesting ways to compute the learning rate for the
   additional layer, as [34]hongzhou lin remarked     skills that are
   currently of great value in deep learning. apart from that, though, a
   three-layer network can discover and generalize other concepts that are
   hidden in the problem. for instance, the hidden layer of our
   three-layer network could very well develop nodes that are sensitive to
   even numbers, odd numbers, numbers less than 4, numbers greater than or
   equal to 4 and so on. it is quite likely that the complex networks of
   the human brain develop such neurons that generalize similar properties
   of real-world data in the process of solving specific problems. no
   doubt, this gives human intelligence its immense flexibility.

   the comments to this puzzle were excellent and very sophisticated. some
   of the commenters were obviously experts and highly trained. i   d like
   to specifically mention [35]nightrider, [36]tom nichols, [37]jr and
   [38]hongzhou lin in this regard. from the point of view of a puzzle
   column, we also enjoy simple elegant solutions like those provided by
   [39]nightrider, [40]irhum shafkat, [41]john smith and [42]russ abbott,
   which can be appreciated by those without formidable technical skills.

   while [43]tom nichols, [44]jr and [45]hongzhou lin submitted excellent
   solutions for all the problems, only one person can win the quanta
   t-shirt for october. that goes to nightrider for comments that appealed
   to both categories of comments described above.

   thank you all for your contributions. see you soon for new insights.

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [46](button) subscribe now
   [47]most recent newsletter
   [template]
   [mutalik_pradeep.jpg]

pradeep mutalik

   puzzle columnist
     __________________________________________________________________

   november 3, 2017
     __________________________________________________________________

   [48]view pdf/print mode
   [49]artificial intelligence[50]computer science[51]deep
   learning[52]insights puzzle[53]machine learning
   [54]alice and bob meet the wall of fire - the biggest ideas in science
   from quanta     available now! [55]alice and bob meet the wall of fire -
   the biggest ideas in science from quanta     available now!

share this article
     __________________________________________________________________

newsletter

   get quanta magazine delivered to your inbox
   [56](button) subscribe now
   [57]most recent newsletter

the quanta newsletter

   get highlights of the most important news delivered to your email inbox
   ____________________
   (button) subscribe

[58]most recent newsletter

comment on this article

   quanta magazine moderates comments to facilitate an informed,
   substantive, civil conversation. abusive, profane, self-promotional,
   misleading, incoherent or off-topic comments will be rejected.
   moderators are staffed during regular business hours (new york time)
   and can only accept comments written in english.
   (button) show comments

next article

life   s first molecule was protein, not rna, new model suggests
     __________________________________________________________________

     * [59]about quanta
     * [60]archive
     * [61]contact us
     * [62]terms & conditions
     * [63]privacy policy
     * [64]simons foundation

   all rights reserved    2019

references

   visible links
   1. https://www.quantamagazine.org/physics/
   2. https://www.quantamagazine.org/mathematics/
   3. https://www.quantamagazine.org/biology/
   4. https://www.quantamagazine.org/computer-science/
   5. https://www.quantamagazine.org/archive/
   6. https://www.quantamagazine.org/tag/insights-puzzle/
   7. https://www.quantamagazine.org/authors/pradeep/
   8. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
   9. https://www.quantamagazine.org/tag/artificial-intelligence/
  10. https://www.quantamagazine.org/tag/computer-science/
  11. https://www.quantamagazine.org/tag/deep-learning/
  12. https://www.quantamagazine.org/tag/insights-puzzle/
  13. https://www.quantamagazine.org/tag/machine-learning/
  14. https://www.amazon.com/dp/026253634x/
  15. https://www.amazon.com/dp/026253634x/
  16. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/
  17. https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921
  18. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3559745908
  19. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3560744422
  20. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3563532095
  21. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  22. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3561081603
  23. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3570011490
  24. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3559959409
  25. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  26. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3561081603
  27. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3570011490
  28. https://www.nytimes.com/2017/10/22/technology/artificial-intelligence-experts-salaries.html?_r=0
  29. https://mitpress.mit.edu/books/id88s
  30. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  31. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  32. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3592389159
  33. http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html
  34. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3570011490
  35. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3592389159
  36. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  37. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3561081603
  38. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3570011490
  39. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3559745908
  40. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3560744422
  41. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3563532095
  42. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3559959409
  43. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3566305123
  44. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3561081603
  45. https://www.quantamagazine.org/how-to-win-at-deep-learning-20171009/#comment-3570011490
  46. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#newsletter
  47. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  48. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  49. https://www.quantamagazine.org/tag/artificial-intelligence/
  50. https://www.quantamagazine.org/tag/computer-science/
  51. https://www.quantamagazine.org/tag/deep-learning/
  52. https://www.quantamagazine.org/tag/insights-puzzle/
  53. https://www.quantamagazine.org/tag/machine-learning/
  54. https://www.amazon.com/dp/026253634x/
  55. https://www.amazon.com/dp/026253634x/
  56. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#newsletter
  57. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  58. http://us1.campaign-archive2.com/home/?u=0d6ddf7dc1a0b7297c8e06618&id=f0cb61321c
  59. https://www.quantamagazine.org/about/
  60. https://www.quantamagazine.org/archive
  61. https://www.quantamagazine.org/contact-us/
  62. https://www.quantamagazine.org/terms-conditions/
  63. https://www.quantamagazine.org/privacy-policy/
  64. http://www.simonsfoundation.org/

   hidden links:
  66. https://www.quantamagazine.org/
  67. https://www.quantamagazine.org/
  68. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#comments
  69. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  70. https://twitter.com/share?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&text=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&via=quantamagazine
  71. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#0
  72. mailto:?subject=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&body=when%20equipped%20with%20hidden%20layers,%20deep%20neural%20networks%20can%20accomplish%20nonlinear%20feats%20that%20are%20difficult%20even%20with%20sophisticated%20mathematics.%0a%0ahttps://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  73. https://getpocket.com/save?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&title=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  74. http://digg.com/submit?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  75. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&t=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  76. https://share.flipboard.com/bookmarklet/popout?v=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  77. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#comments
  78. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#comments
  79. https://www.quantamagazine.org/authors/pradeep/
  80. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  81. https://twitter.com/share?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&text=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&via=quantamagazine
  82. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#0
  83. mailto:?subject=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&body=when%20equipped%20with%20hidden%20layers,%20deep%20neural%20networks%20can%20accomplish%20nonlinear%20feats%20that%20are%20difficult%20even%20with%20sophisticated%20mathematics.%0a%0ahttps://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  84. https://getpocket.com/save?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&title=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  85. http://digg.com/submit?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  86. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&t=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  87. https://share.flipboard.com/bookmarklet/popout?v=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  88. https://www.quantamagazine.org/authors/pradeep/
  89. http://www.facebook.com/sharer.php?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  90. https://twitter.com/share?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&text=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&via=quantamagazine
  91. https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/#0
  92. mailto:?subject=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&body=when%20equipped%20with%20hidden%20layers,%20deep%20neural%20networks%20can%20accomplish%20nonlinear%20feats%20that%20are%20difficult%20even%20with%20sophisticated%20mathematics.%0a%0ahttps://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  93. https://getpocket.com/save?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&title=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  94. http://digg.com/submit?url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  95. https://news.ycombinator.com/submitlink?u=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/&t=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99
  96. https://share.flipboard.com/bookmarklet/popout?v=solution:%20%e2%80%98how%20to%20win%20at%20deep%20learning%e2%80%99&url=https://www.quantamagazine.org/deep-learning-puzzle-solution-20171103/
  97. https://www.quantamagazine.org/lifes-first-molecule-was-protein-not-rna-new-model-suggests-20171102/
  98. https://www.quantamagazine.org/
  99. https://www.facebook.com/quantanews
 100. https://twitter.com/quantamagazine
 101. http://youtube.com/c/quantamagazineorgnews
 102. https://instagram.com/quantamag
 103. http://www.dogstudio.be/
