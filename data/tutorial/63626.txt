   #[1]tim wheeler    feed [2]tim wheeler    comments feed [3]tim wheeler   
   alphago zero - how and why it works comments feed [4]apteryx flight
   data with c3.js [5]how we wrote a textbook [6]alternate [7]alternate

   tim wheeler

menu

   [8]skip to content

     [9]about

     [10]archive

     [11]blog

post navigation

   [12]    apteryx flight data with c3.js
   [13]how we wrote a textbook    

alphago zero - how and why it works

   [14]november 2, 2017 by [15]tim

   deepmind's alphago made waves when it became the first ai to beat a top
   human go player in march of 2016. this version of alphago - alphago lee
   - used a large set of go games from the best players in the world
   during its training process. [16]a new paper was released a few days
   ago detailing a new neural net---[17]alphago zero---that does not need
   humans to show it how to play go. not only does it outperform all
   previous go players, human or machine, it does so after only three days
   of training time. this article will explain how and why it works.

id169

   the go-to algorithm for writing bots to play discrete, deterministic
   games with perfect information is id169 (mcts). a bot
   playing a game like go, chess, or checkers can figure out what move it
   should make by trying them all, then checking all possible responses by
   the opponent, all possible moves after that, etc. for a game like go
   the number of moves to try grows really fast.
   id169 will selectively try moves based on how good it
   thinks they are, thereby focusing its effort on moves that are most
   likely to happen.


   [18]alphagozero_01

   more technically, the algorithm works as follows.  the game-in-progress
   is in an initial state s_0 , and it is the bot's turn to play. the bot
   can choose from a set of actions \mathcal{a} . id169
   begins with a tree consisting of a single node for s_0 . this node is
   expanded by trying every action a \in \mathcal{a} and constructing a
   corresponding child node for each action. below we show this expansion
   for a game of tic-tac-toe:

   [19]alphagozero02

   the value of each new child node must then be determined. the game in
   the child node is rolled out by randomly taking moves from the child
   state until a win, loss, or tie is reached. wins are scored at +1 ,
   losses at -1 , and ties at 0 .

   [20]alphagozero03

   the random rollout for the first child given above estimates a value of
   +1 . this value may not represent optimal play-it can vary based on how
   the rollout progresses. one can run rollouts unintelligently, drawing
   moves uniformly at random. one can often do better by following a
   better-though still typically random-strategy, or by estimating the
   value of the state directly. more on that later.

   [21]alphagozero04

   above we show the expanded tree with approximate values for each child
   node. note that we store two properties: the accumulated value w and
   the number of times rollouts have been run at or below that node, n
   . we have only visited each node once.

   the information from the child nodes is then propagated back up the
   tree by increasing the parent's value and visit count. its accumulated
   value is then set to the total accumulated value of its children:[22]
   alphagozero05

   id169 continues for multiple iterations consisting of
   selecting a node, expanding it, and propagating back up the new
   information. expansion and propagation have already been covered.

   id169 does not expand all leaf nodes, as that would
   be very expensive. instead, the selection process chooses nodes that
   strike a balance between being lucrative-having high estimated
   values-and being relatively unexplored-having low visit counts.

   a leaf node is selected by traversing down the tree from the root node,
   always choosing the child i with the highest upper confidence tree
   (uct) score:

   u_i = \frac{w_i}{n_i} + c\sqrt{\frac{\ln n_p}{n_i}}
   where w_i is the accumulated value of the i th child, n_i is the visit
   count for i th child, and n_p is the number of visit counts for the
   parent node. the parameter c \geq 0 controls the tradeoff between
   choosing lucrative nodes (low c ) and exploring nodes with low visit
   counts (high c ). it is often set empirically.

   the uct scores ( u 's) for the tic-tac-toe tree with c=1 are:

   [23]alphagozero06

   in this case we pick the first node, s_{0,1} . (in the event of a tie
   one can either randomly break the tie or just pick the first of the bet
   nodes.) that node is expanded and the values are propagated back up:

   [24]alphagozero07

   note that each accumulated value w reflects whether x's won or
   lost. during selection, we keep track of whether it is x's or o's turn
   to move, and flip the sign of w whenever it is o's turn.

   we continue to run iterations of id169 until we run
   out of time. the tree is gradually expanded and we (hopefully) explore
   the possible moves, identifying the best move to take. the bot then
   actually makes a move in the original, real game by picking the first
   child with the highest number of visits. for example, if the top of our
   tree looks like:

   [25]alphagozero08

   then the bot would choose the first action and proceed to s_{0,1} .

efficiency through expert policies

   games like chess and go have very large branching factors. in a given
   game state there are many possible actions to take, making it very
   difficult to adequately explore the future game states. as a result,
   there are an estimated 10^{46} [26]board states in chess, and go played
   on a traditional 19 \times 19 board has around 10^{170} (tic-tac-toe
   only has 5478 states).

   move evaluation with vanilla id169 just isn't
   efficient enough. we need a way to further focus our attention to
   worthwhile moves.

   suppose we have an expert policy  \pi that, for a given state s , tells
   us how likely an expert-level player is to make each possible action.
   for the tic-tac-toe example, this might look like:

   [27]alphagozero09

   where each p_i = \pi(a_i \mid s_0) is the id203 of choosing the i
   th action a_i given the root state s_0 .

   if the expert policy is really good then we can produce a strong bot by
   directly drawing our next action according to the probabilities
   produces by \pi , or better yet, by taking the move with the highest
   id203. unfortunately, getting an expert policy is difficult, and
   verifying that one's policy is optimal is difficult as well.

   fortunately, one can improve on a policy by using a modified form of
   id169. this version will also store the id203
   of each node according to the policy, and this id203 is used to
   adjust the node's score during selection. the probabilistic upper
   confidence tree score used by deepmind is:

   u_i = \frac{w_i}{n_i} + c p_i\sqrt{\frac{\ln n_p}{1 + n_i}}

   as before, the score trades off between nodes that consistently produce
   high scores and nodes that are unexplored. now, node exploration is
   guided by the expert policy, biasing exploration towards moves the
   expert policy considers likely. if the expert policy truly is good,
   then id169 efficiently focuses on good evolutions of
   the game state. if the expert policy is poor, then monte carlo tree
   search may focus on bad evolutions of the game state. either way, in
   the limit as the number of samples gets large, the value of a node is
   dominated by the win/loss ratio w_i/n_i , as before.

efficiency through value approximation

   a second form of efficiency can be achieved by avoiding expensive and
   potentially inaccurate random rollouts. one option is to use the expert
   policy from the previous section to guide the random rollout. if the
   policy is good, then the rollout should reflect more realistic,
   expert-level game progressions and thus more reliably estimate a
   state's value.

   a second option is to avoid rollouts altogether, and directly
   approximate the value of a state with a value approximator function
   \hat{w}(x) . this function takes a state and directly computes a value
   in [-1,1] , without conducting rollouts. clearly, if \hat{w} is a good
   approximation of the true value, but can be executed faster than a
   rollout, then execution time can be saved without sacrificing
   performance.

   [28]alphagozero10

   value approximation can be used in tandem with an expert policy to
   speed up id169. a serious concern remains-how does
   one obtain an expert policy and a value function? does an algorithm
   exist for training the expert policy and value function?

the alpha zero neural net

   the alpha zero algorithm produces better and better expert policies and
   value functions over time by playing games against itself with
   accelerated id169. the expert policy \pi and the
   approximate value function \hat{w} are both represented by deep neural
   networks. in fact, to increase efficiency, alpha zero uses one neural
   network f that takes in the game state and produces both the
   probabilities over the next move and the approximate state value.
   (technically, it takes in the previous eight game states and an
   indicator telling it whose turn it is.)

   f(s) \rightarrow [\boldsymbol{\mathbf{p}}, w]

   leaves in the search tree are expanded by evaluating them with the
   neural network. each child is initialized with n = 0 , w = 0 , and with
   p corresponding to the prediction from the network. the value of the
   expanded node is set to the predicted value and this value is then
   backed up the tree.

   [29]alphagozero11

   selection and backup are unchanged. simply put, during backup a
   parent's visit counts are incremented and its value is increased
   according to w .

   the search tree following another selection, expansion, and backup step
   is:

   [30]alphagozero12

   the core idea of the alpha zero algorithm is that the predictions of
   the neural network can be improved, and the play generated by monte
   carlo tree search can be used to provide the training data. the policy
   portion of the neural network is improved by training the predicted
   probabilities \boldsymbol{\mathbf{p}} for s_0 to match the improved
   id203 {\boldsymbol{\mathbf{\pi}}} obtained from running monte
   carlo tree on s_0 . after running id169, the improved
   policy prediction is:

   \pi_i = n_i^{1/\tau}
   for a constant \tau .
   values of \tau close to zero produce policies that choose the best move
   according to the id169 evaluation.

   the value portion of the neural network is improved by training the
   predicted value to match the eventual win/loss/tie result of the game,
   z . their id168 is:

   (w - z)^2 + {\boldsymbol{\mathbf{\pi}}}^\top \ln\boldsymbol{\mathbf{p}}
   + \lambda \|\boldsymbol{\mathbf{\theta}}\|_2^2
   where (w - z)^2 is the value loss, \pi^\top \ln\boldsymbol{\mathbf{p}}
   is the policy loss, and \lambda \|\theta\|_2^2 is an extra
   id173 term with parameter \lambda \geq 0 and
   \boldsymbol{\mathbf{\theta}} represents the parameters in the neural
   network.

   training is done entirely in self-play. one starts with a randomly
   initialized set of neural network parameters
   \boldsymbol{\mathbf{\theta}} . this neural network is then used in
   multiple games in which it plays itself. in each of these games, for
   each move, id169 is used to calculate \pi . the final
   outcome of each game determines that game's value for z . the
   parameters \boldsymbol{\mathbf{\theta}} are then improved by using
   id119 (or any of the more sophisticated accelerated descent
   methods-alpha zero used stochastic id119 with momentum and
   learning rate annealing.) on the id168 for a random selection
   of states played.

closing comments

   and that's it. the folks at deepmind contributed a clean and stable
   learning algorithm that trains game-playing agents efficiently using
   only data from self-play. while the current zero algorithm only works
   for discrete games, it will be interesting whether it will be extended
   to mdps or their partially observable counterparts in the future.

   it is interesting to see how quickly the field of ai is progressing.
   those who claim we will be able to see the robot overlords coming in
   time should take heed - these ai's will only be human-level for a brief
   instant before blasting past us into superhuman territories, never to
   look back.

   this entry was posted in [31]uncategorized. bookmark the [32]permalink.

post navigation

   [33]    apteryx flight data with c3.js
   [34]how we wrote a textbook    

2 thoughts on    alphago zero - how and why it works   

    1. pingback: [35]how to build your own alphazero ai using python and
       keras | copy paste programmers
    2. pingback: [36]alphazero and the curse of human knowledge | copy
       paste programmers

   comments are closed.

meta

     * [37]log in

   [38]running on wordpress | theme: mog by [39]hndr.

references

   visible links
   1. http://tim.hibal.org/blog/feed/
   2. http://tim.hibal.org/blog/comments/feed/
   3. http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/feed/
   4. http://tim.hibal.org/blog/apteryx-flight-data-with-c3-js/
   5. http://tim.hibal.org/blog/how-we-wrote-a-textbook/
   6. http://tim.hibal.org/blog/wp-json/oembed/1.0/embed?url=http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/
   7. http://tim.hibal.org/blog/wp-json/oembed/1.0/embed?url=http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/&format=xml
   8. http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/#content
   9. http://tim.hibal.org/blog/
  10. http://tim.hibal.org/blog/archive/
  11. http://tim.hibal.org/blog/blog/
  12. http://tim.hibal.org/blog/apteryx-flight-data-with-c3-js/
  13. http://tim.hibal.org/blog/how-we-wrote-a-textbook/
  14. http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/
  15. http://tim.hibal.org/blog/author/admin/
  16. https://www.nature.com/articles/nature24270.epdf?author_access_token=vjxbvjashxfoctqq4p2k4trgn0jajwel9jnr3zotv0pvw4gb86eepgqtrdtpiz-2rmo8-kg06gqvobu5nscfehilhcvfuemsbvws-lxjqqgg98faovwjxetugzaumnrq
  17. https://deepmind.com/blog/alphago-zero-learning-scratch/
  18. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_294.png
  19. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_280.png
  20. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_281.png
  21. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_282.png
  22. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_283.png
  23. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_284.png
  24. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_285.png
  25. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_286.png
  26. http://tromp.github.io/chess/chess.html
  27. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_287.png
  28. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_288.png
  29. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_289.png
  30. http://tim.hibal.org/blog/wp-content/uploads/2017/11/selection_290.png
  31. http://tim.hibal.org/blog/category/uncategorized/
  32. http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/
  33. http://tim.hibal.org/blog/apteryx-flight-data-with-c3-js/
  34. http://tim.hibal.org/blog/how-we-wrote-a-textbook/
  35. http://copypasteprogrammers.com/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188/
  36. http://copypasteprogrammers.com/alphazero-and-the-curse-of-human-knowledge-324a3a0ad1b6/
  37. http://tim.hibal.org/blog/wp-login.php
  38. http://wordpress.org/
  39. http://hndr.me/

   hidden links:
  41. http://tim.hibal.org/blog/
