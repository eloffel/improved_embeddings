   [1][logo.png]
   hyperparameter space
     * [2]all posts
     * [3]about

   [4]github [5]twitter [6]linkedin [7]google scholar [8]email [9]rss

   when not to use deep learning
   jun 16, 2017
   12 minutes read

   [10]gold blog, july 2017 i know it   s a weird way to start a blog with a
   negative, but there was a wave of discussion in the last few days that
   i think serves as a good hook for some topics on which i   ve been
   thinking recently. it all started with a [11]post in the simply stats
   blog by jeff leek on the caveats of using deep learning in the small
   sample size regime. in sum, he argues that when the sample size is
   small (which happens a lot in the bio domain), linear models with few
   parameters perform better than deep nets even with a modicum of layers
   and hidden units. he goes on to show that a very simple linear
   predictor, with top ten most informative features, performs better than
   a simple deep net when trying to classify zeros and ones in the mnist
   dataset using only 80 or so samples. this prompted andrew beam to write
   a [12]rebuttal in which a properly trained deep net was able to beat
   the simple linear model, even with very few training samples. this
   back-and-forth comes at a time where more and more researchers in
   biomedical informatics are adopting deep learning for various problems.
   is the hype real or are linear models really all we need? the answer,
   as alwyas, is that it depends. in this post, i want to visit use cases
   in machine learning where using deep learning does not really make
   sense as well as tackle preconceptions that i think prevent deep
   learning to be used effectively, especially for newcomers.

breaking deep learning preconceptions

   first, let   s tackle some preconceptions that i perceive most folks
   outside the field have that turn out to be half-truths. there   s two
   broad ones and one a bit more technical that i   m going to elaborate on.
   this is somewhat of an extension to andrew beam   s excellent
      misconceptions    section in his [13]post.

deep learning can really work on small sample sizes

   deep learning   s claim to fame was in a context with lots of data
   (remember that the first google brain project was feeding lots of
   youtube videos to a deep net), and ever since it has constantly been
   publicized as complex algorithms running in lots of data.
   unfortunately, this big data/deep learning pair somehow translated into
   the converse as well: the myth that it cannot be used in the small
   sample regime. if you have just a few samples, tapping into a neural
   net with a high parameter-per-sample ratio may superficially seem like
   a sure road to overfitting. however, just considering sample size and
   dimensionality for a given problem, be it supervised or unsupervised,
   is sort of modeling the data in a vacuum, without any context. it is
   probably the case that you have data sources that are related to your
   problem, or that there   s a strong prior that a domain expert can
   provide, or that the data is structured in a very particular way (e.g.
   is encoded in a graph or image). in all of these cases, there   s a
   chance deep learning can make sense as a method of choice     for
   example, you can encode useful representations of bigger, related
   datasets and use those representations in your problem. a classic
   illustration of this is common in natural language processing, where
   you can learn id27s on a large corpus like wikipedia and then
   use those as embeddings in a smaller, narrower corpus for a supervised
   task. in the extreme, you can have a set of neural nets jointly learn a
   representation and an effective way to reuse the representation in
   small sets of samples. this is called id62 and has been
   successfully applied in a number of fields with high-dimensional data
   including [14]id161 and [15]drug discovery.

   id62 networks for drug discovery, taken from altae-tran et
   al. acs cent. sci. 2017

deep learning is not the answer to everything

   the second preconception i hear the most is the hype. many yet-to-be
   practitioners expect deep nets to give them a mythical performance
   boost just because it worked in other fields. others are inspired by
   impressive work in modeling and manipulating images, music, and
   language     three data types close to any human heart     and rush
   headfirst into the field by trying to train the latest gan
   architecture. the hype is real in many ways. deep learning has become
   an undeniable force in machine learning and an important tool in the
   arsenal of any data modeler. it   s popularity has brought forth
   essential frameworks such as tensorflow and pytorch that are incredibly
   useful even outside deep learning. it   s underdog to superstar origin
   story has inspired researchers to revisit other previously obscure
   methods like evolutionary strategies and id23. but
   it   s not a panacea by any means. aside from [16]no-free-lunch
   considerations, deep learning models can be very nuanced and require
   careful and sometimes very expensive hyperparameter searches, tuning,
   and testing (much more on this later in the post). besides, there are
   many cases where using deep learning just doesn   t make sense from a
   practical perspective and simpler models work much better.

deep learning is more than .fit()

   there is also an aspect of deep learning models that i see gets sort of
   lost in translation when coming from other fields of machine learning.
   most tutorials and introductory material to deep learning describe
   these models as composed by hierarchically-connected layers of nodes
   where the first layer is the input and the last layer is the output and
   that you can train them using some form of stochastic id119.
   after maybe some brief mentions on how stochastic id119
   works and what id26 is, the bulk of the explanation focuses
   on the rich landscape of neural network types (convolutional,
   recurrent, etc.). the optimization methods themselves receive little
   additional attention, which is unfortunate since it   s likely that a big
   (if not the biggest) part of why deep learning works is because of
   those particular methods (check out, e.g. [17]this post from ferenc
   husz  r   s and [18]this paper taken from that post), and knowing how to
   optimize their parameters and how to partition data to use them
   effectively is crucial to get good convergence in a reasonable amount
   of time. exactly why stochastic gradients matter so much is still
   unknown, but some clues are emerging here and there. one of my
   favorites is the interpretation of the methods as part of performing
   bayesian id136. in essence, every time that you do some form of
   numerical optimization, you   re performing some bayesian id136 with
   particular assumptions and priors. indeed, there   s a whole field,
   called [19]probabilistic numerics, that has emerged from taking this
   view. stochastic id119 is no different, and [20]recent work
   suggests that the procedure is really a markov chain that, under
   certain assumptions, has a stationary distribution that can be seen as
   a sort of variational approximation to the posterior. so when you stop
   your sgd and take the final parameters, you   re basically sampling from
   this approximate distribution. i found this idea to be illuminating,
   because the optimizer   s parameters (in this case, the learning rate)
   make so much more sense that way. as an example, as you increase the
   learning parameter of sgd the markov chain becomes unstable until it
   finds wide local minima that samples a large area; that is, you
   increase the variance of procedure. on the other hand, if you decrease
   the learning parameter, the markov chain slowly approximates narrower
   minima until it converges in a tight region; that is, you increase the
   bias for a certain region. another parameter, the batch size in sgd,
   also controls what type of region the algorithm converges two: wider
   regions for small batches and sharper regions with larger batches.

   sgd prefers wide or sharp minima depending on its learning rate or
   batch size

   this complexity means that optimizers of deep nets become first class
   citizens: they are a very central part of the model, every bit as
   important as the layer architecture. this doesn   t quite happen with
   many other models in machine learning. linear models (even regularized
   ones, like the lasso) and id166s are id76 problems for
   which there is not as much nuance and really only one answer. that   s
   why folks that come from other fields and/or using tools like
   scikit-learn are puzzled when they don   t find a very simple api with a
   .fit() method (although there are some tools, like skflow, that attempt
   to bottle simple nets into a .fit() signature, i think it   s a bit
   misguided since the whole point of deep learning is its flexibility).

when not to use deep learning

   so, when is deep learning not ideal for a task? from my perspective,
   these are the main scenarios where deep learning is more of a
   hinderance than a boon.

low-budget or low-commitment problems

   deep nets are very flexible models, with a multitude of architecture
   and node types, optimizers, and id173 strategies. depending on
   the application, your model might have convolutional layers (how wide?
   with what pooling operation?) or recurrent structure (with or without
   gating?); it might be really deep (hourglass, siamese, or other of the
   many architectures?) or with just a few hidden layers (with how many
   units?); it might use rectifying linear units or other activation
   functions; it might or might not have dropout (in what layers? with
   what fraction?) and the weights should probably be regularized (l1, l2,
   or something weirder?). this is only a partial list, there are lots of
   other types of nodes, connections, and even id168s out there to
   try. those are a lot of hyperparameters to tweak and architectures to
   explore while even training one instance of large networks can be very
   time consuming. google recently boasted that its automl pipeline can
   automatically find the best architecture, which is very impressive, but
   still requires more than 800 gpus churning full time for weeks,
   something out of reach for almost anyone else. the point is that
   training deep nets carries a big cost, in both computational and
   debugging time. such expense doesn   t make sense for lots of day-to-day
   prediction problems and the roi of tweaking a deep net to them, even
   when tweaking small networks, might be too low. even when there   s
   plenty of budget and commitment, there   s no reason not to try
   alternative methods first even as a baseline. you might be pleasantly
   surprised that a linear id166 is really all you needed.

interpreting and communicating model parameters/feature importances to a
general audience

   deep nets are also notorious for being black boxes with high predictive
   power but low interpretability. even though there   s been a lot of
   recent tools like saliency maps and [21]activation differences that
   work great for some domains, they don   t transfer completely to all
   applications. mainly, these tools work well when you want to make sure
   that the network is not deceiving you by memorizing the dataset or
   focusing on particular features that are spurious, but it is still
   difficult to interpret per-feature importances to the overall decision
   of the deep net. in this realm, nothing really beats linear models
   since the learned coefficients have a direct relationship to the
   response. this is especially crucial when communicating these
   interpretations to general audiences that need to make decisions based
   on them. physicians for example need to incorporate all sorts of
   disparate data to elicit a diagnosis. the simpler and more direct
   relationship between a variable and an outcome, the better a physician
   will leverage it and not under/over-estimate it   s value. further, there
   are cases where the accuracy of the model (typically where deep
   learning excels at) is not as important as interpretability. for
   example, a policy maker might want to know the effect some demographic
   variable has on e.g. mortality, and will likely be more interested in a
   direct approximation of this relationship than in the accuracy of the
   prediction. in both of these cases, deep learning is at a disadvantage
   compared to simpler, more penetrable methods.

establishing causal mechanisms

   the extreme case of model interpretability is when we are trying to
   establish a mechanistic model, that is, a model that actually captures
   the phenomena behind the data. good examples include trying to guess
   whether two molecules (e.g. drugs, proteins, nucleic acids, etc.)
   interact in a particular cellular environment or hypothesizing if a
   particular marketing strategy is having an actual effect on sales.
   nothing really beats old-style bayesian methods informed by expert
   opinion in this realm; they are our best (if imperfect) way we have to
   represent and infer causality. vicarious has some [22]nice recent work
   illustrating why this more principled approach generalizes better than
   deep learning in videogame tasks.

learning from    unstructured    features

   this one might be up for debate. i find that one area in which deep
   learning excels at is finding useful representations of the data for a
   particular task. a very good illustration of this is the aforementioned
   id27s. natural language has a rich and complex structure that
   can be approximated with    context-aware    networks: each word can be
   represented in a vector that encodes the context in which it is mostly
   used. using id27s learned in large corpora for nlp tasks can
   sometimes provide a boost in a particular task on another corpus.
   however, it might not be of any use if the corpus in question is
   completely unstructured. for example, say you are trying to classify
   objects by looking at unstructured lists of keywords. since the
   keywords are not used in any particular structure (like in a sentence),
   it   s unlikely that id27s will help all that much. in this
   case, the data is truly a bag of words and such representations are
   likely sufficient for the task. a counter-argument to this might be
   that id27s are not really that expensive if you use
   pretrained ones and may capture keyword similarity better. however, i
   still would prefer to start with the bag of words representation and
   see if i can get good predictions. after all, each dimension of the bag
   of words is easier to interpret than the corresponding id27
   slot.

the future is deep

   the deep learning field is hot, well-funded, and moves crazy fast. by
   the time you read a paper published in a conference, it   s likley there
   are two or three iterations on it that already deprecate it. this
   brings a big caveat to the points i   ve made above: deep learning might
   still be super useful for these scenarios in the near future. tools for
   interpretation of deep learning models for images and discrete
   sequences are getting better. recent software such as [23]edward marry
   bayesian modeling and deep net frameworks, allowing for quantification
   of uncertainty of neural network parameters and easy bayesian id136
   via probabilistic programming and automated variational id136. in
   the longer term, there might be a reduced modeling vocabulary that
   nails the salient properties that a deep net can have and thus reduce
   the parameter space of stuff that needs to be tried. so keep refreshing
   your arxiv feed, this post might be deprecated in a month or two.

   edward marries probabilistic programming with tensorflow, allowing for
   models that are both deep and bayesian. taken from tran et al. iclr
   2017

   [24]back to posts
   please enable javascript to view the [25]comments powered by disqus.
   [26]comments powered by disqus

references

   visible links
   1. http://hyperparameter.space/
   2. http://hyperparameter.space/blog/
   3. http://hyperparameter.space/about/
   4. http://github.com/dimenwarper
   5. http://twitter.com/tsuname
   6. http://linkedin.com/in/sergio-pablo-cordero/
   7. http://scholar.google.com/citations?user=ngethnyaaaaj&hl=en
   8. mailto:dimenwarper@gmail.com
   9. http://hyperparameter.space/index.xml
  10. http://www.kdnuggets.com/2017/07/top-news-week-0724-0730.html
  11. https://simplystatistics.org/2017/05/31/deeplearning-vs-leekasso/
  12. http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html
  13. http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html
  14. https://arxiv.org/abs/1606.04080
  15. https://arxiv.org/abs/1611.03199
  16. https://en.wikipedia.org/wiki/no_free_lunch_theorem
  17. http://www.id136.vc/everything-that-works-works-because-its-bayesian-2/
  18. https://arxiv.org/abs/1609.04836
  19. http://probabilistic-numerics.org/
  20. https://arxiv.org/pdf/1704.04289.pdf
  21. https://arxiv.org/abs/1704.02685
  22. https://www.vicarious.com/img/icml2017-schemas.pdf
  23. https://arxiv.org/abs/1701.03757
  24. http://hyperparameter.space/blog/
  25. http://disqus.com/?ref_noscript
  26. http://disqus.com/

   hidden links:
  28. http://hyperparameter.space/
  29. http://hyperparameter.space/
