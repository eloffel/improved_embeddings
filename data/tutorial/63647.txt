   (button)
     * [1]home
     * [2]explore nlvr

   [3]nlvr

                                    nlvr

natural language for visual reasoning

cornell natural language for visual reasoning

   the natural language for visual reasoning corpora are two language
   grounding datasets containing natural language sentences grounded in
   images. the task is to determine whether a sentence is true about a
   visual input. the data was collected through id104s, and
   solving the task requires reasoning about sets of objects, comparisons,
   and spatial relations. this includes two corpora: nlvr, with
   synthetically generated images, and nlvr^2, which includes natural
   photographs.
     __________________________________________________________________

natural language for visual reasoning for real

   new nlvr^2 contains 107,292 examples of human-written english sentences
   grounded in pairs of photographs. nlvr^2 retains the linguistic
   diversity of nlvr, while including much more visually complex images.

                     [4]data [5]paper (suhr et al. 2018)
                     (button) show examples from nlvr^2

   [ex0_0.jpg] [ex0_1.jpg]

    the left image contains twice the number of dogs as the right image,
                and at least two dogs in total are standing.
                                    true

   [acorns_1.jpg] [acorns_6.jpg]

   one image shows exactly two brown acorns in back-to-back caps on green
                                  foliage.
                                    false

   image credit (in order left-right, top-bottom): memorycatcher (cc0),
   calabash13 (cc by-sa 3.0), charles rondeau (cc0), andale (cc0).

   we only publicly release the sentence annotations and original image
   urls, and scripts that download the images from the urls. if you would
   like direct access to the images, please fill out this [6]google form.
   this form asks for your basic information and asks you to agree to our
   terms of service. we will get back to you within a week.
     __________________________________________________________________

natural language for visual reasoning

   nlvr contains 92,244 pairs of human-written english sentences grounded
   in synthetic images. because the images are synthically generated, this
   dataset can be used for id29.

                     [7]data [8]paper (suhr et al. 2017)
                      (button) show examples from nlvr

   [ex0.png]

          there is exactly one black triangle not touching any edge
                                    true

   [sup_ex4.png]

   there is at least one tower with four blocks with a yellow block at the
                  base and a blue block below the top block
                                    true

   [sup_ex6.png]

    there is a box with multiple items and only one item has a different
                                   color.
                                    false

   [sup_ex10.png]

     there is exactly one tower with a blue block at the base and yellow
                            block at the topfalse

       more examples (from the development set) are available [9]here.
     __________________________________________________________________

leaderboards

   both nlvr and nlvr^2 are split into training, development, and two test
   sets. one test set is public (test-p) and available with the data, and
   the other is not released (test-u). we maintain a leaderboard
   displaying accuracy and consistency on the unreleased test set (as well
   as accuracy on the development and public test sets). results are
   ordered by accuracy on the unreleased test set; ties are broken with
   consistency.

   we require two months or more between runs on each leaderboard test
   set. we will do our best to run within two weeks (usually we will run
   much faster). we will only post results on the leaderboard when an
   online description of the system is available. testing on the
   leaderboard test set is meant to be the final step before publication.
   under extreme circumstances, we reserve the right to limit running on
   the leaderboard test set to systems that are mature for publication.
   your model should generate a prediction file in the format specified in
   the nlvr readme and run with the provided evaluation scripts. you can
   request to add your model to the leaderboard even if you don't evaluate
   on the unreleased test set.

   for both datasets, we use two id74: accuracy and
   consistency. accuracy (acc) is computed as the proportion of examples
   (sentence-image pairs) for which a model correctly predicted a truth
   value. consistency (cons) measures the generalization of a model. it is
   computed as the proportion of unique sentences for which a model
   correctly predicted the truth value for all paired images [10](goldman
   et al., 2018).
     __________________________________________________________________

questions?

   please visit our [11]github issues page or email us at

                        nlvr < at > googlegroups.com

   please email us if you wish to run on an unreleased test set. to keep
   up to date with major changes, please subscribe:
   ____________________
   ____________________
   subscribe
     __________________________________________________________________

acknowledgments

   this research was supported by the nsf (crii-1656998), a facebook
   parlai research award, an ai2 key scientific challenges award, amazon
   cloud credits grant, and support from women in technology new york.
   this material is based on work supported by the national science
   foundation graduate research fellowship under grant no. dge-1650441. we
   thank mark yatskar and noah snavely for their comments and suggestions,
   and the workers who participated in our data collection for their
   contributions.

   also thanks to [12]squad for allowing us to use their code to create
   this website!

nlvr^2 leaderboard

   nlvr^2 presents the task of determining whether a natural language
   sentence is true about a pair of photographs.
   rank model dev. (acc) test-p (acc) test-u (acc) test-u (cons)
   human performance

   cornell university
   [13](suhr et al. 2018) 96.2 96.3 96.1 -

   1
   nov 1, 2018 maxent

   cornell university
   [14](suhr et al. 2018) 54.1 54.8 53.5 12.0

   2
   nov 1, 2018 id98+id56

   cornell university
   [15](suhr et al. 2018) 53.4 52.4 53.2 11.2

   3
   nov 1, 2018 film

   mila, ran by cornell university
   [16](perez et al. 2018) 51.0 52.1 53.0 10.6

   4
   nov 1, 2018 image only (id98)

   cornell university
   [17](suhr et al. 2018) 51.6 51.9 51.9 7.1

   5
   nov 1, 2018 n2nmn, policy search from scratch

   uc berkeley, ran by cornell university
   [18](hu et al. 2017) 51.0 51.1 51.5 5.0

   6
   nov 1, 2018 majority class

   cornell university
   [19](suhr et al. 2018) 50.9 51.1 51.4 4.6

   6
   nov 1, 2018 text only (id56)

   cornell university
   [20](suhr et al. 2018) 50.9 51.1 51.4 4.6^*

   8
   nov 1, 2018 mac-network

   stanford university, ran by cornell university
   [21](hudson and manning 2018) 50.8 51.4 51.2 11.2
   ^*the current arxiv version reports this as 4.8; this is a typo. it
   will be updated as soon as we can update the arxiv version.

nlvr leaderboard

   nlvr presents the task of determining whether a natural language
   sentence is true about a synthetically generated image. we divide
   results into whether they process the image pixels directly (images) or
   whether they use the structured representations of the images
   (structured representations).

images

   rank model dev. (acc) test-p (acc) test-u (acc) test-u (cons)
   human performance

   cornell university
   94.6 95.4 94.9 -

   1
   apr 20, 2018 id98-biatt

   unc chapel hill
   [22](tan and bansal 2018) 66.9 69.7 66.1 28.9

   2
   nov 1, 2018 n2nmn, policy search from scratch

   uc berkeley, ran by cornell university
   [23](hu et al. 2017) 65.3 69.1 66.0 17.7

   3
   apr 22, 2017 neural module networks

   uc berkeley, ran by cornell university
   [24](andreas et al. 2016) 63.1 66.1 62.0 -

   4
   nov 1, 2018 film

   mila, ran by cornell university
   [25](perez et al. 2018) 60.1 62.2 61.2 18.1

   5
   apr 22, 2017 majority class

   cornell university
   [26](suhr et al. 2017) 55.3 56.2 55.4 -

   6
   nov 1, 2018 mac-network

   stanford university, ran by cornell university
   [27](hudson and manning 2018) 55.4 57.6 54.3 8.6

   unranked
   sept 7, 2018 cmm

   chinese academy of sciences
   [28](yao et al. 2018) 68.0 69.9 - -

   unranked
   may 23, 2018 w-memnn

   federico santa mar  a technical university & pont  fica universidad
   cat  lica de valpara  so
   [29](pavez et al. 2018) 65.6 65.8 - -

structured representations

   rank model dev. (acc) test-p (acc) test-u (acc) test-u (cons)
   human performance

   cornell university
   94.6 95.4 94.9 -

   1
   nov 14, 2017 abstau

   tel-aviv university
   [30](goldman et al. 2017) 85.7 84.0 82.5 63.9

   2
   apr 4, 2018 biatt-pointer

   unc chapel hill
   [31](tan and bansal 2018) 74.6 73.9 71.8 37.2

   3
   apr 22, 2017 maxent

   cornell university
   [32](suhr et al. 2017) 68.0 67.7 67.8 -

   4
   apr 22, 2017 majority class

   cornell university
   [33](suhr et al. 2017) 55.3 56.2 55.4 -

     * [34]cornell language in context lab

references

   1. http://lic.nlp.cornell.edu/nlvr/index.html
   2. http://lic.nlp.cornell.edu/nlvr/nlvr.html
   3. http://lic.nlp.cornell.edu/nlvr/index.html
   4. https://github.com/clic-lab/nlvr/tree/master/nlvr2
   5. https://arxiv.org/abs/1811.00491
   6. https://goo.gl/forms/ys29stwnfwzrdbfh3
   7. https://github.com/clic-lab/nlvr/tree/master/nlvr
   8. https://yoavartzi.com/pub/slya-acl.2017.pdf
   9. http://lic.nlp.cornell.edu/nlvr/examples.html
  10. https://arxiv.org/abs/1711.05240
  11. https://github.com/clic-lab/nlvr/issues
  12. https://rajpurkar.github.io/squad-explorer/
  13. https://arxiv.org/abs/1811.00491
  14. https://arxiv.org/abs/1811.00491
  15. https://arxiv.org/abs/1811.00491
  16. https://arxiv.org/pdf/1709.07871.pdf
  17. https://arxiv.org/abs/1811.00491
  18. https://arxiv.org/pdf/1704.05526.pdf
  19. https://arxiv.org/abs/1811.00491
  20. https://arxiv.org/abs/1811.00491
  21. https://arxiv.org/abs/1803.03067
  22. https://arxiv.org/pdf/1804.06870.pdf
  23. https://arxiv.org/pdf/1704.05526.pdf
  24. https://arxiv.org/pdf/1511.02799.pdf
  25. https://arxiv.org/pdf/1709.07871.pdf
  26. http://alanesuhr.com/suhr2017.pdf
  27. https://arxiv.org/abs/1803.03067
  28. http://aclweb.org/anthology/d18-1118
  29. https://arxiv.org/pdf/1805.09354.pdf
  30. https://arxiv.org/abs/1711.05240
  31. https://arxiv.org/pdf/1804.06870.pdf
  32. http://alanesuhr.com/suhr2017.pdf
  33. http://alanesuhr.com/suhr2017.pdf
  34. https://github.com/clic-lab
