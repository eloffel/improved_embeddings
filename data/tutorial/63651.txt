   #[1]flair of machine learning - a virtual proof that name is awesome!

   (button) toggle navigation [2]flair of machine learning
     * [3]about me
     * [4]open learning
     * [5]projects

   [6][avatar.jpg]

understanding the backward pass through batch id172 layer

   posted on february 12, 2016

   at the moment there is a wonderful course running at standford
   university, called [7]cs231n - convolutional neural networks for visual
   recognition, held by andrej karpathy, justin johnson and fei-fei li.
   fortunately all the [8]course material is provided for free and all the
   lectures are recorded and uploaded on [9]youtube. this class gives a
   wonderful intro to machine learning/deep learning coming along with
   programming assignments.

batch id172

   one topic, which kept me quite busy for some time was the
   implementation of [10]batch id172, especially the backward
   pass. batch id172 is a technique to provide any layer in a
   neural network with inputs that are zero mean/unit variance - and this
   is basically what they like! but batchnorm consists of one more step
   which makes this algorithm really powerful. let   s take a look at the
   batchnorm algorithm:
   [bn_algorithm.png]
   algorithm of batch id172 copied from the paper by ioffe and
   szegedy mentioned above.

   look at the last line of the algorithm. after normalizing the input x
   the result is squashed through a linear function with parameters gamma
   and beta. these are learnable parameters of the batchnorm layer and
   make it basically possible to say    hey!! i don   t want zero mean/unit
   variance input, give me back the raw input - it   s better for me.    if
   gamma = sqrt(var(x)) and beta = mean(x), the original activation is
   restored. this is, what makes batchnorm really powerful. we initialize
   the batchnorm parameters to transform the input to zero mean/unit
   variance distributions but during training they can learn that any
   other distribution might be better. anyway, i don   t want to spend to
   much time on explaining batch id172. if you want to learn more
   about it, the [11]paper is very well written and [12]here andrej is
   explaining batchnorm in class.

   btw: it   s called    batch    id172 because we perform this
   transformation and calculate the statistics only for a subpart (a
   batch) of the entire trainingsset.

id26

   in this blog post i don   t want to give a lecture in id26 and
   stochastic id119 (sgd). for now i will assume that whoever
   will read this post, has some basic understanding of these principles.
   for the rest, let me quote wiki:

     id26, an abbreviation for    backward propagation of
     errors   , is a common method of training id158s
     used in conjunction with an optimization method such as gradient
     descent. the method calculates the gradient of a id168 with
     respect to all the weights in the network. the gradient is fed to
     the optimization method which in turn uses it to update the weights,
     in an attempt to minimize the id168.

   uff, sounds tough, eh? i will maybe write another post about this topic
   but for now i want to focus on the concrete example of the backwardpass
   through the batchnorm-layer.

computational graph of batch id172 layer

   i think one of the things i learned from the cs231n class that helped
   me most understanding id26 was the explanation through
   computational graphs. these graphs are a good way to visualize the
   computational flow of fairly complex functions by small, piecewise
   differentiable subfunctions. for the batchnorm-layer it would look
   something like this:
   [bncircuit.png]
   computational graph of the batchnorm-layer. from left to right,
   following the black arrows flows the forward pass. the inputs are a
   matrix x and gamma and beta as vectors. from right to left, following
   the red arrows flows the backward pass which distributes the gradient
   from above layer to gamma and beta and all the way back to the input.

   i think for all, who followed the course or who know the technique the
   forwardpass (black arrows) is easy and straightforward to read. from
   input x we calculate the mean of every dimension in the feature space
   and then subtract this vector of mean values from every training
   example. with this done, following the lower branch, we calculate the
   per-dimension variance and with that the entire denominator of the
   id172 equation. next we invert it and multiply it with
   difference of inputs and means and we have x_normalized. the last two
   blobs on the right perform the squashing by multiplying with the input
   gamma and finally adding beta. et voil  , we have our batch-normalized
   output.

   a vanilla implementation of the forwardpass might look like this:
def batchnorm_forward(x, gamma, beta, eps):

  n, d = x.shape

  #step1: calculate mean
  mu = 1./n * np.sum(x, axis = 0)

  #step2: subtract mean vector of every trainings example
  xmu = x - mu

  #step3: following the lower branch - calculation denominator
  sq = xmu ** 2

  #step4: calculate variance
  var = 1./n * np.sum(sq, axis = 0)

  #step5: add eps for numerical stability, then sqrt
  sqrtvar = np.sqrt(var + eps)

  #step6: invert sqrtwar
  ivar = 1./sqrtvar

  #step7: execute id172
  xhat = xmu * ivar

  #step8: nor the two transformation steps
  gammax = gamma * xhat

  #step9
  out = gammax + beta

  #store intermediate
  cache = (xhat,gamma,xmu,ivar,sqrtvar,var,eps)

  return out, cache

   note that for the exercise of the cs231n class we had to do a little
   more (calculate running mean and variance as well as implement
   different forward pass for trainings mode and test mode) but for the
   explanation of the backwardpass this piece of code will work. in the
   cache variable we store some stuff that we need for the computing of
   the backwardpass, as you will see now!

the power of chain rule for id26

   for all who kept on reading until now (congratulations!!), we are close
   to arrive at the backward pass of the batchnorm-layer. to fully
   understand the channeling of the gradient backwards through the
   batchnorm-layer you should have some basic understanding of what the
   [13]chain rule is. as a little refresh follows one figure that
   exemplifies the use of chain rule for the backward pass in
   computational graphs.
   [chainrule_example.png]
   the forwardpass on the left in calculates `z` as a function `f(x,y)`
   using the input variables `x` and `y` (this could literally be any
   function, examples are shown in the batchnorm-graph above). the right
   side of the figures shows the backwardpass. receiving `dl/dz`, the
   gradient of the id168 with respect to `z` from above, the
   gradients of `x` and `y` on the id168 can be calculate by
   applying the chain rule, as shown in the figure.

   so again, we only have to multiply the local gradient of the function
   with the gradient of above to channel the gradient backwards. some
   derivatives of some basic functions are listed in the [14]course
   material. if you understand that, and with some more basic knowledge in
   calculus, what will follow is a piece of cake!

finally: the backpass of the batch id172

   in the comments of aboves code snippet i already numbered the
   computational steps by consecutive numbers. the id26 follows
   these steps in reverse order, as we are literally backpassing through
   the computational graph. we will know take a more detailed look at
   every single computation of the backwardpass and by that deriving step
   by step a naive algorithm for the backward pass.

step 9

   [step9.png]
   backwardpass through the last summation gate of the batchnorm-layer.
   enclosed in brackets i put the dimensions of input/output

   recall that the derivative of a function f = x + y with respect to any
   of these two variables is 1. this means to channel a gradient through a
   summation gate, we only need to multiply by 1. for our final loss
   evaluation, we sum the gradient of all samples in the batch. through
   this operation, we also get a vector of gradients with the correct
   shape for beta. so after the first step of id26 we already
   got the gradient for one learnable parameter: beta

step 8

   [step8.png]
   next follows the backward pass through the multiplication gate of the
   normalized input and the vector of gamma.

   for any function f = x * y the derivative with respect to one of the
   inputs is simply just the other input variable. this also means, that
   for this step of the backward pass we need the variables used in the
   forward pass of this gate (luckily stored in the cache of aboves
   function). so again we get the gradients of the two inputs of these
   gates by applying chain rule ( = multiplying the local gradient with
   the gradient from above). for gamma, as for beta in step 9, we need to
   sum up the gradients over dimension n. so we now have the gradient for
   the second learnable parameter of the batchnorm-layer gamma and    only   
   need to backprop the gradient to the input x, so that we then can
   backpropagate the gradient to any layer further downwards.

step 7

   [step7.png]
   this step during the forward pass was the final step of the
   id172 combining the two branches (nominator and denominator) of
   the computational graph. during the backward pass we will calculate the
   gradients that will flow separately through these two branches
   backwards.

   it   s basically the exact same operation, so lets not waste much time
   and continue. the two needed variables xmu and ivar for this step are
   also stored cache variable we pass to the backprop function. (and
   again: this is one of the main advantages of computational graphs.
   splitting complex functions into a handful of simple basic operations.
   and like this you have a lot of repetitions!)

step 6

   [step6.png]
   this is a "one input-one output" node where, during the forward pass,
   we inverted the input (square root of the variance).

   the local gradient is visualized in the image and should not be hard to
   derive by hand. multiplied by the gradient from above is what we
   channel to the next step. sqrtvar is also one of the variables passed
   in cache.

step 5

   [step5.png]
   again "one input-one output". this node calculates during the forward
   pass the denominator of the id172.

   the calculation of the derivative of the local gradient is little magic
   and should need no explanation. var and eps are also passed in the
   cache. no more words to lose!

step 4

   [step4.png]
   also a "one input-one output" node. during the forward pass the output
   of this node is the variance of each feature `d for d in [1...d]`.

   the calculation of the derivative of this steps local gradient might
   look unclear at the very first glance. but it   s not that hard at the
   end. let   s recall that a normal summation gate (see step 9) during the
   backward pass only transfers the gradient unchanged and evenly to the
   inputs. with that in mind, it should not be that hard to conclude, that
   a column-wise summation during the forward pass, during the backward
   pass means that we evenly distribute the gradient over all rows for
   each column. and not much more is done here. we create a matrix of ones
   with the same shape as the input sq of the forward pass, divide it
   element-wise by the number of rows (thats the local gradient) and
   multiply it by the gradient from above.

step 3

   [step3.png]
   this node outputs the square of its input, which during the forward
   pass was a matrix containing the input `x` subtracted by the
   per-feature `mean`.

   i think for all who followed until here, there is not much to explain
   regarding the derivative of the local gradient.

step 2

   [step2.png]
   now this looks like a more fun gate! two inputs-two outputs! this node
   subtracts the per-feature mean row-wise of each trainings example `n
   for n in [1...n]` during the forward pass.

   okay lets see. one of the definitions of backprogatation and
   computational graphs is, that whenever we have two gradients coming to
   one node, we simply add them up. knowing this, the rest is little magic
   as the local gradient for a subtraction is as hard to derive as for a
   summation. note that for mu we have to sum up the gradients over the
   dimension n (as we did before for gamma and beta).

step 1

   [step1.png]
   the function of this node is exactly the same as of step 4. only that
   during the forward pass the input was `x` - the input to the
   batchnorm-layer and the output here is `mu`, a vector that contains the
   mean of each feature.

   as this node executes the exact same operation as the one explained in
   step 4, also the id26 of the gradient looks the same. so
   let   s continue to the last step.

step 0 - arriving at the input

   [step0.png]

   i only added this image to again visualize that at the very end we need
   to sum up the gradients dx1 and dx2 to get the final gradient dx. this
   matrix contains the gradient of the id168 with respect to the
   input of the batchnorm-layer. this gradient dx is also what we give as
   input to the backwardpass of the next layer, as for this layer we
   receive dout from the layer above.

naive implemantation of the backward pass through the batchnorm-layer

   putting together every single step the naive implementation of the
   backwardpass might look something like this:
def batchnorm_backward(dout, cache):

  #unfold the variables stored in cache
  xhat,gamma,xmu,ivar,sqrtvar,var,eps = cache

  #get the dimensions of the input/output
  n,d = dout.shape

  #step9
  dbeta = np.sum(dout, axis=0)
  dgammax = dout #not necessary, but more understandable

  #step8
  dgamma = np.sum(dgammax*xhat, axis=0)
  dxhat = dgammax * gamma

  #step7
  divar = np.sum(dxhat*xmu, axis=0)
  dxmu1 = dxhat * ivar

  #step6
  dsqrtvar = -1. /(sqrtvar**2) * divar

  #step5
  dvar = 0.5 * 1. /np.sqrt(var+eps) * dsqrtvar

  #step4
  dsq = 1. /n * np.ones((n,d)) * dvar

  #step3
  dxmu2 = 2 * xmu * dsq

  #step2
  dx1 = (dxmu1 + dxmu2)
  dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)

  #step1
  dx2 = 1. /n * np.ones((n,d)) * dmu

  #step0
  dx = dx1 + dx2

  return dx, dgamma, dbeta


   note: this is the naive implementation of the backward pass. there
   exists an alternative implementation, which is even a bit faster, but i
   personally found the naive implementation way better for the purpose of
   understanding id26 through the batchnorm-layer. [15]this
   well written blog post gives a more detailed derivation of the
   alternative (faster) implementation. however, there is a much more
   calculus involved. but once you have understood the naive
   implementation, it might not be to hard to follow.

some final words

   first of all i would like to thank the team of the cs231n class, that
   gratefully make all the material freely available. this gives people
   like me the possibility to take part in high class courses and learn a
   lot about deep learning in self-study. (secondly it made me motivated
   to write my first blog post!)

   and as we have already passed the deadline for the second assignment, i
   might upload my code during the next days on github.

   share: [16]twitter [17]facebook [18]linkedin
     * [19]next post    

   please enable javascript to view the [20]comments powered by disqus.

     * [21]github
     * [22]twitter
     * [23]email me
     * [24]linkedin
     * [25]rss

   frederik kratzert       2018       [26]kratzert.github.io

   theme by [27]beautiful-jekyll

references

   1. https://kratzert.github.io/feed.xml
   2. https://kratzert.github.io/
   3. https://kratzert.github.io/aboutme
   4. https://kratzert.github.io/openlearning
   5. https://kratzert.github.io/projects
   6. https://kratzert.github.io/
   7. http://cs231n.stanford.edu/
   8. http://cs231n.stanford.edu/syllabus.html
   9. https://www.youtube.com/playlist?list=plkt2usq6rbvctenovbg1tpcc7oqi31alc
  10. http://arxiv.org/abs/1502.03167
  11. http://arxiv.org/abs/1502.03167
  12. https://youtu.be/gypojmlgyxa?list=plkt2usq6rbvctenovbg1tpcc7oqi31alc&t=3078
  13. https://en.wikipedia.org/wiki/chain_rule
  14. http://cs231n.github.io/optimization-2/#sigmoid
  15. http://cthorey.github.io/id26/
  16. https://twitter.com/intent/tweet?text=understanding+the+backward+pass+through+batch+id172+layer+https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-id172-layer.html
  17. https://www.facebook.com/sharer/sharer.php?u=https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-id172-layer.html
  18. https://www.linkedin.com/sharearticle?mini=true&url=https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-id172-layer.html
  19. https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html
  20. http://disqus.com/?ref_noscript
  21. https://github.com/kratzert
  22. https://twitter.com/fkratzert
  23. mailto:f.kratzert@gmail.com
  24. https://linkedin.com/in/frederik-kratzert-226a33148
  25. https://kratzert.github.io/feed.xml
  26. https://kratzert.github.io/
  27. http://deanattali.com/beautiful-jekyll/
