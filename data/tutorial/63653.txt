   [1]burlap [2]home | [3]updates | [4]information | [5]f.a.q. |
   [6]tutorials | [7]java doc

          [blocksworldimage.png] [gwvf.png] [mountaincarimage.png]

about

   the brown-umbc id23 and planning (burlap) java code
   library is for the use and development of single or multi-agent
   planning and learning algorithms and domains to accompany them. burlap
   uses a highly flexible system for defining states and and actions of
   nearly any kind of form, supporting discrete continuous, and relational
   domains. planning and learning algorithms range from classic forward
   search planning to value function-based stochastic planning and
   learning algorithms. also included is a set of analysis tools such as a
   common framework for the visualization of domains and agent performance
   in various domains.

   burlap is licensed under the permissive [8]apache 2.0 license.

   for more background information on the project and the people involved,
   see the [9]information page.

where to git it

   buralp uses maven and is available on maven central! that means that if
   you'd like to create a project that uses burlap, all you need to do is
   add the following dependency to the <dependencies> section of your
   projects pom.xml

<dependency>
  <groupid>edu.brown.cs.burlap</groupid>
  <artifactid>burlap</artifactid>
  <version>3.0.0</version>
</dependency>

   and the library will automatically be downloaded and linked to your
   project! if you do not have maven installed, you can get it from
   [10]here.

   you can also get the full burlap source to manually compile/modify from
   github at:
   [11]https://github.com/jmacglashan/burlap

   alternatively, you can directly download precompiled jars from maven
   central from [12]here. use the jar-with-dependencies if you want all
   dependencies included.

   prior versions of burlap are also available on maven central, and
   branches on github.

tutorials and example code

   short video tutorials, longer text tutorials, and example code are
   available for burlap. all code can be found in our examples repository,
   which also provides the kind of pom file and file sturcture you should
   consider using for a burlap project. the example repository can be
   found at:

   [13]https://github.com/jmacglashan/burlap_examples/

video tutorials

   iframe: [14]https://www.youtube.com/embed/nmgemwt-288

written tutorials

     * [15]hello gridworld! - a tutorial on acquiring and linking with
       burlap
     * [16]building a domain
     * [17]using basic planning and learning algorithms
     * [18]creating a planning and learning algorithm
     * [19]solving continuous domains
     * [20]building an oo-mdp domain

documentation

   java documentation is provided for all of the source code in burlap.
   you can find an online copy of the javadoc at the below location.

   [21]http://burlap.cs.brown.edu/doc/index.html

features

current

     * a highly felixible state representation in which you define states
       with regular java code and only need to implement a short
       interface. this enables support for discrete, continuous,
       relational, or any other kind of state representation that you can
       code! burlap also has optional interfaces to provide first class
       support for the rich oo-mdp state representation [1].
     * supported problem formalisms
          + id100 (single agent)
          + stochastic games (multi-agent)
          + partially observable id100 (single agent)
     * tools for visualizing and defining visualizations of states,
       episodes, value functions, and policies.
     * tools for setting up experiments with multiple learning algorithms
       and plotting the performance using multiple performance metrics.
     * an extendable shell framework for controlling experiments at
       runtime.
     * tools for creating multi-agent tournaments.
     * classic goal-directed deterministic forward-search planning.
          + breadth-first search
          + depth-first search
          + a*
          + ida*
          + statically weighted a* [2]
          + dynamically weighted a* [3]
     * stochastic planning.
          + value iteration [4]
          + policy iteration
          + prioritized sweeping [20]
          + real-time id145 [5]
          + uct [6]
          + sparse sampling [17]
          + bounded real-time id145 [21]
     * learning.
          + id24 [7]
          + sarsa(  ) [8]
          + actor critic [9]
          + potential shaped rmax [12]
          + artdp [5]
     * value function approximation
          + id119 sarsa(  ) [8]
          + least-squares policy iteration [18]
          + fitted value iteration [24]
          + framework for implementing linear and non-linear vfa
          + cmacs/tile coding [10]
          + radial basis functions
          + fourier basis functions [19]
     * the options framework [11] (supported in most single agent planning
       and learning algorithms).
     * reward shaping
     * inverse id23
          + maximum margin apprenticeship learning [16]
          + multiple intentions maximum-likelihood inverse reinforcement
            learning [22]
          + receding horizon inverse id23 [23]
     * multi-agent id24 and value iteration, supporting
          + id24 with an n-step action history memory
          + friend-q [13]
          + foe-q [13]
          + correlated-q [14]
          + coco-q [15]
     * single-agent partially observable planning algorithms
          + finite horizon optimal tree search
          + qmdp [25]
          + belief mdp conversion for use with standard mdp algorithms
     * pre-made domains and domain generators.
          + grid worlds
          + domains represented as graphs
          + blocks world
          + lunar lander
          + mountain car
          + cart pole
          + frostbite
          + blockdude
          + grid games (a multi-agent stochastic games domain)
          + multiple classic bimatrix games.
     * [22]rlglue agent and environment interfacing
     * [23]extensions for controlling [24]ros-powered robots
     * [25]extensions for controlling [26]minecraft

features in development

     * learning from human feedback algorithms
     * pomdp algorithms like pomcp and pbvi
     * general growth of all other algorithm classes already included

references

    1. diuk, c., cohen, a., and littman, m.l.. "an object-oriented
       representation for efficient id23." proceedings
       of the 25th international conference on machine learning (2008).
       240-270.
    2. pohl, ira. "first results on the effect of error in heuristic
       search". machine intelligence 5 (1970): 219-236.
    3. pohl, ira. "the avoidance of (relative) catastrophe, heuristic
       competence, genuine dynamic weighting and computational issues in
       heuristic problem solving (august, 1973)
    4. puterman, martin l., and moon chirl shin. "modified policy
       iteration algorithms for discounted markov decision problems."
       management science 24.11 (1978): 1127-1137.
    5. barto, andrew g., steven j. bradtke, and satinder p. singh.
       "learning to act using real-time id145." artificial
       intelligence 72.1 (1995): 81-138.
    6. kocsis, levente, and csaba szepesvari. "bandit based monte-carlo
       planning." ecml (2006). 282-293.
    7. watkins, christopher jch, and peter dayan. "id24." machine
       learning 8.3-4 (1992): 279-292.
    8. rummery, gavin a., and mahesan niranjan. on-line id24 using
       connectionist systems. university of cambridge, department of
       engineering, 1994.
    9. barto, andrew g., richard s. sutton, and charles w. anderson.
       "neuronlike adaptive elements that can solve difficult learning
       control problems." systems, man and cybernetics, ieee transactions
       on 5 (1983): 834-846.
   10. albus, james s. "a theory of cerebellar function." mathematical
       biosciences 10.1 (1971): 25-61.
   11. sutton, richard s., doina precup, and satinder singh. "between mdps
       and semi-mdps: a framework for temporal abstraction in
       id23." artificial intelligence 112.1 (1999):
       181-211.
   12. asmuth, john, michael l. littman, and robert zinkov.
       "potential-based shaping in model-based id23."
       aaai. 2008.
   13. littman, michael l. "markov games as a framework for multi-agent
       id23." icml. vol. 94. 1994.
   14. greenwald, amy, keith hall, and roberto serrano. "correlated
       id24." icml. vol. 3. 2003.
   15. sodomka, eric, hilliard, e., littman, m., & greenwald, a. "coco-q:
       learning in stochastic games with side payments." proceedings of
       the 30th international conference on machine learning (icml-13).
       2013.
   16. abbeel, pieter, and andrew y. ng. "apprenticeship learning via
       inverse id23." proceedings of the twenty-first
       international conference on machine learning. acm, 2004.
   17. kearns, michael, yishay mansour, and andrew y. ng. "a sparse
       sampling algorithm for near-optimal planning in large markov
       decision processes." machine learning 49.2-3 (2002): 193-208.
   18. lagoudakis, michail g., and ronald parr. "least-squares policy
       iteration." the journal of machine learning research 4 (2003):
       1107-1149
   19. g.d. konidaris, s. osentoski and p.s. thomas. value function
       approximation in id23 using the fourier basis. in
       proceedings of the twenty-fifth conference on artificial
       intelligence, pages 380-385, august 2011.
   20. li, lihong, michael l. littman, and l. littman. prioritized
       sweeping converges to the optimal value function. tech. rep.
       dcs-tr-631, 2008.
   21. mcmahan, h. brendan, maxim likhachev, and geoffrey j. gordon.
       "bounded real-time id145: rtdp with monotone upper
       bounds and performance guarantees." proceedings of the 22nd
       international conference on machine learning. acm, 2005.
   22. babes, monica, et al. "apprenticeship learning about multiple
       intentions." proceedings of the 28th international conference on
       machine learning (icml-11). 2011.
   23. macglashan, james and littman, micahel, "between imitation and
       intention learning," in proceedings of the international joint
       conference on artificial intelligence, 2015.
   24. gordon, geoffrey j. "stable function approximation in dynamic
       programming." proceedings of the twelfth international conference
       on machine learning. 1995.
   25. littman, m.l., cassandra, a.r., kaelbling, l.p., "learning policies
       for partially observable environments: scaling up," in proceedings
       of the 12th internaltion conference on machine learning. 1995.

references

   1. http://burlap.cs.brown.edu/index.html
   2. http://burlap.cs.brown.edu/index.html
   3. http://burlap.cs.brown.edu/updates.html
   4. http://burlap.cs.brown.edu/information.html
   5. http://burlap.cs.brown.edu/faq.html
   6. http://burlap.cs.brown.edu/tutorials/index.html
   7. http://burlap.cs.brown.edu/doc/index.html
   8. http://www.apache.org/licenses/license-2.0
   9. http://burlap.cs.brown.edu/information.html
  10. https://maven.apache.org/download.cgi
  11. https://github.com/jmacglashan/burlap
  12. http://search.maven.org/#search|ga|1|g:"edu.brown.cs.burlap" and a:"burlap"
  13. https://github.com/jmacglashan/burlap_examples/
  14. https://www.youtube.com/embed/nmgemwt-288
  15. http://burlap.cs.brown.edu/tutorials/hgw/p1.html
  16. http://burlap.cs.brown.edu/tutorials/bd/p1.html
  17. http://burlap.cs.brown.edu/tutorials/bpl/p1.html
  18. http://burlap.cs.brown.edu/tutorials/cpl/p1.html
  19. http://burlap.cs.brown.edu/tutorials/scd/p1.html
  20. http://burlap.cs.brown.edu/tutorials/oomdp/p1.html
  21. http://burlap.cs.brown.edu/doc/index.html
  22. http://glue.rl-community.org/wiki/main_page
  23. https://github.com/h2r/burlap_rosbridge
  24. http://www.ros.org/
  25. https://github.com/h2r/burlapcraft
  26. http://www.minecraft.net/
