   #[1]fewer lacunae    feed [2]fewer lacunae    comments feed [3]fewer
   lacunae    id100 comments feed [4]an introduction to
   markov chains [5]an introduction to prospect theory [6]alternate
   [7]alternate [8]fewer lacunae [9]wordpress.com

   [10]skip to content

[11]fewer lacunae

distilled, integrative research

primary menu

   (button) menu
     * [12]home
     * [13]cognitive science
     * [14]maths (incl. stats and compsci)
     * [15]other disciplines
     * [16]about me

id100

   [17]october 19, 2016may 9, 2017[18]kevinbinz

   part of: [19]id23 sequence
   followup to: [20]an introduction to markov chains
   content summary: 900 words, 9 min read

   motivations

   today, we turn our gaze to id100 (mdps), a
   decision-making environment which supports our propensity to learn from
   good and bad outcomes. we represent outcome desirability with a single
   number, r. this value is used to refine action selection: given a
   particular situation, what action will maximize expected reward?

   in biology, we can describe the primary work performed by an organism
   is to maintain homeostasis: maintaining metabolic energy reserves, body
   temperature, etc in a widely varying world.

   cybernetics provide a clear way of conceptualizing biological reward.
   in [21]neuroendocrine integration, we discussed how brains must respond
   both to internal and external changes. this dichotomy expresses itself
   as two perception-action loops: a visceral body-oriented loop, and a
   cognitive world-centered one.

   rewards are computed by the visceral loop. to a first approximation,
   reward encode progress towards homeostasis. food is perceived as more
   rewarding when the body is hungry, this is known as alliesthesia.
   reward information is delivered to the cognitive loop, which helps
   refine its decision making.

   id23- reward as visceral efferent

   extending markov chains

   recall that a [22]markov chain contains a set of states s, and a
   transition model p. a markov decision process (mdp) extends this
   device, by adding three new elements.

   specifically, an mdp is a 5-tuple (s, p, a, r,   ):
     * a set of states s     s
     * a transition model p[a](s    | s).
     * a set of actions a     a
     * a reward function r(s, s   )
     * a discount factor   

   to illustrate, consider gridworld. in this example, every location in
   this two-dimensional grid is a state, for example (1,0). state (3,0) is
   a desirable location: r(s(3,0)) = +1.0, but state (3,1) is
   undesirable, r(s(3,1)) = -1.0. all other states are neutral.

   gridworld supports four actions, or movements: up, down, left, and
   right.  however, locomotion is imperfect: if up is selected, the agent
   will only move up with 80% id203: 20% of the time it will go left
   or right instead. finally, attempting to move into a forbidden square
   will simply return the agent to its original location (   hitting the
   wall   ).

   id23- example mdp gridworld

   the core problem of mdps is to find a policy (  ), a function that
   specifies the agent   s response to all possible states. in general,
   policies should strive to maximize reward, e.g., something like this:

   id23- example mdp policy

   why is the policy at (2,2) left instead of up? because (2,1) is
   dangerous: despite selecting up, there is a 10% chance that the agent
   will accidentally move right, and be punished.

   let   s now consider an environment with only three states a, b, and c.
   first, notice how different policies change the resultant markov chain:

   reinforcement-learning-policy-vs-markov-chain-1

   this observation is important. policy determines the transition model.

   towards policy valuation v(s)

   an agent seeks to maximize reward. but what does that mean, exactly?

   imagine an agent selects     [1]. given the resultant markov chain, we
   [23]already know how to use id127 to predict future
   locations s[t]. the predicted reward p[t] is simply the dot product of
   expected location and the reward function.

   p_t = s_t \cdot r

   markov-chains-linear-algebra-expected-value

   we might be tempted to define the value function v(s) as the sum of
   all predicted future rewards:

   v_o(s) = p_0 + p_1 + p_2 + p_3 + \dots = \sum{p_k}

   however, this approach is flawed.  animals value temporal proximity:
   all else equal, we prefer to obtain rewards quickly. this is temporal
   discounting: as rewards are further removed from the present, their
   value is discounted.

   in id23, we implement temporal discounting with the
   gamma parameter: rewards that are k timesteps away are multiplied by
   the exponential discount factor \gamma^k . the value function becomes:

   v_o(s) = p_0 + \gamma p_1 + \gamma^2 p_2 + \gamma^3 p_3 + \dots =
   \sum{\gamma^k p_k}

   without temporal discounting, v(s) can approach infinity. but
   [24]exponential discounting ensures v(s) equals a finite value. finite
   valuations promote easier computation and comparison of state
   evaluations. for more on temporal discounting, and an alternative to
   the rl approach, see [25]an introduction to hyperbolic discounting.

   intertemporal consistency

   in our example, at time zero our agent starts in state a. we have
   already used id202 to compute our p[k] predictions. to
   calculate value, we simply compute $latex \sum{\gamma^k p_k}$

   v_0(a) = 0 + 0 + 0.64 \gamma^2 + 0.896 \gamma^3

   agents compute v(s) at every time step. at t=1, two valuations are
   relevant:

   v_1(a) = 0 + 0 + 0.64 \gamma^2 + \dots

   v_1(b) = 0 + 0.8 \gamma + 0.96 \gamma^2 + \dots

   mdp-value-function-timeslice

   what is the relationship between the value functions at t=0 and t=1? to
   answer this, we need to multiply each term by \gamma p(x|a) , where x
   is the state being considered at the next time step.

   w_1(a) \triangleq \gamma 0.2 v_1(a)

   w_1(a) = 0 + 0 + (0.2)(0.64)\gamma^3 + \dots

   similarly,

   w_1(b) \triangleq \gamma p(b|a)v_1(b) = \gamma 0.8 v_1(b)

   w_1(b) 0 + (0.8)(0.8) \gamma^2 + (0.8)(0.96) \gamma^3 + \dots

   critically, consider the sum x = r_0(s) + w_1(a) + w_1(b) :

   x = 0 + 0 + 0.64 \gamma^2 + 0.896 \gamma^3 + \dots

   mdp- intertemporal consistency

   does x_0 look familiar? that   s because it equals v_0(a) ! in this way,
   we have a way of equating a valuation at t=0 and t=1. this property
   is known as intertemporal consistency.

   bellman equation

   we have seen that v_0(a) = x_0 . let   s flesh out this equation, and
   generalize to time t.

   v_t(s) = r_t(a) + \gamma \sum{p(s'|s)v_{t+1}(s')}

   this is the [26]bellman equation, and it is a central fixture in
   control systems. at its heart, we define value in terms of both
   immediate reward and future predicted value. we thereby break up a
   complex problem into small subproblems, a key optimization technique
   that can be approached with id145.

   next time, we will explore how id23 uses the bellman
   equation to learn strategies with which to engage its environment (the
   optimal policy     ). see you then!
   advertisements

share this:

     * [27]facebook
     * [28]twitter
     * [29]reddit
     * [30]tumblr
     * [31]pocket
     * [32]email
     *

like this:

   like loading...

related

   categories: [33]introduction, [34]latex, [35]quantitative tags:
   [36]bellman equation, [37]exponential vs hyperbolic discounting,
   [38]intertemporal consistency, [39]markov chain, [40]markov decision
   process, [41]mdp, [42]policy, [43]predicted reward, [44]reward
   function, [45]temporal discounting, [46]transition model, [47]value
   function

post navigation

   [48]    an introduction to markov chains
   [49]an introduction to prospect theory    

leave a reply [50]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [51]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [52]log out /
   [53]change )
   google photo

   you are commenting using your google account. ( [54]log out /
   [55]change )
   twitter picture

   you are commenting using your twitter account. ( [56]log out /
   [57]change )
   facebook photo

   you are commenting using your facebook account. ( [58]log out /
   [59]change )
   [60]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   search for: ____________________ search

recent posts

     * [61]born to run: a theory of human anatomy
     * [62][excerpt] how language evolved
     * [63][excerpt] when language evolved
     * [64]the walking ape
     * [65]cooking and the hominin revolution
     * [66]an introduction to domestication
     * [67][excerpt] replicators and their vehicles
     * [68][excerpt] self-domestication and human homosexuality
     * [69][sequence] history
     * [70]the polyvagal theory

   [71]follow fewer lacunae on wordpress.com

      kevin binz and kevinbinz.com, 2016. unauthorized use and/or
   duplication of this material without express and written permission
   from this site   s author and/or owner is strictly prohibited. excerpts
   and links may be used, provided that full and clear credit is given to
   kevin binz and kevinbinz.com with appropriate and specific direction to
   the original content.

twitter feed

   [72]my tweets

     * [73]rss - posts
     * [74]rss - comments

meta

     * [75]register
     * [76]log in
     * [77]entries rss
     * [78]comments rss
     * [79]wordpress.com

   [80]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [81]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [82]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [83]cookie policy

   iframe: [84]likes-master

   %d bloggers like this:

references

   visible links
   1. https://kevinbinz.com/feed/
   2. https://kevinbinz.com/comments/feed/
   3. https://kevinbinz.com/2016/10/19/mdp/feed/
   4. https://kevinbinz.com/2016/10/17/markov-chains/
   5. https://kevinbinz.com/2016/10/26/prospect-theory/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://kevinbinz.com/2016/10/19/mdp/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://kevinbinz.com/2016/10/19/mdp/&for=wpcom-auto-discovery
   8. https://kevinbinz.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://kevinbinz.com/2016/10/19/mdp/#content
  11. https://kevinbinz.com/
  12. https://kevinbinz.com/
  13. https://kevinbinz.com/cognitive-science/
  14. https://kevinbinz.com/quantitative/
  15. https://kevinbinz.com/miscellaneous/
  16. https://kevinbinz.com/about-me/
  17. https://kevinbinz.com/2016/10/19/mdp/
  18. https://kevinbinz.com/author/kevinbinz/
  19. https://kevinbinz.com/2017/05/09/sequence-reinforcement-learning/
  20. https://kevinbinz.com/2016/10/17/markov-chains/
  21. https://kevinbinz.com/2016/01/16/neuroendocrine-integration/
  22. https://kevinbinz.com/2016/10/17/markov-chains/
  23. https://kevinbinz.com/2016/10/17/markov-chains/
  24. http://www.wolframalpha.com/widget/widgetpopup.jsp?p=v&id=29c546473e1c796d6076bb18901b15e7&i0=0.9^n&i1=1&i2=3000000
  25. https://kevinbinz.com/2014/09/14/an-introduction-to-hyperbolic-discounting/
  26. https://en.wikipedia.org/wiki/bellman_equation
  27. https://kevinbinz.com/2016/10/19/mdp/?share=facebook
  28. https://kevinbinz.com/2016/10/19/mdp/?share=twitter
  29. https://kevinbinz.com/2016/10/19/mdp/?share=reddit
  30. https://kevinbinz.com/2016/10/19/mdp/?share=tumblr
  31. https://kevinbinz.com/2016/10/19/mdp/?share=pocket
  32. https://kevinbinz.com/2016/10/19/mdp/?share=email
  33. https://kevinbinz.com/category/introduction/
  34. https://kevinbinz.com/category/latex/
  35. https://kevinbinz.com/category/quantitative/
  36. https://kevinbinz.com/tag/bellman-equation/
  37. https://kevinbinz.com/tag/exponential-vs-hyperbolic-discounting/
  38. https://kevinbinz.com/tag/intertemporal-consistency/
  39. https://kevinbinz.com/tag/markov-chain/
  40. https://kevinbinz.com/tag/markov-decision-process/
  41. https://kevinbinz.com/tag/mdp/
  42. https://kevinbinz.com/tag/policy/
  43. https://kevinbinz.com/tag/predicted-reward/
  44. https://kevinbinz.com/tag/reward-function/
  45. https://kevinbinz.com/tag/temporal-discounting/
  46. https://kevinbinz.com/tag/transition-model/
  47. https://kevinbinz.com/tag/value-function/
  48. https://kevinbinz.com/2016/10/17/markov-chains/
  49. https://kevinbinz.com/2016/10/26/prospect-theory/
  50. https://kevinbinz.com/2016/10/19/mdp/#respond
  51. https://gravatar.com/site/signup/
  52. javascript:highlandercomments.doexternallogout( 'wordpress' );
  53. https://kevinbinz.com/2016/10/19/mdp/
  54. javascript:highlandercomments.doexternallogout( 'googleplus' );
  55. https://kevinbinz.com/2016/10/19/mdp/
  56. javascript:highlandercomments.doexternallogout( 'twitter' );
  57. https://kevinbinz.com/2016/10/19/mdp/
  58. javascript:highlandercomments.doexternallogout( 'facebook' );
  59. https://kevinbinz.com/2016/10/19/mdp/
  60. javascript:highlandercomments.cancelexternalwindow();
  61. https://kevinbinz.com/2019/03/31/born-to-run/
  62. https://kevinbinz.com/2019/03/17/how-language-evolved/
  63. https://kevinbinz.com/2019/03/17/when-language-evolved/
  64. https://kevinbinz.com/2019/03/09/bipedality/
  65. https://kevinbinz.com/2019/02/24/cooking-ape/
  66. https://kevinbinz.com/2019/02/17/domestication/
  67. https://kevinbinz.com/2019/02/11/excerpt-replicators-and-their-vehicles/
  68. https://kevinbinz.com/2019/02/09/excerpt-self-domestication-and-human-homosexuality/
  69. https://kevinbinz.com/2019/02/05/sequence-history/
  70. https://kevinbinz.com/2019/02/05/the-polyvagal-theory/
  71. https://kevinbinz.com/
  72. https://twitter.com/408341604483596289
  73. https://kevinbinz.com/feed/
  74. https://kevinbinz.com/comments/feed/
  75. https://wordpress.com/start?ref=wplogin
  76. https://kevinbinz.wordpress.com/wp-login.php
  77. https://kevinbinz.com/feed/
  78. https://kevinbinz.com/comments/feed/
  79. https://wordpress.com/
  80. https://wordpress.com/?ref=footer_blog
  81. https://kevinbinz.com/2016/10/19/mdp/
  82. https://kevinbinz.com/2016/10/19/mdp/#cancel
  83. https://automattic.com/cookies
  84. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  86. https://kevinbinz.com/
  87. https://kevinbinz.com/2016/10/19/mdp/#comment-form-guest
  88. https://kevinbinz.com/2016/10/19/mdp/#comment-form-load-service:wordpress.com
  89. https://kevinbinz.com/2016/10/19/mdp/#comment-form-load-service:twitter
  90. https://kevinbinz.com/2016/10/19/mdp/#comment-form-load-service:facebook
  91. https://www.facebook.com/fewerlacunae/
