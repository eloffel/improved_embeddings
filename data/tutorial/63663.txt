    #[1]federico tomassetti - software architect    feed [2]federico
   tomassetti - software architect    comments feed

   [tr?id=1814680788781617&ev=pageview&noscript=1] [3]federico tomassetti
   - software architect rss2 feed

     * [4]twitter
     * [5]linkedin
     * [6]mail
     * [7]rss

   [8]federico tomassetti - software architect

     * [9]services
     * [10]products
     * [11]learning to build languages
     * [12]learning to process code
     * [13]using antlr like a professional
     * [14]newsletter
     * [15]about me
     * [16]let   s talk
     * [17]search
     * [18]menu

   [19]blog

[20]analyze and understand text: guide to natural language processing

   november 14, 2017/in [21]miscellany, [22]parsing /by [23]gabriele
   tomassetti

guide to natural language processing guide to natural language processing

what can you do with natural language processing?

   natural language processing (nlp) comprises a set of techniques to work
   with documents written in a natural language to achieve many different
   objectives. they range from simple ones that any developer can
   implement, to extremely complex ones that require a lot of expertise.

   the following table illustrate which technique can solve a particular
   problem.
   what you need where to look
   grouping similar words [24]id30, [25]splitting words, [26]parsing
   documents
   finding words with the same meaning [27]latent semantic analysis
   generating realistic names [28]splitting words
   understanding how much time it takes to read a text [29]reading time
   understanding how difficult to read is a text [30]readability of a text
   identifying the language of a text [31]identifying a language
   generating a summary of a text [32]sumbasic (word-based),
   [33]graph-based methods: textrank (relationship-based), [34]latent
   semantic analysis (semantic-based)
   finding similar documents [35]latent semantic analysis
   identifying entities (e.g., cities, people) in a text [36]parsing
   documents
   understanding the attitude expressed in a text [37]parsing documents
   translating a text [38]parsing documents

   this article uses terms like parsing and understanding in a loose way,
   at least compared to traditional meaning of these terms. we are going
   to talk about parsing in the general sense of analyzing a document and
   extracting its meaning. so, we are going to talk about actual parsing
   of natural languages, but we will spend most of the time on other
   techniques. when it comes to understanding programming languages, to
   parsing is the way to go. however, for natural languages you can pick
   specific alternatives. in other words, we are mostly going to talk
   about what you would use instead of parsing, to accomplish your goals.

   for instance, if you wanted to find all for statements a programming
   language file, you would parse it and then count the number of for
   statements. instead, to find all mentions of cats in a natural language
   document you are probably going to use something like id30.

   this is necessary because [39]the theory behind the parsing of natural
   languages might be the same one that is behind the parsing of
   programming languages, however the practice is very dissimilar. in
   fact, you are not going to build a parser for a natural language. that
   is unless you work in artificial intelligence or as researcher. you are
   even rarely going to use one. rather you are going to find an algorithm
   that work a simplified model of the document that can only solve your
   specific problem.

   in short, you are going to find tricks to avoid to actually having to
   parse a natural language. that is why this area of computer science is
   usually called natural language processing rather than natural language
   parsing.

algorithms that require data

   we are going to see specific solutions to each problem. mind you that
   these specific solutions can be quite complex themselves. the more
   advanced they are, the less they rely on simple algorithms. usually
   they need a vast database of data about the language. a logical
   consequence of this is that it is rarely easy to adopt a tool for one
   language to be used for another one. or rather, the tool might work
   with few adaptations, but to build the database would require a lot of
   investment. so, for example, you would probably find a ready to use
   tool to create a summary of an english text, but maybe not one for an
   italian one.

   for this reason, in this article we concentrate mostly on english
   language tools. although we mention if these tools work for other
   languages. you do not need to know the theoretical differences between
   languages, such as the number of genders or cases they have. however,
   you should be aware that the more different a language is from english,
   the harder would be to apply these techniques or tools to it.

   for example, you should not expect to find tools that can work with
   chinese (or rather the chinese writing system). it is not necessarily
   that these languages are harder to understand programmatically, but
   there might be less research on them or the methods might be completely
   different from the ones adopted for english.

the structure of this guide

   this article is organized according to the tasks we want to accomplish.
   which means that the tools and explanation are grouped according to the
   task they are used for. for instance, there is a section about
   measuring the properties of a text, such as its difficulty. they are
   also generally in ascending order of difficulty: it is easier to
   classify words than entire documents. we start with simple information
   retrieval techniques and we end in the proper field of natural language
   processing.

   we think it is the most useful way to provide the information you need:
   you need to do x, we directly show the methods and tools you can use.

table of contents

   the following table of contents shows the whole content of this guide.
    1. [40]classifying words
          + [41]grouping similar words
               o [42]id30
               o [43]splitting words
    2. [44]classifying documents
          + [45]text metrics
               o [46]reading time
               o [47]calculating the readability of a text
          + [48]identifying a language
    3. [49]understanding documents
          + [50]generation of summaries
               o [51]sumbasic
               o [52]graph-based methods: textrank
               o [53]latent semantic analysis
               o [54]other methods and libraries
               o [55]other uses
          + [56]parsing documents
               o [57]you need data
               o [58]the things you can do
               o [59]the libraries you can use
    4. [60]summary

classifying words

   with the expression classifying words, we intend to include techniques
   and libraries that group words together. some people consider these
   techniques more part of information retrieval than natural language
   processing. we think it depends on the intent of the developers. sure,
   they are used in information retrieval, but they are also fundamental
   to make advanced natural language processing algorithms work well.

grouping similar words

   we are going to talk about two methods that can group together similar
   words. they are very used in information retrieval (i.e, to make easier
   finding the right stuff), but they are also necessary to connect
   similar words for any purpose. in information retrieval these methods
   are used to find the documents, with the words we care about, from a
   pool of documents. that is useful because if a user search for
   documents containing the word friend he is probably equally interested
   in documents containing friends and possibly friended and friendship.
   the same principle can be useful in the context of creating a summary
   for an article. if an article talk about friends it is also about
   friend.

   so, to be clear, in this section we are not going to talk about methods
   to group semantically connected words, such as identifying all pets or
   all english towns.

   the two methods are: id30 and division of words into group of
   characters. the algorithms for the first ones are language dependent,
   while the ones for the second ones are not. we are going to examine
   each of them in separate paragraphs.

id30

   id30 is the process of finding the stem, or root, of a word. in
   this context, the stem is not necessarily the morphological root
   according to linguists. so, it is not the form of a word that you would
   find, say, in a vocabulary. for example, an algorithm may produce the
   stem consol for the word consoling. while in a vocabulary, as a root,
   you would find console.

   a typical application of id30 is grouping together all instances of
   words with the same stem for usage in a search library. so, if a user
   search for documents containing friend he can also find ones with
   friends or friended.

porter id30 algorithm

   let   s talk about an algorithm that remove suffixes to find the stem:
   the effective and widely used porter id30 algorithm. the algorithm
   was originally created by martin porter for english. there are also
   porter based/inspired algorithms for other languages: such as french or
   russian. you can find all of them at the website of [61]snowball.
   snowball is a simple language to describe id30 algorithms, but the
   algorithms are also described in plain english.

   a complete description of the algorithm is beyond the scope of this
   guide. however, its foundation is easy to grasp. fundamentally the
   algorithm divides a word in regions and then replace or remove certain
   suffixes, if they are completely contained in said region. so, for
   example, the porter2 (i.e., the updated version) algorithm, state that:

     r1 is the region after the first non-vowel following a vowel, or the
     end of the word if there is no such non-vowel

   and then, there is a rule that says that you should replace -tional
   with -tion, if it is found inside r1.

   for example:
     * the word confrontational has as r1 region -frontational
     * -tional is completely contained in its r1
     * so confrontational becomes confrontation

   the porter stemmer is purely algorithmic, it does not rely on an
   external database or computed rules (i.e., rules created according to a
   training database). this is a great advantage, because it makes it
   predictable and easy to implement. the disadvantage is that it cannot
   handle exceptional cases and known mistakes cannot be easily solved.
   for example, the algorithm creates the same stem for university and
   universal.

   a porter stemmer is not perfect, but it is simple, effective and easy
   to implement. for a language like english, a stemmer can be realized by
   any competent developer. so, there are many out there for all notable
   programming languages and we are not going to list them here.

typical issues with other languages

   most languages that are somewhat close to english, like german or even
   romance languages, are generally easy to stem. actually, the creation
   of the algorithm itself is complex and requires a great knowledge of
   the language. however, once somebody has done the hard work of creating
   an algorithm, implementing one is easy.

   in id30 there are many problems with two kinds of languages you
   will usually encounter. the first kind is [62]agglutinative languages.
   setting aside the linguistic meaning of the expression, the issue is
   that agglutinative languages pile up prefixes and suffixes on the root
   of a word.

   in particular turkish is problematic because is both an agglutinative
   language and a concatenative one. which mean basically that in turkish
   a word can represent a whole english sentence. this makes hard to
   develop a id30 algorithm for turkish, but it also makes it less
   useful. that is because if you stem a turkish word you might end up
   with one stem for each sentence, so you lose a lot of information.

   the second kind of issue is related to language with no clearly defined
   words. chinese is the prime example as a language that has no alphabet,
   but only symbols that represent concepts. so, id30 has no meaning
   for chinese. even determining the boundaries of concepts is hard. the
   problem of dividing a text in its component words is called word
   segmentation. with english documents, you can find the boundaries of
   words just by looking at whitespace or punctuation. there are no such
   things in a chinese text.

splitting words

   an alternative method to group together similar words relies on
   splitting them. the foundation of this method is taking apart words
   into sequence of characters. these characters are called k-grams, but
   they are also known as id165s characters (id165s might also indicate
   groups of words). the sequence of characters is built in a sliding
   manner, advancing by one character at each step, starting and ending
   with a special symbol that indicates the boundaries of the word. for
   example, the 3-grams for happy are:
     * $ha
     * hap
     * app
     * ppy
     * py$

   with the symbol $ used to indicate the beginning and the end of the
   word.

   the exact method used for search is beyond the scope of this article,
   but you need just an overview to understand it. in general terms:
    1. you apply this process to the words in each document of your
       database
    2. you apply the same process to the search term(s)
    3. finally, you just have to compare the occurrences of the k-grams of
       the input with the one of the words in the documents

   usually you apply a statistical coefficient, like the [63]jaccard
   coefficient, to determine how much similar the words have to be to be
   grouped together (i.e., how many grams have to have in common). for
   example, by choosing a different coefficient you might group together
   cat and cats or divide cat and catty.

   it is important to note a couple of things: the order of the k-grams
   and spelling mistakes. the order of the k-grams does not matter, in
   theory you could have completely different words that happens to have
   the same k-grams. in practice, this does not happen. this method is
   imprecise, which means that it can also protect from spelling mistakes
   of the user. for example, even if the user input locamotive instead of
   locomotive, it will probably still show the correct results. that is
   because 7 of 10 3-grams matches; exact matches would rank higher, but
   the words locamotive does not exist and so it probably has no matches.

limits and effectiveness

   the great advantage of this technique is that it is not just purely
   algorithmic and very simple, but it also works with all languages. you
   do not need to build k-grams for english differently from the ones for
   french. you just take apart the words in the same way. although it is
   important to note that the effectiveness is in the details: you have to
   pick the right number of k to have the best results.

   the ideal number depends on the average length of the word in the
   language: it should be lower or equal than that. different languages
   might have different values, but in general you can get away with 4 or
   5. you will not have the absolute best results with only one choice,
   but it will work.

   the disadvantage is that it looks extremely stupid. let   s face it: it
   so simple that it should not work. but it actually does, [64]it works
   well if not better than id30 (pdf). it is shamelessly effective,
   and it has many other uses. we are going to see one right now.

generating names

   this is a great example of how k-grams can be used in natural language
   processing. the general case of generating fake words that looks like
   real words is hard and of limited use. you could create phrases for a
   fake language, but that is pretty much it. however, there are a few use
   case to create realistic fake names: for use in games or for any world
   building need. and it is possible to create them programmatically

   there are several variantsof this simple method. the base method works
   roughly like that:
    1. create a database of names of the same kind you want to generate
       (e.g., roman names, space lizards names, etc.)
    2. divide the input names in k-grams (e.g., 3-grams of mark -> $ma    
       mar     ark     rk$)
    3. associate a id203 to the k-grams: the more frequently they
       appear in the original database, the higher the chance they appear
       in the generated name)
    4. generate the new names

   a variant can be more elaborate to improve the quality of the generated
   names or their variability. for instance, you could combine a different
   number of k-grams for specific purposes (e.g., all names start with a
   2-gram, but end in a 4-gram). this can be useful to force all names to
   end in a similar way or to fulfill a need for your story (e.g., if you
   start with a certain 2-gram you belong to a certain tribe or family).

   you could also improve the soundness of the generated names, simply by
   looking at the probabilities of the sequences appearing in a certain
   order. for example, if you randomly start with ar the following
   syllable might be more likely th than or. you usually calculate these
   probabilities based on the original database of names, but you could
   manipulate them to achieve specific sounds.

   this method is not perfect, but it generally works good enough. you can
   see a few simple libraries like [65]langgen or [66]vnamegenerator,
   which shows variations of said method and a few others.

classifying documents

   in this section we include techniques and libraries that measure and
   analyze documents. for example, they can detect the language in which a
   document is written or measure how difficult it is to read it. from now
   on, we are strictly in the natural language processing territory: you
   do not search for documents that take a little time to read. however,
   you might need to understand how much time it takes to read a document
   so maybe you can save it for the weekend.

text metrics

   there are two popular metrics of a text that can be easily implemented:
   reading time and difficulty of the text. these measurements are useful
   to inform the reader or to help the writer checking that the document
   respects certain criteria, such as being accessible to a young audience
   (i.e., low difficulty).

reading time

   the simplest way of measuring the reading time of a text is to
   calculate the words in the document, and then divide them by a
   predefined words per minute (wpm) number. the words per minute figure
   represent the words read by an average reader in a minute. so, if a
   text has 1000 words and the wpm is set to 200, the text would take 5
   minutes to read.

   that is easy to understand and easy to implement. the catch is that you
   have to pick the correct wpm rate and that the rate varies according to
   each language. for example, english readers might read 230 wpm, but
   french readers might instead read 200 wpm. this is related to the
   length of the words and the natural flow of the language (i.e., a
   language could be more concise than another, for instance it might
   frequently omit subjects). these average figures are found by
   researcher, so you just need to find them somewhere.

   the first issue is easily solved: for english most estimates put the
   correct wpm between 200 and 230. however, there is still the problem of
   dealing with different languages. this requires having the correct data
   for each language and to be able to understand the language in which a
   text is written.

   to mitigate both problems you might opt to use [67]a measurement of
   characters count in order to estimate the reading time. basically, you
   remove the punctuation and spaces, then count the characters and divide
   the sum by 900-1000.

   on linguistic grounds the measure makes less sense, since people do not
   read single characters. however, the difference between languages are
   less evident by counting characters. for example, an [68]agglutinative
   language might have very long words, and thus fewer of them. so it ends
   up with a similar number of characters to a fusional language like
   english.

   this works better because the differences in speed of reading
   characters in each language is smaller as a percentage of the total
   speed. imagine for example that the typical reader of english can read
   200 wpm and 950 cpm, while the typical reader of french can read 250
   wpm and 1000 cpm. the absolute difference is the same, but it is less
   relevant for reading characters. of course, this is still less than
   ideal, but it is a simple solution.

   neither of the measure consider the difficulty of the text. that texts
   that are difficult to read take more time to read, even with the same
   number of words or characters. so, depending on your need, you might
   want to combine this measurement with one for the difficulty of the
   text.

calculating the readability of a text

   usually the calculation of the readability of a text is linked to
   grades of education (i.e., years of schooling). so, an easy text might
   be one that can be read by 4th graders, while a harder one might need a
   10th grade education. that is both a byproduct of the fact that the
   algorithms were created for educational purposes and because education
   is a useful anchor for ranking difficulty. saying that a text is
   difficult in absolute terms is somewhat meaningless, saying that it is
   difficult for 7th-graders makes more sense.

   there are several formulas, but they are generally all based on the
   number of words and sentences in addition to either syllables or the
   number of difficult words. let   s see two of the most famous:
   flesch-kincaid and dale   chall.

   none of these formulas is perfect, but both have been scientifically
   tested. the only caveat is that they should be used only for checking
   and not as a guideline. they work if you write normally. if you try to
   edit a text to lower the score, the results might be incorrect and
   unnatural. for example, if you use short words just to make a text seem
   easy, it looks bad.

flesch-kincaid readability formula

   there are two variants of this formula: flesch reading ease and
   flesch   kincaid grade level. they are equivalent, but one output a score
   (the higher it is, the easier is the text) and the other a
   corresponding us grade level. we are going to show the first one.

   flesch reading ease flesch reading ease the readability is generally
   between 100 to 20. a result of 100 indicates a document that can be
   easily understood by a 11-years old student, while a result of 30 or
   less indicates a document suited for university graduates. you can find
   a more complete explanation and ranking table in [69]how to write plain
   english.

   the different parameters can be obtained easily. only calculating the
   total number of syllables requires a significant amount of work. the
   good news is that it is actually doable and there is a reliable
   algorithm for it. the bad news is that the author of the tex
   hyphenation algorithm (frank liang) [70]wrote his phd thesis about his
   hyphenation algorithm. you can find an implementation and an accessible
   explanation of the algorithm in [71]hyphenopoly.js. the two problems
   are equivalent, since you can only divide a word in two parts between
   two syllables.

   an alternative is to use a hack: instead of calculating syllables,
   count the vowels. this hack has been reported to work for english, but
   it is not applicable to other languages. although, if you use it, you
   lose the scientific validity of the formula and you just get a somewhat
   accurate number.

   the general structure of the formula has been applied to other
   languages (e.g., [72]flesch-vacca for italian), but each language have
   different coefficients.

dale   chall readability formula

   this formula relies also on the number of words and sentences, but
   instead of syllables it uses the number of difficult words present in
   the text.

   a difficult word is defined as one that do not belong to a list of 3000
   simple words, that 80% of fourth graders understand.

   dale-chall readability formula dale-chall readability formula thus, the
   formula is easy to use and calculate. the only inconvenience is that
   you have to maintain a database of these 3000 words. we are not aware
   of the formula having been adapted to languages other than english.

   the formula generally output a score between 4 and 10. less than 5
   indicates a text suitable for 4th graders, a result of 9 or more
   indicates a text for college students. you can find a complete table of
   results at [73]the new dale-chall readability formula.

   it is natural to think that you could modify the formula, to calculate
   the difficulty in understanding a specialized text. that is to say you
   could define difficult words as words belonging to technical
   terminology. for example, you could calculate how difficult would be to
   understand a text for an average person, according to how much computer
   jargon it contains. words like parser or compiler could be difficult,
   while computer or mouse could be considered easy. in theory this might
   work: however you would have to calculate the correct coefficients
   yourself. so, you would have to actually recruit people to conduct a
   proper research.

identifying a language

   when you need to work with a natural language document, the need to
   identifying a language comes up often. for starters, almost all the
   algorithms we have seen works only on a specific language and not all
   of them. even overcoming this problem it is useful to be able to
   understand in which language is written a certain document. for
   instance, so that you can show or hide a document to certain users
   based on the language it understands. imagine that a user search for
   documents containing the word computer: if you simply return all
   documents that contain that word you will get results even in languages
   other than english. that is because the word has been adopted by other
   languages (e.g., italian).

   reliable id46 can be achieved with statistical
   methods. we are going to talk about two methods: a vocabulary based one
   and one based on frequencies of groups of letters.

   in theory you could also hack together ad-hoc methods based on language
   clues, such as the one listed in [74]wikipedia:language recognition
   chart. and then come up with a ranking method to order the id203
   of each language. the advantage of this approach is that you would not
   need any database. however, it has not been tested scientifically.

   for example, two features of italian are: most words end with a vowel
   and the word    (i.e., is) is quite common. so you could check for the
   presence and frequencies of these features in a certain document and
   use them to calculate the id203 that said document were in
   italian (e.g., at least 70% of the words ends in vowel -> +50% chance
   that the document is in italian; the word    is present in at least 10%
   phrases -> +50% chance that the document is in italian).

words in a vocabulary

   a basic method consists in comparing words in the text with the ones
   included in a vocabulary. first, you get a vocabulary of each language
   you care about. then, you compare the text with the words in each
   vocabulary and count the number of occurrences. the vocabulary which
   includes the highest number of words denotes the language of the text.

   for example, the following phrase: the cat was in the boudoir, would be
   identified as english because there are 5 english words (the, cat, was,
   in, the) and 1 french word (boudoir). in case you are wondering, a
   boudoir is essentially a female version of a man cave, but for
   sophisticated people.

   despite its simplicity this method works well for large documents, but
   not so much for twitter-like texts: texts that are short and frequently
   contains errors. the author of the [75]whatlanguage library, which
   implements this method, suggest a minimum of 10 words for reliable
   identification.

frequencies of groups of letters

   the most currently used technique relies on building language models of
   the frequencies of groups of letters. we are again talking about
   k-grams, that have to be analyzed for each language. we are going to
   describe the procedure outlined by the original paper [76]id165-based
   text categorization (pdf), although an implementation might adopt a
   slight variation of it. two well-known libraries implementing this
   method are: [77]the php textcat library by wikimedia and [78]ntextcat.

   first we are going to build a model for a language and then we are
   going to use it for scoring a document.

building a language model

   to create such language models, you need to have a large set of
   documents for each language. you divide the words in these documents to
   find the most used k-grams. the paper suggests calculating the number
   of k-grams for k from 1 to 5.

   once you have the frequencies of k-grams, you order them in descending
   order to build a language model. for instance, a language model for
   english might have th in first place, ing in second and so on. in this
   final stage the actual number of occurrences it is not relevant, only
   their final ranking in frequency. that is to say the fact that th might
   appear 2648 or 5895 times depends only on the size of your set of
   documents and it is irrelevant for the success of the method. what it
   is relevant is just the relative frequency in your set of documents and
   thus the respective ranking.

   once you have built this language model, you can use it to identify the
   language in which a document is written. you have to apply the same
   procedure to build a document model of the text that you are trying to
   identify. so, you end up with a ranking of the frequencies of k-gram in
   the document.

   finally, you calculate the differences between the rankings in the
   document and the ones for each language model you have. you could
   calculate this distance metric with many statistical formulas, but the
   paper uses a simple out-of-place method.

scoring languages

   the method consists of comparing the position in each language model.
   for each position, you add a number equivalent to the differences of
   ranks between each language model and the document model. for example,
   if the language model for english put th in first place, but the
   document model for the document that we are classifying put it in 6th
   place you add 5 to the english score. at the end of the process, the
   language with the lowest score should be the language of the text.

   obviously, you do not compare all positions of k-grams up to the last
   one, because the lower you go the more the position becomes somewhat
   arbitrary. it will depend on the particular set of documents you will
   have chosen to build a language model. the paper uses a limit of 300
   positions, but it also says that it depends on the short length of the
   documents the scientists have chosen. so, you would have to find the
   best limit for yourself.

   there is also the chance of wrongly classifying a text. this is more
   probable if a text is written in a language for which there is no
   model. in that case the algorithm might wrongly classify the text as
   one belonging to a language close to the real one. that could happen
   because the method finds a language that has a good enough score for
   the document. the real language would have scored better, but it is not
   available, so the good enough score wins.

   for instance, if you have a model for italian, but do not have one for
   spanish, your software might classify a spanish text as an italian one
   because italian is similar enough to spanish and the closest you have
   to it.

   this problem can be mitigated by using a proper threshold. that is to
   say a match is found only if the input has a low score (compared to its
   length) for a language. of course, now you have the problem of finding
   such proper threshold.

limitations

   this method works better with short texts, but it is not perfect. the
   suggested limit is based on the implementation and the quality of text
   and model. for example, ntextcat recommends a limit of 5 words.
   generally speaking this method would not work reliably for twitter
   messages or similar short and frequently incorrect texts. however, that
   would be true even for a human expert.

understanding documents

   this section contains more advanced libraries of natural language
   processing, the ones used to understand documents and their content. we
   use the concept understand somewhat loosely: we talk about how a
   computer can extract or manage the content of a document beyond simple
   manipulation of words and characters.

   we are going to see how you can:
     * generate summary of a document (i.e., an algorithmic answer to the
       question what is this article about?)
     * id31 (i.e., does this document contain a positive or
       negative opinion?)
     * parsing a document written in a natural language
     * translate a document in another language

   for the methods listed in the previous sections you could build a
   library yourself with a reasonable effort. from now on, it will get
   harder. that is because they might require vast amount of annotated
   data (e.g., a vocabulary having each word with the corresponding part
   of speech) or rely on complex machine learning methods. so, we will
   mostly suggest using libraries.

   this is an area with many open problems and active research, so you
   could find most libraries in python, a language adopted by the research
   community. though you could find the occasional research-ready library
   in another language.

   a final introductory note is that statistics and machine learning are
   the current kings of natural language processing. so there is probably
   somebody trying to use [79]tensorflow to accomplish each of these tasks
   (e.g., [80]deep news summarization). you might try that too, if you
   take in account a considerable amount of time for research.

generation of summaries

   the creation of a summary, or a headline, to correctly represent the
   meaning of a document it is achievable with several methods. some of
   them rely on information retrieval techniques, while others are more
   advanced. the theory is also divided in two strategies: extracting
   sentences or parts thereof from the original text, generating abstract
   summaries.

   the second strategy it is still an open area of research, so we will
   concentrate on the first one.

sumbasic

   sumbasic is a method that relies on the id203 of individual words
   being present in a sentence to determine the most representative
   sentence:
    1. first, you have to account the number of times a word appears in
       the whole document. with that you calculate the id203 of each
       word appearing in the document. i.e., if the word appears 5 times
       and the document has 525 words, its id203 is 5/525.
    2. you calculate a weight for each sentence that is the average of the
       probabilities of all the words in the sentence. i.e., if a sentence
       contains three words with id203 3/525, 5/525 and 10/525, the
       weight would be 6/525.
    3. finally, you score the sentences by multiplying the highest
       id203 word of each sentence with its weight. i.e., a sentence
       with a weight of 0.1 and whose best word had the id203 of 0.5
       would score 0.1 * 0. 5 = 0.05, while another one with weight 0.2
       and a word with id203 0.4 would score 0.2 * 0.4 = 0.08.

   having found the best sentence, you recalculate the probabilities for
   each word in the chosen sentence. you recalculate the probabilities as
   if the chosen sentence was removed from the document. the idea is that
   the included sentence already contains a part of the whole meaning of
   the document. so that part become less important and this helps
   avoiding excessive repetition. you repeat the process until you reach
   the needed summary length.

   this technique is quite simple. it does not require to have a database
   of documents to build a general id203 of a word appearing in any
   document. you just need to calculate the probabilities in each input
   document. however, for this to work you have to exclude what are called
   stopwords. these are common words present in most documents, such as
   the or is. otherwise you might include meaningless sentences that
   include lots of them. you could also perform id30 before applying
   this algorithm to improve the results.

   it was first described in [81]the impact of frequency on summarization
   (pdf); there is an implementation available as [82]a python library.

   the approach based on frequencies is an old and popular one, because it
   is generally effective and simple to implement. sumbasic is good enough
   that is frequently used as a baseline in the literature. however, there
   are even simpler methods. for example, [83]open text summarizer is a
   2003 library that uses an even simpler approach. basically you count
   the frequency of each word, then you exclude the common english words
   (e.g., the, is) and finally you calculate the score of a sentence
   according to the frequencies of the word it contains.

graph-based methods: textrank

   there are more complex methods of calculating the relevance of the
   individual sentences. a couple of them take inspiration from id95:
   they are called lexrank and textrank. they both rely on the
   relationship between different sentences to obtain a more sophisticate
   measurement of the importance of sentences, but they differ in the way
   they calculate similarity of sentences.

   id95 measures the importance of a document according to the
   importance of other documents that links to it. the importance of each
   document, and thus each link, is computed recursively until a balance
   is reached.

   textrank works on the same principle: the relationship between elements
   can be used to understand the importance of each individual element.
   textrank actually uses a more complex formula than the original
   id95 algorithm, because a link can be only present or not, while
   textual connections might be partially present. for instance, you might
   calculate that two sentences containing different words with the same
   stem (e.g., cat and cats both have cat as their stem) are only
   partially related.

   the original paper describes a generic approach, rather than a specific
   method. in fact, it also describes two applications: keyword extraction
   and summarization. the key differences are:
     * the units you choose as a foundation of the relationship
     * the way you calculate the connection and its strength

   for instance, you might choose as units id165s of words or whole
   phrases. id165s of words are sequences of n words, computed the same
   way you do k-gram for characters. so, for the phrase dogs are better
   than cats, there are these 3-grams:
     * dogs are better
     * are better than
     * better than cats

   phrases might create weighted links according to how similar they are.
   or they might simply create links according to the position they are
   (i.e., a phrase might link to the previous and following one). the
   method works the same.

textrank for sentence extraction

   textrank for extracting phrases uses as a unit whole sentences, and as
   a similarity measure the number of words in common between them. so, if
   two phrases contain the words tornado, data and center they are more
   similar than if they contain only two common words. the similarity is
   normalized based on the length of the phrases. to avoid the issue of
   having longer phrases having higher similarity than shorter ones.

   the words used for the similarity measure could be stemmed. stopwords
   are usually excluded by the calculation. a further improvement could be
   to also exclude verbs, although that might be complicated if you do not
   already have a way to identify the parts of speech.

   lexrank differs mainly because as a similarity measure it uses a
   standard tf-idf (term frequency     inverse document frequency).
   basically with tf-idf the value of individual words is first weighted
   according to how frequently they appear in all documents and in each
   specific document. for example, if you are summarizing articles for a
   car magazine, there will be a lot of occurrences of the word car in
   every document. so, the word car would be of little relevance for each
   document. however, the word explosion would appear in few documents
   (hopefully), so it will matter more in each document it appears.

   the paper [84]textrank: bringing order into texts (pdf) describe the
   approach. [85]explaintome contains a python implementation of textrank.

latent semantic analysis

   the methods we have seen so far have a weakness: they do not take into
   account semantics. this weakness is evident when you consider that
   there are words that have similar meanings (i.e., synonyms) and that
   most words can have different meaning depending on the context (i.e.,
   polysemy).  latent semantic analysis attempt to overcome these issues.

   the expression latent semantic analysis describes a technique more than
   a specific method. a technique that could be useful whenever you need
   to represent the meaning of words. it can be used for summarization,
   but also for search purposes, to find words like the query of the user.
   for instance, if the user search for happiness a search library using
   lsa could also return results for joy.

a simple description

   the specific mathematical formulas are a bit complex and involve
   matrices and operations on them. however, the founding idea is quite
   simple: words with similar meaning will appear in similar parts of a
   text. so you start with a normal tf-idf matrix. such matrix contains
   nothing else than the frequencies of individual words, both inside a
   specific document and in all the documents evaluated.

   the problem is that we want to find a relation between words that do
   not necessarily appear together. for example, imagine that different
   documents contain phrases containing the words joy and happiness
   together other words cookie or chocolate. the words do not appear in
   the same sentence, but they appear in the same document. one document
   contains a certain number of such phrases: a dog creates happiness and
   dogs bring joy to children. for this document, lsa should be able to
   find a connection between joy and happiness through their mutual
   connection with dog.

   the connection is build based on the frequency the words appear
   together or with related words in the whole set of documents. this
   allows to build connection even in a sentence or document where they do
   not appear together. so if joy and happiness appears frequently with
   dog, lsa would associate the specific document with the words (joy,
   happiness) and dog.

   basically, this technique will transform the original matrix from one
   linking each term with its frequency, into one with a (weighted)
   combination of terms linked to each document.

   the issue is that there are a lot of words, and combinations thereof,
   so you need to make a lot of calculation and simplifications. and that
   is where the complex math is needed.

   once you have this matrix, the world is your oyster. that is to say you
   could use this measurement of meaning in any number of ways. for
   instance, you could find the most relevant phrase and then find the
   phrases with are most close to it, using a graph-based method.

   [86]text summarization and singular value decomposition describes one
   way to find the best sentences. the python library [87]sumy offers an
   implementation.

other methods and libraries

   the creation of summaries is a fertile area of research with many valid
   methods already devised. in fact, much more than the ones we have
   described here. they vary for the approaches and the objective they are
   designed for. for example, some are created specifically to provide an
   answer to a question of the user, others to summarize multiple
   documents, etc.

   you can read a brief taxonomy of other methods in [88]automatic text
   summarization (pdf). the python library [89]sumy, that we have already
   mentioned, implements several methods, though not necessarily the ones
   mentioned in the paper.

   [90]classifier4j (java), [91]nclassifier (c#) and [92]summarize
   (python) implements a bayes classifier in an algorithm described as
   such:

     in order to summarize a document this algorithm first determines the
     frequencies of the words in the document. it then splits the
     document into a series of sentences. then it creates a summary by
     including the first sentence that includes each of the most frequent
     words. finally summary   s sentences are reordered to reflect that of
     those in the original document.

         [93]summarize.py

   these projects that implements a bayes classifier are all dead, but
   they are useful to understand how the method could be implemented.

   [94]datateaser and [95]pyteaser (both in python, but originally
   datateaser was in scala) use a custom approach that combines several
   simple measurements to create a summary of an article.

other uses

   you can apply the same techniques employed to create summaries to
   different tasks. that is particularly true for the more advanced and
   semantic based ones. notice that the creation of only one summary for
   many documents is also a different task. that is because you have to
   take in account different documents lengths and avoid the repetitions,
   among other things.

   a natural application is the identification of similar documents. if
   you can devise a method to identify the most meaningful sentences of
   one document, you can also compare the meaning of two documents.

   another objective with common techniques is information retrieval. in
   short, if a user search for one word, say car, you could use some of
   these techniques to find documents containing also automobile.

   finally, there is topic modelling, which consists in finding the topics
   of a collection of documents. in simple terms, it means grouping
   together words with similar themes. it uses more complex statistical
   methods that the one used for the creation of summaries. the current
   state of art is based upon a method called id44.

   [96]gensim is a very popular and production-ready library, that have
   many such applications. naturally is written in python.

   [97]mallet is a java library mainly designed for topic modelling.

parsing documents

   most computer languages are easy to parse. this is not true for natural
   languages. there are approaches that give good results, but ultimately
   this is still an open area of research. fundamentally the issue is that
   the parsing a sentence (i.e., analyzing its syntax) and its meaning are
   interconnected in a natural language. a subject or a verb, a noun or an
   adverb are all words and most words that can be subject can also be
   object.

   in practical terms, this means that there are no ready to use libraries
   that are good for every use you can think of. we present some libraries
   that can be used for restricted tasks, such as recognizing parts of
   speech, that can also be of use for improving other methods, like the
   ones for creation of summaries.

   there is also the frustrating fact is that a lot of software is made by
   academic researchers. which means that it could be easily abandoned for
   another approach or lacking documentation. you cannot really use a
   work-in-progress, badly maintained software for anything production
   project. especially if you care about a language other than english,
   you might find yourself seeing a good working demo, which was written
   ten years ago, by somebody with no contact information and without any
   open source code available.

you need data

   to achieve any kind of result with parsing, or generally extracting
   information from a natural language document, you need a lot of data to
   train the algorithms. this group of data is called a corpus. for use in
   a system that uses statistical or machine learning techniques, you
   might just need a lot of real world data possibly divided in the proper
   groups (e.g., wikipedia articles divided by category).

   however, if you are using a smart system, you might need this corpus of
   data to be manually constructed or annotated (e.g., the word dog is a
   noun that has these x possible meanings). a smart system is one that
   tries to imitate human understanding, or at a least that uses a process
   that can be followed by humans. for instance, a parser that relies on a
   grammar which uses rules such as phrase     subject verb (read: a phrase
   is made of a subject and a verb), but also defines several classes of
   verbs that humans would not normally use (e.g., verbs related to
   motion).

   in these cases, the corpus often uses a custom format and is built for
   specific needs. for example, this [98]system that can answer
   geographical questions about united states uses information stored in a
   prolog format. the natural consequence is that even what is generally
   available information, such as dictionary data, can be incompatible
   between different programs.

   on the other hand, there are also good databases that are so valuable
   that many programs are built around them. [99]id138 is an example of
   such database. it is a lexical database that links groups of words with
   similar meaning (i.e., synonyms) with their associated definition. it
   works thus as both a dictionary and a thesaurus. the original version
   is for english, but it has inspired similar databases for other
   languages.

what you can do

   we have presented some of the practical challenges to build your own
   library to understand text. and we have not even mentioned all the
   issues related to ambiguity of human languages. so differently from
   what we did for past sections we are just going to explain what you can
   do. we are not going to explain the algorithms used to realized them,
   both because there is no space and also without the necessary data they
   would be worthless. instead in the next paragraph we are just going to
   introduce the most used libraries that you can use to achieve what you
   need.

named-entity recognition

   named-entity recognition basically means finding the entities mentioned
   in the document. for example, in the phrase john smith is going to
   italy, it should identify john smith and italy as entities. it should
   also be able to correctly keep track of them in different documents.

id31

   id31 classifies the sentiment represented by a phrase. in
   the most basic terms, it means understanding if a phrase indicates a
   positive or negative statement. a naive bayes classifier can suffice
   for this level of understanding. it works in a similar way a spam
   filter works: it divides the messages into two categories (i.e., spam
   and non-spam) relying on the id203 of each word being present in
   any of the two categories.

   an alternative is to manually associate an emotional ranking to a word.
   for example, a value between -10/-5 and 0 for catastrophic and one
   between 0 and 5/10 for nice.

   if you need a subtler evaluation you need to resort to machine learning
   techniques.

parts of speech tagging

   parts of speech tagging (usually abbreviated as pos-tagging) indicates
   the identification and labelling of the different parts of speech
   (e.g., what is a noun, verb, adjective, etc.). while is an integral
   part of parsing, it can also be used to simplify other tasks. for
   instance, it can be used in the creation of summaries to simplify the
   sentences chosen for the summary (e.g., removing subordinates   
   clauses).

lemmatizer

   a lemmatizer return the lemma for a given word and a part of speech
   tag. basically, it gives the corresponding dictionary form of a word.
   in some ways it can be considered an advanced form of a stemmer. it can
   also be used for similar purposes, namely it can ensure that all
   different forms of a word are correctly linked to the same concepts.

   for instance, it can transform all instances of cats in cat, for search
   purposes. however, it can also distinguish between the cases of run as
   in the verb to run and run as in the noun synonym of a jog.

chunking

   parts of speech tagging can be considered equivalent to lexing in
   natural languages. chunking, also known as id66, is a step
   above parts of speech tagging, but one below the final parsing. it
   connects parts of speech in higher units of meaning, for example
   complements. imagine the phrase john always wins our matches of russian
   roulette:
     * a pos-tagger identifies that russian is an adjective and roulette a
       noun
     * a chunker groups together (of) russian roulette as a complement or
       two related parts of speech

   the chunker might work to produce units that are going to be used by a
   parser. it can also work independently, for example to help in
   named-entity recognition.

parsing

   the end result is the same as for computer languages: [100]a parse
   tree. though the process is quite different, and it might start with a
   probabilistic grammar or even with no grammar at all. it also usually
   continues with a lot of probabilities and statistical methods.

   the following is a parse tree created by the stanford parser (we are
   going to see it later) for the phrase my dog likes hunting cats and
   people. groups of letters such as np indicates parts of speech or
   complements.

   (root_______________________________________________________
     (s________________________________________________________
       (np (prp$ my) (nn dog))_________________________________
       (vp (vbz likes)_________________________________________
         (np (nn hunting) (nns cats)___________________________
           (cc and)____________________________________________
           (nns people)))))____________________________________
   1
   2
   3
   4
   5
   6
   7
   (root
     (s
       (np (prp$ my) (nn dog))
       (vp (vbz likes)
         (np (nn hunting) (nns cats)
           (cc and)
           (nns people)))))

translation

   the current best methods for automatic machine translation rely on
   machine learning. the good news is that this means you just need a
   great number of documents in the languages you care about, without any
   annotation. typical sources of such texts are wikipedia and the
   official documentation of the european union (which requires documents
   to be translated in all the official languages of the union).

   as anybody that have tried google translate or bing translator can
   attest, the results are generally good enough for understanding, but
   still often a bit off. they cannot substitute a human translator.

the best libraries available

   the following libraries can be used for multiple purposes, so we are
   going to divide this section by the title of the libraries. most of
   them are in python or java.

apache opennlp

     the [101]apache opennlp library is a machine learning based toolkit
     for the processing of natural language text. it supports the most
     common nlp tasks, such as id121, sentence segmentation,
     part-of-speech tagging, named entity extraction, chunking, parsing,
     and coreference resolution. these tasks are usually required to
     build more advanced text processing services. opennlp also included
     maximum id178 and id88-based machine learning.

   apache opennlp is a java library with an [102]excellent documentation
   that can fulfill most of the tasks we have just discussed, except for
   id31 and translation. the developers provide
   [103]language models for a few languages in addition to english, the
   most notable are german, spanish and portuguese.

the classical language toolkit

     the [104]classical language toolkit (cltk) offers natural language
     processing (nlp) support for the languages of ancient, classical,
     and medieval eurasia. greek and latin functionality are currently
     most complete.

   as the name implies the major feature of the classical language toolkit
   is the support for classical (ancient) languages, such as greek and
   latin. it has basic nlp tools, such as a lemmatizer, but also
   indispensable tools to work with ancient languages, such as
   id68 support, and peculiar things like [105]clausulae
   analysis. it has a good documentation and it is your only choice for
   ancient languages.

freeling

     [106]freeling is a c++ library providing language analysis
     functionalities (morphological analysis, named entity detection,
     pos-tagging, parsing, id51, semantic role
     labelling, etc.) for a variety of languages (english, spanish,
     portuguese, italian, french, german, russian, catalan, galician,
     croatian, slovene, among others).

   it is a library with a good documentation and even a [107]demo. it
   supports many languages usually excluded by other tools, but it is
   released the affero gpl which is probably the least user-friendly
   license ever conceived.

moses

     [108]moses is a id151 system that allows
     you to automatically train translation models for any language pair.
     all you need is a collection of translated texts (parallel corpus).
     once you have a trained model, an efficient search algorithm quickly
     finds the highest id203 translation among the exponential
     number of choices.

   the only thing to add is that the system is written in c++ and there is
   ample documentation.

nltk

     nltk is a leading platform for building python programs to work with
     human language data. it provides easy-to-use interfaces to over 50
     corpora and lexical resources such as id138, along with a suite of
     text processing libraries for classification, id121,
     id30, tagging, parsing, and semantic reasoning, wrappers for
     industrial-strength nlp libraries, and an active discussion forum.

   natural language toolkit (nltk) is probably the most known nlp library
   for python. the library can accomplish many tasks in different ways
   (i.e., using different algorithms). it even has a good documentation
   (if you include the freely available book).

   simply put: it is the standard library for nlp research. though one
   issue that some people have is exactly that: it is designed for
   research and educational purposes. if there are ten ways to do
   something nltk would allow you to choose among them all. the intended
   user is a person with a deep understanding of nlp

   [109]textblob is a library that builds upon nltk (and pattern) to
   simplify processing of textual data. the library also provides
   translation, but it does not implement it directly: it is simply an
   interface for google translate.

pattern

   [110]pattern is the most peculiar software in our group because it is a
   collection of python libraries for web mining. it has support for data
   mining from services such as google and twitter (i.e., it provide
   functions to directly search from google/twitter) , an html parser and
   many other things. among these things there is natural language
   processing for english and a few other languages, including german,
   spanish, french and italian.

   though english support is more advanced than the rest.

     the pattern.en module contains a fast part-of-speech tagger for
     english (identifies nouns, adjectives, verbs, etc. in a sentence),
     id31, tools for english verb conjugation and noun
     singularization & pluralization, and a id138 interface.

   the rest of the libraries can only support pos-tagging.

polyglot

   [111]polyglot is a set of nlp libraries for many natural languages in
   python. it looks great, although it has only little documentation.

   it supports fewer languages for the more advanced tasks, such as pos
   tagging (16) or id39 (40). however, for sentiment
   analysis and id46 can work with more than a hundred
   of them.

sentiment and sentiment

   [112]sentiment is javascript (node.js) library for id31.
   the library relies on afinn (a collection of english words with an
   associated emotional value) and a similar database for emoji. these
   database associate to each word/emoji a positive or negative value, to
   indicate a positive or negative sentiment. for example, the word joy
   has a score of 3, while sad has -2.

   the code for the library itself is quite trivial, but it works, and it
   is easy to use.

   var sentiment = require('sentiment');_______________________
   ____________________________________________________________
   var r1 = sentiment('cats are stupid.');_____________________
   console.dir(r1);        // score: -2, comparative: -0.666___
   ____________________________________________________________
   var r2 = sentiment('cats are totally amazing!');____________
   console.dir(r2);        // score: 4, comparative: 1_________
   1
   2
   3
   4
   5
   6
   7
   var sentiment = require('sentiment');

   var r1 = sentiment('cats are stupid.');
   console.dir(r1);        // score: -2, comparative: -0.666

   var r2 = sentiment('cats are totally amazing!');
   console.dir(r2);        // score: 4, comparative: 1

     id31 using machine learning techniques.

   that is the extent of the documentation for the python library
   [113]sentiment. although there is also a [114]paper and a [115]demo.
   the paper mentions that:

     we have explored different methods of improving the accuracy of a
     naive bayes classifier for id31. we observed that a
     combination of methods like negation handling, word id165s and
     feature selection by mutual information results in a significant
     improvement in accuracy.

   which means that it can be a good starting point to understand how to
   build your own id31 library.

spacy

     industrial-strength natural language processing in python

   the library [116]spacy claims to be a much more efficient, ready for
   the real world and easy to use library than nltk. in practical terms it
   has two advantages over nltk:
     * better performance
     * it does not give you the chance of choosing among the many
       algorithms the one you think is best, instead it chooses the best
       one for each task. while less choices might seem bad, it can
       actually be a good thing. that is if you have no idea what the
       algorithms do and you have to learn them before making a decision.

   in practical terms it is a library that supports most of the basic
   tasks we mentioned (i.e., things like id39 and
   pos-tagging, but not translation or parsing) with a great code-first
   documentation.

   [117]textacy is a library built on top of spacy for higher-level nlp
   tasks. basically, it simplifies some things including features for
   cleaning data or managing it better.

the stanford natural language processing group software

     the stanford nlp group makes some of our natural language processing
     software available to everyone! we provide statistical nlp, deep
     learning nlp, and rule-based nlp tools for major computational
     linguistics problems, which can be incorporated into applications
     with human language technology needs. these packages are widely used
     in industry, academia, and government.

   the stanford nlp group creates and support many great tools that cover
   all the purposes we have just mentioned. the only thing missing is
   id31. the most notable software are [118]corenlp and
   [119]parser. the parser can be seen in action in a [120]web demo.
   corenlp is a combination of several tools, including the parser.

   the tools are all in java. the parser supports a few languages:
   english, chinese, arabic, spanish, etc. the only downside is that the
   tools are licensed under the gpl. commercial licensing is available for
   proprietary software.

excluded software

   we think that the libraries we choose are the best ones for parsing, or
   processing, natural languages. however we excluded some other
   interesting software, which are usually mentioned, like[121] cogcompnlp
   or [122]gate for several reasons:
     * there might have little to no documentation
     * it might have a purely educational or any non-standard license
     * it might not be designed for developers, but for end-users

summary

   in this article we have seen many ways to deal with a document in a
   natural language to get the information you need from it. most of them
   tried to find smart ways to bypass the complex task of parsing natural
   language. despite being hard to parse natural languages it is still
   possible to do so, if you use the libraries available.

   essentially, when dealing with natural languages hacking a solution is
   the suggested way of doing things, since nobody can figure out how to
   do it properly.

   where it was possible we explained the algorithms that you can use. for
   the most advanced tasks this would have been impractical, so we just
   pointed at ready-to-use libraries. in any case, if you think we missed
   something, be it a subject or an important library, you can tell us.

the guide to natural language processing

   guide to natural language processing guide to natural language
   processing

   get the guide to nlp delivered to your email and read it when you want
   on the device you want

   success! now check your email to confirm your subscription.

   there was an error submitting your subscription. please try again.
   first name ____________________
   email address ____________________
   we use this field to detect spam bots. if you fill this in, you will be
   marked as a spammer. ____________________
   [x]

   i'd like to learn more about nlp and language engineering
   (button) sign up & get the pdf [123]powered by convertkit

   tags: [124]artificial intelligence, [125]natural language, [126]nlp
   https://mk0tuzolorusfnc7thxk.kinstacdn.com/wp-content/uploads/2017/11/g
   uide-to-natural-languages.jpg 512 1024 gabriele tomassetti
   https://tomassetti.me/wp-content/uploads/2017/07/federico-tomassetti-so
   ftware-architect-300.png gabriele tomassetti2017-11-14
   18:16:422018-08-31 10:24:09analyze and understand text: guide to
   natural language processing

you might also like

   [127]create a simple parser in c# with sprache create a simple parser
   in c# with sprache
   [128]develop dsls for eclipse and intellij using xtext develop dsls for
   eclipse and intellij using xtext
   [129]getting started with antlr: building a simple expression language
   getting started with antlr: building a simple expression language
   [130]turin programming language for the jvm: building advanced lexers
   with antlr turin programming language for the jvm: building advanced
   lexers with antlr
   [131]parsing in java: tools and libraries parsing in java: tools and
   libraries
   [132]antlr mega tutorial antlr mega tutorial
   [133]getting started with antlr in c++ getting started with antlr in
   c++

do you need a parser?

   we can design parsers for new languages, or rewrite parsers for
   existing languages built in house.

   on top of parsers we can then help building interpreters, compilers,
   code generators, documentation generators, or translators (code
   converters) to other languages.
   [134]let   s talk!

course: using antlr like a professional

   [135]antlr course image antlr course image

   [136]a complete video course on parsing and antlr, that will teach you
   how to build parser for everything from programming languages to data
   formats.
   taught from professionals that build parsers for a living.

book: how to create pragmatic, lightweight languages

   [137][waaacwaaaaaaqabaeacakqbads=] [low-resolution.png]

   this [138]book on building languages is about building in a simple
   manner productive languages with parsers, compilers, interpreters,
   editors and more.

strumenta     consulting

   if you need help designing and developing dsls, languages, parsers,
   compilers, interpreters, and editors you can check the [139]services
   page of the consulting studio we founded: [140]strumenta.[141]
   strumenta logo strumenta logo

blog categories

     * [142]antlr (14)
     * [143]code processing (23)
     * [144]consulting (14)
     * [145]domain specific languages (14)
     * [146]jetbrains mps (11)
     * [147]language design (13)
     * [148]language engineering (29)
     * [149]miscellany (4)
     * [150]model driven development (4)
     * [151]non software development (3)
     * [152]open-source (8)
     * [153]parsing (19)
     * [154]research (4)
     * [155]software development (16)
     * [156]software engineering (12)
     * [157]turin programming language (4)
     * [158]whole platform (2)
     * [159]xtext (3)

menu

     * [160]about me
     * [161]how i work
     * [162]services
          + [163]antlr consulting
          + [164]jetbrains mps consulting
          + [165]code processing consulting
     * [166]language engineering
     * [167]contact me
     * [168]linkedin

blog categories

     * [169]antlr (14)
     * [170]code processing (23)
     * [171]consulting (14)
     * [172]domain specific languages (14)
     * [173]jetbrains mps (11)
     * [174]language design (13)
     * [175]language engineering (29)
     * [176]miscellany (4)
     * [177]model driven development (4)
     * [178]non software development (3)
     * [179]open-source (8)
     * [180]parsing (19)
     * [181]research (4)
     * [182]software development (16)
     * [183]software engineering (12)
     * [184]turin programming language (4)
     * [185]whole platform (2)
     * [186]xtext (3)

tags

   [187]automation [188]bytecode [189]c# [190]clojure [191]code generation
   [192]dsl [193]ebnf [194]editors [195]effectivejava [196]formats
   [197]frege [198]functional programming [199]guide [200]haskell
   [201]icse [202]image processing [203]interview [204]java
   [205]javaparser [206]javascript [207]javasymbolsolver [208]jetbrains
   mps [209]kotlin [210]language integration [211]language server protocol
   [212]language worbenches [213]libav [214]links [215]mbeddr [216]mise
   [217]open-source [218]opensource [219]programming languages [220]python
   [221]refactoring [222]review [223]roslyn [224]sparkweb
   [225]static-analysis [226]testing [227]tools [228]tripadvisor
   [229]tutorial [230]web [231]webassembly

don   t miss the next updates!

      2018 strumenta | [232]privacy policy of strumenta websites
     * [233]twitter
     * [234]linkedin
     * [235]mail
     * [236]rss

   [237]a guide to parsing: algorithms and terminology a guide to parsing:
   algorithms and terminology a guide to parsing: algorithms and
   terminology [238]understand webassembly understand webassembly
   understand webassembly: why it will change the web

   [239]scroll to top

   this site uses cookies. by continuing to browse the site, you are
   agreeing to our use of cookies.
   [240]privacy policy of strumenta websites[241]ok

   [242]miscellany

analyze and understand text: guide to natural language processing

   by gabriele tomassetti time to read: 39 min
   miscellany langdev 2018: a meeting for language engineering e   
   parsing a guide to parsing: algorithms and terminology
     *
     *
     *

references

   visible links
   1. https://tomassetti.me/feed/
   2. https://tomassetti.me/comments/feed/
   3. https://tomassetti.me/feed/
   4. http://twitter.com/ftomasse
   5. http://ie.linkedin.com/in/federicotomassetti
   6. mailto:federico@tomassetti.me
   7. https://tomassetti.me/feed/
   8. https://tomassetti.me/
   9. https://strumenta.com/services/
  10. https://strumenta.com/products
  11. https://tomassetti.me/learning-build-languages/
  12. https://tomassetti.me/learning-process-code/
  13. https://tomassetti.me/antlr-course/
  14. https://tomassetti.me/newsletter/
  15. https://tomassetti.me/about-me/
  16. https://tomassetti.me/contact-me-about-a-project/
  17. https://tomassetti.me/guide-natural-language-processing/?s=
  18. https://tomassetti.me/guide-natural-language-processing/
  19. https://tomassetti.me/blog/
  20. https://tomassetti.me/guide-natural-language-processing/
  21. https://tomassetti.me/category/miscellany/
  22. https://tomassetti.me/category/language-engineering/parsing/
  23. https://tomassetti.me/author/gtomassetti/
  24. https://tomassetti.me/guide-natural-language-processing/#id30
  25. https://tomassetti.me/guide-natural-language-processing/#id30
  26. https://tomassetti.me/guide-natural-language-processing/#parsing
  27. https://tomassetti.me/guide-natural-language-processing/#lsa
  28. https://tomassetti.me/guide-natural-language-processing/#id30
  29. https://tomassetti.me/guide-natural-language-processing/#readingtime
  30. https://tomassetti.me/guide-natural-language-processing/#readability
  31. https://tomassetti.me/guide-natural-language-processing/#identifying
  32. https://tomassetti.me/guide-natural-language-processing/#sumbasic
  33. https://tomassetti.me/guide-natural-language-processing/#graphmethods
  34. https://tomassetti.me/guide-natural-language-processing/#lsa
  35. https://tomassetti.me/guide-natural-language-processing/#lsa
  36. https://tomassetti.me/guide-natural-language-processing/#parsing
  37. https://tomassetti.me/guide-natural-language-processing/#parsing
  38. https://tomassetti.me/guide-natural-language-processing/#parsing
  39. https://en.wikipedia.org/wiki/formal_language
  40. https://tomassetti.me/guide-natural-language-processing/#classwords
  41. https://tomassetti.me/guide-natural-language-processing/#grouping
  42. https://tomassetti.me/guide-natural-language-processing/#id30
  43. https://tomassetti.me/guide-natural-language-processing/#id30
  44. https://tomassetti.me/guide-natural-language-processing/#classdocs
  45. https://tomassetti.me/guide-natural-language-processing/#measuring
  46. https://tomassetti.me/guide-natural-language-processing/#readingtime
  47. https://tomassetti.me/guide-natural-language-processing/#readability
  48. https://tomassetti.me/guide-natural-language-processing/#identifying
  49. https://tomassetti.me/guide-natural-language-processing/#understandingdocs
  50. https://tomassetti.me/guide-natural-language-processing/#summaries
  51. https://tomassetti.me/guide-natural-language-processing/#sumbasic
  52. https://tomassetti.me/guide-natural-language-processing/#graphmethods
  53. https://tomassetti.me/guide-natural-language-processing/#lsa
  54. https://tomassetti.me/guide-natural-language-processing/#othermethods
  55. https://tomassetti.me/guide-natural-language-processing/#otherapps
  56. https://tomassetti.me/guide-natural-language-processing/#parsing
  57. https://tomassetti.me/guide-natural-language-processing/#needdata
  58. https://tomassetti.me/guide-natural-language-processing/#things
  59. https://tomassetti.me/guide-natural-language-processing/#libraries
  60. https://tomassetti.me/guide-natural-language-processing/#summary
  61. http://snowballstem.org/algorithms/
  62. https://en.wikipedia.org/wiki/agglutinative_language
  63. https://en.wikipedia.org/wiki/jaccard_index
  64. https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/hollink-monolingual-2004.pdf
  65. https://github.com/ftomassetti/langgen
  66. https://github.com/valkryst/vnamegenerator
  67. http://iovs.arvojournals.org/article.aspx?articleid=2166061
  68. https://en.wikipedia.org/wiki/agglutinative_language
  69. http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml
  70. https://www.tug.org/docs/liang/
  71. https://github.com/mnater/hyphenopoly
  72. https://it.wikipedia.org/wiki/roberto_vacca#indice_di_leggibilit.c3.a0_flesch-vacca
  73. http://www.readabilityformulas.com/new-dale-chall-readability-formula.php
  74. https://en.wikipedia.org/wiki/wikipedia:language_recognition_chart
  75. https://github.com/peterc/whatlanguage
  76. http://odur.let.rug.nl/~vannoord/textcat/textcat.pdf
  77. https://github.com/wikimedia/wikimedia-textcat
  78. https://github.com/ivanakcheurov/ntextcat
  79. https://www.tensorflow.org/
  80. https://github.com/hengluchang/deep-news-summarization
  81. http://www.cs.middlebury.edu/~mpettit/tr-2005-101.pdf
  82. https://github.com/ethanmacdonald/sumbasic
  83. https://github.com/neopunisher/open-text-summarizer
  84. http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf
  85. https://github.com/jjangsangy/explaintome
  86. https://www.researchgate.net/publication/226424326_text_summarization_and_singular_value_decomposition
  87. https://github.com/miso-belica/sumy
  88. http://textmining.zcu.cz/publications/z08.pdf
  89. https://github.com/miso-belica/sumy
  90. https://sourceforge.net/projects/classifier4j/
  91. https://sourceforge.net/projects/nclassifier/
  92. https://github.com/thavelick/summarize/
  93. https://github.com/thavelick/summarize/blob/master/summarize.py
  94. https://github.com/datateaser/textteaser
  95. https://github.com/xiaoxu193/pyteaser
  96. https://radimrehurek.com/gensim/index.html
  97. http://mallet.cs.umass.edu/
  98. http://www.cs.utexas.edu/users/ml/geo.html
  99. https://en.wikipedia.org/wiki/id138
 100. https://tomassetti.me/guide-parsing-algorithms-terminology/#parsingtree
 101. http://opennlp.apache.org/
 102. http://opennlp.apache.org/docs/1.8.1/manual/opennlp.html
 103. http://opennlp.sourceforge.net/models-1.5/
 104. http://cltk.org/
 105. http://docs.cltk.org/en/latest/latin.html#clausulae-analysis
 106. http://nlp.lsi.upc.edu/freeling/
 107. http://nlp.lsi.upc.edu/freeling/demo/demo.php
 108. http://www.statmt.org/moses/index.php?n=main.homepage
 109. https://textblob.readthedocs.io/en/dev/index.html
 110. https://github.com/clips/pattern
 111. http://polyglot-nlp.com/
 112. https://github.com/thisandagain/sentiment
 113. https://github.com/vivekn/sentiment
 114. https://arxiv.org/abs/1305.6143
 115. http://sentiment.vivekn.com/
 116. https://spacy.io/
 117. https://github.com/chartbeat-labs/textacy
 118. https://stanfordnlp.github.io/corenlp/
 119. https://nlp.stanford.edu/software/lex-parser.html
 120. http://nlp.stanford.edu:8080/parser/index.jsp
 121. https://github.com/cogcomp/cogcomp-nlp
 122. https://gate.ac.uk/
 123. https://convertkit.com/?utm_source=dynamic&utm_medium=referral&utm_campaign=poweredby&utm_content=form
 124. https://tomassetti.me/tag/artificial-intelligence/
 125. https://tomassetti.me/tag/natural-language/
 126. https://tomassetti.me/tag/nlp/
 127. https://tomassetti.me/create-simple-parser-c-sprache/
 128. https://tomassetti.me/develop-dsls-for-eclipse-and-intellij/
 129. https://tomassetti.me/getting-started-with-antlr-building-a-simple-expression-language/
 130. https://tomassetti.me/turin-programming-language-for-the-jvm-building-advanced-lexers-with-antlr/
 131. https://tomassetti.me/parsing-in-java/
 132. https://tomassetti.me/antlr-mega-tutorial/
 133. https://tomassetti.me/getting-started-antlr-cpp/
 134. http://tomassetti.me/contact-me-about-a-project/
 135. https://tomassetti.me/antlr-course
 136. https://tomassetti.me/antlr-course
 137. https://tomassetti.me/create-languages
 138. https://tomassetti.me/create-languages
 139. https://strumenta.com/services
 140. https://strumenta.com/
 141. https://strumenta.com/
 142. https://tomassetti.me/category/language-engineering/antlr/
 143. https://tomassetti.me/category/static-analysis/
 144. https://tomassetti.me/category/consulting/
 145. https://tomassetti.me/category/language-engineering/domain-specific-languages/
 146. https://tomassetti.me/category/language-engineering/jetbrains-mps/
 147. https://tomassetti.me/category/language-engineering/language-design/
 148. https://tomassetti.me/category/language-engineering/
 149. https://tomassetti.me/category/miscellany/
 150. https://tomassetti.me/category/language-engineering/mdd/
 151. https://tomassetti.me/category/random-stuff/
 152. https://tomassetti.me/category/open-source/
 153. https://tomassetti.me/category/language-engineering/parsing/
 154. https://tomassetti.me/category/research/
 155. https://tomassetti.me/category/development/
 156. https://tomassetti.me/category/software-engineering/
 157. https://tomassetti.me/category/turin-programming-language/
 158. https://tomassetti.me/category/language-engineering/whole-platform/
 159. https://tomassetti.me/category/language-engineering/xtext/
 160. https://tomassetti.me/about-me/
 161. https://tomassetti.me/how-i-work/
 162. https://tomassetti.me/services/
 163. https://strumenta.com/antlr-consulting/
 164. https://tomassetti.me/jetbrains-mps-consulting/
 165. https://tomassetti.me/code-processing-consulting/
 166. https://tomassetti.me/language-engineering/
 167. https://tomassetti.me/contact-me-about-a-project/
 168. http://ie.linkedin.com/in/federicotomassetti
 169. https://tomassetti.me/category/language-engineering/antlr/
 170. https://tomassetti.me/category/static-analysis/
 171. https://tomassetti.me/category/consulting/
 172. https://tomassetti.me/category/language-engineering/domain-specific-languages/
 173. https://tomassetti.me/category/language-engineering/jetbrains-mps/
 174. https://tomassetti.me/category/language-engineering/language-design/
 175. https://tomassetti.me/category/language-engineering/
 176. https://tomassetti.me/category/miscellany/
 177. https://tomassetti.me/category/language-engineering/mdd/
 178. https://tomassetti.me/category/random-stuff/
 179. https://tomassetti.me/category/open-source/
 180. https://tomassetti.me/category/language-engineering/parsing/
 181. https://tomassetti.me/category/research/
 182. https://tomassetti.me/category/development/
 183. https://tomassetti.me/category/software-engineering/
 184. https://tomassetti.me/category/turin-programming-language/
 185. https://tomassetti.me/category/language-engineering/whole-platform/
 186. https://tomassetti.me/category/language-engineering/xtext/
 187. https://tomassetti.me/tag/automation/
 188. https://tomassetti.me/tag/bytecode/
 189. https://tomassetti.me/tag/c/
 190. https://tomassetti.me/tag/clojure/
 191. https://tomassetti.me/tag/code-generation/
 192. https://tomassetti.me/tag/dsl/
 193. https://tomassetti.me/tag/ebnf/
 194. https://tomassetti.me/tag/editors/
 195. https://tomassetti.me/tag/effectivejava/
 196. https://tomassetti.me/tag/formats/
 197. https://tomassetti.me/tag/frege/
 198. https://tomassetti.me/tag/functional-programming/
 199. https://tomassetti.me/tag/guide/
 200. https://tomassetti.me/tag/haskell/
 201. https://tomassetti.me/tag/icse/
 202. https://tomassetti.me/tag/image-processing/
 203. https://tomassetti.me/tag/interview/
 204. https://tomassetti.me/tag/java/
 205. https://tomassetti.me/tag/javaparser/
 206. https://tomassetti.me/tag/javascript/
 207. https://tomassetti.me/tag/javasymbolsolver/
 208. https://tomassetti.me/tag/jetbrains-mps/
 209. https://tomassetti.me/tag/kotlin/
 210. https://tomassetti.me/tag/language-integration/
 211. https://tomassetti.me/tag/language-server-protocol/
 212. https://tomassetti.me/tag/language-worbenches/
 213. https://tomassetti.me/tag/libav/
 214. https://tomassetti.me/tag/links/
 215. https://tomassetti.me/tag/mbeddr/
 216. https://tomassetti.me/tag/mise/
 217. https://tomassetti.me/tag/open-source/
 218. https://tomassetti.me/tag/opensource/
 219. https://tomassetti.me/tag/programming-languages/
 220. https://tomassetti.me/tag/python/
 221. https://tomassetti.me/tag/refactoring/
 222. https://tomassetti.me/tag/review/
 223. https://tomassetti.me/tag/roslyn/
 224. https://tomassetti.me/tag/sparkweb/
 225. https://tomassetti.me/tag/static-analysis/
 226. https://tomassetti.me/tag/testing/
 227. https://tomassetti.me/tag/tools/
 228. https://tomassetti.me/tag/tripadvisor/
 229. https://tomassetti.me/tag/tutorial/
 230. https://tomassetti.me/tag/web/
 231. https://tomassetti.me/tag/webassembly/
 232. https://strumenta.com/privacy-policy/
 233. http://twitter.com/ftomasse
 234. http://ie.linkedin.com/in/federicotomassetti
 235. mailto:federico@tomassetti.me
 236. https://tomassetti.me/feed/
 237. https://tomassetti.me/guide-parsing-algorithms-terminology/
 238. https://tomassetti.me/introduction-to-webassembly/
 239. https://tomassetti.me/guide-natural-language-processing/#top
 240. https://strumenta.com/privacy-policy
 241. https://tomassetti.me/guide-natural-language-processing/
 242. https://tomassetti.me/category/miscellany/

   hidden links:
 244. https://tomassetti.me/antlr-and-jetbrains-mps-parsing-files-and-display-the-ast-usign-the-tree-notation/
 245. https://tomassetti.me/langdev-2018/
 246. https://tomassetti.me/guide-parsing-algorithms-terminology/
 247. https://tomassetti.me/guide-natural-language-processing/
 248. https://tomassetti.me/guide-natural-language-processing/
 249. https://tomassetti.me/guide-natural-language-processing/
