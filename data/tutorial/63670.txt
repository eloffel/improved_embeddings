   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

linguistics, nlp, and interdisciplinarity
or: look at your data

   [9]go to the profile of emily m. bender
   [10]emily m. bender (button) blockedunblock (button) followfollowing
   nov 6, 2017

prologue

   this is meant to be a brief blog post reflecting on a couple of twitter
   megathreads i   ve been involved in over the past week. this is my
   attempt to summarize my side of the story, while hopefully not
   mis-representing anyone else. some of this concerns specific papers, in
   one case anonymous, in another case not. let me say at the outset that
   wang & eisner acknowledge an important error. in the post, i   m going to
   talk about that error from my point of view before i get to their
   acknowledgment, but the point is only to relay the story of what
   unfolded in that megathread and not to suggest that they haven   t
   addressed the issue.

the beginning

   it starts with this tweet of mine, on thursday nov 2, borne of
   frustration while reviewing:

   iframe: [11]/media/efbc1c0a1510a6f03a65006af9a788b1?postid=e49e03d37c9c

   in particular, i was reading a paper that was making claims of
   cross-linguistic applicability (being able to parse text in a new
   language, without labeled training data in that language) and testing
   them with the [12]universal dependencies (ud) treebanks. i happen to
   know that the ud treebanks         being collected more or less
   opportunistically, rather than deliberately sampling the way a
   typologist would         include sets of quite closely related languages. so
   i was of course curious to see whether the evaluation respected this
   fact, or whether the scandinavian languages in the set (danish,
   swedish, norwegian) say, or the romance languages (french, italian,
   portuguese, spanish, romanian, latin) or the slavic languages (church
   slavic, croatian, czech, slovenian) were split across training and
   test. this, in my opinion, would significantly weaken any claim of
   language independence.

   the paper doesn   t actually list the languages. rather, the results
   tables give the identifiers of each treebank. squinting at those,
   trying to work out where the romance languages etc. were, i noticed
   something worse: finnish and latin are represented by two treebanks
   each in the ud 1.2 release and, in each case, one of the treebanks was
   in train and one was in test.

   looking a bit further, i found that this paper was basing its
   train/test split off of that in previous work (wang & eisner [13]2016,
   [14]2017) from which they were borrowing also the galactic dependencies
   dataset (dependency treebanks for synthetic languages).

   how could this have come about? my guess was that it connects to a
   trend that i have noticed in nlp to avoid looking at the data and
   another trend that of treating all language(s) as interchangeable.
   (this is what leads people to fail to name the language they are
   working on when that language is english, as if english were a suitable
   stand in for all languages.) it is only because this is accepted that
   it is seen as acceptable practice to not name the languages in a study
   like this one, but only put the treebank identifiers in the table. and
   because that is acceptable, it   s possible to miss that you   ve got the
   same language in both training & test, even if you   re trying to show
   that a method can generalize to new (i.e. unseen) languages.

   i was frustrated that wang & eisner had put out a dataset with this
   kind of train/test split and also that two sets of tacl reviewers (wang
   & eisner 2016 and 2017 both appeared in tacl) missed it. based on the
   discussion i   ve had with wang & eisner, on twitter, [15]github and over
   email, it   s clear that they had thought about the question. i think the
   reviewers should have asked, which i think would have prompted wang &
   eisner to include the discussion there.

   wang & eisner point out that in the work where they set that train/test
   split they aren   t claiming language independence (especially in the
   2016 paper) and so repeating languages between train/dev and test isn   t
   as problematic as i said. (for some of that, see the exchange on the
   [16]gd github page.) they agree that it is misleading to confuse
      language    and    treebank    in their paper and talk about    37 languages   
   when the 37 ud 1.2 treebanks represent 33 languages. they have tweeted
   that they are working on an update to clarify this         i look forward to
   that!

   i was most frustrated that the authors of the original paper i was
   reading picked up wang & eisner   s train/test split and apparently used
   it without considering whether it was appropriate to test their
   hypothesis with.

megathread #1

   from the massive twitter megathread that followed, i think the
   following are the most important points:

   1. language independence

   there is on-going disagreement as to what this term means. when i
   encounter it, i assume it means    works reasonably similarly well across
   all human languages   , unless it   s modified to suggest a narrower set
   (widely spoken languages, indo-european languages, etc.). yoav goldberg
   in this thread (and others before) have asserted that means something
   more like    can be run on any human language   . this strikes me as a
   singularly uninteresting thing to claim and also an unlikely
   interpretation when the claim comes together with e.g. fluff about how
   humans learn language. even without that fluff, just in the context of
      can work without labeled training data    suggests something more like
   my interpretation. because if what you   re saying is    will run without
   labeled training data, but will give back utter garbage for all i
   know   , how is that an interesting claim?

   more generally: i don   t actually care whether or not people develop
   language independent nlp systems (in my sense). (okay, maybe i care a
   little         the [17]grammar matrix project is after all pushing in that
   direction and we do test each addition with held-out languages from
   held-out language families.)

   but in the broader nlp context, what i care about is that people match
   their experiments to their claims. so if you want to claim language
   independence, then you   ll need to spend some time thinking about how
   you   ll establish it. training something on english and testing on
   french just isn   t going to cut it. (for more on this, see my [18]2011
   paper in lilt.)

   now, the experiments we can run are limited by the resources we have
   available. the ud project is making things much better than they have
   been, but the ud languages (as noted above) aren   t chosen to be a
   legitimate typological sample. however, we can still pay attention to
   how to make the most of the resources we have. and given that closely
   related languages are structurally very similar to each other, a good
   first step would be to keep the subfamilies of indo-european together,
   i.e. all in train or all in test. i   ve emailed with joakim nivre about
   making language family information more immediately obvious on the ud
   webpage and he says he   ll work towards that in an up-coming release.
   yay!

   [update 11/17/17: ud 2.1 was released two days ago and now the
   [19]universal dependencies web page foregrounds language family and
   subfamily information.]

   2. look at your data

   i contend that we can   t do good science if we don   t understand the data
   we are working with. say technique x works better than technique y on
   some task. that information isn   t all that useful without some sense of
   why x should work better than y. what kind of information is it
   capturing that y isn   t? those kinds of questions can   t be answered
   without getting our hands dirty with data         i.e. with careful error
   analysis. x and y might be the published system and the previous state
   of the art. or they might be two iterations of a system under
   development. careful error analysis can help point to how a system can
   be improved, too.

   in this connection, hal daume iii tweeted:

   iframe: [20]/media/242916be09ae01a302933e4a76f57cb4?postid=e49e03d37c9c

   iframe: [21]/media/7aca14d3387a09d858953f45e0097b0d?postid=e49e03d37c9c

   iframe: [22]/media/10fea7fa3ce85601e3e7d605ba11752e?postid=e49e03d37c9c

   i can understand that there are good reasons for keeping test data
   blind. but then error analysis should be done on dev data. and
   *someone* has to have responsibility for making sure the test data is
   appropriate for testing the claims at hand. that means knowing
   something about the data, even if you don   t examine it in detail. and
   knowing what language(s) the data is drawn from is the very minimum.

   3. cite the linguistics literature

   one of my other frustrutations with the way nlp and linguistics do(n   t)
   interact is the lack of citaitons from nlp into the linguistics
   literature. there   s two kinds of ways this happens: first, nlp authors
   might not know enough linguistics to be able to do the literature
   search to find the linguistic work that could actually inform their
   nlp. second, nlp authors might know the relevant linguistic points
   themselves, but then just write about them as if they were common
   knowledge (e.g. there are typological tendencies by which languages
   with ov order tend to have np-p order within adpositional phrases   ).
   this obscures the fact that these are actually research results which
   (a) reflect people   s work & ingenuity and (b) should be traceable and
   checkable.

   4. talk to experts!

   part way through the thread, brendan o   connor tweeted:

   iframe: [23]/media/20e3438f037aac7c62fe8a75fab2863e?postid=e49e03d37c9c

   my first response to this is to say that the solution is obvious:
   collaborate with experts in x! and i think that   s the solution. i think
   the tweet points to a deeper problem though. it seems to suggest (note
   the hedge: this is not what brendan said) that some people in cs think
   that it   s okay to publish work on x that you wouldn   t be comfortable
   having critically reviewed by experts in x. if that is a belief that   s
   out there, it strikes me as terribly arrogant.

   even more importantly, though: if the point of using nlp or any other
   area as an application area for ml is to show what ml can do, then i
   don   t think this can be done effectively without having domain experts
   in the mix. if an ml person who doesn   t know anything about x creates a
   dataset related to x and    solves    the task the dataset represents    
   that tells us precisely nothing. and even if the ml person didn   t
   create the dataset, without serious discussion of what the dataset
   represents and how it relates to some larger problem of interest, we   ve
   learned nothing.

   the pithiest way of saying this that i   ve come up with is:

   iframe: [24]/media/24a1b9d37cf42fe8bac063ca28214059?postid=e49e03d37c9c

   5. this is reviewers    responsibility too

   i   m asking for culture change here and that has to come in part through
   our reviewing. a lot of this connects to bonnie webber   s points in
   [25]this comment on the coling 2018 pc blog post about review forms.
   specifically she says that reviewers should ask the following the
   questions:

         is it clear what the authors    hypothesis is? what is it?
         is it clear how the authors have tested their hypothesis?
         is it clear how the results confirm/refute the hypothesis, or are
     the results
     inconclusive?
         do the authors explain how the results follow from their
     hypothesis (as
     opposed to say, other possible confounding factor)?

   my version of those includes:
     * is it clear what data was used to test the hypothesis?
     * is that data appropriate for testing the hypothesis?

   beyond that, i   d add:
     * is the work grounded appropriately in the relevant literature, not
       just from nlp/ml but also from linguistics?

megathread #2

   the first megathread seemed to wrap up on sunday night, but then a new
   one was launched, by kyunghun cho:

   iframe: [26]/media/f579e1d9861eb06578b138899ad95c5e?postid=e49e03d37c9c

   this one turned into another iteration of the debate about the
   relationship between linguistics and nlp.

   ryan cotterell ([27]@_shrdlu_) put a lot of energy into trying to
   convince everyone that nlp isn   t    interdisciplinary    based on an
   extremely stringent definition of    interdisciplinary    (must build on
   current work from both disciplines) and current practice in nlp (most
   of it doesn   t).

   to this, i   d like to make the following replies: an area of research is
   interdisciplinary in principle if the questions it asks require
   expertise from multiple areas to effectively approach. it is
   interdisciplinary in practice if that expertise is actually applied. a
   field that is interdisciplinary in principle but not in practice is
   clearly in trouble.

   by my definition, nlp is interdisciplinary in principle and i agree
   with ryan that it is not sufficiently interdisciplinary in practice,
   though i don   t think it needs to hit his high bar to succeed. (and he
   doesn   t think that   s required either.) similarly, i don   t think that
   all subfields of linguistics are equally relevant to nlp         and any
   given subfield of linguistics will be potentially relevant to only
   certain parts of nlp.

   my message is: learn something about how language works and/or
   collaborate with people who have that expertise; it   ll lead to be
   better nlp.

   the argument in the second megathread struck me as terribly
   counter-productive, especially since most of the people tweeting were
   people who usually support the notion of more linguistically-informed
   nlp. the high-bar definition of    interdisciplinary    is unhelpful: i
   don   t want people walking away thinking,    if i can   t undesrtand a
   grad-level class in linguistics, then i can   t do interdisciplinary
   work.    nor do i want people taking away the impression    linguistics is
   irrelevant.   

   perhaps the most frustrating part of this thread was the way in which
   it erased both the areas of linguistics and the areas of cl/nlp that i
   personally work in. for much of it, it seemed that    linguistics    was
   equated with    current chomskyan syntax    (there was even a subthread
   about whether or not sociolinguistics counts as linguistics (!)). on
   the other hand, the bald assertations that nlp in general (as
   extrapolated from    most work in nlp   ) doesn   t use linguistics suggests
   that the work that does (which includes mine) doesn   t    count   .

   so hey world, there   s more to linguistics than chomsky   s latest. and we
   should expect more of nlp than just numbers on some black-box dataset
   that   s trendy right now on arxiv.

     * [28]machine learning
     * [29]linguistics
     * [30]computational linguistics
     * [31]naturallanguageprocessing

   (button)
   (button)
   (button) 437 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of emily m. bender

[33]emily m. bender

   professor, linguistics, university of washington// faculty director,
   professional ms program in computational linguistics (clms)
   faculty.washington.edu/ebender

     * (button)
       (button) 437
     * (button)
     *
     *

   [34]go to the profile of emily m. bender
   never miss a story from emily m. bender, when you sign up for medium.
   [35]learn more
   never miss a story from emily m. bender
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e49e03d37c9c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@emilymenonbender/linguistics-nlp-and-interdisciplinarity-or-look-at-your-data-e49e03d37c9c&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@emilymenonbender/linguistics-nlp-and-interdisciplinarity-or-look-at-your-data-e49e03d37c9c&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@emilymenonbender?source=post_header_lockup
  10. https://medium.com/@emilymenonbender
  11. https://medium.com/media/efbc1c0a1510a6f03a65006af9a788b1?postid=e49e03d37c9c
  12. http://universaldependencies.org/
  13. http://cs.jhu.edu/~jason/papers/wang+eisner.tacl16.pdf
  14. http://cs.jhu.edu/~jason/papers/wang+eisner.tacl17.pdf
  15. https://github.com/gdtreebank/gdtreebank/issues/2
  16. https://github.com/gdtreebank/gdtreebank/issues/2
  17. http://matrix.delph-in.net/
  18. http://journals.linguisticsociety.org/elanguage/lilt/article/view/2624.html
  19. http://universaldependencies.org/
  20. https://medium.com/media/242916be09ae01a302933e4a76f57cb4?postid=e49e03d37c9c
  21. https://medium.com/media/7aca14d3387a09d858953f45e0097b0d?postid=e49e03d37c9c
  22. https://medium.com/media/10fea7fa3ce85601e3e7d605ba11752e?postid=e49e03d37c9c
  23. https://medium.com/media/20e3438f037aac7c62fe8a75fab2863e?postid=e49e03d37c9c
  24. https://medium.com/media/24a1b9d37cf42fe8bac063ca28214059?postid=e49e03d37c9c
  25. http://coling2018.org/call-for-input-paper-types-and-associated-review-forms/#comment-125
  26. https://medium.com/media/f579e1d9861eb06578b138899ad95c5e?postid=e49e03d37c9c
  27. http://twitter.com/_shrdlu_
  28. https://medium.com/tag/machine-learning?source=post
  29. https://medium.com/tag/linguistics?source=post
  30. https://medium.com/tag/computational-linguistics?source=post
  31. https://medium.com/tag/naturallanguageprocessing?source=post
  32. https://medium.com/@emilymenonbender?source=footer_card
  33. https://medium.com/@emilymenonbender
  34. https://medium.com/@emilymenonbender
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/e49e03d37c9c/share/twitter
  38. https://medium.com/p/e49e03d37c9c/share/facebook
  39. https://medium.com/p/e49e03d37c9c/share/twitter
  40. https://medium.com/p/e49e03d37c9c/share/facebook
