   #[1]rss

[2]fast.ai

   making neural nets uncool again

   [3]home [4]about [5]our mooc [6]posts by topic

      fast.ai 2019. all rights reserved.

how (and why) to create a good validation set

   written: 13 nov 2017 by rachel thomas

   an all-too-common scenario: a seemingly impressive machine learning
   model is a complete failure when implemented in production. the fallout
   includes leaders who are now skeptical of machine learning and
   reluctant to try it again. how can this happen?

   one of the most likely culprits for this disconnect between results in
   development vs results in production is a poorly chosen validation set
   (or even worse, no validation set at all). depending on the nature of
   your data, choosing a validation set can be the most important step.
   although sklearn offers a [7]train_test_split method, this method takes
   a random subset of the data, which is a poor choice for many real-world
   problems.

   the definitions of training, validation, and test sets can be fairly
   nuanced, and the terms are sometimes inconsistently used. in the deep
   learning community,    test-time id136    is often used to refer to
   evaluating on data in production, which is not the technical definition
   of a test set. as mentioned above, sklearn has a train_test_split
   method, but no train_validation_test_split. kaggle only provides
   training and test sets, yet to do well, you will need to split their
   training set into your own validation and training sets. also, it turns
   out that kaggle   s test set is actually sub-divided into two sets. it   s
   no suprise that many beginners may be confused! i will address these
   subtleties below.

first, what is a    validation set   ?

   when creating a machine learning model, the ultimate goal is for it to
   be accurate on new data, not just the data you are using to build it.
   consider the below example of 3 different models for a set of data:

   under-fitting and over-fitting

             source: andrew ng's machine learning coursera class

   the error for the pictured data points is lowest for the model on the
   far right (the blue curve passes through the red points almost
   perfectly), yet it   s not the best choice. why is that? if you were to
   gather some new data points, they most likely would not be on that
   curve in the graph on the right, but would be closer to the curve in
   the middle graph.

   the underlying idea is that:
     * the training set is used to train a given model
     * the validation set is used to choose between models (for instance,
       does a id79 or a neural net work better for your problem?
       do you want a id79 with 40 trees or 50 trees?)
     * the test set tells you how you   ve done. if you   ve tried out a lot
       of different models, you may get one that does well on your
       validation set just by chance, and having a test set helps make
       sure that is not the case.

   a key property of the validation and test sets is that they must be
   representative of the new data you will see in the future. this may
   sound like an impossible order! by definition, you haven   t seen this
   data yet. but there are still a few things you know about it.

when is a random subset not good enough?

   it   s instructive to look at a few examples. although many of these
   examples come from kaggle competitions, they are representative of
   problems you would see in the workplace.

time series

   if your data is a time series, choosing a random subset of the data
   will be both too easy (you can look at the data both before and after
   the dates your are trying to predict) and not representative of most
   business use cases (where you are using historical data to build a
   model for use in the future). if your data includes the date and you
   are building a model to use in the future, you will want to choose a
   continuous section with the latest dates as your validation set (for
   instance, the last two weeks or last month of the available data).

   suppose you want to split the time series data below into training and
   validation sets:
   time series data time series data

   a random subset is a poor choice (too easy to fill in the gaps, and not
   indicative of what you   ll need in production):
   a poor choice for your training set a poor choice for your training set

   use the earlier data as your training set (and the later data for the
   validation set):
   a better choice for your training set a better choice for your training
   set

   kaggle currently has a competition to [8]predict the sales in a chain
   of ecuadorian grocery stores. kaggle   s    training data    runs from jan 1
   2013 to aug 15 2017 and the test data spans aug 16 2017 to aug 31 2017.
   a good approach would be to use aug 1 to aug 15 2017 as your validation
   set, and all the earlier data as your training set.

new people, new boats, new   

   you also need to think about what ways the data you will be making
   predictions for in production may be qualitatively different from the
   data you have to train your model with.

   in the kaggle [9]distracted driver competition, the independent data
   are pictures of drivers at the wheel of a car, and the dependent
   variable is a category such as texting, eating, or safely looking
   ahead. if you were the insurance company building a model from this
   data, note that you would be most interested in how the model performs
   on drivers you haven   t seen before (since you would likely have
   training data only for a small group of people). this is true of the
   kaggle competition as well: the test data consists of people that
   weren   t used in the training set.

    two images of the same person talking on the phone while driving. two
        images of the same person talking on the phone while driving.

   if you put one of the above images in your training set and one in the
   validation set, your model will seem to be performing better than it
   would on new people. another perspective is that if you used all the
   people in training your model, your model may be overfitting to
   particularities of those specific people, and not just learning the
   states (texting, eating, etc).

   a similar dynamic was at work in the [10]kaggle fisheries competition
   to identify the species of fish caught by fishing boats in order to
   reduce illegal fishing of endangered populations. the test set
   consisted of boats that didn   t appear in the training data. this means
   that you   d want your validation set to include boats that are not in
   the training set.

   sometimes it may not be clear how your test data will differ. for
   instance, for a problem using satellite imagery, you   d need to gather
   more information on whether the training set just contained certain
   geographic locations, or if it came from geographically scattered data.

the dangers of cross-validation

   the reason that sklearn doesn   t have a train_validation_test split is
   that it is assumed you will often be using cross-validation, in which
   different subsets of the training set serve as the validation set. for
   example, for a 3-fold cross validation, the data is divided into 3
   sets: a, b, and c. a model is first trained on a and b combined as the
   training set, and evaluated on the validation set c. next, a model is
   trained on a and c combined as the training set, and evaluated on
   validation set b. and so on, with the model performance from the 3
   folds being averaged in the end.

   however, the problem with cross-validation is that it is rarely
   applicable to real world problems, for all the reasons describedin the
   above sections. cross-validation only works in the same cases where you
   can randomly shuffle your data to choose a validation set.

kaggle   s    training set    = your training + validation sets

   one great thing about kaggle competitions is that they force you to
   think about validation sets more rigorously (in order to do well). for
   those who are new to kaggle, it is a platform that hosts machine
   learning competitions. kaggle typically breaks the data into two sets
   you can download:
    1. a training set, which includes the independent variables, as well
       as the dependent variable (what you are trying to predict). for the
       example of an ecuadorian grocery store trying to predict sales, the
       independent variables include the store id, item id, and date; the
       dependent variable is the number sold. for the example of trying to
       determine whether a driver is engaging in dangerous behaviors
       behind the wheel, the independent variable could be a picture of
       the driver, and the dependent variable is a category (such as
       texting, eating, or safely looking forward).
    2. a test set, which just has the independent variables. you will make
       predictions for the test set, which you can submit to kaggle and
       get back a score of how well you did.

   this is the basic idea needed to get started with machine learning, but
   to do well, there is a bit more complexity to understand. you will want
   to create your own training and validation sets (by splitting the
   kaggle    training    data). you will just use your smaller training set (a
   subset of kaggle   s training data) for building your model, and you can
   evaluate it on your validation set (also a subset of kaggle   s training
   data) before you submit to kaggle.

   the most important reason for this is that kaggle has split the test
   data into two sets: for the public and private leaderboards. the score
   you see on the public leaderboard is just for a subset of your
   predictions (and you don   t know which subset!). how your predictions
   fare on the private leaderboard won   t be revealed until the end of the
   competition. the reason this is important is that you could end up
   overfitting to the public leaderboard and you wouldn   t realize it until
   the very end when you did poorly on the private leaderboard. using a
   good validation set can prevent this. you can check if your validation
   set is any good by seeing if your model has similar scores on it to
   compared with on the kaggle test set.

   another reason it   s important to create your own validation set is that
   kaggle limits you to two submissions per day, and you will likely want
   to experiment more than that. thirdly, it can be instructive to see
   exactly what you   re getting wrong on the validation set, and kaggle
   doesn   t tell you the right answers for the test set or even which data
   points you   re getting wrong, just your overall score.

   understanding these distinctions is not just useful for kaggle. in any
   predictive machine learning project, you want your model to be able to
   perform well on new data.
   this post is tagged: [ [11]technical ] (click a tag for more posts in
   that category).

related posts

     * [12]fast.ai embracing swift for deep learning 06 mar 2019
     * [13]a conversation about tech ethics with the new york times chief
       data scientist 04 mar 2019
     * [14]dairy farming, solar panels, and diagnosing parkinson's
       disease: what can you do with deep learning? 21 feb 2019

references

   1. https://www.fast.ai/2017/11/13/validation-sets/atom.xml
   2. https://www.fast.ai/
   3. https://www.fast.ai/
   4. https://www.fast.ai/about/
   5. http://course.fast.ai/
   6. https://www.fast.ai/topics
   7. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
   8. https://www.kaggle.com/c/favorita-grocery-sales-forecasting
   9. https://www.kaggle.com/c/state-farm-distracted-driver-detection
  10. https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring
  11. https://www.fast.ai/tag/technical
  12. https://www.fast.ai/2019/03/06/fastai-swift/
  13. https://www.fast.ai/2019/03/04/ethics-framework/
  14. https://www.fast.ai/2019/02/21/dl-projects/
