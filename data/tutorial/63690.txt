   #[1]github [2]recent commits to yaraparser:transfer

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]1
     * [35]star [36]3
     * [37]fork [38]11

[39]rasoolims/[40]yaraparser forked from [41]yahoo/yaraparser

   [42]code [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   yara k-beam arc-eager dependency parser
     * [47]59 commits
     * [48]2 branches
     * [49]3 releases
     * [50]fetching contributors
     * [51]apache-2.0

    1. [52]java 100.0%

   (button) java
   branch: transfer (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/r
   [54]download zip

downloading...

   want to be notified of new releases in rasoolims/yaraparser?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]pull request [62]compare this branch is 7 commits ahead of
   yahoo:master.
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [63]permalink
   type       name      latest commit message commit time
        failed to load latest commit information.
        [64]meta-inf
        [65]punc_files
        [66]sample_data
        [67]src
        [68]license.md
        [69]readme.md
        [70]license

readme.md

yara parser

      copyright 2014-2015, yahoo! inc; 2015-2016 mohammad sadegh rasooli.

      licensed under the terms of the apache license 2.0. see license file
   at the project root for terms.

yara transfer parser

   this is a modification to the yara parser for parser transfer.

the main change

   the parser assumes that the training treebank can be from different
   languages. the third column shows the original word in that language
   and the second column shows the translation (use ``_'' for unknown
   translations). the trainer can get two brown cluster dictionaries:

   -cluster [cluster-file] this cluster is cross-lingual and the parser
   looks at the third column.

   -cluster_id [cluster-file] this cluster is monolingual and the parser
   looks at the second column.

citation:

     * mohammad sadegh rasooli and michael collins. "cross-lingual
       syntactic transfer with limited resources", in press.

the original readme file (note that the cluster feature option is changed):

yara k-beam arc-eager dependency parser

   this core functionality of the project is implemented by mohammad
   sadegh rasooli during his internship in yahoo! labs and it was later
   modified in columbia university. for more details, see the technical
   details. the parser can be trained on any syntactic dependency treebank
   with conll'06 format and can parse sentences afterwards. it can also
   parse partial sentences (a sentence with partial gold dependencies) and
   it is also possible to train on partial trees.

   please cite the following technical report if you use yara in your
   research:
     * mohammad sadegh rasooli and joel tetreault. [71]yara parser: a fast
       and accurate dependency parser. arxiv:1503.06733v2 [cs.cl] mar
       2015.

version log

     * v0.3 (21 apr. 2016) fix a bug in third-order feature computation.
     * v0.2.1 (18 mar. 2015) some minor changes to packaging and
       punctuation defaults.
     * v0.2 (10 feb. 2015) some problems fixed in search pruning and
       dependency features, and brown cluster features added; compressed
       model file saving.
     * v0.1 (january 2015) first version of the parser with features
       roughly the same as zhang and nivre (2011).

warning

   if you use the extended feature set or brown cluster features,
   currently the parser supports just 64 unique dependency relations and
   1m unique words in the training data. if the number of unique relations
   in your training data is more than 64, your results with extended or
   brown cluster features may not be precise!

performance and speed on wsj/id32 (v0.2)

   performance and speed really depends on the quality of pos taggers and
   machine power and memory. i used [72]my own pos tagger v0.2 and tagged
   the train file with 10-way jackknifing. i got pos accuracy of 97.14,
   97.18 and 97.37 in the train, dev and test files respectively. i
   converted the data to dependencies with [73]penn2malt tool. the
   following table shows the results.
   yaraparser.parser dep. rep. beam features iter# dev uas test uas test
   las sen/sec
   zpar penn2malt 64 [74]zn (11) 15 93.14 92.9 91.8 29
   yara penn2malt 1 [75]zn (11) (11) basic 6 89.54 89.34 88.02 6000
   yara penn2malt 1 [76]zn (11) (11) + bc 13 89.98 89.74 88.52 1300
   yara penn2malt 64 [77]zn (11) (11) 13 93.31 92.97 91.93 133
   yara penn2malt 64 [78]zn (11) (11) + bc 13 93.42 93.32 92.32 45

meaning of yara

   yara (yahoo-rasooli) is a word that means strength , boldness, bravery
   or something robust in persian. in other languages it has other
   meanings, e.g. in amazonian language it means daughter of the forests,
   in malaysian it means the beams of the sun and in ancient egyptian it
   means pure and beloved.

compilation

   go to the root directory of the project and run the following command:
javac yaraparser.parser/yaraparser.java

   then you can test the code via the following command:
java yaraparser.parser.yaraparser

   or
java yaraparser.parser/yaraparser

command line options

   note: all the examples bellow are using the jar file for running the
   application but you can also use the java class files after manually
   compiling the code.

train a yaraparser.parser

   warning: the training code ignores non-projective trees in the training
   data. if you want to include them, try to projectivize them; e.g. by
   [79]tree projectivizer.
     * java -jar jar/yaraparser.jar train -train-file [train-file] -dev
       [dev-file] -model [model-file] -punc [punc-file]
          + the model for each iteration is with the pattern
            [model-file]_iter[iter#]; e.g. mode_iter2
          + [punc-file]: file contains list of pos tags for punctuations
            in the treebank, each in one line
          + other options (there are 128 combinations of options and also
            beam size and thread but the default is the best setting given
            a big beam (e.g. 64)) * -cluster [cluster-file] brown cluster
            file: at most 4096 clusters are supported by the parser
            (default: empty)
* the format should be the same as https://github.com/percyliang/brown-cluster/b
lob/master/output.txt

               o beam:[beam-width] (default:64)
               o iter:[training-iterations] (default:20)
               o unlabeled (default: labeled parsing, unless explicitly
                 put `unlabeled')
               o lowercase (default: case-sensitive words, unless
                 explicitly put 'lowercase')
               o basic (default: use extended feature set, unless
                 explicitly put 'basic')
               o static (default: use dynamic oracles, unless explicitly
                 put `static' for static oracles)
               o early (default: use max violation update, unless
                 explicitly put `early' for early update)
               o random (default: choose maximum scoring oracle, unless
                 explicitly put `random' for randomly choosing an oracle)
               o nt:[#_of_threads] (default:8)
               o root_first (default: put root in the last position,
                 unless explicitly put 'root_first')

parse a conll_2006 file

     * java -jar jar/yaraparser.jar parse_conll -input [test-file] -out
       [output-file] -model [model-file]
          + the test file should have the conll 2006 format
          + optional: nt:#_of_threads (default:8)
          + optional: -score [score file] averaged score of each output
            parse tree in a file

parse a pos tagged file

     * java -jar jar/yaraparser.jar parse_tagged -input [test-file] -out
       [output-file] -model [model-file]
          + the test file should have each sentence in line and word_tag
            pairs are space-delimited
          + optional: -delim [delim] (default is _)
          + optional: nt:#_of_threads (default:8)
          + example line: he_prp is_vbz nice_aj ._.

train a yaraparser.parser with partial training data

   there are some occasions that the training data does not always have
   full trees. in such cases, you can still train a parsing model with
   conll format but the words that do not have a head, should have -1 as
   their heads. for the partial trees in the training data, dynamic
   oracles will be applied, regardless of your choice of using static or
   dynamic oracles.

   the parser starts parsing only on full trees for 2 iterations and then
   works on partial trees as well. if you want to change its default (3),
   use pt:#pt (e.g. pt:5) as an argument.

   note: if there is a tree in the data that has a cycle or cannot be
   projectivized at all, the trainer gives a message no oracle(sen#) and
   ignores that specific sentence and continues its training procedure.
   there is no crucial difference in the command line options compared to
   training a parser on full trees.

   warning training on partial trees is noisy because the dynamic oracle
   decides about what path to choose as a gold tree and in many cases, the
   choice by the dynamic oracle is not correct, leading to noisy data
   situation.

parse a partial tree with some gold dependencies

   note: there are some occasions where you need to parse a sentence, but
   already know about some of its dependencies. yara tries to find the
   best parse tree for a sentence: 1) if the original partial tree is
   projective and there is at least one way to fill in the other heads and
   preserve projectivity, all gold parses will be preserved, 2) if there
   is some nonprojectivity or loop in the partial tree, some or even all
   of gold dependencies will be ignored.

   warning because of some technical reasons, all words connected to the
   dummy root word, will be labeled as root. if your treebank convention
   is different, try to refactor the root dependency in the final output.
     * _java -jar yaraparser.jar parse_partial -input [test-file] -out
       [output-file] -model [model-file] nt:[#of_threads (optional --
       default:8)]
          + the test file should have the conll 2006 format; each word
            that does not have a parent, should have a -1 parent-index
          + optional: -score [score file] averaged score of each output
            parse tree in a file

evaluate the yaraparser.parser

   warning the evaluation script is yara, takes care of root output, so
   you do not have to change anything in the output.
     * java -jar yaraparser.jar eval -gold [gold-file] -parse
       [parsed-file] -punc [punc-file]
          + [punc-file]: file contains list of pos tags for punctuations
            in the treebank, each in one line
          + both files should have conll 2006 format

example usage

   there is small portion from google universal treebank for the german
   language in the sample_data directory.

training without brown cluster features

 java -jar jar/yaraparser.jar train  -train-file sample_data/train.conll  -dev s
ample_data/dev.conll -model /tmp/model iter:10  -punc punc_files/google_universa
l.puncs

   you can kill the process whenever you find that the model performance
   is converging on the dev data. the parser achieved an unlabeled
   accuracy 87.52 and labeled accuracy 81.15 on the dev set in the 7th
   iteration.

   performance numbers are produced after each iteration. the following is
   the performance on the dev after the 10th iteration:
1.07 ms for each arc!
14.18 ms for each sentence!

labeled accuracy: 79.93
unlabeled accuracy:  87.15
labeled exact match:  22.54
unlabeled exact match:  45.07

   next, you can run the developed model on the test data:
 java -jar jar/yaraparser.jar parse_conll -input sample_data/test.conll -model /
tmp/model_iter10  -out /tmp/test.output.conll

   you can finally evaluate the output data:
java -jar jar/yaraparser.jar eval  -gold sample_data/test.conll  -parse /tmp/tes
t.output.conll

labeled accuracy: 70.91
unlabeled accuracy:  75.97
labeled exact match:  17.54
unlabeled exact match:  28.07

training with brown cluster features

   you can apply the same way you did without brown cluster features but
   with -cluster option.
java -jar jar/yaraparser.jar train  -train-file sample_data/train.conll  -dev sa
mple_data/dev.conll -model /tmp/model iter:10  -punc punc_files/google_universal
.puncs  -cluster sample_data/german_clusters_europarl_universal_train.cluster

api usage

   you can look at the class parser/api_usageexample to see an example of
   using the parser inside your code.

notes

how to create word clusters?

   given a tokenized raw text file, you can use [80]percy liang's brown
   id91 code to cluster the words. i put the cluster files for
   english and german but if you think you have a bigger text file for
   those you can use them as long as the formatting is ok.

memory size

   for very large training sets, you may need to increase the java memory
   heap size by -xmx option; e.g. java -xmx10g jar/yaraparser.jar.

technical details

   this parser is an implementation of the arc-eager dependency model
   [nivre, 2004] with averaged structured id88 [collins, 2002]. the
   feature setting is from zhang and nivre [2011] with additional brown
   cluster features inspired from koo et al. [2008] and honnibal and
   johnson [2014]. the model can be trained with early update strategy
   [collins and roark, 2004] or max-violation update [huang et al., 2012].
   oracle search for training is done with either dynamic oracles
   [goldberg and nivre, 2013] or original static oracles. choosing the
   best oracles in the dynamic oracle can be done via latent structured
   id88 [sun et al., 2013] and also randomly. the dummy root token
   can be placed in the end or in the beginning of the sentence
   [ballesteros and nivre, 2013]. when the dummy root token is placed in
   the beginning, tree constraints are applied [nivre and
   fern  ndez-gonz  lez, 2014].

   ##references

   [ballesteros and nivre, 2013] ballesteros, miguel, and joakim nivre.
   "going to the roots of id33." computational linguistics
   39.1 (2013): 5-13.

   [collins, 2002] collins, michael. "discriminative training methods for
   id48: theory and experiments with id88
   algorithms." proceedings of the acl-02 conference on empirical methods
   in natural language processing-volume 10. association for computational
   linguistics, 2002.

   [collins and roark, 2004] collins, michael, and brian roark.
   "incremental parsing with the id88 algorithm." proceedings of the
   42nd annual meeting on association for computational linguistics.
   association for computational linguistics, 2004.

   [goldberg and nivre, 2013] goldberg, yoav, and joakim nivre. "training
   deterministic parsers with non-deterministic oracles." tacl 1 (2013):
   403-414.

   [honnibal and johnson] honnibal, matthew, and mark johnson. "joint
   incremental disfluency detection and id33." transactions
   of the association for computational linguistics 2 (2014): 131-142.

   [huang et al., 20012] huang, liang, suphan fayong, and yang guo.
   "structured id88 with inexact search." proceedings of the 2012
   conference of the north american chapter of the association for
   computational linguistics: human language technologies. association for
   computational linguistics, 2012.

   [koo et al., 2008] koo, terry, xavier carreras, and michael collins.
   "simple semi-supervised id33." proceedings of acl-08:
   hlt, pages 595   603, columbus, ohio, usa, association for computational
   linguistics, 2008.

   [nivre, 2004] nivre, joakim. "incrementality in deterministic
   id33." in proceedings of the workshop on incremental
   parsing: bringing engineering and cognition together, pp. 50-57.
   association for computational linguistics, 2004.

   [nivre and fern  ndez-gonz  lez, 2014] nivre, joakim, and daniel
   fern  ndez-gonz  lez. "arc-eager parsing with the tree constraint."
   computational linguistics, 40(2), (2014): 259-267.

   [sun et al., 2013] sun, xu, takuya matsuzaki, and wenjie li. "latent
   structured id88s for large-scale learning with hidden
   information." ieee transactions on knowledge and data engineering, 25.9
   (2013): 2063-2075.

   [zhang and nivre, 2011] zhang, yue, and joakim nivre. "transition-based
   id33 with rich non-local features." proceedings of the
   49th annual meeting of the association for computational linguistics:
   human language technologies: short papers-volume 2. association for
   computational linguistics, 2011.s, 2011.

     *    2019 github, inc.
     * [81]terms
     * [82]privacy
     * [83]security
     * [84]status
     * [85]help

     * [86]contact github
     * [87]pricing
     * [88]api
     * [89]training
     * [90]blog
     * [91]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [92]reload to refresh your
   session. you signed out in another tab or window. [93]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/rasoolims/yaraparser/commits/transfer.atom
   3. https://github.com/rasoolims/yaraparser/tree/transfer#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/rasoolims/yaraparser/tree/transfer
  32. https://github.com/join
  33. https://github.com/login?return_to=/rasoolims/yaraparser
  34. https://github.com/rasoolims/yaraparser/watchers
  35. https://github.com/login?return_to=/rasoolims/yaraparser
  36. https://github.com/rasoolims/yaraparser/stargazers
  37. https://github.com/login?return_to=/rasoolims/yaraparser
  38. https://github.com/rasoolims/yaraparser/network/members
  39. https://github.com/rasoolims
  40. https://github.com/rasoolims/yaraparser
  41. https://github.com/yahoo/yaraparser
  42. https://github.com/rasoolims/yaraparser/tree/transfer
  43. https://github.com/rasoolims/yaraparser/pulls
  44. https://github.com/rasoolims/yaraparser/projects
  45. https://github.com/rasoolims/yaraparser/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/rasoolims/yaraparser/commits/transfer
  48. https://github.com/rasoolims/yaraparser/branches
  49. https://github.com/rasoolims/yaraparser/releases
  50. https://github.com/rasoolims/yaraparser/graphs/contributors
  51. https://github.com/rasoolims/yaraparser/blob/master/license
  52. https://github.com/rasoolims/yaraparser/search?l=java
  53. https://github.com/rasoolims/yaraparser/find/transfer
  54. https://github.com/rasoolims/yaraparser/archive/transfer.zip
  55. https://github.com/login?return_to=https://github.com/rasoolims/yaraparser/tree/transfer
  56. https://github.com/join?return_to=/rasoolims/yaraparser
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/rasoolims/yaraparser/pull/new/transfer
  62. https://github.com/rasoolims/yaraparser/compare/transfer
  63. https://github.com/rasoolims/yaraparser/tree/bc39d19288971dcf6d00b262928c4b6ba295f376
  64. https://github.com/rasoolims/yaraparser/tree/transfer/meta-inf
  65. https://github.com/rasoolims/yaraparser/tree/transfer/punc_files
  66. https://github.com/rasoolims/yaraparser/tree/transfer/sample_data
  67. https://github.com/rasoolims/yaraparser/tree/transfer/src
  68. https://github.com/rasoolims/yaraparser/blob/transfer/license.md
  69. https://github.com/rasoolims/yaraparser/blob/transfer/readme.md
  70. https://github.com/rasoolims/yaraparser/blob/transfer/license
  71. http://arxiv.org/abs/1503.06733
  72. https://github.com/rasoolims/semisupervisedpostagger/releases/tag/v0.2
  73. http://stp.lingfil.uu.se/~nivre/research/penn2malt.html
  74. http://www.sutd.edu.sg/cmsresource/faculty/yuezhang/acl11j.pdf
  75. http://www.sutd.edu.sg/cmsresource/faculty/yuezhang/acl11j.pdf
  76. http://www.sutd.edu.sg/cmsresource/faculty/yuezhang/acl11j.pdf
  77. http://www.sutd.edu.sg/cmsresource/faculty/yuezhang/acl11j.pdf
  78. http://www.sutd.edu.sg/cmsresource/faculty/yuezhang/acl11j.pdf
  79. http://www.cs.bgu.ac.il/~yoavg/software/projectivize/
  80. https://github.com/percyliang/brown-cluster
  81. https://github.com/site/terms
  82. https://github.com/site/privacy
  83. https://github.com/security
  84. https://githubstatus.com/
  85. https://help.github.com/
  86. https://github.com/contact
  87. https://github.com/pricing
  88. https://developer.github.com/
  89. https://training.github.com/
  90. https://github.blog/
  91. https://github.com/about
  92. https://github.com/rasoolims/yaraparser/tree/transfer
  93. https://github.com/rasoolims/yaraparser/tree/transfer

   hidden links:
  95. https://github.com/
  96. https://github.com/rasoolims/yaraparser/tree/transfer
  97. https://github.com/rasoolims/yaraparser/tree/transfer
  98. https://github.com/rasoolims/yaraparser/tree/transfer
  99. https://help.github.com/articles/which-remote-url-should-i-use
 100. https://github.com/rasoolims/yaraparser/tree/transfer#yara-parser
 101. https://github.com/rasoolims/yaraparser/tree/transfer#yara-transfer-parser
 102. https://github.com/rasoolims/yaraparser/tree/transfer#the-main-change
 103. https://github.com/rasoolims/yaraparser/tree/transfer#citation
 104. https://github.com/rasoolims/yaraparser/tree/transfer#the-original-readme-file-note-that-the-cluster-feature-option-is-changed
 105. https://github.com/rasoolims/yaraparser/tree/transfer#yara-k-beam-arc-eager-dependency-parser
 106. https://github.com/rasoolims/yaraparser/tree/transfer#version-log
 107. https://github.com/rasoolims/yaraparser/tree/transfer#warning
 108. https://github.com/rasoolims/yaraparser/tree/transfer#performance-and-speed-on-wsjpenn-treebank-v02
 109. https://github.com/rasoolims/yaraparser/tree/transfer#meaning-of-yara
 110. https://github.com/rasoolims/yaraparser/tree/transfer#compilation
 111. https://github.com/rasoolims/yaraparser/tree/transfer#command-line-options
 112. https://github.com/rasoolims/yaraparser/tree/transfer#train-a-yaraparserparser
 113. https://github.com/rasoolims/yaraparser/tree/transfer#parse-a-conll_2006-file
 114. https://github.com/rasoolims/yaraparser/tree/transfer#parse-a-pos-tagged-file
 115. https://github.com/rasoolims/yaraparser/tree/transfer#train-a-yaraparserparser-with-partial-training-data
 116. https://github.com/rasoolims/yaraparser/tree/transfer#parse-a-partial-tree-with-some-gold-dependencies
 117. https://github.com/rasoolims/yaraparser/tree/transfer#evaluate-the-yaraparserparser
 118. https://github.com/rasoolims/yaraparser/tree/transfer#example-usage
 119. https://github.com/rasoolims/yaraparser/tree/transfer#training-without-brown-cluster-features
 120. https://github.com/rasoolims/yaraparser/tree/transfer#training-with-brown-cluster-features
 121. https://github.com/rasoolims/yaraparser/tree/transfer#api-usage
 122. https://github.com/rasoolims/yaraparser/tree/transfer#notes
 123. https://github.com/rasoolims/yaraparser/tree/transfer#how-to-create-word-clusters
 124. https://github.com/rasoolims/yaraparser/tree/transfer#memory-size
 125. https://github.com/rasoolims/yaraparser/tree/transfer#technical-details
 126. https://github.com/
