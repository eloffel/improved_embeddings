   #[1]learn opencv    feed [2]learn opencv    comments feed [3]learn opencv
      understanding autoencoders using tensorflow (python) comments feed
   [4]best project award : id161 for faces [5]image
   classification using convolutional neural networks in keras
   [6]alternate [7]alternate

   [tr?id=193036094536652&ev=pageview&noscript=1]

     * [8]skip to primary navigation
     * [9]skip to content
     * [10]skip to primary sidebar

   [11]learn opencv

   opencv examples and tutorials ( c++ / python )

     * [12]home
     * [13]about
     * [14]resources
     * [15]ai consulting
     * [16]courses

understanding autoencoders using tensorflow (python)

   november 15, 2017 by [17]aditya sharma [18]9 comments

   this post is part of the series on deep learning for beginners, which
   consists of the following tutorials :
    1. [19]neural networks : a 30,000 feet view for beginners
    2. [20]installation of deep learning frameworks (tensorflow and keras
       with cuda support )
    3. [21]introduction to keras
    4. [22]understanding feedforward neural networks
    5. [23]image classification using feedforward neural networks
    6. [24]image recognition using convolutional neural network
    7. [25]understanding id180
    8. understanding autoencoders using tensorflow
    9. [26]image classification using pre-trained models in keras
   10. [27]id21 using pre-trained models in keras
   11. [28]fine-tuning pre-trained models in keras
   12. more to come . . .

   in this article, we will learn about autoencoders in deep learning. we
   will show a practical implementation of using a denoising autoencoder
   on the [29]mnist handwritten digits dataset as an example. in addition,
   we are sharing an implementation of the idea in tensorflow.

1. what is an autoencoder?

   an autoencoder is an unsupervised machine learning algorithm that takes
   an image as input and reconstructs it using fewer number of bits. that
   may sound like image compression, but the biggest difference between an
   autoencoder and a general purpose image compression algorithms is that
   in case of autoencoders, the compression is achieved by learning on a
   training set of data. while reasonable compression is achieved when an
   image is similar to the training set used, autoencoders are poor
   general-purpose image compressors; jpeg compression will do vastly
   better.

   autoencoders are similar in spirit to id84
   techniques like [30]principal component analysis. they create a space
   where the essential parts of the data are preserved, while
   non-essential ( or noisy ) parts are removed.

   there are two parts to an autoencoder
    1. encoder: this is the part of the network that compresses the input
       into a fewer number of bits. the space represented by these fewer
       number of bits is called the    latent-space    and the point of
       maximum compression is called the bottleneck. these compressed bits
       that represent the original input are together called an    encoding   
       of the input.
    2. decoder: this is the part of the network that reconstructs the
       input image using the encoding of the image.

   let   s look at an example to understand the concept better.
   [31]autoencoder figure: 2-layer autoencoder

   in the above picture, we show a vanilla autoencoder     a 2-layer
   autoencoder with one hidden layer. the input and output layers have the
   same number of neurons. we feed five real values into the autoencoder
   which is compressed by the encoder into three real values at the
   bottleneck (middle layer). using these three real values, the decoder
   tries to reconstruct the five real values which we had fed as an input
   to the network.

   in practice, there are a far larger number of hidden layers in between
   the input and the output.

   there are various kinds of autoencoders like sparse autoencoder,
   variational autoencoder, and denoising autoencoder. in this post, we
   will learn about a denoising autoencoder.

2. denoising autoencoder

   denoising autoencoder figure: denoising autoencoder

   the idea behind a denoising autoencoder is to learn a representation
   (latent space) that is robust to noise. we add noise to an image and
   then feed this noisy image as an input to our network. the encoder part
   of the autoencoder transforms the image into a different space that
   preserves the handwritten digits but removes the noise. as we will see
   later, the original image is 28 x 28 x 1 image, and the transformed
   image is 7 x 7 x 32. you can think of the 7 x 7 x 32 image as a 7 x 7
   image with 32 color channels.

   the decoder part of the network then reconstructs the original image
   from this 7 x 7 x 32 image and voila the noise is gone!

   how does this magic happen?

   during training, we define a loss (cost function) to minimize the
   difference between the reconstructed image and the original noise-free
   image. in other words, we learn a 7 x 7 x 32 space that is noise free.

   download code
   to easily follow along this tutorial, please download the ipython
   notebook code by clicking on the button below. it   s free!
   [32]download code

3. implementation of denoising autoencoder

   this implementation is inspired by this excellent post [33]building
   autoencoders in keras.

3.1 the network

   the images are matrices of size 28 x 28. we reshape the image to be of
   size 28 x 28 x 1, convert the resized image matrix to an array, rescale
   it between 0 and 1, and feed this as an input to the network. the
   encoder transforms the 28 x 28 x 1 image to a 7 x 7 x 32 image. you can
   think of this 7 x 7 x 32 image as a point in a 1568 ( because 7 x 7 x
   32 = 1568 ) dimensional space. this 1568 dimensional space is called
   the bottleneck or the latent space. the architecture is graphically
   shown below.
   encodermodel figure: architecture of encoder model

   the decoder does the exact opposite of an encoder; it transforms this
   1568 dimensional vector back to a 28 x 28 x 1 image. we call this
   output image a    reconstruction    of the original image. the structure of
   the decoder is shown below.
   decodermodel figure: architecture of decoder model

   let   s dive into the implementation of an autoencoder using tensorflow.

3.2 encoder

   the encoder has two convolutional layers and two max pooling layers.
   both convolution layer-1 and convolution layer-2 have 32-3 x 3 filters.
   there are two max-pooling layers each of size 2 x 2.
#encoder
with tf.name_scope('en-convolutions'):
 conv1 = tf.layers.conv2d(inputs_,filters=32,kernel_size=(3,3),strides=(1,1),pad
ding='same',use_bias=true,activation=lrelu,name='conv1')
# now 28x28x32

with tf.name_scope('en-pooling'):
 maxpool1 = tf.layers.max_pooling2d(conv1,pool_size=(2,2),strides=(2,2),name='po
ol1')
# now 14x14x32

with tf.name_scope('en-convolutions'):
 conv2 = tf.layers.conv2d(maxpool1,filters=32,kernel_size=(3,3),strides=(1,1),pa
dding='same',use_bias=true,activation=lrelu,name='conv2')
# now 14x14x32

with tf.name_scope('encoding'):
 encoded = tf.layers.max_pooling2d(conv2,pool_size=(2,2),strides=(2,2),name='enc
oding')
# now 7x7x32.

#latent space


   figure: encoder block diagram

3.3 decoder

   the decoder has two conv2d_transpose layers, two convolution layers,
   and one sigmoid activation function. conv2d_transpose is for upsampling
   which is opposite to the role of a convolution layer. the
   conv2d_transpose layer upsamples the compressed image by two times each
   time we use it.
#decoder
with tf.name_scope('decoder'):
 conv3 = tf.layers.conv2d(encoded,filters=32,kernel_size=(3,3),strides=(1,1),nam
e='conv3',padding='same',use_bias=true,activation=lrelu)
#now 7x7x32

 upsample1 = tf.layers.conv2d_transpose(conv3,filters=32,kernel_size=3,padding='
same',strides=2,name='upsample1')
# now 14x14x32

 upsample2 = tf.layers.conv2d_transpose(upsample1,filters=32,kernel_size=3,paddi
ng='same',strides=2,name='upsample2')
# now 28x28x32

 logits = tf.layers.conv2d(upsample2,filters=1,kernel_size=(3,3),strides=(1,1),n
ame='logits',padding='same',use_bias=true)
#now 28x28x1

# pass logits through sigmoid to get denoisy image
 decoded = tf.sigmoid(logits,name='recon')


   figure: decoder block diagram

   finally, we calculate the loss of the output using [34]cross-id178
   id168 and use [35]adam optimizer to optimize our id168.

3.4 why do we use a leaky relu and not a relu as an activation function?

   we want gradients to flow while we backpropagate through the network.
   we stack many layers in a system in which there are some neurons whose
   value drop to zero or become negative. using a relu as an activation
   function clips the negative values to zero and in the backward pass,
   the gradients do not flow through those neurons where the values become
   zero. because of this the weights do not get updated, and the network
   stops learning for those values. so using relu is not always a good
   idea. however, we encourage you to change the activation function to
   relu and see the difference.
def lrelu(x,alpha=0.1):
  return tf.maximum(alpha*x,x)


   therefore, we use a leaky relu which instead of clipping the negative
   values to zero, cuts them to a specific amount based on a
   hyperparameter alpha. this ensures that the network learns something
   even when the pixel value is below zero.

3.5 load the data

   once the architecture has been defined, we load the training and
   validation data.

   as shown below, tensorflow allows us to easily load the mnist data. the
   training and testing data loaded is stored in variables train_x and
   test_x respectively. since its an unsupervised task we do not care
   about the labels.
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("mnist_data/", one_hot=true)
train_x = mnist.train.images
test_x = mnist.test.images

3.6 data analysis

   before training a neural network, it is always a good idea to do a
   sanity check on the data.

   let   s see how the data looks like. the data consists of handwritten
   numbers ranging from 0 to 9, along with their ground truth labels. it
   has 55,000 train samples and 10,000 test samples. each sample is a
   28  28 grayscale image.
print('training data shape' :train_x.shape)
print('testing data shape' :test_x.shape)

nsample = 1
rand_train_idx = np.random.randint(mnist.train.images.shape[0], size=nsample)

for i in rand_train_idx:
  curr_img = np.reshape(mnist.train.images[i, :], (28,28))
  curr_lbl = np.argmax(mnist.train.labels[i, :])
  plt.matshow(curr_img, cmap=plt.get_cmap('gray'))
  plt.title(""+str(i)+"th training image "+ "(label: " + str(curr_lbl) + ")")
  plt.show()

rand_test_idx = np.random.randint(mnist.test.images.shape[0], size=nsample)

for i in rand_test_idx:
  curr_img = np.reshape(mnist.test.images[i, :], (28,28))
  curr_lbl = np.argmax(mnist.test.labels[i, :])
  plt.matshow(curr_img, cmap=plt.get_cmap('gray'))
  plt.title(""+str(i)+"th test image "+ "(label: " + str(curr_lbl) + ")")
  plt.show()


   output:
(training data shape :&amp;nbsp; (55000, 784))
(testing data shape :&amp;nbsp; (10000, 784))

3.7 preprocessing the data

   the images are grayscale and the pixel values range from 0 to 255. we
   apply following preprocessing to the data before feeding it to the
   network.
    1.
         1. convert each 784-dimensional vector into a matrix of size 28 x
            28 x 1 which is fed into the network.

batch_train_x = mnist.train.next_batch(batch_size)
batch_test_x = mnist.test.next_batch(batch_size)
imgs_train= batch_train_x[0].reshape((-1, 28, 28, 1))

imgs_test = batch_test_x[0].reshape((-1, 28, 28, 1))



    1.
         1. add noise to both train and test images which we then feed
            into the network. noise factor is a hyperparamter and can be
            tuned accordingly.

noise_factor = 0.5
x_train_noisy = imgs_train + noise_factor * np.random.normal(loc=0.0, scale=1.0,
 size=imgs_train.shape)
x_test_noisy = imgs_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, s
ize=imgs_test.shape)
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)


3.8 training the model

   the network is ready to get trained. we specify the number of epochs as
   25 with batch size of 64. this means that the whole dataset will be fed
   to the network 25 times. we will be using the test data for validation.
batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: x_train_noisy,targets_
: imgs,learning_rate:lr})


3.9 evaluate the model

   we check the performance of the model on our test set by checking the
   cost (loss).
batch_cost_test = sess.run(cost, feed_dict={inputs_: x_test_noisy,targets_: imgs
_test})


   output
('epoch: 25/25...', 'training loss: 0.1196', 'validation loss: 0.1171')

   after 25 epochs we can see our training loss and validation loss is
   quite low which means our network did a pretty good job. let   s now see
   the loss plot between training and validation data.

3.10 training vs. validation loss plot

loss.append(batch_cost)
valid_loss.append(batch_cost_test)
plt.plot(range(e+1), loss, 'bo', label='training loss')
plt.plot(range(e+1), valid_loss, 'r', label='validation loss')
plt.title('training and validation loss')
plt.xlabel('epochs ',fontsize=16)
plt.ylabel('loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()


   from the above loss plot, we can observe that the validation loss and
   training loss are both steadily decreasing in the first ten epochs.
   this training loss and the validation loss are also very close to each
   other. this means that our model has generalized well to unseen test
   data.

   we can further validate our results by observing the original, noisy
   and reconstruction of test images.

3.11 results

   [36]original images figure: original images[37] noisy images figure:
   images with noise[38] reconstructed denoised images figure:
   reconstruction of noisy images

   from the above figures, we can observe that our model did a good job in
   denoising the noisy images that we had fed into our model.

subscribe & download code

   if you liked this article and would like to download code (ipython
   notebook), please [39]subscribe to our newsletter. you will also
   receive a free [40]id161 resource guide. in our newsletter,
   we share opencv tutorials and examples written in c++/python, and
   id161 and machine learning algorithms and news.

   [41]subscribe now

   filed under: [42]application, [43]deep learning, [44]how-to,
   [45]tutorial tagged with: [46]autoencoder, [47]convolutional neural
   network, [48]denoising
   (button) load comments

   search this website ____________________ search

join course

   [49]id161 for faces

resources

   [50]download code (c++ / python)

disclaimer

   this site is not affiliated with opencv.org

   i am an entrepreneur with a love for id161 and machine
   learning with a dozen years of experience (and a ph.d.) in the field.

   in 2007, right after finishing my ph.d., i co-founded taaz inc. with my
   advisor dr. david kriegman and kevin barnes. the scalability, and
   robustness of our id161 and machine learning algorithms have
   been put to rigorous test by more than 100m users who have tried our
   products. [51]read more   

recent posts

     * [52]image inpainting with opencv (c++/python)
     * [53]hough transform with opencv (c++/python)
     * [54]xeus-cling: run c++ code in jupyter notebook
     * [55]pose detection comparison : wrnchai vs openpose
     * [56]gender & age classification using opencv deep learning (
       c++/python )

   copyright    2019    big vision llc

references

   1. https://www.learnopencv.com/feed/
   2. https://www.learnopencv.com/comments/feed/
   3. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/feed/
   4. https://www.learnopencv.com/best-project-award-computer-vision-for-faces/
   5. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
   6. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/
   7. https://www.learnopencv.com/wp-json/oembed/1.0/embed?url=https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/&format=xml
   8. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/#genesis-nav-primary
   9. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/#genesis-content
  10. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/#genesis-sidebar-primary
  11. https://www.learnopencv.com/
  12. https://www.learnopencv.com/
  13. https://www.learnopencv.com/about/
  14. https://www.learnopencv.com/computer-vision-resources
  15. https://www.learnopencv.com/computer-vision-machine-learning-artificial-intelligence-consulting/
  16. http://courses.learnopencv.com/
  17. https://www.learnopencv.com/author/adityasharma/
  18. https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/#disqus_thread
  19. https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/
  20. https://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/
  21. https://www.learnopencv.com/deep-learning-using-keras-the-basics/
  22. https://www.learnopencv.com/understanding-feedforward-neural-networks/
  23. https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/
  24. https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/
  25. https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/
  26. https://www.learnopencv.com/keras-tutorial-using-pre-trained-id163-models/
  27. https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/
  28. https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models
  29. https://en.wikipedia.org/wiki/mnist_database
  30. https://en.wikipedia.org/wiki/principal_component_analysis
  31. https://www.learnopencv.com/wp-content/uploads/2017/11/autoencoder.png
  32. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  33. https://blog.keras.io/building-autoencoders-in-keras.html
  34. https://en.wikipedia.org/wiki/cross_id178
  35. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
  36. https://www.learnopencv.com/wp-content/uploads/2017/11/original-mnist-2.png
  37. https://www.learnopencv.com/wp-content/uploads/2017/11/noisy-mnist-1.png
  38. https://www.learnopencv.com/wp-content/uploads/2017/11/reconstruction-mnist-1.png
  39. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  40. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  41. https://bigvisionllc.leadpages.net/leadbox/143948b73f72a2:173c9390c346dc/5649050225344512/
  42. https://www.learnopencv.com/category/application/
  43. https://www.learnopencv.com/category/deep-learning/
  44. https://www.learnopencv.com/category/how-to/
  45. https://www.learnopencv.com/category/tutorial/
  46. https://www.learnopencv.com/tag/autoencoder/
  47. https://www.learnopencv.com/tag/convolutional-neural-network/
  48. https://www.learnopencv.com/tag/denoising/
  49. https://courses.learnopencv.com/p/computer-vision-for-faces
  50. https://bigvisionllc.lpages.co/leadbox/143044973f72a2:173c9390c346dc/5720147234914304/
  51. https://www.learnopencv.com/about/
  52. https://www.learnopencv.com/image-inpainting-with-opencv-c-python/
  53. https://www.learnopencv.com/hough-transform-with-opencv-c-python/
  54. https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/
  55. https://www.learnopencv.com/pose-detection-comparison-wrnchai-vs-openpose/
  56. https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/
