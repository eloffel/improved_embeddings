   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

gans for simulation, representation and id136

   [14]go to the profile of ari heljakka
   [15]ari heljakka (button) blockedunblock (button) followfollowing
   nov 18, 2017

   (update 2018/10/15: note that this review covers the models only up to
   06/2017. on the whole, the analysis is still valid, but of course,
   there have been further improvements.)

   over 100 variants of gans (id3) were
   introduced in 2017 alone. to understand this development, we select a
   subset of these to observe some of the major axes of the variation, and
   examine gans from the perspectives of simulation, representation and
   id136. the reader is expected to have general understanding of
   neural networks and at least superficial familiarity of [16]gans.
   [0*wxtudnkg1rr-kcas.png]
   image interpolation with boundary equilibrium gan. the real training
   samples at far left and far right. [17][1]

   generative neural network models have recently aroused much interest,
   primarily for two reasons. first, trained on a certain dataset, they
   can learn to generate synthetic sample data that exhibits the salient
   features of the original data. second, there is a hypothesis that, in
   comparison to other learning tasks, the generation task requires (and
   perhaps guides) the network to learn a relatively powerful internal
   representation of the data. to this end, the network must compactly
   capture the underlying factors that explain most of the variation in
   the data. correspondingly, the actual task to generate (or transform)
   content can be considered either an end in itself, or a measure of the
   power of the internal representation. this distinction is often not
   made explicit, but it is a fundamental one.

   generative adversarial network (gan) variants are networks that learn
   complex data distributions with an internal representation that allows
   generating novel realistic samples that resemble the training data. so
   far, they have been found most applicable for various image generation
   and transformation tasks, with unsupervised and semi-supervised
   learning.
   [1*7kwrprbygke9jt2u-olctw.png]

   here, we define the term    gan pattern    to stand for the idea that
   underlies all gan variants. gan pattern is similar to what is often
   called    adversarial training   , but it also involves a specific
   architecture constraint. the architecture requires two trainable
   networks g and d and a non-trainable data provider g*, where d takes
   input from both g and g*. the learning goal of d includes developing
   and maintaining sensitivity to the factors that distinguish between the
   samples from the data sources g and g*. the goal of g includes the
   opposite of this         hence the term    adversarial   . g is called generator
   and d discriminator or critic. if balanced carefully, d provides
   sufficient gradient to guide g towards creating samples that are
   increasingly g*-like, until the output of g becomes indistinguishable
   from g*.

   further, we invoke a simple concept that has intuitive meaning across
   many disciplines         simulator. a simulator imitates another system by
   creating sample events, states or structures based on some computable
   rules we come up with         e.g. approximations to the laws of nature, your
   theory of human relationships, or the latest crazy mesh of
   deconvolutional networks. in the gan pattern, we can say that g learns
   to simulate g*. gan pattern is, then, a specific way to learn a
   simulator, in the form of its generator.

   gan variants (including the original gan [18][2]) implement the gan
   pattern in one or more ways. often, the data from g* is static training
   data such as images, but it can be any distribution of random
   variables. note that g is intended to mimic the data generating process
   g*, not to reproduce the exact same data that g* produces during
   training. we can then utilize the synthetic data samples from g
   directly, or use the internal representation of the network for
   unsupervised and semi-supervised learning. now, importantly, gan
   pattern alone does not do any id136 from input variables. however,
   we will see how to extend g to take such input.

   major recent gan applications include e.g. generating images with
   increasingly high resolution, image-to-image translation from one
   domain to another without labeled image pairs, image classification
   with only a small number of labeled samples [19][3], and transforming
   existing imagery e.g. by way of super-resolution [20][4] or by refining
   a synthetic    toy image    into a more realistic one [21][5]. there is an
   increasingly wide variation of applications that falls outside the
   scope of this article, as well as those intended for supervised or
   semi-supervised (class-conditioned) scenarios (e.g. stackgan which in
   some respects has been state-of-the-art for image generation), domain
   adaptation models, and models with discrete variables. gans are also
   connected to id23 (rl). in e.g. inverse rl, the
   learning task involves recovering the reward function itself, much like
   learning the discriminator in gan.

   major recent theoretical development around gans include the analysis
   of the gan convergence problem ([22][6], [23][4]), generalizing gans to
   the wider class of f-divergences [24][7], and connections to implicit
   models ([25][8], [26][9]). other important theoretical work includes
   the analysis of evaluation methods ([27][10], [28][11]), extensions
   towards two-sample testing ([29][12], [30][13]), and making gans
   amenable for likelihood analysis [31][14].

   major limitations of gans have, until recently, been usually related to
   the stability of training and the lack of diversity of generated
   samples. several improvements were introduced by [32][3], followed by
   e.g. various modifications to the id168, supporting the
   training with an autoencoder, and adding noise to the distributions in
   particular ways. at present, perhaps the most pressing problem of gan
   research is the lack of sufficient standard performance benchmarks.
   even though gans are customarily treated as probabilistic models, the
   likelihood of a sample under the learned model, in particular, appears
   to be unsuitable as a gan metric. we will look at various alternatives.

   in the following, we will first establish the major axes of variation
   between gan variants, and then proceed to look at them from the
   perspective of simulation, representation and id136. simulator
   learning is identified as the general goal of the gan pattern, and we
   look at it separately in terms of id168s and architecture. if
   you only need to generate random samples for another downstream
   purpose, this may be enough. if your goal is representation learning
   for e.g. downstream semi-supervised learning, you might expect a good
   simulation to imply a good underlying representation. however, we need
   to cover a range of measurement approaches to understand what that
   means. finally, if you need to guide or control[33]   the generation
   process, or condition it on a partial or full sample as an input to the
   network, we need id136. this enables applications that process,
   transform or complete an input sample in some way.
     __________________________________________________________________

navigating the gan variant space

   we start with a map of some[34]   of the most important generic gan
   variants published by the end of h1/2017. over 20 variants are
   included, primarily from the perspective of unsupervised learning.

   confusingly, the contemporary *gan nomenclature mixes together gan
   variants that only change the id168, ones that also change the
   architecture, ones that support some kind of an id136, and ones
   with application-specific extensions or changes in the training
   algorithm. even ignoring the applications, this calls for a map that
   allows for at least three levels.
   [1*qh87uidby810evtnjaatzg.png]
   [1*xga4k-93ksl3n0y1aetxxw.png]

   the top-level distinction (the green and blue halves of the figure) is
   between the two prominent loss behavior families (f-divergences vs.
   integral id203 metrics). loosely, these gan variants fall on one
   side or the other. the second-level distinction (the outer and inner
   rings) is based on whether the architecture of the gan variant has been
   extended with an autoencoder. next, please bear in mind that the gan
   pattern as such can only learn a mapping from latent space to generated
   samples. the third-level distinction, then, is made based on whether
   the gan variant has been extended to allow for inverting the sample
   generation process with an id136 network that maps from samples
   back to latent space. note that this can be done with or without an
   autoencoder. finally, we separate the variants that invert the
   generator with variational id136.

   crucially, this map provides a bottom-up view, in the sense that we
   have started from the gan variants that are out there, and crafted a
   map to accommodate them. a more insightful approach, however, is to
   take a top-down view of what the gan variants are expected to do, and
   justify the distinctive virtues of each gan variant on this basis. this
   view brings us to simulation, representation, and id136.
     __________________________________________________________________

simulator learning         the overarching goal

   we mentioned that the gan pattern is a specific way to learn a
   simulator. now, we will show how simulators can be connected to
   id203 density in rather surprising ways. we do this to explain
   two separate things: why the gan id168s have recently split
   into two directions, and how we can use the gan pattern for
   probabilistic id136.
   [0*l4nswrx5u4isdvmy.png]
   simulated faces from celeba by alpha-gan. [35][15]

   as the gan generator begins to learn, it becomes a simulator that
   favors the stochastic generation of certain kinds of samples over the
   others. thus, it behaves as if it used a specific id203
   distribution for the generated samples, but implicitly (unless of
   course the simulator was defined according to explicit id203
   rules).

   in a neural network model, we may have several distributions to
   consider, such as the prior distribution of the latent variable and the
   likelihood of the observed variable under the model. under certain
   conditions, a gan pattern can be used to turn any or all those
   distributions implicit![36]  

   such a distribution cannot be used directly for calculating
   probabilities. luckily, it turns out that often we need just a ratio of
   two distributions, and in that case, one or both can be implicit. we
   will just have to resort to the gan discriminator, built for comparing
   two distributions. so, instead of dealing with the distributions as
   distributions in a closed form, we compare the population of generated
   samples of each distribution as they come. we can use this to estimate
   either density ratio or density difference of these populations.
   correspondingly, there are two popular approaches for gan loss
   functions to implement this density estimation-by-comparison ([37][7],
   [38][9], [39][16]). (for succinctness, our explanation is extremely
   simplified.)
   [0*iqg-mzgkonkfafdz.png]
   a more precise breakdown of density difference and density ratio
   estimation methods [40][9].

   (1) f-divergence measures the ratio of id203 densities between
   the two distributions. f-divergences include jensen-shannon divergence,
   id181, and others. for two distributions p(x) and
   q(x), an f-divergence is, effectively, an expectation of their density
   ratio:
   [1*jzrf-17fb2iqc02rp-4tjq.png]

   with a suitable choice of f, this equation can directly reproduce the
   said divergences, and more. in gans, the distributions p and q are
   typically the real and the generated one. now, gan id168 can
   either converge into f-divergence behavior via class id203
   estimation, or use it explicitly. for instance, the original gan loss
   function has no mention of f-divergences, but under certain conditions,
   it was famously shown to converge to jensen-shannon divergence.
   similarly, e.g. lsgan [41][43] has been shown to converge to pearson
   chi-square divergence. the conditions under which these training
   algorithms actually do converge are a topic of intense research.
   alternatively, we can make any f-divergence our explicit loss measure
   by divergence minimization, as demonstrated in f-gan [42][7].

   (2) integral id203 metrics compare expectations of the
   distributions under a suitable learnable smooth transformation function
   f (called witness) that aims to find the biggest differences between
   the distributions:
   [1*70usqll4gmk2downinsryw.png]

   varying the f, this form produces e.g. 1-wasserstein distance (aka
   earth mover distance), maximum mean discrepancy (mmd) distance, as well
   as the feature matching loss of [43][3]. moment matching looks at the
   higher moments of the distributions of this form, either explicitly or
   via use of [44]embedding kernels. next, we will see examples of all of
   these!
     __________________________________________________________________

simulator learning         id168 perspective

   the id168 formulation of the original gan has been found
   challenging to work with. it was supported later by the set of improved
   gan techniques [45][3] that provided generic tools for gan training
   while also showing good results in semi-supervised scenarios. still,
   problems remained.

   at least one major cause appears to be dimensional misspecification
   ([46][6], [47][4], [48][17]) that troubles most gans of the
   f-divergence family. to learn the generator properly, the gan
   discriminator needs to provide it with a sufficiently stable gradient.
   but in gans, the generated data and the real data behave as if they
   were two distinct relatively low-dimensional manifolds, with very
   little initial overlap between them. it can be shown that in this
   setup, gradients based on e.g. js divergence or kl divergence will
   easily become overtly sharp or vanishing (due to the log id203
   ratios), and cannot point the id119 process to the right
   direction. as soon as the discriminator becomes good enough, it can use
   the gap between the distributions for perfect discrimination,
   preventing the generator from improving.

   as a solution, [49][6] offered the 1-wasserstein distance, leading us
   to wasserstein gan (wgan) [50][18]. wgan was a major a milestone due to
   its convergence and stability properties that were unparalleled at the
   time of its introduction. it makes a simple but major shift in the
   objective function. instead of making the discriminator compare
   something like contrastive log probabilities, it now compares something
   like quality scores. technically, the scores give rise to an
   approximation of 1-wasserstein distance, using the fairly involved
   theory of optimal transport. the theory looks for the smallest amount
   of changes required to move enough mass from one distribution to
   another, to make them equal. unlike previous gans, wgan showed stable
   training convergence that clearly correlated with increasing quality of
   generated samples. most of us can skip the complex theory of wgans, and
   just keep in mind that the wgan removes the logarithms of the loss
   function and aggressively clips the weights of the discriminator to
   keep the network on a stable learning path. this clipping reduces the
   learning capacity of the network, but the problem was fixed in a
   follow-up variant wgan-gp [51][19].

   wgan led us down the ipm track, soon accompanied with methods based on
   maximum mean discrepancy (mmd) distance, descending from generative
   moment matching networks [52][20]. by operating on populations of
   samples rather than individual ones, they tend to offer robustness at
   the cost of performance. generative matching networks utilized fixed
   kernels for measuring distances between distributions. mmd-gan [53][21]
   and distributional adversarial networks [54][22] improve upon this by
   making those kernels learnable with adversarial setup. fishergan
   [55][23] uses a variance control scheme based on [56]mahalanobis
   distance, to the purported effect of achieving the stability of
   ipm-based training with performance superior to wgan and wgan-gp.

   to overcome the dimensional misspecification, [57][4] proposed
      instance noise    as a solution to increase the overlap between the
   distributions, while remaining on the f-divergence track. to the same
   effect, [58][17] offers a cleaner method of adding the noise
   analytically         with a penalty on the weighted gradient norm. this
   id173 term can be added to almost any gan network, potentially
   with major stability gains without increasing variance.

   the distinction between ipm and f-divergence objectives should be taken
   with a grain of salt, however. first, the f-divergence behavior of the
   original gan objective is only achieved under the assumption of a
   perfect discriminator. almost all gan variants change the objective in
   some way, and might no longer share this convergence property, and even
   when they do, the practical relevance of this is not clear. second,
   different ipm variations behave in very different ways. for instance,
   the use of the second moment in mmd methods makes the scaling quadratic
   with respect to the training batch sizes, unlike the first moment used
   in wasserstein. empirically, the practical effect might be tolerable
   ([59][12], [60][21]) but more research is clearly needed. third, we can
   have a setup like fishergan which utilizes an ipm-based id168,
   but at the non-parametric limit is shown to converge to chi-squared
   divergence (which is no longer ipm).

   the major drawback of wgan and wgan-gp is the inferior performance in
   comparison to f-divergence models. nonetheless, they have already
   powered a number of applications, and their performance can be
   improved. an early example is the    fast-learning    wasserstein critic
   [61][14]. at the moment, an alternative approach may be to use the
   generally less stable but faster f-divergence gans by stabilizing them
   with new techniques like the penalty on the weighted gradient norm.
     __________________________________________________________________

simulator learning         architecture perspective

   since the introduction of the original gan architecture, dcgan [62][24]
   is by far the most often cited implementation for image generation, and
   remains a reference architecture or baseline for the recent models.
   dcgan successfully combined a convolutional architecture with gan and
   was able to produce images with compelling quality.
   [1*3m-jyfwrxmqc8hhaec8vwg.png]
   architecture of boundary equilibrium gan [63][1], extending the
   original dcgan.

   autoencoders have been introduced to stabilize the training. to see why
   this is, bear in mind that an autoencoder takes a sample, encodes it to
   latent format, and then tries to decode it back into the original
   sample, like a compression/decompression utility such as jpeg. now, for
   the autoencoder to succeed, it needs to make the decoded sample as
   similar to the input sample as possible, i.e. we minimize the
   difference between the two, a measure known as reconstruction loss.
   this enables the network to learn even in the absence of adversarially
   generated samples.

   autoencoders can, of course, be used in very different ways. for
   instance, boundary equilibrium gan (began) [64][1] uses a particular
   equilibrium enforcing method to balance the training and control the
   trade-off between sharpness and diversity of generated images. dfm-gan
   [65][25] improves stability by training a denoising autoencoder to
   denoise the feature representations of real data, and matches the
   features of the real and generated data only after denoising both,
   achieving high inception score (more on which later). energy-based gan
   [66][26] uses a regular autoencoder to a similar effect.

   the critical part of any dcgan-style architecture for high-resolution
   image generation is, of course, the id98. therein lies a question. as
   such, the id98 part could of course be used with other generative
   models. what is, then, the specific contribution of the gan pattern for
   the results? how do we know that gans are not just one successful
   generative method for piggybacking on the massive opportunities created
   by id98s? surprisingly little attention has been given to this
   fundamental question, so [67][27] attempted to find out exactly this.
   with a generator network devoid of adversarial training component, they
   produce results with quality high enough to at least warrant further
   research. if this approach turns out to produce increasingly good
   results with no adversarial training at all, this could imply that, in
   the context of image generation,    gan    as such is not the most
   interesting level of abstraction to focus on!
     __________________________________________________________________

representation learning

   by now, we have covered some candidates for architecture and loss
   functions that may provide a quick and stable way for training our gan
   variant. next, we can proceed to measure the model   s ability to
   represent the data. how much can the model actually learn, and exactly
   what can it actually generate?

   we can start from likelihood estimation, but we will not get far. in a
   gan pattern, the generator is presumed to learn a rich representation
   of the original data distribution. traditionally, such models have been
   measured via the likelihood of some test data under the learned model.
   now, the gan pattern does not immediately lend itself for likelihood
   evaluation, but methods have been developed to that end.[68]    examples
   include annealed importance sampling [69][11] and analyses based on
   real nvps, such as [70][14]. the results, however, are confusing. when
   e.g. the latter method was used to train an increasingly powerful
   generator with wgan, the measured log-id203 density actually
   decreased. other approaches ([71][11], [72][10]) have also showed that
   likelihood of validation data under the model is not necessarily
   correlated with the quality of the generated samples. the papers
   [73][28] and [74][29] show that the generator of gan can    win    the
   discriminator even though it actually converges to a distribution that,
   at best, takes into account only a part of the training distribution.

   from this, various conclusions are possible. on possibility is that we
   have simply confirmed the difference between representation and
   id203 estimation. producing a    more representative    sample is
   simply a different thing than producing a    more probable    sample. a
   network that can produce a variety of different sharp pink elephants
   must have a fairly good representation of elephants, yet the
   id203 of its creations remains zero. in general, should it learn
   the features that can be combined to reproduce any sample in the
   training distribution, or the joint id203 density of the actual
   combinations of those features? we clearly need to step up the game and
   try something else.

   fortunately, instead of likelihood estimates, we can use the generative
   ability of the system as a proxy for its learning ability. that is, if
   you can generate something, you prove that you have learned something
   about it. hence, we can focus on measuring the quality and diversity of
   samples that the network can generate, within and across classes, with
   respect to the sampling speed.

   less fortunately, manual comparison by humans is the most popular
   method actually used to evaluate the generated images for quality and
   realism. this is obviously insufficient due to various inaccuracies. it
   is especially hard for humans to evaluate the diversity of generated
   images.

   inception score is the most established quantitative measure of gan
   performance [75][3]. it uses a pre-trained inception model to measure
   two things. first, for each generated sample to have an unambiguous
   class, we want low id178 for p(y|x). second, for classes to be evenly
   represented, we want high id178 for
   [1*0t5rjg1z5adnyjw0v02s_g.png]

   these two conditions combine into
   [1*49xz3u6_q9agv_kx3aedzg.png]

   however, inception score does not account for diversity within a class.
   e.g. if the most specific matched class is    beagle   , then your score
   will not improve by actually making more than one kind of a beagle
   image, resulting in mode collapse. in order to measure diversity within
   each class, multiscale structural similarity (ms-ssim) score [30] has
   been used by [76][15] and [77][31], but does not directly work across
   classes in unsupervised setup.

   training a separate evaluation network with ipm-based methods has
   recently been introduced as a powerful generic evaluation measure.
   wasserstein distance could provide a fair measure of sample quality as
   well as detection of overfitting and mode collapse. [78][14] trains a
   separate wasserstein critic with validation data, uses it for the
   evaluation of the trained gan, and could further use it to compare
   different gan architectures. in addition, classifier 2-sample testing
   (c2st) [79][13] and a separate mmd estimator [80][12] can serve as
   fairly advanced sample quality evaluation approaches that also promise
   human-interpretability. note that [81][12] extends the idea to gan
   training, too.

   note that, up to now, we have stayed in the context of representation
   learning before any downstream task, which is known to be a limiting
   setup for evaluating any unsupervised method, since ultimately
   different tasks would benefit from learning different kinds of things.
   by adding any auxiliary task with its own performance measure, models
   are rendered comparable. it is, then, the pragmatic measure of
   task-specific performance, not a universal objective evaluation method,
   that gives us a fixed reference point. semi-supervised learning can
   provide one such measure for generative models, e.g. in the form of
   classification performance, used effectively by [82][3].

   finally, while gan is a general-purpose network model, most of the gan
   work so far has focused on image generation, to the unfortunate effect
   that we do not yet know how well the analyses done in the image domain
   will hold across domains.

   still, at least within the image domain, we can see several paths that
   may lead us to more established standards and benchmarks. fortunately
   so, since staring at two generated bedrooms and trying to convince
   yourself of the existence of a meaningful discernible difference
   between them is a particular purgatory which nobody would equate with
   good science or engineering.
   [1*xbrpgop9hxuhzqxwmjyfiq.png]
   bedrooms created with different architectures. can you spot the one
   produced with no adversarial training at all?
     __________________________________________________________________

id136 learning

   moving beyond mere simulation and representation learning, there are
   several ways to use gan pattern for likelihood-free id136, i.e. to
   invert the generator.

   strictly speaking, there are indirect ways to invert any gan generator
   and approximately recover the latent code of a sample. a simple way is
   to use regression on a fully-trained gan to find the latent variables
   that produce the sample that is closest to our target. recovering the
   codes allows linear arithmetic in the latent space, and has been used
   for arithmetic of the kind shown below.
   [1*twcsdjr-ptxkq61nhulp8g.png]
   arithmetic on images in latent space.[83][24]

   should we require more control, it may suffice to have a subset of
   latent dimensions be interpretable. this is the approach of infogan,
   which separates a few information-maximizing latent codes from the
   others that are treated as incompressible noise.

   a more advanced approach is to use the gan pattern with some kind of a
   probabilistic autoencoder. in order to get there, we can start from the
   regular autoencoder, and extend it with two things. first, we would
   like to sample easily from the latent space of the autoencoder, and in
   order to do this, we should have our latent space values distributed in
   a nice manner; for instance, as a normal distribution. this would allow
   us to generate all of the interesting variation of the samples by using
   latent values only within a focused narrow range (say, walking from -5
   to +5 for each dimension, instead of random guessing or some kind of
   mcmc approach). second, we would like to have some notion of    degree of
   precision    in the input. we need this to express the idea that a
   certain latent code value range, say the value between 1.161   1.163,
   corresponds to the same pixel value in the image space. in other words,
   we would like to say things like    differences smaller than 0.002 in the
   latent variable values should not make any difference in the output
   images   .

   variational id136 provides us with these tools, and can be combined
   with autoencoders as variational autoencoder (vae) ([84][32],
   [85][33]). alternatively, there is a way to extend the gan approach to
   learn the input and latent variables in a joint manner. this can be
   done deterministically (bigan [86][34]) or stochastically (ali
   [87][35]).

   for vae, here we can only provide a simplified explanation. vae loss
   function maximizes so-called evidence lower bound (elbo) of the
   marginal likelihood of the data under the model. elbo contains three
   components:
     * prior latent distribution p(z) that reflects our chosen target for
       the distribution of the latent variables,
     * approximated posterior q(z|x) that measures the encoding success,
       and
     * likelihood p(x|z) that measures the decoding success.

   specifically, vae minimizes the sum of kl divergence
   [1*nhoi3vqimlt0ncb_ae-juw.png]

   and cross-id178
   [1*fmz1ck-vzgyqu-wonh6tza.png]

   hence, we often focus on the kl divergence term rather than on q(z|x)
   and p(z) separately. vae in itself can generate fairly nice samples for
   many use-cases.[88]   
   [0*lhkzm6b2bo79kjn-.png]
   car coloring based on a grayscale input image by age network. [89][36]

   now, in order to use gan pattern with variational id136, recall
   that we can leverage density estimation-by-comparison to approximate
   any ratio of two distributions. this    density ratio trick    [90][15]
   relieves us from the burden of having to figure out an analytical form
   for both distributions, which typically need approximations that may be
   detrimental to model performance. because each of the three elbo terms
   is a distribution, we can use the density ratio trick separately to
   replace any or all of them (with a separate gan pattern for each). in
   principle, we will then be protected against any limitations that
   explicit approximations of those distributions would have entailed
   ([91][8], [92][15]).

   for gan-vae variants, we will cover the adversarial generator encoder
   networks (age) [93][36], vae-gan [94][37], adversarial autoencoder
   (aae) [95][38], adversarial id58 (avb) [96][39] and
   alpha-gan [97][15]. they all implement an autoencoder, and most of them
   define an extra discriminator.
     * age replaces the likelihood term with an l1 loss and the kl
       divergence term with an empirical approximation, and connects the
       id168s of the encoder and decoder (generator) in an
       adversarial setup.
     * vae-gan extends vae with an extra discriminator for the data space.
     * avb and aae (and also affgan [98][4]) modify vae by replacing the
       kl divergence with a discriminator for the latent space.
     * alpha-gan further modifies aae by also replacing the likelihood
       term with a combination of a separate discriminator and l1
       reconstruction loss.

   going deeper, the picture gets rather complicated, with various
   differences between the models. first, while e.g. vae-gan, avb and age
   are strictly variational, aae is not[99]   . second, ali and bigan are
   closely related to these models. while also not strictly variational,
   they have the benefit of symmetrically learning the mapping not only
   from x to z but vice versa [100][8], with the downside that they do not
   really use the id136 model for id136. empirically, these models
   are often not able to reconstruct the input image well, but it is too
   early to say what that really means.

   alpha-gan has some superior results over the other models, and it uses
   the density ratio trick in both the likelihood and divergence terms.
   however, it still needs the l1 reconstruction loss for the explicit
   purpose of avoiding mode collapse. also, as a replacement for the
   divergence term in alpha-gan, the age-style empirical approximation
   actually outperformed the discriminator that was based on the ratio
   density trick, when evaluated on inception score metric [101][15]. in
   terms of architecture, age remains the simplest one of these, avoiding
   separate discriminators altogether.
     __________________________________________________________________

conclusion

   we have presented one possible map for categorizing recent gan
   variants. we illustrated their differences with respect to their
   ability to learn to simulate, represent and infer.

   for simulator learning to succeed in the first place, we observed at
   least three promising approaches: complementing f-divergence gans with
   the penalty on the weighted gradient norm [102][17], wgan-gp with
   performance improvements from e.g. the fast learning critic [103][14],
   and mmd-based methods such as fishergan [104][23].

   for representation learning, the key question is how to measure the
   representational power. currently, we can use either a combination of
   partial metrics, train an auxiliary network to do the evaluation, or
   measure with an auxiliary task such as classification in
   semi-supervised learning context. more research is needed to address
   open questions about the ability of gans to learn the distribution in
   the first place, as opposed to learning the representation in a
   different sense.

   for invertible gans, we have identified several gan-vae hybrids that
   show reasonable results. they mainly differ in how extensively they use
   the gan pattern, but it is not yet clear under which conditions    more
   is better   .

   fortunately, there are already several plausible answers to most of our
   questions. we conclude with an example of how to stabilize training in
   the case of unsupervised adversarial id20. the same domain
   adaptation model was used by cyclegan [105][44], discogan [106][45],
   and dualgan [107][46]. they all work well, despite the fact that each
   one uses a different id168 form (cyclegan uses lsgan, dualgan
   uses wgan, and discogan uses the original gan loss). it may turn out
   that many different solutions work. but first, we need to standardize
   the id74 to find out.
     __________________________________________________________________

notes

      for semi-supervised learning, we can of course utilize labels to make
   the whole gan effectively class-conditional, which has shown good
   results already, but here we will keep our focus on the unsupervised
   setup. [108][back]

      why only these gan variants have been included, and not
   myfavouritegan? we attempted to include most of the gan variants that
   are important from the unsupervised perspective, published prior to
   june 30th, 2017.

   the variants have been selected as follows. the original gan,
      improved    gan [109][3] and dc-gan[110][24] are considered reference
   architectures due to being universally cited by nearly all the gan
   papers. the division between f-divergences and ipm variants is here
   considered a primary one, as defined by [111][7] and [112][9]. due to
   the particularly many recent papers on mmd methods, the mmd class of
   papers is included here since the appearance of these papers is a
   strong signal for the mmd direction (fishergan, mmd-gan, dan). mcgan is
   considered less relevant after the introduction of fishergan (by the
   same authors).

   therefore, we include the gan variants created by the authors of the
   primary theoretical papers, the gan variants that those papers
   quantitatively compare to, and the mmd family. affgan is a specialized
   model, but its stochastic version was shown in the paper to be related
   to variational id136, and the paper contains theoretical results
   that overlap with [113][6]. generative moment matching network is
   explicitly not a gan model. class-conditional models (stackgan, ppgn)
   and discrete models (e.g. boundary seeking gan) were not included. the
   criteria for considering aae, ali and bigan as non-variational is based
   on [114][8]. f-gan stands as the canonical example of generalizing gans
   from jensen-shannon divergence to all f-divergences [115][7].

   you could argue that the subset of models chosen here for examination
   is too narrow, especially insofar as one assumes that the applications
   of gans are what matters in the end. unless stated otherwise, the
   omission of this or that gan variant from this article only implies
   that it did not meet the specific selection criteria at the time of
   writing. consequently, outside the sources we cite, the mileage of any
   and all conclusions may vary. or, you could argue that our approach is
   too    wide and thin   , since we cover e.g. the gans with id136 but
   not e.g. the gan variants for id20. finally, one could
   claim that the level of abstraction is wrong, since treating these
   models specifically as gans (as opposed to e.g. the wider class of
   implicit models) will hinder us from seeing the connections to the
   other disciplines that tackle very similar problems.

   is it appropriate to call these models    gan variants   ? in the general
   context, this simplification should be used with care since some of
   these are hybrid generative models that incorporate several ideas (to
   the point that e.g. some gan-vae hybrids might better be considered
      vae variants   ). with the recent flood of network models using *gan
   naming scheme, the distinction is not trivial. even several
   near-duplicate models have been introduced under different names, e.g.
   ali/bigan, vae-gan/mdgan, discogan/cyclegan/dualgan. [116][back]

      this is why gans, simulators and simulation-based methods are often
   also called implicit models or implicit generative models. however, the
   terminology easily gets messy. since the network may contain multiple
   distributions, it would be more appropriate to talk about specific
   implicit distributions within the model, rather than addressing the
   whole (neural network) model as implicit or not. just like the network
   can have two or more sub-networks that separately implement the gan
   pattern, there can be two or more distributions that are being
   (separately) implicitly modelled. some more terminology: first, a model
   is sometimes called prescribed or explicit if it has explicit
   parameters for observation likelihood (i.e. the likelihood of the data
   under the model that we try to learn). an implicit model does not have
   one. second, note that some authors use the term    generative model    to
   mean    implicit model   , which gets confusing since we can have
   prescribed models that generate samples. third, note that the whole gan
   pattern, with both a generator and a discriminator, is usually called
      generative model   . finally, it is important to bear in mind that other
   implicit models exist, e.g. the classifier approximate bayesian
   computation (abc) [117][40]. classifier abc has similarities to gans,
   although abc is built for id136, not sample generation. [118][back]

       kernel density estimates, popular in many domains of statistics, do
   not work here [119][10].    birthday testing    (estimating id178 by
   counting coincidences) was used by [120][29] to find the size of the
   support that the model distribution has, but it also requires manual
   human participation. mode score from [121][41] was proposed to improve
   on inception score, but it has not been widely adopted. [122][back]

       why should we involve gan pattern at all, then? the typical narrative
   on this mentions that in image generation tasks, the vae-generated
   images appear blurry. (by extension, it is maybe implied but not proven
   that this would be a problem with other application domains as well.)
   there is a wide range of hypotheses for why this would be so. possibly
   the most convincing one is the following. vae is derived with kl
   divergence which is unsymmetrical by definition. therefore, even though
   putting id203 mass on the parts of q(x,z) that are not shared by
   p(x,z) is quickly corrected, the reverse does not hold. this may leave
   enough    elbo room    for the encoder to assign id203 to places
   where there should be none! still, there is much unexplored space
   around vaes to tackle this problem without gans, too, and progress has
   been made. [123][back]

       unlike avb, aae   s discriminator only depends on z (and not on x) and
   hence violates the requirements of variational id136. while this
   approximation might be dangerous, it was recently connected to the
   minimization of 2-wasserstein distance between the generative model and
   data, unlike the avb approach [124][42]. [125][back]

references

   [1] began: boundary equilibrium id3
   [126][pdf]
   berthelot, d., schumm, t. and metz, l. corr, abs/1703.10717, 2017.

   [2] id3 [127][pdf]
   goodfellow, i., pouget-abadie, j., mirza, m., xu, b., warde-farley, d.,
   ozair, s., courville, a. and bengio, y. in nips, 2014.

   [3] improved techniques for training gans [128][pdf]
   salimans, t., goodfellow, i., zaremba, w., cheung, v., radford, a. and
   chen, x. in nips, 2016.

   [4] amortised map id136 for image super-resolution [129][pdf]
   kaae sonderby, c., caballero, j., theis, l., shi, w. and huszar, f. in
   iclr, 2017.

   [5] learning from simulated and unsupervised images through adversarial
   training [130][pdf]
   shrivastava, a., pfister, t., tuzel, o., susskind, j., wang, w. and
   webb, r. in cvpr, 2017.

   [6] towards principled methods for training generative adversarial
   networks [131][pdf]
   arjovsky, m. and bottou, l. in iclr, 2017.

   [7] f-gan: training generative neural samplers using variational
   divergence minimization [132][pdf]
   nowozin, s., cseke, b. and tomioka, r. in nips, 2016.

   [8] variational id136 using implicit distributions [133][pdf]
   huszar, f. arxiv:1702.08235, 2017.

   [9] learning in implicit generative models [134][pdf]
   mohamed, s. and lakshminarayanan, b. in iclr, 2016.

   [10] a note on the evaluation of generative models [135][pdf]
   theis, l., van den oord, a. and bethge, m. arxiv:1511.01844, 2015.

   [11] on the quantitative analysis of decoder-based generative models
   [136][pdf]
   wu, y., burda, y., salakhutdinov, r. and grosse, r. in iclr, 2017.

   [12] generative models and model criticism via optimized maximum mean
   discrepancy [137][pdf]
   sutherland, d., tung, h., strathmann, h., de, s., ramdas, a., smola, a.
   and gretton, a. in icml, 2016.

   [13] revisiting classifier two-sample tests [138][pdf]
   lopez-paz, d. and oquab, m. arxiv:1610.06545, 2016.

   [14] comparison of maximum likelihood and gan-based training of real
   nvps [139][pdf]
   danihelka, i., lakshminarayanan, b., uria, b., wierstra, d. and dayan,
   p. arxiv:1705.05263, 2017.

   [15] variational approaches for auto-encoding generative adversarial
   networks [140][pdf]
   rosca, m., lakshminarayanan, b., warde-farley, d. and mohamed, s.
   arxiv:1706.04987, 2017.

   [16] density-ratio matching under the bregman divergence: a unified
   framework of density-ratio estimation [141][pdf]
   sugiyama, m. and kanamori, t. annals of the institute of statistical
   mathematics, 2012.

   [17] stabilizing training of id3 through
   id173 [142][pdf]
   roth, k., lucchi, a., nowozin, s. and hofmann, t. corr abs/1705.09367,
   2017.

   [18] wasserstein gan [143][pdf]
   arjovsky, m., chintala, s. and bottou, l. corr, abs/1701.07875, 2017.

   [19] improved training of wasserstein gans [144][pdf]
   gulrajani, i., ahmed, f., arjovsky, m., dumoulin, v. and courville, a.
   corr, abs/1704.00028, 2017.

   [20] generative moment matching networks [145][pdf]
   li, y., swersky, k. and zemel, r., 2015. arxiv:1502.02761, 2015.

   [21] mmd gan: towards deeper understanding of moment matching network
   [146][pdf]
   li, c., chang, w., cheng, y., yang, y. and poczos, b. arxiv:1705.08584,
   2017.

   [22] distributional adversarial networks [147][pdf]
   li, c., alvarez-melis, d., xu, k., jegelka, s. and sra, s.
   arxiv:1706.09549, 2017.

   [23] fisher gan [148][pdf]
   mroueh, y. and sercu, t. arxiv:1705.09675, 2017.

   [24] unsupervised representation learning with deep convolutional
   id3 [149][pdf]
   radford, a., metz, l. and chintala, s. in iclr, 2016.

   [25] improving id3 with denoising feature
   matching [150][link]
   warde-farley, d. in iclr, 2017.

   [26] energy-based generative adversarial network [151][pdf]
   zhao, j., mathieu, m. and lecun, y. in iclr, 2017.

   [27] optimizing the latent space of generative networks [152][pdf]
   bojanowski, p., joulin, a., lopez-paz, d. and szlam, a.
   arxiv:1707.05776, 2017.

   [28] generalization and equilibrium in generative adversarial nets
   (gans) [153][pdf]
   arora, s., ge, r., liang, y., ma, t. and zhang, y. in icml, 2017.

   [29] do gans actually learn the distribution? an empirical study
   [154][pdf]
   arora, s. and zhang, y. in icml, 2017.

   [30] image quality assessment: from error visibility to structural
   similarity
   wang, z. and simoncelli, e.p. ieee transactions on image processing,
   13(4):600   612, 2004.

   [31] conditional image synthesis with auxiliary classifier gans
   [155][pdf]
   odena, a., olah, c. and shlens, j. in icml, 2017.

   [32] auto-encoding id58 [156][pdf]
   kingma, d. and welling, m. arxiv:1312.6114, 2013.

   [33] stochastic id26 and approximate id136 in deep
   generative models [157][pdf]
   jimenez rezende, d., mohamed, s. and wierstra, d. in icml, pages
   1278   1286, 2014.

   [34] adversarial id171 [158][pdf]
   donahue, j., krahenbuhl, p. and darrell, t. arxiv:1605.09782, 2016.

   [35] adversarially learned id136 [159][pdf]
   dumoulin, v., belghazi, i., poole, b., mastropietro, o., lamb, a.,
   arjovsky, m. and courville, a., 2016. corr, abs/1606.00704.

   [36] it takes (only) two: adversarial generator-encoder networks
   [160][pdf]
   ulyanov, d., vedaldi, a. and lempitsky, v. corr, abs/1704.02304, 2017.

   [37] autoencoding beyond pixels using a learned similarity metric
   [161][pdf]
   boesen lindbo larsen, a., kaae sonderby, s., larochelle, h. and
   winther, o. in icml, pages 1558   1566, 2016.

   [38] adversarial autoencoders [162][pdf]
   makhzani, a., shlens, j., jaitly, n., goodfellow, i. and frey, b.
   arxiv:1511.05644, 2015.

   [39] adversarial id58: unifying id5
   and id3 [163][pdf]
   mescheder, l., nowozin, s. and geiger, a. in icml, 2017.

   [40] likelihood-free id136 via classification [164][pdf]
   gutmann, m., dutta, r., kaski, s. and corander, j. in statistics and
   computing, 2017.

   [41] mode regularized id3 [165][pdf]
   che, t., li, y., jacob, a., bengio, y. and li, w. iclr, 2017.

   [42] from optimal transport to generative modeling: the vegan cookbook
   [166][pdf]
   bousquet, o., gelly, s., tolstikhin, i., simon-gabriel, c. and
   schoelkopf, b., arxiv:1705.07642, 2017.

   [43] least squares id3 [[167]pdf]
   mao, x., li, q., xie, h., lau, r., wang, z., smolley, s.
   arxiv:1611.04076, 2017.

   [44] unpaired image-to-image translation using cycle-consistent
   adversarial networks [168][pdf]
   zhu, j., park, t., isola, p., efros, a. arxiv:1703.10593, 2017.

   [45] learning to discover cross-domain relations with generative
   adversarial networks [169][pdf]
   kim, t., cha, m., kim, h., lee, j.k., kim, j. arxiv:1703.05192, 2017.

   [46] dualgan: unsupervised dual learning for image-to-image translation
   [170][pdf]
   yi, z., zhang, h., tan, p., gong, m. arxiv:1704.02510, 2017.

   iframe:
   [171]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=d215c251ed12

   [172][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [173][1*v-ppfkswhbvlwwamsvhhwg.png]
   [174][1*wt2auqisieaozxj-i7brdq.png]

   [175]some rights reserved
     * [176]machine learning
     * [177]artificial intelligence
     * [178]deep learning
     * [179]unsupervised learning
     * [180]generative model

   (button)
   (button)
   (button) 574 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [181]go to the profile of ari heljakka

[182]ari heljakka

   machine learning scientist at genmind. entrepreneur, developer.
   co-founder of dream broker. author of model zero.

     (button) follow
   [183]becoming human: artificial intelligence magazine

[184]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 574
     * (button)
     *
     *

   [185]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [186]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d215c251ed12
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_qiovonhu5to7---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@ariheljakka?source=post_header_lockup
  15. https://becominghuman.ai/@ariheljakka
  16. https://arxiv.org/pdf/1701.00160
  17. http://arxiv.org/abs/1703.10717
  18. http://arxiv.org/abs/1406.2661
  19. http://arxiv.org/abs/1606.03498
  20. http://arxiv.org/abs/1610.04490
  21. http://arxiv.org/abs/1612.07828
  22. https://arxiv.org/abs/1701.04862
  23. http://arxiv.org/abs/1610.04490
  24. http://arxiv.org/abs/1606.00709
  25. http://arxiv.org/abs/1702.08235
  26. http://arxiv.org/abs/1610.03483
  27. http://arxiv.org/abs/1511.01844
  28. http://arxiv.org/abs/1611.04273
  29. http://arxiv.org/abs/1611.04488
  30. http://arxiv.org/abs/1610.06545
  31. http://arxiv.org/abs/1705.05263
  32. http://arxiv.org/abs/1606.03498
  33. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#a7bb
  34. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#fe0f
  35. http://arxiv.org/abs/1706.04987
  36. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#1ded
  37. http://arxiv.org/abs/1606.00709
  38. http://arxiv.org/abs/1610.03483
  39. http://www.ism.ac.jp/editsec/aism/pdf/10463_2011_article_343.pdf
  40. http://arxiv.org/abs/1610.03483
  41. https://arxiv.org/abs/1611.04076
  42. http://arxiv.org/abs/1606.00709
  43. http://arxiv.org/abs/1606.03498
  44. http://www.gatsby.ucl.ac.uk/~dougals/slides/dali/
  45. http://arxiv.org/abs/1606.03498
  46. https://arxiv.org/abs/1701.04862
  47. http://arxiv.org/abs/1610.04490
  48. http://arxiv.org/abs/1705.09367
  49. https://arxiv.org/abs/1701.04862
  50. http://arxiv.org/abs/1701.07875
  51. http://arxiv.org/abs/1704.00028
  52. http://arxiv.org/abs/1502.02761
  53. http://arxiv.org/abs/1705.08584
  54. http://arxiv.org/abs/1706.09549
  55. http://arxiv.org/abs/1705.09675
  56. https://en.wikipedia.org/wiki/mahalanobis_distance
  57. http://arxiv.org/abs/1610.04490
  58. http://arxiv.org/abs/1705.09367
  59. http://arxiv.org/abs/1611.04488
  60. http://arxiv.org/abs/1705.08584
  61. http://arxiv.org/abs/1705.05263
  62. http://arxiv.org/abs/1511.06434
  63. http://arxiv.org/abs/1703.10717
  64. http://arxiv.org/abs/1703.10717
  65. https://openreview.net/pdf?id=s1x7nhsxl
  66. http://arxiv.org/abs/1609.03126
  67. http://arxiv.org/abs/1707.05776
  68. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#fc7b
  69. http://arxiv.org/abs/1611.04273
  70. http://arxiv.org/abs/1705.05263
  71. http://arxiv.org/abs/1611.04273
  72. http://arxiv.org/abs/1511.01844
  73. http://arxiv.org/abs/1703.00573
  74. http://arxiv.org/abs/1706.08224
  75. http://arxiv.org/abs/1606.03498
  76. http://arxiv.org/abs/1706.04987
  77. https://arxiv.org/abs/1610.09585
  78. http://arxiv.org/abs/1705.05263
  79. http://arxiv.org/abs/1610.06545
  80. http://arxiv.org/abs/1611.04488
  81. http://arxiv.org/abs/1611.04488
  82. http://arxiv.org/abs/1606.03498
  83. http://arxiv.org/abs/1511.06434
  84. http://arxiv.org/abs/1312.6114
  85. http://arxiv.org/abs/1401.4082
  86. http://arxiv.org/abs/1605.09782
  87. http://arxiv.org/abs/1606.00704
  88. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#8fee
  89. http://arxiv.org/abs/1704.02304
  90. http://arxiv.org/abs/1706.04987
  91. http://arxiv.org/abs/1702.08235
  92. http://arxiv.org/abs/1706.04987
  93. http://arxiv.org/abs/1704.02304
  94. http://arxiv.org/abs/1512.09300
  95. http://arxiv.org/abs/1511.05644
  96. http://arxiv.org/abs/1701.04722
  97. http://arxiv.org/abs/1706.04987
  98. http://arxiv.org/abs/1610.04490
  99. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#5c38
 100. http://arxiv.org/abs/1702.08235
 101. http://arxiv.org/abs/1706.04987
 102. http://arxiv.org/abs/1705.09367
 103. http://arxiv.org/abs/1705.05263
 104. http://arxiv.org/abs/1705.09675
 105. https://arxiv.org/abs/1703.10593
 106. https://arxiv.org/abs/1703.05192
 107. https://arxiv.org/abs/1704.02510
 108. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#b8ec
 109. http://arxiv.org/abs/1606.03498
 110. http://arxiv.org/abs/1511.06434
 111. http://arxiv.org/abs/1606.00709
 112. http://arxiv.org/abs/1610.03483
 113. https://arxiv.org/abs/1701.04862
 114. http://arxiv.org/abs/1702.08235
 115. http://arxiv.org/abs/1606.00709
 116. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#7703
 117. http://arxiv.org/abs/1407.4981
 118. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#b41b
 119. http://arxiv.org/abs/1511.01844
 120. http://arxiv.org/abs/1706.08224
 121. http://arxiv.org/abs/1612.02136
 122. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#ef17
 123. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#3e0e
 124. https://arxiv.org/abs/1705.07642
 125. https://becominghuman.ai/gans-for-simulation-representation-and-id136-d215c251ed12?gi=deb16b662310#ce18
 126. http://arxiv.org/pdf/1703.10717.pdf
 127. http://arxiv.org/pdf/1406.2661.pdf
 128. http://arxiv.org/pdf/1606.03498.pdf
 129. http://arxiv.org/pdf/1610.04490.pdf
 130. http://arxiv.org/pdf/1612.07828.pdf
 131. https://arxiv.org/pdf/1701.04862.pdf
 132. http://arxiv.org/pdf/1606.00709.pdf
 133. http://arxiv.org/pdf/1702.08235.pdf
 134. http://arxiv.org/pdf/1610.03483.pdf
 135. http://arxiv.org/pdf/1511.01844.pdf
 136. http://arxiv.org/pdf/1611.04273.pdf
 137. http://arxiv.org/pdf/1611.04488.pdf
 138. http://arxiv.org/pdf/1610.06545.pdf
 139. http://arxiv.org/pdf/1705.05263.pdf
 140. http://arxiv.org/pdf/1706.04987.pdf
 141. http://www.ism.ac.jp/editsec/aism/pdf/10463_2011_article_343.pdf
 142. http://arxiv.org/pdf/1705.09367.pdf
 143. http://arxiv.org/pdf/1701.07875.pdf
 144. http://arxiv.org/pdf/1704.00028.pdf
 145. http://arxiv.org/pdf/1502.02761.pdf
 146. http://arxiv.org/pdf/1705.08584.pdf
 147. http://arxiv.org/pdf/1706.09549.pdf
 148. http://arxiv.org/pdf/1705.09675.pdf
 149. http://arxiv.org/pdf/1511.06434.pdf
 150. https://openreview.net/pdf?id=s1x7nhsxl
 151. http://arxiv.org/pdf/1609.03126.pdf
 152. http://arxiv.org/pdf/1707.05776.pdf
 153. http://arxiv.org/pdf/1703.00573.pdf
 154. http://arxiv.org/pdf/1706.08224.pdf
 155. https://arxiv.org/pdf/1610.09585.pdf
 156. http://arxiv.org/pdf/1312.6114.pdf
 157. http://arxiv.org/pdf/1401.4082.pdf
 158. http://arxiv.org/pdf/1605.09782.pdf
 159. http://arxiv.org/pdf/1606.00704.pdf
 160. http://arxiv.org/pdf/1704.02304.pdf
 161. http://arxiv.org/pdf/1512.09300.pdf
 162. http://arxiv.org/pdf/1511.05644.pdf
 163. http://arxiv.org/pdf/1701.04722.pdf
 164. http://arxiv.org/pdf/1407.4981.pdf
 165. http://arxiv.org/pdf/1612.02136.pdf
 166. http://arxiv.org/pdf/1705.07642.pdf
 167. https://arxiv.org/pdf/1611.04076
 168. https://arxiv.org/pdf/1703.10593
 169. https://arxiv.org/pdf/1703.05192
 170. https://arxiv.org/pdf/1704.02510
 171. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=d215c251ed12
 172. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
 173. https://upscri.be/8f5f8b
 174. https://becominghuman.ai/write-for-us-48270209de63
 175. http://creativecommons.org/licenses/by/4.0/
 176. https://becominghuman.ai/tagged/machine-learning?source=post
 177. https://becominghuman.ai/tagged/artificial-intelligence?source=post
 178. https://becominghuman.ai/tagged/deep-learning?source=post
 179. https://becominghuman.ai/tagged/unsupervised-learning?source=post
 180. https://becominghuman.ai/tagged/generative-model?source=post
 181. https://becominghuman.ai/@ariheljakka?source=footer_card
 182. https://becominghuman.ai/@ariheljakka
 183. https://becominghuman.ai/?source=footer_card
 184. https://becominghuman.ai/?source=footer_card
 185. https://becominghuman.ai/
 186. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 188. https://medium.com/p/d215c251ed12/share/twitter
 189. https://medium.com/p/d215c251ed12/share/facebook
 190. https://medium.com/p/d215c251ed12/share/twitter
 191. https://medium.com/p/d215c251ed12/share/facebook
