   #[1]github [2]recent commits to boltzmann-machines:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]30
     * [35]star [36]648
     * [37]fork [38]100

[39]yell/[40]boltzmann-machines

   [41]code [42]issues 5 [43]pull requests 1 [44]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [45]sign up
   id82s in tensorflow with examples
   [46]ais [47]annealed-importance-sampling [48]boltzmann-machines
   [49]contrastive-divergence-algorithm [50]dbm [51]deep-learning
   [52]energy-based-model [53]gibbs-sampling [54]keras
   [55]machine-learning [56]mcmc [57]pcd [58]rbm
   [59]restricted-boltzmann-machine [60]sklearn-compatible [61]tensorflow
   [62]tensorflow-models [63]variational-id136
     * [64]765 commits
     * [65]1 branch
     * [66]0 releases
     * [67]fetching contributors
     * [68]mit

    1. [69]jupyter notebook 99.8%
    2. other 0.2%

   (button) jupyter notebook other
   branch: master (button) new pull request
   [70]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/y
   [71]download zip

downloading...

   want to be notified of new releases in yell/boltzmann-machines?
   [72]sign in [73]sign up

launching github desktop...

   if nothing happens, [74]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [75]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [76]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [77]download the github extension for visual studio
   and try again.

   (button) go back
   [78]@yell
   [79]yell [80]merge pull request [81]#9 [82]from
   srmourasilva/issue-7-python3-batch (button)    
fix [83]#7 minibatches in python3

   latest commit [84]8a2efc0 mar 6, 2019
   [85]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [86]boltzmann_machines
   [87]data [88]add fetch_additional_data.sh nov 14, 2017
   [89]docs [90]fix repo urls nov 20, 2017
   [91]examples [92]rename 'bm' -> 'boltzmann_machines' jun 10, 2018
   [93]img
   [94]models [95]add fetch_models.sh nov 12, 2017
   [96]notebooks [97]rename 'bm' -> 'boltzmann_machines' jun 10, 2018
   [98]tex [99]add tex notes nov 19, 2017
   [100].gitignore [101]initial update to python 3 feb 1, 2019
   [102].noserc
   [103]dockerfile [104]make 2 docker files sep 24, 2017
   [105]dockerfile-gpu
   [106]license
   [107]readme.md
   [108]makefile
   [109]requirements.txt [110]remove unnecessary dependency nov 6, 2017
   [111]setup.py
   [112]tb

readme.md

   [113][rbm1.png] [114][samples.png] [115][rbm_small_0.png]
   [116][grbm.png] [117][mrbm.png] [118][samples.png]

id82s

   this repository implements generic and flexible rbm and dbm models with
   lots of features and reproduces some experiments from "deep boltzmann
   machines" [119][1], "learning with hierarchical-deep models" [120][2],
   "learning multiple layers of features from tiny images" [121][3], and
   some others.

table of contents

     * [122]what's implemented
          + [123]restricted id82s (rbm)
          + [124]deep id82s (dbm)
          + [125]common features
     * [126]examples
          + [127]#1 rbm mnist: [128]script, [129]notebook
          + [130]#2 dbm mnist: [131]script, [132]notebook
          + [133]#3 dbm cifar-10 "na  ve": [134]script, [135]notebook
          + [136]#4 dbm cifar-10: [137]script, [138]notebook
          + [139]how to use examples
          + [140]memory requirements
     * [141]download models and stuff
     * [142]tex notes
     * [143]how to install
          + [144]common installation issues
     * [145]possible future work
     * [146]contributing
     * [147]references

what's implemented

restricted id82s (rbm)

     * [148][computational graph]
     * k-step contrastive divergence;
     * whether to sample or use probabilities for visible and hidden
       units;
     * variable learning rate, momentum and number of gibbs steps per
       weight update;
     * id173: l2 weight decay, dropout, sparsity targets;
     * different types of stochastic layers and rbms: implement new type
       of stochastic units or create new rbm from existing types of units;
     * predefined stochastic layers: bernoulli, multinomial, gaussian;
     * predefined rbms: bernoulli-bernoulli, bernoulli-multinomial,
       gaussian-bernoulli;
     * initialize weights randomly, from np.ndarray-s or from another rbm;
     * can be modified for greedy layer-wise pretraining of dbm (see
       [149]notes or [150][1] for details);
     * visualizations in tensorboard (hover images for details) and more:

      [151]mean squared reconstruction error [152]pseudo log-likelihood
                          [153]free energy gap [4]

   [154]l2 loss (weight decay cost times 0.5||w||^2) [155]distribution of
         weights and biases [156]distribution of weights and biases

    [157]distribution of weights and biases updates [158]distribution of
       weights and biases updates [159]histogram of weights and biases

     [160]histogram of weights and biases [161]histogram of weights and
         biases updates [162]histogram of weights and biases updates

      [163]hidden activations probabilities (means) [164]weight filters
                             [165]weight filters

   [166]weight filters [167]weight filters [168]weight filters [169]weight
                         filters [170]weight filters

deep id82s (dbm)

     * [171][computational graph]
     * em-like learning algorithm based on pcd and mean-field variational
       id136 [172][1];
     * arbitrary number of layers of any types;
     * initialize from greedy layer-wise pretrained rbms (no random
       initialization for now);
     * whether to sample or use probabilities for visible and hidden
       units;
     * variable learning rate, momentum and number of gibbs steps per
       weight update;
     * id173: l2 weight decay, maxnorm, sparsity targets;
     * estimate partition function using annealed importance sampling
       [173][1];
     * estimate variational lower-bound (elbo) using log    (currently only
       for 2-layer binary bm);
     * generate samples after training;
     * initialize negative particles (visible and hidden in all layers)
       from data;
     * dbm class can be used also for training rbm and its features: more
       powerful learning algorithm, estimating log    and elbo, generating
       samples after training;
     * visualizations in tensorboard (hover images for details) and more:

   [174]mean squared reconstruction error [175]actual number of mean-field
    updates [176]maximum absolute value of weight matrix (for each layer)

           [177]distribution of weights and biases (in each layer)
           [178]distribution of weights and biases (in each layer)
           [179]distribution of weights and biases (in each layer)

       [180]distribution of weights and biases updates (in each layer)
       [181]distribution of weights and biases updates (in each layer)
         [182]distribution of variational parameters (in each layer)

   [183]histogram of weights and biases (in each layer) [184]histogram of
        weights and biases updates (in each layer) [185]histogram of
                   variational parameters (in each layer)

   [186]weight filters (in each layer) [187]weight filters (in each layer)
   [188]weight filters (in each layer) [189]weight filters (in each layer)
                     [190]weight filters (in each layer)

     [191]negative particles (in each layer) [192]negative particles (in
      each layer) [193]negative particles (in each layer) [194]negative
      particles (in each layer) [195]negative particles (in each layer)

                   [196]negative particles (in each layer)

common features

     * easy to use with sklearn-like interface;
     * easy to load and save models;
     * easy to reproduce (random_seed make reproducible both tensorflow
       and numpy operations inside the model);
     * all models support any precision (tested float32 and float64);
     * configure metrics to display during learning (which ones,
       frequency, format etc.);
     * easy to resume training (note that changing parameters other than
       placeholders or python-level parameters (such as batch_size,
       learning_rate, momentum, sample_v_states etc.) between fit calls
       have no effect as this would require altering the computation
       graph, which is not yet supported; however, one can build model
       with new desired tf graph, and initialize weights and biases from
       old model by using init_from method);
     * visualization: apart from tensorboard, there also plenty of python
       routines to display images, learned filters, confusion matrices etc
       and more.

examples

#1 rbm mnist: [197]script, [198]notebook

   train bernoulli rbm with 1024 hidden units on mnist dataset and use it
   for classification.

                                  algorithm

             test error, %
   rbm features + id92                2.88
   rbm features + id28 1.83
   rbm features + id166                 1.80
   rbm + discriminative fine-tuning   1.27

   [199][filters.png] [200][filters_finetuned.png]
   [201][confusion_matrix.png]

   another simple experiment illustrates main idea of id62
   approach proposed in [202][2]: to train generative neural network (rbm
   or dbm) on large corpus of unlabeled data and after that to fine-tune
   model only on limited amount of labeled data. of course, in [203][2]
   they do much more complex things than simply pre-training rbm or dbm,
   but the difference is already noticeable:
   number of labeled data pairs (train + val) rbm + fine-tuning random
   initialization gain
   60k (55k + 5k) 98.73% 98.20% +0.53%
   10k (9k + 1k) 97.27% 94.73% +2.54%
   1k (900 + 100) 93.65% 88.71% +4.94%
   100 (90 + 10) 81.70% 76.02% +5.68%

   how to reproduce this table see [204]here. in these experiments only
   rbm was tuned to have high pseudo log-likelihood on a held-out
   validation set. even better results can be obtained if one will tune
   mlp and other classifiers.
     __________________________________________________________________

#2 dbm mnist: [205]script, [206]notebook

   train 784-512-1024 bernoulli dbm on mnist dataset with pre-training
   and:
     * use it for classification;
     * generate samples after training;
     * estimate partition function using ais and average elbo on the test
       set.

   algorithm # intermediate distributions proposal (p[0]) log    log(      
     [z]) avg. test elbo tightness of test elbo
   [207][1] 20'000 base-rate? [208][5] 356.18 356.06, 356.29 -84.62 about
   0.5 nats
   this example 200'000 uniform 1040.39 1040.18, 1040.58 -86.37    
   this example 20'000 uniform 1040.58 1039.93, 1041.03 -86.59    

   one can probably get better results by tuning the model slightly more.
   also couple of nats could have been lost because of single-precision
   (for both training and ais estimation).

   [209][rbm1.png] [210][w1_joint.png] [211][w1_finetuned.png]

   [212][rbm2.png] [213][w2_joint.png] [214][w2_finetuned.png]

   [215][mnist.png] [216][samples.png] [217][samples.gif]
   number of labeled data pairs (train + val) dbm + fine-tuning random
   initialization gain
   60k (55k + 5k) 98.68% 98.28% +0.40%
   10k (9k + 1k) 97.11% 94.50% +2.61%
   1k (900 + 100) 93.54% 89.14% +4.40%
   100 (90 + 10) 83.79% 76.24% +7.55%

   how to reproduce this table see [218]here.

   again, mlp is not tuned. with tuned mlp and slightly more tuned
   generative model in [219][1] they achieved 0.95% error on full test
   set.
   performance on full training set is slightly worse compared to rbm
   because of harder optimization problem + possible vanishing gradients.
   also because the optimization problem is harder, the gain when not much
   datapoints are used is typically larger.
   large number of parameters is one of the most crucial reasons why
   id62 is not (so) successful by utilizing deep learning
   only. instead, it is much better to combine deep learning and
   hierarchical bayesian modeling by putting hdp prior over units from
   top-most hidden layer as in [220][2].
     __________________________________________________________________

#3 dbm cifar-10 "na  ve": [221]script, [222]notebook

   (simply) train 3072-5000-1000 gaussian-bernoulli-multinomial dbm on
   "smoothed" cifar-10 dataset (with 1000 least significant singular
   values removed, as suggested in [223][3]) with pre-training and:
     * generate samples after training;
     * use pre-trained gaussian rbm (g-rbm) for classification.

   [224][grbm.png] [225][w1_joint.png] [226][mrbm.png] [227][w2_joint.png]

   [228][cifar10_smoothed.png] [229][samples.png] [230][samples.gif]

   despite poor-looking g-rbm features, classification performance after
   discriminative fine-tuning is much larger than reported backprop from
   random initialization [231][3], and is 5% behind best reported result
   using rbm (with twice larger number of hidden units). note also that
   g-rbm is modified for dbm pre-training ([232]notes or [233][1] for
   details):

                                  algorithm

   test accuracy, %
   best known mlp w/o data augmentation: 8 layer zlin net [234][6] 69.62
   best known method using rbm (w/o data augmentation?): 10k hiddens +
   fine-tuning [235][3] 64.84
   gaussian rbm + discriminative fine-tuning (this example) 59.78
   pure backprop 3072-5000-10 on smoothed data (this example) 58.20
   pure backprop 782-10k-10 on pca whitened data [236][3] 51.53

   [237][grbm.png] [238][grbm_finetuned.png]
   [239][grbm_confusion_matrix.png]
     __________________________________________________________________

#4 dbm cifar-10: [240]script, [241]notebook

   train 3072-7800-512 g-b-m dbm with pre-training on cifar-10, augmented
   (x10) using shifts by 1 pixel in all directions and horizontal
   mirroring and using more advanced training of g-rbm which is
   initialized from pre-trained 26 small rbm on patches of images, as in
   [242][3].
   notice how some of the particles are already resemble natural images of
   horses, cars etc. and note that the model is trained only on augmented
   cifar-10 (490k images), compared to 4m images that were used in
   [243][2].

   [244][rbm_small_0.png] [245][rbm_small_2.png] [246][rbm_small_10.png]
   [247][rbm_small_20.png]

   [248][grbm.png] [249][mrbm.png] [250][samples.png]

   [251][w1_joint.png] [252][w2_joint.png] [253][samples.gif]

   i also trained for longer with
python dbm_cifar.py --small-l2 2e-3 --small-epochs 120 --small-sparsity-cost 0 \
                    --increase-n-gibbs-steps-every 20 --epochs 80 72 200 \
                    --l2 2e-3 0.01 1e-8 --max-mf-updates 70

   while all rbms have nicer features, this means that they overfit more
   than previously, and thus overall dbm performance is slightly worse.

   [254][rbm_small_0.png] [255][rbm_small_2.png] [256][rbm_small_10.png]
   [257][rbm_small_20.png]

   [258][grbm.png] [259][mrbm.png] [260][samples.png]

   [261][w1_joint.png] [262][w2_joint.png] [263][samples.gif]

   the training with all pre-trainings takes quite a lot of time, but once
   trained, these nets can be used for other (similar) datasets/tasks.
   discriminative performance of gaussian rbm now is very close to state
   of the art (having 7800 vs. 10k hidden units), and data augmentation
   given another 4% of test accuracy:

                                  algorithm

   test accuracy, %
   gaussian rbm + discriminative fine-tuning + augmentation (this example)
   68.11
   best known method using rbm (w/o data augmentation?): 10k hiddens +
   fine-tuning [264][3] 64.84
   gaussian rbm + discriminative fine-tuning (this example) 64.38
   gaussian rbm + discriminative fine-tuning (example [265]#3) 59.78

   how to reproduce this table see [266]here.

   [267][grbm.png] [268][grbm_no_aug_finetuned.png]
   [269][grbm_confusion_matrix.png]
     __________________________________________________________________

how to use examples

   use scripts for training models from scratch, for instance
$ python rbm_mnist.py -h

(...)

usage: rbm_mnist.py [-h] [--gpu id] [--n-train n] [--n-val n]
                    [--data-path path] [--n-hidden n] [--w-init std]
                    [--vb-init] [--hb-init hb] [--n-gibbs-steps n [n ...]]
                    [--lr lr [lr ...]] [--epochs n] [--batch-size b] [--l2 l2]
                    [--sample-v-states] [--dropout p] [--sparsity-target t]
                    [--sparsity-cost c] [--sparsity-damping d]
                    [--random-seed n] [--dtype t] [--model-dirpath dirpath]
                    [--mlp-no-init] [--mlp-l2 l2] [--mlp-lrm lrm [lrm ...]]
                    [--mlp-epochs n] [--mlp-val-metric s] [--mlp-batch-size n]
                    [--mlp-save-prefix prefix]

optional arguments:
  -h, --help            show this help message and exit
  --gpu id              id of the gpu to train on (or '' to train on cpu)
                        (default: 0)
  --n-train n           number of training examples (default: 55000)
  --n-val n             number of validation examples (default: 5000)
  --data-path path      directory for storing augmented data etc. (default:
                        ../data/)
  --n-hidden n          number of hidden units (default: 1024)
  --w-init std          initialize weights from zero-centered gaussian with
                        this standard deviation (default: 0.01)
  --vb-init             initialize visible biases as logit of mean values of
                        features, otherwise (if enabled) zero init (default:
                        true)
  --hb-init hb          initial hidden bias (default: 0.0)
  --n-gibbs-steps n [n ...]
                        number of gibbs updates per weights update or sequence
                        of such (per epoch) (default: 1)
  --lr lr [lr ...]      learning rate or sequence of such (per epoch)
                        (default: 0.05)
  --epochs n            number of epochs to train (default: 120)
  --batch-size b        input batch size for training (default: 10)
  --l2 l2               l2 weight decay coefficient (default: 1e-05)
  --sample-v-states     sample visible states, otherwise use probabilities w/o
                        sampling (default: false)
  --dropout p           id203 of visible units being on (default: none)
  --sparsity-target t   desired id203 of hidden activation (default:
                        0.1)
  --sparsity-cost c     controls the amount of sparsity penalty (default:
                        1e-05)
  --sparsity-damping d  decay rate for hidden activations probs (default: 0.9)
  --random-seed n       random seed for model training (default: 1337)
  --dtype t             datatype precision to use (default: float32)
  --model-dirpath dirpath
                        directory path to save the model (default:
                        ../models/rbm_mnist/)
  --mlp-no-init         if enabled, use random initialization (default: false)
  --mlp-l2 l2           l2 weight decay coefficient (default: 1e-05)
  --mlp-lrm lrm [lrm ...]
                        learning rate multipliers of 1e-3 (default: (0.1,
                        1.0))
  --mlp-epochs n        number of epochs to train (default: 100)
  --mlp-val-metric s    metric on validation set to perform early stopping,
                        {'val_acc', 'val_loss'} (default: val_acc)
  --mlp-batch-size n    input batch size for training (default: 128)
  --mlp-save-prefix prefix
                        prefix to save mlp predictions and targets (default:
                        ../data/rbm_)

   or download pretrained ones with default parameters using
   models/fetch_models.sh,
   and check notebooks for corresponding id136 / visualizations etc.
   note that training is skipped if there is already a model in
   model-dirpath, and similarly for other experiments (you can choose
   different location for training another model).
     __________________________________________________________________

memory requirements

     * gpu memory: at most 2-3 gb for each model in each example, and it
       is always possible to decrease batch size and number of negative
       particles;
     * ram: at most 11gb (to run last example, features from gaussian rbm
       are in half precision) and (much) lesser for other examples.
     __________________________________________________________________

download models and stuff

   all models from all experiments can be downloaded by running
   models/fetch_models.sh or manually from [270]google drive.
   also, you can download additional data (fine-tuned models' predictions,
   fine-tuned weights, means and standard deviations for datasets for
   examples [271]#3, [272]#4) using data/fetch_additional_data.sh

tex notes

   check also my supplementary [273]notes (or [274]dropbox) with some
   historical outlines, theory, derivations, observations etc.

how to install

   by default, the following commands install (among others)
   tensorflow-gpu~=1.3.0. if you want to install tensorflow without gpu
   support, replace corresponding line in [275]requirements.txt. if you
   have already tensorflow installed, comment that line.
git clone https://github.com/monsta-hd/boltzmann-machines.git
cd boltzmann-machines
pip install -r requirements.txt

   see [276]here how to run from a virtual environment.
   see [277]here how to run from a docker container.

   to run some notebooks you also need to install [278]jsanimation:
git clone https://github.com/jakevdp/jsanimation
cd jsanimation
python setup.py install

   after installation, tests can be run with:
make test

   all the necessary data can be downloaded with:
make data

common installation issues

   importerror: libcudnn.so.6: cannot open shared object file: no such
   file or directory.
   tensorflow 1.3.0 assumes cudnn v6.0 by default. if you have different
   one installed, you can create symlink to libcudnn.so.6 in
   /usr/local/cuda/lib64 or /usr/local/cuda-8.0/lib64. more details
   [279]here.

possible future work

     * add stratification;
     * add id167 visualization for extracted features;
     * generate half mnist digit conditioned on the other half using rbm;
     * implement centering [280][7] for all models;
     * implement classification rbms/dbms?;
     * implement elbo and ais for arbitrary dbm (again, visible and
       topmost hidden units can be analytically summed out);
     * optimize input pipeline e.g. use queues instead of feed_dict etc.

contributing

   feel free to improve existing code, documentation or implement new
   feature (including those listed in [281]possible future work). please
   open an issue to propose your changes if they are big enough.

references

   [1] r. salakhutdinov and g. hinton. deep id82s. in:
   artificial intelligence and statistics, pages 448   455, 2009. [[282]pdf]

   [2] r. salakhutdinov, j. b. tenenbaum, and a. torralba. learning with
   hierarchical-deep models. ieee transactions on pattern analysis and
   machine intelligence, 35(8):1958   1971, 2013. [[283]pdf]

   [3] a. krizhevsky and g. hinton. learning multiple layers of features
   from tiny images. 2009. [[284]pdf]

   [4] g. hinton. a practical guide to training restricted boltzmann
   machines. momentum, 9(1):926, 2010. [[285]pdf]

   [5] r. salakhutdinov and i. murray. on the quantitative analysis of
   id50. in a. mccallum and s. roweis, editors,
   proceedings of the 25th annual international conference on machine
   learning (icml 2008), pages 872   879. omnipress, 2008 [[286]pdf]

   [6] lin z, memisevic r, konda k. how far can we go without convolution:
   improving fully-connected networks, icml 2016. [[287]arxiv]

   [7] g. montavon and k.-r. m  ller. deep id82s and the
   centering trick. in neural networks: tricks of the trade, pages
   621   637. springer, 2012. [[288]pdf]

     *    2019 github, inc.
     * [289]terms
     * [290]privacy
     * [291]security
     * [292]status
     * [293]help

     * [294]contact github
     * [295]pricing
     * [296]api
     * [297]training
     * [298]blog
     * [299]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [300]reload to refresh your
   session. you signed out in another tab or window. [301]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/yell/boltzmann-machines/commits/master.atom
   3. https://github.com/yell/boltzmann-machines#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/yell/boltzmann-machines
  32. https://github.com/join
  33. https://github.com/login?return_to=/yell/boltzmann-machines
  34. https://github.com/yell/boltzmann-machines/watchers
  35. https://github.com/login?return_to=/yell/boltzmann-machines
  36. https://github.com/yell/boltzmann-machines/stargazers
  37. https://github.com/login?return_to=/yell/boltzmann-machines
  38. https://github.com/yell/boltzmann-machines/network/members
  39. https://github.com/yell
  40. https://github.com/yell/boltzmann-machines
  41. https://github.com/yell/boltzmann-machines
  42. https://github.com/yell/boltzmann-machines/issues
  43. https://github.com/yell/boltzmann-machines/pulls
  44. https://github.com/yell/boltzmann-machines/pulse
  45. https://github.com/join?source=prompt-code
  46. https://github.com/topics/ais
  47. https://github.com/topics/annealed-importance-sampling
  48. https://github.com/topics/boltzmann-machines
  49. https://github.com/topics/contrastive-divergence-algorithm
  50. https://github.com/topics/dbm
  51. https://github.com/topics/deep-learning
  52. https://github.com/topics/energy-based-model
  53. https://github.com/topics/gibbs-sampling
  54. https://github.com/topics/keras
  55. https://github.com/topics/machine-learning
  56. https://github.com/topics/mcmc
  57. https://github.com/topics/pcd
  58. https://github.com/topics/rbm
  59. https://github.com/topics/restricted-boltzmann-machine
  60. https://github.com/topics/sklearn-compatible
  61. https://github.com/topics/tensorflow
  62. https://github.com/topics/tensorflow-models
  63. https://github.com/topics/variational-id136
  64. https://github.com/yell/boltzmann-machines/commits/master
  65. https://github.com/yell/boltzmann-machines/branches
  66. https://github.com/yell/boltzmann-machines/releases
  67. https://github.com/yell/boltzmann-machines/graphs/contributors
  68. https://github.com/yell/boltzmann-machines/blob/master/license
  69. https://github.com/yell/boltzmann-machines/search?l=jupyter-notebook
  70. https://github.com/yell/boltzmann-machines/find/master
  71. https://github.com/yell/boltzmann-machines/archive/master.zip
  72. https://github.com/login?return_to=https://github.com/yell/boltzmann-machines
  73. https://github.com/join?return_to=/yell/boltzmann-machines
  74. https://desktop.github.com/
  75. https://desktop.github.com/
  76. https://developer.apple.com/xcode/
  77. https://visualstudio.github.com/
  78. https://github.com/yell
  79. https://github.com/yell/boltzmann-machines/commits?author=yell
  80. https://github.com/yell/boltzmann-machines/commit/8a2efc020ef2272df02983cef98ee7480e5b84db
  81. https://github.com/yell/boltzmann-machines/pull/9
  82. https://github.com/yell/boltzmann-machines/commit/8a2efc020ef2272df02983cef98ee7480e5b84db
  83. https://github.com/yell/boltzmann-machines/issues/7
  84. https://github.com/yell/boltzmann-machines/commit/8a2efc020ef2272df02983cef98ee7480e5b84db
  85. https://github.com/yell/boltzmann-machines/tree/8a2efc020ef2272df02983cef98ee7480e5b84db
  86. https://github.com/yell/boltzmann-machines/tree/master/boltzmann_machines
  87. https://github.com/yell/boltzmann-machines/tree/master/data
  88. https://github.com/yell/boltzmann-machines/commit/0431de445895ff9b6c6316de6a357718cac64cd6
  89. https://github.com/yell/boltzmann-machines/tree/master/docs
  90. https://github.com/yell/boltzmann-machines/commit/b198a0e5829b105b9266a661ed1928d388eb4250
  91. https://github.com/yell/boltzmann-machines/tree/master/examples
  92. https://github.com/yell/boltzmann-machines/commit/ef9dcdab8028f31d7394ac580581c1d479f7b750
  93. https://github.com/yell/boltzmann-machines/tree/master/img
  94. https://github.com/yell/boltzmann-machines/tree/master/models
  95. https://github.com/yell/boltzmann-machines/commit/9376cfbb8416ae7104e5fbdedf583a34c4e1c69e
  96. https://github.com/yell/boltzmann-machines/tree/master/notebooks
  97. https://github.com/yell/boltzmann-machines/commit/ef9dcdab8028f31d7394ac580581c1d479f7b750
  98. https://github.com/yell/boltzmann-machines/tree/master/tex
  99. https://github.com/yell/boltzmann-machines/commit/7ac779afcb2f3686df05a86e8c8ce9ccd438254d
 100. https://github.com/yell/boltzmann-machines/blob/master/.gitignore
 101. https://github.com/yell/boltzmann-machines/commit/281a5e98096c3cef821c03097aea3770ccf10c9e
 102. https://github.com/yell/boltzmann-machines/blob/master/.noserc
 103. https://github.com/yell/boltzmann-machines/blob/master/dockerfile
 104. https://github.com/yell/boltzmann-machines/commit/5e874b3d533502e6baae72ac860fb9aa6608807e
 105. https://github.com/yell/boltzmann-machines/blob/master/dockerfile-gpu
 106. https://github.com/yell/boltzmann-machines/blob/master/license
 107. https://github.com/yell/boltzmann-machines/blob/master/readme.md
 108. https://github.com/yell/boltzmann-machines/blob/master/makefile
 109. https://github.com/yell/boltzmann-machines/blob/master/requirements.txt
 110. https://github.com/yell/boltzmann-machines/commit/2600d1da6a6d3fdfafdcf8891305671a15ac6197
 111. https://github.com/yell/boltzmann-machines/blob/master/setup.py
 112. https://github.com/yell/boltzmann-machines/blob/master/tb
 113. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/rbm1.png
 114. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/samples.png
 115. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/rbm_small_0.png
 116. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/grbm.png
 117. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/mrbm.png
 118. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/samples.png
 119. https://github.com/yell/boltzmann-machines#1
 120. https://github.com/yell/boltzmann-machines#2
 121. https://github.com/yell/boltzmann-machines#3
 122. https://github.com/yell/boltzmann-machines#whats-implemented
 123. https://github.com/yell/boltzmann-machines#restricted-boltzmann-machines-rbm
 124. https://github.com/yell/boltzmann-machines#deep-boltzmann-machines-dbm
 125. https://github.com/yell/boltzmann-machines#common-features
 126. https://github.com/yell/boltzmann-machines#examples
 127. https://github.com/yell/boltzmann-machines#1-rbm-mnist-script-notebook
 128. https://github.com/yell/boltzmann-machines/blob/master/examples/rbm_mnist.py
 129. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/rbm_mnist.ipynb
 130. https://github.com/yell/boltzmann-machines#2-dbm-mnist-script-notebook
 131. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_mnist.py
 132. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_mnist.ipynb
 133. https://github.com/yell/boltzmann-machines#3-dbm-cifar-10-na  ve-script-notebook
 134. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_cifar_naive.py
 135. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_cifar_naive.ipynb
 136. https://github.com/yell/boltzmann-machines#4-dbm-cifar-10-script-notebook
 137. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_cifar.py
 138. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_cifar.ipynb
 139. https://github.com/yell/boltzmann-machines#how-to-use-examples
 140. https://github.com/yell/boltzmann-machines#memory-requirements
 141. https://github.com/yell/boltzmann-machines#download-models-and-stuff
 142. https://github.com/yell/boltzmann-machines#tex-notes
 143. https://github.com/yell/boltzmann-machines#how-to-install
 144. https://github.com/yell/boltzmann-machines#common-installation-issues
 145. https://github.com/yell/boltzmann-machines#possible-future-work
 146. https://github.com/yell/boltzmann-machines#contributing
 147. https://github.com/yell/boltzmann-machines#references
 148. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/tf_graph.png
 149. https://github.com/yell/boltzmann-machines#tex-notes
 150. https://github.com/yell/boltzmann-machines#1
 151. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/msre.png
 152. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/pll.png
 153. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/feg.png
 154. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/l2_loss.png
 155. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/dist_w.png
 156. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/dist_hb.png
 157. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/dist_dw.png
 158. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/dist_dvb.png
 159. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/hist_w.png
 160. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/hist_hb.png
 161. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/hist_dw.png
 162. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/hist_dvb.png
 163. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/hidden_activations.gif
 164. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/mnist_5.gif
 165. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/mnist_8.gif
 166. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/cifar_small_6_1.gif
 167. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/cifar_small_8_6.gif
 168. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/cifar_6.gif
 169. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/cifar_18.gif
 170. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_rbm/cifar_9.gif
 171. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/tf_graph.png
 172. https://github.com/yell/boltzmann-machines#1
 173. https://github.com/yell/boltzmann-machines#1
 174. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/msre.png
 175. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/n_mf_updates.png
 176. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/w_norm.png
 177. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_w.png
 178. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_w2.png
 179. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_hb2.png
 180. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_dw.png
 181. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_dvb.png
 182. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/dist_mu2.png
 183. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/hist_w.png
 184. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/hist_dw.png
 185. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/hist_mu.png
 186. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/mnist_filter_l1_5.gif
 187. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/mnist_filter_l2_6.gif
 188. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_filter_l1_6.gif
 189. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_filter_l2_2.gif
 190. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_filter_l2_6.gif
 191. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/mnist_particle_l1_2.gif
 192. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/mnist_particle_l1_4.gif
 193. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_particle_l1_1.gif
 194. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_particle_l1_2.gif
 195. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/cifar_particle_l1_4.gif
 196. https://github.com/yell/boltzmann-machines/blob/master/img/tensorboard_dbm/mnist_particles_l23.gif
 197. https://github.com/yell/boltzmann-machines/blob/master/examples/rbm_mnist.py
 198. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/rbm_mnist.ipynb
 199. https://github.com/yell/boltzmann-machines/blob/master/img/rbm_mnist/filters.png
 200. https://github.com/yell/boltzmann-machines/blob/master/img/rbm_mnist/filters_finetuned.png
 201. https://github.com/yell/boltzmann-machines/blob/master/img/rbm_mnist/confusion_matrix.png
 202. https://github.com/yell/boltzmann-machines#2
 203. https://github.com/yell/boltzmann-machines#2
 204. https://github.com/yell/boltzmann-machines/blob/master/docs/rbm_discriminative.md
 205. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_mnist.py
 206. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_mnist.ipynb
 207. https://github.com/yell/boltzmann-machines#1
 208. https://github.com/yell/boltzmann-machines#5
 209. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/rbm1.png
 210. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/w1_joint.png
 211. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/w1_finetuned.png
 212. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/rbm2.png
 213. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/w2_joint.png
 214. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/w2_finetuned.png
 215. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/mnist.png
 216. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/samples.png
 217. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_mnist/samples.gif
 218. https://github.com/yell/boltzmann-machines/blob/master/docs/dbm_discriminative.md
 219. https://github.com/yell/boltzmann-machines#1
 220. https://github.com/yell/boltzmann-machines#2
 221. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_cifar_naive.py
 222. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_cifar_naive.ipynb
 223. https://github.com/yell/boltzmann-machines#3
 224. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/grbm.png
 225. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/w1_joint.png
 226. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/mrbm.png
 227. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/w2_joint.png
 228. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/cifar10_smoothed.png
 229. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/samples.png
 230. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/samples.gif
 231. https://github.com/yell/boltzmann-machines#3
 232. https://github.com/yell/boltzmann-machines#tex-notes
 233. https://github.com/yell/boltzmann-machines#1
 234. https://github.com/yell/boltzmann-machines#6
 235. https://github.com/yell/boltzmann-machines#3
 236. https://github.com/yell/boltzmann-machines#3
 237. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/grbm.png
 238. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/grbm_finetuned.png
 239. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar_naive/grbm_confusion_matrix.png
 240. https://github.com/yell/boltzmann-machines/blob/master/examples/dbm_cifar.py
 241. https://nbviewer.jupyter.org/github/monsta-hd/boltzmann-machines/blob/master/notebooks/dbm_cifar.ipynb
 242. https://github.com/yell/boltzmann-machines#3
 243. https://github.com/yell/boltzmann-machines#2
 244. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/rbm_small_0.png
 245. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/rbm_small_2.png
 246. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/rbm_small_10.png
 247. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/rbm_small_20.png
 248. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/grbm.png
 249. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/mrbm.png
 250. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/samples.png
 251. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/w1_joint.png
 252. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/w2_joint.png
 253. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar/samples.gif
 254. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/rbm_small_0.png
 255. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/rbm_small_2.png
 256. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/rbm_small_10.png
 257. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/rbm_small_20.png
 258. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/grbm.png
 259. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/mrbm.png
 260. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/samples.png
 261. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/w1_joint.png
 262. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/w2_joint.png
 263. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/samples.gif
 264. https://github.com/yell/boltzmann-machines#3
 265. https://github.com/yell/boltzmann-machines#3-dbm-cifar-10-na  ve-script-notebook
 266. https://github.com/yell/boltzmann-machines/blob/master/docs/grbm_discriminative.md
 267. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/grbm.png
 268. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/grbm_no_aug_finetuned.png
 269. https://github.com/yell/boltzmann-machines/blob/master/img/dbm_cifar2/grbm_confusion_matrix.png
 270. https://drive.google.com/open?id=1jfsh4jh3s41b-_hphe_vs9apkmmiwiny
 271. https://github.com/yell/boltzmann-machines#3-dbm-cifar-10-na  ve-script-notebook
 272. https://github.com/yell/boltzmann-machines#4-dbm-cifar-10-script-notebook
 273. https://github.com/yell/boltzmann-machines/blob/master/tex/notes.pdf
 274. https://www.dropbox.com/s/7pk4yeixkxogcem/bm_notes.pdf?dl=0
 275. https://github.com/yell/boltzmann-machines/blob/master/requirements.txt
 276. https://github.com/yell/boltzmann-machines/blob/master/docs/virtualenv.md
 277. https://github.com/yell/boltzmann-machines/blob/master/docs/docker.md
 278. https://github.com/jakevdp/jsanimation
 279. https://stackoverflow.com/questions/42013316/after-building-tensorflow-from-source-seeing-libcudart-so-and-libcudnn-errors
 280. https://github.com/yell/boltzmann-machines#7
 281. https://github.com/yell/boltzmann-machines#possible-future-work
 282. http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf
 283. https://www.cs.toronto.edu/~rsalakhu/papers/hd_pami.pdf
 284. https://www.cs.toronto.edu/~kriz/learning-features-2009-tr.pdf
 285. https://www.cs.toronto.edu/~hinton/absps/guidetr.pdf
 286. http://homepages.inf.ed.ac.uk/imurray2/pub/08dbn_ais/dbn_ais.pdf
 287. https://arxiv.org/abs/1511.02580
 288. http://gregoire.montavon.name/publications/montavon-lncs12.pdf
 289. https://github.com/site/terms
 290. https://github.com/site/privacy
 291. https://github.com/security
 292. https://githubstatus.com/
 293. https://help.github.com/
 294. https://github.com/contact
 295. https://github.com/pricing
 296. https://developer.github.com/
 297. https://training.github.com/
 298. https://github.blog/
 299. https://github.com/about
 300. https://github.com/yell/boltzmann-machines
 301. https://github.com/yell/boltzmann-machines

   hidden links:
 303. https://github.com/
 304. https://github.com/yell/boltzmann-machines
 305. https://github.com/yell/boltzmann-machines
 306. https://github.com/yell/boltzmann-machines
 307. https://help.github.com/articles/which-remote-url-should-i-use
 308. https://github.com/yell/boltzmann-machines#boltzmann-machines
 309. https://github.com/yell/boltzmann-machines#table-of-contents
 310. https://github.com/yell/boltzmann-machines#whats-implemented
 311. https://github.com/yell/boltzmann-machines#restricted-boltzmann-machines-rbm
 312. https://github.com/yell/boltzmann-machines#deep-boltzmann-machines-dbm
 313. https://github.com/yell/boltzmann-machines#common-features
 314. https://github.com/yell/boltzmann-machines#examples
 315. https://github.com/yell/boltzmann-machines#1-rbm-mnist-script-notebook
 316. https://github.com/yell/boltzmann-machines#2-dbm-mnist-script-notebook
 317. https://github.com/yell/boltzmann-machines#3-dbm-cifar-10-na%c3%afve-script-notebook
 318. https://github.com/yell/boltzmann-machines#4-dbm-cifar-10-script-notebook
 319. https://github.com/yell/boltzmann-machines#how-to-use-examples
 320. https://github.com/yell/boltzmann-machines#memory-requirements
 321. https://github.com/yell/boltzmann-machines#download-models-and-stuff
 322. https://github.com/yell/boltzmann-machines#tex-notes
 323. https://github.com/yell/boltzmann-machines#how-to-install
 324. https://github.com/yell/boltzmann-machines#common-installation-issues
 325. https://github.com/yell/boltzmann-machines#possible-future-work
 326. https://github.com/yell/boltzmann-machines#contributing
 327. https://github.com/yell/boltzmann-machines#references
 328. https://github.com/
