   #[1]rss feed

[2]count bayesie

   probably a id203 blog

     * [3]blog
     * [4]all posts
     * [5]about
     * [6]books
     * [7]subscribe

   [blog______]

[8]6 neat tricks with monte carlo simulations

   [9]march 24, 2015 [10]by will kurt

   if there is one trick you should know about id203, its how to
   write a monte carlo simulation. if you can program, even just a little,
   you can write a monte carlo simulation. most of my work is in either r
   or python, these examples will all be in r since out-of-the-box r has
   more tools to run simulations. the basics of a monte carlo simulation
   are simply to model your problem, and than randomly simulate it until
   you get an answer. the best way to explain is to just run through a
   bunch of examples, so let's go!

integration

   we'll start with basic integration. suppose we have an instance of a
   normal distribution with a mean of 1 and a standard deviation of 10.
   then we want to find [11]the integral from 3 to 6 $$\int_{3}^6
   \frac{1}{10\sqrt{2\pi}}e^{-\frac{(x-1)^2}{2\cdot10^2}}dx $$as
   visualized below
   in blue is the area we wish to integrate over in blue is the area we
   wish to integrate over

   in blue is the area we wish to integrate over

   we can simply write a simulation that samples from this
   distribution 100,000 times and see how many values are between 3 and 6.
runs <- 100000
sims <- rnorm(runs,mean=1,sd=10)
mc.integral <- sum(sims >= 3 & sims <= 6)/runs

   the result we get is:

   mc.integral  = 0.1122

   which isn't too far off from the 0.112203 that [12]wolfram alpha gives
   us.

approximating the binomial distribution

   we flip a coin 10 times and we want to know the id203 of getting
   more than 3 heads. now this is a trivial problem for [13]the binomial
   distribution, but suppose we have forgotten about this or never learned
   it in the first place. we can easily solve this problem with a monte
   carlo simulation. we'll use the common trick of representing tails with
   0 and heads with 1, then simulate 10 coin tosses 100,000 times and see
   how often that happens.
runs <- 100000
#one.trail simulates a single round of toss 10 coins
#and returns true if the number of heads is > 3
one.trial <- function(){
  sum(sample(c(0,1),10,replace=t)) > 3
}
#now we repeat that trial 'runs' times.
mc.binom <- sum(replicate(runs,one.trial()))/runs

   for our ad hoc binomial distribution we get
   mc.binom = 0.8279

   which we can compare to r's builtin binomial distribution function
   pbinom(3,10,0.5,lower.tail=false) = 0.8281

approximating pi

   next we'll move on to something a bit trickier, approximating pi!

   we'll start by refreshing on some basic facts. the area of a circle is
   $$a_{\text{circle}} = \pi r^2$$ and if we draw a square containing that
   circle its area will be $$a_{\text{square}} = 4r^2$$ this is because
   each side is simply \(2r\) as can be seen in this image:
   basic properties of a circle basic properties of a circle

   basic properties of a circle

   now how do we get \(\pi\)? the ratio of the area of the circle to the
   area of the square is$$\frac{\pi r^2}{4r^2}$$ which we can reduce to
   simply $$\frac{\pi}{4}$$ given this fact, if we can empiricaly
   determine the ratio of the area of the circle to the area of the square
   we can simply multiply this number by 4 and we'll get our approximation
   of \(\pi\).

   to do this we can randomly sample \(x\) and \(y\) values from a unit
   square centered around 0. if \(x^2 + y^2 \leq r^2 \) then the point is
   in the circle (in this case \(r = 0.5\)). the ratio of points in the
   circle to total points sample multiplied by 4 should then approximate
   \(pi\).
runs <- 100000
#runif samples from a uniform distribution
xs <- runif(runs,min=-0.5,max=0.5)
ys <- runif(runs,min=-0.5,max=0.5)
in.circle <- xs^2 + ys^2 <= 0.5^2
mc.pi <- (sum(in.circle)/runs)*4
plot(xs,ys,pch='.',col=ifelse(in.circle,"blue","grey")
     ,xlab='',ylab='',asp=1,
     main=paste("mc approximation of pi =",mc.pi))

   sampling 100,000 points inside and outside of a circle sampling 100,000
   points inside and outside of a circle

   sampling 100,000 points inside and outside of a circle

   the more runs we have the more accurately we can approximate \(\pi\).

finding our own p-values

   let's suppose we're comparing two webpages to see which one converts
   our customers to "sign up" at a higher rate (this is commonly referred
   to as [14]an a/b test). for page a we have seen 20 convert and 100 not
   convert, for page b we have 38 converting and 110 not converting. we'll
   model this as two beta distributions as we can see below:
   visualizing the possible overlap between two beta distributions
   visualizing the possible overlap between two beta distributions

   visualizing the possible overlap between two beta distributions

   it certainly looks like b is the winner, but we'd really like to know
   how likely this is. we could of course run a single tailed t-test, that
   would require that we assume that these are normal distributions (which
   isn't a terrible approximation in this case). however we can also solve
   this via a monte carlo simulation! we're going to take 100,000 samples
   from a and 100,000 samples from b and see how often a ends up being
   larger than b.
runs <- 100000
a.samples <- rbeta(runs,20,100)
b.samples <- rbeta(runs,38,110)
mc.p.value <- sum(a.samples > b.samples)/runs

   and we have mc.p.value =  0.0348

   awesome our results are "statistically significant"!

   ..but wait, there's more! we can also plot out a histogram for of the
   differences to see how big a difference there might be between our two
   tests!
hist(b.samples/a.samples)

   by simulating samples between two distributions we can see the average
   improvement. by simulating samples between two distributions we can see
   the average improvement.

   by simulating samples between two distributions we can see the average
   improvement.


   now we can actually reason about how much of a risk we are taking if we
   go with b over a!

games of chance

   if we bring back the spinner from the post on [15]expectation we can
   play a new game!
   the great spinner of id203! the great spinner of id203!

   the great spinner of id203!

   in this game landing on 'yellow' you gain 1 point, 'red' you lose 1
   point and 'blue' you gain 2 points. we can easily calculate the
   expectation: $$e(\text{game}) = 1/2 \cdot 1 + 1/4 \cdot -1 + 1/4 \cdot
   2 = 0.75$$ this could have been calculated with a monte carlo
   simulation, but the hand calculation is really easy. let's ask a
   trickier question "after 10 spins what is the probabilitly that you'll
   have less then 0 points?" there are methods to analytically solve this
   type of problem, but by the time they are even explained we could have
   already written our simulation!

   to solve this with a monte carlo simulation we're going to sample from
   our spinner 10 times, and return 1 if we're below 0 other wise we'll
   return 0. we'll repeat this 100,000 times to see how often it happens!
runs <- 100000
#simulates on game of 10 spins, returns whether the sum of all the spins is < 1
play.game <- function(){
  results <- sample(c(1,1,-1,2),10,replace=t)
  return(sum(results) < 0)
}
mc.prob <- sum(replicate(runs,play.game()))/runs

   and now we have our estimate on the likelihood of having negative
   points after just 10 spins.

   mc.prob = 0.01314

predicting the stock market

   finally countbayes.com has ipo'd! it trades under the ticker symbol
   bayz. on average it gains 1.001 times its opening price during the
   trading day, but that can vary by a standard deviation of 0.005 on any
   given day (this is its volatility). we can simulate a single sample
   path for bayz by taking the cumulative product from a normal
   distribution with a mean of 1.001 and a sd of 0.005. assuming bayz
   opens at $20/per share here is a sample path for 200 days of bayz
   trading.
days <- 200
changes <- rnorm(200,mean=1.001,sd=0.005)
plot(cumprod(c(20,changes)),type='l',ylab="price",xlab="day",main="bayz closing
price (sample path)")

   bayz stock ticker bayz stock ticker

   bayz stock ticker

   but this is just one possible future! if you are thinking of investing
   in bayz you want to know what are the possible closing prices of the
   stock at the end of 200. to assess risk in this stock we need to know
   what are reasonable upper and lower bounds on the future price.

   to solve this we'll just simulate 100,000 different possible paths the
   stock could take and then look at the distribution of closing prices.
runs <- 100000
#simulates future movements and returns the closing price on day 200
generate.path <- function(){
  days <- 200
  changes <- rnorm(200,mean=1.001,sd=0.005)
  sample.path <- cumprod(c(20,changes))
  closing.price <- sample.path[days+1] #+1 because we add the opening price
  return(closing.price)
}

mc.closing <- replicate(runs,generate.path())

   the median price of bayz at the end of 200 days is simply
   median(mc.closing) = 24.36
   but we can also see the upper and lower 95th percentiles

   quantile(mc.closing,0.95) = 27.35

   quantile(mc.closing,0.05) = 21.69

   nb - this is a toy model of stock market movements, even models that
   are generally considered poor models of stock prices at the very least
   would use a log-normal distribution. but those details deserve a post
   of their own! real world quantitative finance makes heavy use of monte
   carlo simulations.

just the beginning!

   by now it should be clear that a few lines of r can create extremely
   good estimates to a whole host of problems in id203 and
   statistics. there comes a point in problems involving id203 where
   we are often left no other choice than to use a monte carlo simulation.
   this is just the beginning of the incredible things that can be done
   with some extraordinarily simple tools. it also turns out that monte
   carlo simulations are at the heart of many forms of [16]bayesian
   id136.

   for more examples of using monte carlo simulations check out these
   posts:
     * build your own [17]rejection sampler in r.
     * using a mc simulation to solve [18]a variant of the coupon
       collectors puzzle.
     * as an alternative, [19]solve markov chains with id202
       instead of monte carlo methods


   if you enjoyed this post please[20] subscribe to keep up to date and
   follow [21]@willkurt!

   [22]march 24, 2015 /[23]will kurt
   [24]march 2015, [25]popular

     * [26]newer
     * [27]older

   powered by [28]squarespace

references

   1. http://www.countbayesie.com/blog?format=rss
   2. https://www.countbayesie.com/
   3. https://www.countbayesie.com/
   4. https://www.countbayesie.com/all-posts
   5. https://www.countbayesie.com/about
   6. https://www.countbayesie.com/books
   7. https://www.countbayesie.com/subscribe
   8. https://www.countbayesie.com/blog/2015/3/3/6-amazing-trick-with-monte-carlo-simulations
   9. https://www.countbayesie.com/blog/2015/3/3/6-amazing-trick-with-monte-carlo-simulations
  10. https://www.countbayesie.com/?author=54e50c13e4b006a58919aa7a
  11. https://www.countbayesie.com/blog/2015/7/19/fundamental-theorem-of-calculus
  12. http://www.wolframalpha.com/input/?i=integral(pdf(normaldistribution(1,10),x),3,6)
  13. https://www.countbayesie.com/blog/2015/3/17/interrogating-id203-distributions
  14. https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing
  15. http://www.countbayesie.com/blog/2015/2/20/random-variables-and-expectation
  16. https://www.countbayesie.com/blog/2015/6/20/tricky-priors-and-rejection-sampling
  17. https://www.countbayesie.com/blog/2015/6/20/tricky-priors-and-rejection-sampling
  18. https://www.countbayesie.com/blog/2015/10/13/the-toy-collectors-puzzle
  19. https://www.countbayesie.com/blog/2015/11/21/the-black-friday-puzzle-understanding-markov-chains
  20. http://countbayesie.com/subscribe
  21. https://twitter.com/willkurt
  22. https://www.countbayesie.com/blog/2015/3/3/6-amazing-trick-with-monte-carlo-simulations
  23. https://www.countbayesie.com/?author=54e50c13e4b006a58919aa7a
  24. https://www.countbayesie.com/?tag=march+2015
  25. https://www.countbayesie.com/?tag=popular
  26. https://www.countbayesie.com/blog/2015/3/17/interrogating-id203-distributions
  27. https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors
  28. http://www.squarespace.com/
