   image alt text

   babble labble:
   learning from natural language explanations

   post by braden hancock, percy liang and chris r  
   posted march 2017
   update (dec 2017): nips 2017 [1]demo video
   update (may 2018): acl 2018 paper: [2]training classifiers with natural
   language explanations
   back to [3]snorkel blog

   and referencing work by many other [4]members of hazy research

     modern machine learning models that automate feature extraction
     (such as deep neural nets) can do incredible things with enough
     labeled training data! the hard part is getting enough labeled
     training data to properly train their thousands (or millions!) of
     parameters. and in the age where "[5]data never sleeps", the
     bottleneck isn   t getting the data    it   s getting the labels.

     we propose babble labble, a framework for generating labels for
     large training sets from natural language explanations. in this
     framework, a semantic parser converts explanations into executable
     functions, which feed a [6]data programming application and
     ultimately train a powerful downstream classifier for a given task.
     by learning from natural language explanations of labeling
     decisions, we achieve comparable quality to fully supervised
     approaches with a fraction of the data.

                               image alt text

how should i label my data?

   imagine you   re building a classifier to predict whether or not two
   people mentioned in a sentence are married. you've collected a few
   million sentences to draw from, and you've hired some mechanical
   turkers (humans online who perform simple tasks for money) to help you
   collect some labels. how you should have them label your data?

  option 1: with labels (one by one)

     in traditional supervision, labelers find reasons for giving certain
     labels, return only the labels, then make algorithms work to
     rediscover the reasons.

   imagine a turker (let's call him joe) who sees the following example:

                               image alt text

   joe reads the sentence. he looks at the words barack and michelle, and
   the words around them. he decides that because barack and michelle have
   the word    and    between them, and the words    their daughter    occur later
   in the sentence, they   re likely married. having found a solid reason
   for labeling true, joe marks    true   . as joe goes through the data set,
   specific reasons for labeling examples one way or another come up over
   and over again, but all we ever collect is the binary label    true    or
      false   .

   downstream, you'd like your classifier to look at this example and
   others like it and learn to use some of the same reasons--or
   *features*--that joe found, but instead of conveying these reasons
   directly, we expect the classifier to learn them from scratch using
   nothing but "true"s and "false"s. (it   s like the classifier keeps
   asking for directions and instead of pointing exactly where to go, the
   turkers just tease it with a vague    you   re getting warmer    or    you   re
   getting colder   !). especially in situations where large sets of labels
   aren't readily available, this seems like an incredibly wasteful
   process!

                               image alt text

  option 2: with labeling functions (via data programming)

     in data programming, users write labeling functions that are used to
     assign noise-aware labels to unlabeled data.

   in a [7]data programming approach, instead of providing binary labels,
   joe writes functions (called "labeling functions" or lfs) that label
   data "accurately enough". these functions don't necessarily have
   perfect recall or precision, and they are allowed to overlap and
   conflict with one another.

                               image alt text

   by looking at how often the labeling functions agree or disagree with
   one another, we learn estimated accuracies for each supervision source
   (e.g., an lf that all the other lfs tend to agree with will have a high
   learned accuracy, whereas an lf that seems to be disagreeing with all
   the others whenever they vote on the same example will have a low
   learned accuracy). and by combining the votes of all the labeling
   functions (weighted by their estimated accuracies), we   re able to
   assign each example a fuzzy "noise-aware" label (between 0 and 1)
   instead of a hard label (either 0 or 1).

   [majority_vote.png]

   three big pros of this approach are:
    1. we've improved the scalability of our labeling approach: each lf
       can contribute label information to tens, hundreds, or thousands of
       examples--not just one.
    2. we now have a use for unlabeled data. we can apply our lfs on all
       the unlabeled examples to create a whole lot of not perfect, but
          good enough    labels for a potentially huge training data set.
    3. these labels can be used to train a powerful discriminative
       classifier with a large feature set that generalizes beyond the
       reasons directly addressed by the lfs. (so even if we only use 100
       lfs, the examples they label may each have thousands of features
       whose weights are learned by the discriminative classifier).

   one big con of this approach is:
    1. joe can't program! he is, after all, just your average joe.

  option 3: with explanations (via babble labble)

     the babble labble approach uses a semantic parser to convert natural
     language explanations into    accurate enough    labeling functions for
     use with data programming.

   babble labble converts natural language (babble) into labels (or
   labbles   we've heard it both ways). now, joe's labeling interface looks
   like this:

   [hit_babble.png]
   what are the benefits of having a natural language interface? here are
   a few:
    1. ease of use
       the world is moving to conversational interfaces. sure, i can type,
       but sometimes i prefer to just say    hey google/siri/alexa    and talk
       to my computer/phone/television/toaster.
    2. faster supervision
       even if you could count on all turkers being able to code, you can
       be fairly certain it would take them longer to write small programs
       than it would for them type a sentence or two.
    3. more sources of supervision
       there   s a lot of natural language in the world. in its current
       relative infancy, babble labble works with explanations that were
       meant to be used for supervision. but imagine a future where your
       virtual assistant can learn how to classify certain things in the
       world just by reading what people say about them in subreddits and
       blog comments!

   [robot_reading.jpg]

babble labble

   the core component of babble labble is a semantic parser. semantic
   parsers convert natural language explanations into formal programs (or
   in our case, labeling functions). part of the reason why programs are
   written in programming languages instead of natural languages is
   because natural language can be ambiguous. this means that a single
   explanation often maps to many different possible lfs   one that was
   intended and others that are spurious (but which may also be factually
   correct).

   [semantic_parser.png]
   in a traditional approach, one might use a bunch of labeled data to
      train    the semantic parser and learn which of the generated lfs is
   most likely to be the correct one. but fortunately, in this framework
   with a downstream application that was made to handle noise, we don't
   need a perfect parser to still get great results! just like how we
   found that we can use a larger set of    accurate enough    labels to train
   the discriminative model in data programming, we propose that we can
   use a larger set of    accurate enough    labeling functions to train the
   generative model in data programming. this may seem a little cavalier
   of us, but there are reasons for believing this just might work:

   reason #1. common sense filters catch many spurious lfs
     * filter 1: example contradiction
       with the labeling interface joe used, each lf comes from an
       explanation that is tied to a specific example. if the lf   s
       explanation doesn   t agree with the label given to its own
       corresponding example, then we can confidently toss it.
     * filter 2: uniform labels
       if an lf does correctly label its corresponding example, but it
       does so by naively labeling all examples as true or all as false,
       then it provides no useful signal and can also be tossed.
     * filter 3: redundant signature
       suppose one explanation yielded three lfs that label identically
       every single example in the large training set extracted from the
       unlabeled data. while the three lfs may in fact be unique in some
       way, their semantic difference is small enough that they provide
       redundant information for our application, and all but one can be
       tossed.

   reason #2. data programming mitigates the negative effects of spurious
   lfs
     * model the lfs' accuracies
       in the data programming paradigm, labeling functions don   t need to
       be perfectly accurate to be useful. if a mistake in the semantic
       parser results in a labeling function that is less accurate than it
       was intended to be, the generative model will have the opportunity
       to recognize this when it sees its disagreement with other lfs and
       reduce its estimated accuracy accordingly.
     * model the lfs' dependencies
       what if one particularly ambiguous explanation generates a whole
       bunch of lfs that pass the filters and all agree with each other
       despite being wrong   won   t that falsely boost their learned
       accuracies? it would...if we believed that all lfs label
       independently. but because we are able to automatically [8]learn
       the dependencies between the lfs in data programming, this mob
       behavior can be recognized for what it is and voting power of all
       the lfs participating in this    echo chamber    can be reduced
       accordingly.

   reason #3. spurious lfs can be sometimes even helpful!
   the labeler   s explanation describes a very specific signal for the
   learner to pay attention to. in the extremely high dimensional space of
   all possible features, minor parsing inaccuracies may still result in a
   relevant signal making it through. for example, the latter half of
   joe   s explanation may be misinterpreted by the semantic parser as    the
   words    their daughter    occur anywhere in the sentence   . while not as
   specific as the intended explanation, it may end up correctly labeling
   all of the same examples as the intended function as well as new
   examples such as    following the birth of their daughter, beyonc   and
   jay-z took to twitter to celebrate.    the key here is that most of what
   joe was trying to convey in this case is captured in the string "their
   daughter"; many imperfect parses will still result in that most
   important signal getting through just fine.

case study: classifying disease-causing chemicals

   consider the task of extracting chemical-disease pairs from the
   scientific literature where a chemical is reported to cause a certain
   disease. since this task requires domain-specific knowledge that the
   average turker lacks, we took 30 labeling functions that were first
   written by a biomedical researcher and had them paraphrased into
   natural language by someone completely unfamiliar with our system. the
   paraphraser was also given dictionaries that were assembled by the
   researcher, such as a dictionary of    treatment    words and a dictionary
   of chemical-disease pairs where the chemical is known to be used for
   therapy of the disease. examples of the resulting explanations include:

   [cdr_explanations.png]
   using id125, the 30 explanations were parsed into 104 lfs.
   applying the filters removed 77 of these, resulting in 27 remaining
   lfs. the data programming platform, snorkel, identified 11 dependencies
   among the lfs, and in one case, a spurious lf ended up having a higher
   empirical accuracy than the intended one. on this very difficult task:
     * traditional supervision with 1k gold labels achieves an f1 score of
       41.9.
     * babble labble with 30 explanation/example pairs (resulting in 27
       natural-language-based lfs) and a 6.7k unlabeled training set
       achieves an f1 score of 42.3.

   that is, by utilizing natural language explanations corresponding to 30
   labeled examples, we were able to achieve the same quality as a fully
   supervised system on this task while using 33x fewer gold training
   examples.

   [cdr_results.png]

next steps

   we are very excited about our initial results! next up:
     * we want to run tests over more domains to really tease out under
       what conditions babble labble excels.
     * we would like to explore whether a feedback loop can be
       incorporated from the downstream task back to the semantic parser:
       the semantic parser generates functions, the top k of which we use
       to generate approximate labels for a bunch of training examples,
       which are used to train the semantic parser, allowing us to select
       a better top k labeling functions, from which we get more accurate
       approximate labels...

references

   1. https://www.youtube.com/watch?v=ybeax-demdg
   2. https://arxiv.org/abs/1805.03818
   3. http://snorkel.stanford.edu/
   4. http://cs.stanford.edu/people/chrismre/#students
   5. https://www.domo.com/blog/data-never-sleeps-4-0/
   6. http://hazyresearch.github.io/snorkel/blog/weak_supervision.html
   7. http://hazyresearch.github.io/snorkel/blog/weak_supervision.html
   8. https://arxiv.org/abs/1703.00854
