   #[1]alternate [2]alternate

   hiring? toptal handpicks [3]top machine learning engineers to suit your
   needs.

     * [4]start hiring
     * [5]log in

     *
     * [6]top 3%
     * [7]why
     * [8]clients
     * [9]enterprise
     * [10]community
     * [11]blog
     * [12]about us
     * [13]start hiring
     * [14]apply as a developer
     * [15]login
     *
          + questions?
          + [16]contact us
          +
          +
          +

   ____________________ (button)
   [17]hire a developer

   hiring? toptal handpicks [18]top machine learning engineers to suit
   your needs.

     * [19]start hiring
     * [20]log in

   9 min read

from solving equations to deep learning: a tensorflow python tutorial

   [21]view all articles

   [small_a14e7ad7daeea1a490068f7f80d3ee58.jpg]

   by [22]oliver holloway - freelance software engineer @ [23]toptal
   [24]#deeplearning [25]#python [26]#tensorflow
     * 0shares
     * [twitter_83c6d4.png]
     * [facebook_dc66c9.png]
     * [linkedin_b923c3.png]
     * [pocket_e4260c.png]

   read the [27]spanish version of this article translated by marisela
   ordaz

   there have been some remarkable developments lately in the world of
   artificial intelligence, from much publicized progress with
   self-driving cars to machines now [28]composing chopin imitations or
   [29]just being really good at video games.

   central to these advances are a number of [30]tools around to help
   derive deep learning and other machine learning models, with torch,
   caffe, and theano amongst those at the fore. however, since google
   brain went open source in november 2015 with their own framework,
   tensorflow, we have seen the popularity of this software library
   skyrocket to be the most popular deep learning framework.

   tensorflow

   why has this happened? reasons include the wealth of support and
   documentation available, its production readiness, the ease of
   distributing calculations across a range of devices, and an excellent
   visualization tool: [31]tensorboard.

   ultimately, tensorflow manages to combine a comprehensive and flexible
   set of technical features with great ease of use.

   in this article, you will gain an understanding of the mechanics of
   this tool by using it to solve a general numerical problem, quite
   outside of what machine learning usually involves, before introducing
   its uses in deep learning with a simple neural network implementation.

before you begin

   a basic knowledge of machine learning methods is assumed. if you need
   catching up, check out this very useful [32]post.

   as we will be demonstrating the python api, an understanding of numpy
   is also beneficial.

   to set up tensorflow, please follow the instructions found [33]here.

   if you are using windows, it should be noted that, at the time of
   writing, you must use python 3.4+, not 2.7.

   then when you are ready, you should be able to import the library with:
import tensorflow as tf

step 1 of 2 to a tensorflow solution: create a graph

   the construction of tensorflow programs generally consist of two major
   steps, the first of which is to build a computational graph, which will
   describe the computations you wish to carry out, but not actually carry
   them out or hold any values.

   as with any graph, we have nodes and edges. the edges represent
   tensors, a tensor representing an n-dimensional array. for example, a
   tensor with dimension (or rank in tensorflow speak) 0 is a scalar, rank
   1 a vector, rank 2 a matrix and so on.

   nodes represent operations which produce an output tensor, taking
   tensors as inputs if needed. such operations include additions
   (tf.add), id127s (tf.matmul), and also the creation of
   constants (tf.constant).

   so, let   s combine a few of these for our first graph.
a = tf.constant([2.5, 2])
b = tf.constant([3, 6], dtype=tf.float32)
total = tf.add(a, b)

   here we have created three operations, two of them to create constant
   1-d arrays.

   the data types are inferred from the values argument passed in, or you
   can denote these with the dtype argument. if i hadn   t have done this
   for b, then an int32 would have been inferred and an error thrown as
   tf.add would have been trying to define an addition on two different
   types.

step 2 of 2 to a tensorflow solution: execute the operations

   the graph is defined, but in order to actually do any calculations on
   it (or any part of it) we have to set up a tensorflow session.
sess = tf.session()

   alternatively, if we are running a session in an interactive shell,
   such as ipython, then we use:
sess = tf.interactivesession()

   the run method on the session object is one way to evaluate a tensor.

   therefore, to evaluate the addition calculation defined above, we pass
      total   , the tensor to retrieve, which represents the output of the
   tf.add op.
print(sess.run(total)) # [ 5.5  8. ]

   at this point, we introduce tensorflow   s variable class. whereas
   constants are a fixed part of the graph definition, variables can be
   updated. the class constructor requires an initial value, but even
   then, variables need an operation to explicitly initialize them before
   any other operations on them are carried out.

   variables hold the state of the graph in a particular session so we
   should observe what happens with multiple sessions using the same graph
   to better understand variables.
# create a variable with an initial value of 1
some_var = tf.variable(1)

# create op to run variable initializers
init_op = tf.global_variables_initializer()

# create an op to replace the value held by some_var to 3
assign_op = some_var.assign(3)

# set up two instances of a session
sess1 = tf.session()
sess2 = tf.session()

# initialize variables in both sessions
sess1.run(init_op)
sess2.run(init_op)
print(sess1.run(some_var)) # outputs 1

# change some_var in session1
sess1.run(assign_op)
print(sess1.run(some_var)) # outputs 3
print(sess2.run(some_var)) # outputs 1

# close sessions
sess1.close()
sess2.close()

   we   ve set up the graph and two sessions.

   after executing the initialization on both sessions (if we don   t run
   this and then evaluate the variable we hit an error) we only execute
   the assign op on one session. as one can see, the variable value
   persists, but not across sessions.

feeding the graph to tackle numerical problems

   another important concept of tensorflow is the placeholder. whereas
   variables hold state, placeholders are used to define what inputs the
   graph can expect and their data type (and optionally their shape). then
   we can feed data into the graph via these placeholders when we run the
   computation.

   the tensorflow graph is beginning to resemble the neural networks we
   want to eventually train, but before that, lets use the concepts to
   solve a common numerical problem from the financial world.

   suppose we want to find y in an equation like this:

                   v = ce^-0.5y+ ce^-y+ce^-1.5y+(c+p)e^-2y

   for a given v (with c and p constant).

   this is a formula for working out the yield-to-maturity (y) on a bond
   with market value v, principal p, and coupon c paid semi-annually but
   with the cash flows discounted with continuous compounding.

   we basically have to solve an equation like this with trial and error,
   and we will choose the bisection method to zero in on our final value
   for y.

   first, we will model this problem as a tensorflow graph.

   c and p are fixed constants and form part of the definition of our
   graph. we wish to have a process that refines the lower and upper
   bounds of y. therefore, these bounds (denoted a and b) are good
   candidates for variables that need to be changed after each guess of y
   (taken to be the midpoint of a and b).
# specify the values our constant ops will output
c = tf.constant(5.0)
p = tf.constant(100.0)

# we specify the initial values that our lower and upper bounds will be when ini
tialised.
# obviously the ultimate success of this algorithm depends on decent start point
s
a = tf.variable(-10.0)
b = tf.variable(10.0)

# we expect a floating number to be inserted into the graph
v_target = tf.placeholder("float")

# remember the following operations are definitions,
# none are carried out until an operation is evaluated in a session!
y = (a+b)/2
v_guess = c*tf.exp(-0.5*y) + c*tf.exp(-y) + c*tf.exp(-1.5*y) + (c + p)*tf.exp(-2
*y)

# operations to set temporary values (a_ and b_) intended to be the next values
of a and b.
# e.g. if the guess results in a v greater than the target v,
# we will set a_ to be the current value of y
a_ = tf.where(v_guess > v_target, y, a)
b_ = tf.where(v_guess < v_target, y, b)

# the last stage of our graph is to assign the two temporary vals to our variabl
es
step = tf.group( a.assign(a_), b.assign(b_) )

   so we now have a list of operations and variables, any of which can be
   evaluated against a particular session. some of these operations rely
   on other operations to be run, so running, say, v_guess will set off a
   chain reaction to have other tensors, such as c and p, to be evaluated
   first.

   some of these operations rely on a placeholder for which a value needs
   to be specified, so how do we actually feed that value in?

   this is done through the feed_dict argument in the run function itself.

   if we want to evaluate a_, we plug in the value for our placeholder
   v_target, like so:
sess.run(a_, feed_dict={v_target: 100})

   giving us 0.0.

   plug in a v_target of 130 and we get -10.0.

   it   s our    step    operation which actually requires all of the other
   operations to be performed as a prerequisite and in effect executes the
   entire graph. it   s also an operation that actually changes the actual
   state across our session. therefore, the more we run the step, the more
   we incrementally nudge our variables a and b towards the actual value
   of y.

   so, let   s say our value for v in our equation is equal to 95. let   s set
   up a session and execute our graph on it 100 times.
# set up a session and initialize variables
sess = tf.session()
tf.global_variables_initializer().run()
# run the step operation (and therefore whole graph) 100 times
for i in range (100):
        sess.run(step, feed_dict={v_target:95})

   if we evaluate the y tensor now, we get something resembling a
   desirable answer
print(sess.run(y)) # 0.125163

neural networks

   now that we have an understanding of the mechanics of tensorflow, we
   can bring this together with some additional machine learning
   operations built into tensorflow to train a simple neural network.

   here, we would like to classify data points on a 2d coordinate system
   depending on whether they fall within a particular region   a circle of
   radius 0.5 centered at the origin.

   of course, this can be concretely verified by just checking for a given
   point (a,b) if a^2 + b^2 < 0.5, but for the purposes of this machine
   learning experiment, we   d like to instead pass in a training set: a
   series of random points and whether they fall into our intended region.
   here   s one way of creating this:
import numpy as np
no_of_random_points = 100
circle_radius = 0.5
random_spots = np.random.rand(no_of_random_points, 2) * 2 - 1
is_inside_circle = (np.power(random_spots[:,0],2) + np.power(random_spots[:,1],2
) < circle_radius).astype(int)

   we   ll make a neural network with the following characteristics:
    1. it consists of an input layer with two nodes, in which we feed our
       series of two-dimensional vectors contained within    random_spots   .
       this will be represented by a placeholder awaiting the training
       data.
    2. the output layer will also have two nodes, so we need to feed our
       series of training labels (   is_inside_circle   ) into a placeholder
       for a scalar, and then convert those values into a one-hot
       two-dimensional vector.
    3. we will have one hidden layer consisting of three nodes, so we will
       need to use variables for our weights matrices and bias vectors, as
       these are the values that need to be refined when performing the
       training.

input_layer_size = 2
hidden_layer_size = 3
output_layer_size = 2

# starting values for weights and biases are drawn randomly and uniformly from
[-1, 1]
# for example w1 is a matrix of shape 2x3
w1 = tf.variable(tf.random_uniform([input_layer_size, hidden_layer_size], -1, 1)
)
b1 = tf.variable(tf.random_uniform([hidden_layer_size], -1, 1))
w2 = tf.variable(tf.random_uniform([hidden_layer_size, output_layer_size], -1, 1
))
b2 = tf.variable(tf.random_uniform([output_layer_size], -1, 1))
# specifying that the placeholder x can expect a matrix of 2 columns (but any nu
mber of rows)
# representing random spots
x = tf.placeholder(tf.float32, [none, input_layer_size])
# placeholder y can expect integers representing whether corresponding point is
in the circle
# or not (no shape specified)
y = tf.placeholder(tf.uint8)
# an op to convert to a one hot vector
onehot_output = tf.one_hot(y, output_layer_size)

   to complete the definition of our graph, we define some ops which will
   help us train the variables to reach a better classifier. these include
   the matrix calculations, id180, and optimizer.
learning_rate = 0.01
# op to perform matrix calculation x*w1 + b1
hidden_layer = tf.add(tf.matmul(x, w1), b1)
# use sigmoid activation function on the outcome
activated_hidden_layer = tf.sigmoid(hidden_layer)
# apply next weights and bias (w2, b2) to hidden layer and then apply softmax fu
nction
# to get our output layer (each vector adding up to 1)
output_layer = tf.nn.softmax(tf.add(tf.matmul(activated_hidden_layer, w2), b2))
# calculate cross id178 for our id168
loss = -tf.reduce_sum(onehot_output * tf.log(output_layer))
# use id119 optimizer at specified learning rate to minimize value gi
ven by loss tensor
train_step = tf.train.gradientdescentoptimizer(learning_rate).minimize(loss)

   having set up our graph, it   s time to set up a session and run the
      train_step    (which also runs any prerequisite ops). some of these ops
   use placeholders, so values for those need to be provided. this
   training step represents one epoch in our learning algorithm and, as
   such, is looped over the number of epochs we wish to run. we can run
   other parts of the graph, such as the    loss    tensor for informational
   purposes.
epoch_count = 1000
sess = tf.session()
tf.global_variables_initializer().run()
for i in range(epoch_count):
    if i%100 == 0:
        print('loss after %d runs: %f' % (i, sess.run(loss, feed_dict={x: random
_spots, y: is_inside_circle})))
        sess.run(train_step, feed_dict={x: random_spots, y: is_inside_circle})
print('final loss after %d runs: %f' % (i, sess.run(loss, feed_dict={x: random_s
pots, y: is_inside_circle})))

   once we have trained the algorithm, we can feed in a point and get the
   output of the neural network like so:
sess.run(output_layer, feed_dict={x: [[1, 1]]}) # hopefully something close to [
1, 0]
sess.run(output_layer, feed_dict={x: [[0, 0]]}) # hopefully something close to [
0, 1]

   we can classify the point as out of the circle if the first member of
   the output vector is greater than 0.5, inside otherwise.

   by running the output_layer tensor for many points, we can get an idea
   of how the learner envisages the region containing the positively
   classified points. it   s worth playing around with the size of the
   training set, the learning rate, and other parameters to see how close
   we can get to the circle we intended.
   almost triangle training set: 100 points
   learning rate: 0.01
   epochs: 1000 small triangle training set: 1000 points
   learning rate: 0.01
   epochs: 1000 large triangle training set: 1000 points
   learning rate: 0.01
   epochs: 10000 almost circle training set: 1000 points
   learning rate: 0.001
   epochs: 10000

wrap up

   this is a good lesson that an increased training set or epoch amount is
   no guarantee for a good learner   the learning rate should be
   appropriately adjusted.

   hopefully, these demonstrations have given you a good insight into the
   core principles of tensorflow, and provide a solid foundation in which
   to implement more complex techniques.

   we haven   t covered concepts such as the tensorboard or training our
   models across gpus, but these are well-covered in the [34]tensorflow
   documentation. a number of recipes can be found in the documentation
   that can help you get up to speed with tackling exciting deep learning
   tasks using this powerful framework!

understanding the basics

what is a dataflow graph?

   a dataflow graph is a computational graph, which describes the
   computations you wish to carry out but does not actually carry them out
   or hold any values.

what is a tensorflow variable?

   variables in tensorflow hold the state of the graph in a particular
   session. whereas constants in a tensorflow graph are a fixed part of
   the graph definition, variables can be updated using operations.

about the author

   [35]oliver holloway
   view full profile   
   [36]hire the author
   [37]oliver holloway, united kingdom
   member since march 6, 2016
   [38]angular[39]python[40]machine learning[41]pandas[42]+   more
   oliver is a versatile data scientist and software engineer combining 8
   years of professional experience and a postgraduate mathematics degree
   from oxford. career assignments have ranged from building machine
   learning solutions for startups to leading project teams and handling
   vast amounts of data at goldman sachs. with this background, he is
   adept at picking up new skills quickly to deliver robust solutions to
   the most demanding of businesses. [43][click to continue...]
   [44]hiring? meet the top 10 freelance machine learning engineers for
   hire in april 2019

comments

   claude coulombe
   cute tutorial which has the merit to be different of the mnist
   handwritten recognition that we always see. with one hidden layer and
   handful of neurons / units, you should avoid the term   deep learning  .
   ok, i know you're not alone... tensorflow is often synonym of deep
   learning and can do small things. too bad, the visualization code is
   missing but it's not too difficult to figure it out. we can see it as a
   small matplotlib exercise...
                                    
   hi, oliver! thank you for tutorial. i wanted to ask if there is a
   possibility to to integrate tensorflow solutions, especially
   tensorboard with some sort of high level api's or frameworks like
   plot.ly or may be some close example, that reffers to industrial
   cross-platform solution which combines tf and something? really looking
   forward to your answer!
   please enable javascript to view the [45]comments powered by
   disqus.[46]comments powered by disqus
   subscribe
   free email updates
   get the latest content first.
   ____________________
   get exclusive updates
   no spam. just great articles & insights.
   free email updates
   get the latest content first.
   thank you for subscribing!
   check your inbox to confirm subscription. you'll start receiving posts
   after you confirm.
     * 0shares
     * [twitter_83c6d4.png]
     * [facebook_dc66c9.png]

   trending articles

   [47][side_list_cover-0401-howtomathematicallyconfigureyourapplicationfo
   rauto-scaling-waldek_newsletter-003822358e45fdefb90d9dc0354cc33f.png]
   do the math: scaling microservices applications with orchestrators1 day
   ago[48]
   [side_list_cover-0104-wodpressroots-luke_social-14cea8a367cb03c4a39a386
   0f9a11823.png] modern wordpress development workflow with the roots
   stack3 days ago[49]
   [side_list_cover-0327-introtobitcoinlightning-waldek_banner-e63121da140
   c50e3b77db0e134d4f9f8.png] scale with speed: the bitcoin lightning
   network explained4 days ago[50]
   [side_list_cover-0326-wordpressmaintenancetips-luke_newsletter-24d38612
   919504bbefbe7d83c73c42c9.png] 10 tips to make wordpress maintenance
   smooth10 days ago[51]
   [side_list_cover-0319-lifecriticalsystems-luke_newsletter-8c3a1ee7ec272
   731e84d07c1e1cb92e8.png] innovation with life-critical systems17 days
   ago[52]
   [side_list_cover-0318-phytonparameterization_dan_newsletter-b9dfeef97a7
   6cce4ba5d826d3fff10ce.png] ensuring clean code: a look at python,
   parameterized17 days ago[53]
   [side_list_cover-0226-wordpressapiguide-luke_newsletter-b6331b0a068dc12
   ff60a355b94c02302.png] five battle-tested techniques your wordpress api
   developer isn't using24 days ago[54]
   [side_list_cover-0306-googlecloudcd_dan_newsletter-f65e8af970b57182c527
   6eda3446426b.png] a better approach to google cloud continuous
   deployment24 days ago
   relevant technologies
     * [55]machine learning
     * [56]python

   about the author
   oliver holloway
   [57]oliver holloway
   python developer
   oliver is a versatile data scientist and software engineer combining 8
   years of professional experience and a postgraduate mathematics degree
   from oxford. career assignments have ranged from building machine
   learning solutions for startups to leading project teams and handling
   vast amounts of data at goldman sachs. with this background, he is
   adept at picking up new skills quickly to deliver robust solutions to
   the most demanding of businesses.
   [58]hire the author

   toptal connects the [59]top 3% of freelance talent all over the world.

toptal developers

     * [60]android developers
     * [61]angularjs developers
     * [62]back-end developers
     * [63]c++ developers
     * [64]data scientists
     * [65]devops engineers
     * [66]ember.js developers
     * [67]freelance developers
     * [68]front-end developers
     * [69]full-stack developers
     * [70]html5 developers
     * [71]ios developers
     * [72]java developers
     * [73]javascript developers
     * [74]machine learning engineers
     * [75]magento developers
     * [76]mobile app developers
     * [77].net developers
     * [78]node.js developers
     * [79]php developers
     * [80]python developers
     * [81]react.js developers
     * [82]ruby developers
     * [83]ruby on rails developers
     * [84]salesforce developers
     * [85]scala developers
     * [86]software developers
     * [87]unity or unity3d developers
     * [88]virtual reality developers
     * [89]web developers
     * [90]wordpress developers

   see more freelance developers
   [91]learn how enterprises benefit from toptal experts.

join the toptal community.

   [92]hire a developer
   or
   [93]apply as a developer

highest in-demand talent

     * [94]ios developers
     * [95]front-end developers
     * [96]ux designers
     * [97]ui designers
     * [98]financial modeling consultants
     * [99]interim cfos
     * [100]digital project managers

about

     * [101]top 3%
     * [102]clients
     * [103]freelance developers
     * [104]freelance designers
     * [105]freelance finance experts
     * [106]freelance project managers
     * [107]freelance product managers
     * [108]about us

contact

     * [109]contact us
     * [110]press center
     * [111]careers
     * [112]faq

social

     * [113]facebook
     * [114]twitter
     * [115]instagram
     * [116]linkedin

   [117]toptal

   hire the top 3% of freelance talent
     *    copyright 2010 - 2019 toptal, llc
     * [118]privacy policy
     * [119]website terms

   [a6qhozmiylpv2vkc8wshaemc3saomhcrxafccfat?ga_tid=ua-21104039-1&amp;amp;
   ga_cdi=14] [tr?id=463369723801939&amp;ev=viewcontent&amp;noscript=1]

   iframe:
   [120]https://www.attributiontracker.com/tracker/account_visit?tt_visit=
   1

   [121]home     [122]blog     [123]from solving equations to deep learning: a
   tensorflow python tutorial

references

   visible links
   1. https://www.toptal.com/machine-learning/tensorflow-python-tutorial
   2. https://www.toptal.com/machine-learning/de-resolver-ecuaciones-a-aprendizaje-profundo-un-tutorial-de-tensorflow-python/es
   3. https://www.toptal.com/machine-learning
   4. https://www.toptal.com/machine-learning
   5. https://www.toptal.com/users/login
   6. https://www.toptal.com/top-3-percent
   7. https://www.toptal.com/why
   8. https://www.toptal.com/clients
   9. https://www.toptal.com/enterprise
  10. https://www.toptal.com/community
  11. https://www.toptal.com/developers/blog
  12. https://www.toptal.com/about
  13. https://www.toptal.com/machine-learning
  14. https://www.toptal.com/developers/join
  15. https://www.toptal.com/users/login
  16. https://www.toptal.com/contact
  17. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  18. https://www.toptal.com/machine-learning
  19. https://www.toptal.com/machine-learning
  20. https://www.toptal.com/users/login
  21. https://www.toptal.com/developers/blog
  22. https://www.toptal.com/resume/oliver-holloway
  23. https://www.toptal.com/
  24. https://www.toptal.com/developers/blog/tags/deeplearning
  25. https://www.toptal.com/developers/blog/tags/python
  26. https://www.toptal.com/developers/blog/tags/tensorflow
  27. https://www.toptal.com/machine-learning/de-resolver-ecuaciones-a-aprendizaje-profundo-un-tutorial-de-tensorflow-python/es
  28. https://www.youtube.com/watch?v=j60j1cginx4
  29. https://www.theguardian.com/technology/2015/feb/25/google-develops-computer-program-capable-of-learning-tasks-independently
  30. https://www.toptal.com/machine-learning/supervised-machine-learning-algorithms
  31. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  32. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
  33. https://www.tensorflow.org/install/
  34. https://www.tensorflow.org/tutorials/
  35. https://www.toptal.com/resume/oliver-holloway
  36. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  37. https://www.toptal.com/resume/oliver-holloway
  38. https://www.toptal.com/angular
  39. https://www.toptal.com/python
  40. https://www.toptal.com/machine-learning
  41. https://www.toptal.com/python
  42. https://www.toptal.com/resume/oliver-holloway
  43. https://www.toptal.com/resume/oliver-holloway
  44. https://www.toptal.com/machine-learning
  45. https://disqus.com/?ref_noscript
  46. https://disqus.com/
  47. https://www.toptal.com/devops/scaling-microservices-applications
  48. https://www.toptal.com/wordpress/wordpress-roots-stack
  49. https://www.toptal.com/bitcoin/intro-to-bitcoin-lightning-network
  50. https://www.toptal.com/wordpress/wordpress-maintenance-tips
  51. https://www.toptal.com/software/life-critical-systems
  52. https://www.toptal.com/python/python-parameterized-design-patterns
  53. https://www.toptal.com/wordpress/wordpress-api-integration
  54. https://www.toptal.com/devops/better-google-cloud-continuous-deployment
  55. https://www.toptal.com/machine-learning
  56. https://www.toptal.com/python
  57. https://www.toptal.com/resume/oliver-holloway
  58. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  59. https://www.toptal.com/top-3-percent
  60. https://www.toptal.com/android
  61. https://www.toptal.com/angular-js
  62. https://www.toptal.com/back-end
  63. https://www.toptal.com/c-plus-plus
  64. https://www.toptal.com/data-science
  65. https://www.toptal.com/devops
  66. https://www.toptal.com/emberjs
  67. https://www.toptal.com/freelance
  68. https://www.toptal.com/front-end
  69. https://www.toptal.com/full-stack
  70. https://www.toptal.com/html5
  71. https://www.toptal.com/ios
  72. https://www.toptal.com/java
  73. https://www.toptal.com/javascript
  74. https://www.toptal.com/machine-learning
  75. https://www.toptal.com/magento
  76. https://www.toptal.com/app
  77. https://www.toptal.com/dot-net
  78. https://www.toptal.com/nodejs
  79. https://www.toptal.com/php
  80. https://www.toptal.com/python
  81. https://www.toptal.com/react
  82. https://www.toptal.com/ruby
  83. https://www.toptal.com/ruby-on-rails
  84. https://www.toptal.com/salesforce
  85. https://www.toptal.com/scala
  86. https://www.toptal.com/software
  87. https://www.toptal.com/unity-unity3d
  88. https://www.toptal.com/virtual-reality
  89. https://www.toptal.com/web
  90. https://www.toptal.com/wordpress
  91. https://www.toptal.com/enterprise
  92. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  93. https://www.toptal.com/developers/join
  94. https://www.toptal.com/ios
  95. https://www.toptal.com/front-end
  96. https://www.toptal.com/designers/ux
  97. https://www.toptal.com/designers/ui
  98. https://www.toptal.com/finance/financial-modeling
  99. https://www.toptal.com/finance/interim-cfos
 100. https://www.toptal.com/project-managers/digital
 101. https://www.toptal.com/top-3-percent
 102. https://www.toptal.com/clients
 103. https://www.toptal.com/developers
 104. https://www.toptal.com/designers
 105. https://www.toptal.com/finance
 106. https://www.toptal.com/project-managers
 107. https://www.toptal.com/product-managers
 108. https://www.toptal.com/about
 109. https://www.toptal.com/contact
 110. https://www.toptal.com/press-center
 111. https://www.toptal.com/careers
 112. https://www.toptal.com/faq
 113. https://www.facebook.com/toptal
 114. https://twitter.com/toptal
 115. https://www.instagram.com/toptal
 116. https://www.linkedin.com/company/toptal
 117. https://www.toptal.com/
 118. https://www.toptal.com/privacy
 119. https://www.toptal.com/tos
 120. https://www.attributiontracker.com/tracker/account_visit?tt_visit=1
 121. https://www.toptal.com/
 122. https://www.toptal.com/developers/blog
 123. https://www.toptal.com/machine-learning/tensorflow-python-tutorial

   hidden links:
 125. https://www.toptal.com/developers
 126. https://www.toptal.com/machine-learning/tensorflow-python-tutorial
 127. https://www.toptal.com/developers
 128. https://www.facebook.com/toptal
 129. https://twitter.com/toptal
 130. https://www.linkedin.com/company/toptal
 131. https://www.toptal.com/developers
 132. https://www.toptal.com/developers
