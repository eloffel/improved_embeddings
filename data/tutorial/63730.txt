soundnet: learning sound
representations from unlabeled video

   [1]yusuf aytar * [2]carl vondrick * [3]antonio torralba
   massachusetts institute of technology
   nips 2016
   * contributed equally

   [4][paper.png]
   download paper

abstract

   we learn rich natural sound representations by capitalizing on large
   amounts of unlabeled sound data collected in the wild. we leverage the
   natural synchronization between vision and sound to learn an acoustic
   representation using two-million unlabeled videos. unlabeled video has
   the advantage that it can be economically acquired at massive scales,
   yet contains useful signals about natural sound. we propose a
   student-teacher training procedure which transfers discriminative
   visual knowledge from well established visual recognition models into
   the sound modality using unlabeled video as a bridge. our sound
   representation yields significant performance improvements over the
   state-of-the-art results on standard benchmarks for acoustic
   scene/object classification. visualizations suggest some high-level
   semantics automatically emerge in the sound network, even though it is
   trained without ground truth labels.

recognizing objects and scenes from sound

   given a video, our model recognizes objects and scenes from sound only.
   click the videos below to hear some of the sounds and our model's
   predictions. red are scene categories, and blue are objects. turn on
   your speakers!

   the images are shown only for visualization purpose, and not used in
   recognizing the sounds. we blurred the beginning of the video so you
   can try the task too and recognize it from sound alone.
   (button) show me more videos

hearing the hidden representation

   although the network is trained without ground truth labels, it learns
   rich sound features. we visualize these features by finding sounds that
   maximally activate a particular hidden unit. click the images below to
   hear what sounds activate that unit. turn on your speakers! you will
   hear the top 9 sounds that activate that unit.

visualizing conv7

   we visualize units in the deep layers in the network from conv7. since
   we are deep in the network, sound detectors for high-level concepts can
   emerge automatically. note the images are shown only for visualization
   purposes, and not used in analyzing the sounds.
   [0128.jpg]
   [speaker.png] motor-like
   [0126.jpg]
   [speaker.png] dog-like
   [0659.jpg]
   [speaker.png] bird-like
   [0037.jpg]
   [speaker.png] sports chatter-like
   [0113.jpg]
   [speaker.png] music-like
   [0350.jpg]
   [speaker.png] marching band-like
   [0137.jpg]
   [speaker.png] fireworks-like
   [0142.jpg]
   [speaker.png] underwater-like
   [0139.jpg]
   [speaker.png] car-like
   [0103.jpg]
   [speaker.png] parents-like
   [0051.jpg]
   [speaker.png] water-like
   [0745.jpg]
   [speaker.png] baby talk-like
   [0912.jpg]
   [speaker.png] race car-like
   [0028.jpg]
   [speaker.png] open space-like
   [0411.jpg]
   [speaker.png] talking-like
   [0168.jpg]
   [speaker.png] cheering-like

visualizing conv5

   we can also visualize middle layers in the network. interestingly,
   detectors for mid-level concepts automatically emerge in conv5.
   [speaker]
   tapping-like
   [speaker]
   thumping-like
   [speaker]
   yelling-like
   [speaker]
   voice-like
   [speaker]
   swooshing-like
   [speaker]
   chiming-like
   [speaker]
   smacking-like
   [speaker]
   laughing-like
   [speaker]
   music tune-like
   [speaker]
   clicking-like

visualizing conv1

   we visualize the first layer of the network by looking at the learned
   weights of conv1, which you can see below. the network operates on raw
   waveforms, so the filters are in the time-domain.
   [6.jpg]
   [13.jpg]
   [1.jpg]
   [4.jpg]
   [2.jpg]
   [5.jpg]
   [9.jpg]
   [8.jpg]
   [3.jpg]
   [10.jpg]
   [12.jpg]
   [14.jpg]
   [15.jpg]
   [7.jpg]
   [11.jpg]
   [16.jpg]

video overview

   iframe: [5]https://www.youtube.com/embed/yjcjvviy4du?rel=0&showinfo=0

performance

   we experiment with soundnet features on several tasks. they generally
   advance the state-of-the-art in environmental sound recognition by over
   10%. by leveraging millions of unconstrained videos, we can learn
   better sound features.
   [dcase.png] [esc.png]
      dcase    esc 10 and esc 50

   we also analyzed the performance of different components of our system.
   our experiments suggest that one may obtain better performance simply
   by downloading more videos, creating deeper networks, and leveraging
   richer vision models.
   [breakdown.png]

   check out the [6]paper for full details and more analysis.

code & trained models

   [7][github-mark.png]

   the code and models are available on github and open source. it is
   implemented in torch7. using our pre-trained model, you can extract
   discriminative features for natural sound recognition. in our
   experiments, pool5 seems to work the best with a linear id166.
     * [8]code on github
     * [9]pretrained models (101 mb zip file)

   using the code is easy in torch7:

sound = audio.load('file.mp3'):mul(2^-23):view(1,1,-1,1):cuda()
predictions = net:forward(sound)

   soundnet outputs two id203 distributions of the categories that
   it recognizes for the input sounds. the first distribution are object
   categories, and the second distribution is scene categories. you can
   find the list of categories below:
     * [10]object categories (txt)
     * [11]scene categories (txt)

   minor note: soundnet was trained with an older version of places365.
   while places365 will give good results, if you want to strictly
   reproduce our results, please use this [12]vgg16 model that has 401
   categories instead.

data

   we are releasing our flickr video dataset for cross-modal recognition.
   using our dataset, you can train large-scale sound recognition models.
   to train soundnet, you need the raw mp3s and the image features.
   optionally, you can download the original frames.
     * [13]mp3s (359 gb)
     * [14]image features (88 gb)
     * [15]frames (62 gb)
     * [16]list of urls of videos (150 mb)
     * [17]lists of videos and train/val split (84 mb)

   videos from "videos1" are part of the [18]yahoo flickr creative commons
   dataset. videos from "videos2" are downloaded by querying flickr for
   common tags and english words. there should be no overlap.

   if you use this data in your research project, please cite [19]the
   yahoo dataset and our paper.

   we also release re-packaged versions of dcase 2014 and esc-50, which
   you can download [20]here. these are the same as the original datasets,
   but converted to mp3 for easier processing. please cite the dcase and
   esc-50 papers if you use this download.

   below are some sample scenes in this dataset:
   [dataset.png]

bibtex

   if you find this project useful in your research, please cite:

   yusuf aytar, carl vondrick, and antonio torralba. "soundnet: learning
   sound representations from unlabeled video." advances in neural
   information processing systems. 2016.

@inproceedings{aytar2016soundnet,
  title={soundnet: learning sound representations from unlabeled video},
  author={aytar, yusuf and vondrick, carl and torralba, antonio},
  booktitle={advances in neural information processing systems},
  year={2016}
}

related work

   cross-modal learning and perception is an exciting area of research!
   check out some related work below:
     * [21]id98 architectures for large-scale audio classification
       by hershey et al (arxiv 2016)
     * [22]visually indicated sounds
       by owens et al (cvpr 2016)
     * [23]multimodal deep learning
       by ngiam et al (icml 2011)
     * [24]recommending music on spotify with deep learning
       by dieleman et al (nips 2013)
     * [25]cross modal distillation for supervision transfer
       by gupta et al (cvpr 2016)

acknowledgements

   we thank mit tig, especially garrett wollman, for helping store 26 tb
   of video. we are grateful for the gpus donated by nvidia. this work was
   supported by nsf grant #1524817 to at and the google phd fellowship to
   cv.

references

   1. http://people.csail.mit.edu/yusuf/
   2. http://mit.edu/vondrick
   3. http://web.mit.edu/torralba/www/
   4. https://arxiv.org/pdf/1610.09001.pdf
   5. https://www.youtube.com/embed/yjcjvviy4du?rel=0&showinfo=0
   6. https://arxiv.org/pdf/1610.09001.pdf
   7. https://github.com/cvondrick/soundnet
   8. https://github.com/cvondrick/soundnet
   9. http://data.csail.mit.edu/soundnet/soundnet_models_public.zip
  10. http://data.csail.mit.edu/soundnet/categories/categories_id163.txt
  11. http://data.csail.mit.edu/soundnet/categories/categories_places2.txt
  12. http://soundnet.csail.mit.edu/vgg16_places2/
  13. http://data.csail.mit.edu/soundnet/mp3_public.tar.gz
  14. http://data.csail.mit.edu/soundnet/image_features_public.tar.gz
  15. http://data.csail.mit.edu/soundnet/frames_public.tar.gz
  16. http://data.csail.mit.edu/soundnet/urls_public.txt
  17. http://data.csail.mit.edu/soundnet/lists_public.tar.gz
  18. https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67
  19. https://webscope.sandbox.yahoo.com/catalog.php?datatype=i&did=67
  20. http://data.csail.mit.edu/soundnet/data_prepro.zip
  21. https://arxiv.org/abs/1609.09430
  22. http://vis.csail.mit.edu/
  23. https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf
  24. http://benanne.github.io/2014/08/05/spotify-id98s.html
  25. https://arxiv.org/abs/1507.00448
