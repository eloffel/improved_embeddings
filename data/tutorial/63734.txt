   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*ctec2mxxucoydotk13vevg.jpeg]

principal component analysis- intro

variable reduction technique

   [16]go to the profile of anuja nagpal
   [17]anuja nagpal (button) blockedunblock (button) followfollowing
   nov 21, 2017

     too many variables? should you be using all possible variables to
     generate model?

   in order to handle    curse of dimensionality    and avoid issues like
   over-fitting in high dimensional space, methods like principal
   component analysis is used.

   pca is a method used to reduce number of variables in your data by
   extracting important one from a large pool. it reduces the dimension of
   your data with the aim of retaining as much information as possible. in
   other words, this method combines highly correlated variables together
   to form a smaller number of an artificial set of variables which is
   called    principal components    that account for most variance in the
   data.

   let   s dive in to understand how to pca is implemented behind the scene.

   start by normalizing the predictors by subtracting the mean from each
   data point. it is important to normalize the predictor as original
   predictors can be on the different scale and can contribute
   significantly towards variance. the result will look like table 2 with
   a mean of zero.
   [1*yfftlw7lrodgjpzsb-al1a.png]
   normalized data

   next, calculate the covariance matrix for the data which would measure
   how two predictors move together. it is measured between two predictors
   but if you have 3-dimensional data (x, x1, x2), then measure the
   covariance between x x1, x x2, x1 x2. for reference covariance formula
   is:
   [1*vylpm9cl1agpo8vnps1zmg.png]

   in our case covariance matrix would look like this:
   [1*lsf7qhtkw4o-jsn4ygyqvw.png]
   covariance matrix

   now, calculate eigen values and eigen vector of the above matrix. this
   helps in finding underlying patterns in the data. in our case it would
   be approximately:
   [1*4w39v2-lwplufpqrs-0ozg.png]
   eigen value and vector

   we are almost there :). perform reorientation. to convert the data into
   new axes multiply original data with eigenvectors, which suggests the
   direction of new axes. note, that you can choose to leave out smaller
   eigen vector or use both. also, decide how many set of features to keep
   based on which set accounts for 95% or more variance.

   finally, the scores calculated from above step can be plotted and and
   fed into the predictive model. plots gives us the sense of how
   close/highly correlated two variables are. instead of using original
   data to plot x and y axis which doesn   t tell us much how points are
   related to each other, we plot transformed data (using eigen vectors)
   that find patterns and shows the relationships between points.

   end note: it is easy to confuse pca with factor analysis but there is a
   conceptual difference between these two methods. i will be going into
   details of factor analysis and how it is different from pca in my next
   post.. stay tuned.

     * [18]data science
     * [19]id84
     * [20]machine learning
     * [21]analytics

   (button)
   (button)
   (button) 535 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of anuja nagpal

[23]anuja nagpal

   data scientist- keeping up with data science and machine learning.
   #datascience #machinelearning #womenintech

     (button) follow
   [24]towards data science

[25]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 535
     * (button)
     *
     *

   [26]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [27]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/61f236064b38
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/principal-component-analysis-intro-61f236064b38&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/principal-component-analysis-intro-61f236064b38&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_iaanpzutbfpf---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@anujanagpal?source=post_header_lockup
  17. https://towardsdatascience.com/@anujanagpal
  18. https://towardsdatascience.com/tagged/data-science?source=post
  19. https://towardsdatascience.com/tagged/dimensionality-reduction?source=post
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/analytics?source=post
  22. https://towardsdatascience.com/@anujanagpal?source=footer_card
  23. https://towardsdatascience.com/@anujanagpal
  24. https://towardsdatascience.com/?source=footer_card
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/p/61f236064b38/share/twitter
  30. https://medium.com/p/61f236064b38/share/facebook
  31. https://medium.com/p/61f236064b38/share/twitter
  32. https://medium.com/p/61f236064b38/share/facebook
