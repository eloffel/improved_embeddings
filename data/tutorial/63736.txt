   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]slav
   [7]slav
   [8]sign in[9]get started
     __________________________________________________________________

a gentle intro to id21

   [10]go to the profile of slav ivanov
   [11]slav ivanov (button) blockedunblock (button) followfollowing
   nov 16, 2017

   nowadays most applications of deep learning rely on id21.
   this is especially true in the domain of id161. we will
   explore what id21 is, how to do it and what the potential
   pitfalls are. to do this, we   ll go on a little startup quest.

the startup

   in a otherwise uneventful day, a friend approaches us with a
   revolutionary idea. he wants to build the next big thing in social
   media: facebook for pets. there people would upload pictures of their
   pets and enjoy others    pet pictures. naturally, we jump onboard.
   [1*5weddc_w759x2znp-eazfg.jpeg]
   who wouldn   t want to look at dog pictures all day long?

   we quickly raise funding and recruit people to help us to build it.
   after a round of customer interviews, we realize that cat people don   t
   really enjoy looking at dog photos. also, it turns out parrot owners
   plainly hate cats and folks who have a goldfish only like to look at
   other fish.

   since this is 2017, we don   t want people to tell us manually what is in
   the picture they are uploading. instead, we will use the power of
   machine learning to recognize the animal in each image. so we   d be able
   to group them into separate sections of our app.
     __________________________________________________________________

the convolutional neural network

   we talk about our project to a friend who recently got his phd in
   machine learning (ml). he advises us to use convolutional neural
   networks (id98s), a particular ml architecture. they were invented by
   [12]yann lecun, who is currently head of ai at facebook. he explains
   that alex krizhevsky et al. popularized id98s in 2012. the team crushed
   the competition in the high-profile id161 competition
      id163 large scale visual recognition challenge   . ever since then,
   id98s has dominated the field of id161. they are also making
   forays into related deep learning domains.

     for more info on deep learning in general, i highly recommend the
     part 1 and part 2 of the excellent    practical deep learning for
     coders   . they are available for free from [13]fast.ai.

   convolutional neural networks have an input, which is the image we are
   trying to classify. then there are the learnable weights, organized in
   groups called filters or kernels. these are filters layered on top of
   each other. at the end of the network, we have our output. it tells us
   whether the image we fed is a cat, dog, bird, etc.
   [1*huxi2p_ndwxfs4modd_jdw.png]
   source: [14]a beginner   s guide to understanding convolutional
   neural networks

   we take the [15]fast.ai course and teach ourselves to build id98s. we
   know that deep learning needs large data, so we download the id163
   dataset. it contains about a million images of, among other things,
   cats, dogs, and goldfish.

   we start training the model using this data, optimizing it with a
   [16]stochastic id119 algorithm.

   sure enough, soon our id98 can tell apart cats from dogs from parrots
   and more. we call it the pet-cognizer  .

     in reality, people have already done the hard work of training many
     id98 models on id163. such pretrained networks are distributed
     freely. see [17]pretrained models on keras and [18]torchvision for
     pytorch.

   the filters in the first layer of the id98 have learned to recognize
   edges, gradients, and solid color areas. the second layer group
   combines the data from the first layer to detect stripes, right angles,
   circles and more.
   [1*qlsr_xh03fez8ozy8wqfjq.png]
   source: [19]visualizing and understanding convolutional networks

   by the time we have reached the 3rd layer group, we are already
   recognizing people, birds, and car wheels. this process goes on for
   many layers.
   [1*xfred4t4u8o5u-kn_ymk1g.png]
   source: [20]visualizing and understanding convolutional networks

   finally, the output layer of the network tells us the id203 of
   each class for this image.

   our pet-cognizer   works!
     __________________________________________________________________

the new task

   just as we are ready to launch our project, someone suggest that it
   would be great if we can further segment dog sections into breeds. you
   oppose the idea, but the team strong-arms you into it. huffing and
   puffing, you go back to the model. we call it the breed-congizer and
   get to work.

   the problem is that we don   t have a million pictures of dogs to train
   our new model. [21]our dataset has 120 breeds with about 100 images for
   each. we try to train our id98 from scratch with this tiny dataset, but
   the results are not great.
   [1*ii10bu7tco9qu7pbhpxvyg.png]
   results from training from scratch. all of these are incorrect

the id21

   we remember about a discussion on [22]reddit about id21.

   the idea is to take the knowledge learned in a model and apply it to
   another task.

   id21 sounds like what we want to do. we decide to reuse
   the already trained pet-cognizer  .

scenario 1: new dataset is similar to initial dataset

   our new breed dataset is close to the id163 dataset we first trained
   on. it   s similar in the sense that they both contain pictures of the
      real world    (as opposed to images of documents or medical scans). thus
   the filters in the id98 can be reused, and we don   t have to learn them
   again.

   for breed-cognizer  , we swap the only the last layer. instead of
   telling us what animal is in the picture, it will give us probabilities
   for each dog breed .

   because the weights on the last layer are initially random we have to
   train it. but training these random weights might also change the great
   filters in the earlier layers. to avoid this, we freeze all layers but
   the last. freezing means that the layer weights won   t be updated during
   training.

   rather miraculously, this works to an extent. we are able to achieve
   80% accuracy on over 120 classes, just by training the last layer.
   [1*5wc3tzk7ojcoeke76kp9gq.png]
   results from training the last layer only
     __________________________________________________________________

fine-tuning

   but the information that we have learned in the later layers might not
   be beneficial to the current task. for example, surely in the model
   there are weights that help distinguish between a cat and a goldfish.
   this is probably irrelevant to dog breed classification. to further
   better our network, we unfreeze the last few layers of the id98 and
   retrain them.

   how many layers to unfreeze? it depends, so we start with one and
   experiment with more until there is no significant improvement.

   but because some of these layers are useful, we only use very small
   learning rate for the unfrozen filters. this will help preserve the
   useful knowledge while changing what is not needed.

   the learning rate controls how much we update weights on each gradient
   descent iteration.

   fine-tuning brings our breed-cognizer   accuracy to 90% (without data
   augmentation).
   [1*zilytzxdggvoqgqeugkolg.png]
   fine-tuned network results
     __________________________________________________________________

scenario 2: new dataset is not similar to initial dataset

   word of our awesome id161 abilities spreads and people start
   contacting us for all kinds of projects. one of them involves satellite
   imagery and another is for a medical startup.
   [1*dz8xl6dpmbzj-e2ba7z9yg.png]
   satellite imagery. source: planet: [23]understanding the amazon
   from space

   both of these are not similar to the dataset we used to train our
   pet-congizer  . especially medical images, for example obtained via ct
   scans.
   [1*qfo5hlf-rwbwdfvrkxggog.png]
   ct scan of a lung. source: [24]data science bowl 2017

   in such cases it is still a good idea to start with pretrained weights,
   but unfreeze all layers. then we train on the new dataset, with a
   normal learning rate. also, stanford   s cs231 has an [25]good discussion
   on this.
     __________________________________________________________________

   id21 is used on almost all id161 tasks nowadays.
   it   s rare to train from scratch unless you have a massive dataset. this
   primer should have given you some intuition on how and why it works.

   iframe: [26]/media/9d841a5bf2e7a49250ea63cb232937bc?postid=2c0b674375a0

   did i miss anything? is anything wrong? let me know by leaving a reply
   below.

     if you liked this article, please help others find it: hold the clap
     icon for as long as you think this article is worth it. thanks a
     lot!

     * [27]machine learning
     * [28]deep learning
     * [29]id21
     * [30]artificial intelligence
     * [31]id161

   (button)
   (button)
   (button) 695 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of slav ivanov

[33]slav ivanov

   entrepreneur / hacker

     (button) follow
   [34]slav

[35]slav

   machine learning, deep learning and other types of learning.

     * (button)
       (button) 695
     * (button)
     *
     *

   [36]slav
   never miss a story from slav, when you sign up for medium. [37]learn
   more
   never miss a story from slav
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.slavv.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2c0b674375a0
   4. https://medium.com/
   5. https://medium.com/
   6. https://blog.slavv.com/?source=avatar-lo_p3hoilnv9x2m-a1fee82e231
   7. https://blog.slavv.com/?source=logo-lo_p3hoilnv9x2m---a1fee82e231
   8. https://medium.com/m/signin?redirect=https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0&source=--------------------------nav_reg&operation=register
  10. https://blog.slavv.com/@slavivanov?source=post_header_lockup
  11. https://blog.slavv.com/@slavivanov
  12. https://en.wikipedia.org/wiki/yann_lecun
  13. http://course.fast.ai/
  14. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  15. http://course.fast.ai/
  16. http://ruder.io/optimizing-gradient-descent/
  17. https://keras.io/applications/
  18. http://pytorch.org/docs/master/torchvision/models.html
  19. http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf
  20. http://www.matthewzeiler.com/wp-content/uploads/2017/07/arxive2013.pdf
  21. https://www.kaggle.com/c/dog-breed-identification/data
  22. http://reddit.com/r/machinelearning/
  23. https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data
  24. https://www.kaggle.com/c/data-science-bowl-2017/data
  25. http://cs231n.github.io/transfer-learning/
  26. https://blog.slavv.com/media/9d841a5bf2e7a49250ea63cb232937bc?postid=2c0b674375a0
  27. https://blog.slavv.com/tagged/machine-learning?source=post
  28. https://blog.slavv.com/tagged/deep-learning?source=post
  29. https://blog.slavv.com/tagged/transfer-learning?source=post
  30. https://blog.slavv.com/tagged/artificial-intelligence?source=post
  31. https://blog.slavv.com/tagged/computer-vision?source=post
  32. https://blog.slavv.com/@slavivanov?source=footer_card
  33. https://blog.slavv.com/@slavivanov
  34. https://blog.slavv.com/?source=footer_card
  35. https://blog.slavv.com/?source=footer_card
  36. https://blog.slavv.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/2c0b674375a0/share/twitter
  40. https://medium.com/p/2c0b674375a0/share/facebook
  41. https://medium.com/p/2c0b674375a0/share/twitter
  42. https://medium.com/p/2c0b674375a0/share/facebook
