
   [1]andy zeng
   pacman goes to school

teach pacman how to make himself smarter!

   inspired by behaviorist psychology, id23 is an area
   of machine learning in computer science, concerned with how software
   agents ought to take actions in an environment so as to maximize some
   notion of cumulative reward. the problem, due to its generality, is
   studied in many other disciplines, such as game theory, control theory,
   operations research, id205, simulation-based optimization,
   statistics, and id107. in the operations research and
   control literature, the field where id23 methods are
   studied is called approximate id145. the problem has been
   studied in the theory of optimal control, though most studies there are
   concerned with existence of optimal solutions and their
   characterization, and not with the learning or approximation aspects.
   in machine learning, the environment is typically formulated as a
   markov decision process (mdp), and many id23
   algorithms for this context are highly related to id145
   techniques. the main difference between the classical techniques and
   id23 algorithms is that the latter do not need
   knowledge about the mdp and they target large mdps where exact methods
   become infeasible. id23 differs from standard
   supervised learning in that correct input/output pairs are never
   presented, nor sub-optimal actions explicitly corrected. further, there
   is a focus on on-line performance, which involves finding a balance
   between exploration (of uncharted territory) and exploitation (of
   current knowledge). the exploration vs. exploitation trade-off in
   id23 has been most thoroughly studied through the
   multi-armed bandit problem and in finite mdps. this project uses a
   visual gridworld mdp to help us implement some id23
   algorithms.

value iteration

   [valueiteration2.png] [valueiteration1.png] [default.png] notes:
   - this value iteration agent is an offline planner, not a reinforcement
   learning agent, and so the relevant training option is the number of
   iterations of value iteration it should run in its initial planning
   phase. - takes an mdp on construction and runs value iteration for the
   specified number of iterations before the constructor returns. - value
   iteration computes k-step estimates of the optimal values, vk.
   - note in code: the method computeactionfromvalues(state) computes the
   best action according to the value function given by self.values. the
   method computeqvaluefromvalues(state, action) returns the q-value of
   the (state, action) pair given by the value function given by
   self.values. these quantities are all displayed in the gui: values are
   numbers in squares, q-values are numbers in square quarters, and
   policies are arrows out from each square.
   - this uses the "batch" version of value iteration where each vector vk
   is computed from a fixed vector vk-1, not the "online" version where
   one single weight vector is updated in place. this means that when a
   state's value is updated in iteration k based on the values of its
   successor states, the successor state values used in the value update
   computation are those from iteration k-1 (even if some of the successor
   states had already been updated in iteration k).
   - a policy synthesized from values of depth k (which reflect the next k
   rewards) will actually reflect the next k+1 rewards (i.e. when   k+1 is
   returned). similarly, the q-values will also reflect one more reward
   than the values (i.e. when qk+1 is returned).
   - handles the case when a state has no available actions in an mdp.
   - the images show an ai agent's values, q-values, and policies after
   fixed numbers of iterations and at convergence.

bridge crossing

   [optimalpolicy2.png] [optimalpolicy1.png] notes:
   - the following is a grid world map with the a low-reward terminal
   state and a high-reward terminal state separated by a narrow "bridge",
   on either side of which is a chasm of high negative reward. the agent
   starts near the low-reward state. with the default discount of 0.9 and
   the default noise of 0.2, the optimal policy does not cross the bridge.
   - by simply changing one of the discount and noise parameters, the
   optimal policy can cause the agent to attempt to cross the bridge.

optimal policies

   [discountgrid.png] notes:
   - this grid has two terminal states with positive payoff (in the middle
   row), a close exit with payoff +1 and a distant exit with payoff +10.
   the bottom row of the grid consists of terminal states with negative
   payoff (shown in red); each state in this "cliff" region has payoff
   -10. the starting state is the yellow square. we distinguish between
   two types of paths: (1) paths that "risk the cliff" and travel near the
   bottom row of the grid; these paths are shorter but risk earning a
   large negative payoff, and are represented by the red arrow in the
   right-most figure. (2) paths that "avoid the cliff" and travel along
   the top edge of the grid. these paths are longer but are less likely to
   incur huge negative payoffs. these paths are represented by the green
   arrow in the right-most figure. - the key is choosing the best settings
   of the discount, noise, and living reward parameters for this mdp to
   produce optimal policies for certain conditions. - tests are performed
   across the following policy types produced:     - prefer the close exit
   (+1), risking the cliff (-10).     - prefer the close exit (+1), but
   avoiding the cliff (-10).     - prefer the distant exit (+10), risking
   the cliff (-10).     - prefer the distant exit (+10), avoiding the
   cliff (-10).     - avoid both exits and the cliff (infinite loop).

id24

   [qlearning.gif] notes:
   - note that our value iteration agent does not actually learn from
   experience. rather, it ponders its mdp model to arrive at a complete
   policy before ever interacting with a real environment. when it does
   interact with the environment, it simply follows the precomputed policy
   (e.g. it becomes a reflex agent). this distinction may be subtle in a
   simulated environment like a gridword, but it's very important in the
   real world, where the real mdp is not available. - this id24
   agent, which does very little on construction, learns by trial and
   error from interactions with the environment through its update(state,
   action, nextstate, reward) method. - ties between values are broken
   randomly for better behavior. - in a particular state, actions that the
   agent hasn't seen before still have a q-value, specifically a q-value
   of zero, and if all of the actions that the agent has seen before have
   a negative q-value, an unseen action may be optimal. - the test on the
   right is performed with 10 episodes of learning.

epsilon greedy

   [crawler.gif] notes:
   - implemented epsilon-greedy action selection, meaning the agent
   chooses random actions an epsilon fraction of the time, and follows its
   current best q-values otherwise. - note that choosing a random action
   may result in choosing the best action - that is, not a random
   sub-optimal action... but any random legal action. - the final q-values
   resemble those of the value iteration agent, especially along
   well-traveled paths. however, the average returns are lower than the
   q-values predicted because of the random actions and the initial
   learning phase. - we can apply this algorithm to a id24 crawler
   robot (right figure). it attempts to learn how to, well, crawl towards
   the right.

approximate id24 and state abstraction

   [pacmanql2.gif] [pacmanql1.gif] notes:
   - in the first phase, training, pacman will begin to learn about the
   values of positions and actions. because it takes a very long time to
   learn accurate q-values even for tiny grids, pacman's training games
   run in quiet mode by default, with no gui (or console) display. once
   pacman's training is complete, he will enter testing mode. when
   testing, pacman's epsilon and alpha will be set to 0.0, effectively
   stopping id24 and disabling exploration, in order to allow pacman
   to exploit his learned policy. - because unseen actions have by
   definition a q-value of zero, if all of the actions that have been seen
   have negative q-values, an unseen action may be optimal. - the mdp
   state is the exact board configuration facing pacman, with the now
   complex transitions describing an entire ply of change to that state.
   the intermediate game configurations in which pacman has moved but the
   ghosts have not replied are not mdp states, but are bundled in to the
   transitions. - the left image depicts a pacman before id24. - the
   right image depicts a pacman after id24 2000 training episodes.

approximate id24 and features

   [pacmanqlfeature.gif] notes:
   - implementation of an approximate id24 agent that learns weights
   for features of states, where many states might share the same
   features. - approximate id24 assumes the existence of a feature
   function f(s,a) over state and action pairs, which yields a vector
   f1(s,a) .. fi(s,a) .. fn(s,a) of feature values. - applying the feature
   function to our id24 pacman agent gives us the following results
   (right figure), after 50 training episodes.

references

   1. http://www.cs.princeton.edu/~andyz/index
