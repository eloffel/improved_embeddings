   #[1]xrds    feed [2]xrds    comments feed [3]xrds    convolutional neural
   networks (id98s): an illustrated explanation comments feed [4]alternate
   [5]alternate

[6]xrds

crossroads     the acm magazine for students

   [7]xrds search ____________________ search

main menu

   [8]skip to primary content
     * [9]home
     * [10]about the xrds blog
     * [11]authors

   [12]home [13]news [14]events [15]magazine [16]blog [17]archives
   [18]getinvolved [19]resources

post navigation

   [20]    previous [21]next    

convolutional neural networks (id98s): an illustrated explanation

   posted on [22]june 29, 2016 by [23]abhineet saxena

   id158s (anns) are used everyday for tackling a
   broad spectrum of prediction and classification problems, and for
   scaling up applications which would otherwise require intractable
   amounts of data. ml has been witnessing a    neural revolution   ^1 since
   the mid 2000s, as anns found application in tools and technologies such
   as search engines, automatic translation, or video classification.
   though structurally diverse, convolutional neural networks (id98s) stand
   out for their ubiquity of use, expanding the ann domain of
   applicability from feature vectors to variable-length inputs.

   the aim of this article is to give a detailed description of the inner
   workings of id98s, and an account of the their recent merits and trends.

   table of contents:
    1. background
    2. motivation
    3. id98 concepts
          + input/output volumes
          + features
          + filters (convolution kernels)
               o kernel operations detailed
          + receptive field
          + zero-padding
          + hyperparameters
    4. the id98 architecture
          + convolutional layer
          + the relu (rectified linear unit) layer
          + the fully connected layer
    5. id98 design principles
    6. conclusion
    7. references
     __________________________________________________________________

    ^1the neural revolution is a reference to the period beginning 1982,
   when academic interest in the field of neural networks was invigorated
   by caltech professor john j. hopfield, who authored a research paper[1]
   that detailed the neural network architecture named after himself. the
   crucial breakthrough, however, occurred  in 1986, when the
   id26 algorithm was proposed as such by david rumelhart,
   geoffrey e. hinton and r.j. williams [2]. for a history of neural
   networks, please see andrey kurenkov   s blog [3].
     __________________________________________________________________

acknowledgement

   i would like to thank adrian scoica and pedro lopez for their immense
   patience and help with writing this piece. the sincerity of efforts and
   guidance that they   ve provided is ineffable. i   m forever inspired.

background

   the modern convolutional neural networks owe their inception to a
   well-known 1998 research paper[4] by yann lecun and l  on bottou. in
   this highly instructional and detailed paper, the authors propose a
   neural architecture called lenet 5 used for recognizing hand-written
   digits and words that established a new state of the art^2
   classification accuracy of 99.2% on the mnist dataset[5].

   according to the author   s accounts, id98s are biologically-inspired
   models. the research investigations carried out by d. h. hubel and t.
   n. wiesel in their paper[6] proposed an explanation for the way in
   which mammals visually perceive the world around them using a layered
   architecture of neurons in the brain, and this in turn inspired
   engineers to attempt to develop similar pattern recognition mechanisms
   in id161.
   the most popular application for id98s in the recent times has been
   image analysis, but many researchers have also found other interesting
   and exciting ways to use them: from winning go matches against human
   players([7], a related [24]video [8]) to an innovative application in
   discovering new drugs by training over large quantities of molecular
   structure data of organic compounds[9].

motivation

   a first question to answer with id98s is why are they called
   convolutional in the first place.

   convolution is a mathematical concept used heavily in digital signal
   processing when dealing with signals that take the form of a time
   series. in lay terms, convolution is a mechanism to combine or
      blend   [10] two functions of time^3 in a coherent manner. it can be
   mathematically described as follows:

   for a discrete domain of one variable:

   [25]codecogseqn (1)

   for a discrete domain of two variables:

   [26]codecogseqn (2)
     __________________________________________________________________

   ^2a point to note here is the improvement is, in fact, modest.
   classification accuracies greater than or equal to 99% on mnist have
   been achieved using non-neural methods as well, such as k-nearest
   neighbours (knn) or support vector machines (id166). for a list of ml
   methods applied and the respective classification accuracies attained,
   please refer to this[11] table.

   ^3or, for that matter, of another parameter.
     __________________________________________________________________

   eq. 2 is perhaps more descriptive of what convolution truly is: a
   summation of pointwise products of function values, subject to
   traversal.

   though conventionally called as such, the operation performed on image
   inputs with id98s is not strictly convolution, but rather a slightly
   modified variant called cross-correlation[10], in which one of the
   inputs is time-reversed:

   [27]2016-06-29 22_48_55-id98_blogpost_ - google docs

id98 concepts

   id98s have an associated terminology and a set of concepts that is
   unique to them, and that sets them apart from other types of neural
   network architectures. the main ones are explained as follows:

input/output volumes

   id98s are usually applied to image data. every image is a matrix of
   pixel values. the range of values that can be encoded in each pixel
   depends upon its bit size. most commonly, we have 8 bit or 1 byte-sized
   pixels. thus the possible range of values a single pixel can represent
   is [0, 255]. however, with coloured images, particularly rgb (red,
   green, blue)-based images, the presence of separate colour channels (3
   in the case of rgb images) introduces an additional    depth    field to
   the data, making the input 3-dimensional. hence, for a given rgb image
   of size, say 255  255 (width x height) pixels, we   ll have 3 matrices
   associated with each image, one for each of the colour channels. thus
   the image in it   s entirety, constitutes a 3-dimensional structure
   called the input volume (255x255x3).

   figure 1: the cross-section of an input volume of size: 4 x 4 x 3. it
   comprises of the 3 colour channel matrices of the input image.

features

   just as its literal meaning implies, a feature is a distinct and useful
   observation or pattern obtained from the input data that aids in
   performing the desired image analysis. the id98 learns the features from
   the input images. typically, they emerge repeatedly from the data to
   gain prominence. as an example, when performing face detection, the
   fact that every human face has a pair of eyes will be treated as a
   feature by the system, that will be detected and learned by the
   distinct layers. in generic object classification, the edge contours of
   the objects serve as the features.

filters (convolution kernels)

   a filter (or kernel) is an integral component of the layered
   architecture.

   generally, it refers to an operator applied to the entirety of the
   image such that it transforms the information encoded in the pixels. in
   practice, however, a kernel is a smaller-sized matrix in comparison to
   the input dimensions of the image, that consists of real valued
   entries.

   the kernels are then convolved with the input volume to obtain
   so-called    activation maps   . activation maps indicate    activated   
   regions, i.e. regions where features specific to the kernel have been
   detected in the input. the real values of the kernel matrix change with
   each learning iteration over the training set, indicating that the
   network is learning to identify which regions are of significance for
   extracting features from the data.

kernel operations detailed

   the exact procedure for convolving a kernel (say, of size 16 x 16) with
   the input volume (a 256 x 256 x 3 sized rgb image in our case) involves
   taking patches from the input image of size equal to that of the kernel
   (16 x 16), and convolving (or calculating the dot product) between the
   values in the patch and those in the kernel matrix.

   the convolved value obtained by summing the resultant terms from the
   dot product forms a single entry in the activation matrix. the patch
   selection is then slided (towards the right, or downwards when the
   boundary of the matrix is reached) by a certain amount called the
      stride    value, and the process is repeated till the entire input image
   has been processed. the process is carried out for all colour channels.
   for id172 purposes, we divide the calculated value of the
   activation matrix by the sum of values in the kernel matrix.

   the process is demonstrated in figure 2, using a toy example consisting
   of a 3-channel 4  4-pixels input image and a 3  3 kernel matrix.  note
   that:
     * pixels are numbered from 1 in the example;
     * the values in the activation map are normalized to ensure the same
       intensity range between the input volume and the output volume.
       hence, for id172, we divide the calculated value for the
          red    channel by 2 (the sum of values in the kernel matrix);
     * we assume the same kernel matrix for all the three channels, but it
       is possible to have a separate kernel matrix for each colour
       channel;
     * for a more detailed and intuitive explanation of the convolution
       operation, you can refer to the excellent blog-posts by chris
       olah[12] and by tim dettmers[13].

   [28]figure_2

   figure 2: the convolution value is calculated by taking the dot product
   of the corresponding values in the kernel and the channel matrices. the
   current path is indicated by the red-coloured, bold outline in the
   input image volume. here, the entry in the activation matrix is
   calculated as:

   [29]codecogseqn (6)

receptive field

   it is impractical to connect all neurons with all possible regions of
   the input volume. it would lead to too many weights to train, and
   produce too high a computational complexity. thus, instead of
   connecting each neuron to all possible pixels, we specify a 2
   dimensional region called the    receptive field[14]    (say of size 5  5
   units) extending to the entire depth of the input (5x5x3 for a 3 colour
   channel input), within which the encompassed pixels are fully connected
   to the neural network   s input layer. it   s over these small regions that
   the network layer cross-sections (each consisting of several neurons
   (called    depth columns   )) operate and produce the activation map.

zero-padding

   zero-padding refers to the process of symmetrically adding zeroes to
   the input matrix. it   s a commonly used modification that allows the
   size of the input to be adjusted to our requirement. it is mostly used
   in designing the id98 layers when the dimensions of the input volume
   need to be preserved in the output volume.

   [30]figure_3

   figure 3: a zero-padded 4 x 4 matrix becomes a 6 x 6 matrix.

hyperparameters

   in id98s, the properties pertaining to the structure of layers and
   neurons, such spatial arrangement and receptive field values, are
   called hyperparameters. hyperparameters uniquely specify layers. the
   main id98 hyperparameters are receptive field (r), zero-padding (p), the
   input volume dimensions (width x height x depth, or w x h x d ) and
   stride length (s).

the id98 architecture

   now that we are familiar with the id98 terminology, let   s go on ahead
   and study the id98 architecture in detail.

   the architecture of a typical id98 is composed of multiple layers where
   each layer performs a specific function of transforming its input into
   a useful representation. there are 3 major types of layers that are
   commonly observed in complex neural network architectures:

   convolutional layer
   also referred to as conv. layer, it forms the basis of the id98 and
   performs the core operations of training and consequently firing the
   neurons of the network. it performs the convolution operation over the
   input volume as specified in the previous section, and consists of a
   3-dimensional arrangement of neurons (a stack of 2-dimensional layers
   of neurons, one for each channel depth).

   [31]figure_4

   figure 4: a 3-d representation of the convolutional layer with 3 x 3 x
   4 = 36 neurons.

   each neuron is connected to a certain region of the input volume called
   the receptive field (explained in the previous section). for example,
   for an input image of dimensions 28x28x3, if the receptive field is 5 x
   5, then each neuron in the conv. layer is connected to a region of
   5x5x3 (the region always comprises the entire depth of the input, i.e.
   all the channel matrices) in the input volume. hence each neuron will
   have 75 weighted inputs. for a particular value of r (receptive field),
   we have a cross-section of neurons entirely dedicated to taking inputs
   from this region. such a cross-section is called a    depth column   . it
   extends to the entire depth of the conv. layer.

   for optimized conv. layer implementations, we may use a shared weights
   model that reduces the number of unique weights to train and
   consequently the matrix calculations to be performed per layer. in this
   model, each    depth slice    or a single 2-dimensional layer of neurons in
   the conv architecture all share the same weights. the caveat with
   parameter sharing is that it doesn   t work well with images that
   encompass a spatially centered structure (such as face images), and in
   applications where we want the distinct features of the image to be
   detected in spatially different locations of the layer.

   [32]figure_5

   figure 5: concept of receptive field.

   we must keep in mind though that the network operates in the same way
   that a feed-forward network would: the weights in the conv layers are
   trained and updated in each learning iteration using a back-propagation
   algorithm extended to be applicable to 3-dimensional arrangements of
   neurons.

the relu (rectified linear unit) layer

   relu refers to the rectifier unit, the most commonly deployed
   activation function for the outputs of the id98 neurons. mathematically,
   it   s described as:

   [33]codecogseqn (3)

   unfortunately, the relu function is not differentiable at the origin,
   which makes it hard to use with id26 training. instead, a
   smoothed version called the softplus function is used in practice:

   [34]codecogseqn (4)

   the derivative of the softplus function is the sigmoid function, as
   mentioned in a prior blog post.

   [35]codecogseqn (5)

the pooling layer

   the pooling layer is usually placed after the convolutional layer. its
   primary utility lies in reducing the spatial dimensions (width x
   height) of the input volume for the next convolutional layer. it does
   not affect the depth dimension of the volume.

   the operation performed by this layer is also called    down-sampling   ,
   as the reduction of size leads to loss of information as well. however,
   such a loss is beneficial for the network for two reasons:
    1. the decrease in size leads to less computational overhead for the
       upcoming layers of the network;
    2. it work against over-fitting.

   much like the convolution operation performed above, the pooling layer
   takes a sliding window or a certain region that is moved in stride
   across the input transforming the values into representative values.
   the transformation is either performed by taking the maximum value from
   the values observable in the window (called    max pooling   ), or by
   taking the average of the values. max pooling has been favoured over
   others due to its better performance characteristics.

   the operation is performed for each depth slice. for example, if the
   input is a volume of size 4x4x3, and the sliding window is of size 2  2,
   then for each color channel, the values will be down-sampled to their
   representative maximum value if we perform the max pooling operation.

   no new parameters are introduced in the matrix by this operation. the
   operation can be thought of as applying a function over input values,
   taking fixed sized portions at a time, with the size, modifiable as a
   parameter. pooling is optional in id98s, and many architectures do not
   perform pooling operations.

   [36]figure_6

   figure 6: the max-pooling operation can be observed in sub-figures (i),
   (ii) and (iii) that max-pools the 3 colour channels for an example
   input volume for the pooling layer. the operation uses a stride value
   of [2, 2]. the dark and red boundary regions describe the window
   movement. sub-figure (iv) shows the operation applied for a stride
   value of [1,1], resulting in a 3  3 matrix here we observe overlap
   between regions.

the fully connected layer

   the fully connected layer is configured exactly the way its name
   implies: it is fully connected with the output of the previous layer.
   fully-connected layers are typically used in the last stages of the id98
   to connect to the output layer and construct the desired number of
   outputs.

id98 design principles

   given the aforementioned building blocks, the last detail before
   implementing a id98 is to specify its design end to end, and to decide
   on the layer dimensions of the convolutional layers.

   a quick and dirty empirical formula[15] for calculating the spatial
   dimensions of the convolutional layer as a function of the input volume
   size and the hyperparameters we discussed before can be written as
   follows:

   for each (ith) dimension of the input volume, pick:

   [37]eq_7

   where is the (ith) input dimension, r is the receptive field value, p
   is the padding value, and s is the value of the stride. note that the
   formula does not rely on the depth of the input.

   to better understand better how it works, let   s consider the following
   example:
    1. let the dimensions of the input volume be 288x288x3, the stride
       value be 2 (both along horizontal and vertical directions).
    2. now, since w[in]=288 and s = 2, (2.p     r) must be an even integer
       for the calculated value to be an integer. if we set the padding to
       0 and r = 4, we get w[out]=(288-4+2.0)/2+1 =284/2 + 1 = 143. as the
       spatial dimensions are symmetrical (same value for width and
       height), the output dimensions are going to be: 143 x 143 x k,
       where k is the depth of the layer. k can be set to any value, with
       increasing values for every conv. layer added. for larger networks
       values of 512 are common.
    3. the output volume from a conv. layer either has the same dimensions
       as that of the conv. layer (143x143x2 for the example considered
       above), or the same as that of the input volume (288x288x3 for the
       example above).

   the generic arrangement of layers can thus be summarized as
   follows[15]:

   [38]eq_8

   where n usually takes values between 0 and 3, m >= 0 and k   [0,3).

   the expression indicates multiple layers, with or without per
   layer-pooling. the final layer is the fully-connected output layer. see
   [8] for more case-studies of id98 architectures, as well as a detailed
   discussion of layers and hyper-parameters.

conclusion

   id98s showcase the awesome levels of control over performance that can
   be achieved by making effective use of theoretical and mathematical
   insights. many real world problems are being efficiently tackled using
   id98s, and mnist represents a simple,    hello world   -type use-case of
   this technique. more complex problems such as object and image
   recognition require the use of deep neural networks with millions of
   parameters to obtain state-of-the-art results. cifar-10 is a good
   problem to solve in this domain, and it was first solved by alex
   krizhevsky et al.[16] in 2009. you can read through the technical
   report and try and grasp the approach before making way to the
   tensorflow tutorial that solves the same problem[17].

   furthermore, applications are not limited to id161. the most
   recent win of google   s alphago project over lee sedol in the go game
   series relied on a id98 at its core. the self-driving cars which, in the
   coming years, will arguably become a regular sight on our streets, rely
   on id98s for steering[18]. google even held an art-show[19] for imagery
   created by its deepdream project that showcased beautiful works of art
   created by visualizing the transformations of the network!

   thus a machine learning researcher or engineer in today   s world can
   rejoice at the technological melange of techniques at her disposal,
   among which an in-depth understanding of id98s is both indispensable and
   empowering.
     __________________________________________________________________


   references

   [1] hopfield, john j.    neural networks and physical systems with
   emergent collective computational abilities.    proceedings of the
   national academy of sciences 79.8 (1982):
   2554-2558.[[39]http://www.pnas.org/content/79/8/2554.abstract]

   [2]  rumelhart, d. e., hinton, g. e., and williams, r. j. (1986).
   learning representations by back-propagating errors. nature, 323,
   533   536.[[40]http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/bac
   kprop_old.pdf]

   [3]   andrey kurenkov,    a brief history of neural nets and deep
   learning   .[[41]http://www.andreykurenkov.com/writing/a-brief-history-of
   -neural-nets-and-deep-learning/]

   [4]  lecun, yann, et al.    gradient-based learning applied to document
   recognition.    proceedings of the ieee 86.11 (1998): 2278-2324.

   [5] [42]the mnist database of handwritten digits

   [6] hubel, david h., and torsten n. wiesel.    receptive fields and
   functional architecture of monkey striate cortex.    the journal of
   physiology 195.1 (1968): 215-243.

   [7] alpha go video by nature.

   [[43]http://www.nature.com/news/google-ai-algorithm-masters-ancient-gam
   e-of-go-1.19234]

   [8] clark, christopher, and amos storkey.    teaching deep convolutional
   neural networks to play go.    arxiv preprint arxiv:1412.3409
   (2014).[[44]http://arxiv.org/pdf/1412.3409.pdf]

   [9] wallach, izhar, michael dzamba, and abraham heifets.    atomnet: a
   deep convolutional neural network for bioactivity prediction in
   structure-based drug discovery.    arxiv preprint arxiv:1510.02855
   (2015).

   [[45]http://arxiv.org/pdf/1510.02855.pdf]

   [10] [46]weisstein, eric w.    convolution.    from [47]mathworld     a
   wolfram web resource.
   [h[48]ttp://mathworld.wolfram.com/convolution.html]

   [11] table of classification accuracies attained over mnist.
   [[49]https://en.wikipedia.org/wiki/mnist_database#performance]

   [12] chris olah,    understanding convolutions   .

   [[50]http://colah.github.io/posts/2014-07-understanding-convolutions/]

   [13] tim dettmers,    understanding convolution in deep
   learning   .[[51]http://timdettmers.com/2015/03/26/convolution-deep-learn
   ing/]

   [14] tensorflow documentation: convolution
   [[52]https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#c
   onvolution]

   [15] andrej karpathy,    cs231n: convolutional neural networks for visual
   recognition    [[53]http://cs231n.github.io/convolutional-networks/]

   [16] krizhevsky, alex, and geoffrey hinton.    learning multiple layers
   of features from tiny images.   
   (2009).[[54]http://www.cs.toronto.edu/~kriz/learning-features-2009-tr.p
   df]

   [17] tensorflow: convolutional
   networks.[[55]https://www.tensorflow.org/versions/r0.7/tutorials/deep_c
   nn/index.html#cifar-10-model]

   [18] google deepmind   s alphago: how it works.
   [[56]https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works
   /]

   [19] an empirical evaluation of deep learning on highway
   driving.[[57]http://arxiv.org/pdf/1504.01716.pdf]

   [20] inside google   s first deepdream art project.
   [[58]http://www.fastcodesign.com/3057368/inside-googles-first-deepdream
   -art-show/11]

   this entry was posted in [59]machine learning, [60]uncategorized,
   [61]xrds by [62]abhineet saxena. bookmark the [63]permalink.

about abhineet saxena

   abhineet saxena is a bachelors of technology (b.tech.) undergraduate
   student at guru gobind singh indraprastha university, new delhi, india.
   he   s majoring in computer science and engineering. his topics of
   interest include machine learning with special emphasis on cluster
   analysis and neural networks. he enjoys programming in python, blogging
   and composing poetry in leisure time.
   [64]view all posts by abhineet saxena    

15 thoughts on    convolutional neural networks (id98s): an illustrated
explanation   

    1.
   changkaizhao on [65]july 1, 2016 at 8:34 am said:
       how to calculate the rcc2 in figure 2?
       [66]reply    
          +
        poornachandra sandur on [67]january 16, 2017 at 3:15 pm said:
            hi abhineet,
            studied your blog on id98 , it is very good insight
            into id98. the theoretical aspects were covered to a superb
            extent. but ,
            i want to know how the id98s work practically in image
            recognition tasks.
            suppose , i have a training dataset consisting images of
            tables and
            chairs , and i want to design a classifier which recognizes
            table and
            chair on the test data set . can you please suggest me some
            pointers on
            how to feed these images into a id98 using python language. i
            am a
            newbie in deep learning , please suggest me some pointers on
            it.
            [68]reply    
               o
             [69]abhineet saxena on [70]january 14, 2018 at 4:28 am said:
                 i apologise for such a late reply. if it offers any help,
                 you can pre-process the data such that a 2d pixel matrix
                 can be supplied as input to the id98. if you wish to
                 perform the same using tensorflow, you can refer here for
                 some explanations and insights:
                 [71]https://github.com/acmxrds/summer-2016/tree/master/he
                 lloworld
                 [72]reply    
    2.
   [73]donnell chappelle on [74]july 30, 2016 at 8:36 am said:
       thanks!
       [75]reply    
    3.
   dipti on [76]november 20, 2016 at 2:57 pm said:
       great article!
       [77]reply    
    4.
   liliang on [78]december 12, 2016 at 10:32 am said:
       thank u so much! i have understand id98 now!
       [79]reply    
          +
        adam on [80]november 1, 2017 at 5:58 pm said:
            thanks
            [81]reply    
    5.
   hauk on [82]december 31, 2016 at 1:19 am said:
       hi,
       how do you update the convolutional filters at back propagation ?
       [83]reply    
    6.
   jagesh maharjan on [84]april 10, 2017 at 10:25 am said:
       well explained, but i have a question. if i had to train on the
       text using id27, and i take a filter size equal to the
       dimension of the token. should i stride (make a movement)
       horizontally.
       [85]reply    
    7.
   suhas jadhav on [86]june 8, 2017 at 11:17 am said:
       is id98 algorithm is applicable on text (array)?
       [87]reply    
          +
        [88]abhineet saxena on [89]january 14, 2018 at 4:18 am said:
            for dealing with text data, the mapping of text to integer
            values (for each unique word) has to be done firstly. the
            integer map so obtained is then supplied as input. since id98s
            deal with 2d data, your text must be in a 2d grid format. for
            more information, you can refer here:
            [90]http://www.wildml.com/2015/12/implementing-a-id98-for-text-
            classification-in-tensorflow/
            [91]reply    
    8.
   shantanu on [92]october 4, 2017 at 1:59 am said:
       thank you abhineet!
       [93]reply    
    9.
   [94]test1 on [95]october 25, 2017 at 3:41 pm said:
       outstanding post, i think people should learn a lot from this web
       blog its very user pleasant.
       [96]reply    
   10.
   guoshikun on [97]december 2, 2017 at 12:33 am said:
       hello,i think there will be a problem with the convolution of image
       and kernel,you should flip the kernel first , please look at the
       website of
       [98]http://www.songho.ca/dsp/convolution/convolution2d_example.html
       the case that we didn   t flip the kernel is the kernel is symmetry.
       [99]reply    
   11.
   johan on [100]january 28, 2018 at 8:07 am said:
          the process is demonstrated in figure 2, using a toy example
       consisting of a 3-channel 4  4-pixels input image and a 3  3 kernel
       matrix. note that:   
       this is a very smal image, would be great if you can focus on the
       256  256
       [101]reply    

leave a reply [102]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] notify me of follow-up comments by email.

   [ ] notify me of new posts by email.

   post comment

   [103]proudly powered by wordpress

references

   visible links
   1. https://blog.xrds.acm.org/feed/
   2. https://blog.xrds.acm.org/comments/feed/
   3. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/feed/
   4. https://blog.xrds.acm.org/wp-json/oembed/1.0/embed?url=https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/
   5. https://blog.xrds.acm.org/wp-json/oembed/1.0/embed?url=https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/&format=xml
   6. https://blog.xrds.acm.org/
   7. https://blog.xrds.acm.org/
   8. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#content
   9. https://blog.xrds.acm.org/
  10. https://blog.xrds.acm.org/about-the-xrds-blog/
  11. https://blog.xrds.acm.org/authors/
  12. https://xrds.acm.org/
  13. https://xrds.acm.org/news.cfm
  14. https://xrds.acm.org/events.cfm
  15. https://xrds.acm.org/current-issue.cfm
  16. https://blog.xrds.acm.org/
  17. https://xrds.acm.org/archives.cfm
  18. https://xrds.acm.org/get-involved.cfm
  19. https://xrds.acm.org/resources.cfm
  20. https://blog.xrds.acm.org/2016/06/road-hell-paved-good-intensions/
  21. https://blog.xrds.acm.org/2016/07/thoughts-on-usability-studies/
  22. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/
  23. https://blog.xrds.acm.org/author/abhineetsaxena/
  24. http://www.nature.com/news/google-ai-algorithm-masters-ancient-game-of-go-1.19234
  25. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-1.png
  26. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-2.png
  27. http://xrds.acm.org/blog/wp-content/uploads/2016/06/2016-06-29-22_48_55-id98_blogpost_-google-docs.png
  28. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure_2.png
  29. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-6.png
  30. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure_3.png
  31. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure_4.png
  32. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure_5.png
  33. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-3.png
  34. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-4.png
  35. http://xrds.acm.org/blog/wp-content/uploads/2016/06/codecogseqn-5.png
  36. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure_6.png
  37. http://xrds.acm.org/blog/wp-content/uploads/2016/06/eq_7.gif
  38. http://xrds.acm.org/blog/wp-content/uploads/2016/06/eq_8.png
  39. http://www.pnas.org/content/79/8/2554.abstract
  40. http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf
  41. http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/
  42. http://yann.lecun.com/exdb/mnist/
  43. http://www.nature.com/news/google-ai-algorithm-masters-ancient-game-of-go-1.19234
  44. http://arxiv.org/pdf/1412.3409.pdf
  45. http://arxiv.org/pdf/1510.02855.pdf
  46. http://mathworld.wolfram.com/about/author.html
  47. http://mathworld.wolfram.com/
  48. http://mathworld.wolfram.com/convolution.html
  49. https://en.wikipedia.org/wiki/mnist_database#performance
  50. http://colah.github.io/posts/2014-07-understanding-convolutions/
  51. http://timdettmers.com/2015/03/26/convolution-deep-learning/
  52. https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#convolution
  53. http://cs231n.github.io/convolutional-networks/
  54. http://www.cs.toronto.edu/~kriz/learning-features-2009-tr.pdf
  55. https://www.tensorflow.org/versions/r0.7/tutorials/deep_id98/index.html#cifar-10-model
  56. https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/
  57. http://arxiv.org/pdf/1504.01716.pdf
  58. http://www.fastcodesign.com/3057368/inside-googles-first-deepdream-art-show/11
  59. https://blog.xrds.acm.org/category/machine-learning/
  60. https://blog.xrds.acm.org/category/uncategorized/
  61. https://blog.xrds.acm.org/category/xrds-2/
  62. https://blog.xrds.acm.org/author/abhineetsaxena/
  63. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/
  64. https://blog.xrds.acm.org/author/abhineetsaxena/
  65. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-16644
  66. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-16644
  67. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-25013
  68. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-25013
  69. https://abhineetsaxena.wordpress.com/
  70. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39852
  71. https://github.com/acmxrds/summer-2016/tree/master/helloworld
  72. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39852
  73. http://www.airindia.in/
  74. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-17599
  75. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-17599
  76. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-21487
  77. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-21487
  78. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-22922
  79. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-22922
  80. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-37861
  81. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-37861
  82. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-24029
  83. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-24029
  84. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-30256
  85. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-30256
  86. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-32298
  87. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-32298
  88. https://abhineetsaxena.wordpress.com/
  89. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39851
  90. http://www.wildml.com/2015/12/implementing-a-id98-for-text-classification-in-tensorflow/
  91. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39851
  92. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-36543
  93. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-36543
  94. http://www.test1.co.il/
  95. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-37554
  96. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-37554
  97. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39018
  98. http://www.songho.ca/dsp/convolution/convolution2d_example.html
  99. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-39018
 100. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-40128
 101. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#comment-40128
 102. https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-id98s-illustrated-explanation/#respond
 103. https://wordpress.org/

   hidden links:
 105. http://xrds.acm.org/blog/wp-content/uploads/2016/06/figure1.png
