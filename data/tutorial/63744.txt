
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [wavenet_abstract.2e16d0ba.fill-1100x400_ffj0zzm.png]

high-fidelity id133 with wavenet

   in october we announced that our state-of-the-art id133
   model [35]wavenet was being used to generate realistic-sounding voices
   for the[36] google assistant globally in japanese and the us english.
   this production model - known as parallel wavenet - is more than 1000
   times faster than the [37]original and also capable of creating higher
   quality audio.

   our [38]latest paper introduces details of the new model and the
      id203 density distillation    technique we developed to allow the
   system to work in a massively parallel computing environment.

   the original wavenet model used autoregressive connections to
   synthesise the waveform one sample at a time, with each new sample
   conditioned on the previous samples. while this produces high-quality
   audio with up to 24,000 samples per second, this sequential generation
   is too slow for production environments.
   the structure of the convolutional neural network that underpins the
   original wavenet model
   the original model synthesises one sample at a time with each sample
   conditioned on previous ones

   to get around this we needed a solution that could generate long
   sequences of samples all at once and with no loss of quality. our
   solution is called id203 density distillation, where we used a
   fully-trained wavenet model to teach a second,    student    network that
   is both smaller and more parallel and therefore better suited to modern
   computational hardware. this student network is a smaller dilated
   [39]convolutional neural network, similar to the original wavenet. but,
   crucially, generation of each sample does not depend on any of the
   previously generated samples, meaning we can generate the first and
   last word - and everything in between -  at the same time, as shown in
   the animation below.

   parallel wavenet architecture
   the new wavenet model uses white noise as an input and synthesises all
   of the output samples in parallel

   during training, the student network starts off in a random state. it
   is fed random white noise as an input and is tasked with producing a
   continuous audio waveform as output. the generated waveform is then fed
   to the trained wavenet model, which scores each sample, giving the
   student a signal to understand how far away it is from the teacher
   network   s desired output. over time, the student network can be tuned -
   via id26 - to learn what sounds it should produce. put
   another way, both the teacher and the student output a id203
   distribution for the value of each audio sample, and the goal of the
   training is to minimise the [40]kl divergence between the teacher   s
   distribution and the student   s distribution.

   the training method has parallels to the set-up for generative
   adversarial networks (gans), with the student playing the role of
   generator and the teacher as the discriminator. however, unlike gans,
   the student   s aim is not to    fool    the teacher but to cooperate and try
   to match the teacher   s performance.

   although the training technique works well, we also need to add a few
   extra id168s to guide the student towards the desired
   behaviour. specifically, we add a [41]perceptual loss to avoid bad
   pronunciations, a contrastive loss to further reduce the noise, and a
   power loss to help match the energy of the human speech. without the
   latter, for example, the trained model whispers rather than speaking
   out loud.

   adding all of these together allowed us to train the parallel wavenet
   to achieve the same quality of speech as the original wavenet, as shown
   by the mean opinion scores (mos) - a scale of 1-5 that measures of how
   natural sounding the speech is according to tests with human listeners.
   note that even human speech is rated at just 4.667 on the mos scale.

   [table_smaller_still.width-400_rtwtdq7.png]

   of course, the development of id203 density distillation was just
   one of the steps needed to allow wavenet to meet the speed and quality
   requirements of a production system. incorporating parallel wavenet
   into the serving pipeline of the google assistant required an equally
   significant engineering effort by the deepmind applied and google
   speech teams. it was only by working together that we could move from
   fundamental research to google-scale product in a little over 12
   months.
     __________________________________________________________________

   read the [42]new paper.

   read more about [43]wavenet in the google assistant.

   read the [44]original wavenet blog post.

   read the original [45]wavenet paper.

   this work was done by aaron van den oord, yazhe li, igor babuschkin,
   karen simonyan, oriol vinyals, koray kavukcuoglu, george van den
   driessche, edward lockhart, luis c. cobo, florian stimberg, norman
   casagrande, dominik grewe, seb noury, sander dieleman, erich elsen, nal
   kalchbrenner, heiga zen, alex graves, helen king, tom walters, dan
   belov and demis hassabis.

   share article
     *
     *
     *
     *
     * [ ]
          + [46]linkedin
          + whatsapp
          + sms
          + [47]reddit

   authors
   wednesday, 22 november 2017
     * [avdnoord.2e16d0ba.fill-80x80_fbmu1cv.png]
       a  ron van den oord
       research scientist, deepmind
     * [yazhe%2520li.2e16d0ba.fill-80x80_fsgoeaj.png]
       yazhe li
       research engineer, deepmind
     * [ibab.2e16d0ba.fill-80x80_0oorcld.png]
       igor babuschkin
       research engineer, deepmind

   ____________________
   ____________________
   [48]show all results
   (button) close

                               [49]deepmind logo

   follow
     *
     *
     *

     * [50]research [51]research
     * [52]applied [53]applied
     * [54]news & blog [55]news & blog
     * [56]about us [57]about us
     * [58]careers [59]careers

     * [60]press
     * [61]terms and conditions
     * [62]privacy policy     updated
     * [63]modern slavery statement
     * [64]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [65]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  36. https://deepmind.com/blog/wavenet-launches-google-assistant/
  37. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  38. https://arxiv.org/abs/1711.10433
  39. https://en.wikipedia.org/wiki/convolutional_neural_network
  40. https://en.wikipedia.org/wiki/kullback   leibler_divergence
  41. https://arxiv.org/abs/1508.06576
  42. https://arxiv.org/abs/1711.10433
  43. https://deepmind.com/blog/wavenet-launches-google-assistant/
  44. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  45. https://arxiv.org/pdf/1609.03499.pdf
  46. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/&title=high-fidelity id133 with wavenet&summary=&source=google deepmind
  47. http://www.reddit.com/submit?url=https://deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/&title=high-fidelity id133 with wavenet
  48. https://deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/
  49. https://deepmind.com/
  50. https://deepmind.com/research/
  51. https://deepmind.com/research/
  52. https://deepmind.com/applied/
  53. https://deepmind.com/applied/
  54. https://deepmind.com/blog/
  55. https://deepmind.com/blog/
  56. https://deepmind.com/about/
  57. https://deepmind.com/about/
  58. https://deepmind.com/careers/
  59. https://deepmind.com/careers/
  60. https://deepmind.com/press/
  61. https://deepmind.com/terms-and-conditions/
  62. https://deepmind.com/privacy-policy/
  63. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  64. https://abc.xyz/
  65. https://deepmind.com/privacy-policy/

   hidden links:
  67. https://twitter.com/deepmindai
  68. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  69. https://plus.google.com/+deepmindai
  70. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/
  71. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/&t=
  72. https://plus.google.com/share?url=https%3a//deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/
  73. mailto:?subject=high-fidelity%20speech%20synthesis%20with%20wavenet&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/
  74. https://twitter.com/deepmindai
  75. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  76. https://plus.google.com/+deepmindai
