   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

probabilistic id114 tutorial         part 2

parameter estimation and id136 algorithms

   [16]go to the profile of prasoon goyal
   [17]prasoon goyal (button) blockedunblock (button) followfollowing
   nov 23, 2017

   in the [18]previous part of this probabilistic id114
   tutorial for the [19]statsbot team, we looked at the two types of
   id114, namely id110s and markov networks. we also
   explored the problem setting, conditional independences, and an
   application to the monty hall problem. in this post, we will cover
   parameter estimation and id136, and look at another application.
   [0*lg-yjw6tozrpizvk.]

parameter estimation

id110s

   estimating the numbers in the cpd tables of a id110 simply
   amounts to counting how many times that event occurred in our training
   data. that is, to estimate p(sat=s1 | intelligence = i1), we simply
   count the fraction of data points where sat=s1 and intelligence = i1,
   out of the total data points where intelligence = i1. while this
   approach may appear ad hoc, it turns out that the parameters so
   obtained maximize the likelihood of the observed data.

markov networks

   for markov networks, unfortunately, the above counting approach does
   not have a statistical justification (and will therefore lead to
   suboptimal parameters). so, we need to use more sophisticated
   techniques. the basic idea behind most of these techniques is gradient
   descent         we define parameters that describe the id203
   distribution, and then use id119 to find values for these
   parameters that maximize the likelihood of the observed data.

   finally, now that we have the parameters of our model, we want to use
   them on new data, to perform id136!

id136

   the bulk of the literature in probabilistic id114 focuses on
   id136. the reasons are two-fold:
    1. id136 is why we came up with this entire framework         being able
       to make predictions from what we already know.
    2. id136 is computationally hard! in some specific kinds of
       graphs, we can perform id136 fairly efficiently, but on general
       graphs, it is intractable. so we need to use approximate algorithms
       that trade off accuracy for efficiency.

   there are several questions we can answer with id136:
     * marginal id136: finding the id203 distribution of a
       specific variable. for instance, given a graph with variables a, b,
       c, and d, where a takes values 1, 2, and 3, find p(a=1), p(a=2) and
       p(a=3).
     * posterior id136: given some observed variables v_e (e for
       evidence) that take values e, finding the posterior distribution
       p(v_h | v_e=e) for some hidden variables v_h.
     * maximum-a-posteriori (map) id136: given some observed variables
       v_e that take values e, finding the setting of other variables v_h
       that have the highest id203.

   answers to these questions may be useful by themselves, or may need to
   be used as part of larger tasks.

   in what follows, we are going to look at some of the popular algorithms
   for answering these questions, both exact and approximate. all these
   algorithms are applicable on both id110s and markov
   networks.

variable elimination

   using the definition of id155, we can write the
   posterior distribution as:
   [1*kuistmmr3h1xuwx9ytaeew.png]

   let   s see how we can compute the numerator and the denominator above,
   using a simple example. consider a network with three variables, and
   the joint distribution defined as follows:
   [0*l7cmtzsdzdab2iaa.]

   let   s say we want to compute p(a | b=1). note that this means that we
   want to compute the values p(a=0 | b=1)and p(a=1 | b=1), which should
   sum to one. using the above equation, we can write
   [1*ovlbtb8gj9n-dsna6stp4w.png]

   the numerator is the id203 that a = 0 and b = 1. we don   t care
   about the values of c. so we would sum over all the values of c. (this
   comes from basic id203         p(a=0, b=1, c=0) and p(a=0, b=1, c=1)
   are mutually exclusive events, so their union p(a = 0, b=1) is just the
   sum of the individual probabilities.)

   so we add rows 3 and 4 to get p(a=0, b=1) = 0.15. similarly, adding
   rows 7 and 8 gives usp(a=1, b=1) = 0.40. also, we can compute the
   denominator by summing over all rows that contain b=1, that is, rows 3,
   4, 7, and 8, to get p(b=1) = 0.55. this gives us the following:

   p(a = 0 | b = 1) = 0.15 / 0.55 = 0.27

   p(a = 1 | b = 1) = 0.40 / 0.55 = 0.73

   if you look at the above computation closely, you would notice that we
   did some repeated computations         adding rows 3 & 4, and 7 & 8 twice. a
   more efficient way to compute p(b=1)would have been to simply add the
   values p(a=0, b=1) and p(a=1, b=1). this is the basic idea of variable
   elimination.

   in general, when you have a lot of variables, not only can you use the
   values of the numerator to compute the denominator, but the numerator
   by itself will contain repeated computations, if evaluated naively. you
   can use id145 to use precomputed values efficiently.

   because we are summing over one variable at a time, thereby eliminating
   it, the process of summing out multiple variables amounts to
   eliminating these variables one at a time. hence, the name    variable
   elimination.   

   it is straightforward to extend the above process to solve the marginal
   id136 or map id136 problems as well. similarly, it is easy to
   generalize the above idea to apply it to markov networks too.

   the time complexity of variable elimination depends on the graph
   structure, and the order in which you eliminate the variables. in the
   worst case, it has exponential time complexity.

belief propagation

   the ve algorithm that we just saw gives us only one final distribution.
   suppose we want to find the marginal distributions for all variables.
   instead of running variable elimination multiple times, we can do
   something smarter.

   suppose you have a graph structure. to compute a marginal, you need to
   sum the joint distribution over all other variables, which amounts to
   aggregating information from the entire graph. here   s an alternate way
   of aggregating information from the entire graph         each node looks at
   its neighbors, and approximates the distribution of variables locally.

   then, every pair of neighboring nodes send    messages    to each other
   where the messages contain the local distributions. now, every node
   looks at the messages it receives, and aggregates them to update its
   id203 distributions of variables.
   [0*b0qprhbem60oe0jq.]

   in the figure above, c aggregates information from its neighbors a and
   b, and sends a message to d. then, d aggregates this message with the
   information from e and f.

   the advantage of this approach is that if you save the messages that
   you are sending at every node, one forward pass of messages followed by
   one backward pass gives all nodes information about all other nodes.
   that information can then be used to compute all the marginals, which
   was not possible in variable elimination.

   if the graph does not contain cycles, then this process converges after
   a forward and a backward pass. if the graph contains cycles, then this
   process may or may not converge, but it can often be used to get an
   approximate answer.

approximate id136

   because exact id136 may be prohibitively time consuming for large
   id114, numerous approximate id136 algorithms have been
   developed for id114, most of which fall into one of the
   following two categories:

   sampling-based
   these algorithms estimate the desired id203 using sampling. as a
   simple example, consider the following scenario         given a coin, how you
   would determine the id203 of getting heads when the coin is
   tossed? the simplest thing is to flip the coin, say, 100 times, and
   find out the fraction of tosses in which you get heads.

   this is a sampling-based algorithm to estimate the id203 of
   heads. for more complex questions in probabilistic id114,
   you can use a similar procedure. sampling-based algorithms can further
   be divided into two classes. in the first one, the samples are
   independent of each other, as in the coin toss example above. these
   algorithms are called monte carlo methods.

   for problems with many variables, generating good quality independent
   samples is difficult, and therefore, we generate dependent samples,
   that is, each new sample is random, but close to the last sample. such
   algorithms are called id115 (mcmc) methods, because
   the samples form a    markov chain.    once we have the samples, we can use
   them to answer various id136 questions.

   variational methods
   instead of using sampling, variational methods try to approximate the
   required distribution analytically. suppose you write out the
   expression for computing the distribution of interest         marginal
   id203 distribution or posterior id203 distribution.

   often, these expressions have summations or integrals in them that are
   computationally expensive to evaluate exactly. a good way to
   approximate these expressions is to then solve for an alternate
   expression, and somehow ensure that this alternate expression is close
   to the original expression. this is the basic idea behind variational
   methods.

   when we are trying to estimate a complex id203 distribution
   p_complex, we define a separate set of id203 distributions
   p_simple, which are easier to work with, and then find the id203
   distribution p_approx from p_simple that is closest to p_complex.

application: image denoising

   let us now use some of the ideas we just discussed on a real problem.
   let   s say you have the following image:
   [0*vgqdthrooo6qbvmp.]

   now suppose that it got corrupted by random noise, so that your noisy
   image looks as follows:
   [0*waxttfupbuse9xpx.]

   the goal is to recover the original image. let   s see how we can use
   probabilistic id114 to do this.

   the first step is to think about what our observed and unobserved
   variables are, and how we can connect them to form a graph. let us
   define each pixel in the noisy image as an observed random variable,
   and each pixel in the ground truth image as an unobserved variable. so,
   if the image is m x n, then there are mn observed variables and mn
   unobserved variables. let us denote observed variables as x_ij and
   unobserved variables as y_ij. each variable takes values +1 and -1
   (corresponding to black and white pixels, respectively). given the
   observed variables, we want to find the most likely values of the
   unobserved variables. this corresponds to map id136.

   now, let us use some domain knowledge to build the graph structure.
   clearly, the observed variable at position (i, j) in the noisy image
   depends on the unobserved variable at position (i, j) in the ground
   truth image. this is because most of the time, they are identical.

   what more can we say? for ground truth images, the neighboring pixels
   usually have the same values         this is not true at the boundaries of
   color change, but inside a single-colored region, this property holds.
   therefore, we connect y_ij and y_kl if they are neighboring pixels.

   so, our graph structure looks as follows:
   [0*oj8nferkb_kt_2ay.]

   here, the white nodes denote the unobserved variables y_ij and the grey
   nodes denote observed variables x_ij. each x_ij is connected to the
   corresponding y_ij, and each y_ij is connected to its neighbors.

   note that this is a markov network, because there is no cause-effect
   relation between pixels of an image, and therefore, defining directions
   of arrows in id110s is unnatural here.

   our map id136 problem can be mathematically written as follows:
   [1*8mbwtwpamogfrwrfj8wgyq.png]

   here, we used some standard simplification techniques common in maximum
   log likelihood computation. we will use x and y(without subscripts) to
   denote the collection of all x_ij and y_ij values, respectively.

   now, we need to define our joint distribution p(x, y) based on our
   graph structure. let   s assume that p(x, y) consists of two kinds of
   factors           (x_ij, y_ij) and   (y_ij,y_kl), corresponding to the two
   kinds of edges in our graph. next, we define the factors as follows:
     *   (x_ij, y_ij) = exp(w_e x_ij y_ij), where w_e is a parameter
       greater than zero. this factor takes large values when x_ij and
       y_ij are identical, and takes small values when x_ij and y_ij are
       different.
     *   (y_ij, y_kl) = exp(w_s y_ij y_kl), where w_s is a parameter
       greater than zero, as before. this factor favors identical values
       of y_ij and y_kl.

   therefore, our joint distribution is given by:
   [1*gkooarb_kshmjkysmrwovw.png]

   where (i, j) and (k, l) in the second product are adjacent pixels, and
   z is a id172 constant.

   plugging this into our map id136 equation gives:
   [1*ne9o_ckpsx_epf1vb1awla.png]

   note that we have dropped the term containing zsince it does not affect
   the solution.

   the values of w_e and w_s are obtained using parameter estimation
   techniques from pairs of ground truth and noisy images. this process is
   fairly mathematically involved (although, at the end of the day, it is
   just id119 on a complicated function), and therefore, we
   shall not delve into it here. we will assume that we have obtained the
   following values of these parameters         w_e = 8 and w_s = 10.

   the main focus of this example will be id136. given these
   parameters, we want to solve the map id136 problem above. we can
   use a variant of belief propagation to do this, but it turns out that
   there is a much simpler algorithm called iterated conditional modes
   (icm) for graphs with this specific structure.

   the basic idea is that at each step, you choose one node, y_ij, look at
   the value of the map id136 expression for both y_ij = -1 and y_ij =
   1, and pick the one with the higher value. repeating this process for a
   fixed number of iterations or until convergence usually works
   reasonably well.

     you can use [20]this python code to do this for our model.

   this is the denoised image returned by the algorithm:
   [0*0swysyj8f2obmdtd.]

   pretty good, isn   t it? of course, you can use more fancy techniques,
   both within id114, and outside, to generate something
   better, but the takeaway from this example is that a simple markov
   network with a simple id136 algorithm already gives you reasonably
   good results.

   quantitatively, the noisy image has about 10% of the pixels that are
   different from the original image, while the denoised image produced by
   our algorithm has about 0.6% of the pixels that are different from the
   original image.

   it is important to note that the graph that we used is fairly
   large         the image size is about 440 x 300, so the total number of nodes
   is close to 264,000. therefore, exact id136 in such models is
   essentially infeasible, and what we get out of most algorithms,
   including icm, is a local optimum.

let   s recap

   in this section, let us briefly review the key concepts we covered in
   this two-part series:
     * id114: a graphical model consists of a graph structure
       where nodes represent random variables and edges represent
       dependencies between variables.
     * id110s: these are directed id114, with a
       id155 distribution table associated with each
       node.
     * markov networks: these are undirected id114, with a
       potential function associated with each clique.
     * conditional independences: based on how the nodes in the graph are
       connected, we can write conditional independence statements of the
       form    x is independent of y given z.   
     * parameter estimation: given some data and the graph structure, we
       want to fill the cpd tables or compute the potential functions.
     * id136: given a graphical model, we want to answer questions
       about unobserved variables. these questions are usually one of the
       following         marginal id136, posterior id136, and map
       id136.
     * id136 on general id114 is computationally
       intractable. we can divide id136 algorithms into two broad
       categories         exact and approximate. variable elimination and belief
       propagation in acyclic graphs are examples of exact id136
       algorithms. approximate id136 algorithms are necessary for
       large-scale graphs, and usually fall into sampling-based methods or
       variational methods.

conclusions

   we looked at some of the core ideas in probabilistic id114
   in this two-part tutorial. as you should be able to appreciate at this
   point, id114 provide an interpretable way to model many
   real-world tasks, where there are dependencies. using id114
   gives us a way to work on such tasks in a principled manner.

   before we close, it is important to point out that this tutorial, by no
   means, is complete         many details have been skipped to keep the content
   intuitive and simple. the standard textbook on probabilistic graphical
   models is over a thousand pages! this tutorial is meant to serve as a
   starting point, to get you interested in the field, so that you can
   look up more rigorous resources.

   here are some additional resources that you can use to dig deeper into
   the field:
     * [21]id114 in a nutshell
     * [22]id114 textbook

   you should also be able to find a few chapters on id114 in
   standard machine learning textbooks.

   iframe: [23]/media/02231cd5403151a2a463e36cc3b88462?postid=d855ba0107d1

you   d also like:

   [24]probabilistic id114 tutorial         part 1
   basic terminology and the problem settingblog.statsbot.co
   [25]machine learning algorithms: which one to choose for your problem
   intuition of using different kinds of algorithms in different
   tasksblog.statsbot.co
   [26]neural networks for beginners: popular types and applications
   an introduction to neural networks learningblog.statsbot.co

     * [27]machine learning
     * [28]data science
     * [29]algorithms
     * [30]bayesian statistics
     * [31]markov chains

   (button)
   (button)
   (button) 600 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of prasoon goyal

[33]prasoon goyal

   phd candidate at ut austin. for more content on machine learning by me,
   check my quora profile
   ([34]https://www.quora.com/profile/prasoon-goyal).

     (button) follow
   [35]stats and bots

[36]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 600
     * (button)
     *
     *

   [37]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [38]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d855ba0107d1
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/probabilistic-graphical-models-tutorial-d855ba0107d1&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/probabilistic-graphical-models-tutorial-d855ba0107d1&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_m6vnxlpgqxfc---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@prasoongoyal13?source=post_header_lockup
  17. https://blog.statsbot.co/@prasoongoyal13
  18. https://blog.statsbot.co/probabilistic-graphical-models-tutorial-and-solutions-e4f1d72af189?utm_source=blog&utm_medium=article&utm_campaign=pgm2
  19. https://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=pgm2
  20. https://github.com/prasoongoyal/image-denoising-mrf
  21. https://ai.stanford.edu/~koller/papers/koller+al:srl07.pdf
  22. https://www.amazon.com/probabilistic-graphical-models-principles-computation/dp/0262013193
  23. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=d855ba0107d1
  24. https://blog.statsbot.co/probabilistic-graphical-models-tutorial-and-solutions-e4f1d72af189
  25. https://blog.statsbot.co/machine-learning-algorithms-183cc73197c
  26. https://blog.statsbot.co/neural-networks-for-beginners-d99f2235efca
  27. https://blog.statsbot.co/tagged/machine-learning?source=post
  28. https://blog.statsbot.co/tagged/data-science?source=post
  29. https://blog.statsbot.co/tagged/algorithms?source=post
  30. https://blog.statsbot.co/tagged/bayesian-statistics?source=post
  31. https://blog.statsbot.co/tagged/markov-chains?source=post
  32. https://blog.statsbot.co/@prasoongoyal13?source=footer_card
  33. https://blog.statsbot.co/@prasoongoyal13
  34. https://www.quora.com/profile/prasoon-goyal
  35. https://blog.statsbot.co/?source=footer_card
  36. https://blog.statsbot.co/?source=footer_card
  37. https://blog.statsbot.co/
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://blog.statsbot.co/probabilistic-graphical-models-tutorial-and-solutions-e4f1d72af189
  41. https://blog.statsbot.co/machine-learning-algorithms-183cc73197c
  42. https://blog.statsbot.co/neural-networks-for-beginners-d99f2235efca
  43. https://medium.com/p/d855ba0107d1/share/twitter
  44. https://medium.com/p/d855ba0107d1/share/facebook
  45. https://medium.com/p/d855ba0107d1/share/twitter
  46. https://medium.com/p/d855ba0107d1/share/facebook
