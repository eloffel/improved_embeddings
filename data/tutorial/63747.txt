   [1]home [2]download pdf

     * introduction
     * part one
     * classification/localisation
     * id164
     * object tracking
     * part two
     * segmentation
     * super-resolution, style transfer & colourisation
     * action recognition
     * part three
     * 3d objects
     * human pose estimation
     * reconstruction
     * other uncategorised 3d
     * 3d summation and slam
     * part four
     * convnet architectures
     * datasets
     * ungroupable extras
     * conclusion
     * references

   a year in id161: the m tank, 2017

a year in id161

   edited for the m tank by

   benjamin f. duffy

   &

   daniel r. flynn

   the m tank

   also on medium: [3]part 1, [4]part 2, [5]part 3, [6]part 4

introduction

   id161 typically refers to the scientific discipline of giving
   machines the ability of sight, or perhaps more colourfully, enabling
   machines to visually analyse their environments and the stimuli within
   them. this process typically involves the evaluation of an image,
   images or video. the british machine vision association (bmva) defines
   id161 as    the automatic extraction, analysis and
   understanding of useful information from a single image or a sequence
   of images.   ^[7][1]

   the term understanding provides an interesting counterpoint to an
   otherwise mechanical definition of vision, one which serves to
   demonstrate both the significance and complexity of the id161
   field. true understanding of our environment is not achieved through
   visual representations alone. rather, visual cues travel through the
   optic nerve to the primary visual cortex and are interpreted by the
   brain, in a highly stylised sense. the interpretations drawn from this
   sensory information encompass the near-totality of our natural
   programming and subjective experiences, i.e. how evolution has wired us
   to survive and what we learn about the world throughout our lives.

   in this respect, vision only relates to the transmission of images for
   interpretation; while computing said images is more analogous to
   thought or cognition, drawing on a multitude of the brain   s faculties.
   hence, many believe that id161, a true understanding of
   visual environments and their contexts, paves the way for future
   iterations of strong artificial intelligence, due to its cross-domain
   mastery.

   however, put down the pitchforks as we   re still very much in the
   embryonic stages of this fascinating field. this piece simply aims to
   shed some light on 2016   s biggest id161 advancements. and
   hopefully ground some of these advancements in a healthy mix of
   expected near-term societal-interactions and, where applicable,
   tongue-in-cheek prognostications of the end of life as we know it.

   while our work is always written to be as accessible as possible,
   sections within this particular piece may be oblique at times due to
   the subject matter. we do provide rudimentary definitions throughout,
   however, these only convey a facile understanding of key concepts. in
   keeping our focus on work produced in 2016, often omissions are made in
   the interest of brevity.

   one such glaring omission relates to the functionality of convolutional
   neural networks (hereafter id98s or convnets), which are ubiquitous
   within the field of id161. the success of alexnet ^[8][2] in
   2012, a id98 architecture which blindsided id163 competitors, proved
   instigator of a de facto revolution within the field, with numerous
   researchers adopting neural network-based approaches as part of
   id161   s new period of    normal science   .^[9][3]

   over four years later and id98 variants still make up the bulk of new
   neural network architectures for vision tasks, with researchers
   reconstructing them like legos; a working testament to the power of
   both open source information and deep learning. however, an explanation
   of id98s could easily span several postings and is best left to those
   with a deeper expertise on the subject and an affinity for making the
   complex understandable.

   for casual readers who wish to gain a quick grounding before proceeding
   we recommend the first two resources below. for those who wish to go
   further still, we have ordered the resources below to facilitate that:

     * what a deep neural network thinks about your #selfie from andrej
       karpathy is one of our favourites for helping people understand the
       applications and functionalities behind id98s.^[10][4]
     * quora:    what is a convolutional neural network?    - has no shortage
       of great links and explanations. particularly suited to those with
       no prior understanding.^[11][5]
     * cs231n: convolutional neural networks for visual recognition from
       stanford university is an excellent resource for more
       depth.^[12][6]
     * deep learning (goodfellow, bengio & courville, 2016) provides
       detailed explanations of id98 features and functionality in chapter
       9. the textbook has been kindly made available for free in html
       format by the authors.^[13][7]

   for those wishing to understand more about neural networks and deep
   learning in general we suggest:

     * neural networks and deep learning (nielsen, 2017) is a free online
       textbook which provides the reader with a really intuitive
       understanding of the complexities of neural networks and deep
       learning. even just completing chapter one should greatly
       illuminate the subject matter of this piece for
       first-timers.^[14][8]

   as a whole this piece is disjointed and spasmodic, a reflection of the
   authors    excitement and the spirit in which it was intended to be
   utilised, section by section. information is partitioned using our own
   heuristics and judgements, a necessary compromise due to the
   cross-domain influence of much of the work presented.

   we hope that readers benefit from our aggregation of the information
   here to further their own knowledge, regardless of previous experience.

   from all our contributors,

   the m tank

part one: classification/localisation, id164, object tracking

classification/localisation

   the task of classification, when it relates to images, generally refers
   to assigning a label to the whole image, e.g.    cat   . assuming this,
   localisation may then refer to finding where the object is in said
   image, usually denoted by the output of some form of bounding box
   around the object. current classification/localisation techniques
   on id163^[15][9] have likely surpassed an ensemble of trained
   humans.^[16][10] for this reason, we place greater emphasis on
   subsequent sections of the blog.

   figure 1: id161 tasks

   source: fei-fei li, andrej karpathy & justin johnson (2016) cs231n,
   lecture 8 - slide 8, spatial localization and detection (01/02/2016).
   available:
   [17]http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf

   however, the introduction of larger datasets with an increased number
   of classes^[18][11] will likely provide new metrics for progress in the
   near future. on that point, fran  ois chollet, the creator of
   keras,^[19][12] has applied new techniques, including the popular
   architecture xception, to an internal google dataset with over 350
   million multi-label images containing 17,000 classes.
   ^[20][13],^[21][14]

   figure 2: classification/localisation results from ilsvrc (2010-2016)

   note: id163 large scale visual recognition challenge (ilsvrc). the
   change in results from 2011-2012 resulting from the alexnet
   submission. for a review of the challenge requirements relating to
   classification and localization see:
   [22]http://www.image-net.org/challenges/lsvrc/2016/index#comp

   source: jia deng (2016). ilsvrc2016 object localisation: introduction,
   results. slide 2. available:
   [23]http://image-net.org/challenges/talks/2016/ilsvrc2016_10_09_clsloc.
   pdf

   interesting takeaways from the id163 lsvrc (2016):

     * scene classification refers to the task of labelling an image with
       a certain scene class like    greenhouse   ,    stadium   ,    cathedral   ,
       etc. id163 held a scene classification challenge last year with
       a subset of the places2^[24][15] dataset: 8 million images for
       training with 365 scene categories.
       hikvision^[25][16] won with a 9% top-5 error with an ensemble of
       deep inception-style networks, and not-so-deep residuals networks.

     * trimps-soushen won the id163 classification task with 2.99%
       top-5 classification error and 7.71% localisation error. the team
       employed an ensemble for classification (averaging the results of
       inception, inception-resnet, resnet and wide residual networks
       models^[26][17]) and faster r-id98 for localisation based on the
       labels.^[27][18] the dataset was distributed across 1000 image
       classes with 1.2 million images provided as training data. the
       partitioned test data compiled a further 100 thousand unseen
       images.

     * resnext by facebook came a close second in top-5 classification
       error with 3.03% by using a new architecture that extends the
       original resnet architecture.^[28][19]

id164

   as one can imagine the process of id164 does exactly that,
   detects objects within images. the definition provided for object
   detection by the ilsvrc 2016^[29][20] includes outputting bounding
   boxes and labels for individual objects. this differs from the
   classification/localisation task by applying classification and
   localisation to many objects instead of just a single dominant object.


   figure 3: id164 with face as the only class

   note: picture is an example of face detection, id164 of a
   single class. the authors cite one of the persistent issues in object
   detection to be the detection of small objects. using small faces as a
   test class they explore the role of scale invariance, image resolution,
   and contextual reasoning.
   source: hu and ramanan (2016, p. 1)^[30][21]

   one of 2016   s major trends in id164 was the shift towards a
   quicker, more efficient detection system. this was visible in
   approaches like yolo, ssd and r-fcn as a move towards sharing
   computation on a whole image. hence, differentiating themselves from
   the costly subnetworks associated with fast/faster r-id98 techniques.
   this is typically referred to as    end-to-end training/learning    and
   features throughout this piece.

   the rationale generally is to avoid having separate algorithms focus on
   their respective subproblems in isolation as this typically
   increases training time and can lower network accuracy. that being said
   this end-to-end adaptation of networks typically takes place after
   initial sub-network solutions and, as such, is a retrospective
   optimisation. however, fast/faster r-id98 techniques remain highly
   effective and are still used extensively for id164.


     * ssd: single shot multibox detector^[31][22] utilises a single
       neural network which encapsulates all the necessary computation and
       eliminates the costly proposal generation of other methods. it
       achieves    75.1% map, outperforming a comparable state of the art
       faster r-id98 model    (liu et al. 2016).

     * one of the most impressive systems we saw in 2016 was from the
       aptly named    yolo9000: better, faster, stronger   ^[32][23], which
       introduces the yolov2 and yolo9000 detection
       systems.^[33][24] yolov2 vastly improves the initial yolo model
       from mid-2015,^[34][25] and is able to achieve better results at
       very high fps (up to 90 fps on low resolution images using the
       original gtx titan x). in addition to completion speed, the system
       outperforms faster rid98 with resnet and ssd on certain object
       detection datasets.


   yolo9000 implements a joint training method for detection and
   classification extending its prediction capabilities beyond the
   labelled detection data available i.e. it is able to detect objects
   that it has never seen labelled detection data for. the yolo9000 model
   provides real-time id164 across 9000+ categories, closing
   the dataset size gap between classification and detection. additional
   details, pre-trained models and a video showing it in action is
   available [35]here. ^[36][26]

   iframe: [37]https://www.youtube.com/embed/voc3huqhrss

     * feature pyramid networks for id164^[38][27] comes from
       fair ^[39][28] and capitalises on the    inherent multi-scale,
       pyramidal hierarchy of deep convolutional networks to construct
       feature pyramids with marginal extra cost   , meaning that
       representations remain powerful without compromising speed or
       memory. lin et al. (2016) achieve state-of-the-art (hereafter sota)
       single-model results on coco^[40][29]. beating the results achieved
       by winners in 2016 when combined with a basic faster r-id98 system.

     * r-fcn: id164 via region-based fully convolutional
       networks:^[41][30] this is another method that avoids applying a
       costly per-region subnetwork hundreds of times over an image by
       making the region-based detector fully convolutional and sharing
       computation on the whole image.    our result is achieved at a
       test-time speed of 170ms per image, 2.5-20x faster than the faster
       r-id98 counterpart    (dai et al., 2016).

   figure 4: accuracy tradeoffs in id164

   note: y-axis displays map (mean average precision) and the x-axis
   displays meta-architecture variability across each feature extractor
   (vgg, mobilenet...inception resnet v2). additionally, map small, medium
   and large describe the average precision for small, medium and large
   objects, respectively. as such accuracy is    stratified by object size,
   meta-architecture and feature extractor    and    image resolution is fixed
   to 300   . while faster r-id98 performs comparatively well in the above
   sample, it is worth noting that the meta-architecture is considerably
   slower than more recent approaches, such as r-fcn.

   source: huang et al. (2016, p. 9)^[42][31]

   huang et al. (2016)^[43][32] present a paper which provides an in depth
   performance comparison between r-fcn, ssd and faster r-id98. due to the
   issues around accurate comparison of machine learning (ml) techniques
   we   d like to point to the merits of producing a standardised approach
   here. they view these architectures as    meta-architectures    since they
   can be combined with different kinds of feature extractors such as
   resnet or inception.

   the authors study the trade-off between accuracy and speed by varying
   meta-architecture, feature extractor and image resolution. the choice
   of feature extractor for example produces large variations between
   meta-architectures.

   the trend of making id164 cheap and efficient while still
   retaining the accuracy required for real-time commercial applications,
   notably in autonomous driving applications, is also demonstrated by
   squeezedet^[44][33] and pvanet^ [45][34]  papers. while a chinese
   company, deepglint, provides a good example of id164 in
   operation as a cctv integration, albeit in a vaguely orwellian manner:
   [46]video. ^[47][35]

   iframe: [48]https://www.youtube.com/embed/xhp47v5obxq

   results from ilsvrc and coco detection challenge

   coco^[49][36] (common objects in context) is another popular image
   dataset. however, it is comparatively smaller and more curated than
   alternatives like id163, with a focus on object recognition within
   the broader context of scene understanding. the organizers host a
   yearly challenge for id164, segmentation and keypoints.
   detection results from both the ilsvrc^[50][37] and the
   coco^[51][38] detection challenge are;

     * id163 lsvrc id164 from images (det): cuimage 66%
       meanap. won 109 out of 200 object categories.
     * id163 lsvrc id164 from video (vid): nuist 80.8% mean
       ap
     * id163 lsvrc id164 from video with tracking: cuvideo
       55.8% mean ap
     * coco 2016 detection challenge (bounding boxes): g-rmi (google)
       41.5% ap (4.2% absolute percentage increase from 2015 winner
       msravc)

   in review of the detection results for 2016, id163 stated that the
      msravc 2015 set a very high bar for performance [introduction of
   resnets to competition]. performance on all classes has improved across
   entries. localization improved greatly in both challenges. high
   relative improvement on small object instances    (id163,
   2016).^[52][39]

   figure 5: ilsvrc detection results from images (2013-2016)

   note: ilsvrc id164 results from images (det) (2013-2016).

   source: id163. 2016. [online] workshop presentation, slide 2.
   available:
   [53]http://image-net.org/challenges/talks/2016/eccv2016_ilsvrc_coco_det
   ection_segmentation.pdf

object tracking

   refers to the process of following a specific object of interest, or
   multiple objects, in a given scene. it traditionally has applications
   in video and real-world interactions where observations are made
   following an initial id164; the process is crucial to
   autonomous driving systems for example.

     * fully-convolutional siamese networks for object
       tracking^[54][40] combines a basic tracking algorithm with a
       siamese network, trained end-to-end, which achieves sota and
       operates at frame-rates in excess of real-time. this paper attempts
       to tackle the lack of richness available to tracking models from
       traditional online learning methods.

     * learning to track at 100 fps with deep regression
       networks^[55][41] is another paper which attempts to ameliorate the
       existing issues with online training methods. the authors produce a
       tracker which leverages a feed-forward network to learn the generic
       relationships surrounding object motion, appearance and orientation
       which effectively track novel objects without online training.
       provides sota on a standard tracking benchmark while also managing
          to track generic objects at 100 fps    (held et al., 2016).

   video of goturn (generic object tracking using regression

   networks) available: [56]video^[57][42]

   iframe: [58]https://www.youtube.com/embed/kmhwxid86t_i

     * deep motion features for visual tracking^[59][43] merge
       hand-crafted features, deep rgb/appearance features (from id98s),
       and deep motion features (trained on optical flow images) to
       achieve sota. while deep motion features are commonplace in action
       recognition and video classification, the authors claim this is the
       first time they are used for visual tracking. the paper was also
       awarded best paper in icpr 2016, for    id161 and robot
       vision    track.

      this paper presents an investigation of the impact of deep motion
   features in a tracking-by-detection framework. we further show that
   hand-crafted, deep rgb, and deep motion features contain complementary
   information. to the best of our knowledge, we are the first to propose
   fusing appearance information with deep motion features for visual
   tracking. comprehensive experiments clearly suggest that our fusion
   approach with deep motion features outperforms standard methods relying
   on appearance information alone.   
     * virtual worlds as proxy for multi-object tracking
       analysis^[60][44] approaches the lack of true-to-life variability
       present in existing video-tracking benchmarks and datasets. the
       paper proposes a new method for real-world cloning which generates
       rich, virtual, synthetic, photo-realistic environments from scratch
       with full-labels that overcome some of the sterility present in
       existing datasets. the generated images are automatically labelled
       with accurate ground truth allowing a range of applications aside
       from id164/tracking, such as depth and optical flow.

     * globally optimal object tracking with fully convolutional
       networks^[61][45] deals with object variance and occlusion, citing
       these as two of the root limitations within object tracking. "our
       proposed method solves the object appearance variation problem with
       the use of a fully convolutional network and deals with occlusion
       by id145" (lee et al., 2016).

part two: segmentation, super-res/colourisation/style transfer, action
recognition

segmentation

   central to id161 is the process of segmentation, which
   divides whole images into pixel groupings which can then be labelled
   and classified. moreover, semantic segmentation goes further by trying
   to semantically understand the role of each pixel in the image e.g. is
   it a cat, car or some other type of class? instance segmentation takes
   this even further by segmenting different instances of classes e.g.
   labelling three different dogs with three different colours. it is one
   of a barrage of id161 applications currently employed in
   autonomous driving technology suites.

   perhaps some of the best improvements in the area of segmentation come
   courtesy of fair, who continue to build upon their deepmask work from
   2015.^ [62][46] deepmask generates rough    masks    over objects as an
   initial form of segmentation. in 2016, fair introduced
   sharpmask^[63][47] which refines the    masks    provided by deepmask,
   correcting the loss of detail and improving semantic segmentation. in
   addition to this, multipathnet^[64][48] identifies the objects
   delineated by each mask.

      to capture general object shape, you have to have a high-level
   understanding of what you are looking at (deepmask), but to accurately
   place the boundaries you need to look back at lower-level features all
   the way down to the pixels (sharpmask).    - piotr dollar,
   2016.^[65][49]

   figure 6: demonstration of fair techniques in action

   note: the above pictures demonstrate the segmentation techniques
   employed by fair. these include the application of deepmask, sharpmask
   and multipathnet techniques which are applied in that order. this
   process allows accurate segmentation and classification in a variety of
   scenes.

   source: dollar (2016).^[66][50]

   video propagation networks^[67][51] attempt to create a simple model to
   propagate accurate object masks, assigned at first frame, through the
   entire video sequence along with some additional information.

   in 2016, researchers worked on finding alternative network
   configurations to tackle the aforementioned issues of scale and
   localisation. deeplab^[68][52] is one such example of this which
   achieves encouraging results for semantic image segmentation tasks.
   khoreva et al. (2016)^[69][53] build on deeplab   s earlier work (circa
   2015) and propose a weakly supervised training method which achieves
   comparable results to fully supervised networks.

   id161 further refined the network sharing of useful
   information approach through the use of end-to-end networks, which
   reduce the computational requirements of multiple omni-directional
   subtasks for classification. two key papers using this approach are:

     * 100 layers tiramisu^[70][54] is a fully-convolutional densenet
       which connects every layer, to every other layer, in a feed-forward
       fashion. it also achieves sota on multiple benchmark datasets with
       fewer parameters and training/processing.

     * fully convolutional instance-aware semantic
       segmentation^[71][55] performs instance mask prediction and
       classification jointly (two subtasks).
       coco segmentation challenge winner msra. 37.3% ap.
       9.1% absolute jump from msravc in 2015 in coco challenge.

   while enet,^[72][56] a dnn architecture for real-time semantic
   segmentation, is not of this category, it does demonstrate the
   commercial merits of reducing computation costs and giving greater
   access to mobile devices.

   our work wishes to relate as much of these advancements back to
   tangible public applications as possible. with this in mind, the
   following contains some of the most interesting healthcare
   application of segmentation in 2016;

     * [73]a benchmark for endoluminal scene segmentation of colonoscopy
       images^[74][57]
     * [75]3d fully convolutional networks for subcortical segmentation in
       mri: a large-scale study^[76][58]
     * [77]semi-supervised learning using denoising autoencoders for brain
       lesion detection and segmentation^[78][59]
     * [79]3d ultrasound image segmentation: a survey^[80][60]
     * [81]a fully convolutional neural network based structured
       prediction approach towards the retinal vessel
       segmentation^[82][61]
     * [83]3-d convolutional neural networks for glioblastoma
       segmentation^[84][62]

   one of our favourite quasi-medical segmentation applications is
   fusionnet^[85][63]- a deep fully residual convolutional neural network
   for image segmentation in connectomics^[86][64] benchmarked against
   sota electron microscopy (em) segmentation methods.

   iframe: [87]https://www.youtube.com/embed/pnzq4pnzszc

super-resolution, style transfer & colourisation

   not all research in id161 serves to extend the
   pseudo-cognitive abilities of machines, and often the fabled
   malleability of neural networks, as well as other ml techniques, lend
   themselves to a variety of other novel applications that spill into the
   public space. last year   s advancements in super-resolution, style
   transfer & colourisation occupied that space for us.

   super-resolution refers to the process of estimating a high resolution
   image from a low resolution counterpart, and also the prediction of
   image features at different magnifications, something which the human
   brain can do almost effortlessly. originally super-resolution was
   performed by simple techniques like bicubic-interpolation and nearest
   neighbours. in terms of commercial applications, the desire to overcome
   low-resolution constraints id30 from source quality and realisation
   of    csi miami    style image enhancement has driven research in the
   field. here are some of the year   s advances and their potential impact:

     * neural enhance^[88][65] is the brainchild of alex j. champandard
       and combines approaches from four different research papers to
       achieve its super-resolution method.
     * real-time video super resolution was also attempted in 2016 in two
       notable instances.^[89][66],^[90][67]
     * raisr: rapid and accurate image super-resolution^[91][68] from
       google avoids the costly memory and speed requirements of neural
       network approaches by training filters with low-resolution and
       high-resolution image pairs. raisr, as a learning-based framework,
       is two orders of magnitude faster than competing algorithms and has
       minimal memory requirements when compared with neural network-based
       approaches. hence super-resolution is extendable to personal
       devices. there is a research blog available [92]here.^[93][69]

   figure 7: super-resolution srgan example

   note: from left to right: bicubic interpolation (the objective worst
   performer for focus), deep residual network optimised for mse, deep
   residual generative adversarial network optimized for a loss more
   sensitive to human perception, original high resolution (hr) image.
   corresponding peak signal to noise ratio (psnr) and structural
   similarity (ssim) are shown in two brackets. [4 x upscaling] the reader
   may wish to zoom in on the middle two images (srresnet and srgan) to
   see the difference between image smoothness vs more realistic fine
   details.
   source: ledig et al. (2017)^[94][70]

   the use of id3 (gans) represent current
   sota for super-resolution:
     * srgan^[95][71] provides photo-realistic textures from heavily
       downsampled images on public benchmarks, using a discriminator
       network trained to differentiate between super-resolved and
       original photo-realistic images.

   qualitatively srgan performs the best, although srresnet performs best
   with peak-signal-to-noise-ratio (psnr) metric but srgan gets the finer
   texture details and achieves the best mean opinion score (mos).    to our
   knowledge, it is the first framework capable of inferring
   photo-realistic natural images for 4   upscaling factors.   ^[96][72] all
   previous approaches fail to recover the finer texture details at large
   upscaling factors.

     * amortised map id136 for image
       super-resolution^[97][73] proposes a method for calculation of
       maximum a posteriori (map) id136 using a convolutional neural
       network. however, their research presents three approaches for
       optimisation, all of which gans perform markedly better on real
       image data at present.

   figure 8: style transfer from nikulin & novakle

   note: transferring different styles to a photo of a cat (original top
   left).
   source: nikulin & novak (2016)

   undoubtedly, style transfer epitomises a novel use of neural networks
   that has ebbed into the public domain, specifically through last year   s
   facebook integrations and companies like prisma^[98][74] and
   artomatix^[99][75]. style transfer is an older technique but converted
   to a neural networks in 2015 with the publication of a neural algorithm
   of artistic style.^[100][76] since then, the concept of style transfer
   was expanded upon by nikulin and novak^[101][77] and also applied to
   video,^[102][78] as is the common progression within id161.

   figure 9: further examples of style transfer

   note: the top row (left to right) represent the artistic style which is
   transposed onto the original images which are displayed in the first
   column (woman, golden gate bridge and meadow environment). using
   conditional instance normalisation a single style transfer network can
   capture 32 style simultaneously, five of which are displayed here. the
   full suite of images in available in the source paper   s appendix. this
   work will feature in the international conference on learning
   representations (iclr) 2017.
   source: dumoulin et al. (2017, p. 2)^[103][79]
   style transfer as a topic is fairly intuitive once visualised; take an
   image and imagine it with the stylistic features of a different image.
   for example, in the style of a famous painting or artist. this year
   facebook released caffe2go,^[104][80] their deep learning system which
   integrates into mobile devices. google also released some interesting
   work which sought to blend multiple styles to generate entirely unique
   image styles: research blog^[105][81] and full paper.^[106][82]
   besides mobile integrations, style transfer has applications in the
   creation of game assets. members of our team recently saw a
   presentation by the founder and cto of artomatix, eric risser, who
   discussed the technique   s novel application for content generation in
   games (texture mutation, etc.) and, therefore, dramatically minimises
   the work of a conventional texture artist.

   colourisation is the process of changing monochrome images to new
   full-colour versions. originally this was done manually by people who
   painstakingly selected colours to represent specific pixels in each
   image. in 2016, it became possible to automate this process while
   maintaining the appearance of realism indicative of the human-centric
   colourisation process. while humans may not accurately represent the
   true colours of a given scene, their real world knowledge allows the
   application of colours in a way which is consistent with the image and
   another person viewing said image.
   the process of colourisation is interesting in that the network assigns
   the most likely colouring for images based on its understanding of
   object location, textures and environment, e.g. it learns that skin is
   pinkish and the sky is blueish.
   three of the most influential works of the year are as follows:
     * zhang et al.^[107][83] produced a method that was able to
       successfully fool humans on 32% of their trials. their methodology
       is comparable to a    colourisation turing test.   

     * larsson et al.^[108][84] fully automate their image colourisation
       system using deep learning for histogram estimation.

     * finally, lizuka, simo-serra and ishikawa^[109][85] demonstrate a
       colourisation model also based upon id98s. the work outperformed the
       existing sota, we [the team] feel as though this work is
       qualitatively best also, appearing to be the most realistic. figure
       10 provides comparisons, however the image is taken from lizuka et
       al.

   figure 10: comparison of colourisation research

   note: from top to bottom -  column one contains the original monochrome
   image input which is subsequently colourised through various
   techniques. the remaining columns display the results generated by
   other prominent colourisation research in 2016. when viewed from left
   to right, these are larsson et al. 84  2016 (column two), zhang et al.
   83 2016 (column three), and lizuka, simo-serra and ishikawa. 85  2016,
   also referred to as    ours    by the authors (column four). the quality
   difference in colourisation is most evident in row three (from the top)
   which depicts a group of young boys. we believe lizuka et al.   s work to
   be qualitatively superior (column four).

   source: lizuka et al. 2016^[110][86]

      furthermore, our architecture can process images of any resolution,
   unlike most existing approaches based on id98.   
   in a test to see how natural their colourisation was, users were given
   a random image from their models and were asked, "does this image look
   natural to you?"
   their approach achieved 92.6%, the baseline achieved roughly 70% and
   the ground truth (the actual colour photos) were considered 97.7% of
   the time to be natural.

action recognition

   the task of action recognition refers to the both the classification of
   an action within a given video frame, and more recently, algorithms
   which can predict the likely outcomes of interactions given only a few
   frames before the action takes place. in this respect we see recent
   research attempt to imbed context into algorithmic decisions, similar
   to other areas of id161. some key papers in this space are:

     * long-term temporal convolutions for action
       recognition^[111][87] leverages the spatio-temporal structure of
       human actions, i.e. the particular movement and duration, to
       correctly recognise actions using a id98 variant. to overcome the
       sub-optimal temporal modelling of longer term actions by id98s, the
       authors propose a neural network with long-term temporal
       convolutions (ltc-id98) to improve the accuracy of action
       recognition. put simply, the ltcs can look at larger parts of the
       video to recognise actions. their approach uses and extends 3d id98s
          to enable action representation at a fuller temporal scale   .

      we report state-of-the-art results on two challenging benchmarks for
   human action recognition ucf101 (92.7%) and hmdb51 (67.2%).   

     * spatiotemporal residual networks for video action
       recognition^[112][88] apply a variation of two stream id98 to the
       task of action recognition, which combines techniques from both
       traditional id98 approaches and recently popularised residual
       networks (resnets). the two stream approach takes its inspiration
       from a neuroscientific hypothesis on the functioning of the visual
       cortex, i.e. separate pathways recognise object shape/colour and
       movement. the authors combine the classification benefits of
       resnets by injecting residual connections between the two id98
       streams.

      each stream initially performs video recognition on its own and for
   final classification, softmax scores are combined by late fusion. to
   date, this approach is the most effective approach of applying deep
   learning to action recognition, especially with limited training data.
   in our work we directly convert image convnets into 3d architectures
   and show greatly improved performance over the two-stream baseline.    -
   94% on ucf101 and 70.6% on hmdb51. feichtenhofer et al. made
   improvements over traditional improved dense trajectory (idt) methods
   and generated better results through use of both techniques.

     * anticipating visual representations from unlabeled
       video^[113][89] is an interesting paper, although not strictly
       action classification. the program predicts the action which is
       likely to take place given a sequence of video frames up to one
       second before an action. the approach uses visual representations
       rather than pixel-by-pixel classification, which means that the
       program can operate without labeled data, by taking advantage of
       the id171 properties of deep neural networks.^[114][90]

   "the key idea behind our approach is that we can train deep networks to
   predict the visual representation of images in the future. visual
   representations are a promising prediction target because they encode
   images at a higher semantic level than pixels yet are automatic to
   compute. we then apply recognition algorithms on our predicted
   representation to anticipate objects and actions".
     * the organisers of the thumos action recognition
       challenge^[115][91] released a paper describing the general
       approaches for action recognition from the last number of years.
       the paper also provides a rundown of the challenges from 2013-2015,
       future directions for the challenge and ideas on how to give
       computers a more holistic understanding of video through action
       recognition. we hope that the thumos action recognition challenge
       returns in 2017 after its (seemingly) unexpected hiatus.

part three: toward a 3d understanding of the world

      a key goal of id161 is to recover the underlying 3d
   structure from 2d observations of the world.    - rezende et al. (2016,
   p. 1)^[116][92]

   in id161, the classification of scenes, objects and
   activities, along with the output of bounding boxes and image
   segmentation is, as we have seen, the focus of much new research. in
   essence, these approaches apply computation to gain an    understanding   
   of the 2d space of an image. however, detractors note that a 3d
   understanding is imperative for systems to successfully interpret, and
   navigate, the real world.

   for instance, a network may locate a cat in an image, colour all of its
   pixels and classify it as a cat. but does the network fully understand
   where the cat in the image is, in the context of the cat   s environment?

   one could argue that the computer learns very little about the 3d world
   from the above tasks. contrary to this, humans understand the world in
   3d even when examining 2d pictures, i.e. perspective, occlusion, depth,
   how objects in a scene are related, etc. imparting these 3d
   representations and their associated knowledge to artificial systems
   represents one of the next great frontiers of id161. a major
   reason for thinking this is that, generally;

      the 2d projection of a scene is a complex function of the attributes
   and positions of the camera, lights and objects that make up the scene.
   if endowed with 3d understanding, agents can abstract away from this
   complexity to form stable, disentangled representations, e.g.,
   recognizing that a chair is a chair whether seen from above or from the
   side, under different lighting conditions, or under partial
   occlusion.   ^[117][93]

   however, 3d understanding has traditionally faced several impediments.
   the first concerns the problem of both    self and normal
   occlusion    along with the numerous 3d shapes which fit a given 2d
   representation. understanding problems are further compounded by the
   inability to map different images of the same structures to the same 3d
   space, and in the handling of the multi-modality of these
   representations.^[118][94] finally, ground-truth 3d datasets were
   traditionally quite expensive and difficult to obtain which, when
   coupled with divergent approaches for representing 3d structures, may
   have led to training limitations.

   we feel that the work being conducted in this space is important to be
   mindful of. from the embryonic, albeit titillating early theoretical
   applications for future agi systems and robotics, to the immersive,
   captivating applications in augmented, virtual and mixed reality which
   will affect our societies in the near future. we cautiously predict
   exponential growth in this area of id161, as a result of
   lucrative commercial applications, which means that soon computers may
   start reasoning about the world rather than just about pixels.

3d objects

   this first section is a tad scattered, acting as a catch-all for
   computation applied to objects represented with 3d data, id136 of
   3d object shape from 2d images and pose estimation; determining the
   transformation of an object   s 3d pose from 2d images.^[119][95] the
   process of reconstruction also creeps in ahead of the following section
   which deals with it explicitly. however, with these points in mind, we
   present the work which excited our team the most in this general area:

     * octnet: learning deep 3d representations at high
       resolutions^[120][96] continues the recent development of
       convolutional networks which operate on 3d data, or voxels (which
       are like 3d pixels), using 3d convolutions. octnet is    a novel 3d
       representation which makes deep learning with high-resolution
       inputs tractable   . the authors test octnet representations by
          analyzing the impact of resolution on several 3d tasks including
       3d object classification, orientation estimation and point cloud
       labeling.    the paper   s central contribution is its exploitation of
       sparsity in 3d input data which then enables much more efficient
       use of memory and computation.

     * objectnet3d: a large scale database for 3d object
       recognition^[121][97] - contributes a database for 3d object
       recognition, presenting 2d images and 3d shapes for 100 object
       categories.    objects in the images in our database [taken from
       id163] are aligned with the 3d shapes [taken from the shapenet
       repository], and the alignment provides both accurate 3d pose
       annotation and the closest 3d shape annotation for each 2d object.   
       baseline experiments are provided on: region proposal generation,
       2d id164, joint 2d detection and 3d object pose
       estimation, and image-based 3d shape retrieval.
     * 3d-r2n2: a unified approach for single and multi-view 3d object
       reconstruction^[122][98] - creates a reconstruction of an object
          in the form of a 3d occupancy grid using single or multiple images
       of object instance from arbitrary viewpoints.    mappings from images
       of objects to 3d shapes are learned using primarily synthetic data,
       and the network can train and test without requiring    any image
       annotations or object class labels   . the network comprises a
       2d-id98, a 3d convolutional lstm (an architecture newly created for
       purpose) and a 3d deconvolutional neural network. how these
       different components interact and are trained together
       end-to-end is a perfect illustration of the layering capable with
       neural networks.

   figure 11: example of 3d-r2n2 functionality

   screenshot 2017-03-07 18.08.04.png

   note: images taken from ebay (left) and an overview of the
   functionality of 3d-r2n2 (right).

   note from source: some sample images of the objects we [the authors]
   wish to reconstruct - notice that views are separated by a large
   baseline and objects    appearance shows little texture and/or are
   non-lambertian. (b) an overview of our proposed 3d-r2n2: the network
   takes a sequence of images (or just one image) from arbitrary
   (uncalibrated) viewpoints as input (in this example, 3 views of the
   armchair) and generates voxelized 3d reconstruction as an output. the
   reconstruction is incrementally refined as the network sees more views
   of the object.

   source: choy et al. (2016, p. 3)^[123][99]

   3d-r2n2 generates    rendered images and voxelized models    using shapenet
   models and facilitates 3d object reconstruction where structure from
   motion (sfm) and simultaneous localisation and mapping (slam)
   approaches typically fail:

      our extensive experimental analysis shows that our reconstruction
   framework i) outperforms the state-of-the-art methods for single view
   reconstruction, and ii) enables the 3d reconstruction of objects in
   situations when traditional sfm/slam methods fail.   

     * 3d shape induction from 2d views of multiple
       objects^[124][100] uses    projective generative adversarial
       networks    (prgans), which train a deep generative model allowing
       accurate representation of 3d shapes, with the discriminator only
       being shown 2d images. the projection module captures the 3d
       representations and converts them to 2d images before passing to
       the discriminator. through iterative training cycles the generator
       improves projections by improving the 3d voxel shapes it generates.

   figure 12: prgan architecture segment


   note from source: the prgan architecture for generating 2d images of
   shapes. a 3d voxel representation (323) and viewpoint are independently
   generated from the input z (201-d vector). the projection module
   renders the voxel shape from a given viewpoint (  ,   ) to create an
   image. the discriminator consists of 2d convolutional and pooling
   layers and aims to classify if the input image is generated or real.
   source: gadhelha et al. (2016, p. 3)^[125][101]

   in this way the id136 ability is learned through an unsupervised
   environment:

      the addition of a projection module allows us to infer the underlying
   3d shape distribution without using any 3d, viewpoint information, or
   annotation during the learning phase.    

   additionally, the internal representation of the shapes can be
   interpolated, meaning discrete commonalities in voxel shapes allow
   transformations from object to object, e.g. from car to aeroplane.

     * unsupervised learning of 3d structure from
       images^[126][102] presents a completely unsupervised, generative
       model which demonstrates    the feasibility of learning to infer 3d
       representations of the world    for the first time. in a nutshell the
       deepmind team present a model which    learns strong deep generative
       models of 3d structures, and recovers these structures from 3d and
       2d images via probabilistic id136   , meaning that inputs can be
       both 3d and 2d.

   deepmind   s strong generative model runs on both volumetric and
   mesh-based representations. the use of mesh-based representations with
   opengl allows more knowledge to be built in, e.g. how light affects the
   scene and the materials used.    using a 3d mesh-based representation and
   training with a fully-fledged black-box renderer in the loop enables
   learning of the interactions between an object   s colours, materials and
   textures, positions of lights, and of other objects.   ^[127][103]
   the models are of high quality, capture uncertainty and are amenable to
   probabilistic id136, allowing for applications in 3d generation and
   simulation. the team achieve the first quantitative benchmark for 3d
   density modelling on 3d mnist and shapenet. this approach demonstrates
   that models may be trained end-to-end unsupervised on 2d images,
   requiring no ground-truth 3d labels.

human pose estimation and keypoint detection

   human pose estimation attempts to find the orientation and
   configuration of human body parts. 2d human pose estimation, or
   keypoint detection, generally refers to localising body parts of humans
   e.g finding the 2d location of the knees, eyes, feet, etc.

   however, 3d pose estimation takes this even further by finding the
   orientation of the body parts in 3d space and then an optional step of
   shape estimation/modelling can be performed. there has been a
   tremendous amount of improvement across these sub-domains in the last
   few years.

   in terms of competitive evaluation    the coco 2016 keypoint challenge
   involves simultaneously detecting people and localizing their
   keypoints   .^[128][104] the european convention on id161
   (eccv)^[129][105] provides more extensive literature on these subjects,
   however we would like to highlight:

     * realtime multi-person 2d pose estimation using part affinity
       fields.^[130][106]

   this method set sota performance on the inaugural mscoco 2016 keypoints
   challenge with 60% average precision (ap) and won the best demo award
   at eccv, video: [131]video ^[132][107]

   iframe: [133]https://www.youtube.com/embed/pw6nzxewlgm

     * keep it smpl: automatic estimation of 3d human pose and shape from
       a single image.^[134][108] this method first predicts 2d body joint
       locations and then uses another model called smpl to create the 3d
       body shape mesh, which allows it to understand 3d aspects working
       from 2d pose estimation. the 3d mesh is capable of capturing both
       pose and shape, versus previous methods which could only find 2d
       human pose. the authors provide an excellent video analysis of
       their work here: [135]video ^[136][109]

   iframe: [137]https://www.youtube.com/embed/eunz2rjxgae

      we describe the first method to automatically estimate the 3d pose of
   the human body as well as its 3d shape from a single unconstrained
   image. we estimate a full 3d mesh and show that 2d joints alone carry a
   surprising amount of information about body shape. the problem is
   challenging because of the complexity of the human body, articulation,
   occlusion, clothing, lighting, and the inherent ambiguity in inferring
   3d from 2d   .^[138][110]

reconstruction

   as mentioned, a previous section presented some examples of
   reconstruction but with a general focus on objects, specifically their
   shape and pose. while some of this is technically reconstruction, the
   field itself comprises many different types of reconstruction, e.g.
   scene reconstruction, multi-view and single view reconstruction,
   structure from motion (sfm), slam, etc. furthermore, some
   reconstruction approaches leverage additional (and multiple) sensors
   and equipment, such as event or rgb-d cameras, and can often layer
   multiple techniques to drive progress.

   the result? whole scenes can be reconstructed non-rigidly and change
   spatio-temporally, e.g. a high-fidelity reconstruction of yourself, and
   your movements, updated in real-time.

   as identified previously, issues persist around the mapping of 2d
   images to 3d space. the following papers present a plethora of
   approaches to create high-fidelity, real-time reconstructions:
     * fusion4d: real-time performance capture of challenging
       scenes^[139][111] veers towards the domain of computer graphics,
       however the interplay between id161 and graphics cannot
       be overstated. the authors    approach uses rgb-d and segmentation as
       inputs to form a real-time, multi-view reconstruction which is
       outputted using voxels.

   figure 13: fusion4d examples from real-time feed

   note from source:    we present a new method for real-time high quality
   4d (i.e. spatio-temporally coherent) performance capture, allowing for
   incremental non-rigid reconstruction from noisy input from multiple
   rgbd cameras. our system demonstrates unprecedented reconstructions of
   challenging non-rigid sequences, at real-time rates, including robust
   handling of large frame-to-frame motions and topology changes.   

   source: dou et al. (2016, p. 1)^[140][112]
   fusion4d creates real-time, high fidelity voxel representations which
   have impressive applications in virtual reality, augmented reality and
   telepresence. this work from microsoft will likely revolutionise motion
   capture, possibly for live sports. an example of the technology in
   real-time use is available here: [141]video ^[142][113]

   iframe: [143]https://www.youtube.com/embed/2dkcj1yhyw4

   for an astounding example of telepresence/holoportation by microsoft,
   see here: [144]video ^[145][114]

   iframe: [146]https://www.youtube.com/embed/7d59o6cfam0

     * real-time 3d reconstruction and 6-dof tracking with an event
       camera^[147][115]  won best paper at the european convention on
       id161 (eccv) in 2016. the authors propose a novel
       algorithm capable of tracking 6d motion and various reconstructions
       in real-time using a single event camera.

   figure 14: examples of the real-time 3d reconstruction

   note from source: demonstrations in various settings of the different
   aspects of our joint estimation algorithm. (a) visualisation of the
   input event stream; (b) estimated gradient keyframes; (c) reconstructed
   intensity keyframes with super resolution and high dynamic range
   properties; (d) estimated depth maps; (e) semi-dense 3d point clouds.

   source: kim et al. (2016, p. 12)^[148][116]

   the event camera is gaining favour with researchers in id161
   due to its reduced latency, lower power consumption and higher dynamic
   range when compared to traditional cameras. instead of a sequence of
   frames outputted by a regular camera, the event camera outputs    a
   stream of asynchronous spikes, each with pixel location, sign and
   precise timing, indicating when individual pixels record a threshold
   log intensity change.   ^[149][117]

   for an explanation of event camera functionality, real-time 3d
   reconstruction and 6-dof tracking, see the paper   s accompanying video
   here: [150]video ^[151][118]

   iframe: [152]https://www.youtube.com/embed/yhlyhdmsw7w

   this approach is incredibly impressive when one considers the real-time
   image rendering and depth estimation involved using a single
   view-point:

      we propose a method which can perform real-time 3d reconstruction from
   a single hand-held event camera with no additional sensing, and works
   in unstructured scenes of which it has no prior knowledge.   

     * unsupervised id98 for single view depth estimation: geometry to the
       rescue^[153][119] proposes an unsupervised method for training a
       deep id98 for single view depth prediction with results comparable
       to sota using supervised methods. traditional deep id98 approaches
       for single view depth prediction require large amounts of manually
       labelled data, however unsupervised methods again demonstrate their
       value by removing this necessity. the authors achieve this    by
       training the network in a manner analogous to an autoencoder   ,
       using a stereo-rig.

other uncategorised 3d

     * im2cad^[154][120] describes the process of transferring an    image
       to cad model   , cad meaning computer-assisted design, which is a
       prominent method used to create 3d scenes for architectural
       depictions, engineering, product design and many other fields.

      given a single photo of a room and a large database of furniture cad
   models, our goal is to reconstruct a scene that is as similar as
   possible to the scene depicted in the photograph, and composed of
   objects drawn from the database.   

   the authors present an automatic system which    iteratively optimizes
   object placements and scales    to best match input from real images. the
   rendered scenes validate against the original images using metrics
   trained using deep id98s.

   figure 15: example of im2cad rendering bedroom scene

   note : left: input image. right: automatically created cad model from
   input.
   note from source: the reconstruction results. in each example the left
   image is the real input image and the right image is the rendered 3d
   cad model produced by im2cad.
   source: izadinia et al. (2016, p. 10) ^[155][121]

   why care about im2cad?
   the issue tackled by the authors is one of the first meaningful
   advancements on the techniques demonstrated by lawrence roberts in
   1963, which allowed id136 of a 3d scene from a photo using a
   known-object database, albeit in the very simple case of line drawings.

      while robert   s method was visionary, more than a half century of
   subsequent research in id161 has still not yet led to
   practical extensions of his approach that work reliably on realistic
   images and scenes.   

   the authors introduce a variant of the problem, aiming to reconstruct a
   high fidelity scene from a photo using    objects taken from a database
   of 3d object models    for reconstruction.

   the process behind im2cad is quite involved and includes:
     * a fully convolutional network that is trained end-to-end to find
       geometric features for room geometry estimation.
     * faster r-id98 for id164.
     * after finding the objects within the image, cad model alignment  is
       completed to find the closest models within the shapenet repository
       for the detected objects. for example, the type of chair, given
       shape and approximate 3d pose. each 3d model is rendered to 32
       viewpoints which are then compared with the bounding box generated
       in id164 using deep features ^[156][122].
     * object placement in the scene
     * finally scene optimization further refines the placement of the
       objects by optimizing the visual similarity between the camera
       views of the rendered scene and input image.

   again in this domain, shapenet proves invaluable:

      first, we leverage shapenet, which contains millions of 3d models of
   objects, including thousands of different chairs, tables, and other
   household items. this dataset is a game changer for 3d scene
   understanding research, and was key to enabling our work.   

     * learning motion patterns in videos^[157][123] proposes to solve the
       issue of determining object motion independent of camera
       movement using synthetic video sequences to teach the networks.
          the core of our approach is a fully convolutional network, which
       is learnt entirely from synthetic video sequences, and their
       ground-truth optical flow and motion segmentation.    the authors
       test their approach on the new moving object segmentation dataset
       called davis,^[158][124] as well as the berkeley motion
       segmentation dataset and achieve sota on both.

     * deep image homography estimation^[159][125] comes from the magic
       leap team, a secretive us startup working in id161 and
       mixed reality. the authors reclassify the task of homography
       estimation as    a learning problem    and present two deep id98s
       architectures which form    homographynet: a regression network which
       directly estimates the real-valued homography parameters, and a
       classification network which produces a distribution over quantized
       homographies.   

   the term homography comes from projective geometry and refers to a type
   of transformation that maps one plane to another.    estimating a 2d
   homography from a pair of images is a fundamental task in computer
   vision, and an essential part of monocular slam systems   .

   the authors also provide a method for producing a    seemingly infinite
   dataset   , from existing datasets of real images such as ms-coco, which
   offsets some of data requirements of deeper networks. they manage to
   create    a nearly unlimited number of labeled training examples by
   applying random projective transformations to a large image dataset   .
     * gvnn: neural network library for geometric computer
       vision^[160][126] introduces a new neural network library for
       torch, a popular computing framework for machine learning. gvnn
       aims to    bridge the gap between classic geometric id161
       and deep learning   . the gvnn library allows developers to add
       geometric capabilities to their existing networks and training
       methods.

      in this work, we build upon the 2d transformation layers originally
   proposed in the spatial transformer networks and provide various novel
   extensions that perform geometric transformations which are often used
   in geometric id161.   

   "this opens up applications in learning invariance to 3d geometric
   transformation for place recognition, end-to-end visual odometry, depth
   estimation and unsupervised learning through warping with a parametric
   transformation for image reconstruction error."

3d summation and slam

   throughout this section we cut a swath across the field of 3d
   understanding, focusing primarily on the areas of pose estimation,
   reconstruction, depth estimation and homography. but there is
   considerably more superb work which will go unmentioned by us,
   constrained as we are by volume. and so, we hope to have provided the
   reader with a valuable starting point, which is to say by no means an
   absolute.

   a large portion of the highlighted work may be classified under
   geometric vision, which generally deals with measuring real-world
   quantities like distances, shapes, areas and volumes directly from
   images. our heuristic is that recognition-based tasks focus more on
   higher level semantic information than typically concerns applications
   in geometric vision. however, often we find that much of these
   different areas of 3d understanding are inextricably linked.

   one of the largest geometric problems is that of simultaneous
   localisation and mapping (slam), with researchers considering whether
   slam will be in the next problems tackled by deep learning. skeptics of
   the so-called    universality    of deep learning, of which there are many,
   point to the importance and functionality of slam as an algorithm:

      visual slam algorithms are able to simultaneously build 3d maps of the
   world while tracking the location and orientation of the
   camera.   ^[161][127] the geometric estimation portion of the slam
   approach is not currently suited to deep learning approaches and
   end-to-end learning remains unlikely. slam represents one of the most
   important algorithms in robotics and was designed with large input from
   the id161 field. the technique has found its home in
   applications like google maps, autonomous vehicles, ar devices like
   google tango^[162][128] and even the mars rover.

   that being said, tomasz malisiewicz delivers the anecdotal aggregate
   opinion of some prominent researchers on the issue, who agree    that
   semantics are necessary to build bigger and better slam
   systems.   ^[163][129] this potentially shows promise for future
   applications of deep learning in the slam domain.

   we reached out to mark cummins, co-founder of plink and pointy, who
   provided us with his thoughts on the issue. mark completed his phd on
   slam techniques:

      the core geometric estimation part of slam is pretty well solved by
   the current approaches, but the high-level semantics and the
   lower-level system components can all benefit from deep learning. in
   particular:
     * deep learning can greatly improve the quality of map semantics -
       i.e. going beyond poses or point clouds to a full understanding of
       the different kind of objects or regions in the map. this is much
       more powerful for many applications, and can also help with general
       robustness (for example through better handling of dynamic objects
       and environmental changes).

     * at a lower level, many components can likely be improved via deep
       learning. obvious candidates are place recognition / loop closure
       detection / relocalization, better point descriptors for sparse
       slam methods, etc

   overall the structure of slam solvers probably remains the same, but
   the components improve. it is possible to imagine doing something
   radically new with deep learning, like throwing away the geometry
   entirely and have a more recognition-based navigation system. but for
   systems where the goal is a precise geometric map, deep learning in
   slam is likely more about improving components than doing something
   completely new.    

   in summation, we believe that slam is not likely to be
   completely replaced by deep learning. however, it is entirely likely
   that the two approaches may become complements to each other going
   forward. if you wish to learn more about slam, and its current sota, we
   wholeheartedly recommend tomasz malisiewicz   s blog for that task:
   [164]the future of real-time slam and deep learning vs slam^[165][130]

   update: jan-may 2018. what we said earlier might be slightly
   contradicted by some recent research, for example:
     * [166]learning monocular visual odometry with dense 3d mapping from
       dense 3d flow
     * [167]active neural localization
     * [168]global pose estimation with an attention-based recurrent
       network
     * [169]learning to navigate in cities without a map

part four: convnet architectures, datasets, ungroupable extras

convnet architectures

   convnet architectures have recently found many novel applications
   outside of id161, some of which will feature in our
   forthcoming publications. however, they continue to feature prominently
   in id161, with architectural advancements providing
   improvements in speed, accuracy and training for many of the
   aforementioned applications and tasks in this paper.
   for this reason, convnet architectures are of fundamental importance to
   id161 as a whole. the following features some noteworthy
   convnet architectures from 2016, many of which take inspiration from
   the recent success of resnets.
     * inception-v4, inception-resnet and the impact of residual
       connections on learning^[170][131] - present inception v4, a new
       inception architecture which builds on the inception v2 and v3 from
       the end of 2015.^[171][132] the paper also provides an analysis of
       using residual connections for training inception networks along
       with some residual-inception hybrid networks.

     * densely connected convolutional networks^[172][133] or    densenets   
       take direct inspiration from the identity/skip connections of
       resnets. the approach extends this concept to convnets by having
       each layer connect to every other layer in a feed forward fashion,
       sharing feature maps from previous layers as inputs, thus creating
       densenets.

      densenets have several compelling advantages: they alleviate the
   vanishing-gradient problem, strengthen feature propagation, encourage
   feature reuse, and substantially reduce the number of
   parameters   .^[173][134]

   figure 16: example of densenet architecture

   note: a 5-layer dense block with a growth rate of k = 4. each layer
   takes all preceding feature-maps as input.

   source: huang et al. (2016)^[174][135]

   the model was evaluated on cifar-10, cifar-100, svhn and id163; it
   achieved sota on a number of them. impressively, densenets achieve
   these results while using less memory and with reduced computational
   requirements. there are multiple implementations (keras, tensorflow,
   etc) [175]here.^[176][136]

     * fractalnet ultra-deep neural networks without
       residuals^[177][137] - utilises interacting subpaths of different
       lengths, without pass-through or residual connections, instead
       altering internal signals using filter and nonlinearities for
       transformations.

      fractalnets repeatedly combine several parallel layer sequences with
   different numbers of convolutional blocks to obtain a large nominal
   depth, while maintaining many short paths in the network   .^[178][138]

   the network achieved sota performance on cifar and id163, while
   demonstrating some additional properties. for instance, they call into
   question the role of residuals in the success of extremely deep
   convnets, while also providing insight into the nature of answers
   attained by various subnetwork depths.

     * lets keep it simple: using simple architectures to outperform
       deeper architectures^[179][139] focuses on creating a simplified
       mother architecture. the architecture achieved sota results, or
       parity with existing approaches, on    datasets such as cifar10/100,
       mnist and svhn with simple or no data-augmentation   . we feel their
       exact words provide the best description of the motivation here:

      in this work, we present a very simple fully convolutional network
   architecture of 13 layers, with minimum reliance on new features which
   outperforms almost all deeper architectures with 2 to 25 times fewer
   parameters. our architecture can be a very good candidate for many
   scenarios, especially for use in embedded devices.   

      it can be furthermore compressed using methods such as deepcompression
   and thus its memory consumption can be decreased drastically. we
   intentionally tried to create a mother architecture with minimum
   reliance on new features proposed recently, to show the effectiveness
   of a well-crafted yet simple convolutional architecture which can then
   later be enhanced with existing or new methods presented in the
   literature.   ^[180][140]

   here are some additional techniques which complement convnet
   architectures:

     * swapout: learning an ensemble of deep
       architectures^[181][141] generalises dropout and stochastic depth
       methods to prevent co-adaptation of units, both in a specific layer
       and across network layers. the ensemble training method samples
       from multiple architectures including    dropout, stochastic depth
       and residual architectures   . swapout outperforms resnets of
       identical network structure on the cifar-10 and cifar-100 and can
       be classified as a regularisation technique.

     * squeezenet^[182][142] posits that smaller dnns offer various
       benefits, from less computationally taxing training to easier
       information transmission to, and operation on, devices with limited
       storage or processing power. squeezenet is a small dnn architecture
       which achieves    alexnet-level accuracy with significantly reduced
       parameters and memory requirements using model compression
       techniques which make it 510x smaller than alexnet.   

   a rectified linear unit (relu) is traditionally the dominant activation
   function for all neural networks. however, here are some recent
   alternatives:
     * concatenated rectified linear units (crelu)^[183][143]
     * exponential linear units (elus)^[184][144] from the close of 2015
     * parametric exponential linear unit (pelu)^[185][145]

   moving towards equivariance in convnets

   convnets are translation invariant - meaning they can identify the same
   features in multiple parts of an image. however, the typical id98 isn   t
   rotation invariant - meaning that if a feature or the whole image is
   rotated then the network   s performance suffers. usually convnets learn
   to (sort of) deal with rotation invariance through data augmentation
   (e.g. purposefully rotating the images by small random amounts during
   training). this means the network gains slight rotation invariant
   properties without specifically designing rotation invariance into the
   network. this means that rotation invariance is fundamentally limited
   in networks using current techniques. this is an interesting parallel
   with humans who also typically fare worse at recognising characters
   upside down, although there is no reason for machines to suffer this
   limitation.

   the following papers tackle rotation-invariant convnets. while each
   approach has novelties, they all improve rotation invariance through
   more efficient parameter usage leading to eventual global rotation
   equivariance:
     * harmonic id98s^[186][146] replace regular id98 filters with    circular
       harmonics   .
     * group equivariant convolutional networks (g-id98s)^[187][147] uses
       g-convolutions, which are a new type of layer that    enjoys a
       substantially higher degree of weight sharing than regular
       convolution layers and increases the expressive capacity of the
       network without increasing the number of parameters.   
     * exploiting cyclic symmetry in convolutional neural
       networks^[188][148] presents four operations as layers which
       augment neural network layers to partially increase rotational
       equivariance.
     * steerable id98s^[189][149] - cohen and welling build on the work
       they did with g-id98s, demonstrating that    steerable architectures   
       outperform residual and dense networks on the cifars. they also
       provide a succinct overview of the invariance problem:

      to improve the statistical efficiency of machine learning methods,
   many have sought to learn invariant representations. in deep learning,
   however, intermediate layers should not be fully invariant, because the
   relative pose of local features must be preserved for further layers.
   thus, one is led to the idea of equivariance: a network is equivariant
   if the representations it produces transform in a predictable linear
   manner under transformations of the input. in other words, equivariant
   networks produce representations that are steerable. steerability makes
   it possible to apply filters not just in every position (as in a
   standard convolution layer), but in every pose, thus allowing for
   increased parameter sharing.   107

   residual networks

   figure 17: test-error rates on cifar datasets

   screenshot 2017-03-07 15.37.19.png

   note: yellow highlight indicates that these papers feature within this
   piece. pre-resnet refers to "identity mappings in deep residual
   networks" (see following section). furthermore, while not included in
   the table we believe that    learning identity mappings with residual
   gates    produced some of the lowest error rates of 2016 with 3.65% and
   18.27% on cifar-10 and cifar-100, respectively.

   source: abdi and nahavandi (2016, p. 6)^[190][150]

   residual networks and their variants became incredibly popular in 2016,
   following the success of microsoft   s resnet,^[191][151] with many open
   source versions and pre-trained models now available. in 2015, resnet
   won 1st place in id163   s detection, localisation and classification
   tasks as well as in coco   s detection and segmentation
   challenges. although questions still abound about depth, resnets
   tackling of the vanishing gradient problem provided more impetus for
   the    increased depth produces superior abstraction    philosophy which
   underpins much of deep learning at present.

   resnets are often conceptualised as an ensemble of shallower networks,
   which somewhat counteract the hierarchical nature of deep neural
   networks (dnns) by running shortcut connections parallel to their
   convolutional layers. these shortcuts or skip connections mitigate
   vanishing/exploding gradient problems associated with dnns, by allowing
   easier back-propagation of gradients throughout the network layers. for
   more information there is a quora thread available
   [192]here.^[193][152]

   residual learning, theory and improvements

     * wide residual networks^[194][153] is now an extremely common resnet
       approach. the authors conduct an experimental study on the
       architecture of resnet blocks, and improve residual network
       performance by increasing the width and reducing the depth of the
       networks, which mitigates the diminishing feature reuse problem.
       this approach produces new sota on multiple benchmarks including
       3.89% and 18.3% on cifar-10 and cifar-100 respectively. the authors
       show that a    16-layer-deep wide resnet performs as well or better
       in accuracy and efficiency than many other resnets (including 1000
       layer networks)   .

     * deep networks with stochastic depth^[195][154] essentially applies
       dropout to whole layers of neurons instead of to bunches of
       individual neurons.    we start with very deep networks but during
       training, for each mini-batch, randomly drop a subset of layers and
       bypass them with the identity function.    stochastic depth allows
       quicker training and better accuracy even when training networks
       greater than 1200 layers.

     * learning identity mappings with residual gates^[196][155] -    by
       using a scalar parameter to control each gate, we provide a way to
       learn identity mappings by optimizing only one parameter.    the
       authors use these gated resnets to improve the optimisation of deep
       models, while providing    high tolerance to full layer removal    such
       that 90% of performance remains following significant removal at
       random. using wide gated resnets the model achieves 3.65% and
       18.27% error on cifar- 10 and cifar-100, respectively.

     * residual networks behave like ensembles of relatively shallow
       networks^[197][156] - resnets can be viewed as collections of many
       paths, which don   t strongly depend upon one another and hence
       reinforce the notion of ensemble behaviour. furthermore, residual
       pathways vary in length with the short paths contributing to
       gradient during training while the deeper paths don   t factor in
       this stage.

     * identity mappings in deep residual networks^[198][157] comes as an
       improvement from the original resnet authors: kaiming he, xiangyu
       zhang, shaoqing ren, jian sun. identity mappings are shown to allow
          forward and backward signals to be propagated between any resnet
       block when used as the skip connections and after-addition
       activation   . the approach improves generalisation, training and
       results    using a 1001-layer resnet on cifar-10 (4.62% error) and
       cifar-100, and a 200-layer resnet on id163.   

     * multi-residual networks: improving the speed and accuracy of
       residual networks^[199][158] again advocates for the ensemble
       behaviour of resnets and favours a wider-over-deeper approach to
       resnet architecture.    the proposed multi-residual network increases
       the number of residual functions in the residual blocks.    improved
       accuracy produces 3.73% and 19.45% error on cifar-10 and cifar-100,
       respectively. the table presented in fig. 17 was taken from this
       paper, and more up-to-date versions are available which consider
       the work produced in 2017 thus far.

   other residual theory and improvements
   although a relatively recent idea, there is quite a considerable body
   of work being created around resnets presently. the following
   represents some additional theories and improvements which we wished to
   highlight for interested readers:

     * [200]highway and residual networks learn unrolled iterative
       estimation^[201][159]
     * [202]residual networks of residual networks: multilevel residual
       networks^[203][160]
     * [204]resnet in resnet: generalizing residual
       architectures^[205][161]
     * [206]wider or deeper: revisiting the resnet model for visual
       recognition^[207][162]
     * [208]bridging the gaps between residual learning, recurrent neural
       networks and visual cortex^[209][163]

     * [210]convolutional residual memory networks^[211][164]
     * [212]identity matters in deep learning^[213][165]
     * [214]deep residual networks with exponential linear unit^[215][166]
     * [216]weighted residuals for very deep networks^[217][167]

datasets

   the significance of rich datasets for all facets of machine learning
   cannot be overstated. hence, we feel it is prudent to include some of
   the largest advancements in this domain. to paraphrase ben hamner, the
   cto and co-founder of kaggle,    a new dataset can make a thousand papers
   flourish   ,^[218][168] that is to say the availability of data can
   promote new approaches, as well as breath new life into previously
   ineffectual techniques.

   in 2016, traditional datasets such as id163^[219][169], common
   objects in context (coco)^[220][170], the cifars^[221][171] and
   mnist^[222][172] were joined by a host of new entries. we also noted
   the rise of synthetic datasets spurred on by progress in graphics.
   synthetic datasets are an interesting work-around of the large data
   requirements for id158s (anns). in the interest of
   brevity, we have selected our (subjective) most important new datasets
   for 2016:

     * places2^[223][173] is a scene classification dataset, i.e. the task
       is to label an image with a scene class like    stadium   ,    park   ,
       etc. while prediction models and image understanding will
       undoubtedly be improved by the places2 dataset, an interesting
       finding from networks that are trained on this dataset is that in
       the process of learning to classify scenes, the network learns to
       detect objects in them without ever being explicitly taught this.
       for example, that bedrooms contain beds and that sinks can be in
       both kitchens and bathrooms. this means that the objects
       themselves are lower level features in the abstraction hierarchy
       for the classification of scenes.

   figure 18: examples from scenenet rgb-d

   note: examples taken from scenenet rgb-d, a dataset with 5m
   photorealistic images of synthetic indoor trajectories with ground
   truth. the photo (a) is rendered through computer graphics with
   available ground truth for specific tasks from (b) to (e). creation of
   synthetic datasets should aid the process of id20.
   synthetic datasets are somewhat pointless if the knowledge learned from
   them cannot be applied to the real world. this is where domain
   adaptation comes in, which refers to this id21 process of
   moving knowledge from one domain to another, e.g. from synthetic to
   real-world environments. id20 has recently been improving
   very rapidly again highlighting the recent efforts in transfer
   learning. columns (c) vs (d) show the difference between instance and
   semantic/class segmentation.

   source: mccormac et al. (2017)^[224][174]

     * scenenet rgb-d^[225][175] - this synthetic dataset expands on the
       original scenenet dataset and provides pixel-perfect ground truth
       for scene understanding problems such as semantic segmentation,
       instance segmentation, and id164, and also for geometric
       id161 problems such as optical flow, depth estimation,
       camera pose estimation, and 3d reconstruction. the dataset
       granularizes the chosen environment by providing pixel-perfect
       representations.

     * cmplaces^[226][176] is a cross-modal scene dataset from mit. the
       task is to recognize scenes across many different modalities beyond
       natural images and in the process hopefully transfer that knowledge
       across modalities too. some of the modalities are: real, clip art,
       sketches, spatial text (words written which correspond to spatial
       locations of objects) and natural language descriptions. the paper
       also discusses methods for how to deal with this type of problem
       with cross-modal convolutional neural networks.

   figure 19: cmplaces cross-modal scene representations

   note: taken from the cmplaces paper showing two examples, bedrooms and
   kindergarten classrooms,   across different modalities. conventional
   neural network approaches learn representations that don   t transfer
   well across modalities and this paper attempts to generate a shared
   representation    agnostic of modality   .

   source: aytar et al. (2016)^[227][177]

   in cmplaces we see explicit mention of id21, domain
   invariant representations, id20 and id201,
   all of which serve to demonstrate further the current undertow of
   id161 research. the authors focus on trying to find
      domain/modality-independent representations   , which could correspond
   to the higher level abstractions where humans draw their unified
   representations from. for instance take    cat    across its various
   modalities, humans see the word    cat    in writing, a picture drawn in a
   sketchbook, a real world-image or mentioned in speech but we still have
   the same unified representation abstracted at a higher level above
   these modalities.

      humans are able to leverage knowledge and experiences independently of
   the modality they perceive it in, and a similar capability in machines
   would enable several important applications in retrieval and
   recognition   .

     * ms-celeb-1m^[228][178] contains images of one million celebrities
       with ten million training images in a training set for facial
       recognition.

     * open images^[229][179] comes courtesy of google inc. and comprises
       ~9 million urls to images complete with multiple labels, a vast
       improvement over typical single label images. open images spans
       6000 categories, a large improvement over the 1000 classes offered
       previously by id163 (with less focus on canines) and should
       prove indispensable to the machine learning community.

     * youtube-8m^[230][180] also comes courtesy of google with 8 million
       video urls, 500,000 hours of video, 4800 classes, avg. 1.8 labels
       per video. some examples of the labels are:    arts & entertainment   ,
          shopping    and    pets & animals   . video datasets are much more
       difficult to label and collect hence the massive value this dataset
       provides.

   that being said, advancements in image understanding, such as
   segmentation, object classification and detection have brought video
   understanding to the fore of research. however, prior to this dataset
   release there was a real lack in the variety and scale of real-world
   video datasets available. furthermore, this dataset was just recently
   updated,^[231][181] and this year in association with kaggle, google
   is organising a video understanding competition as part of cvpr
   2017.^[232][182]

   general information about youtube-8m: [233]here^[234][183]

ungroupable extras and interesting trends

   as this piece draws to a close, we lament the limitations under which
   we had to construct it. indeed, the field of id161 is too
   expansive to cover in any real, meaningful depth, and as such many
   omissions were made. one such omission is, unfortunately, almost
   everything that didn   t use neural networks. we know there is great work
   outside of nns, and we acknowledge our own biases, but we feel that the
   impetus lies with these approaches currently, and our subjective
   selection of material for inclusion was predominantly based on the
   reception received from the research community at large (and the
   results speak for themselves).

   we would also like to stress that there are hundreds of other papers in
   the above topics, and this amalgam of topics is not curated as a
   definitive, but rather hopes to encourage interested parties to read
   further along the entrances we provide. as such, this final section
   acts as a catch all for some of the other applications we loved, trends
   we wished to highlight and justifications we wanted to make to the
   reader.

   applications/use cases
     * applications for the blind from facebook^[235][184] and hardware
       from baidu.^[236][185]

     * emotion detection combines facial detection and semantic analysis,
       and is growing rapidly. there are 20+ apis currently
       available.^[237][186]
     * extracting roads from aerial imagery,^[238][187] land use
       classification from aerial maps and population density
       maps.^[239][188]

     * amazon go further raised the profile of id161 by
       demonstrating a queue-less shopping experience,^[240][189] although
       there remain some functional issues at present.^[241][190]

     * there is a huge volume of work being done for autonomous vehicles
       that we largely didn   t touch. however, for those wishing to delve
       into general market trends, there   s an excellent piece by moritz
       mueller-freitag of twenty billion neurons about the german auto
       industry and the impact of autonomous vehicles.^[242][191]

     * other interesting areas: id162/search,^[243][192] gesture
       recognition, inpainting and facial reconstruction.

     * there is considerable work around digital imaging and
       communications in medicine (dicom) and other medical applications,
       especially related to imaging. for instance, there have been (and
       still are) numerous kaggle detection competitions (lung cancer,
       cervical cancer), some with large monetary incentives, in which
       algorithms attempt to outperform specialists at the
       classification/detection tasks in question.

   however, while work continues on improving the error rates of these
   algorithms their value as a tool for medical practitioners appears
   increasingly evident. this is particularly striking when we consider
   the performance improvements in breast cancer detection achieved by
   combining ai systems^[244][193] with medical specialists.^[245][194] in
   this instance, robot-human symbiosis produces accuracy far greater than
   the sum of its parts at 99.5%.

   this is just one example of the torrent of medical applications
   currently being pursued by the deep learning/machine learning
   communities. some cynical members of our team jokingly make light of
   these attempts as a means to ingratiate society to the idea of ai
   research as a ubiquitous, benevolent force. but as long as the
   technology helps the healthcare industry, and it is introduced in a
   safe and considered manner, we wholeheartedly welcome such advances.

   hardware/markets
     * growing markets for robotic vision/machine vision (separate fields)
       and potential target markets for iot. a personal favourite of ours
       is the use of deep learning, a raspberry pi and tensorflow by a
       farmer   s son to sort cucumbers in japan based on unique producer
       heuristics for quality, e.g. shape, size and
       colour.^[246][195] this produced massive decreases in human-time
       spent by his mother sorting cucumbers.

     * the trend of shrinking compute requirements and migrating to mobile
       is evident,  but it   s also complemented by steep hardware
       acceleration. soon we   ll see pocket sized id98s and vision
       processing units (vpus) everywhere. for instance, the movidius
       myriad2 is used in google   s project tango and drones.^[247][196]

   iframe: [248]https://www.youtube.com/embed/hx0uelnrr1i

   the movidius fathom stick,^[249][197] which also uses the myriad2   s
   technology, allows users to add sota id161 performance to
   consumer devices. the fathom stick, which has the physical properties
   of a usb stick, brings the power of a neural network to almost any
   device: brains on a stick.

     * sensors and systems that use something other than visible light.
       examples include radar, thermographic cameras, hyperspectral
       imaging, sonar, magnetic resonance imaging, etc.

     * reduction in cost of lidar, which use light and radar to measure
       distances, and offer many advantages over normal rgb cameras. there
       are many lidar devices for currently less than $500.

     * hololens and the near-countless other augmented reality
       headsets^[250][198] entering the market.

     * project tango by google^[251][199] represents the next big
       commercialisation of slam. tango is an augmented reality computing
       platform, comprising both novel software and hardware. tango allows
       the detection of mobile device position, relative to the world,
       without the use of gps or other external information while
       simultaneously mapping the area around the device in 3d.

   corporate partners lenovo brought affordable tango enabled phones to
   market in 2016, allowing hundreds of developers to begin creating
   applications for the platform. tango employs the following
   software technologies: motion tracking, area learning, and depth
   perception.

   omissions based on forthcoming publications

   there is also considerable, and increasing overlap between computer
   vision techniques and other domains in machine learning and artificial
   intelligence. these other domains and hybrid use cases are the subject
   of the m tank   s forthcoming publications and, as with the whole of this
   piece, we partitioned content based on our own heuristics.

   for instance, we decided to place the two integral id161
   tasks, image captioning and visual id53, in our
   forthcoming nlp piece along with visual id103 because of
   the combination of cv and nlp involved. whereas the application of
   generative models to images we place in our work on generative models.
   examples included in these future works are:

     * lip reading: in 2016 we saw huge lip reading advancements in
       programs such as lipnet^[252][200], which combine id161
       and nlp into visual id103.

     * generative models applied to images will feature as part of our
       depiction of the violent* battle between the autoregressive models
       (pixelid56, pixelid98, bytenet, vpn, wavenet), generative adversarial
       networks (gans), id5 and, as you should expect
       by this stage, all of their variants, combinations and hybrids.

   *disclaimer: the team wishes to mention that they do not
   condone network on network (non) violence in any form and are
   sympathisers to the movement towards generative unadversarial networks
   (guns).^[253][201]

   in the final section, we   ll offer some concluding remarks and a
   recapitulation of some of the trends we identified. we would hope that
   we were comprehensive enough to show a bird   s-eye view of where the
   id161 field is loosely situated and where it is headed in the
   near-term. we also would like to draw particular attention to the fact
   that our work does not cover january-august 2017. the blistering pace
   of research output means that much of this work could be outdated
   already; we encourage readers to go and find out whether it is for
   themselves. but this rapid pace of growth also brings with it lucrative
   opportunities as the id161 hardware and software markets are
   expected to reach $48.6 billion by 2022.

   figure 20: id161 revenue by application market^[254][202]


   note: estimation of id161 revenue by application market
   spanning the period from 2015-2022. the largest growth is forecasted to
   come from applications within the automotive, consumer, robotics and
   machine vision sectors.
   source: tractica (2016)^[255][203]

conclusion

   in conclusion we   d like to highlight some of the trends and recurring
   themes that cropped up repeatedly throughout our research review
   process. first and foremost, we   d like to draw attention to the machine
   learning research community   s voracious pursuit of optimisation. this
   is most notable in the year on year changes in accuracy rates, but
   especially in the intra-year changes in accuracy. we   d like to
   underscore this point and return to it in a moment.
   error rates are not the only fanatically optimised parameter, with
   researchers working on improving speed, efficiency and even the
   algorithm   s ability to generalise to other tasks and problems in
   completely new ways. we are acutely aware of the research coming to the
   fore with approaches like id62, generative modelling,
   id21 and, as of recently, evolutionary learning, and we
   feel that these research principles are gradually exerting greater
   influence on the approaches of the best performing work.

   while this last point is unequivocally meant in commendation for,
   rather than denigration of, this trend, one can   t help but to cast
   their mind toward the (very) distant spectre of artificial general
   intelligence, whether merited a thought or not. far from being
   alarmist, we just wish to highlight to both experts and laypersons that
   this concern arises from here, from the startling progress that   s
   already evident in id161 and other ai subfields. properly
   articulated concerns from the public can only come through education
   about these advancements and their impacts in general. this may then in
   turn quell the power of media sentiment and misinformation in ai.

   we chose to focus on a one year timeline for two reasons. the first
   relates to the sheer volume of work being produced. even for people who
   follow the field very closely, it is becoming increasingly difficult to
   remain abreast of research as the number of publications grow
   exponentially. the second brings us back to our point on intra-year
   changes.

   in taking a single year snapshot of progress, the reader can begin to
   comprehend the pace of research at present. we see improvement after
   improvement in such short time spans, but why? researchers have
   cultivated a global community where building on previous approaches
   (architectures, meta-architectures, techniques, ideas, tips, wacky
   hacks, results, etc.), and infrastructures (libraries like keras,
   tensorflow and pytorch, gpus, etc.), is not only encouraged but also
   celebrated. a predominantly open source community with few parallels,
   which is continuously attracting new researchers and having its
   techniques reappropriated by fields like economics, physics and
   countless others.

   it   s important to understand for those who have yet to notice, that
   among the already frantic chorus of divergent voices proclaiming divine
   insight into the true nature of this technology, there is at least
   agreement; agreement that this technology will alter the world in new
   and exciting ways. however, much disagreement still comes over the
   timeline on which these alterations will unravel.

   until such a time as we can accurately model the progress of these
   developments we will continue to provide information to the best of our
   abilities. with this resource we hoped to cater to the spectrum of ai
   experience, from researchers playing catch-up to anyone who simply
   wishes to obtain a grounding in id161 and artificial
   intelligence. with this our project hopes to have added some value to
   the open source revolution that quietly hums beneath the technology of
   a lifetime.

   with thanks,

   the m tank
     __________________________________________________________________

references

   part one

   [256][1] british machine vision association (bmva). 2016. what is
   id161? [online] available at:
   [257]http://www.bmva.org/visionoverview [accessed 21/12/2016]

   [258][2] krizhevsky, a., sutskever, i. and hinton, g. e. 2012. id163
   classification with deep convolutional neural networks, nips 2012:
   neural information processing systems, lake tahoe, nevada. available:
   [259]http://www.cs.toronto.edu/~kriz/id163_classification_with_deep_
   convolutional.pdf

   [260][3] kuhn, t. s. 1962. the structure of scientific revolutions. 4th
   ed. united states: the university of chicago press.

   [261][4] karpathy, a. 2015. what a deep neural network thinks about
   your #selfie. [blog] andrej karpathy blog. available:
   [262]http://karpathy.github.io/2015/10/25/selfie/ [accessed:
   21/12/2016]

   [263][5] quora. 2016. what is a convolutional neural network?
   [online] available:
   [264]https://www.quora.com/what-is-a-convolutional-neural-network [acce
   ssed: 21/12/2016]

   [265][6] stanford university. 2016. convolutional neural networks for
   visual recognition. [online] cs231n. available:
   [266]http://cs231n.stanford.edu/ [accessed 21/12/2016]

   [267][7] goodfellow et al. 2016. deep learning. mit press.
   [online] [268]http://www.deeplearningbook.org/ [accessed: 21/12/2016]
   note: chapter 9, convolutional networks [available:
   [269]http://www.deeplearningbook.org/contents/convnets.html]

   [270][8] nielsen, m. 2017. neural networks and deep learning. [online]
   ebook. available:
   [271]http://neuralnetworksanddeeplearning.com/index.html  [accessed:
   06/03/2017].

   [272][9] id163 refers to a popular image dataset for computer
   vision. each year entrants compete in a series of different tasks
   called the id163 large scale visual recognition challenge (ilsvrc).
   available: [273]http://image-net.org/challenges/lsvrc/2016/index

   [274][10] see    what i learned from competing against a convnet on
   id163    by andrej karpathy. the blog post details the author   s
   journey to provide a human benchmark against the ilsvrc 2014 dataset.
   the error rate was approximately 5.1% versus a then state-of-the-art
   googlenet classification error of 6.8%. available:
   [275]http://karpathy.github.io/2014/09/02/what-i-learned-from-competing
   -against-a-convnet-on-id163/

   [276][11] see new datasets later in this piece.

   [277][12] keras is a popular neural network-based deep learning
   library: [278]https://keras.io/

   [279][13] chollet, f. 2016. information-theoretical label embeddings
   for large-scale image classification. [online] arxiv: 1607.05691.
   available: [280]arxiv:1607.05691v1

   [281][14] chollet, f. 2016. xception: deep learning with depthwise
   separable convolutions. [online] arxiv:1610.02357. available:
   [282]arxiv:1610.02357v2

   [283][15] places2 dataset, details available:
   [284]http://places2.csail.mit.edu/. see also new datasets section.

   [285][16] hikvision. 2016. hikvision ranked no.1 in scene
   classification at id163 2016 challenge. [online] security news desk.
   available:
   [286]http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classi
   fication-id163-2016-challenge/ [accessed: 20/03/2017].

   [287][17] see residual networks in part four of this publication for
   more details.

   [288][18] details available under team information trimps-soushen from:
   [289]http://image-net.org/challenges/lsvrc/2016/results

   [290][19] xie, s., girshick, r., dollar, p., tu, z. & he, k. 2016.
   aggregated residual transformations for deep neural networks.
   [online] arxiv: 1611.05431. available: [291]arxiv:1611.05431v1

   [292][20] id163 large scale visual recognition challenge (2016),
   part ii, available:
   [293]http://image-net.org/challenges/lsvrc/2016/ [accessed: 22/11/2016]

   [294][21] hu and ramanan. 2016. finding tiny faces. [online] arxiv:
   1612.04402. available: [295]arxiv:1612.04402v1

   [296][22] liu et al. 2016. ssd: single shot multibox detector. [online]
   arxiv: 1512.02325v5. available: [297]arxiv:1512.02325v5

   [298][23] redmon, j. farhadi, a. 2016. yolo9000: better, faster,
   stronger. [online] arxiv: 1612.08242v1. available:
   [299]arxiv:1612.08242v1

   [300][24] yolo stands for    you only look once   .

   [301][25] redmon et al. 2016. you only look once: unified, real-time
   id164. [online] arxiv: 1506.02640. available:
   [302]arxiv:1506.02640v5

   [303][26]redmon. 2017. yolo: real-time id164. [website]
   pjreddie.com. available:
   [304]https://pjreddie.com/darknet/yolo/ [accessed: 01/03/2017].

   [305][27] lin et al. 2016. feature pyramid networks for object
   detection. [online] arxiv: 1612.03144. available:
   [306]arxiv:1612.03144v1

   [307][28] facebook   s artificial intelligence research

   [308][29] common objects in context (coco) image dataset

   [309][30] dai et al. 2016. r-fcn: id164 via region-based
   fully convolutional networks. [online] arxiv: 1605.06409. available:
   [310]arxiv:1605.06409v2

   [311][31] huang et al. 2016. speed/accuracy trade-offs for modern
   convolutional object detectors. [online] arxiv: 1611.10012. available:
   [312]arxiv:1611.10012v1

   [313][32] ibid

   [314][33] wu et al. 2016. squeezedet: unified, small, low power fully
   convolutional neural networks for real-time id164 for
   autonomous driving. [online] arxiv: 1612.01051. available:
   [315]arxiv:1612.01051v2

   [316][34] hong et al. 2016. pvanet: lightweight deep neural networks
   for real-time id164. [online] arxiv: 1611.08588v2.
   available: [317]arxiv:1611.08588v2

   [318][35] deepglint official. 2016. deepglint cvpr2016. [online]
   youtube.com. available:
   [319]https://www.youtube.com/watch?v=xhp47v5obxq [accessed:
   01/03/2017].

   [320][36] coco - common objects in common. 2016. [website] available:
   [321]http://mscoco.org/ [accessed: 04/01/2017].

   [322][37] ilsrvc results taken from: id163. 2016. large scale visual
   recognition challenge 2016.

    [website] id164. available:
   [323]http://image-net.org/challenges/lsvrc/2016/results [accessed:
   04/01/2017].

   [324][38] coco detection challenge results taken from: coco - common
   objects in common. 2016. detections leaderboard [website] mscoco.org.
   available:
   [325]http://mscoco.org/dataset/#detections-leaderboard [accessed:
   05/01/2017].

   [326][39] id163. 2016. [online] workshop presentation, slide 31.
   available:
   [327]http://image-net.org/challenges/talks/2016/eccv2016_ilsvrc_coco_de
   tection_segmentation.pdf [accessed: 06/01/2017].

   [328][40] bertinetto et al. 2016. fully-convolutional siamese networks
   for object tracking. [online] arxiv: 1606.09549. available:
   [329]https://arxiv.org/abs/1606.09549v2

   [330][41] held et al. 2016. learning to track at 100 fps with deep
   regression networks. [online] arxiv: 1604.01802. available:
   [331]https://arxiv.org/abs/1604.01802v2

   [332][42] david held. 2016. goturn - a neural network tracker. [online]
   youtube.com. available:
   [333]https://www.youtube.com/watch?v=kmhwxid86t_i [accessed:
   03/03/2017].

   [334][43] gladh et al. 2016. deep motion features for visual tracking.
   [online] arxiv: 1612.06615. available: [335]arxiv:1612.06615v1

   [336][44] gaidon et al. 2016. virtual worlds as proxy for multi-object
   tracking analysis. [online] arxiv: 1605.06457. available:
   [337]arxiv:1605.06457v1

   [338][45] lee et al. 2016. globally optimal object tracking with fully
   convolutional networks. [online] arxiv: 1612.08274. available:
   [339]arxiv:1612.08274v1
   part two

   [340][46] pinheiro, collobert and dollar. 2015. learning to segment
   object candidates. [online] arxiv: 1506.06204. available:
   [341]arxiv:1506.06204v2

   [342][47] pinheiro et al. 2016. learning to refine object segments.
   [online] arxiv: 1603.08695. available: [343]arxiv:1603.08695v2

   [344][48] zagoruyko, s. 2016. a multipath network for id164.
   [online] arxiv: 1604.02135v2. available: [345]arxiv:1604.02135v2

   [346][49] dollar, p. 2016. learning to segment. [blog] fair. available:
   [347]https://research.fb.com/learning-to-segment/

   [348][50] dollar, p. 2016. segmenting and refining images with
   sharpmask. [online] facebook code. available:
   [349]https://code.facebook.com/posts/561187904071636/segmenting-and-ref
   ining-images-with-sharpmask/

   [350][51] jampani et al. 2016. video propagation networks. [online]
   arxiv: 1612.05478. available: [351]arxiv:1612.05478v2

   [352][52] chen et al., 2016. deeplab: semantic image segmentation with
   deep convolutional nets, atrous convolution, and fully connected crfs.
   [online] arxiv: 1606.00915. available: [353]arxiv:1606.00915v1

   [354][53] khoreva et al. 2016. simple does it: weakly supervised
   instance and semantic segmentation. [online] arxiv: 1603.07485v2.
   available: [355]arxiv:1603.07485v2

   [356][54] j  gou et al. 2016. the one hundred layers tiramisu: fully
   convolutional densenets for semantic segmentation. [online] arxiv:
   1611.09326v2. available: [357]arxiv:1611.09326v2

   [358][55] li et al. 2016. fully convolutional instance-aware semantic
   segmentation. [online] arxiv: 1611.07709v1. available:
   [359]arxiv:1611.07709v1

   [360][56] paszke et al. 2016. enet: a deep neural network architecture
   for real-time semantic segmentation. [online] arxiv: 1606.02147v1.
   available: [361]arxiv:1606.02147v1

   [362][57] v  zquez et al. 2016. a benchmark for endoluminal scene
   segmentation of colonoscopy images. [online] arxiv: 1612.00799.
   available: [363]arxiv:1612.00799v1

   [364][58] dolz et al. 2016. 3d fully convolutional networks for
   subcortical segmentation in mri: a large-scale study. [online] arxiv:
   1612.03925. available: [365]arxiv:1612.03925v1

   [366][59] alex et al. 2017. semi-supervised learning using denoising
   autoencoders for brain lesion detection and segmentation. [online]
   arxiv: 1611.08664. available: [367]arxiv:1611.08664v4

   [368][60] mozaffari and lee. 2016. 3d ultrasound image segmentation: a
   survey. [online] arxiv: 1611.09811. available: [369]arxiv:1611.09811v1

   [370][61] dasgupta and singh. 2016. a fully convolutional neural
   network based id170 approach towards the retinal vessel
   segmentation. [online] arxiv: 1611.02064. available:
   [371]arxiv:1611.02064v2

   [372][62] yi et al. 2016. 3-d convolutional neural networks for
   glioblastoma segmentation. [online] arxiv: 1611.04534. available:
   [373]arxiv:1611.04534v1

   [374][63] quan et al. 2016. fusionnet: a deep fully residual
   convolutional neural network for image segmentation in connectomics.
   [online] arxiv: 1612.05360. available: [375]arxiv:1612.05360v2

   [376][64] connectomics refers to the mapping of all connections within
   an organism   s nervous system, i.e. neurons and their connections.

   [377][65] champandard, a.j. 2017. neural enhance (latest commit
   30/11/2016). [online] github. available:
   [378]https://github.com/alexjc/neural-enhance [accessed: 11/02/2017]

   [379][66] caballero et al. 2016. real-time video super-resolution with
   spatio-temporal networks and motion compensation. [online] arxiv:
   1611.05250. available: [380]arxiv:1611.05250v1

   [381][67] shi et al. 2016. real-time single image and video
   super-resolution using an efficient sub-pixel convolutional neural
   network. [online] arxiv: 1609.05158. available: [382]arxiv:1609.05158v2

   [383][68] romano et al. 2016. raisr: rapid and accurate image super
   resolution. [online] arxiv: 1606.01299. available:
   [384]arxiv:1606.01299v3

   [385][69]  milanfar, p. 2016. enhance! raisr sharp images with machine
   learning. [blog] google research blog. available:
   [386]https://research.googleblog.com/2016/11/enhance-raisr-sharp-images
   -with-machine.html [accessed: 20/03/2017].

   [387][70] ledig et al. 2017. photo-realistic single image
   super-resolution using a generative adversarial network. [online]
   arxiv: 1609.04802

   [388][71]ibid . available: [389]arxiv:1609.04802v3

   [390][72] ibid

   [391][73] s  nderby et al. 2016. amortised map id136 for image
   super-resolution. [online] arxiv: 1610.04490. available:
   [392]arxiv:1610.04490v1

   [393][74] prisma. 2017. [website] prisma. available:
   [394]https://prisma-ai.com/ [accessed: 01/04/2017].

   [395][75] artomatix. 2017. [website] artomatix. available:
   [396]https://services.artomatix.com/ [accessed: 01/04/2017].

   [397][76] gatys et al. 2015. a neural algorithm of artistic style.
   [online] arxiv: 1508.06576. available: [398]arxiv:1508.06576v2

   [399][77] nikulin & novak. 2016. exploring the neural algorithm of
   artistic style. [online] arxiv: 1602.07188. available:
   [400]arxiv:1602.07188v2

   [401][78] ruder et al. 2016. artistic style transfer for videos.
   [online] arxiv: 1604.08610. available: [402]arxiv:1604.08610v2

   [403][79] ibid

   [404][80] jia and vajda. 2016. delivering real-time ai in the palm of
   your hand. [online] facebook code. available:
   [405]https://code.facebook.com/posts/196146247499076/delivering-real-ti
   me-ai-in-the-palm-of-your-hand/ [accessed: 20/01/2017].

   [406][81] dumoulin et al. 2016. supercharging style transfer. [online]
   google research blog. available:
   [407]https://research.googleblog.com/2016/10/supercharging-style-transf
   er.html [accessed: 20/01/2017].

   [408][82] dumoulin et al. 2017. a learned representation for artistic
   style. [online] arxiv: 1610.07629. available: [409]arxiv:1610.07629v5

   [410][83] zhang et al. 2016. colorful image colorization. [online]
   arxiv: 1603.08511. available: [411]arxiv:1603.08511v5

   [412][84] larsson et al. 2016. learning representations for automatic
   colorization. [online] arxiv: 1603.06668. available:
   [413]arxiv:1603.06668v2

   [414][85] lizuka, simo-serra and ishikawa. 2016. let there be color!:
   joint end-to-end learning of global and local image priors for
   automatic image colorization with simultaneous classification. [online]
   acm transaction on graphics (proc. of siggraph), 35(4):110. available:
   [415]http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/

   [416][86] ibid

   [417][87] varol et al. 2016. long-term temporal convolutions for action
   recognition. [online] arxiv: 1604.04494. available:
   [418]arxiv:1604.04494v1

   [419][88] feichtenhofer et al. 2016. spatiotemporal residual networks
   for video action recognition. [online] arxiv: 1611.02155. available:
   [420]arxiv:1611.02155v1

   [421][89] vondrick et al. 2016. anticipating visual representations
   from unlabeled video. [online] arxiv: 1504.08023. available:
   [422]arxiv:1504.08023v2

   [423][90] conner-simons, a., gordon, r. 2016. teaching machines to
   predict the future. [online] mit news. available:
   [424]https://news.mit.edu/2016/teaching-machines-to-predict-the-future-
   0621 [accessed: 03/02/2017].

   [425][91] idrees et al. 2016. the thumos challenge on action
   recognition for videos "in the wild". [online] arxiv: 1604.06182.
   available: [426]arxiv:1604.06182v1
   part three

   [427][92] rezende et al. 2016. unsupervised learning of 3d structure
   from images. [online] arxiv: 1607.00662. available:
   [428]arxiv:1607.00662v1

   [429][93] ibid

   [430][94] ibid

   [431][95] pose estimation can refer to either just an object   s
   orientation, or both orientation and position in 3d space.

   [432][96] riegler et al. 2016. octnet: learning deep 3d representations
   at high resolutions. [online] arxiv: 1611.05009. available:
   [433]arxiv:1611.05009v3

   [434][97] xiang et al. 2016. objectnet3d: a large scale database for 3d
   object recognition. [online] id161 and geometry lab, stanford
   university (cvgl.stanford.edu). available from:
   [435]http://cvgl.stanford.edu/projects/objectnet3d/

   [436][98] choy et al. 2016. 3d-r2n2: a unified approach for single and
   multi-view 3d object reconstruction. [online] arxiv: 1604.00449.
   available: [437]arxiv:1604.00449v1

   [438][99] ibid

   [439][100] gadelha et al. 2016. 3d shape induction from 2d views of
   multiple objects. [online] arxiv: 1612.058272. available:
   [440]arxiv:1612.05872v1

   [441][101] ibid

   [442][102] rezende et al. 2016. unsupervised learning of 3d structure
   from images. [online] arxiv: 1607.00662. available:
   [443]arxiv:1607.00662v1

   [444][103] colyer, a. 2017. unsupervised learning of 3d structure from
   images. [blog] the morning paper. available:
   [445]https://blog.acolyer.org/2017/01/05/unsupervised-learning-of-3d-st
   ructure-from-images/ [accessed: 04/03/2017].

   [446][104] coco. 2016. welcome to the coco 2016 keypoint challenge!
   [online] common objects in common (mscoco.org). available:
   [447]http://mscoco.org/dataset/#keypoints-challenge2016 [accessed:
   27/01/2017].

   [448][105] eccv. 2016. webpage. [online] european convention on
   id161 ([449]www.eccv2016.org). available:
   [450]http://www.eccv2016.org/main-conference/ [accessed: 26/01/2017].

   [451][106] cao et al. 2016. realtime multi-person 2d pose estimation
   using part affinity fields. [online] arxiv: 161108050. available:
   [452]arxiv:1611.08050v1

   [453][107] zhe cao. 2016. realtime multi-person 2d human pose
   estimation using part affinity fields, cvpr 2017 oral. [online]
   youtube.com. available:
   [454]https://www.youtube.com/watch?v=pw6nzxewlgm [accessed:
   04/03/2017].

   [455][108] bogo et al. 2016. keep it smpl: automatic estimation of 3d
   human pose and shape from a single image. [online] arxiv: 1607.08128.
   available: [456]arxiv:1607.08128v1

   [457][109] michael black. 2016. smplify: 3d human pose and shape from a
   single image (eccv 2016). [online] youtube.com. available:
   [458]https://www.youtube.com/watch?v=eunz2rjxgae [accessed:
   04/03/2017].

   [459][110] ibid

   [460][111] dou et al. 2016. fusion4d: real-time performance capture of
   challenging scenes. [online]  samehkhamis.com. available:
   [461]http://www.samehkhamis.com/dou-siggraph2016.pdf

   [462][112] ibid

   [463][113] microsoft research. 2016. fusion4d: real-time performance
   capture of challenging scenes. [online] youtube.com. available:
   [464]https://www.youtube.com/watch?v=2dkcj1yhyw4&feature=youtu.be [acce
   ssed: 04/03/2017].

   [465][114] i3d past projects. 2016. holoportation: virtual 3d
   teleportation in real-time (microsoft research). [online] youtube.com.
   available:
   [466]https://www.youtube.com/watch?v=7d59o6cfam0&feature=youtu.be [acce
   ssed: 03/03/2017].

   [467][115] kim et al. 2016. real-time 3d reconstruction and 6-dof
   tracking with an event camera. [online] department of computer,
   imperial college london ([468]www.doc.ic.ac.uk). available:
   [469]https://www.doc.ic.ac.uk/~ajd/publications/kim_etal_eccv2016.pdf

   [470][116] ibid

   [471][117] kim et al. 2014. simultaneous mosaicing and tracking with an
   event camera. [online] department of computer, imperial college london
   ([472]www.doc.ic.ac.uk). available:
   [473]https://www.doc.ic.ac.uk/~ajd/publications/kim_etal_bmvc2014.pdf

   [474][118] hanme kim. 2017. real-time 3d reconstruction and 6-dof
   tracking with an event. [online] youtube.com. available:
   [475]https://www.youtube.com/watch?v=yhlyhdmsw7w [accessed:
   03/03/2017].

   [476][119] garg et al. 2016. unsupervised id98 for single view depth
   estimation: geometry to the rescue. [online] arxiv: 1603.04992.
   available: [477]arxiv:1603.04992v2

   [478][120] izadinia et al. 2016. im2cad. [online] arxiv: 1608.05137.
   available: [479]arxiv:1608.05137v1

   [480][121] ibid

   [481][122] yet more neural network spillover

   [482][123] tokmakov et al. 2016. learning motion patterns in videos.
   [online] arxiv: 1612.07217. available: [483]arxiv:1612.07217v1

   [484][124]davis. 2017. davis: densely annotated video segmentation.
   [website] davis challenge. available:
   [485]http://davischallenge.org/ [accessed: 27/03/2017].

   [486][125] detone et al. 2016. deep image homography estimation.
   [online] arxiv: 1606.03798. available: [487]arxiv:1606.03798v1

   [488][126] handa et al. 2016. gvnn: neural network library for
   geometric id161. [online] arxiv: 1607.07405. available:
   [489]arxiv:1607.07405v3

   [490][127] malisiewicz. 2016. the future of real-time slam and deep
   learning vs slam. [blog] tombone's id161 blog. available:
   [491]http://www.computervisionblog.com/2016/01/why-slam-matters-future-
   of-real-time.html [accessed: 01/03/2017].

   [492][128] google. 2017. tango. [website] get.google.com. available:
   [493]https://get.google.com/tango/ [accessed: 23/03/2017].

   [494][129] ibid

   [495][130] malisiewicz. 2016. the future of real-time slam and deep
   learning vs slam. [blog] tombone's id161 blog. available:
   [496]http://www.computervisionblog.com/2016/01/why-slam-matters-future-
   of-real-time.html [accessed: 01/03/2017].
   part four

   [497][131] szegedy et al. 2016. inception-v4, inception-resnet and the
   impact of residual connections on learning. [online] arxiv: 1602.07261.
   available: [498]arxiv:1602.07261v2

   [499][132] szegedy et al. 2015. rethinking the inception architecture
   for id161. [online] arxiv: 1512.00567. available:
   [500]arxiv:1512.00567v3

   [501][133] huang et al. 2016. densely connected convolutional networks.
   [online] arxiv: 1608.06993. available: [502]arxiv:1608.06993v3

   [503][134] ibid

   [504][135] ibid

   [505][136] liuzhuang13. 2017. code for densely connected convolutional
   networks (densenets). [online] github.com. available:
   [506]https://github.com/liuzhuang13/densenet [accessed: 03/04/2017].

   [507][137] larsson et al. 2016. fractalnet: ultra-deep neural networks
   without residuals. [online] arxiv: 1605.07648. available:
   [508]arxiv:1605.07648v2

   [509][138] huang et al. 2016. densely connected convolutional networks.
   [online] arxiv: 1608.06993. available: [510]arxiv:1608.06993v3, pg. 1.

   [511][139] hossein hasanpour et al. 2016. lets keep it simple: using
   simple architectures to outperform deeper architectures. [online]
   arxiv: 1608.06037. available: [512]arxiv:1608.06037v3

   [513][140] ibid

   [514][141] singh et al. 2016. swapout: learning an ensemble of deep
   architectures. [online] arxiv: 1605.06465. available:
   [515]arxiv:1605.06465v1

   [516][142] iandola et al. 2016. squeezenet: alexnet-level accuracy with
   50x fewer parameters and <0.5mb model size. [online] arxiv: 1602.07360.
   available: [517]arxiv:1602.07360v4

   [518][143] shang et al. 2016. understanding and improving convolutional
   neural networks via concatenated rectified linear units. [online]
   arxiv: 1603.05201. available: [519]arxiv:1603.05201v2

   [520][144] clevert et al. 2016. fast and accurate deep network learning
   by exponential linear units (elus). [online] arxiv: 1511.07289.
   available: [521]arxiv:1511.07289v5

   [522][145] trottier et al. 2016. parametric exponential linear unit for
   deep convolutional neural networks. [online] arxiv: 1605.09332.
   available: [523]arxiv:1605.09332v3

   [524][146] worrall et al. 2016. harmonic networks: deep translation and
   rotation equivariance. [online] arxiv: 1612.04642. available:
   [525]arxiv:1612.04642v1

   [526][147] cohen & welling. 2016. group equivariant convolutional
   networks. [online] arxiv: 1602.07576. available:
   [527]arxiv:1602.07576v3

   [528][148] dieleman et al. 2016. exploiting cyclic symmetry in
   convolutional neural networks. [online] arxiv: 1602.02660. available:
   [529]arxiv:1602.02660v2

   [530][149] cohen & welling. 2016. steerable id98s. [online] arxiv:
   1612.08498. available: [531]arxiv:1612.08498v1

   [532][150] abdi, m., nahavandi, s. 2016. multi-residual networks:
   improving the speed and accuracy of residual networks. [online] arxiv:
   1609.05672. available: [533]arxiv:1609.05672v3

   [534][151] he et al. 2015. deep residual learning for image
   recognition. [online] arxiv: 1512.03385. available:
   [535]arxiv:1512.03385v1

   [536][152] quora. 2017. what is an intuitive explanation of deep
   residual networks? [website] [537]www.quora.com. available:
   [538]https://www.quora.com/what-is-an-intuitive-explanation-of-deep-res
   idual-networks [accessed: 03/04/2017].

   [539][153] zagoruyko, s. and komodakis, n. 2017. wide residual
   networks. [online] arxiv: 1605.07146. available:
   [540]arxiv:1605.07146v3

   [541][154] huang et al. 2016. deep networks with stochastic depth.
   [online] arxiv: 1603.09382. available: [542]arxiv:1603.09382v3

   [543][155] savarese et al. 2016. learning identity mappings with
   residual gates. [online] arxiv: 1611.01260. available:
   [544]arxiv:1611.01260v2

   [545][156] veit, wilber and belongie. 2016. residual networks behave
   like ensembles of relatively shallow networks. [online] arxiv:
   1605.06431. available: [546]arxiv:1605.06431v2

   [547][157] he at al. 2016. identity mappings in deep residual networks.
   [online] arxiv: 1603.05027. available: [548]arxiv:1603.05027v3

   [549][158] abdi, m., nahavandi, s. 2016. multi-residual networks:
   improving the speed and accuracy of residual networks. [online] arxiv:
   1609.05672. available: [550]arxiv:1609.05672v3

   [551][159] greff et al. 2017. highway and residual networks learn
   unrolled iterative estimation. [online] arxiv: 1612. 07771. available:
   [552]arxiv:1612.07771v3

   [553][160] abdi and nahavandi. 2017. multi-residual networks: improving
   the speed and accuracy of residual networks. [online] 1609.05672.
   available: [554]arxiv:1609.05672v4

   [555][161] targ et al. 2016. resnet in resnet: generalizing residual
   architectures. [online] arxiv: 1603.08029. available:
   [556]arxiv:1603.08029v1

   [557][162] wu et al. 2016. wider or deeper: revisiting the resnet model
   for visual recognition. [online] arxiv: 1611.10080. available:
   [558]arxiv:1611.10080v1

   [559][163] liao and poggio. 2016. bridging the gaps between residual
   learning, recurrent neural networks and visual cortex. [online] arxiv:
   1604.03640. available: [560]arxiv:1604.03640v1

   [561][164] moniz and pal. 2016. convolutional residual memory networks.
   [online] arxiv: 1606.05262. available: [562]arxiv:1606.05262v3

   [563][165] hardt and ma. 2016. identity matters in deep learning.
   [online] arxiv: 1611.04231. available: [564]arxiv:1611.04231v2

   [565][166] shah et al. 2016. deep residual networks with exponential
   linear unit. [online] arxiv: 1604.04112. available:
   [566]arxiv:1604.04112v4

   [567][167] shen and zeng. 2016. weighted residuals for very deep
   networks. [online] arxiv: 1605.08831. available:
   [568]arxiv:1605.08831v1

   [569][168] ben hamner. 2016. twitter status. [online]
   twitter. available:
   [570]https://twitter.com/benhamner/status/789909204832227329

   [571][169] id163. 2017. homepage. [online] available:
   [572]http://image-net.org/index [accessed: 04/01/2017]

   [573][170] coco. 2017. common objects in common homepage. [online]
   available: [574]http://mscoco.org/ [accessed: 04/01/2017]

   [575][171] cifars. 2017. the cifar-10 dataset. [online] available:
   [576]https://www.cs.toronto.edu/~kriz/cifar.html [accessed: 04/01/2017]

   [577][172] mnist. 2017. the mnist database of handwritten digits.
   [online] available: [578]http://yann.lecun.com/exdb/mnist/ [accessed:
   04/01/2017]

   [579][173] zhou et al. 2016. places2. [online] available:
   [580]http://places2.csail.mit.edu/ [accessed: 06/01/2017]

   [581][174] mccormac et al. 2017. scenenet rgb-d: 5m photorealistic
   images of synthetic indoor trajectories with ground truth. [online]
   arxiv: 1612.05079v3. available: [582]arxiv:1612.05079v3

   [583][175] ibid

   [584][176] aytar et al. 2016. cross-modal scene networks. [online]
   arxiv: 1610.09003. available: [585]arxiv:1610.09003v1

   [586][177] ibid

   [587][178] guo et al. 2016. ms-celeb-1m: a dataset and benchmark for
   large-scale face recognition. [online] arxiv: 1607.08221. available:
   [588]arxiv:1607.08221v1

   [589][179] open images. 2017. open images dataset. [online] github.
   available: [590]https://github.com/openimages/dataset [accessed:
   08/01/2017]

   [591][180] abu-el-haija et al. 2016. youtube-8m: a large-scale video
   classification benchmark. [online] arxiv: 1609.08675. available:
   [592]arxiv:1609.08675v1

   [593][181] natsev, p. 2017. an updated youtube-8m, a video
   understanding challenge, and a cvpr workshop. oh my!. [online] google
   research blog. available:
   [594]https://research.googleblog.com/2017/02/an-updated-youtube-8m-vide
   o.html [accessed: 26/02/2017].

   [595][182] youtube-8m. 2017. cvpr'17 workshop on youtube-8m large-scale
   video understanding. [online] google research. available:
   [596]https://research.google.com/youtube8m/workshop.html [accessed:
   26/02/2017].

   [597][183] google. 2017. youtube-8m dataset. [online]
   research.google.com. available:
   [598]https://research.google.com/youtube8m/ [accessed: 04/03/2017].

   [599][184] wu, pique & wieland. 2016. using artificial intelligence to
   help blind people    see    facebook. [online] facebook newsroom.
   available:
   [600]http://newsroom.fb.com/news/2016/04/using-artificial-intelligence-
   to-help-blind-people-see-facebook/ [accessed: 02/03/2017].

   [601][185] metz. 2016. artificial intelligence finally entered our
   everyday world. [online] wired. available:
   [602]https://www.wired.com/2016/01/2015-was-the-year-ai-finally-entered
   -the-everyday-world/ [accessed: 02/03/2017].

   [603][186] doerrfeld. 2015. 20+ emotion recognition apis that will
   leave you impressed, and concerned. [online] nordic apis. available:
   [604]http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-
   you-impressed-and-concerned/ [accessed: 02/03/2017].

   [605][187] johnson, a. 2016. trailbehind/deeposm - train a deep
   learning net with openstreetmap features and satellite imagery.
   [online] github.com. available:
   [606]https://github.com/trailbehind/deeposm [accessed: 29/03/2017].

   [607][188] gros and tiecke. 2016. connecting the world with better
   maps. [online] facebook code. available:
   [608]https://code.facebook.com/posts/1676452492623525/connecting-the-wo
   rld-with-better-maps/ [accessed: 02/03/2017].

   [609][189] amazon. 2017. frequently asked questions - amazon go.
   [website] amazon.com. available:
   [610]https://www.amazon.com/b?node=16008589011 [accessed: 29/03/2017].

   [611][190] reisinger, d. 2017. amazon   s cashier-free store might be
   easy to break. [online] fortune tech. available:
   [612]http://fortune.com/2017/03/28/amazon-go-cashier-free-store/ [acces
   sed: 29/03/2017].

   [613][191] mueller-freitag, m. 2017. germany asleep at the wheel?
   [blog] twenty billion neurons - medium.com. available:
   [614]https://medium.com/twentybn/germany-asleep-at-the-wheel-d800445d6d
   a2

   [615][192] gordo et al. 2016. deep id162: learning global
   representations for image search. [online] arxiv: 1604.01325.
   available: [616]arxiv:1604.01325v2

   [617][193] wang et al. 2016. deep learning for identifying metastatic
   breast cancer. [online] arxiv: 1606.05718. available:
   [618]arxiv:1606.05718v1

   [619][194] rosenfeld, j. 2016. ai achieves near-human detection of
   breast cancer. [online] mentalfloss.com. available:
   [620]http://mentalfloss.com/article/82415/ai-achieves-near-human-detect
   ion-breast-cancer [accessed: 27/03/2017].

   [621][195] sato, k. 2016. how a japanese cucumber farmer is using deep
   learning and tensorflow. [blog] google cloud platform. available:
   [622]https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucu
   mber-farmer-is-using-deep-learning-and-tensorflow

   [623][196] banerjee, p. 2016. the rise of vpus: giving eyes to
   machines. [online] [624]www.digit.in. available:
   [625]http://www.digit.in/general/the-rise-of-vpus-giving-eyes-to-machin
   es-29561.html [accessed: 22/03/2017.

   [626][197] movidius. 2017. embedded neural network compute framework:
   fathom. [online] movidius.com. available:
   [627]https://www.movidius.com/solutions/machine-vision-algorithms/machi
   ne-learning [accessed: 03/03/2017].

   [628][198] dzyre, n. 2016. 10 forthcoming augmented reality & smart
   glasses you can buy. [blog] hongkiat. available:
   [629]http://www.hongkiat.com/blog/augmented-reality-smart-glasses/ [acc
   essed: 03/03/2017].

   [630][199] google. 2017. tango. [website] get.google.com. available:
   [631]https://get.google.com/tango/ [accessed: 23/03/2017].

   [632][200] assael et al. 2016. lipnet: end-to-end sentence-level
   lipreading. [online] arxiv: 1611.01599. available:
   [633]arxiv:1611.01599v2

   [634][201] albanie et al. 2017. stopping gan violence: generative
   unadversarial networks. [online] arxiv: 1703.02528. available:
   [635]arxiv:1703.02528v1

   [636][202] tractica. 2016. id161 hardware and software market
   to reach $48.6 billion by 2022. [website] [637]www.tractica.com.
   available:
   [638]https://www.tractica.com/newsroom/press-releases/computer-vision-h
   ardware-and-software-market-to-reach-48-6-billion-by-2022/ [accessed:
   12/03/2017].

   [639][203] ibid

updates and corrections

   if you see mistakes or want to suggest changes, please email
   [640]info@themtank.com.

reuse

   any third party who wants to republish the contents of this publication
   may do so provided that they notify the m tank team
   ([641]info@themtank.com) and acknowledge the projects work accordingly.
   where content of the publication concerns the work of other authors
   (e.g. diagrams, papers, findings, etc.), third parties should contact
   the original rights-holder for permission.

citation

   for attribution in academic contexts, please cite this work as
duffy and flynn, "a year in id161", the m tank, 2017.

   bibtex citation
@techreport{a year in id161,
  author = {duffy, benjamin and flynn, daniel},
  title = {a year in id161},
  institution = {the m tank},
  year = {2017},
  note = {http://www.themtank.org/a-year-in-computer-vision}
}

references

   visible links
   1. http://www.themtank.org/
   2. http://www.themtank.org/pdfs/ayearofcomputervisionpdf.pdf
   3. https://medium.com/@info_84181/a-year-in-computer-vision-part-1-of-4-eaeb040b6f46
   4. https://medium.com/@themtank/a-year-in-computer-vision-part-2-of-4-893e18e12be0
   5. https://medium.com/@themtank/a-year-in-computer-vision-part-3-of-4-861216d71607
   6. https://medium.com/@themtank/a-year-in-computer-vision-part-4-of-4-515c61d41a00
   7. http://www.themtank.org/a-year-in-computer-vision#ftnt1
   8. http://www.themtank.org/a-year-in-computer-vision#ftnt2
   9. http://www.themtank.org/a-year-in-computer-vision#ftnt3
  10. http://www.themtank.org/a-year-in-computer-vision#ftnt4
  11. http://www.themtank.org/a-year-in-computer-vision#ftnt5
  12. http://www.themtank.org/a-year-in-computer-vision#ftnt6
  13. http://www.themtank.org/a-year-in-computer-vision#ftnt7
  14. http://www.themtank.org/a-year-in-computer-vision#ftnt8
  15. http://www.themtank.org/a-year-in-computer-vision#ftnt9
  16. http://www.themtank.org/a-year-in-computer-vision#ftnt10
  17. http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf
  18. http://www.themtank.org/a-year-in-computer-vision#ftnt11
  19. http://www.themtank.org/a-year-in-computer-vision#ftnt12
  20. http://www.themtank.org/a-year-in-computer-vision#ftnt13
  21. http://www.themtank.org/a-year-in-computer-vision#ftnt14
  22. http://www.image-net.org/challenges/lsvrc/2016/index#comp
  23. http://image-net.org/challenges/talks/2016/ilsvrc2016_10_09_clsloc.pdf
  24. http://www.themtank.org/a-year-in-computer-vision#ftnt15
  25. http://www.themtank.org/a-year-in-computer-vision#ftnt16
  26. http://www.themtank.org/a-year-in-computer-vision#ftnt17
  27. http://www.themtank.org/a-year-in-computer-vision#ftnt18
  28. http://www.themtank.org/a-year-in-computer-vision#ftnt19
  29. http://www.themtank.org/a-year-in-computer-vision#ftnt20
  30. http://www.themtank.org/a-year-in-computer-vision#ftnt21
  31. http://www.themtank.org/a-year-in-computer-vision#ftnt22
  32. http://www.themtank.org/a-year-in-computer-vision#ftnt23
  33. http://www.themtank.org/a-year-in-computer-vision#ftnt24
  34. http://www.themtank.org/a-year-in-computer-vision#ftnt25
  35. http://pjreddie.com/darknet/yolo/
  36. http://www.themtank.org/a-year-in-computer-vision#ftnt26
  37. https://www.youtube.com/embed/voc3huqhrss
  38. http://www.themtank.org/a-year-in-computer-vision#ftnt27
  39. http://www.themtank.org/a-year-in-computer-vision#ftnt28
  40. http://www.themtank.org/a-year-in-computer-vision#ftnt29
  41. http://www.themtank.org/a-year-in-computer-vision#ftnt30
  42. http://www.themtank.org/a-year-in-computer-vision#ftnt31
  43. http://www.themtank.org/a-year-in-computer-vision#ftnt32
  44. http://www.themtank.org/a-year-in-computer-vision#ftnt33
  45. http://www.themtank.org/a-year-in-computer-vision#ftnt34
  46. https://www.youtube.com/watch?v=xhp47v5obxq
  47. http://www.themtank.org/a-year-in-computer-vision#ftnt35
  48. https://www.youtube.com/embed/xhp47v5obxq
  49. http://www.themtank.org/a-year-in-computer-vision#ftnt36
  50. http://www.themtank.org/a-year-in-computer-vision#ftnt37
  51. http://www.themtank.org/a-year-in-computer-vision#ftnt38
  52. http://www.themtank.org/a-year-in-computer-vision#ftnt39
  53. http://image-net.org/challenges/talks/2016/eccv2016_ilsvrc_coco_detection_segmentation.pdf
  54. http://www.themtank.org/a-year-in-computer-vision#ftnt40
  55. http://www.themtank.org/a-year-in-computer-vision#ftnt41
  56. https://www.youtube.com/watch?v=kmhwxid86t_i
  57. http://www.themtank.org/a-year-in-computer-vision#ftnt42
  58. https://www.youtube.com/embed/kmhwxid86t_i
  59. http://www.themtank.org/a-year-in-computer-vision#ftnt43
  60. http://www.themtank.org/a-year-in-computer-vision#ftnt44
  61. http://www.themtank.org/a-year-in-computer-vision#ftnt45
  62. http://www.themtank.org/a-year-in-computer-vision#ftnt46
  63. http://www.themtank.org/a-year-in-computer-vision#ftnt47
  64. http://www.themtank.org/a-year-in-computer-vision#ftnt48
  65. http://www.themtank.org/a-year-in-computer-vision#ftnt49
  66. http://www.themtank.org/a-year-in-computer-vision#ftnt50
  67. http://www.themtank.org/a-year-in-computer-vision#ftnt51
  68. http://www.themtank.org/a-year-in-computer-vision#ftnt52
  69. http://www.themtank.org/a-year-in-computer-vision#ftnt53
  70. http://www.themtank.org/a-year-in-computer-vision#ftnt54
  71. http://www.themtank.org/a-year-in-computer-vision#ftnt55
  72. http://www.themtank.org/a-year-in-computer-vision#ftnt56
  73. https://arxiv.org/abs/1612.00799
  74. http://www.themtank.org/a-year-in-computer-vision#ftnt57
  75. https://arxiv.org/abs/1612.03925v1
  76. http://www.themtank.org/a-year-in-computer-vision#ftnt58
  77. https://arxiv.org/abs/1611.08664v3
  78. http://www.themtank.org/a-year-in-computer-vision#ftnt59
  79. https://arxiv.org/abs/1611.09811
  80. http://www.themtank.org/a-year-in-computer-vision#ftnt60
  81. https://arxiv.org/abs/1611.02064
  82. http://www.themtank.org/a-year-in-computer-vision#ftnt61
  83. https://arxiv.org/abs/1611.04534
  84. http://www.themtank.org/a-year-in-computer-vision#ftnt62
  85. http://www.themtank.org/a-year-in-computer-vision#ftnt63
  86. http://www.themtank.org/a-year-in-computer-vision#ftnt64
  87. https://www.youtube.com/embed/pnzq4pnzszc
  88. http://www.themtank.org/a-year-in-computer-vision#ftnt65
  89. http://www.themtank.org/a-year-in-computer-vision#ftnt66
  90. http://www.themtank.org/a-year-in-computer-vision#ftnt67
  91. http://www.themtank.org/a-year-in-computer-vision#ftnt68
  92. https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html
  93. http://www.themtank.org/a-year-in-computer-vision#ftnt69
  94. http://www.themtank.org/a-year-in-computer-vision#ftnt70
  95. http://www.themtank.org/a-year-in-computer-vision#ftnt71
  96. http://www.themtank.org/a-year-in-computer-vision#ftnt72
  97. http://www.themtank.org/a-year-in-computer-vision#ftnt73
  98. http://www.themtank.org/a-year-in-computer-vision#ftnt74
  99. http://www.themtank.org/a-year-in-computer-vision#ftnt75
 100. http://www.themtank.org/a-year-in-computer-vision#ftnt76
 101. http://www.themtank.org/a-year-in-computer-vision#ftnt77
 102. http://www.themtank.org/a-year-in-computer-vision#ftnt78
 103. http://www.themtank.org/a-year-in-computer-vision#ftnt79
 104. http://www.themtank.org/a-year-in-computer-vision#ftnt80
 105. http://www.themtank.org/a-year-in-computer-vision#ftnt81
 106. http://www.themtank.org/a-year-in-computer-vision#ftnt82
 107. http://www.themtank.org/a-year-in-computer-vision#ftnt83
 108. http://www.themtank.org/a-year-in-computer-vision#ftnt84
 109. http://www.themtank.org/a-year-in-computer-vision#ftnt85
 110. http://www.themtank.org/a-year-in-computer-vision#ftnt86
 111. http://www.themtank.org/a-year-in-computer-vision#ftnt87
 112. http://www.themtank.org/a-year-in-computer-vision#ftnt88
 113. http://www.themtank.org/a-year-in-computer-vision#ftnt89
 114. http://www.themtank.org/a-year-in-computer-vision#ftnt90
 115. http://www.themtank.org/a-year-in-computer-vision#ftnt91
 116. http://www.themtank.org/a-year-in-computer-vision#ftnt92
 117. http://www.themtank.org/a-year-in-computer-vision#ftnt93
 118. http://www.themtank.org/a-year-in-computer-vision#ftnt94
 119. http://www.themtank.org/a-year-in-computer-vision#ftnt95
 120. http://www.themtank.org/a-year-in-computer-vision#ftnt96
 121. http://www.themtank.org/a-year-in-computer-vision#ftnt97
 122. http://www.themtank.org/a-year-in-computer-vision#ftnt98
 123. http://www.themtank.org/a-year-in-computer-vision#ftnt99
 124. http://www.themtank.org/a-year-in-computer-vision#ftnt100
 125. http://www.themtank.org/a-year-in-computer-vision#ftnt101
 126. http://www.themtank.org/a-year-in-computer-vision#ftnt102
 127. http://www.themtank.org/a-year-in-computer-vision#ftnt103
 128. http://www.themtank.org/a-year-in-computer-vision#ftnt104
 129. http://www.themtank.org/a-year-in-computer-vision#ftnt105
 130. http://www.themtank.org/a-year-in-computer-vision#ftnt106
 131. https://www.youtube.com/watch?v=pw6nzxewlgm
 132. http://www.themtank.org/a-year-in-computer-vision#ftnt107
 133. https://www.youtube.com/embed/pw6nzxewlgm
 134. http://www.themtank.org/a-year-in-computer-vision#ftnt108
 135. https://www.youtube.com/watch?v=eunz2rjxgae
 136. http://www.themtank.org/a-year-in-computer-vision#ftnt109
 137. https://www.youtube.com/embed/eunz2rjxgae
 138. http://www.themtank.org/a-year-in-computer-vision#ftnt110
 139. http://www.themtank.org/a-year-in-computer-vision#ftnt111
 140. http://www.themtank.org/a-year-in-computer-vision#ftnt112
 141. https://youtu.be/2dkcj1yhyw4
 142. http://www.themtank.org/a-year-in-computer-vision#ftnt113
 143. https://www.youtube.com/embed/2dkcj1yhyw4
 144. https://youtu.be/7d59o6cfam0
 145. http://www.themtank.org/a-year-in-computer-vision#ftnt114
 146. https://www.youtube.com/embed/7d59o6cfam0
 147. http://www.themtank.org/a-year-in-computer-vision#ftnt115
 148. http://www.themtank.org/a-year-in-computer-vision#ftnt116
 149. http://www.themtank.org/a-year-in-computer-vision#ftnt117
 150. https://www.youtube.com/watch?v=yhlyhdmsw7w
 151. http://www.themtank.org/a-year-in-computer-vision#ftnt118
 152. https://www.youtube.com/embed/yhlyhdmsw7w
 153. http://www.themtank.org/a-year-in-computer-vision#ftnt119
 154. http://www.themtank.org/a-year-in-computer-vision#ftnt120
 155. http://www.themtank.org/a-year-in-computer-vision#ftnt121
 156. http://www.themtank.org/a-year-in-computer-vision#ftnt122
 157. http://www.themtank.org/a-year-in-computer-vision#ftnt123
 158. http://www.themtank.org/a-year-in-computer-vision#ftnt124
 159. http://www.themtank.org/a-year-in-computer-vision#ftnt125
 160. http://www.themtank.org/a-year-in-computer-vision#ftnt126
 161. http://www.themtank.org/a-year-in-computer-vision#ftnt127
 162. http://www.themtank.org/a-year-in-computer-vision#ftnt128
 163. http://www.themtank.org/a-year-in-computer-vision#ftnt129
 164. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 165. http://www.themtank.org/a-year-in-computer-vision#ftnt130
 166. https://arxiv.org/abs/1803.02286
 167. https://arxiv.org/abs/1801.08214
 168. https://arxiv.org/abs/1802.06857
 169. https://deepmind.com/blog/learning-to-navigate-cities-without-a-map/
 170. http://www.themtank.org/a-year-in-computer-vision#ftnt131
 171. http://www.themtank.org/a-year-in-computer-vision#ftnt132
 172. http://www.themtank.org/a-year-in-computer-vision#ftnt133
 173. http://www.themtank.org/a-year-in-computer-vision#ftnt134
 174. http://www.themtank.org/a-year-in-computer-vision#ftnt135
 175. https://github.com/liuzhuang13/densenet
 176. http://www.themtank.org/a-year-in-computer-vision#ftnt136
 177. http://www.themtank.org/a-year-in-computer-vision#ftnt137
 178. http://www.themtank.org/a-year-in-computer-vision#ftnt138
 179. http://www.themtank.org/a-year-in-computer-vision#ftnt139
 180. http://www.themtank.org/a-year-in-computer-vision#ftnt140
 181. http://www.themtank.org/a-year-in-computer-vision#ftnt141
 182. http://www.themtank.org/a-year-in-computer-vision#ftnt142
 183. http://www.themtank.org/a-year-in-computer-vision#ftnt143
 184. http://www.themtank.org/a-year-in-computer-vision#ftnt144
 185. http://www.themtank.org/a-year-in-computer-vision#ftnt145
 186. http://www.themtank.org/a-year-in-computer-vision#ftnt146
 187. http://www.themtank.org/a-year-in-computer-vision#ftnt147
 188. http://www.themtank.org/a-year-in-computer-vision#ftnt148
 189. http://www.themtank.org/a-year-in-computer-vision#ftnt149
 190. http://www.themtank.org/a-year-in-computer-vision#ftnt150
 191. http://www.themtank.org/a-year-in-computer-vision#ftnt151
 192. https://www.quora.com/what-is-an-intuitive-explanation-of-deep-residual-networks
 193. http://www.themtank.org/a-year-in-computer-vision#ftnt152
 194. http://www.themtank.org/a-year-in-computer-vision#ftnt153
 195. http://www.themtank.org/a-year-in-computer-vision#ftnt154
 196. http://www.themtank.org/a-year-in-computer-vision#ftnt155
 197. http://www.themtank.org/a-year-in-computer-vision#ftnt156
 198. http://www.themtank.org/a-year-in-computer-vision#ftnt157
 199. http://www.themtank.org/a-year-in-computer-vision#ftnt158
 200. https://arxiv.org/abs/1612.07771
 201. http://www.themtank.org/a-year-in-computer-vision#ftnt159
 202. https://arxiv.org/abs/1609.05672
 203. http://www.themtank.org/a-year-in-computer-vision#ftnt160
 204. https://arxiv.org/abs/1603.08029
 205. http://www.themtank.org/a-year-in-computer-vision#ftnt161
 206. https://arxiv.org/abs/1611.10080
 207. http://www.themtank.org/a-year-in-computer-vision#ftnt162
 208. https://arxiv.org/abs/1604.03640
 209. http://www.themtank.org/a-year-in-computer-vision#ftnt163
 210. https://arxiv.org/abs/1606.05262
 211. http://www.themtank.org/a-year-in-computer-vision#ftnt164
 212. https://arxiv.org/abs/1611.04231
 213. http://www.themtank.org/a-year-in-computer-vision#ftnt165
 214. https://arxiv.org/abs/1604.04112
 215. http://www.themtank.org/a-year-in-computer-vision#ftnt166
 216. https://arxiv.org/abs/1605.08831
 217. http://www.themtank.org/a-year-in-computer-vision#ftnt167
 218. http://www.themtank.org/a-year-in-computer-vision#ftnt168
 219. http://www.themtank.org/a-year-in-computer-vision#ftnt169
 220. http://www.themtank.org/a-year-in-computer-vision#ftnt170
 221. http://www.themtank.org/a-year-in-computer-vision#ftnt171
 222. http://www.themtank.org/a-year-in-computer-vision#ftnt172
 223. http://www.themtank.org/a-year-in-computer-vision#ftnt173
 224. http://www.themtank.org/a-year-in-computer-vision#ftnt174
 225. http://www.themtank.org/a-year-in-computer-vision#ftnt175
 226. http://www.themtank.org/a-year-in-computer-vision#ftnt176
 227. http://www.themtank.org/a-year-in-computer-vision#ftnt177
 228. http://www.themtank.org/a-year-in-computer-vision#ftnt178
 229. http://www.themtank.org/a-year-in-computer-vision#ftnt179
 230. http://www.themtank.org/a-year-in-computer-vision#ftnt180
 231. http://www.themtank.org/a-year-in-computer-vision#ftnt181
 232. http://www.themtank.org/a-year-in-computer-vision#ftnt182
 233. https://research.google.com/youtube8m/
 234. http://www.themtank.org/a-year-in-computer-vision#ftnt183
 235. http://www.themtank.org/a-year-in-computer-vision#ftnt184
 236. http://www.themtank.org/a-year-in-computer-vision#ftnt185
 237. http://www.themtank.org/a-year-in-computer-vision#ftnt186
 238. http://www.themtank.org/a-year-in-computer-vision#ftnt187
 239. http://www.themtank.org/a-year-in-computer-vision#ftnt188
 240. http://www.themtank.org/a-year-in-computer-vision#ftnt189
 241. http://www.themtank.org/a-year-in-computer-vision#ftnt190
 242. http://www.themtank.org/a-year-in-computer-vision#ftnt191
 243. http://www.themtank.org/a-year-in-computer-vision#ftnt192
 244. http://www.themtank.org/a-year-in-computer-vision#ftnt193
 245. http://www.themtank.org/a-year-in-computer-vision#ftnt194
 246. http://www.themtank.org/a-year-in-computer-vision#ftnt195
 247. http://www.themtank.org/a-year-in-computer-vision#ftnt196
 248. https://www.youtube.com/embed/hx0uelnrr1i
 249. http://www.themtank.org/a-year-in-computer-vision#ftnt197
 250. http://www.themtank.org/a-year-in-computer-vision#ftnt198
 251. http://www.themtank.org/a-year-in-computer-vision#ftnt199
 252. http://www.themtank.org/a-year-in-computer-vision#ftnt200
 253. http://www.themtank.org/a-year-in-computer-vision#ftnt201
 254. http://www.themtank.org/a-year-in-computer-vision#ftnt202
 255. http://www.themtank.org/a-year-in-computer-vision#ftnt203
 256. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref1
 257. http://www.bmva.org/visionoverview
 258. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref2
 259. http://www.cs.toronto.edu/~kriz/id163_classification_with_deep_convolutional.pdf
 260. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref3
 261. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref4
 262. http://karpathy.github.io/2015/10/25/selfie/
 263. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref5
 264. https://www.quora.com/what-is-a-convolutional-neural-network
 265. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref6
 266. http://cs231n.stanford.edu/
 267. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref7
 268. http://www.deeplearningbook.org/
 269. http://www.deeplearningbook.org/contents/convnets.html
 270. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref8
 271. http://neuralnetworksanddeeplearning.com/index.html
 272. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref9
 273. http://image-net.org/challenges/lsvrc/2016/index
 274. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref10
 275. http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-id163/
 276. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref11
 277. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref12
 278. https://keras.io/
 279. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref13
 280. https://arxiv.org/abs/1607.05691v1
 281. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref14
 282. https://arxiv.org/abs/1610.02357v2
 283. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref15
 284. http://places2.csail.mit.edu/
 285. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref16
 286. http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-id163-2016-challenge/
 287. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref17
 288. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref18
 289. http://image-net.org/challenges/lsvrc/2016/results
 290. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref19
 291. https://arxiv.org/abs/1611.05431v1
 292. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref20
 293. http://image-net.org/challenges/lsvrc/2016/#det
 294. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref21
 295. https://arxiv.org/abs/1612.04402v1
 296. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref22
 297. https://arxiv.org/abs/1512.02325v5
 298. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref23
 299. https://arxiv.org/abs/1612.08242v1
 300. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref24
 301. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref25
 302. https://arxiv.org/abs/1506.02640v5
 303. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref26
 304. https://pjreddie.com/darknet/yolo/
 305. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref27
 306. https://arxiv.org/abs/1612.03144v1
 307. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref28
 308. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref29
 309. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref30
 310. https://arxiv.org/abs/1605.06409v2
 311. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref31
 312. https://arxiv.org/abs/1611.10012v1
 313. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref32
 314. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref33
 315. https://arxiv.org/abs/1612.01051v2
 316. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref34
 317. https://arxiv.org/abs/1611.08588v2
 318. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref35
 319. https://www.youtube.com/watch?v=xhp47v5obxq
 320. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref36
 321. http://mscoco.org/
 322. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref37
 323. http://image-net.org/challenges/lsvrc/2016/results
 324. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref38
 325. http://mscoco.org/dataset/#detections-leaderboard
 326. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref39
 327. http://image-net.org/challenges/talks/2016/eccv2016_ilsvrc_coco_detection_segmentation.pdf
 328. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref40
 329. https://arxiv.org/abs/1606.09549v2
 330. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref41
 331. https://arxiv.org/abs/1604.01802v2
 332. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref42
 333. https://www.youtube.com/watch?v=kmhwxid86t_i
 334. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref43
 335. https://arxiv.org/abs/1612.06615v1
 336. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref44
 337. https://arxiv.org/abs/1605.06457v1
 338. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref45
 339. https://arxiv.org/abs/1612.08274v1
 340. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref46
 341. https://arxiv.org/abs/1506.06204v2
 342. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref47
 343. https://arxiv.org/abs/1603.08695v2
 344. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref48
 345. https://arxiv.org/abs/1604.02135v2
 346. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref49
 347. https://research.fb.com/learning-to-segment/
 348. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref50
 349. https://code.facebook.com/posts/561187904071636/segmenting-and-refining-images-with-sharpmask/
 350. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref51
 351. https://arxiv.org/abs/1612.05478v2
 352. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref52
 353. https://arxiv.org/abs/1606.00915v1
 354. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref53
 355. https://arxiv.org/abs/1603.07485v2
 356. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref54
 357. https://arxiv.org/abs/1611.09326v2
 358. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref55
 359. https://arxiv.org/abs/1611.07709v1
 360. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref56
 361. https://arxiv.org/abs/1606.02147v1
 362. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref57
 363. https://arxiv.org/abs/1612.00799v1
 364. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref58
 365. https://arxiv.org/abs/1612.03925v1
 366. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref59
 367. https://arxiv.org/abs/1611.08664v4
 368. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref60
 369. https://arxiv.org/abs/1611.09811v1
 370. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref61
 371. https://arxiv.org/abs/1611.02064v2
 372. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref62
 373. https://arxiv.org/abs/1611.04534v1
 374. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref63
 375. https://arxiv.org/abs/1612.05360v2
 376. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref64
 377. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref65
 378. https://github.com/alexjc/neural-enhance
 379. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref66
 380. https://arxiv.org/abs/1611.05250v1
 381. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref67
 382. https://arxiv.org/abs/1609.05158v2
 383. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref68
 384. https://arxiv.org/abs/1606.01299v3
 385. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref69
 386. https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html
 387. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref70
 388. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref71
 389. https://arxiv.org/abs/1609.04802v3
 390. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref72
 391. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref73
 392. https://arxiv.org/abs/1610.04490v1
 393. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref74
 394. https://prisma-ai.com/
 395. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref75
 396. https://services.artomatix.com/
 397. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref76
 398. https://arxiv.org/abs/1508.06576v2
 399. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref77
 400. https://arxiv.org/abs/1602.07188v2
 401. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref78
 402. https://arxiv.org/abs/1604.08610v2
 403. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref79
 404. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref80
 405. https://code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-the-palm-of-your-hand/
 406. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref81
 407. https://research.googleblog.com/2016/10/supercharging-style-transfer.html
 408. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref82
 409. https://arxiv.org/abs/1610.07629v5
 410. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref83
 411. https://arxiv.org/abs/1603.08511v5
 412. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref84
 413. https://arxiv.org/abs/1603.06668v2
 414. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref85
 415. http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/
 416. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref86
 417. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref87
 418. https://arxiv.org/abs/1604.04494v1
 419. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref88
 420. https://arxiv.org/abs/1611.02155v1
 421. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref89
 422. https://arxiv.org/abs/1504.08023v2
 423. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref90
 424. https://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621
 425. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref91
 426. https://arxiv.org/abs/1604.06182v1
 427. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref92
 428. https://arxiv.org/abs/1607.00662v1
 429. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref93
 430. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref94
 431. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref95
 432. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref96
 433. https://arxiv.org/abs/1611.05009v3
 434. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref97
 435. http://cvgl.stanford.edu/projects/objectnet3d/
 436. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref98
 437. https://arxiv.org/abs/1604.00449v1
 438. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref99
 439. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref100
 440. https://arxiv.org/abs/1612.05872v1
 441. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref101
 442. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref102
 443. https://arxiv.org/abs/1607.00662v1
 444. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref103
 445. https://blog.acolyer.org/2017/01/05/unsupervised-learning-of-3d-structure-from-images/
 446. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref104
 447. http://mscoco.org/dataset/#keypoints-challenge2016
 448. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref105
 449. http://www.eccv2016.org/
 450. http://www.eccv2016.org/main-conference/
 451. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref106
 452. https://arxiv.org/abs/1611.08050v1
 453. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref107
 454. https://www.youtube.com/watch?v=pw6nzxewlgm
 455. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref108
 456. https://arxiv.org/abs/1607.08128v1
 457. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref109
 458. https://www.youtube.com/watch?v=eunz2rjxgae
 459. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref110
 460. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref111
 461. http://www.samehkhamis.com/dou-siggraph2016.pdf
 462. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref112
 463. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref113
 464. https://www.youtube.com/watch?v=2dkcj1yhyw4&feature=youtu.be
 465. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref114
 466. https://www.youtube.com/watch?v=7d59o6cfam0&feature=youtu.be
 467. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref115
 468. http://www.doc.ic.ac.uk/
 469. https://www.doc.ic.ac.uk/~ajd/publications/kim_etal_eccv2016.pdf
 470. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref116
 471. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref117
 472. http://www.doc.ic.ac.uk/
 473. https://www.doc.ic.ac.uk/~ajd/publications/kim_etal_bmvc2014.pdf
 474. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref118
 475. https://www.youtube.com/watch?v=yhlyhdmsw7w
 476. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref119
 477. https://arxiv.org/abs/1603.04992v2
 478. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref120
 479. https://arxiv.org/abs/1608.05137v1
 480. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref121
 481. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref122
 482. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref123
 483. https://arxiv.org/abs/1612.07217v1
 484. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref124
 485. http://davischallenge.org/
 486. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref125
 487. https://arxiv.org/abs/1606.03798v1
 488. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref126
 489. https://arxiv.org/abs/1607.07405v3
 490. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref127
 491. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 492. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref128
 493. https://get.google.com/tango/
 494. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref129
 495. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref130
 496. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 497. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref131
 498. https://arxiv.org/abs/1602.07261v2
 499. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref132
 500. https://arxiv.org/abs/1512.00567v3
 501. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref133
 502. https://arxiv.org/abs/1608.06993v3
 503. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref134
 504. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref135
 505. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref136
 506. https://github.com/liuzhuang13/densenet
 507. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref137
 508. https://arxiv.org/abs/1605.07648v2
 509. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref138
 510. https://arxiv.org/abs/1608.06993v3
 511. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref139
 512. https://arxiv.org/abs/1608.06037v3
 513. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref140
 514. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref141
 515. https://arxiv.org/abs/1605.06465v1
 516. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref142
 517. https://arxiv.org/abs/1602.07360v4
 518. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref143
 519. https://arxiv.org/abs/1603.05201v2
 520. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref144
 521. https://arxiv.org/abs/1511.07289v5
 522. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref145
 523. https://arxiv.org/abs/1605.09332v3
 524. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref146
 525. https://arxiv.org/abs/1612.04642v1
 526. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref147
 527. https://arxiv.org/abs/1602.07576v3
 528. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref148
 529. https://arxiv.org/abs/1602.02660v2
 530. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref149
 531. https://arxiv.org/abs/1612.08498v1
 532. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref150
 533. https://arxiv.org/abs/1609.05672v3
 534. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref151
 535. https://arxiv.org/abs/1512.03385v1
 536. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref152
 537. http://www.quora.com/
 538. https://www.quora.com/what-is-an-intuitive-explanation-of-deep-residual-networks
 539. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref153
 540. https://arxiv.org/abs/1605.07146v3
 541. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref154
 542. https://arxiv.org/abs/1603.09382v3
 543. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref155
 544. https://arxiv.org/abs/1611.01260v2
 545. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref156
 546. https://arxiv.org/abs/1605.06431v2
 547. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref157
 548. https://arxiv.org/abs/1603.05027v3
 549. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref158
 550. https://arxiv.org/abs/1609.05672v3
 551. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref159
 552. https://arxiv.org/abs/1612.07771v3
 553. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref160
 554. https://arxiv.org/abs/1609.05672v4
 555. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref161
 556. https://arxiv.org/abs/1603.08029v1
 557. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref162
 558. https://arxiv.org/abs/1611.10080v1
 559. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref163
 560. https://arxiv.org/abs/1604.03640v1
 561. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref164
 562. https://arxiv.org/abs/1606.05262v3
 563. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref165
 564. https://arxiv.org/abs/1611.04231v2
 565. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref166
 566. https://arxiv.org/abs/1604.04112v4
 567. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref167
 568. https://arxiv.org/abs/1605.08831v1
 569. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref168
 570. https://twitter.com/benhamner/status/789909204832227329
 571. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref169
 572. http://image-net.org/index
 573. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref170
 574. http://mscoco.org/
 575. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref171
 576. https://www.cs.toronto.edu/~kriz/cifar.html
 577. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref172
 578. http://yann.lecun.com/exdb/mnist/
 579. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref173
 580. http://places2.csail.mit.edu/
 581. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref174
 582. https://arxiv.org/abs/1612.05079v3
 583. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref175
 584. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref176
 585. https://arxiv.org/abs/1610.09003v1
 586. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref177
 587. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref178
 588. https://arxiv.org/abs/1607.08221v1
 589. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref179
 590. https://github.com/openimages/dataset
 591. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref180
 592. https://arxiv.org/abs/1609.08675v1
 593. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref181
 594. https://research.googleblog.com/2017/02/an-updated-youtube-8m-video.html
 595. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref182
 596. https://research.google.com/youtube8m/workshop.html
 597. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref183
 598. https://research.google.com/youtube8m/
 599. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref184
 600. http://newsroom.fb.com/news/2016/04/using-artificial-intelligence-to-help-blind-people-see-facebook/
 601. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref185
 602. https://www.wired.com/2016/01/2015-was-the-year-ai-finally-entered-the-everyday-world/
 603. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref186
 604. http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/
 605. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref187
 606. https://github.com/trailbehind/deeposm
 607. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref188
 608. https://code.facebook.com/posts/1676452492623525/connecting-the-world-with-better-maps/
 609. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref189
 610. https://www.amazon.com/b?node=16008589011
 611. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref190
 612. http://fortune.com/2017/03/28/amazon-go-cashier-free-store/
 613. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref191
 614. https://medium.com/twentybn/germany-asleep-at-the-wheel-d800445d6da2
 615. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref192
 616. https://arxiv.org/abs/1604.01325v2
 617. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref193
 618. https://arxiv.org/abs/1606.05718v1
 619. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref194
 620. http://mentalfloss.com/article/82415/ai-achieves-near-human-detection-breast-cancer
 621. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref195
 622. https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow
 623. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref196
 624. http://www.digit.in/
 625. http://www.digit.in/general/the-rise-of-vpus-giving-eyes-to-machines-29561.html
 626. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref197
 627. https://www.movidius.com/solutions/machine-vision-algorithms/machine-learning
 628. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref198
 629. http://www.hongkiat.com/blog/augmented-reality-smart-glasses/
 630. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref199
 631. https://get.google.com/tango/
 632. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref200
 633. https://arxiv.org/abs/1611.01599v2
 634. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref201
 635. https://arxiv.org/abs/1703.02528v1
 636. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref202
 637. http://www.tractica.com/
 638. https://www.tractica.com/newsroom/press-releases/computer-vision-hardware-and-software-market-to-reach-48-6-billion-by-2022/
 639. http://www.themtank.org/a-year-in-computer-vision#ftnt_ref203
 640. mailto:info@themtank.com
 641. mailto:info@themtank.com

   hidden links:
 643. https://arxiv.org/abs/1611.04534
