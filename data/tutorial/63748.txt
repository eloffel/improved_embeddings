   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

demystifying    matrix capsules with em routing.   

   [16]go to the profile of sahaj garg
   [17]sahaj garg (button) blockedunblock (button) followfollowing
   nov 22, 2017

   recently, geoffrey hinton, one of the fathers of deep learning, made
   waves in the machine learning community by publishing a revolutionary
   id161 architecture: id22. hinton has been pushing
   for using id22 since 2012, after he first revolutionized
   the use of convolutional neural networks (id98s) for image detection,
   but only now has he made them feasible. the initial successful
   approach, published two weeks ago, is titled    dynamic routing between
   capsules.    dynamic routing         which we   ll be exploring in depth
   throughout this post         allows networks to more intuitively understand
   part-whole relationships.

   in the three days following the release of this paper, another paper on
   dynamic routing in id22 was submitted for review to iclr
   2018. this paper, titled    matrix capsules for em routing,    is widely
   speculated to have been authored by hinton, and discusses a
   revolutionary new method for dynamic routing         even compared to his
   first paper. although this approach shows significant promise, there   s
   been relatively less media attention surrounding it thus far. because
   of the uncertainty of authorship, i refer to    the authors    for the rest
   of this post.

   in this blog post, i   ll be discussing the intuition behind dynamic
   routing for matrix capsules and the mechanism for dynamic routing. the
   first part will outline the intuition for id22 and how they
   leverage dynamic routing to be more effective than traditional id98s.
   the second part will take a deeper dive into the mathematical and
   technical details of em routing.

   (note: if you   ve read sabour et. al.,    dynamic routing between
   capsules,    the intuition is quite similar but the routing mechanism is
   very different.)

what   s wrong with id98s?

   since 2012, when hinton co-authored a paper introducing alexnet, a id98
   that performed better than any previous method on image classification
   for id163, id98s have been the standard method for id161.
   in essence, id98s work by detecting the presence of features in an
   image, and using the knowledge of these features to predict whether an
   object exists. however, id98s only detect the existence of features         so
   even the image to the right, which contains a misplaced eye, nose, and
   ear, would be considered a face because it consists of all the
   necessary features!
   [1*r5sjf4xtv9fapc7n39mpew.png]
   a id98 would classify both images as faces because they both contain the
   elements of faces. [18]source.

   the reason for id98s failure to deal with such images is the reason why
   hinton has criticized id98s for the last several years. the problem
   boils down to the way id98s pass information from layer to layer: max
   pooling. max pooling examines a grid of pixels, and takes the maximum
   activation in that region. this procedure essentially detects whether a
   feature is present in any region of the image, but loses spatial
   information about the feature.
   [1*pwduhurk7-u4abbezousha.png]
   max pooling preserves existence (maximum activation) in each region,
   but loses spatial information. [19]source.

   id22 attempt to address the limitations of id98s by
   capturing part-whole relationships. for example, in a face, each of the
   two eyes is part of a forehead. in order to understand these part-whole
   relationships, id22 need to know more than just whether a
   feature exists         it needs to know where the feature is, how it is
   oriented, and other similar information. id22 succeed in
   ways that id98s have not because they successfully capture this
   information and route information from parts to their wholes.

so, what exactly is a capsule network?

   a traditional neural network is made up of neurons. id22
   impose structure on a id98 by grouping neurons into modules called
   capsules.

   capsules are groups of neurons whose output represents different
   properties of the same feature. by grouping neurons, the output of a
   capsule is a 4x4 matrix (or 16 dimensional vector), whose entries
   represent information such as the x and y-coordinates of the feature,
   the angle of rotation of the object, and other characteristics. for
   example, in digit recognition, one layer of capsules correspond to
   digits. one capsule in the output layer may correspond to a 6.
   different entries in the 4x4 matrix will correspond to different
   properties of the 6. as seen below, if you modify one entry of the
   matrix, the 6 changes in scale and thickness, and if you modify another
   entry of the matrix, the top hook of the six changes.
   [1*wjhnme3wy5adyhvx87yrhq.png]
   this image, from hinton   s first paper, shows what happens to numbers
   when perturbing one dimension of the capsule output. this demonstrates
   spatial information about the numbers stored in each entry. [20]source.

   when thinking of neural networks, we think in terms of connections
   between neurons. although capsules are made up of neurons, it   s easier
   (and more effective) to think of the network as made up of components
   called capsules. the entire capsule network consists of layers of
   capsules. each layer may consist of any number of capsules (often
   8   32).

   it is intuitively helpful to compare capsules to neurons in a
   traditional neural network. a single neuron in a nn outputs an
   activation from (0,1), and represents the existence of a feature. on
   the other hand, a capsule outputs a logistic unit (0,1) about the
   existence of a feature as well as a 4x4 matrix representing additional
   information about the feature. this 4x4 matrix is called the pose
   matrix because it represents spatial information about a feature. the
   diagram above illustrates what happens when you modify entries in the
   pose matrix.

   in addition, we can consider neuron-neuron connections versus
   capsule-capsule connections, which would be represented by an single
   arrow in a typical diagram. in a nn, one neuron is connected to the
   next using a scalar weight. on the other hand, the connection between
   two capsules is now a 4x4 transformation matrix, because both the
   inputs and outputs of a capsule are 4x4 matrices!

   so far, id22 haven   t done anything revolutionary: after
   all, capsules are often just convolutional layers. the effectiveness of
   id22 comes from the way information is transferred from
   layer to layer         what   s called a routing mechanism. unlike id98s, which
   route information using max pooling, id22 use a new process
   called dynamic routing to capture part-whole relationships.

dynamic routing through agreement: from part to whole

   in this section, we   ll consider how to route outputs from layer (l) to
   layer (l+1). this procedure replaces max pooling from id98s. remember,
   as we discussed previously, our network learns transformation matrices
   (analogous to weights) as the connections between capsules. however, in
   addition to using these transformation matrices to route information
   from layer to layer, each connection is also multiplied by a routing
   coefficient that is dynamically computed. don   t worry if this isn   t
   clear yet; we   ll first move through the intuition behind these routing
   coefficients, and then discuss the math later.

   to make our analysis more concrete, we consider the example of digit
   recognition, and only classify the numbers 2 and 3. our hidden layer
   (l) consists of a capsule layer with six capsules, corresponding to
   features, and our final layer (l+1) consists of 2 capsules,
   corresponding to the numbers 2 or 3. the features in layer (l)
   represent different components of the numbers 2 and 3. suppose, for the
   sake of illustration, that our six capsule nodes to correspond to the
   following features. a 3 is composed of the first four features, and a 2
   is composed of the first, fifth, and sixth features. in our example,
   our network will have taken an input image of 3, and will try to
   classify it.
   [1*sdequsmcori2br1j70olyq.png]
   capsules are shown as nodes, but output matrices. arrows represent
   transformation matrices. red capsules are active, and black capsules
   are inactive. the green capsules in the output layer have not yet been
   predicted.

   intuitively, we know that every object in layer (l) comes from one of
   the higher-level objects in layer (l+1). we know that the first capsule
   is active in our network because it is either a 2 or a 3. our goal is
   to figure out the id203 that our feature is activated because of
   each feature in the next layer. based on why we think capsules in layer
   (l) are being activated, we can direct our information from layer (l)
   to (l+1).

   the question becomes how we decide these probabilities. let   s take the
   example of the first capsule feature, which could be a part of a 2 or a
   3. the number that it comes from depends largely on the spatial
   relationships between the first feature and other features in the
   layer. in our example, layer (l) sees the first feature (in the image)
   above the second feature above the third feature above the fourth.
   (note: our pose matrix outputs can capture these spatial relationships
   between the features). so, layer (l) has seen all the information about
   a 3 with the correct relationships. if so, then there is significant
   agreement among layer (l) that a 3 is present. at the same time, the
   two other nodes for 2 are not active, so there is relatively less
   agreement that there is a two in the picture. based on this agreement,
   we can say with high id203 that the first feature actually came
   from a 3, not a 2.

   this overall concept is called routing by agreement. by doing this,
   information from capsules in layer (l) is only supplied to layer (l+1)
   if the feature appears to have come from that capsule in the subsequent
   layer. in other words, the 2 capsule only receives information from the
   first feature if there   s more agreement about the 2. this procedure of
   dynamically computing which higher-level capsules to send information
   to is called dynamic routing. by dynamically routing the first feature
   (top hook) only to the 3, we can be more confident about our prediction
   and assign low id203 to a 2.
   [1*_gr1gfebvj0or-orht16nq.png]
   transformation matrices capture the fixed relationships between part
   and whole

   here   s the trickiest part of dynamic routing: these routing
   coefficients are not learned parts of a network! the next three
   paragraphs will explain why. our learned weights in id22
   are the transformation matrices         analogous to weights in a nn. these
   transformation matrices can capture how each feature is related to the
   whole. for example, this can teach us that the first feature is the top
   of a 3 or the top of a 2, that the second feature is the middle-right
   of a 3, and so on.

   however, we don   t know whether the first feature is part of a 3 or a 2,
   given the individual image. if there   s significant agreement that the
   different parts of a 3 relate properly to the whole, then we think the
   first feature should be routed to a 3. other times, though, we might
   think its part of a two. as a result, it depends on the individual
   image whether the first feature is part of a 2 or a 3. these routing
   coefficients are consequently not fixed weights; they are learned every
   single forward pass of the network, and depend on the individual image.
   this is why the procedure is called dynamic routing         it is part of the
   forward pass.
   [1*sa10x7ghgen3bcgctvyata.png]
   a jumbled 3. it has all the features of a 3, but is not correctly
   aligned.

   let   s consider how this fixes the example when features aren   t in the
   right places, which had caused a problem for id98s. the dynamic routing
   process is what helps fix this issue. consider when the four features
   of a 3 are randomly placed in an input, as to the left. a convolutional
   network would consider this a 3, because it has the four components of
   a 3. however, in a capsule network, the    3 capsule    would receive
   information from the hidden layer saying that the parts of the 3 do not
   properly relate to form a 3! so, when the first capsule is deciding
   where to route its output, it does not see agreement for either the 2
   or the 3, and sends its output equally. as a result, the capsule
   network does not predict either a 2 or a 3 with confidence. hopefully,
   this shows why the routing must by dynamic, because it depends on the
   poses of the features activated by the capsule network.

   as we showed above, this routing is only possible because the output of
   each capsule is a 4x4 matrix. these matrices capture the spatial
   relationships between the different parts of a 3, and if they do not
   agree correctly as parts of a whole, the capsule network does not route
   information with confidence. on the other hand, with id98s, scalar
   activations cannot capture the relationship between each part and the
   whole.

   another way to frame dynamic routing is the concept of voting. each
   capsule in layer (l) can vote which capsule in layer (l+1) it thinks it
   came from. the capsule gets a total of one vote, and can distribute
   this vote fractionally among all subsequent layer capsules. this voting
   mechanism ensures effective routing of information based on spatial
   relationships. the voting/routing mechanism comes from the procedure
   that is discussed below.

em routing: overall idea

   we   ve vaguely outlined the intuition behind dynamic routing, but have
   said nothing about how to compute it. this topic is lengthy and
   complex, and will be covered in full mathematical detail in the next
   part of this series. here, i   ll outline the basic idea behind em
   routing.

   essentially, when routing, we are making the assumption that each
   capsule in layer (l) is activated because it is part of some    whole    in
   the next layer. in this step, we assume there is some latent variable
   that explains which    whole    our information came from, and are trying
   to infer the id203 that each matrix output came from the
   higher-order feature in level (l+1).

   this boils down to an unsupervised learning problem, and we tackle it
   with an algorithm called expectation maximization. in essence,
   expectation maximization attempts to maximize the likelihood that our
   data (outputs of layer (l)) is explained by the capsules in layer
   (l+1). expectation maximization is an iterative algorithm that consists
   of two steps: expectation, and maximization. typically, for capsule
   networks, each step is performed three times, and then terminated.
   [1*-tzpzp8mh5o5axqnct08ha.png]
   the output capsules converge toward the correct classes over the three
   iterations of em. [21]source.

   the authors explain the steps are as follows. expectation assigns
   probabilities that each capsule in layer (l) was explained by the
   capsules in layer (l+1). then, the maximization step decides whether
   the capsules in layer (l+1) are active, if they see significant
   id91 of inputs to it (agreement), and decides what the input is
   to the capsule by taking a weighted average of the routed inputs.

   don   t worry if this didn   t make sense         i explained a lot of dense math
   in very few words. if you want to understand the deep technicalities of
   the routing mechanism, stay tuned for the next blog post later this
   week.

capsule network architecture

   now that we understand the intuition behind id22, we can
   examine how the authors used matrix capsules in their paper    matrix
   capsules with em routing.   

   the paper evaluates matrix capsules on the smallnorb dataset. this
   dataset consists of five types of children   s toys photographed from
   different angles and elevations. the dataset accurately captures
   different viewpoints of the same object. recall that capsules are
   fundamentally better at understanding viewpoints because of the
   multidimensional output of capsules (pose matrices)    this is why the
   smallnorb dataset is an excellent evaluation metric for the goals of
   id22.
   [1*cedrq3r0h2ygocd_ywptgq.png]
   example images from the smallnorb dataset. the second row consists of
   photos taken from a different elevation from the first set. [22]source.

   the architecture consists of a convolutional layer, followed by two
   capsule layers, and a final classification capsule layer with five
   capsules, one per class. this architecture performs better than any
   previous model on the dataset and cuts the previous best error rate by
   over 45%!
   [1*uvdimcxv0nxcucrfdgr1xg.png]
   architecture in    matrix capsules with em routing.    see the [23]paper
   for more details.

advantages of id22

   id22 have four major conceptual advantages over id98s.
    1. viewpoint invariance. as discussed throughout this piece, the use
       of pose matrices allows id22 to recognize objects
       regardless of the viewpoint from which they are viewed.
    2. fewer parameters. because matrix capsules group neurons, the
       connections between layers require fewer parameters. the model that
       achieved the best performance only included 300,000 parameters, as
       compared with the 4,000,000 parameters of their baseline id98. this
       means the model can generalize better.
    3. better generalization to new viewpoints. id98s, when trained to
       understand rotations, often memorize that an object can be viewed
       similarly from several different rotations. however, capsule
       networks also generalize better to new viewpoints because pose
       matrices can capture these characteristics as mere linear
       transformations.
    4. defense against white-box adversarial attacks. the fast gradient
       sign method (fgsm) is a typical method for attacking id98s. it
       evaluates the gradient of each pixel against the loss of the
       network, and changes each pixel by at most epsilon to maximize the
       loss. although this method can drop the accuracy of id98s
       dramatically, to below 20%, id22 maintain an accuracy
       of above 70%.

   in this post, i discuss some details specific to matrix capsules. the
   details of using matrix capsules with em routing, instead of using
   vector capsules in the initial paper, also present three major
   benefits. i explain the technical details of these three benefits in my
   next post.

the future of id161

   it   s been two weeks since id22 have been introduced, and
   nobody knows whether they   re another model that people will forget, or
   if they will revolutionize id161. in their current state,
   they have plenty of drawbacks         most notably the extensive time
   required to dynamically route using em from layer to layer. this can be
   especially problematic when parallelizing over gpus.

   regardless, this paper effectively acted as a proof of concept for
   id22, and did so quite elegantly. in the future, though,
   their efficacy must be tested on a wide variety of datasets, including
   more applicable ones with many classes, such as id163. over time,
   new routing mechanisms may be introduced, and other innovations may
   make capsule nets more efficient (we   ve already seen this in    matrix
   capsules with em routing,    which completely changed the routing
   mechanism!). if they work, though, the applications are endless         to
   image classification, id164, dense captioning, generative
   networks, and few-shot learning.

   acknowledgements         a huge thanks to shubhang desai, adi ganesh, jeffrey
   chang, and jordan alexander for giving me excellent feedback on this
   post. check out the original papers [24]here and [25]here.

     * [26]artificial intelligence
     * [27]machine learning
     * [28]deep learning
     * [29]id161
     * [30]towards data science

   (button)
   (button)
   (button) 781 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of sahaj garg

[32]sahaj garg

   obsessed with machine learning, calvin & hobbes, and optimization    
   stanford cs    20

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 781
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2126133a8457
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/demystifying-matrix-capsules-with-em-routing-part-1-overview-2126133a8457&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/demystifying-matrix-capsules-with-em-routing-part-1-overview-2126133a8457&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_j1kkzfzcs6mo---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sahajg?source=post_header_lockup
  17. https://towardsdatascience.com/@sahajg
  18. http://sharenoesis.com/wp-content/uploads/2010/05/7shapefaceremoveguides.jpg
  19. https://upload.wikimedia.org/wikipedia/commons/e/e9/max_pooling.png
  20. https://arxiv.org/abs/1710.09829
  21. https://openreview.net/pdf?id=hjwlfgwrb
  22. https://openreview.net/pdf?id=hjwlfgwrb
  23. https://openreview.net/pdf?id=hjwlfgwrb
  24. https://arxiv.org/abs/1710.09829
  25. https://openreview.net/pdf?id=hjwlfgwrb
  26. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  27. https://towardsdatascience.com/tagged/machine-learning?source=post
  28. https://towardsdatascience.com/tagged/deep-learning?source=post
  29. https://towardsdatascience.com/tagged/computer-vision?source=post
  30. https://towardsdatascience.com/tagged/towards-data-science?source=post
  31. https://towardsdatascience.com/@sahajg?source=footer_card
  32. https://towardsdatascience.com/@sahajg
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/2126133a8457/share/twitter
  39. https://medium.com/p/2126133a8457/share/facebook
  40. https://medium.com/p/2126133a8457/share/twitter
  41. https://medium.com/p/2126133a8457/share/facebook
