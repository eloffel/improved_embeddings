   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

estimating an optimal learning rate for a deep neural network

   [16]go to the profile of pavel surmenok
   [17]pavel surmenok (button) blockedunblock (button) followfollowing
   nov 12, 2017
   [1*ymwavxyarjsn8oltsirr9a.jpeg]
   source:
   [18]https://www.hellobc.com.au/british-columbia/things-to-do/winter-act
   ivities/skiing-snowboarding.aspx

   the learning rate is one of the most important hyper-parameters to tune
   for training deep neural networks.

   in this post, i   m describing a simple and powerful way to find a
   reasonable learning rate that i learned from [19]fast.ai deep learning
   course. i   m taking the new version of the course in person at
   [20]university of san francisco. it   s not available to the general
   public yet, but will be at the end of the year at [21]course.fast.ai
   (which currently has the last year   s version).

how does learning rate impact training?

   deep learning models are typically trained by a stochastic gradient
   descent optimizer. there are many variations of stochastic gradient
   descent: adam, rmsprop, adagrad, etc. all of them let you set the
   learning rate. this parameter tells the optimizer how far to move the
   weights in the direction opposite of the gradient for a mini-batch.

   if the learning rate is low, then training is more reliable, but
   optimization will take a lot of time because steps towards the minimum
   of the id168 are tiny.

   if the learning rate is high, then training may not converge or even
   diverge. weight changes can be so big that the optimizer overshoots the
   minimum and makes the loss worse.
   [1*ep8stdfdu_oxzfgimczrtq.jpeg]
   id119 with small (top) and large (bottom) learning rates.
   source: andrew ng   s machine learning course on coursera

   the training should start from a relatively large learning rate
   because, in the beginning, random weights are far from optimal, and
   then the learning rate can decrease during training to allow more
   fine-grained weight updates.

   there are multiple ways to select a good starting point for the
   learning rate. a naive approach is to try a few different values and
   see which one gives you the best loss without sacrificing speed of
   training. we might start with a large value like 0.1, then try
   exponentially lower values: 0.01, 0.001, etc. when we start training
   with a large learning rate, the loss doesn   t improve and probably even
   grows while we run the first few iterations of training. when training
   with a smaller learning rate, at some point the value of the loss
   function starts decreasing in the first few iterations. this learning
   rate is the maximum we can use, any higher value doesn   t let the
   training converge. even this value is too high: it won   t be good enough
   to train for multiple epochs because over time the network will require
   more fine-grained weight updates. therefore, a reasonable learning rate
   to start training from will be probably 1   2 orders of magnitude lower.

there must be a smarter way

   leslie n. smith describes a powerful technique to select a range of
   learning rates for a neural network in section 3.3 of the 2015 paper
      [22]cyclical learning rates for training neural networks    .

   the trick is to train a network starting from a low learning rate and
   increase the learning rate exponentially for every batch.
   [1*zgm3iy7ad4zsxliva0xtfg.png]
   learning rate increases after each mini-batch

   record the learning rate and training loss for every batch. then, plot
   the loss and the learning rate. typically, it looks like this:
   [1*hvj_4lwemjvowv-cqo9y9g.png]
   the loss decreases in the beginning, then the training process starts
   diverging

   first, with low learning rates, the loss improves slowly, then training
   accelerates until the learning rate becomes too large and loss goes up:
   the training process diverges.

   we need to select a point on the graph with the fastest decrease in the
   loss. in this example, the id168 decreases fast when the
   learning rate is between 0.001 and 0.01.

   another way to look at these numbers is calculating the rate of change
   of the loss (a derivative of the id168 with respect to
   iteration number), then plot the change rate on the y-axis and the
   learning rate on the x-axis.
   [1*eyewkhrqrygg7usnnax0hg.png]
   rate of change of the loss

   it looks too noisy, let   s smooth it out using simple moving average.
   [1*87mkq_xomyyje29l91k0dw.png]
   rate of change of the loss, simple moving average

   this looks better. on this graph, we need to find the minimum. it is
   close to lr=0.01.

implementation

   jeremy howard and his team at [23]usf data institute developed
   [24]fast.ai, a deep learning library that is a high-level abstraction
   on top of pytorch. it   s an easy to use and yet powerful toolset for
   training state of the art deep learning models. jeremy uses the library
   in the latest version of the deep learning course ([25]fast.ai).

   the library provides an implementation of the learning rate finder. you
   need just two lines of code to plot the loss over learning rates for
   your model:

   iframe: [26]/media/bd54ce4eee1131fa74295d319dc561ae?postid=ce32f2556ce0

   the library doesn   t have the code to plot the rate of change of the
   id168, but it   s trivial to calculate:

   iframe: [27]/media/b5d6aea838d6d305c8731f36ee60b4bd?postid=ce32f2556ce0

   note that selecting a learning rate once, before training, is not
   enough. the optimal learning rate decreases while training. you can
   rerun the same learning rate search procedure periodically to find the
   learning rate at a later point in the training process.

implementing the method using other libraries

   i haven   t seen ready to use implementations of this learning rate
   search method for other libraries like keras, but it should be trivial
   to write. just run the training multiple times, one mini-batch at a
   time. increase the learning rate after each mini-batch by multiplying
   it by a small constant. stop the procedure when the loss gets a lot
   higher than the previously observed best value (e.g., when current loss
   > best loss * 4).

there is more to it

   selecting a starting value for the learning rate is just one part of
   the problem. another thing to optimize is the learning schedule: how to
   change the learning rate during training. the conventional wisdom is
   that the learning rate should decrease over time, and there are
   multiple ways to set this up: step-wise learning rate annealing when
   the loss stops improving, exponential learning rate decay, cosine
   annealing, etc.

   [28]the paper that i referenced above describes a novel way to change
   the learning rate cyclically. this method improves performance of
   convolutional neural networks on a variety of image classification
   tasks.

   please send me a message if you know other interesting tips and tricks
   for training deep neural networks.
     __________________________________________________________________

   see also:
   [29]fast.ai: what i learned from lessons 1   3
   fast.ai is a great deep learning course for those who prefer to learn
   by doing. unlike other courses, here you will   hackernoon.com
   [30]best sources of deep learning news
   the field of deep learning is very active, arguably there are one or
   two breakthroughs every week. research papers   medium.com
   [31]jeff dean   s talk on large-scale deep learning
   jeff dean is a google senior fellow. he leads the google brain project.
   he spoke at y combinator in august 2017. the   becominghuman.ai

     * [32]machine learning
     * [33]deep learning
     * [34]artificial intelligence
     * [35]neural networks

   (button)
   (button)
   (button) 2.7k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of pavel surmenok

[37]pavel surmenok

   machine learning engineering and self-driving cars. opinions expressed
   are solely my own and do not express the views or opinions of my
   employer.

     (button) follow
   [38]towards data science

[39]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.7k
     * (button)
     *
     *

   [40]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [41]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ce32f2556ce0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_0rukh9masnjz---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@surmenok?source=post_header_lockup
  17. https://towardsdatascience.com/@surmenok
  18. https://www.hellobc.com.au/british-columbia/things-to-do/winter-activities/skiing-snowboarding.aspx
  19. http://www.fast.ai/
  20. https://www.usfca.edu/data-institute/certificates/deep-learning-part-one
  21. http://course.fast.ai/
  22. https://arxiv.org/abs/1506.01186
  23. https://www.usfca.edu/data-institute
  24. https://github.com/fastai/fastai
  25. http://www.fast.ai/
  26. https://towardsdatascience.com/media/bd54ce4eee1131fa74295d319dc561ae?postid=ce32f2556ce0
  27. https://towardsdatascience.com/media/b5d6aea838d6d305c8731f36ee60b4bd?postid=ce32f2556ce0
  28. https://arxiv.org/abs/1506.01186
  29. https://hackernoon.com/fast-ai-what-i-learned-from-lessons-1-3-b10f9958e3ff
  30. https://medium.com/@surmenok/best-sources-of-deep-learning-news-fbc98815bad3
  31. https://becominghuman.ai/jeff-deans-talk-on-large-scale-deep-learning-171fb8c8ac57
  32. https://towardsdatascience.com/tagged/machine-learning?source=post
  33. https://towardsdatascience.com/tagged/deep-learning?source=post
  34. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  35. https://towardsdatascience.com/tagged/neural-networks?source=post
  36. https://towardsdatascience.com/@surmenok?source=footer_card
  37. https://towardsdatascience.com/@surmenok
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://hackernoon.com/fast-ai-what-i-learned-from-lessons-1-3-b10f9958e3ff
  44. https://medium.com/@surmenok/best-sources-of-deep-learning-news-fbc98815bad3
  45. https://becominghuman.ai/jeff-deans-talk-on-large-scale-deep-learning-171fb8c8ac57
  46. https://medium.com/p/ce32f2556ce0/share/twitter
  47. https://medium.com/p/ce32f2556ce0/share/facebook
  48. https://medium.com/p/ce32f2556ce0/share/twitter
  49. https://medium.com/p/ce32f2556ce0/share/facebook
