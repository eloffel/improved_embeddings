   #[1]aylien    id27s and their challenges comments feed
   [2]alternate [3]alternate

   [4]products [5]research [6]blog [7]company [8]contact

   [9]blog

   [10]general [11]product [12]data science [13]research [14]get started

   from the blog
   product
   product updates and how-to guides for our text analysis api, news api,
   and text analysis platform
   data science
   using text and image analysis for fun and insightful data science
   projects
   research
   the latest updates from our research team.
   general
   company updates & news
   from the aylien.com

   [15]products [16]research [17]blog [18]company [19]contact

   [20]parsa ghaffari
   15 jun, 2016
   research

id27s and their challenges

   in recent times deep learning techniques have become more and more
   prevalent in nlp tasks; just take a look at the list of [21]accepted
   papers at this year   s naacl conference, and you can   t miss it. we   ve
   now completely moved away from traditional nlp approaches to focus on
   deep learning and how it can be leveraged in language problems, as
   successfully as it has in both image and audio recognition tasks.

   one of these approaches that has seen great success and is backed by a
   wave of research papers and funding is the concept of id27s.

id27s

   for those of you who aren   t familiar with them, [22]id27s are
   essentially dense vector representations of words.

   similar to the way a painting might be a representation of a person, a
   id27 is a representation of a word, using real-valued
   numbers. they are an arrangement of numbers representing the semantic
   and syntactic information of words and their context, in a format that
   computers can understand.

   here   s a nice little primer you should read if you   re looking for a
   more in depth description:
   [23]http://sebastianruder.com/word-embeddings-1/index.html

   id27s can be trained and used to derive similarities and
   relations between words. this means that by encoding each word as a
   small set of unique digits, say 100, 200 digits or more even that
   represent the word    mother    and another set of digits that represent
      father    we can better understand the context of that word.
   [tumblr_inline_o8tim6jds41u37g00_540.png]


   source:
   [24]https://www.tensorflow.org/versions/r0.9/tutorials/id97/index.h
   tml

   word vectors created through this process manifest interesting
   characteristics that almost look and sound like magic at first. for
   instance, if we subtract the vector of man from the vector of king, the
   result will be almost equal to the vector resulting from subtracting
   woman from queen. even more surprisingly, the result of subtracting
   walked from walking almost equates to that of swam minus swimming.
   these examples show that the model has not only learnt the meaning and
   the semantics of these words, but also the syntax and the grammar to
   some degree.


   [tumblr_inline_o8tinsmw081u37g00_540.png]


   relations between words according to id27s

   source:
   [25]https://www.tensorflow.org/versions/r0.9/tutorials/id97/index.h
   tml

   as our very own nlp research scientist, [26]sebastian ruder, explains
   that    id27s are one of the few currently successful
   applications of unsupervised learning. their main benefit arguably is
   that they don   t require expensive annotation, but can be derived from
   large unannotated corpora that are readily available. pre-trained
   embeddings can then be used in downstream tasks that use small amounts
   of labeled data.   

   although id27s have almost become the de facto input layer in
   many nlp tasks, they do have some drawbacks. let   s take a look at some
   of the challenges we face with id97, probably the most popular and
   commercialized model used today.

id97 challenges

inability to handle unknown or oov words

   perhaps the biggest problem with id97 is the inability to handle
   unknown or out-of-vocabulary (oov) words.

   if your model hasn   t encountered a word before, it will have no idea
   how to interpret it or how to build a vector for it. you are then
   forced to use a random vector, which is far from ideal. this can
   particularly be an issue in domains like twitter where you have a lot
   of noisy and sparse data, with words that may only have been used once
   or twice in a very large corpus.

no shared representations at sub-word levels

   there are no shared representations at sub-word levels with id97.
   for example, you and i might encounter a new word that ends in    less   ,
   and from our knowledge of words that end similarly we can guess that
   it   s probably an adjective indicating a lack of something, like
   flawless or careless.

   id97 represents every word as an independent vector, even though
   many words are morphologically similar, just like our two examples
   above.

   this can also become a challenge in morphologically rich, and
   polysynthetic languages such as arabic, german or turkish.

scaling to new languages requires new embedding matrices

   scaling to new languages requires new embedding matrices and does not
   allow for parameter sharing, meaning cross-lingual use of the same
   model isn   t an option.

cannot be used to initialize state-of-the-art architectures

   as explained earlier, pre-training id27s on weakly supervised
   or unsupervised data has become increasingly popular, as have various
   state-of-the-art architectures that take character sequences as input.
   if you have a model that takes character-based input, you normally
   can   t leverage the benefits of pre-training, which forces you to
   randomize embeddings.

summary

   so while the application of deep learning techniques like word
   embeddings and id97 in particular have brought about great
   improvements and advancements in nlp, they are not without their flaws.

   at aylien, representation learning, the wider field id27s
   comes under, is an active area of research for us. our scientists are
   actively working on better embedding models and approaches for
   overcoming some of the challenges we mentioned.

   stay tuned for some exciting updates over the next few weeks ;).


                       [27]text analysis api - sign up

   stay informed
   subscribe to our newsletter to recieve regular updates about aylien,
   direct to your inbox.
   [28]parsa ghaffari
   ceo and founder of aylien
   parsa is an ai, machine learning and nlp enthusiast, whose aim is to
   make these techniques and technologies more accessible and easier to
   use for developers and data scientists. when he   s not working he likes
   to play chess ('parsabg' on lichess.org).
   twitter: @parsaghaffari
   read next
   [29]noel bambrick
   [30]feature bundle     text analysis for customer support
   17 jun, 2016
   feature bundle     text analysis for customer support
   related articles
   [31]sebastian ruder
   3 aug, 2017
   [32]learning to select data for id21

   in machine learning, the traditional assumption is that the data our
   model is applied to is the same as the data we used for training. this
   assumption is proven false [   ]
   [33]read    
   if you love your code, set it free! how and why you should open source
   your next project
   [34]afshin mehrabani
   12 jan, 2017
   [35]if you love your code, set it free! how and why you should open
   source your next project

   introduction in this post, software engineer at aylien, afshin
   mehrabani talks us through his experience in building and launching an
   incredibly popular open source library called intro.js. 4 years ago,
   [   ]
   [36]read    
   [37]sebastian ruder
   1 oct, 2018
   [38]a review of the neural history of natural language processing

   this is the first blog post in a two-part series. the series expands on
   the frontiers of natural language processing session organized by
   herman kamper and me at the deep [   ]
   [39]read    
   lisbon machine learning summer school highlights
   [40]sebastian ruder
   12 aug, 2016
   [41]lisbon machine learning summer school highlights

   from july 20th to july 28th 2016, i had the opportunity of  attending
   the 6th lisbon machine learning school. the lisbon machine learning
   school (lxmls) is an annual event that [   ]
   [42]read    
   let's talk

   products

   [43]text analysis api [44]news api [45]text analysis platform
   company

   [46]about us [47]in the news [48]jobs
   content

   [49]resources [50]blog
   get in touch

   [51]contact sales [52]press [53]email us
   stay informed

   [54]privacy policy [55]terms and conditions

   copyright    2018 aylien ltd. all rights reserved.

references

   visible links
   1. http://blog.aylien.com/word-embeddings-and-their-challenges/feed/
   2. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/word-embeddings-and-their-challenges/
   3. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/word-embeddings-and-their-challenges/&format=xml
   4. https://aylien.com/products
   5. https://aylien.com/research
   6. http://blog.aylien.com/
   7. https://aylien.com/about
   8. https://aylien.com/contact
   9. http://blog.aylien.com/
  10. http://blog.aylien.com/category/general/
  11. http://blog.aylien.com/category/product/
  12. http://blog.aylien.com/category/data-science/
  13. http://blog.aylien.com/category/research/
  14. https://aylien.com/products/
  15. https://aylien.com/products
  16. https://aylien.com/research
  17. http://blog.aylien.com/
  18. https://aylien.com/about
  19. https://aylien.com/contact
  20. http://blog.aylien.com/author/parsa/
  21. http://m-mitchell.com/naacl-2016/naacl-hlt2016/
  22. https://en.wikipedia.org/wiki/word_embedding
  23. http://sebastianruder.com/word-embeddings-1/index.html
  24. https://www.tensorflow.org/versions/r0.9/tutorials/id97/index.html
  25. https://www.tensorflow.org/versions/r0.9/tutorials/id97/index.html
  26. http://sebastianruder.com/word-embeddings-1/index.html
  27. http://cta-redirect.hubspot.com/cta/redirect/1942801/02bc9816-b470-4328-9625-fad8d92a8811
  28. http://blog.aylien.com/author/parsa/
  29. http://blog.aylien.com/author/noel/
  30. http://blog.aylien.com/feature-bundle-text-analysis-for-customer/
  31. http://blog.aylien.com/author/sebastian/
  32. http://blog.aylien.com/learning-select-data-transfer-learning/
  33. http://blog.aylien.com/learning-select-data-transfer-learning/
  34. http://blog.aylien.com/author/afshin/
  35. http://blog.aylien.com/if-you-love-your-code-set-it-free-how-and-why-you-should-open-source-your-next-project/
  36. http://blog.aylien.com/if-you-love-your-code-set-it-free-how-and-why-you-should-open-source-your-next-project/
  37. http://blog.aylien.com/author/sebastian/
  38. http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/
  39. http://blog.aylien.com/a-review-of-the-recent-history-of-natural-language-processing/
  40. http://blog.aylien.com/author/sebastian/
  41. http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/
  42. http://blog.aylien.com/lisbon-machine-learning-summer-school-highlights/
  43. https://aylien.com/text-api/
  44. https://aylien.com/news-api/
  45. https://aylien.com/text-analysis-platform/
  46. https://aylien.com/about/
  47. https://aylien.com/in-the-news/
  48. https://aylien.com/jobs/
  49. https://aylien.com/resources/
  50. http://blog.aylien.com/
  51. https://aylien.com/contact/
  52. https://aylien.com/in-the-news/
  53. https://aylien.com/contact/
  54. https://aylien.com/privacy/
  55. https://aylien.com/terms/

   hidden links:
  57. https://aylien.com/
  58. http://blog.aylien.com/?s=
  59. http://blog.aylien.com/?s=
  60. http://blog.aylien.com/category/product/
  61. http://blog.aylien.com/category/data-science/
  62. http://blog.aylien.com/category/research/
  63. http://blog.aylien.com/category/general/
  64. http://blog.aylien.com/word-embeddings-and-their-challenges/parsa@aylien.com
  65. https://twitter.com/_aylien
  66. https://www.linkedin.com/company-beta/1195911/
  67. https://www.facebook.com/aylieninc
