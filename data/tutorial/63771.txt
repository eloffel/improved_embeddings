   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a wizard   s guide to adversarial autoencoders: part 1, autoencoder?

   [16]go to the profile of naresh nagabushan
   [17]naresh nagabushan (button) blockedunblock (button) followfollowing
   jul 30, 2017
   [1*x457uwyab4_wx4g9zjel2q.png]

      if you know how to write a code to classify mnist digits using
   tensorflow, then you are all set to read the rest of this post or else
   i   d highly suggest you go through [18]this article on tensorflow   s
   website.   

        we know now that we don   t need any big new breakthroughs to get to
     true ai.

     that is completely, utterly, ridiculously wrong. as i   ve said in
     previous statements: most of human and animal learning is
     unsupervised learning. if intelligence was a cake, unsupervised
     learning would be the cake, supervised learning would be the icing
     on the cake, and id23 would be the cherry on the
     cake.

     we know how to make the icing and the cherry, but we don   t know how
     to make the cake. we need to solve the unsupervised learning problem
     before we can even think of getting to true ai. and that   s just an
     obstacle we know about. what about all the ones we don   t know
     about?   

   this is a quote from yan lecun (i know, another one from yan lecun),
   the director of ai research at facebook after [19]alphago   s victory.

   we know that a convolutional neural networks (id98s) or in some cases
   dense fully connected layers (mlp         multi layer id88 as some
   would like to call it) can be used to perform image recognition. but, a
   id98 (or mlp) alone cannot be used to perform tasks like content and
   style separation from an image, generate real looking images (a
   generative model), classify images using a very small set of labeled or
   perform data compression (like zipping a file).

   each of these tasks might require its own architecture and training
   algorithm. but, wouldn   t it be cool if we were able to implement all
   the above mentioned tasks using just one architecture. an adversarial
   autoencoder (one that trained in a semi-supervised manner) can perform
   all of them and more using just one architecture.

   we   ll build an adversarial autoencoder that can compress data (mnist
   digits in a lossy way), separate style and content of the digits
   (generate numbers with different styles), classify them using a small
   subset of labeled data to get high classification accuracy (about 95%
   using just 1000 labeled digits!) and finally also act as a generative
   model (to generate real looking fake digits).

   before we go into the theoretical and the implementation parts of an
   adversarial autoencoder, let   s take a step back and discuss about
   autoencoders and have a look at a simple tensorflow implementation.
     __________________________________________________________________

   [1*elielqiprdp4bgc63288kq.png]
   autoencoder architecture

   an autoencoder is a neural network that is trained to produce an output
   which is very similar to its input (so it basically attempts to copy
   its input to its output) and since it doesn   t need any targets
   (labels), it can be trained in an unsupervised manner.

   it has two parts:
    1. encoder: it takes in an input x (this can be an image, word
       embeddings, video or audio data) and produces an output h (where h
       usually has a lower dimensionality than x). for example, the
       encoder can take in an image x of size 100 x 100 and produce an
       output h (also known as the latent code) of size 100 x 1 (this can
       be any size). the encoder in this case just compresses the image
       such that it   ll occupy a lower dimensional space, on doing so we
       can now see that h (100 x 1 in size) could be stored using 100
       times less memory than directly storing the image x (this will
       result in some loss of data though).

   let   s think of a compression software like winrar (still on a free
   trial?) which can be used to compress a file to get a zip (or rar,   )
   file that occupies lower amounts of space. a similar operation is
   performed by the encoder in an autoencoder architecture.

   if the encoder is represented by the function q, then
   [1*mddxrlbv73zxzohkeoakwg.png]
   encoder

   2. decoder: it takes in the output of an encoder h and tries to
   reconstruct the input at its output. continuing from the encoder
   example, h is now of size 100 x 1, the decoder tries to get back the
   original 100 x 100 image using h. we   ll train the decoder to get back
   as much information as possible from h to reconstruct x.

   so, the decoder   s operation is similar to performing an unzipping on
   winrar.

   if the function p represents our decoder then the reconstructed image
   x_ is:
   [1*un49zl3mfalfi3mxtvzczw.png]
   decoder

   id84 works only if the inputs are correlated (like
   images from the same domain). it fails if we pass in completely random
   inputs each time we train an autoencoder. so in the end, an autoencoder
   can produce lower dimensional output (at the encoder) given an input
   much like principal component analysis ([20]pca). and since we don   t
   have to use any labels during training, it   s an unsupervised model as
   well.
     __________________________________________________________________

     but, what can autoencoders be used for other than dimensionality
     reduction?

     * image [21]denoising wherein a clear noise free image could be
       generated using a noisy one.

   [0*eapgsicjddsbcosd.png]
   denoising autoencoder example on handwritten digits. source:
   [22]https://www.doc.ic.ac.uk/~js4416/163/website/autoencoders/denoising
   .html
     * [23]semantic hashing where id84 could be used
       to make information retrieval faster (i found this very
       interesting!).
     * and recently where autoencoders trained in an adversarial manner
       could be used as generative models (we   ll go deeper into this
       later).

   i   ve divided this post into four parts:
     * part 1: autoencoders?

   we   ll start with an implementation of a simple autoencoder using
   tensorflow and reduce the dimensionality of mnist (you   ll definitely
   know what this dataset is about) dataset images.
     * part 2: exploring the latent space with adversarial autoencoders.

   we   ll introduce constraints on the latent code (output of the encoder)
   using adversarial learning.
     * part 3: disentanglement of style and content.

   here we   ll generate different images with the same style of writing.
     * part 4: classify mnist with 1000 labels.

   we   ll train an aae to classify mnist digits to get an accuracy of about
   95% using only 1000 labeled inputs (impressive ah?).
     __________________________________________________________________

   let   s begin part 1 by having a look at the network architecture we   ll
   need to implement.

   as stated earlier an autoencoder (ae) as two parts an encoder and a
   decoder, let   s begin with a simple dense fully connected encoder
   architecture:
   [1*hud7t2vly2jip3sxn4wtda.png]
   encoder architecture

   it consists of an input layer with 784 neurons (cause we have flattened
   the image to have a single dimension), two sets of 1000 relu activated
   neurons form the hidden layers and an output layer consisting of 2
   neurons without any activation provides the latent code.

   if you just want to get your hands on the code check out this link:
   [24]naresh1318/adversarial_autoencoder
   contribute to adversarial_autoencoder development by creating an
   account on github.github.com

   to implement the above architecture in tensorflow we   ll start off with
   a dense() function which   ll help us build a dense fully connected layer
   given input x, number of neurons at the input n1 and number of neurons
   at output n2. the name parameter is used to set a name for
   variable_scope. more on shared variables and using variable scope can
   be found [25]here (i   d highly recommend having a look at it).

   iframe: [26]/media/39599a2800c17139da84c91c513239ad?postid=d9a5f8795af4

   i   ve used tf.get_variable()instead of tf.variable()to create the weight
   and bias variables so that we can later reuse the trained model (either
   the encoder or decoder alone) to pass in any desired value and have a
   look at their output.

   next, we   ll use this dense() function to implement the encoder
   architecture. the code is straight forward, but note that we haven   t
   used any activation at the output.

   iframe: [27]/media/b35fb8154f7753c6ca6b18f42f3351a0?postid=d9a5f8795af4

     * reuse flag is used to reuse the trained encoder architecture.
     * here input_dim = 784, n_l1 = 1000, n_l2 = 1000, z_dim = 2 .

   the decoder is implemented in a similar manner, the architecture we   ll
   need is:
   [1*0t7jrvuqyzg7adqgdjzkrw.png]
   decoder architecture

   iframe: [28]/media/591a76a66e65241c2afadab38bfd53e4?postid=d9a5f8795af4

   again we   ll just use the dense() function to build our decoder.
   however, i   ve used sigmoid activation for the output layer to ensure
   that the output values range between 0 and 1 (the same range as our
   input).
     * z_dim = 2, n_l2 = 1000, n_l1 = 1000, input_dim = 784 same as the
       encoder.

   the encoder output can be connected to the decoder just like this:

   iframe: [29]/media/171819acb84423fd71e402ff8295cd89?postid=d9a5f8795af4

   this now forms the exact same autoencoder architecture as shown in the
   architecture diagram. we   ll pass in the inputs through the placeholder
   x_input (size: batch_size, 784), set target to be same as x_input and
   compare the decoder_output to x_input.

   the id168 used is the mean squared error (mse) which finds the
   distance between the pixels in the input (x_input) and the output image
   (decoder_output). we call this the reconstruction loss as our main aim
   is to reconstruct the input at the output.
   [1*d9boebmcephfdo2b92vmka.png]
   mean squared error

   this is nothing but the mean of the squared difference between the
   input and the output. which can easily be implemented in tensorflow as
   follows:

   iframe: [30]/media/c7a0b1b787ced3763861123e3c2aaf09?postid=d9a5f8795af4

   the optimizer i   ve used is the adamoptimizer (feel free to try out new
   ones, i   ve haven   t experimented on others) with a learning rate of 0.01
   and beta1 as 0.9. it   s directly available on tensorflow and can be used
   as follows:

   iframe: [31]/media/e421f5575116ca12cfe2bc8716ed4252?postid=d9a5f8795af4

   notice that we are backpropagating through both the encoder and the
   decoder using the same id168. (i could have changed only the
   encoder or the decoder weights using the var_list parameter under the
   minimize() method. since i haven   t mentioned any, it defaults to all
   the trainable variables.)

   lastly, we train our model by passing in our mnist images using a batch
   size of 100 and using the same 100 images as the target.

   iframe: [32]/media/9c0c19d80b00583dc7cd0b34d81633b8?postid=d9a5f8795af4

   the entire code is available on github:
   [33]naresh1318/adversarial_autoencoder
   contribute to adversarial_autoencoder development by creating an
   account on github.github.com

things to note:

     * the generate_image_grid() function generates a grid of images by
       passing a set of numbers to the trained decoder (this is where
       get_variable comes in handy).
     * each run generates the required tensorboard files under:

   ./results/<model>/<time_stamp_and_parameters>/tensorboard
     * the trainig logs are stored in:

   ./results/<model>/<time_stamp_and_parameters>/log/log.txt file.
     * set the train flag to true to train the model or set it to false to
       show the decoder output for some random input.

   i   ve trained the model for 200 epochs and shown the variation of loss
   and the generated images below:
   [1*-vrfb68o7d-0v27ppdruta.png]
   variation of reconstruction loss

   the reconstruction loss is reducing, which just what we want.
   [1*he1sxptnzxdyymhlpuzl2w.png]
   generated images

   notice how the decoder generalised the output 3 by removing small
   irregularities like the line on top of the input 3.

   now, what if we only consider the trained decoder and pass in some
   random numbers (i   ve passed 0, 0 as we only have a 2-d latent code) as
   it   s inputs, we should get some digits right?
   [1*cnlhhxzud4rqtee9jrg2pq.png]
   decoder output at (0, 0)

   but this doesn   t represent a clear digit at all (well, at least for
   me).

   the reason for this is because the encoder output does not cover the
   entire 2-d latent space (it has a lot of gaps in its output
   distribution). so if we feed in values that the encoder hasn   t fed to
   the decoder during the training phase, we   ll get weird looking output
   images. this can be overcome by constraining the encoder output to have
   a random distribution (say normal with 0.0 mean and a standard
   deviation of 2.0) when producing the latent code. this is exactly what
   an adversarial autoencoder is capable of and we   ll look into its
   implementation in part 2.

     have a look at the cover image again!!

     got it?
     __________________________________________________________________

   hope you liked this short article on autoencoders. i would openly
   encourage any criticism or suggestions to improve my work.

   if you think this content is worth sharing hit the       , i like the
   notifications it sends me!!

       part 2: [34]exploring latent space with adversarial autoencoders.

     * [35]machine learning
     * [36]artificial intelligence
     * [37]tensorflow
     * [38]unsupervised learning
     * [39]towards data science

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of naresh nagabushan

[41]naresh nagabushan

   master   s student [42]@virginia_tech interested in all things ai.

     (button) follow
   [43]towards data science

[44]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [45]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [46]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d9a5f8795af4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ac0zvglpfswj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rnaresh.n?source=post_header_lockup
  17. https://towardsdatascience.com/@rnaresh.n
  18. https://www.tensorflow.org/get_started/mnist/beginners
  19. https://en.wikipedia.org/wiki/alphago_versus_lee_sedol
  20. https://en.wikipedia.org/wiki/principal_component_analysis
  21. http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf
  22. https://www.doc.ic.ac.uk/~js4416/163/website/autoencoders/denoising.html
  23. http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/carreira-perpinan_hashing_with_binary_2015_cvpr_paper.pdf
  24. https://github.com/naresh1318/adversarial_autoencoder
  25. https://www.tensorflow.org/programmers_guide/variable_scope
  26. https://towardsdatascience.com/media/39599a2800c17139da84c91c513239ad?postid=d9a5f8795af4
  27. https://towardsdatascience.com/media/b35fb8154f7753c6ca6b18f42f3351a0?postid=d9a5f8795af4
  28. https://towardsdatascience.com/media/591a76a66e65241c2afadab38bfd53e4?postid=d9a5f8795af4
  29. https://towardsdatascience.com/media/171819acb84423fd71e402ff8295cd89?postid=d9a5f8795af4
  30. https://towardsdatascience.com/media/c7a0b1b787ced3763861123e3c2aaf09?postid=d9a5f8795af4
  31. https://towardsdatascience.com/media/e421f5575116ca12cfe2bc8716ed4252?postid=d9a5f8795af4
  32. https://towardsdatascience.com/media/9c0c19d80b00583dc7cd0b34d81633b8?postid=d9a5f8795af4
  33. https://github.com/naresh1318/adversarial_autoencoder/blob/master/autoencoder.py
  34. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9
  35. https://towardsdatascience.com/tagged/machine-learning?source=post
  36. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  37. https://towardsdatascience.com/tagged/tensorflow?source=post
  38. https://towardsdatascience.com/tagged/unsupervised-learning?source=post
  39. https://towardsdatascience.com/tagged/towards-data-science?source=post
  40. https://towardsdatascience.com/@rnaresh.n?source=footer_card
  41. https://towardsdatascience.com/@rnaresh.n
  42. http://twitter.com/virginia_tech
  43. https://towardsdatascience.com/?source=footer_card
  44. https://towardsdatascience.com/?source=footer_card
  45. https://towardsdatascience.com/
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://github.com/naresh1318/adversarial_autoencoder
  49. https://github.com/naresh1318/adversarial_autoencoder/blob/master/autoencoder.py
  50. https://medium.com/p/d9a5f8795af4/share/twitter
  51. https://medium.com/p/d9a5f8795af4/share/facebook
  52. https://medium.com/p/d9a5f8795af4/share/twitter
  53. https://medium.com/p/d9a5f8795af4/share/facebook
