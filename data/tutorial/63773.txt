
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [shutterstock_640998361.2e16d0ba.fill-1100x400_h6sxk8f.jpg]

population based training of neural networks

   neural networks have shown great success in everything from playing go
   and atari games to image recognition and language translation. but
   often overlooked is that the success of a neural network at a
   particular application is often determined by a series of choices made
   at the start of the research, including what type of network to use and
   the data and method used to train it. currently, these choices - known
   as hyperparameters - are chosen through experience, random search or a
   computationally intensive search processes.

   in our [35]most recent paper, we introduce a new method for training
   neural networks which allows an experimenter to quickly choose the best
   set of hyperparameters and model for the task. this technique - known
   as population based training (pbt) - trains and optimises a series of
   networks at the same time, allowing the optimal set-up to be quickly
   found. crucially, this adds no computational overhead, can be done as
   quickly as traditional techniques and is easy to integrate into
   existing machine learning pipelines.

   the technique is a hybrid of the two most commonly used methods for
   hyperparameter optimisation: random search and hand-tuning. in random
   search, a population of neural networks are trained independently in
   parallel and at the end of training the highest performing model is
   selected. typically, this means that  a small fraction of the
   population will be trained with good hyperparameters, but many more
   will be trained with bad ones, wasting computer resources.

   [fig%25201.width-400_kkikuop.png]
   random search of hyperparameters, where many hyperparameters are tried
   in parallel, but independently. some hyperparameters will lead to
   models with good performance, but others will not

   with hand tuning, researchers must guess at the best hyperparameters,
   train their models using them, and then evaluate the performance. this
   is done over and over, until the researcher is happy with the
   performance of the network. although this can result in better
   performance, the downside is that this takes a long time, sometimes
   taking weeks or even months to find the perfect set-up. and while there
   are ways of automating this process -  such as bayesian optimisation -
   it still takes a long time and requires many sequential training runs
   to find the best hyperparameters.

   [fig2_resize_2.width-400_15t8aye.png]
   methods like hand tuning and bayesian optimisation make changes to the
   hyperparameters by observing many training runs sequentially, making
   these methods slow

   pbt - like random search - starts by training many neural networks in
   parallel with random hyperparameters. but instead of the networks
   training independently, it uses information from the rest of the
   population to refine the hyperparameters and direct computational
   resources to models which show promise. this takes its inspiration from
   id107 where each member of the population, known as a
   worker, can exploit information from the remainder of the
   population. for example, a worker might copy the model parameters from
   a better performing worker. it can also explore new hyperparameters by
   changing the current values randomly.
   as the training of the population of neural networks progresses, this
   process of exploiting and exploring is performed periodically, ensuring
   that all the workers in the population have a good base level of
   performance and also that new hyperparameters are consistently
   explored.  this means that pbt can quickly exploit good
   hyperparameters, can dedicate more training time to promising models
   and, crucially, can adapt the hyperparameter values throughout
   training, leading to automatic learning of the best configurations.

   [fig3_712.width-400_xmpr1ei.png]
   population based training of neural networks starts like random search,
   but allows workers to exploit the partial results of other workers and
   explore new hyperparameters as training progresses

   our experiments show that pbt is very effective across a whole host of
   tasks and domains. for example, we rigorously tested the algorithm on a
   suite of challenging id23 problems with
   state-of-the-art methods on deepmind lab, atari, and starcraft ii. in
   all cases, pbt stabilised training, quickly found good hyperparameters,
   and delivered results that were beyond state-of-the-art baselines.

   [fig4.width-400_ujycvqz.png]

   we have also found pbt to be effective for training generative
   adversarial network (gan), which are notoriously difficult to tune.
   specifically, we used the pbt framework to maximise the inception score
   - a measure of visual fidelity -  resulting in a significant
   improvement from 6.45 to 6.9.

   we have also applied it to one of google   s state-of-the-art machine
   translation neural networks, which are usually trained with carefully
   hand tuned hyperparameter schedules that take months to perfect. with
   pbt we automatically found hyperparameter schedules that match and even
   exceed existing performance, but without any tuning and in the same
   time it normally takes to do a single training run.

   pbt evolution gif
   the evolution of the population during training of gans on cifar-10 and
   feudal networks (fun) on ms pacman. pink dots represent initial agents,
   blue ones the final ones.

   we believe this is only the beginning for the technique. at deepmind,
   we have also found pbt is particularly useful for training new
   algorithms and neural network architectures that introduce new
   hyperparameters. as we continue to refine the process, it offers up the
   possibility of finding and developing ever more sophisticated and
   powerful neural network models.
     __________________________________________________________________

   read the [36]full paper.

   this work was done by max jaderberg, valentin dalibard, simon osindero,
   wojciech m. czarnecki, jeff donahue, ali razavi, oriol vinyals, tim
   green, iain dunning, karen simonyan, chrisantha fernando and  koray
   kavukcuoglu.

   share article
     *
     *
     *
     *
     * [ ]
          + [37]linkedin
          + whatsapp
          + sms
          + [38]reddit

   author
   monday, 27 november 2017
     * [img_0022.2e16d0ba.fill-80x80_9nuehou.jpg]
       max jaderberg
       research scientist, deepmind

   ____________________
   ____________________
   [39]show all results
   (button) close

                               [40]deepmind logo

   follow
     *
     *
     *

     * [41]research [42]research
     * [43]applied [44]applied
     * [45]news & blog [46]news & blog
     * [47]about us [48]about us
     * [49]careers [50]careers

     * [51]press
     * [52]terms and conditions
     * [53]privacy policy     updated
     * [54]modern slavery statement
     * [55]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [56]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://arxiv.org/abs/1711.09846
  36. https://arxiv.org/abs/1711.09846
  37. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/population-based-training-neural-networks/&title=population based training of neural networks&summary=&source=google deepmind
  38. http://www.reddit.com/submit?url=https://deepmind.com/blog/population-based-training-neural-networks/&title=population based training of neural networks
  39. https://deepmind.com/blog/population-based-training-neural-networks/
  40. https://deepmind.com/
  41. https://deepmind.com/research/
  42. https://deepmind.com/research/
  43. https://deepmind.com/applied/
  44. https://deepmind.com/applied/
  45. https://deepmind.com/blog/
  46. https://deepmind.com/blog/
  47. https://deepmind.com/about/
  48. https://deepmind.com/about/
  49. https://deepmind.com/careers/
  50. https://deepmind.com/careers/
  51. https://deepmind.com/press/
  52. https://deepmind.com/terms-and-conditions/
  53. https://deepmind.com/privacy-policy/
  54. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  55. https://abc.xyz/
  56. https://deepmind.com/privacy-policy/

   hidden links:
  58. https://twitter.com/deepmindai
  59. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  60. https://plus.google.com/+deepmindai
  61. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/population-based-training-neural-networks/
  62. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/population-based-training-neural-networks/&t=
  63. https://plus.google.com/share?url=https%3a//deepmind.com/blog/population-based-training-neural-networks/
  64. mailto:?subject=population%20based%20training%20of%20neural%20networks&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/population-based-training-neural-networks/
  65. https://twitter.com/deepmindai
  66. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  67. https://plus.google.com/+deepmindai
