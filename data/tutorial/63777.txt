   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

deep learning specialization by andrew ng         21 lessons learned

   [16]go to the profile of ryan shrott
   [17]ryan shrott (button) blockedunblock (button) followfollowing
   oct 25, 2017
   [1*5zulcsb1kxepghu-qj8wxq.png]

   i recently completed all available material (as of october 25, 2017)
   for andrew ng   s new deep learning course on coursera. there are
   currently 3 courses available in the specialization:
    1. neural networks and deep learning
    2. improving deep neural networks: hyperparamater tuning,
       id173 and optimization
    3. structuring machine learning projects

   i found all 3 courses extremely useful and learned an incredible amount
   of practical knowledge from the instructor, andrew ng. ng does an
   excellent job of filtering out the buzzwords and explaining the
   concepts in a clear and concise manner. for example, ng makes it clear
   that supervised deep learning is nothing more than a multidimensional
   curve fitting procedure and that any other representational
   understandings, such as the common reference to the human biological
   nervous system, are loose at best.

   the specialization only requires basic id202 knowledge and
   basic programming knowledge in python. in my opinion, however, you
   should also know vector calculus to understand the inner workings of
   the optimization procedure. if you don   t care about the inner workings
   and only care about gaining a high level understanding you could
   potentially skip the calculus videos.

lesson 1: why deep learning is taking off?

   90% of all data was collected in the past 2 years. deep neural networks
   (dnn   s) are capable of taking advantage of a very large amount of data.
   as a result, dnn   s can dominate smaller networks and traditional
   learning algorithms.
   [1*1adlhcqxhmtgdbkmf7b2hw.png]
   shows how scale drives performance in dnn   s

   furthermore, there have been a number of algorithmic innovations which
   have allowed dnn   s to train much faster. for example, switching from a
   sigmoid activation function to a relu activation function has had a
   massive impact on optimization procedures such as id119.
   these algorithmic improvements have allowed researchers to iterate
   throughout the idea -> experiment -> code cycle much more quickly,
   leading to even more innovation.
   [1*5hmtyau_nv5cxm3zhnyt1q.png]
   deep learning development cycle

lesson 2: vectorization in deep learning

   before taking this course, i was not aware that a neural network could
   be implemented without any explicit for loops (except over the layers).
   ng does an excellent job at conveying the importance of a vectorized
   code design in python. the homework assignments provide you with a
   boilerplate vectorized code design which you could easily transfer to
   your own application.

lesson 3: deep understanding of dnn   s

   the first course actually gets you to implement the forward and
   backward propagation steps in numpy from scratch. by doing this, i have
   gained a much deeper understanding of the inner workings of higher
   level frameworks such as tensorflow and keras. ng explains the idea
   behind a computation graph which has allowed me to understand how
   tensorflow seems to perform    magical optimization   .

lesson 4: why deep representations?

   ng gives an intuitive understanding of the layering aspect of dnn   s.
   for example, in face detection he explains that earlier layers are used
   to group together edges in the face and then later layers use these
   edges to form parts of faces (i.e. nose, eyes, mouth etc.) and then
   further layers are used to put the parts together and identify the
   person. he also explains the idea of circuit theory which basically
   says that there exists functions which would require an exponential
   number of hidden units to fit the data in a shallow network. the
   exponential problem could be alleviated simply by adding a finite
   number of additional layers.

lesson 5: tools for addressing bias and variance

   ng explains the steps a researcher would take to identify and fix
   issues related to bias and variance problems. the picture he draws
   gives a systematic approach to addressing these issues.
   [1*s1lsedjemgjilklfgiio7w.png]
   cookbook for addressing bias and variance problems

   he also addresses the commonly quoted    tradeoff    between bias and
   variance. he explains that in the modern deep learning era we have
   tools to address each problem separately so that the tradeoff no longer
   exists.

lesson 6: intuition for id173

   why does a penalization term added to the cost function reduce variance
   effects? the intuition i had before taking the course was that it
   forced the weight matrices to be closer to zero producing a more
      linear    function. ng gave another interpretation involving the tanh
   activation function. the idea is that smaller weight matrices produce
   smaller outputs which centralizes the outputs around the linear section
   of the tanh function.
   [1*mmwgrduajjbem7vwrvoprq.png]
   tanh activation function

   he also gave an interesting intuitive explanation for dropout. prior to
   taking the course i thought that dropout is basically killing random
   neurons on each iteration so it   s as if we are working with a smaller
   network, which is more linear. his intuition is to look at life from
   the perspective of a single neuron.
   [1*plljnhuhrs_ftireeji4mq.png]
   perspective from a single neuron

   since dropout is randomly killing connections, the neuron is
   incentivized to spread it   s weights out more evenly among its parents.
   by spreading out the weights, it tends to have the effect of shrinking
   the squared norm of the weights. he also explains that dropout is
   nothing more than an adaptive form of l2 id173 and that both
   methods have similar effects.

lesson 7: why id172 works?

   ng demonstrates why id172 tends to improve the speed of the
   optimization procedure by drawing contour plots. he explicitly goes
   through an example of iterating through a id119 example on a
   normalized and non-normalized contour plot.

lesson 8: the importance of initialization

   ng shows that poor initialization of parameters can lead to vanishing
   or exploding gradients. he demonstrates several procedure to combat
   these issues. the basic idea is to ensure that each layer   s weight
   matrices has a variance of approximately 1. he also discusses xavier
   initialization for tanh activation function.

lesson 9: why mini-batch id119 is used?

   using contour plots, ng explains the tradeoff between smaller and
   larger mini-batch sizes. the basic idea is that a larger size becomes
   to slow per iteration, while a smaller size allows you to make progress
   faster but cannot make the same guarantees regarding convergence. the
   best approach is do something in between which allows you to make
   progress faster than processing the whole dataset at once, while also
   taking advantage of vectorization techniques.

lesson 10: intuitive understanding of advanced optimization techniques

   ng explains how techniques such as momentum and rmsprop allow gradient
   descent to dampen it   s path toward the minimum. he also gives an
   excellent physical explanation of the process with a ball rolling down
   a hill. he ties the methods together to explain the famous adam
   optimization procedure.

lesson 11: basic backend tensorflow understanding

   ng explains how to implement a neural network using tensorflow and also
   explains some of the backend procedures which are used in the
   optimization procedure. one of the homework exercises encourages you to
   implement dropout and l2 id173 using tensorflow. this further
   strengthened my understanding of the backend processes.

lesson 12: orthogonalization

   ng discusses the importance of orthogonalization in machine learning
   strategy. the basic idea is that you would like to implement controls
   that only affect a single component of your algorithms performance at a
   time. for example, to address bias problems you could use a bigger
   network or more robust optimization techniques. you would like these
   controls to only affect bias and not other issues such as poor
   generalization. an example of a control which lacks orthogonalization
   is stopping your optimization procedure early (early stopping). this is
   because it simultaneously affects the bias and variance of your model.

lesson 13: importance of a single number evaluation metric

   ng stresses the importance of choosing a single number evaluation
   metric to evaluate your algorithm. you should only change the
   evaluation metric later on in the model development process if your
   target changes. ng gives an example of identifying pornographic photos
   in a cat classification application!

lesson 14: test/dev distributions

   always ensure that the dev and test sets have the same distribution.
   this ensures that your team is aiming at the correct target during the
   iteration process. this also means that if you decide to correct
   mislabeled data in your test set then you must also correct the
   mislabelled data in your development set.

lesson 15: dealing with different training and test/dev distributions

   ng gives reasons for why a team would be interested in not having the
   same distribution for the train and test/dev sets. the idea is that you
   want the evaluation metric to be computed on examples that you actually
   care about. for example, you may want to use examples that are not as
   relevant to your problem for training, but you would not want your
   algorithm to be evaluated against these examples. this allows your
   algorithm to be trained with much more data. it has been empirically
   shown that this approach will give you better performance in many
   cases. the downside is that you have different distributions for your
   train and test/dev sets. the solution is to leave out a small piece of
   your training set and determine the generalization capabilities of the
   training set alone. then you could compare this error rate to the
   actual development error and compute a    data mismatch    metric. ng then
   explains methods of addressing this data mismatch problem such as
   artificial data synthesis.

lesson 16: train/dev/test sizes

   the guidelines for setting up the split of train/dev/test has changed
   dramatically during the deep learning era. before taking the course, i
   was aware of the usual 60/20/20 split. ng stresses that for a very
   large dataset, you should be using a split of about 98/1/1 or even
   99/0.5/0.5. this is due to the fact that the dev and test sets only
   need to be large enough to ensure the confidence intervals provided by
   your team. if you are working with 10,000,000 training examples, then
   perhaps 100,000 examples (or 1% of the data) is large enough to
   guarantee certain confidence bounds on your dev and/or test set.

lesson 17: approximating bayes optimal error

   ng explains how human level performance could be used as a proxy for
   bayes error in some applications. for example, for tasks such as vision
   and audio recognition, human level error would be very close to bayes
   error. this allows your team to quantify the amount of avoidable bias
   your model has. without a benchmark such as bayes error, it   s difficult
   to understand the variance and avoidable bias problems in your network.

lesson 18: error analysis

   ng shows a somewhat obvious technique to dramatically increase the
   effectiveness of your algorithms performance using error analysis. the
   basic idea is to manually label your misclassified examples and to
   focus your efforts on the error which contributes the most to your
   misclassified data.
   [1*8ti-xfewb0eiy7ibllgqsg.png]
   cat recognition app error analysis

   for example, in the cat recognition ng determines that blurry images
   contribute the most to errors. this sensitivity analysis allows you see
   how much your efforts are worth on reducing the total error. it may be
   the case that fixing blurry images is an extremely demanding task,
   while other errors are obvious and easy to fix. both the sensitivity
   and approximate work would be factored into the decision making
   process.

lesson 19: when to use id21?

   id21 allows you to transfer knowledge from one model to
   another. for example, you could transfer image recognition knowledge
   from a cat recognition app to a radiology diagnosis. implementing
   id21 involves retraining the last few layers of the
   network used for a similar application domain with much more data. the
   idea is that hidden units earlier in the network have a much broader
   application which is usually not specific to the exact task that you
   are using the network for. in summary, id21 works when
   both tasks have the same input features and when the task you are
   trying to learn from has much more data than the task you are trying to
   train.

lesson 20: when to use id72?

   id72 forces a single neural network to learn multiple
   tasks at the same time (as opposed to having a separate neural network
   for each task). ng explains that the approach works well when the set
   of tasks could benefit from having shared lower-level features and when
   the amount of data you have for each task is similar in magnitude.

lesson 21: when to use end-to-end deep learning?

   end-to-end deep learning takes multiple stages of processing and
   combines them into a single neural network. this allows the data to
   speak for itself without the bias displayed by humans in hand
   engineering steps in the optimization procedure. to the contrary, this
   approach needs much more data and may exclude potentially hand designed
   components.

conclusion

   ng   s deep learning course has given me a foundational intuitive
   understanding of the deep learning model development process. the
   lessons i explained above only represent a subset of the materials
   presented in the course. after completing the course you will not
   become an expert in deep learning. my only complaint of the course is
   that the homework assignments were too easy. i was not endorsed by
   deeplearning.ai for writing this article.

      

   that   s all folks         if you   ve made it this far, please comment below and
   add me on [18]linkedin.

   my github is [19]here.

     * [20]machine learning
     * [21]deep learning
     * [22]neural networks
     * [23]artificial intelligence
     * [24]towards data science

   (button)
   (button)
   (button) 4.5k claps
   (button) (button) (button) 23 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of ryan shrott

[26]ryan shrott

   equity desk quant at bmo capital markets
   [27]https://github.com/ryanshrott

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 4.5k
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/15ffaaef627c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_tdpvhgpyec5w---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ryanshrott?source=post_header_lockup
  17. https://towardsdatascience.com/@ryanshrott
  18. https://www.linkedin.com/in/ryanshrott/
  19. https://github.com/ryanshrott
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/deep-learning?source=post
  22. https://towardsdatascience.com/tagged/neural-networks?source=post
  23. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  24. https://towardsdatascience.com/tagged/towards-data-science?source=post
  25. https://towardsdatascience.com/@ryanshrott?source=footer_card
  26. https://towardsdatascience.com/@ryanshrott
  27. https://github.com/ryanshrott
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/15ffaaef627c/share/twitter
  34. https://medium.com/p/15ffaaef627c/share/facebook
  35. https://medium.com/p/15ffaaef627c/share/twitter
  36. https://medium.com/p/15ffaaef627c/share/facebook
