   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

enriching word vectors with lexicon knowledge and semantic relations: an
efficient retrofitting algorithm

   [14]go to the profile of aneesha bakharia
   [15]aneesha bakharia (button) blockedunblock (button) followfollowing
   nov 29, 2017

   id97 and glove are two popular algorithms that produce word vectors
   or id27s. both algorithms work in an unsupervised manner to
   determine vector representations of words (i.e., they map words to a
   latent dimensions). glove is trained using corpus word-word
   co-occurrence statistics. the id97 skip-gram algorithm uses a
   log-linear classifier and a continuous projection layer to predict
   words within a context window. word vectors have been useful in a
   multitude of tasks such as id31, id91 and
   classification and have by far replaced manually crafted semantic
   lexicons. i still however believe that valuable semantic information is
   captured in semantic lexicons (e.g., paraphrase database, framenet and
   id138) and have wondered if there was a way to both learn word
   vectors in an unsupervised manner yet incorporate the knowledge
   captured in semantic lexicons as well. this blog post illustrates the
   use of an amazing [16]retrofitting algorithm (faruqui et al., 2015)
   that is able to apply semantic relations from any lexicon to pre-built
   word vectors.

   while a few techniques have been published that are able to add lexicon
   knowledge to word vectors, most of these techniques add an objective
   while the word vector algorithm is being trained. the retrofitting
   algorithm (faruqui et al., 2015) on the other hand is able to apply
   must-link semantic relations on any pre-built word vector (glove,
   id97, and even your own trained word vectors). the retrofitting
   algorithm is also extremely fast!!! a direct quote from the paper:
      about 5 seconds for a graph of 100,000 words and vector length 300   .
   even more amazingly the implementation of the algorithm [17]code is
   open source, implemented in python and contains lexicons you can
   immediately use. in this blog post i   ll describe the algorithm and then
   take you step by step through applying the algorithm to a pre-built
   word vector. fig 1 compares a glove word vector retrofitted with
   knowledge from the paraphrase database, with the output from the
   original word vector for the word    empathy    .
   [1*nntxw2kcwnpl7qknaqwafg.png]
   fig 1: comparing the words close to    empathy    using the original word
   vectors and the retrofitted word vectors.

   a quick overview of the retrofitting algorithm

   the algorithm takes a matrix of word vectors and a graph of semantic
   relationships from the lexicon as input. the algorithm then learns a
   new matrix with a dual objective of trying to keep neighbouring words
   from the original matrix close together and at the same time placing
   similar words from the semantic lexicon in the same neighbourhood. the
   most impressive aspect of the paper is the manner in which the
   objective is trained         rather than use sgd, a highly optimised belief
   propagation algorithm is used.

   testing the algorithm

   instructions for running the algorithm are in the github repo
   [18]readme. the code is command line driven and you need to supply the
   word vector file (in txt format), the lexicon file, the number of
   iterations for the algorithm to run (10 is sufficient) and the output
   file. in the example below i have used the glove.6b.50d.txt word vector
   and the paraphrase database. the code repo also includes framenet and
   id138. the word vectors must be in txt format. you are also not
   restricted to using an existing lexicon. if you have lists of words or
   rules for words that are similar (i.e., must-link rules or
   constraints), simply add these to the required format (space separated
   word lists on each line) and run the retrofitting algorithm.

   the gensim library is used to load and compare the original and
   retrofitted word vectors. gensim is only able to load vectors in
   id97 format but provides helper scripts to convert from glove to
   id97.

   iframe: [19]/media/37ab11b9f2adc20cd95895efeed0b50b?postid=bcb5f1208a3e

   next import gensim, numpy, matplotlib and tsne. we also set up the
   jupyter notebook for interactive matplotlib.

   iframe: [20]/media/0bd2bfe65ad16855cff502e8e7668c75?postid=bcb5f1208a3e

   the [21]display_closestwords_tsnescatterplot() method was introduced in
   a previous blog post. the method takes a id97 model and only
   displays a plot of the words that are closest to a given word.

   iframe: [22]/media/ba9d0f690f133b7033cbbaacb2f232ce?postid=bcb5f1208a3e

   now we can load the original word vectors and the retrofitted vectors
   as separate gensim models and use the
   display_closestwords_tsnescatterplot() method to evaluate how the
   neighbourhood of words around the specified word has changed.

   iframe: [23]/media/cbdb23fc8a5f9d00e43dbda4015ffe24?postid=bcb5f1208a3e

   in figure 2 it is easy to see the improvement the retrofitting
   technique has made to the words neighbouring    happy   . words like
      pleased   ,    delighted    and    glad    are now included that were not
   present in the original word vectors. retrofitting is particularly
   promising especially when used with word vectors that are smaller in
   dimensionality (e.g., the glove word vectors used only had 50 latent
   dimensions) or created from a small corpus.
   [1*elsm9us75dlqyg8x2dawbg.png]
   fig 2: comparing the words close to    happy    using the original word
   vectors and the retrofitted word vectors.

   the full source code is also available as a jupyter notebook:

   iframe: [24]/media/6272206d8c9a19d24dbfb9e337b5dd35?postid=bcb5f1208a3e

   more complex contraints

   the retrofitting algorithm is currently only able to support must-link
   constraints (i.e., it is able to pull similar words together). there
   may however be cases when you need to add cannot link constraints
   (i.e., words that must not be places together or words that repel each
   other). mrk  i   et al (2016) has proposed an [25]extension that adds
   repulsion rules. the [26]source code for the mrk  i   algorithm is also
   available and will be covered in a future blog post.

   references

   faruqui, m., dodge, j., jauhar, s. k., dyer, c., hovy, e., & smith, n.
   a. (2014). retrofitting word vectors to semantic lexicons. arxiv
   preprint arxiv:1411.4166.

   mrk  i  , n., s  aghdha, d. o., thomson, b., ga  i  , m., rojas-barahona,
   l., su, p. h.,     & young, s. (2016). counter-fitting word vectors to
   linguistic constraints. arxiv preprint arxiv:1603.00892.

   iframe: [27]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=bcb5f1208a3e

   [28][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [29][1*v-ppfkswhbvlwwamsvhhwg.png]
   [30][1*wt2auqisieaozxj-i7brdq.png]

     * [31]machine learning
     * [32]nlp
     * [33]data science
     * [34]id27s
     * [35]artificial intelligence

   (button)
   (button)
   (button) 149 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of aneesha bakharia

[37]aneesha bakharia

   data science, topic modelling, deep learning, algorithm usability and
   interpretation, learning analytics, electronics     brisbane, australia

     (button) follow
   [38]becoming human: artificial intelligence magazine

[39]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 149
     * (button)
     *
     *

   [40]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [41]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bcb5f1208a3e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/enriching-word-vectors-with-lexicon-knowledge-and-semantic-relations-an-efficient-retrofitting-bcb5f1208a3e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/enriching-word-vectors-with-lexicon-knowledge-and-semantic-relations-an-efficient-retrofitting-bcb5f1208a3e&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_wwom8css7n8q---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@aneesha?source=post_header_lockup
  15. https://becominghuman.ai/@aneesha
  16. http://www.manaalfaruqui.com/papers/naacl15-retrofitting.pdf
  17. https://github.com/mfaruqui/retrofitting
  18. https://github.com/mfaruqui/retrofitting
  19. https://becominghuman.ai/media/37ab11b9f2adc20cd95895efeed0b50b?postid=bcb5f1208a3e
  20. https://becominghuman.ai/media/0bd2bfe65ad16855cff502e8e7668c75?postid=bcb5f1208a3e
  21. https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-id97-bb8eeaea6229
  22. https://becominghuman.ai/media/ba9d0f690f133b7033cbbaacb2f232ce?postid=bcb5f1208a3e
  23. https://becominghuman.ai/media/cbdb23fc8a5f9d00e43dbda4015ffe24?postid=bcb5f1208a3e
  24. https://becominghuman.ai/media/6272206d8c9a19d24dbfb9e337b5dd35?postid=bcb5f1208a3e
  25. http://www.aclweb.org/anthology/n16-1018
  26. http://github.com/nmrksic/counter-fitting
  27. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=bcb5f1208a3e
  28. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  29. https://upscri.be/8f5f8b
  30. https://becominghuman.ai/write-for-us-48270209de63
  31. https://becominghuman.ai/tagged/machine-learning?source=post
  32. https://becominghuman.ai/tagged/nlp?source=post
  33. https://becominghuman.ai/tagged/data-science?source=post
  34. https://becominghuman.ai/tagged/word-embeddings?source=post
  35. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  36. https://becominghuman.ai/@aneesha?source=footer_card
  37. https://becominghuman.ai/@aneesha
  38. https://becominghuman.ai/?source=footer_card
  39. https://becominghuman.ai/?source=footer_card
  40. https://becominghuman.ai/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/bcb5f1208a3e/share/twitter
  44. https://medium.com/p/bcb5f1208a3e/share/facebook
  45. https://medium.com/p/bcb5f1208a3e/share/twitter
  46. https://medium.com/p/bcb5f1208a3e/share/facebook
