   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ai   | theory, practice, business
     * [9]theory
     * [10]practice
     * [11]business
     __________________________________________________________________

understanding hinton   s id22. part iii: dynamic routing between
capsules.

   [12]go to the profile of max pechyonkin
   [13]max pechyonkin (button) blockedunblock (button) followfollowing
   nov 28, 2017

part of understanding hinton   s id22 series:

   part i: [14]intuition
   part ii: [15]how capsules work
   part iii: [16]dynamic routing between capsules (you are reading it now)
   part iv: [17]capsnet architecture
     __________________________________________________________________

   quick announcement about our new publication [18]ai  . we are getting
   the best writers together to talk about the theory, practice, and
   business of ai and machine learning. follow it to stay up to date on
   the latest trends.
     __________________________________________________________________

introduction

   this is the third post in the series about a new type of neural
   network, based on capsules, called capsnet. i already talked about the
   intuition behind it, as well as what is a capsule and how it works. in
   this post, i will talk about the novel dynamic routing algorithm that
   allows to train id22.
   [1*ia32_jrzlhhfinbnsjetha.png]
   one of the earlier figures explaining capsules and routing between
   them. [19]source.

   as i showed in part ii, a capsule i in a lower-level layer needs to
   decide how to send its output vector to higher-level capsules j. it
   makes this decision by changing scalar weight c_ij that will multiply
   its output vector and then be treated as input to a higher-level
   capsule. notation-wise, c_ij represents the weight that multiplies
   output vector from lower-level capsule i and goes as input to a higher
   level capsule j.

   things to know about weights c_ij:
    1. each weight is a non-negative scalar
    2. for each lower level capsule i, the sum of all weights c_ij equals
       to 1
    3. for each lower level capsule i, the number of weights equals to the
       number of higher-level capsules
    4. these weights are determined by the iterative dynamic routing
       algorithm

   the first two facts allow us to interpret weights in probabilistic
   terms. recall that the length a capsule   s output vector is interpreted
   as id203 of existence of the feature that this capsule has been
   trained to detect. orientation of the output vector is the parametrized
   state of the feature. so, in a sense, for each lower level capsule i,
   its weights c_ij define a id203 distribution of its output
   belonging to each higher level capsule j.
   [1*9clh98qtbi5r2idxufrmrg.png]
   recall: computations inside of a capsule as described in part ii of the
   series. source: author.

dynamic routing between capsules

   so, what exactly happens during dynamic routing? let   s have a look at
   the description of the algorithm as published in the paper. but before
   we dive into the algorithm step by step, i want you to keep in your
   mind the main intuition behind the algorithm:

     lower level capsule will send its input to the higher level capsule
     that    agrees    with its input. this is the essence of the dynamic
     routing algorithm.

   now that we have this in mind, let   s go through the algorithm line by
   line.
   [1*uke9eq6yd6ipiu1cljwseq.png]
   dynamic routing algorithm, as published in the [20]original paper.

   the first line says that this procedure takes all capsules in a lower
   level l and their outputs u_hat, as well as the number of routing
   iterations r. the very last line tells you that the algorithm will
   produce the output of a higher level capsule v_j. essentially, this
   algorithm tells us how to calculate [21]forward pass of the network.

   in the second line you will notice that there is a new coefficient b_ij
   that we haven   t seen before. this coefficient is simply a temporary
   value that will be iteratively updated and, after the procedure is
   over, its value will be stored in c_ij. at start of training the value
   of b_ij is initialized at zero.

   line 3 says that the steps in 4   7 will be repeated r times (the number
   of routing iterations).

   step in line 4 calculates the value of vector c_i which is all routing
   weights for a lower level capsule i. this is done for all lower level
   capsules. why [22]softmax? softmax will make sure that each weight c_ij
   is a non-negative number and their sum equals to one. essentially,
   softmax enforces probabilistic nature of coefficients c_ij that i
   described above.

   at the first iteration, the value of all coefficients c_ij will be
   equal, because on line two all b_ij are set to zero. for example, if we
   have 3 lower level capsules and 2 higher level capsules, then all c_ij
   will be equal to 0.5. the state of all c_ij being equal at
   initialization of the algorithm represents the state of maximum
   confusion and uncertainty: lower level capsules have no idea which
   higher level capsules will best fit their output. of course, as the
   process is repeated these uniform distributions will change.

   after all weights c_ij were calculated for all lower level capsules, we
   can move on to line 5, where we look at higher level capsules. this
   step calculates a linear combination of input vectors, weighted by
   routing coefficients c_ij, determined in the previous step.
   intuitively, this means scaling down input vectors and adding them
   together, which produces output vector s_j. this is done for all higher
   level capsules.

   next, in line 6 vectors from last step are passed through the squash
   nonlinearity, that makes sure the direction of the vector is preserved,
   but its length is enforced to be no more than 1. this step produces the
   output vector v_j for all higher level capsules.
   [1*c2hbww5z_vstusux1mzcaq.jpeg]
   [23]dot product is an operation that takes 2 vectors and outputs a
   scalar. there are several scenarios possible for the two vectors of
   given lengths but different relative orientations: (a) largest positive
   possible values; (b) positive dot product; (c) zero dot product; (d)
   negative dot product; (e) largest possible negative dot product. you
   can think of the dot product as a measure of similarity in the context
   of capsnets. source: author.

   to summarize what we have so far: steps 4   6 simply calculate the output
   of higher level capsules. step on line 7 is where the weight update
   happens. this step captures the essence of the routing algorithm. this
   steps looks at each higher level capsule j and then examines each input
   and updates the corresponding weight b_ij according to the formula. the
   formula says that the new weight value equals to the old value plus the
   dot product of current output of capsule j and the input to this
   capsule from a lower level capsule i. the dot product looks at
   similarity between input to the capsule and output from the capsule.
   also, remember from above, the lower level capsule will sent its output
   to the higher level capsule whose output is similar. this similarity is
   captured by the dot product. after this step, the algorithm starts over
   from step 3 and repeats the process r times.

   after r times, all outputs for higher level capsules were calculated
   and routing weights have been established. the forward pass can
   continue to the next level of network.

intuitive example of weight update step

   [1*spdzizqo3yp6xgbnegvidg.jpeg]
   two higher level capsules with their outputs represented by purple
   vectors, and inputs represented by black and orange vectors. lower
   level capsule with orange output will decrease the weight for higher
   level capsule 1 (left side) and increase the weight for higher level
   capsule 2 (right side). source: author.

   in the figure on the left, imagine that there are two higher level
   capsules, their output is represented by purple vectors v1 and v2
   calculated as described in previous section. the orange vector
   represents input from one of the lower level capsules and the black
   vectors represent all the remaining inputs from other lower level
   capsules.

   we see that in the left part the purple output v1 and the orange input
   u_hat point in the opposite directions. in other words, they are not
   similar. this means their dot product will be a negative number and as
   result routing coefficient c_11 will decrease. in the right part, the
   purple output v2 and the orange input v_hat point in the same
   direction. they are similar. therefore, the routing coefficient c_12
   will increase. this procedure is repeated for all higher level capsules
   and for all inputs of each capsule. the result of this is a set of
   routing coefficients that best matches outputs from lower level
   capsules with outputs of higher level capsules.

how many routing iterations to use?

   the paper examined a range of values for both mnist and cifar data
   sets. author   s conclusion is two-fold:
    1. more iterations tends to overfit the data
    2. it is recommended to use 3 routing iterations in practice

conclusion

   in this article, i explained the dynamic routing algorithm by agreement
   that allows to train the capsnet. the most important idea is that
   similarity between input and output is measured as dot product between
   input and output of a capsule and then routing coefficient is updated
   correspondingly. best practice is to use 3 routing iterations.

   in the next post, i will walk you through capsnet architecture, where
   we will put together all pieces of the puzzle that we learned so far.
     __________________________________________________________________

thanks for reading! if you enjoyed it, hit that clap button below and
[24]subscribe to updates on [25]my website! it would mean a lot to me and
encourage me to write more stories like this.

   you can [26]follow me on twitter. let   s also connect on [27]linkedin.

     * [28]machine learning
     * [29]deep learning
     * [30]geoffrey hinton
     * [31]artificial intelligence
     * [32]theory

   (button)
   (button)
   (button) 7.7k claps
   (button) (button) (button) 30 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of max pechyonkin

[34]max pechyonkin

   [35]https://pechyonkin.me/

     (button) follow
   [36]ai   | theory, practice, business

[37]ai   | theory, practice, business

   the ai revolution is here! navigate the ever changing industry with our
   thoughtfully written articles whether your a researcher, engineer, or
   entrepreneur

     * (button)
       (button) 7.7k
     * (button)
     *
     *

   [38]ai   | theory, practice, business
   never miss a story from ai   | theory, practice, business, when you sign
   up for medium. [39]learn more
   never miss a story from ai   | theory, practice, business
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/349f6d30418
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/ai%c2%b3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/ai%c2%b3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418&source=--------------------------nav_reg&operation=register
   8. https://medium.com/ai  -theory-practice-business?source=logo-lo_r2bs6ttsnury---ab837e78463f
   9. https://medium.com/ai  -theory-practice-business/tagged/theory
  10. https://medium.com/ai  -theory-practice-business/tagged/practice
  11. https://medium.com/ai  -theory-practice-business/tagged/business
  12. https://medium.com/@pechyonkin?source=post_header_lockup
  13. https://medium.com/@pechyonkin
  14. https://pechyonkin.me/capsules-1/
  15. https://pechyonkin.me/capsules-2/
  16. https://pechyonkin.me/capsules-3/
  17. https://pechyonkin.me/capsules-4/
  18. https://medium.com/ai  -theory-practice-business?source=logo-bc670b9a1aca---ab837e78463f
  19. http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10754.pdf
  20. https://arxiv.org/abs/1710.09829
  21. http://cs231n.github.io/optimization-2/
  22. https://en.wikipedia.org/wiki/softmax_function
  23. https://en.wikipedia.org/wiki/dot_product
  24. https://pechyonkin.me/subscribe/
  25. https://pechyonkin.me/
  26. https://twitter.com/max_pechyonkin
  27. https://www.linkedin.com/in/maxim-pechyonkin-phd/
  28. https://medium.com/tag/machine-learning?source=post
  29. https://medium.com/tag/deep-learning?source=post
  30. https://medium.com/tag/geoffrey-hinton?source=post
  31. https://medium.com/tag/artificial-intelligence?source=post
  32. https://medium.com/tag/theory?source=post
  33. https://medium.com/@pechyonkin?source=footer_card
  34. https://medium.com/@pechyonkin
  35. https://pechyonkin.me/
  36. https://medium.com/ai  -theory-practice-business?source=footer_card
  37. https://medium.com/ai  -theory-practice-business?source=footer_card
  38. https://medium.com/ai  -theory-practice-business
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/349f6d30418/share/twitter
  42. https://medium.com/p/349f6d30418/share/facebook
  43. https://medium.com/p/349f6d30418/share/twitter
  44. https://medium.com/p/349f6d30418/share/facebook
