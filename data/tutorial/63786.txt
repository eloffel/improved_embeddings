   #[1]no free hunch    feed [2]no free hunch    comments feed [3]no free
   hunch    introduction to neural networks comments feed [4]alternate
   [5]alternate

   [6]no free hunch [7]navigation

     * [8]kaggle.com

     * [9]kaggle.com

   introduction to neural networks

   [10]13

introduction to neural networks

   [11]ben gorman|11.27.2017

   this tutorial was originally posted [12]here on ben's
   blog, [13]gormanalysis.

   id158s are all the rage. one has to wonder if the
   catchy name played a role in the model   s own marketing and adoption.
   i   ve seen business managers giddy to mention that their products use
      id158s    and    deep learning   . would they be so
   giddy to say their products use    connected circles models    or    fail and
   be penalized machines   ? but make no mistake     artificial neural
   networks are the real deal as evident by their success in a number of
   applications like image recognition, natural language processing,
   automated trading, and id101. as a professional data
   scientist who didn   t fully understand them, i felt embarrassed like a
   builder without a table saw. consequently i   ve done my homework and
   written this article to help others overcome the same hurdles and head
   scratchers i did in my own (ongoing) learning process.

   note that r code for the examples presented in this article can be
   found [14]here in the [15]machine learning problem bible. additionally,
   come back for part 2, to see the details behind designing and coding a
   neural network from scratch.

   we   ll start with a motivational problem. here we have a collection of
   grayscale images, each a 2  2 grid of pixels where each pixel has an
   intensity value between 0 (white) and 255 (black). the goal is to build
   a model that identifies images with a    stairs    pattern.

   at this point, we are only interested in finding a model that could fit
   the data reasonably. we   ll worry about the fitting methodology later.

pre-processing

   for each image, we label the pixels  x_1 x_1 ,  x_2 x_2 ,  x_3 x_3 ,
   x_4 x_4  and generate an input vector  \mathbf{x} = \begin{bmatrix}x_1
   & x_2 & x_3 & x_4\end{bmatrix} \mathbf{x} = \begin{bmatrix}x_1 & x_2 &
   x_3 & x_4\end{bmatrix}  which will be the input to our model. we expect
   our model to predict true (the image has the stairs pattern) or false
   (the image does not have the stairs pattern).

   imageid x1  x2  x3  x4  isstairs
      1    252  4  155 175   true
      2    175 10  186 200   true
      3    82  131 230 100  false
                                           
     498   36  187 43  249  false
     499    1  160 169 242   true
     500   198 134 22  188  false

single layer id88 (model iteration 0)

   a simple model we could build is a single layer [16]id88. a
   id88 uses a weighted linear combination of the inputs to return a
   prediction score. if the prediction score exceeds a selected threshold,
   the id88 predicts true. otherwise it predicts false. more
   formally,

   f(x)={\begin{cases} 1 &{\text{if }}\ w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4
   > threshold\\ 0 & {\text{otherwise}} \end{cases}} f(x)={\begin{cases} 1
   &{\text{if }}\ w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 > threshold\\ 0 &
   {\text{otherwise}} \end{cases}}

   let   s re-express this as follows

   \widehat y = \mathbf w \cdot \mathbf x + b \widehat y = \mathbf w \cdot
   \mathbf x + b

   f(x)={\begin{cases} 1 &{\text{if }}\ \widehat{y} > 0\\ 0 &
   {\text{otherwise}} \end{cases}} f(x)={\begin{cases} 1 &{\text{if }}\
   \widehat{y} > 0\\ 0 & {\text{otherwise}} \end{cases}}

   here  \widehat{y} \widehat{y}  is our prediction score.

   pictorially, we can represent a id88 as input nodes that feed
   into an output node.

   for our example, suppose we build the following id88:

   \widehat{y} = -0.0019x_1 + -0.0016x_2 + 0.0020x_3 + 0.0023x_4 + 0.0003
   \widehat{y} = -0.0019x_1 + -0.0016x_2 + 0.0020x_3 + 0.0023x_4 + 0.0003

   here   s how the id88 would perform on some of our training images.

   this would certainly be better than randomly guessing and it makes some
   logical sense. all the stairs patterns have darkly shaded pixels in the
   bottom row which supports the larger, positive coefficients for  x_3
   x_3  and  x_4 x_4 . nonetheless, there are some glaring problems with
   this model.
    1. the model outputs a real number whose value correlates with the
       concept of likelihood (higher values imply a greater id203
       the image represents stairs) but there   s no basis to interpret the
       values as probabilities, especially since they can be outside the
       range [0, 1].
    2. the model can   t capture the non-linear relationship between the
       variables and the target. to see this, consider the following
       hypothetical scenarios:

case a

   start with an image, x = [100, 0, 0, 125]. increase  x_3 x_3  from 0 to
   60.

case b

   start with the last image, x = [100, 0, 60, 125]. increase  x_3 x_3
   from 60 to 120.

   intuitively, case a should have a much larger increase in  \widehat y
   \widehat y  than case b. however, since our id88 model is a
   linear equation, the equivalent +60 change in  x_3 x_3  resulted in an
   equivalent +0.12 change in  \widehat y \widehat y  for both cases.

   there are more issues with our linear perception, but let   s start by
   addressing these two.

single layer id88 with sigmoid activation function (model iteration 1)

   we can fix problems 1 and 2 above by wrapping our id88 within a
   sigmoid function (and subsequently choosing different weights). recall
   that the [17]sigmoid function is an s shaped curve bounded on the
   vertical axis between 0 and 1, and is thus frequently used to model the
   id203 of a binary event.

   sigmoid(z) = \frac{1}{1 + e^{-z}} sigmoid(z) = \frac{1}{1 + e^{-z}}

   following this idea, we can update our model with the following picture
   and equation.

   z = w \cdot x = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 z = w \cdot x =
   w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4

   \widehat y = sigmoid(z) = \frac{1}{1 + e^{-z}} \widehat y = sigmoid(z)
   = \frac{1}{1 + e^{-z}}

   looks familiar? it   s our old friend, [18]id28. however,
   it   ll serve us well to interpret the model as a linear id88 with
   a sigmoid    activation function    because that gives us more room to
   generalize. also, since we now interpret  \widehat y \widehat y  as
   a id203, we must update our decision rule accordingly.

   f(x)={\begin{cases} 1 &{\text{if }}\ \widehat{y} > 0.5\\ 0 &
   {\text{otherwise}} \end{cases}} f(x)={\begin{cases} 1 &{\text{if }}\
   \widehat{y} > 0.5\\ 0 & {\text{otherwise}} \end{cases}}

   continuing with our example problem, suppose we come up with the
   following fitted model:

   \begin{bmatrix} w_1 & w_2 & w_3 & w_4 \end{bmatrix} = \begin{bmatrix}
   -0.140 & -0.145 & 0.121 & 0.092 \end{bmatrix} \begin{bmatrix} w_1 & w_2
   & w_3 & w_4 \end{bmatrix} = \begin{bmatrix} -0.140 & -0.145 & 0.121 &
   0.092 \end{bmatrix}

   b = -0.008 b = -0.008

   \widehat y = \frac{1}{1 + e^{-(-0.140x_1 -0.145x_2 + 0.121x_3 +
   0.092x_4 -0.008)}} \widehat y = \frac{1}{1 + e^{-(-0.140x_1 -0.145x_2 +
   0.121x_3 + 0.092x_4 -0.008)}}

   observe how this model performs on the same sample images from the
   previous section.

   clearly this fixes problem 1 from above. observe how it also fixes
   problem 2.

case a

   start with an image, x = [100, 0, 0, 125]. increase  x_3 x_3  from 0 to
   60.

case b

   start with the last image, x = [100, 0, 60, 125]. increase  x_3 x_3
   from 60 to 120.

   notice how the curvature of the sigmoid function causes case a to
      fire    (increase rapidly) as  z = w \cdot x z = w \cdot x  increases,
   but the pace slows down as  z z  continues to increase. this aligns
   with our intuition that case a should reflect a greater increase in the
   likelihood of stairs versus case b.

   unfortunately this model still has issues.
    1. \widehat y \widehat y  has a monotonic relationship with each
       variable. what if we want to identify lightly shaded stairs?
    2. the model does not account for variable interaction. assume the
       bottom row of an image is black. if the top left pixel is white,
       darkening the top right pixel should increase the id203 of
       stairs. if the top left pixel is black, darkening the top right
       pixel should decrease the id203 of stairs. in other words,
       increasing  x_3 x_3  should potentially increase or decrease
       \widehat y \widehat y  depending on the values of the other
       variables. our current model has no way of achieving this.

multi-layer id88 with sigmoid activation function (model iteration 2)

   we can solve both of the above issues by adding an extra layer to our
   id88 model. we   ll construct a number of base models like the one
   above, but then we   ll feed the output of each base model as input
   into another id88. this model is in fact a vanilla neural
   network. let   s see how it might work on some examples.

example 1: identify the stairs pattern

    1. build a model that fires when    left stairs    are identified,
       \widehat y_{left} \widehat y_{left}
    2. build a model that fires when    right stairs    are identified,
       \widehat y_{right} \widehat y_{right}
    3. add the score of the base models so that the final sigmoid function
       only fires if both  \widehat y_{left} \widehat y_{left}  and
       \widehat y_{right} \widehat y_{right}  are large

   alternatively
    1. build a model that fires when the bottom row is dark,  \widehat y_1
       \widehat y_1
    2. build a model that fires when the top left pixel is dark and the
       top right pixel is light,  \widehat y_2 \widehat y_2
    3. build a model that fires when the top left pixel is light and the
       top right pixel is dark,  \widehat y_3 \widehat y_3
    4. add the base models so that the final sigmoid function only fires
       if  \widehat y_1 \widehat y_1  and  \widehat y_2 \widehat y_2  are
       large, or  \widehat y_1 \widehat y_1  and  \widehat y_3 \widehat
       y_3  are large. (note that  \widehat y_2 \widehat y_2 and  \widehat
       y_3 \widehat y_3  cannot both be large)

example 2: identify lightly shaded stairs

    1. build models that fire for    shaded bottom row   ,    shaded x1 and
       white x2   ,    shaded x2 and white x1   ,  \widehat y_1 \widehat y_1 ,
       \widehat y_2 \widehat y_2 , and  \widehat y_3 \widehat y_3
    2. build models that fire for    dark bottom row   ,    dark x1 and white
       x2   ,    dark x2 and white x1   ,  \widehat y_4 \widehat y_4 ,  \widehat
       y_5 \widehat y_5 , and  \widehat y_6 \widehat y_6
    3. combine the models so that the    dark    identifiers are essentially
       subtracted from the    shaded    identifiers before squashing the
       result with a sigmoid function

a note about terminology

   a single-layer id88 has a single output layer. consequently, the
   models we just built would be called two-layer id88s because they
   have an output layer which is the input to another output layer.
   however, we could call these same models neural networks, and in this
   respect the networks have three layers     an input layer, a hidden
   layer, and an output layer.

alternative id180

   in our examples we used a sigmoid activation function. however, we
   could use other id180. [19]tanhand [20]relu are common
   choices. the activation function must be non-linear, otherwise the
   neural network would simplify to an equivalent single layer id88.

multiclass classification

   we can easily extend our model to work for multiclass classification by
   using multiple nodes in the final output layer. the idea here is that
   each output node corresponds to one of the  c c  classes we are trying
   to predict. instead of squashing the output with the sigmoid function
   which maps an element in  \mathbb{r} \mathbb{r}  to and element in [0,
   1], we can use the [21]softmax function which maps a vector in
   \mathbb{r}^n \mathbb{r}^n  to a vector in  \mathbb{r}^n \mathbb{r}^n
   such that the resulting vector elements sum to 1. in other words, we
   can design the network such that it outputs the vector [ prob(class_1)
   prob(class_1) ,  prob(class_2) prob(class_2) ,    ,  prob(class_c)
   prob(class_c) ].

using more than two layers (deep learning)

   you might be wondering,    can we extend our vanilla neural network so
   that its output layer is fed into a 4th layer (and then a 5th, and 6th,
   etc.)?   . yes. this is what   s commonly referred to as    deep learning   .
   in practice it can be very effective. however, it   s worth noting that
   any network you build with more than one hidden layer can be mimicked
   by a network with only one hidden layer. in fact, you can approximate
   any continuous function using a neural network with a single hidden
   layer as per the [22]universal approximation theorem. the reason deep
   neural network architectures are frequently chosen in favor of single
   hidden layer architectures is that they tend to converge to a solution
   faster during the fitting procedure.

fitting the model to labeled training samples (id26)

   alas we come to the fitting procedure. so far we   ve discussed how
   neural networks could work effectively, but we haven   t discussed how to
   fit a neural network to labeled training samples. an equivalent
   question would be,    how can we choose the best weights for a network,
   given some labeled training samples?   . id119 is the common
   answer (although id113 can work too). continuing with our example
   problem, the id119 procedure would go something like this:
    1. start with some labeled training data
    2. choose a differentiable id168 to minimize,
       l(\mathbf{\widehat y}, \mathbf{y}) l(\mathbf{\widehat y},
       \mathbf{y})
    3. choose a network structure. specifically detemine how many layers
       and how many nodes in each layer.
    4. initialize the network   s weights randomly
    5. run the training data through the network to generate a prediction
       for each sample. measure the overall error according to the loss
       function,  l(\mathbf{\widehat y}, \mathbf{y}) l(\mathbf{\widehat
       y}, \mathbf{y}) . (this is called forward propagation)
    6. determine how much the current loss will change with respect to a
       small change in each of the weights. in other words, calculate the
       gradient of  l l  with respect to every weight in the network.
       (this is called backward propagation)
    7. take a small    step    in the direction of the negative gradient. for
       example, if  w_{23} = 1.5 w_{23} = 1.5  and  \frac{\partial
       l}{\partial w_{23}} = 2.2 \frac{\partial l}{\partial w_{23}} = 2.2
       , then decreasing  w_{23} w_{23}  by a small amount should result
       in a small decrease in the current loss. hence we update  w_3 :=
       w_3 - 2.2 \times 0.001 w_3 := w_3 - 2.2 \times 0.001  (where 0.001
       is our predetermined    step size   ).
    8. repeat this process (from step 5) a fixed number of times or until
       the loss converges

   that   s the basic idea at least. in practice, this poses a number of
   challenges.

challenge 1     computational complexity

   during the fitting procedure, one of the things we   ll need to calculate
   is the gradient of  l l  with respect to every weight. this is tricky
   because  l l  depends on every node in the output layer, and each of
   those nodes depends on every node in the layer before it, and so on.
   this means calculating  \frac{\partial l}{\partial w_{ab}}
   \frac{\partial l}{\partial w_{ab}}  is a chain-rule nightmare. (keep in
   mind that many real-wold neural networks have thousands of nodes across
   tens of layers.) the key to dealing with this is to recognize that most
   of the  \frac{\partial l}{\partial w_{ab}} \frac{\partial l}{\partial
   w_{ab}} s reuse the same intermediate derivatives when you apply the
   chain-rule. if you   re careful about tracking this, you can avoid
   recalculating the same thing thousands of times.

   another trick is to use special id180 whose derivatives
   can be written as a function of their value. for example, the
   derivative of  sigmoid(x) sigmoid(x)  =  sigmoid(x)(1 - sigmoid(x))
   sigmoid(x)(1 - sigmoid(x)) . this is convenient because during the
   forward pass, when we calculate  \widehat y \widehat y  for each
   training sample, we have to calculate  sigmoid(\mathbf{x})
   sigmoid(\mathbf{x})  element-wise for some vector  \mathbf{x}
   \mathbf{x} . during backprop we can reuse those values when calculating
   the gradient of  l l  with respect to the weights, saving time and
   memory.

   a third trick is to partition the training data into    mini batches    and
   update the weights with respect to each batch, one after another. for
   example, if you partition your training data into {batch1, batch2,
   batch3}, the first pass over the training data would
    1. update the weights using batch1
    2. update the weights using batch2
    3. update the weights using batch3

   where the gradient of  l l  is recalculated after each update.

   the last technique worth mentioning is to make use of gpu as opposed to
   cpu, as gpu is better suited to perform lots of calculations in
   parallel.

challenge 2     id119 may have trouble finding the absolute minimum

   this is not so much a neural network problem as it is a gradient
   descent problem. it   s possible that the weights could get stuck in a
   local minimum during id119. it   s also possible that weights
   can overshoot the minimum. one trick to dealing with this is to tinker
   with different step sizes. another trick is to increase the number of
   nodes and/or layers in the network. (beware of overfitting).
   additionally, some heuristic techniques like using [23]momentum can be
   effective.

challenge 3     how to generalize?

   how might we write a generic program to fit any neural network with any
   number of nodes and layers? the answer is,    you don   t, you
   use [24]tensorflow   . but if you really wanted to, the hard part is
   calculating the gradient of the id168. the trick to doing this
   is to recognize that you can represent the gradient as a recursive
   function. a neural network with 5 layers is just a neural network with
   4 layers that feeds into some id88s. but a neural network with 4
   layers is just a neural network with 3 layers that feed into some
   id88s. and so on it goes. this is more formally known as [25]auto
   differentiation.

share this:

     * [26]click to share on twitter (opens in new window)
     * [27]click to share on facebook (opens in new window)
     *

related

   [28]neural networks

comments 13

    1. [29]reply
   pioneer american
       [30]november 28, 2017 at 6:08 pm
       one thing worth knowing - local minima really only exist in low
       dimensions (few inputs). in high dimensions, most extrema(?) are
       saddle points, because: each extremum in each dimension can be
       either a minimum or a maximum. to be a local minimum, a point has
       to be a minimum in /all/ the dimensions, a 1 / 2^|dimensions|
       chance. that's why local minima aren't a problem in practice.
       source: andrew ng in one of his recent deep learning coursera
       courses.
    2. [31]reply
   tymoteusz cejrowski
       [32]november 29, 2017 at 12:39 am
       case a and b with the sigmod function has some copy-paste bug. you
       say "increase x_3 from 0 to 50" but on the picture we have 0 to 60.
       but overall - nice work keep it up!
         1. [33]reply
        ben gorman
            [34]november 29, 2017 at 7:16 am
            good catch. i've fixed this within my blog. i assume the fix
            will sync to this cross-post at some point. thanks!
    3. [35]reply
   rayed wahed
       [36]december 2, 2017 at 8:36 pm
       a fine post and explanation     
    4. [37]reply
   laci v  gh
       [38]december 3, 2017 at 4:25 am
       very good and easily understandable article. however, the 7th step
       of the id26 is not too clear for me. if we want to
       decrease the weight w23, then shouldn't we update w23 instead of w3
       in the example? is it only a typo? if not, then how is w23 related
       to w3?
         1. [39]reply
        ben gorman
            [40]december 3, 2017 at 12:01 pm
            good catch. it's a typo.
    5. [41]reply
   jeremymburton
       [42]december 5, 2017 at 5:26 pm
       great piece.
    6. [43]reply
   srikantankc kc
       [44]december 6, 2017 at 12:46 am
       its a good brief.but i could not get the weigths and sigmoids in a
       binary framework..any help please...
    7. [45]reply
   david macleod
       [46]december 24, 2017 at 2:08 am
       in example 1, identify the stairs pattern for the output layer you
       write:
       "add the score of the base models so that the final sigmoid
       function only fires if both widehat y_{left} and widehat y_{right}
       are large"
       i am not understanding the use of the word "both", as in the data
       examples directly below #3, #4, #5 have either a left or right
       stairs estimate of 0, and yet the final activation still "fires"
       and they are classified as stairs. only example #6 has a large
       value of yhat for both left and right stairs.
    8. [47]reply
   werner germ  n busch
       [48]january 17, 2018 at 6:34 pm
       i think there is a mistake, in this image
       [49]https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nn
       ets_sketch4-3.png and also in the following one, the final node
       should be z = w.(hat{y}_1,hat{y}_2,hat{y}_3) +b, not w.x +b right?
       also when you say
       "add the score of the base models so that the final sigmoid
       function only fires if both and are large"
       i think it should be an "or" rather an "and" correct? it fires if
       it identifies a right stair or a left stair, not both.
       cheers and thank you for the explanation!
    9. [50]reply
   werner germ  n busch
       [51]january 17, 2018 at 6:49 pm
       another thing, i think this formula:
       [52]https://s0.wp.com/latex.php?zoom=2&latex=z+%3d+w+%5ccdot+x+%3d+
       w_1x_1+%2b+w_2x_2+%2b+w_3x_3+%2b+w_4x_4++&bg=ffffff&fg=000&s=1
       should contain a "+b" right?
   10. [53]reply
   priya jindal
       [54]may 1, 2018 at 10:48 am
       how did you calculate your weights in single id88 model?
   11. [55]reply
   mohit motwani
       [56]july 6, 2018 at 12:06 am
       yes, how do you calculate the weights? plus i think there is a typo
       in the model 2 issues. according to your example, shouldn't the
       increase and decrease in 'x1' potentially effect the value of 'y'?

leave a reply [57]cancel reply

   your email address will not be published. required fields are marked *

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   name * ______________________________

   email * ______________________________

   website ______________________________

   [ ] save my name, email, and website in this browser for the next time
   i comment.

   [ ] notify me of follow-up comments by email.

   [ ] notify me of new posts by email.
   submit

the official blog of kaggle.com

   search ____________________

categories

     * [58]data notes (18)
     * [59]data science news (63)
     * [60]kaggle news (148)
     * [61]kernels (48)
     * [62]open datasets (12)
     * [63]pulse of the competition (1)
     * [64]students (4)
     * [65]tutorials (54)
     * [66]uncategorized (3)
     * [67]winners' interviews (228)

popular tags

   [68]#1 kaggler [69]annual santa competition [70]binary classification
   [71]community [72]id161 [73]convolutional neural networks
   [74]dark matter [75]data notes [76]data science careers [77]data
   visualization [78]deep neural networks [79]deloitte [80]diabetes
   [81]diabetic retinopathy [82]draper satellite image chronology [83]eeg
   data [84]elo chess ratings competition [85]eurovision challenge
   [86]flight quest [87]heritage health prize [88]how much did it rain?
   [89]image classification [90]kaggle datasets [91]kaggle inclass
   [92]kernels [93]id28 [94]march mania [95]meetup
   [96]multiclass classification [97]natural language processing [98]open
   data [99]open data spotlight [100]practice fusion [101]product
   [102]product news [103]profiling top kagglers [104]recruiting
   [105]regression problem [106]scikit-learn [107]scripts of the week
   [108]tourism forecasting [109]tutorial [110]video series [111]wikipedia
   challenge [112]xgboost

archives

   archives [select month__]
   [113]toggle the widgetbar

references

   visible links
   1. http://blog.kaggle.com/feed/
   2. http://blog.kaggle.com/comments/feed/
   3. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/feed/
   4. http://blog.kaggle.com/wp-json/oembed/1.0/embed?url=http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/
   5. http://blog.kaggle.com/wp-json/oembed/1.0/embed?url=http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/&format=xml
   6. http://blog.kaggle.com/
   7. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/
   8. http://www.kaggle.com/
   9. http://www.kaggle.com/
  10. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comments
  11. http://blog.kaggle.com/author/bengorman/
  12. https://gormanalysis.com/introduction-to-neural-networks/
  13. https://gormanalysis.com/
  14. https://github.com/ben519/mlpb/blob/master/problems/classify images of stairs/intro_to_nnets_article_materials.r
  15. https://github.com/ben519/mlpb
  16. https://en.wikipedia.org/wiki/id88
  17. https://en.wikipedia.org/wiki/sigmoid_function
  18. https://en.wikipedia.org/wiki/logistic_regression
  19. https://en.wikipedia.org/wiki/hyperbolic_function#hyperbolic_tangent
  20. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  21. https://en.wikipedia.org/wiki/softmax_function
  22. https://en.wikipedia.org/wiki/universal_approximation_theorem
  23. https://en.wikipedia.org/wiki/stochastic_gradient_descent#momentum
  24. https://www.tensorflow.org/
  25. https://en.wikipedia.org/wiki/automatic_differentiation
  26. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?share=twitter
  27. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?share=facebook
  28. http://blog.kaggle.com/tag/neural-networks/
  29. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14176#respond
  30. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14176
  31. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14178#respond
  32. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14178
  33. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14180#respond
  34. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14180
  35. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14185#respond
  36. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14185
  37. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14188#respond
  38. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14188
  39. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14189#respond
  40. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14189
  41. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14193#respond
  42. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14193
  43. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14194#respond
  44. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14194
  45. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14223#respond
  46. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14223
  47. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14253#respond
  48. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14253
  49. https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch4-3.png
  50. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=14254#respond
  51. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-14254
  52. https://s0.wp.com/latex.php?zoom=2&latex=z+=+w+\cdot+x+=+w_1x_1+++w_2x_2+++w_3x_3+++w_4x_4++&bg=ffffff&fg=000&s=1
  53. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=18257#respond
  54. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-18257
  55. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/?replytocom=26588#respond
  56. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#comment-26588
  57. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/#respond
  58. http://blog.kaggle.com/category/data-notes/
  59. http://blog.kaggle.com/category/data-science-news/
  60. http://blog.kaggle.com/category/kaggle-news/
  61. http://blog.kaggle.com/category/kernels/
  62. http://blog.kaggle.com/category/open-datasets/
  63. http://blog.kaggle.com/category/pulse-of-the-competition/
  64. http://blog.kaggle.com/category/students/
  65. http://blog.kaggle.com/category/tutorials/
  66. http://blog.kaggle.com/category/uncategorized/
  67. http://blog.kaggle.com/category/winners-interviews/
  68. http://blog.kaggle.com/tag/1-kaggler/
  69. http://blog.kaggle.com/tag/annual-santa-competition/
  70. http://blog.kaggle.com/tag/binary-classification/
  71. http://blog.kaggle.com/tag/community/
  72. http://blog.kaggle.com/tag/computer-vision/
  73. http://blog.kaggle.com/tag/convolutional-neural-networks/
  74. http://blog.kaggle.com/tag/dark-matter/
  75. http://blog.kaggle.com/tag/data-notes-tag/
  76. http://blog.kaggle.com/tag/data-science-careers/
  77. http://blog.kaggle.com/tag/data-visualization/
  78. http://blog.kaggle.com/tag/deep-neural-networks/
  79. http://blog.kaggle.com/tag/deloitte/
  80. http://blog.kaggle.com/tag/diabetes/
  81. http://blog.kaggle.com/tag/diabetic-retinopathy/
  82. http://blog.kaggle.com/tag/draper-satellite-image-chronology/
  83. http://blog.kaggle.com/tag/eeg-data/
  84. http://blog.kaggle.com/tag/elo-chess-ratings-competition/
  85. http://blog.kaggle.com/tag/eurovision-challenge/
  86. http://blog.kaggle.com/tag/flight-quest/
  87. http://blog.kaggle.com/tag/heritage-health-prize/
  88. http://blog.kaggle.com/tag/how-much-did-it-rain/
  89. http://blog.kaggle.com/tag/image-classification/
  90. http://blog.kaggle.com/tag/kaggle-datasets/
  91. http://blog.kaggle.com/tag/kaggle-inclass/
  92. http://blog.kaggle.com/tag/kernels/
  93. http://blog.kaggle.com/tag/logistic-regression/
  94. http://blog.kaggle.com/tag/march-mania/
  95. http://blog.kaggle.com/tag/meetup/
  96. http://blog.kaggle.com/tag/multiclass-classification/
  97. http://blog.kaggle.com/tag/natural-language-processing/
  98. http://blog.kaggle.com/tag/open-data/
  99. http://blog.kaggle.com/tag/open-data-spotlight/
 100. http://blog.kaggle.com/tag/practice-fusion/
 101. http://blog.kaggle.com/tag/product/
 102. http://blog.kaggle.com/tag/product-news/
 103. http://blog.kaggle.com/tag/profiling-top-kagglers/
 104. http://blog.kaggle.com/tag/recruiting/
 105. http://blog.kaggle.com/tag/regression-problem/
 106. http://blog.kaggle.com/tag/scikit-learn/
 107. http://blog.kaggle.com/tag/scripts-of-the-week/
 108. http://blog.kaggle.com/tag/tourism-forecasting/
 109. http://blog.kaggle.com/tag/tutorial/
 110. http://blog.kaggle.com/tag/video-series/
 111. http://blog.kaggle.com/tag/wikipedia-challenge/
 112. http://blog.kaggle.com/tag/xgboost/
 113. http://blog.kaggle.com/2017/11/27/introduction-to-neural-networks/

   hidden links:
 115. http://blog.kaggle.com/
 116. http://blog.kaggle.com/2017/12/06/introduction-to-neural-networks-2/
 117. http://blog.kaggle.com/2017/11/27/october-kaggle-dataset-publishing-awards-winners-interview/
 118. https://www.facebook.com/kaggle
 119. https://twitter.com/kaggle
