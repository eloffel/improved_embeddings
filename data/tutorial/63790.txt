   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   july 17, 2017     2 minute read

robust adversarial examples
     __________________________________________________________________

   we've created images that reliably fool neural network classifiers when
   viewed from varied scales and perspectives. this challenges a [11]claim
   from last week that self-driving cars would be hard to trick
   maliciously since they capture images from multiple scales, angles,
   perspectives, and the like.
   this innocuous kitten photo, printed on a standard color printer, fools
   the classifier into thinking it's a monitor or desktop computer
   regardless of how its zoomed or rotated. we expect further parameter
   tuning would also remove any human-visible artifacts.
   [12]read paper
     __________________________________________________________________

   out-of-the-box [13]adversarial examples do fail under image
   transformations. below, we show the same cat picture, adversarially
   perturbed to be incorrectly classified as a desktop computer by
   [14]inception v3 trained on [15]id163. a zoom of as little as 1.002
   causes the classification id203 for the correct label tabby cat
   to override the adversarial label desktop computer.
   your browser does not support the video tag.

   however, we'd suspected that active effort could produce a robust
   adversarial example, as adversarial examples have been shown to
   [16]transfer to the physical world.

scale-invariant adversarial examples

   adversarial examples can be created using an optimization method called
   projected id119 to find small perturbations to the image
   that arbitrarily fool the classifier.

   instead of optimizing for finding an input that's adversarial from a
   single viewpoint, we optimize over a large [17]ensemble of stochastic
   classifiers that randomly rescale the input before classifying it.
   optimizing against such an ensemble produces robust adversarial
   examples that are scale-invariant.
   your browser does not support the video tag.
   a scale-invariant adversarial example.

   even when we restrict ourselves to only modifying pixels corresponding
   to the cat, we can create a single perturbed image that is
   simultaneously adversarial at all desired scales.

transformation-invariant adversarial examples

   by adding random rotations, translations, scales, noise, and mean
   shifts to our training perturbations, the same technique produces a
   single input that remains adversarial under any of these
   transformations.
   your browser does not support the video tag.
   a transformation-invariant adversarial example. note that it is visibly
   more perturbed than its scale-invariant cousin. this might be
   fundamental: it's intuitively plausible that small adversarial
   perturbations are harder to find the more transformations an example
   must be invariant to.

   our transformations are sampled randomly at test time, demonstrating
   that our example is invariant to the whole distribution of
   transformations.
     __________________________________________________________________

   authors
   [18]anish athalye
     __________________________________________________________________

     * [19]about
     * [20]progress
     * [21]resources
     * [22]blog
     * [23]charter
     * [24]jobs
     * [25]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1707.03501
  12. https://arxiv.org/abs/1707.07397
  13. https://blog.openai.com/adversarial-example-research/
  14. https://arxiv.org/abs/1512.00567
  15. http://www.image-net.org/
  16. https://arxiv.org/pdf/1607.02533.pdf
  17. https://people.eecs.berkeley.edu/~liuchang/paper/transferability_iclr_2017.pdf
  18. https://openai.com/blog/authors/anish-athalye/
  19. https://openai.com/about/
  20. https://openai.com/progress/
  21. https://openai.com/resources/
  22. https://openai.com/blog/
  23. https://openai.com/charter/
  24. https://openai.com/jobs/
  25. https://openai.com/press/

   hidden links:
  27. https://openai.com/
  28. https://openai.com/
  29. https://twitter.com/openai
  30. https://www.facebook.com/openai.research
