
   (button)
     * [1]home
     * [2]research
          + [3]publications
          + [4]alphago
          + [5]id25
          + [6]dnc
          + [7]open source
          + [8]access to science
     * [9]applied
          + [10]deepmind health
          + [11]deepmind for google
          + [12]deepmind ethics & society
     * [13]news & blog
     * [14]about us
     * [15]careers

   (button) search (button) search
   [deepmind_logo_swirl.png]

[16]research

highlighted research

     * [17]alphago
     * [18]id25
     * [19]dnc

[20]publications

[21]open source

latest research news

   [22]towards robust and verified ai: specification testing, robust
   training, and formal verification

[23]applied

[24]deepmind health

[25]deepmind for google

[26]deepmind ethics & society

latest applied news

   [27]scaling streams with google

[28]careers

     * [29]home
     * [30]news & blog
     * [31]about us
     * [32]press
     * [33]terms and conditions
     * [34]privacy policy     updated

     *
     *
     *

   [shutterstock_509865907.2e16d0ba.fill-1100x400_wcymnl5.jpg]

going beyond average for id23

   consider the commuter who toils backwards and forwards each day on a
   train. most mornings, her train runs on time and she reaches her first
   meeting relaxed and ready. but she knows that once in awhile the
   unexpected happens: a mechanical problem, a signal failure, or even
   just a particularly rainy day. invariably these hiccups disrupt her
   pattern, leaving her late and flustered.

   randomness is something we encounter everyday and has a profound effect
   on how we experience the world. the same is true in reinforcement
   learning (rl) applications, systems that learn by trial and error and
   are motivated by rewards. typically, an rl algorithm predicts the
   average reward it receives from multiple attempts at a task, and uses
   this prediction to decide how to act. but random perturbations in the
   environment can alter its behaviour by changing the exact amount of
   reward the system receives.

   in [35]a new paper, we show it is possible to model not only the
   average but also the full variation of this reward, what we call the
   value distribution. this results in rl systems that are more accurate
   and faster to train than previous models, and more importantly opens up
   the possibility of rethinking the whole of id23.

   returning to the example of our commuter, let   s consider a journey
   composed of three segments of 5 minutes each, except that once a week
   the train breaks down, adding another 15 minutes to the trip. a simple
   calculation shows that the average commute time is (3 x 5) + 15 / 5 =
   18 minutes.
   [stations_delay.width-400_uunvlys.png]

   in id23, we use bellman's equation to predict this
   average commute time. specifically, bellman   s equation relates our
   current average prediction to the average prediction we make in the
   immediate future. from the first station, we predict an 18 minutes
   journey (the average total duration); from the second, we predict a 13
   minutes journey (average duration minus the first segment   s length).
   finally, assuming the train hasn   t yet broken down, from the third
   station we predict there are 8 minutes (13 - 5) left to our commute,
   until finally we arrive at our destination. bellman   s equation makes
   each prediction sequentially, and updates these predictions on the
   basis of new information.

   what's a little counterintuitive about bellman   s equation is that we
   never actually observe these predicted averages: either the train takes
   15 minutes (4 days out of 5), or it takes 30 minutes     never 18! from a
   purely mathematical standpoint, this isn   t a problem, because decision
   theory tells us we only need averages to make the best choice. as a
   result, this issue has been mostly ignored in practice. yet, there is
   now plenty of [36]empirical [37]evidence that predicting averages is a
   complicated business.

   it   s already evident from our empirical results that the distributional
   perspective leads to better, more stable id23

   in [38]our new paper, we  show that there is in fact a variant of
   bellman's equation which predicts all possible outcomes, without
   averaging them. in our example, we maintain two predictions     a
   distribution     at each station: if the journey goes well, then the
   times are 15, 10, then 5 minutes, respectively; but if the train breaks
   down, then the times are 30, 25, and finally 20 minutes.

   all of id23 can be reinterpreted under this new
   perspective, and its application is already leading to surprising new
   theoretical results. predicting the distribution over outcomes also
   opens up all kinds of algorithmic possibilities, such as:
     * disentangling the causes of randomness: once we observe that
       commute times are bimodal, i.e. take on two possible values, we can
       act on this information, for example checking for train updates
       before leaving home;
     * telling safe and risky choices apart: when two choices have the
       same average outcome (e.g., walking or taking the train), [39]we
       may favour the one which varies the least (walking)..
     * natural auxiliary predictions: predicting a multitude of outcomes,
       such as the distribution of commute times, has [40]been shown to
       [41]be beneficial for training deep networks faster.

   we took our new ideas and implemented them within the [42]deep
   q-network agent, replacing its single average reward output with a
   distribution with 51 possible values. the only other change was a new
   learning rule, reflecting the transition from bellman   s (average)
   equation to its distributional counterpart. incredibly, it turns out
   going from averages to distributions was all we needed to surpass the
   performance of all other comparable approaches, and by a wide margin.
   the graph below shows how we get 75% of a trained deep q-network   s
   performance in 25% of the time, and achieve significantly better human
   performance:

   [train_sup.width-400_urraco6.png]

   one surprising result is that we observe some randomness in atari 2600
   games, even though stella, the underlying game emulator, is itself
   fully predictable. this randomness arises in part because of what   s
   called partial observability: due to the internal programming of the
   emulator, our agents playing the game of pong cannot predict the exact
   time at which their score increases. visualising the agent   s prediction
   over successive frames (graphs below) we see two separate outcomes (low
   and high), reflecting the possible timings. although this intrinsic
   randomness doesn   t directly impact performance, our results highlight
   the limits of our agents    understanding.

   [pong-intrinsic-randomness.width-400_bxd6mow.png]

   randomness also occurs because the agent   s own behaviour is uncertain.
   in space invaders, our agent learns to predict the future id203
   that it might make a mistake and lose the game (zero reward).

   space invaders gif

   just like in our train journey example, it makes sense to keep separate
   predictions for these vastly different outcomes, rather than aggregate
   them into an unrealisable average. in fact, we think that our improved
   results are in great part due to the agent   s ability to model its own
   randomness.

   it   s already evident from our empirical results that the distributional
   perspective leads to better, more stable id23. with
   the possibility that every id23 concept could now
   want a distributional counterpart, it might just be the beginning for
   this approach.
     __________________________________________________________________

   this work was done by marc g. bellemare*, will dabney*, and r  mi munos.

   read [43]the paper in full.

   share article
     *
     *
     *
     *
     * [ ]
          + [44]linkedin
          + whatsapp
          + sms
          + [45]reddit

   authors
   monday, 24 july 2017
     * [bellemare.2e16d0ba.fill-80x80_qfwrijt.png]
       marc gendron-bellemare
       research scientist, deepmind
     * [wdabney.2e16d0ba.fill-80x80_4sluivv.png]
       will dabney
       research scientist, deepmind
     * [munos.2e16d0ba.fill-80x80_jifjh0l.png]
       remi munos
       research scientist, deepmind

   ____________________
   ____________________
   [46]show all results
   (button) close

                               [47]deepmind logo

   follow
     *
     *
     *

     * [48]research [49]research
     * [50]applied [51]applied
     * [52]news & blog [53]news & blog
     * [54]about us [55]about us
     * [56]careers [57]careers

     * [58]press
     * [59]terms and conditions
     * [60]privacy policy     updated
     * [61]modern slavery statement
     * [62]alphabet inc

      2019 deepmind technologies limited

   deepmind.com uses cookies to help give you the best possible user
   experience and to allow us to see how the site is used. by using this
   site, you agree that we can set and use these cookies. for more
   information on cookies and how to change your settings, see our
   [63]privacy policy.

references

   visible links
   1. https://deepmind.com/
   2. https://deepmind.com/research/
   3. https://deepmind.com/research/publications/
   4. https://deepmind.com/research/alphago/
   5. https://deepmind.com/research/id25/
   6. https://deepmind.com/research/dnc/
   7. https://deepmind.com/research/open-source/
   8. https://deepmind.com/research/access-science/
   9. https://deepmind.com/applied/
  10. https://deepmind.com/applied/deepmind-health/
  11. https://deepmind.com/applied/deepmind-google/
  12. https://deepmind.com/applied/deepmind-ethics-society/
  13. https://deepmind.com/blog/
  14. https://deepmind.com/about/
  15. https://deepmind.com/careers/
  16. https://deepmind.com/research/
  17. https://deepmind.com/research/alphago/
  18. https://deepmind.com/research/id25/
  19. https://deepmind.com/research/dnc/
  20. https://deepmind.com/research/publications/
  21. https://deepmind.com/research/open-source/
  22. https://deepmind.com/blog/robust-and-verified-ai/
  23. https://deepmind.com/applied/
  24. https://deepmind.com/applied/deepmind-health/
  25. https://deepmind.com/applied/deepmind-google/
  26. https://deepmind.com/applied/deepmind-ethics-society/
  27. https://deepmind.com/blog/scaling-streams-google/
  28. https://deepmind.com/careers/
  29. https://deepmind.com/
  30. https://deepmind.com/blog/
  31. https://deepmind.com/about/
  32. https://deepmind.com/press/
  33. https://deepmind.com/terms-and-conditions/
  34. https://deepmind.com/privacy-policy/
  35. https://arxiv.org/abs/1707.06887
  36. https://arxiv.org/abs/1512.04860
  37. https://arxiv.org/abs/1509.06461
  38. https://arxiv.org/abs/1707.06887
  39. http://www.mit.edu/~jnt/papers/j145-13-mv-mdp.pdf
  40. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.8707&rep=rep1&type=pdf
  41. https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/
  42. https://deepmind.com/research/id25/
  43. https://arxiv.org/abs/1707.06887
  44. https://www.linkedin.com/sharearticle?mini=true&url=https://deepmind.com/blog/going-beyond-average-reinforcement-learning/&title=going beyond average for id23&summary=&source=google deepmind
  45. http://www.reddit.com/submit?url=https://deepmind.com/blog/going-beyond-average-reinforcement-learning/&title=going beyond average for id23
  46. https://deepmind.com/blog/going-beyond-average-reinforcement-learning/
  47. https://deepmind.com/
  48. https://deepmind.com/research/
  49. https://deepmind.com/research/
  50. https://deepmind.com/applied/
  51. https://deepmind.com/applied/
  52. https://deepmind.com/blog/
  53. https://deepmind.com/blog/
  54. https://deepmind.com/about/
  55. https://deepmind.com/about/
  56. https://deepmind.com/careers/
  57. https://deepmind.com/careers/
  58. https://deepmind.com/press/
  59. https://deepmind.com/terms-and-conditions/
  60. https://deepmind.com/privacy-policy/
  61. https://storage.googleapis.com/deepmind-media/modern_slavery/final_201_google_modern_slavery_statement.pdf
  62. https://abc.xyz/
  63. https://deepmind.com/privacy-policy/

   hidden links:
  65. https://twitter.com/deepmindai
  66. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  67. https://plus.google.com/+deepmindai
  68. http://twitter.com/intent/tweet/?text=&url=https%3a//deepmind.com/blog/going-beyond-average-reinforcement-learning/
  69. http://www.facebook.com/share.php?u=https%3a//deepmind.com/blog/going-beyond-average-reinforcement-learning/&t=
  70. https://plus.google.com/share?url=https%3a//deepmind.com/blog/going-beyond-average-reinforcement-learning/
  71. mailto:?subject=going%20beyond%20average%20for%20reinforcement%20learning&body=%0d%0a%0d%0ahttps%3a//deepmind.com/blog/going-beyond-average-reinforcement-learning/
  72. https://twitter.com/deepmindai
  73. https://www.youtube.com/channel/ucp7jmxsy2xbc3kcae0mhq-a
  74. https://plus.google.com/+deepmindai
