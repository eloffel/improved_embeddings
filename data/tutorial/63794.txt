   #[1]alternate

   hiring? toptal handpicks [2]top machine learning engineers to suit your
   needs.

     * [3]start hiring
     * [4]log in

     *
     * [5]top 3%
     * [6]why
     * [7]clients
     * [8]enterprise
     * [9]community
     * [10]blog
     * [11]about us
     * [12]start hiring
     * [13]apply as a developer
     * [14]login
     *
          + questions?
          + [15]contact us
          +
          +
          +

   ____________________ (button)
   [16]hire a developer

   hiring? toptal handpicks [17]top machine learning engineers to suit
   your needs.

     * [18]start hiring
     * [19]log in

   18 min read

getting started with tensorflow: a machine learning tutorial

   [20]view all articles

   [small_fe2df41a0d4d03df16700674a2a7f9e4.jpeg]

   by [21]dino causevic - freelance software engineer @ [22]toptal
   [23]#knn [24]#linearregression [25]#tensorflow
     * 0shares
     * [twitter_83c6d4.png]
     * [facebook_dc66c9.png]
     * [linkedin_b923c3.png]
     * [pocket_e4260c.png]

   tensorflow is an open source software library created by google that is
   used to implement machine learning and deep learning systems. these two
   names contain a series of powerful algorithms that share a common
   challenge   to allow a computer to learn how to automatically spot
   complex patterns and/or to make best possible decisions.

   if you   re interested in details about these systems, you can learn more
   from the toptal blog posts on [26]machine learning and [27]deep
   learning.

   tensorflow tutorial

   tensorflow, at its heart, is a library for dataflow programming. it
   leverages various optimization techniques to make the calculation of
   mathematical expressions easier and more performant.

   some of the key features of tensorflow are:
     * efficiently works with mathematical expressions involving
       multi-dimensional arrays
     * good support of deep neural networks and machine learning concepts
     * gpu/cpu computing where the same code can be executed on both
       architectures
     * high scalability of computation across machines and huge data sets

   together, these features make tensorflow the perfect framework for
   machine intelligence at a production scale.

   in this tensorflow tutorial, you will learn how you can use simple yet
   powerful machine learning methods in tensorflow and how you can use
   some of its auxiliary libraries to debug, visualize, and tweak the
   models created with it.

installing tensorflow

   we will be using the tensorflow python api, which works with python 2.7
   and python 3.3+. the gpu version (linux only) requires the cuda toolkit
   7.0+ and cudnn v2+.

   we shall use the conda package dependency management system to install
   tensorflow. conda allows us to separate multiple environments on a
   machine. you can learn how to install conda from [28]here.

   after installing conda, we can create the environment that we will use
   for tensorflow installation and use. the following command will create
   our environment with some additional libraries like [29]numpy, which is
   very useful once we start to use tensorflow.

   the python version installed inside this environment is 2.7, and we
   will use this version in this article.
conda create --name tensorflowenv biopython

   to make things easy, we are installing biopython here instead of just
   numpy. this includes numpy and a few other packages that we will be
   needing. you can always install the packages as you need them using the
   conda install or the pip install commands.

   the following command will activate the created conda environment. we
   will be able to use packages installed within it, without mixing with
   packages that are installed globally or in some other environments.
source activate tensorflowenv

   the pip installation tool is a standard part of a conda environment. we
   will use it to install the tensorflow library. prior to doing that, a
   good first step is updating pip to the latest version, using the
   following command:
pip install --upgrade pip

   now we are ready to install tensorflow, by running:
pip install tensorflow

   the download and build of tensorflow can take several minutes. at the
   time of writing, this installs tensorflow 1.1.0.

data flow graphs

   in tensorflow, computation is described using data flow graphs. each
   node of the graph represents an instance of a mathematical operation
   (like addition, division, or multiplication) and each edge is a
   multi-dimensional data set (tensor) on which the operations are
   performed.

   a simple data flow graph

   as tensorflow works with computational graphs, they are managed where
   each node represents the instantiation of an operation where each
   operation has zero or more inputs and zero or more outputs.

   edges in tensorflow can be grouped in two categories: normal edges
   transfer data structure (tensors) where it is possible that the output
   of one operation becomes the input for another operation and special
   edges, which are used to control dependency between two nodes to set
   the order of operation where one node waits for another to finish.

simple expressions

   before we move on to discuss elements of tensorflow, we will first do a
   session of working with tensorflow, to get a feeling of what a
   tensorflow program looks like.

   let   s start with simple expressions and assume that, for some reason,
   we want to evaluate the function y = 5*x + 13 in tensorflow fashion.

   in simple python code, it would look like:
x = -2.0
y = 5*x + 13
print y

   which gives us in this case a result of 3.0.

   now we will convert the above expression into tensorflow terms.

constants

   in tensorflow, constants are created using the function constant, which
   has the signature constant(value, dtype=none, shape=none, name='const',
   verify_shape=false), where value is an actual constant value which will
   be used in further computation, dtype is the data type parameter (e.g.,
   float32/64, int8/16, etc.), shape is optional dimensions, name is an
   optional name for the tensor, and the last parameter is a boolean which
   indicates verification of the shape of values.

   if you need constants with specific values inside your training model,
   then the constant object can be used as in following example:
z = tf.constant(5.2, name="x", dtype=tf.float32)

variables

   variables in tensorflow are in-memory buffers containing tensors which
   have to be explicitly initialized and used in-graph to maintain state
   across session. by simply calling the constructor the variable is added
   in computational graph.

   variables are especially useful once you start with training models,
   and they are used to hold and update parameters. an initial value
   passed as an argument of a constructor represents a tensor or object
   which can be converted or returned as a tensor. that means if we want
   to fill a variable with some predefined or random values to be used
   afterwards in the training process and updated over iterations, we can
   define it in the following way:
k = tf.variable(tf.zeros([1]), name="k")

   another way to use variables in tensorflow is in calculations where
   that variable isn   t trainable and can be defined in the following way:
k = tf.variable(tf.add(a, b), trainable=false)

sessions

   in order to actually evaluate the nodes, we must run a computational
   graph within a session.

   a session encapsulates the control and state of the tensorflow runtime.
   a session without parameters will use the default graph created in the
   current session, otherwise the session class accepts a graph parameter,
   which is used in that session to be executed.

   below is a brief code snippet that shows how the terms defined above
   can be used in tensorflow to calculate a simple linear function.
import tensorflow as tf

x = tf.constant(-2.0, name="x", dtype=tf.float32)
a = tf.constant(5.0, name="a", dtype=tf.float32)
b = tf.constant(13.0, name="b", dtype=tf.float32)

y = tf.variable(tf.add(tf.multiply(a, x), b))

init = tf.global_variables_initializer()

with tf.session() as session:
    session.run(init)
    print session.run(y)

using tensorflow: defining computational graphs

   the good thing about working with dataflow graphs is that the execution
   model is separated from its execution (on cpu, gpu, or some
   combination) where, once implemented, software in tensorflow can be
   used on the cpu or gpu where all complexity related to code execution
   is hidden.

   the computation graph can be built in the process of using the
   tensorflow library without having to explicitly instantiate [30]graph
   objects.

   a graph object in tensorflow can be created as a result of a simple
   line of code like c = tf.add(a, b). this will create an operation node
   that takes two tensors a and b that produce their sum c as output.

   the computation graph is a built-in process that uses the library
   without needing to call the [31]graph object directly. a graph object
   in tensorflow, which contains a set of operations and tensors as units
   of data, is used between operations which allows the same process and
   contains more than one graph where each graph will be assigned to a
   different session. for example, the simple line of code c = tf.add(a,
   b) will create an operation node that takes two tensors a and b as
   input and produces their sum c as output.

   tensorflow also provides a feed mechanism for patching a tensor to any
   operation in the graph, where the feed replaces the output of an
   operation with the tensor value. the feed data are passed as an
   argument in the run() function call.

   a placeholder is tensorflow   s way of allowing developers to inject data
   into the computation graph through placeholders which are bound inside
   some expressions. the signature of the placeholder is:
placeholder(dtype, shape=none, name=none)

   where dtype is the type of elements in the tensors and can provide both
   the shape of the tensors to be fed and the name for the operation.

   if the shape isn   t passed, this tensor can be fed with any shape. an
   important note is that the placeholder tensor has to be fed with data,
   otherwise, upon execution of the session and if that part is missing,
   the placeholder generates an error with the following structure:
invalidargumenterror (see above for traceback): you must feed a value for placeh
older tensor 'y' with dtype float

   the advantage of placeholders is that they allow developers to create
   operations, and the computational graph in general, without needing to
   provide the data in advance for that, and the data can be added in
   runtime from external sources.

   let   s take a simple problem of multiplying two integers x and y in
   tensorflow fashion, where a placeholder will be used together with a
   feed mechanism through the session run method.
import tensorflow as tf

x = tf.placeholder(tf.float32, name="x")
y = tf.placeholder(tf.float32, name="y")

z = tf.multiply(x, y, name="z")

with tf.session() as session:
    print session.run(z, feed_dict={x: 2.1, y: 3.0})

visualizing the computational graph with tensorboard

   tensorboard is a visualization tool for analyzing data flow graphs.
   this can be useful for gaining better understanding of machine learning
   models.

   with tensorboard, you can gain insight into different types of
   statistics about the parameters and details about the parts of the
   computational graph in general. it is not unusual that a deep neural
   network has large number of nodes. tensorboard allows developers to get
   insight into each node and how the computation is executed over the
   tensorflow runtime.

   tensorboard graph view

   now let   s get back to our example from the beginning of this tensorflow
   tutorial where we defined a linear function with the format y = a*x +
   b.

   in order to log events from session which later can be used in
   tensorboard, tensorflow provides the filewriter class. it can be used
   to create an event file for storing [32]summaries and [33]events where
   the constructor accepts six parameters and looks like:
__init__(logdir, graph=none, max_queue=10, flush_secs=120, graph_def=none, filen
ame_suffix=none)

   where the logdir parameter is required, and others have default values.
   the graph parameter will be passed from the session object created in
   the training program. the full example code looks like:
import tensorflow as tf


x = tf.constant(-2.0, name="x", dtype=tf.float32)
a = tf.constant(5.0, name="a", dtype=tf.float32)
b = tf.constant(13.0, name="b", dtype=tf.float32)


y = tf.variable(tf.add(tf.multiply(a, x), b))


init = tf.global_variables_initializer()


with tf.session() as session:
    merged = tf.summary.merge_all() // new
    writer = tf.summary.filewriter("logs", session.graph) // new


    session.run(init)
    print session.run(y)

   we added just two new lines. we merge all the summaries collected in
   the default graph, and filewriter is used to dump events to the file as
   we described above, respectively.

   after running the program, we have the file in the directory logs, and
   the last step is to run tensorboard:
tensorboard --logdir logs/

   now tensorboard is started and running on the default port 6006. after
   opening http://localhost:6006 and clicking on the graphs menu item
   (located at the top of the page), you will be able to see the graph,
   like the one in the picture below:

   tensorboard data flow graph

   tensorboard marks constants and summary nodes specific symbols, which
   are described below.

   graph icons

mathematics with tensorflow

   tensors are the basic data structures in tensorflow, and they represent
   the connecting edges in a dataflow graph.

   a tensor simply identifies a multidimensional array or list. the tensor
   structure can be identified with three parameters: rank, shape, and
   type.
     * rank: identifies the number of dimensions of the tensor. a rank is
       known as the order or n-dimensions of a tensor, where for example
       rank 1 tensor is a vector or rank 2 tensor is matrix.
     * shape: the shape of a tensor is the number of rows and columns it
       has.
     * type: the data type assigned to tensor elements.

   to build a tensor in tensorflow, we can build an n-dimensional array.
   this can be done easily by using the numpy library, or by converting a
   python n-dimensional array into a tensorflow tensor.

   tensors with different dimensions

   to build a 1-d tensor, we will use a numpy array, which we   ll construct
   by passing a built-in python list.
import numpy as np
tensor_1d = np.array([1.45, -1, 0.2, 102.1])

   working with this kind of array is similar to working with a built-in
   python list. the main difference is that the numpy array also contains
   some additional properties, like dimension, shape, and type.
>> print tensor_1d
[   1.45   -1.      0.2   102.1 ]

>> print tensor_1d[0]
1.45

>> print tensor_1d[2]
0.2

>> print tensor_1d.ndim
1

>> print tensor_1d.shape
(4,)

>> print tensor_1d.dtype
float64

   a numpy array can be easily converted into a tensorflow tensor with the
   auxiliary function [34]convert_to_tensor, which helps developers
   convert python objects to tensor objects. this function accepts tensor
   objects, numpy arrays, python lists, and python scalars.
tensor = tf.convert_to_tensor(tensor_1d, dtype=tf.float64)

   now if we bind our tensor to the tensorflow session, we will be able to
   see the results of our conversion.
tensor = tf.convert_to_tensor(tensor_1d, dtype=tf.float64)

with tf.session() as session:
    print session.run(tensor)
    print session.run(tensor[0])
    print session.run(tensor[1])

   output:
[   1.45   -1.      0.2   102.1 ]
1.45
-1.0

   we can create a 2-d tensor, or matrix, in a similar way:
tensor_2d = np.array(np.random.rand(4, 4), dtype='float32')
tensor_2d_1 = np.array(np.random.rand(4, 4), dtype='float32')
tensor_2d_2 = np.array(np.random.rand(4, 4), dtype='float32')

m1 = tf.convert_to_tensor(tensor_2d)
m2 = tf.convert_to_tensor(tensor_2d_1)
m3 = tf.convert_to_tensor(tensor_2d_2)
mat_product = tf.matmul(m1, m2)
mat_sum = tf.add(m2, m3)
mat_det = tf.matrix_determinant(m3)

with tf.session() as session:
    print session.run(mat_product)
    print session.run(mat_sum)
    print session.run(mat_det)

tensor operations

   in the example above, we introduce a few tensorflow operations on the
   vectors and matrices. the operations perform certain calculations on
   the tensors. which calculations those are is shown in the table below.
   tensorflow operator description
   [35]tf.add          x+y
   [36]tf.subtract     x-y
   [37]tf.multiply     x*y
   [38]tf.div          x/y
   [39]tf.mod          x % y
   [40]tf.abs          |x|
   [41]tf.negative     -x
   [42]tf.sign         sign(x)
   [43]tf.square       x*x
   [44]tf.round        round(x)
   [45]tf.sqrt         sqrt(x)
   [46]tf.pow          x^y
   [47]tf.exp          e^x
   [48]tf.log          log(x)
   [49]tf.maximum      max(x, y)
   [50]tf.minimum      min(x, y)
   [51]tf.cos          cos(x)
   [52]tf.sin          sin(x)

   tensorflow operations listed in the table above work with tensor
   objects, and are performed element-wise. so if you want to calculate
   the cosine for a vector x, the tensorflow operation will do
   calculations for each element in the passed tensor.
tensor_1d = np.array([0, 0, 0])
tensor = tf.convert_to_tensor(tensor_1d, dtype=tf.float64)
with tf.session() as session:
    print session.run(tf.cos(tensor))

   output:
[ 1.  1.  1.]

matrix operations

   matrix operations are very important for machine learning models, like
   id75, as they are often used in them. tensorflow supports
   all the most common matrix operations, like [53]multiplication,
   [54]transposing, [55]inversion, calculating the [56]determinant,
   solving [57]linear equations, and [58]many more.

   next up, we will explain some of the matrix operations. they tend to be
   important when comes to machine learning models, like in linear
   regression. let   s write some code that will do basic matrix operations
   like multiplication, getting the [59]transpose, getting the
   determinant, multiplication, sol, and many more.

   below are basic examples of calling these operations.
import tensorflow as tf
import numpy as np

def convert(v, t=tf.float32):
    return tf.convert_to_tensor(v, dtype=t)

m1 = convert(np.array(np.random.rand(4, 4), dtype='float32'))
m2 = convert(np.array(np.random.rand(4, 4), dtype='float32'))
m3 = convert(np.array(np.random.rand(4, 4), dtype='float32'))
m4 = convert(np.array(np.random.rand(4, 4), dtype='float32'))
m5 = convert(np.array(np.random.rand(4, 4), dtype='float32'))

m_tranpose = tf.transpose(m1)
m_mul = tf.matmul(m1, m2)
m_det = tf.matrix_determinant(m3)
m_inv = tf.matrix_inverse(m4)
m_solve = tf.matrix_solve(m5, [[1], [1], [1], [1]])

with tf.session() as session:
    print session.run(m_tranpose)
    print session.run(m_mul)
    print session.run(m_inv)
    print session.run(m_det)
    print session.run(m_solve)

transforming data

reduction

   tensorflow supports different kinds of reduction. reduction is an
   operation that removes one or more dimensions from a tensor by
   performing certain operations across those dimensions. a list of
   supported reductions for the current version of tensorflow can be found
   here. we will present a few of them in the example below.
import tensorflow as tf
import numpy as np

def convert(v, t=tf.float32):
    return tf.convert_to_tensor(v, dtype=t)

x = convert(
    np.array(
        [
            (1, 2, 3),
            (4, 5, 6),
            (7, 8, 9)
        ]), tf.int32)

bool_tensor = convert([(true, false, true), (false, false, true), (true, false,
false)], tf.bool)

red_sum_0 = tf.reduce_sum(x)
red_sum = tf.reduce_sum(x, axis=1)

red_prod_0 = tf.reduce_prod(x)
red_prod = tf.reduce_prod(x, axis=1)

red_min_0 = tf.reduce_min(x)
red_min = tf.reduce_min(x, axis=1)

red_max_0 = tf.reduce_max(x)
red_max = tf.reduce_max(x, axis=1)

red_mean_0 = tf.reduce_mean(x)
red_mean = tf.reduce_mean(x, axis=1)

red_bool_all_0 = tf.reduce_all(bool_tensor)
red_bool_all = tf.reduce_all(bool_tensor, axis=1)

red_bool_any_0 = tf.reduce_any(bool_tensor)
red_bool_any = tf.reduce_any(bool_tensor, axis=1)


with tf.session() as session:
    print "reduce sum without passed axis parameter: ", session.run(red_sum_0)
    print "reduce sum with passed axis=1: ", session.run(red_sum)

    print "reduce product without passed axis parameter: ", session.run(red_prod
_0)
    print "reduce product with passed axis=1: ", session.run(red_prod)

    print "reduce min without passed axis parameter: ", session.run(red_min_0)
    print "reduce min with passed axis=1: ", session.run(red_min)

    print "reduce max without passed axis parameter: ", session.run(red_max_0)
    print "reduce max with passed axis=1: ", session.run(red_max)

    print "reduce mean without passed axis parameter: ", session.run(red_mean_0)
    print "reduce mean with passed axis=1: ", session.run(red_mean)

    print "reduce bool all without passed axis parameter: ", session.run(red_boo
l_all_0)
    print "reduce bool all with passed axis=1: ", session.run(red_bool_all)

    print "reduce bool any without passed axis parameter: ", session.run(red_boo
l_any_0)
    print "reduce bool any with passed axis=1: ", session.run(red_bool_any)

   output:
reduce sum without passed axis parameter:  45
reduce sum with passed axis=1:  [ 6 15 24]
reduce product without passed axis parameter:  362880
reduce product with passed axis=1:  [  6 120 504]
reduce min without passed axis parameter:  1
reduce min with passed axis=1:  [1 4 7]
reduce max without passed axis parameter:  9
reduce max with passed axis=1:  [3 6 9]
reduce mean without passed axis parameter:  5
reduce mean with passed axis=1:  [2 5 8]
reduce bool all without passed axis parameter:  false
reduce bool all with passed axis=1:  [false false false]
reduce bool any without passed axis parameter:  true
reduce bool any with passed axis=1:  [ true  true  true]

   the first parameter of reduction operators is the tensor that we want
   to reduce. the second parameter is the indexes of dimensions along
   which we want to perform the reduction. that parameter is optional, and
   if not passed, reduction will be performed along all dimensions.

   we can take a look at the [60]reduce_sum operation. we pass a 2-d
   tensor, and want to reduce it along dimension 1.

   in our case, the resulting sum would be:
[1 + 2 + 3 = 6, 4 + 5 + 6 = 15, 7 + 8 + 9 = 24]

   if we passed dimension 0, the result would be:
[1 + 4 + 7 = 12, 2 + 5 + 8 = 15, 3 + 6 + 9 = 18]

   if we don   t pass any axis, the result is just the overall sum of:
1 + 4 + 7 = 12, 2 + 5 + 8 = 15, 3 + 6 + 9 = 45

   all reduction functions have a similar interface and are listed in the
   tensorflow [61]reduction documentation.

segmentation

   segmentation is a process in which one of the dimensions is the process
   of mapping dimensions onto provided segment indexes, and the resulting
   elements are determined by an index row.

   segmentation is actually grouping the elements under repeated indexes,
   so for example, in our case, we have segmented ids [0, 0, 1, 2, 2]
   applied on tensor tens1, meaning that the first and second arrays will
   be transformed following segmentation operation (in our case summation)
   and will get a new array, which looks like (2, 8, 1, 0) = (2+0, 5+3,
   3-2, -5+5). the third element in tensor tens1 is untouched because it
   isn   t grouped in any repeated index, and last two arrays are summed in
   same way as it was the case for the first group. beside summation,
   tensorflow supports [62]product, [63]mean, [64]max, and [65]min.

   segmentation summation
import tensorflow as tf
import numpy as np




def convert(v, t=tf.float32):
    return tf.convert_to_tensor(v, dtype=t)


seg_ids = tf.constant([0, 0, 1, 2, 2])
tens1 = convert(np.array([(2, 5, 3, -5), (0, 3, -2, 5), (4, 3, 5, 3), (6, 1, 4,
0), (6, 1, 4, 0)]), tf.int32)
tens2 = convert(np.array([1, 2, 3, 4, 5]), tf.int32)


seg_sum = tf.segment_sum(tens1, seg_ids)
seg_sum_1 = tf.segment_sum(tens2, seg_ids)


with tf.session() as session:
    print "segmentation sum tens1: ", session.run(seg_sum)
    print "segmentation sum tens2: ", session.run(seg_sum_1)

segmentation sum tens1:
[[ 2  8  1  0]
 [ 4  3  5  3]
 [12  2  8  0]]

segmentation sum tens2: [3 3 9]

sequence utilities

   sequence utilities include methods such as:
     * [66]argmin function, which returns the index with min value across
       the axes of the input tensor,
     * [67]argmax function, which returns the index with max value across
       the axes of the input tensor,
     * [68]setdiff, which computes the difference between two lists of
       numbers or strings,
     * [69]where function, which will return elements either from two
       passed elements x or y, which depends on the passed condition, or
     * [70]unique function, which will return unique elements in a 1-d
       tensor.

   we demonstrate a few execution examples below:
import numpy as np
import tensorflow as tf

def convert(v, t=tf.float32):
    return tf.convert_to_tensor(v, dtype=t)

x = convert(np.array([
    [2, 2, 1, 3],
    [4, 5, 6, -1],
    [0, 1, 1, -2],
    [6, 2, 3, 0]
]))

y = convert(np.array([1, 2, 5, 3, 7]))
z = convert(np.array([1, 0, 4, 6, 2]))

arg_min = tf.argmin(x, 1)
arg_max = tf.argmax(x, 1)
unique = tf.unique(y)
diff = tf.setdiff1d(y, z)

with tf.session() as session:
    print "argmin = ", session.run(arg_min)
    print "argmax = ", session.run(arg_max)

    print "unique_values = ", session.run(unique)[0]
    print "unique_idx = ", session.run(unique)[1]

    print "setdiff_values = ", session.run(diff)[0]
    print "setdiff_idx = ", session.run(diff)[1]

    print session.run(diff)[1]

   output:
argmin = [2 3 3 3]
argmax =  [3 2 1 0]
unique_values =  [ 1.  2.  5.  3.  7.]
unique_idx =  [0 1 2 3 4]
setdiff_values =  [ 5.  3.  7.]
setdiff_idx =  [2 3 4]

machine learning with tensorflow

   in this section, we will present a machine learning use case with
   tensorflow. the first example will be an algorithm for classifying data
   with the [71]knn approach, and the second will use the [72]linear
   regression algorithm.

knn

   the first algorithm is k-nearest neighbors (knn). it   s a supervised
   learning algorithm that uses distance metrics, for example euclidean
   distance, to classify data against training. it is one of the simplest
   algorithms, but still really powerful for classifying data. pros of
   this algorithm:
     * gives high accuracy when the training model is big enough, and
     * isn   t usually sensitive to outliers, and we don   t need to have any
       assumptions about data.

   cons of this algorithm:
     * computationally expensive, and
     * requires a lot of memory where new classified data need to be added
       to all initial training instances.

   knn overview

   the distance which we will use in this code sample is euclidean, which
   defines the distance between two points like this:

   mathematical equation

   in this formula, n is the number of dimensions of the space, x is the
   vector of the training data, and y is a new data point that we want to
   classify.
import os
import numpy as np
import tensorflow as tf

ccf_train_data = "train_dataset.csv"
ccf_test_data = "test_dataset.csv"

dataset_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../datase
ts'))

ccf_train_filepath = os.path.join(dataset_dir, ccf_train_data)
ccf_test_filepath = os.path.join(dataset_dir, ccf_test_data)

def load_data(filepath):
    from numpy import genfromtxt

    csv_data = genfromtxt(filepath, delimiter=",", skip_header=1)
    data = []
    labels = []

    for d in csv_data:
        data.append(d[:-1])
        labels.append(d[-1])

    return np.array(data), np.array(labels)

train_dataset, train_labels = load_data(ccf_train_filepath)
test_dataset, test_labels = load_data(ccf_test_filepath)

train_pl = tf.placeholder("float", [none, 28])
test_pl = tf.placeholder("float", [28])

knn_prediction = tf.reduce_sum(tf.abs(tf.add(train_pl, tf.negative(test_pl))), a
xis=1)

pred = tf.argmin(knn_prediction, 0)

with tf.session() as tf_session:
    missed = 0

    for i in xrange(len(test_dataset)):
        knn_index = tf_session.run(pred, feed_dict={train_pl: train_dataset, tes
t_pl: test_dataset[i]})

        print "predicted class {} -- true class {}".format(train_labels[knn_inde
x], test_labels[i])

        if train_labels[knn_index] != test_labels[i]:
            missed += 1

    tf.summary.filewriter("../samples/article/logs", tf_session.graph)

print "missed: {} -- total: {}".format(missed, len(test_dataset))

   the dataset which we used in above example is one which can be found on
   the [73]kaggle datasets section. we used the [74]one which contains
   transactions made by credit cards of european cardholders. we are using
   the data without any cleaning or filtering and as per the description
   in kaggle for this dataset, it is highly unbalanced. the dataset
   contains 31 variables: time, v1,    , v28, amount, and class. in this
   code sample we use only v1,    , v28 and class. class labels transactions
   which are fraudulent with 1 and those which aren   t with 0.

   the code sample contains mostly the things which we described in
   previous sections with exception where we introduced the function for
   loading a dataset. the function load_data(filepath) will take a csv
   file as an argument and will return a tuple with data and labels
   defined in csv.

   just below that function, we have defined placeholders for the test and
   trained data. trained data are used in the prediction model to resolve
   the labels for the input data that need to be classified. in our case,
   knn use euclidian distance to get the nearest label.

   the error rate can be calculated by simple division with the number
   when a classifier missed by the total number of examples which in our
   case for this dataset is 0.2 (i.e., the classifier gives us the wrong
   data label for 20% of test data).

id75

   the id75 algorithm looks for a linear relationship between
   two variables. if we label the dependent variable as y, and the
   independent variable as x, then we   re trying to estimate the parameters
   of the function y = wx + b.

   id75 is a widely used algorithm in the field of applied
   sciences. this algorithm allows adding in implementation two important
   concepts of machine learning: [75]cost function and the [76]gradient
   descent method for finding the minimum of the function.

   a machine learning algorithm that is implemented using this method must
   predict values of y as a function of x where a id75
   algorithm will determinate values w and b, which are actually unknowns
   and which are determined across training process. a cost function is
   chosen, and usually the mean square error is used where the gradient
   descent is the optimization algorithm used to find a local minimum of
   the cost function.

   the id119 method is only a local function minimum, but it
   can be used in the search for a global minimum by randomly choosing a
   new start point once it has found a local minimum and repeating this
   process many times. if the number of minima of the function is limited
   and there are very high number of attempts, then there is a good chance
   that at some point the global minimum is spotted. some more details
   about this technique we will leave for the [77]article which we
   mentioned in the introduction section.
import tensorflow as tf
import numpy as np

test_data_size = 2000
iterations = 10000
learn_rate = 0.005

def generate_test_values():
    train_x = []
    train_y = []

    for _ in xrange(test_data_size):
        x1 = np.random.rand()
        x2 = np.random.rand()
        x3 = np.random.rand()
        y_f = 2 * x1 + 3 * x2 + 7 * x3 + 4
        train_x.append([x1, x2, x3])
        train_y.append(y_f)

    return np.array(train_x), np.transpose([train_y])

x = tf.placeholder(tf.float32, [none, 3], name="x")
w = tf.variable(tf.zeros([3, 1]), name="w")
b = tf.variable(tf.zeros([1]), name="b")
y = tf.placeholder(tf.float32, [none, 1])

model = tf.add(tf.matmul(x, w), b)

cost = tf.reduce_mean(tf.square(y - model))
train = tf.train.gradientdescentoptimizer(learn_rate).minimize(cost)

train_dataset, train_values = generate_test_values()

init = tf.global_variables_initializer()

with tf.session() as session:
    session.run(init)

    for _ in xrange(iterations):

        session.run(train, feed_dict={
            x: train_dataset,
            y: train_values
        })

    print "cost = {}".format(session.run(cost, feed_dict={
        x: train_dataset,
        y: train_values
    }))

    print "w = {}".format(session.run(w))
    print "b = {}".format(session.run(b))

   output:
cost = 3.1083032809e-05
w = [[ 1.99049103]
 [ 2.9887135 ]
 [ 6.98754263]]
b = [ 4.01742554]

   in the above example, we have two new variables, which we called cost
   and train. with those two variables, we defined an optimizer which we
   want to use in our training model and the function which we want to
   minimize.

   at the end, the output parameters of w and b should be identical as
   those defined in the generate_test_values function. in line 17, we
   actually defined a function which we used to generate the linear data
   points to train where w1=2, w2=3, w3=7 and b=4. id75 from
   the above example is multivariate where more than one independent
   variable are used.

conclusion

   as you can see from this tensorflow tutorial, tensorflow is a powerful
   framework that makes working with mathematical expressions and
   multi-dimensional arrays a breeze   something fundamentally necessary in
   machine learning. it also abstracts away the complexities of executing
   the data graphs and scaling.

   over time, tensorflow has grown in popularity and is now being used by
   developers for solving problems using deep learning methods for image
   recognition, video detection, text processing like id31,
   etc. like any other library, you may need some time to get used to the
   concepts that tensorflow is built on. and, once you do, with the help
   of documentation and community support, representing problems as data
   graphs and solving them with tensorflow can make machine learning at
   scale a less tedious process.

understanding the basics

how are tensorflow constants created?

   in tensorflow, constants are created using the constant function which
   takes a few parameters: value, dtype (data type), shape, name and
   (verify_shape) shape verification.

what is a tensorflow session?

   a session encapsulates the control and state of the tensorflow runtime.
   a session without parameters will use the default graph created in the
   current session, otherwise the session class accepts a graph parameter,
   which is used in that session to be executed.

what is tensorboard?

   tensorboard is a visualization tool for analyzing data flow graphs.
   this can be useful for gaining better understanding of machine learning
   models.

about the author

   [78]dino causevic
   view full profile   
   [79]hire the author
   [80]dino causevic, bosnia and herzegovina
   member since september 28, 2014
   [81]elasticsearch[82]mongodb[83]mysql[84]twitter api[85]facebook
   api[86]web app development[87]spring integration[88]spring
   mvc[89]spring social[90]python[91]aws s3[92]flask[93]+   more
   dino has over five years of experience as a software developer. for the
   past two years, he has worked in java and related technologies, mostly
   in implementing big data solutions using nosql technologies and in
   implementing rest services. he also has experience with .net and pci
   dss security standards and with rest solutions using python combined
   with established frameworks. [94][click to continue...]
   [95]hiring? meet the top 10 freelance machine learning engineers for
   hire in april 2019

comments

   training4developers
   this is a cool article, but some comments in the code would help. if
   this is for newbies to tensorflow, it would be wrong to assume mastery
   of the intro material on tensorflow before reading the machine learning
   part of the post.
   royi
   great tutorial. really appreciate your efforts. it would be great to
   have something similar for pytorch. thank you.
   izzy lazerson
   great post - thanks!
   andrew franklin
   as someone not interested in machine learning but just doing linear
   algebra is there any reason why i would want to use tensorflow over
   numpy or scipy?
   gajendra s
   great tutorial, sir my question is if i do not want to use cpu/gpu for
   processing instead i want to use fpga to execute my functions that is
   interfaced with pci then which part/apis i need to look/customized.
   would you please guide me.
   abdennacer ayeb
   know i realized i understood tf. thank you
   please enable javascript to view the [96]comments powered by
   disqus.[97]comments powered by disqus
   subscribe
   free email updates
   get the latest content first.
   ____________________
   get exclusive updates
   no spam. just great articles & insights.
   free email updates
   get the latest content first.
   thank you for subscribing!
   check your inbox to confirm subscription. you'll start receiving posts
   after you confirm.
     * 0shares
     * [twitter_83c6d4.png]
     * [facebook_dc66c9.png]

   trending articles

   [98][side_list_cover-0401-howtomathematicallyconfigureyourapplicationfo
   rauto-scaling-waldek_newsletter-003822358e45fdefb90d9dc0354cc33f.png]
   do the math: scaling microservices applications with orchestrators1 day
   ago[99]
   [side_list_cover-0104-wodpressroots-luke_social-14cea8a367cb03c4a39a386
   0f9a11823.png] modern wordpress development workflow with the roots
   stack3 days ago[100]
   [side_list_cover-0327-introtobitcoinlightning-waldek_banner-e63121da140
   c50e3b77db0e134d4f9f8.png] scale with speed: the bitcoin lightning
   network explained4 days ago[101]
   [side_list_cover-0326-wordpressmaintenancetips-luke_newsletter-24d38612
   919504bbefbe7d83c73c42c9.png] 10 tips to make wordpress maintenance
   smooth10 days ago[102]
   [side_list_cover-0319-lifecriticalsystems-luke_newsletter-8c3a1ee7ec272
   731e84d07c1e1cb92e8.png] innovation with life-critical systems17 days
   ago[103]
   [side_list_cover-0318-phytonparameterization_dan_newsletter-b9dfeef97a7
   6cce4ba5d826d3fff10ce.png] ensuring clean code: a look at python,
   parameterized17 days ago[104]
   [side_list_cover-0226-wordpressapiguide-luke_newsletter-b6331b0a068dc12
   ff60a355b94c02302.png] five battle-tested techniques your wordpress api
   developer isn't using24 days ago[105]
   [side_list_cover-0306-googlecloudcd_dan_newsletter-f65e8af970b57182c527
   6eda3446426b.png] a better approach to google cloud continuous
   deployment24 days ago
   relevant technologies
     * [106]machine learning
     * [107]python

   about the author
   dino causevic
   [108]dino causevic
   c++ developer
   dino has over five years of experience as a software developer. for the
   past two years, he has worked in java and related technologies, mostly
   in implementing big data solutions using nosql technologies and in
   implementing rest services. he also has experience with .net and pci
   dss security standards and with rest solutions using python combined
   with established frameworks.
   [109]hire the author

   toptal connects the [110]top 3% of freelance talent all over the world.

toptal developers

     * [111]android developers
     * [112]angularjs developers
     * [113]back-end developers
     * [114]c++ developers
     * [115]data scientists
     * [116]devops engineers
     * [117]ember.js developers
     * [118]freelance developers
     * [119]front-end developers
     * [120]full-stack developers
     * [121]html5 developers
     * [122]ios developers
     * [123]java developers
     * [124]javascript developers
     * [125]machine learning engineers
     * [126]magento developers
     * [127]mobile app developers
     * [128].net developers
     * [129]node.js developers
     * [130]php developers
     * [131]python developers
     * [132]react.js developers
     * [133]ruby developers
     * [134]ruby on rails developers
     * [135]salesforce developers
     * [136]scala developers
     * [137]software developers
     * [138]unity or unity3d developers
     * [139]virtual reality developers
     * [140]web developers
     * [141]wordpress developers

   see more freelance developers
   [142]learn how enterprises benefit from toptal experts.

join the toptal community.

   [143]hire a developer
   or
   [144]apply as a developer

highest in-demand talent

     * [145]ios developers
     * [146]front-end developers
     * [147]ux designers
     * [148]ui designers
     * [149]financial modeling consultants
     * [150]interim cfos
     * [151]digital project managers

about

     * [152]top 3%
     * [153]clients
     * [154]freelance developers
     * [155]freelance designers
     * [156]freelance finance experts
     * [157]freelance project managers
     * [158]freelance product managers
     * [159]about us

contact

     * [160]contact us
     * [161]press center
     * [162]careers
     * [163]faq

social

     * [164]facebook
     * [165]twitter
     * [166]instagram
     * [167]linkedin

   [168]toptal

   hire the top 3% of freelance talent
     *    copyright 2010 - 2019 toptal, llc
     * [169]privacy policy
     * [170]website terms

   [a6qhozmiylpv2vkc8wshaemc3saomhcrxafccfat?ga_tid=ua-21104039-1&amp;amp;
   ga_cdi=14] [tr?id=463369723801939&amp;ev=viewcontent&amp;noscript=1]

   iframe:
   [171]https://www.attributiontracker.com/tracker/account_visit?tt_visit=
   1

   [172]home     [173]blog     [174]getting started with tensorflow: a machine
   learning tutorial

references

   visible links
   1. https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial
   2. https://www.toptal.com/machine-learning
   3. https://www.toptal.com/machine-learning
   4. https://www.toptal.com/users/login
   5. https://www.toptal.com/top-3-percent
   6. https://www.toptal.com/why
   7. https://www.toptal.com/clients
   8. https://www.toptal.com/enterprise
   9. https://www.toptal.com/community
  10. https://www.toptal.com/developers/blog
  11. https://www.toptal.com/about
  12. https://www.toptal.com/machine-learning
  13. https://www.toptal.com/developers/join
  14. https://www.toptal.com/users/login
  15. https://www.toptal.com/contact
  16. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  17. https://www.toptal.com/machine-learning
  18. https://www.toptal.com/machine-learning
  19. https://www.toptal.com/users/login
  20. https://www.toptal.com/developers/blog
  21. https://www.toptal.com/resume/dino-causevic
  22. https://www.toptal.com/
  23. https://www.toptal.com/developers/blog/tags/knn
  24. https://www.toptal.com/developers/blog/tags/linearregression
  25. https://www.toptal.com/developers/blog/tags/tensorflow
  26. https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
  27. https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-id88s-to-deep-networks
  28. https://conda.io/docs/user-guide/install/index.html
  29. http://www.numpy.org/
  30. https://www.tensorflow.org/versions/master/api_docs/python/tf/graph
  31. https://www.tensorflow.org/versions/master/api_docs/python/tf/graph
  32. https://www.tensorflow.org/api_guides/python/summary
  33. https://www.tensorflow.org/api_docs/python/tf/event
  34. https://www.tensorflow.org/versions/master/api_docs/python/tf/convert_to_tensor
  35. https://www.tensorflow.org/api_docs/python/tf/add
  36. https://www.tensorflow.org/api_docs/python/tf/subtract
  37. https://www.tensorflow.org/api_docs/python/tf/multiply
  38. https://www.tensorflow.org/api_docs/python/tf/div
  39. https://www.tensorflow.org/versions/master/api_docs/python/tf/floormod
  40. https://www.tensorflow.org/versions/master/api_docs/python/tf/abs
  41. https://www.tensorflow.org/versions/master/api_docs/python/tf/negative
  42. https://www.tensorflow.org/versions/master/api_docs/python/tf/sign
  43. https://www.tensorflow.org/api_docs/python/tf/square
  44. https://www.tensorflow.org/api_docs/python/tf/round
  45. https://www.tensorflow.org/api_docs/python/tf/sqrt
  46. https://www.tensorflow.org/api_docs/python/tf/pow
  47. https://www.tensorflow.org/api_docs/python/tf/exp
  48. https://www.tensorflow.org/api_docs/python/tf/log
  49. https://www.tensorflow.org/api_docs/python/tf/maximum
  50. https://www.tensorflow.org/api_docs/python/tf/minimum
  51. https://www.tensorflow.org/api_docs/python/tf/cos
  52. https://www.tensorflow.org/api_docs/python/tf/sin
  53. https://www.tensorflow.org/versions/master/api_docs/python/tf/matmul
  54. https://www.tensorflow.org/versions/master/api_docs/python/tf/transpose
  55. https://www.tensorflow.org/versions/master/api_docs/python/tf/matrix_inverse
  56. https://www.tensorflow.org/versions/master/api_docs/python/tf/matrix_determinant
  57. https://www.tensorflow.org/versions/master/api_docs/python/tf/matrix_solve
  58. https://www.tensorflow.org/versions/master/api_guides/python/math_ops#matrix_math_functions
  59. https://www.tensorflow.org/versions/master/api_docs/python/tf/transpose
  60. https://www.tensorflow.org/versions/master/api_docs/python/tf/reduce_sum
  61. https://www.tensorflow.org/api_guides/python/math_ops
  62. https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_prod
  63. https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_mean
  64. https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_max
  65. https://www.tensorflow.org/versions/master/api_docs/python/tf/segment_min
  66. https://www.tensorflow.org/versions/master/api_docs/python/tf/argmin
  67. https://www.tensorflow.org/versions/master/api_docs/python/tf/argmax
  68. https://www.tensorflow.org/versions/master/api_docs/python/tf/setdiff1d
  69. https://www.tensorflow.org/api_docs/python/tf/where
  70. https://www.tensorflow.org/versions/master/api_docs/python/tf/unique
  71. https://en.wikipedia.org/wiki/k-nearest_neighbors_algorithm
  72. https://en.wikipedia.org/wiki/linear_regression
  73. https://www.kaggle.com/datasets
  74. https://www.researchgate.net/post/where_can_i_find_credit_card_fraud_detection_data_set
  75. https://en.wikipedia.org/wiki/loss_function
  76. https://en.wikipedia.org/wiki/gradient_descent
  77. https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
  78. https://www.toptal.com/resume/dino-causevic
  79. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
  80. https://www.toptal.com/resume/dino-causevic
  81. https://www.toptal.com/elasticsearch
  82. https://www.toptal.com/mongodb
  83. https://www.toptal.com/mysql
  84. https://www.toptal.com/twitter
  85. https://www.toptal.com/facebook-api
  86. https://www.toptal.com/web
  87. https://www.toptal.com/spring
  88. https://www.toptal.com/spring
  89. https://www.toptal.com/spring
  90. https://www.toptal.com/python
  91. https://www.toptal.com/amazon-s3
  92. https://www.toptal.com/flask
  93. https://www.toptal.com/resume/dino-causevic
  94. https://www.toptal.com/resume/dino-causevic
  95. https://www.toptal.com/machine-learning
  96. https://disqus.com/?ref_noscript
  97. https://disqus.com/
  98. https://www.toptal.com/devops/scaling-microservices-applications
  99. https://www.toptal.com/wordpress/wordpress-roots-stack
 100. https://www.toptal.com/bitcoin/intro-to-bitcoin-lightning-network
 101. https://www.toptal.com/wordpress/wordpress-maintenance-tips
 102. https://www.toptal.com/software/life-critical-systems
 103. https://www.toptal.com/python/python-parameterized-design-patterns
 104. https://www.toptal.com/wordpress/wordpress-api-integration
 105. https://www.toptal.com/devops/better-google-cloud-continuous-deployment
 106. https://www.toptal.com/machine-learning
 107. https://www.toptal.com/python
 108. https://www.toptal.com/resume/dino-causevic
 109. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
 110. https://www.toptal.com/top-3-percent
 111. https://www.toptal.com/android
 112. https://www.toptal.com/angular-js
 113. https://www.toptal.com/back-end
 114. https://www.toptal.com/c-plus-plus
 115. https://www.toptal.com/data-science
 116. https://www.toptal.com/devops
 117. https://www.toptal.com/emberjs
 118. https://www.toptal.com/freelance
 119. https://www.toptal.com/front-end
 120. https://www.toptal.com/full-stack
 121. https://www.toptal.com/html5
 122. https://www.toptal.com/ios
 123. https://www.toptal.com/java
 124. https://www.toptal.com/javascript
 125. https://www.toptal.com/machine-learning
 126. https://www.toptal.com/magento
 127. https://www.toptal.com/app
 128. https://www.toptal.com/dot-net
 129. https://www.toptal.com/nodejs
 130. https://www.toptal.com/php
 131. https://www.toptal.com/python
 132. https://www.toptal.com/react
 133. https://www.toptal.com/ruby
 134. https://www.toptal.com/ruby-on-rails
 135. https://www.toptal.com/salesforce
 136. https://www.toptal.com/scala
 137. https://www.toptal.com/software
 138. https://www.toptal.com/unity-unity3d
 139. https://www.toptal.com/virtual-reality
 140. https://www.toptal.com/web
 141. https://www.toptal.com/wordpress
 142. https://www.toptal.com/enterprise
 143. https://www.toptal.com/hire?interested_in=developers&skill=machine-learning
 144. https://www.toptal.com/developers/join
 145. https://www.toptal.com/ios
 146. https://www.toptal.com/front-end
 147. https://www.toptal.com/designers/ux
 148. https://www.toptal.com/designers/ui
 149. https://www.toptal.com/finance/financial-modeling
 150. https://www.toptal.com/finance/interim-cfos
 151. https://www.toptal.com/project-managers/digital
 152. https://www.toptal.com/top-3-percent
 153. https://www.toptal.com/clients
 154. https://www.toptal.com/developers
 155. https://www.toptal.com/designers
 156. https://www.toptal.com/finance
 157. https://www.toptal.com/project-managers
 158. https://www.toptal.com/product-managers
 159. https://www.toptal.com/about
 160. https://www.toptal.com/contact
 161. https://www.toptal.com/press-center
 162. https://www.toptal.com/careers
 163. https://www.toptal.com/faq
 164. https://www.facebook.com/toptal
 165. https://twitter.com/toptal
 166. https://www.instagram.com/toptal
 167. https://www.linkedin.com/company/toptal
 168. https://www.toptal.com/
 169. https://www.toptal.com/privacy
 170. https://www.toptal.com/tos
 171. https://www.attributiontracker.com/tracker/account_visit?tt_visit=1
 172. https://www.toptal.com/
 173. https://www.toptal.com/developers/blog
 174. https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial

   hidden links:
 176. https://www.toptal.com/developers
 177. https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial
 178. https://www.toptal.com/developers
 179. https://www.facebook.com/toptal
 180. https://twitter.com/toptal
 181. https://www.linkedin.com/company/toptal
 182. https://www.toptal.com/developers
 183. https://www.toptal.com/developers
