   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

parfit         quick and powerful hyper-parameter optimization with visualizations

   [11]go to the profile of jason carpenter
   [12]jason carpenter (button) blockedunblock (button) followfollowing
   nov 26, 2017

   machine learning can be complex         there are several steps involved
   which can limit the speed of the iterative modeling process, even for
   experienced practitioners. as a student in [13]jeremy howard   s
   introductory machine learning course at the university of san
   francisco, i have learned that the modeling process can be simplified
   and sped up. this is the goal of the [14]fast.ai library, which is
   constantly being improved upon. the aim of this post is to facilitate
   quick and effective hyper-parameter optimization of machine learning
   models, a key part of the iterative modeling process.
     __________________________________________________________________

introduction to hyper-parameters

   first, let   s take a step back and ask    what are hyper-parameters, and
   why do i care?    if you are familiar with these concepts, then please
   skip ahead to the next section.

   hyper-parameters in the context of machine learning are the parameters
   of the machine learning model which make an impact on how the model
   fits to the training data, and therefore the predictions on the test
   data. we care about these because, for example, incorrectly specified
   hyper-parameters can lead to the model overfitting on the training data
   and predicting poorly on the test data when the model is put into
   production. on the other hand, optimally specified hyper-parameters can
   greatly increase the performance of the model.

   in general, the [15]sklearn package in python has reasonably intuitive
   and somewhat effective default hyper-parameters for most models, but we
   can do better.
     __________________________________________________________________

scoring on validation sets

   it is important to understand what it means to give a score to a
   model   s ability to make predictions on a validation set. a variety of
   [16]metrics can be applied, dependent on the type of problem
   ([17]classification/[18]regression). while selecting a metric is
   important, dependent on the use case of the model, the most crucial
   decision is how to create an effective validation set which has the
   properties of the test/production set.

   rachel thomas from fast.ai wrote an excellent blog post on [19]how (and
   why) to create a good validation set. before continuing, i highly
   recommend reading this post.

   here are some key definitions extracted directly from her article.
     * the training set is used to train a given model
     * the validation set is used to choose between models (for instance,
       does a id79 or a neural network better for your problem?
       do you want a id79 with 40 trees or 50 trees?)
     * the test set tells you how you   ve done. if you   ve tried out a lot
       of different models, you may get one that does well on your
       validation set just by chance, and having a test set helps make
       sure that is not the case.

   the important take-home message from her post is:

     for certain data sets (e.g. time series), a random subset of the
     training data is not a good validation set. that is, it will not be
     representative of the test set for various reasons (e.g. a time
     component). we need to create an effective validation set for
     scoring our models.

   i am going to continue assuming that you have read rachel   s article.
   that is, i will assume you know how to create an effective validation
   set, and have already done so.
     __________________________________________________________________

how do we perform hyper-parameter optimization?

   let   s work with an example here: [20]randomforestclassifier
   (implemented in sklearn). for reasons taught in jeremy   s introductory
   machine learning course (soon to be available as a mooc), the two key
   parameters we are interested in are max_features and min_samples_leaf.
   that is, we want to know what combination of max_features and
   min_samples_leaf give us the best score on our validation set.

   to do this, we typically perform an exhaustive grid search over all
   combinations of max_features and min_samples_leaf. the most commonly
   used package for this technique is [21]gridsearchcv (from sklearn).
   this package is highly effective if you are using a completely
   randomized training set that does not contain a time component as, by
   default, it internally scores using [22]cross-validation. this is an
   optimal method for estimating hyper-parameters because it takes the
   average score over all folds in the cross-validation routine and gives
   the best set of hyper-parameters based on this average.

   so    what   s the problem?

   cross-validation using gridsearchcv optimizes on the training data! as
   mentioned in rachel   s post, this is dangerous and not applicable to
   most real-world modeling problems (especially time series data).
   instead, we want to optimize our hyper-parameters on the validation
   set.

   note that in gridsearchcv it is possible to specify a
   [23]predefinedsplit object for separating the training and validation
   sets within the gridsearchcv, but it is better practice to split the
   training and validation sets beforehand and enter the validation set as
   the scoring set to avoid confusion and bleeding over between your
   training and validation sets.

   now that we understand the problem, what   s our solution?
     __________________________________________________________________

the solution is [24]parfit!

   introducing: [25]parfit, a new package for hyper-parameter
   optimization, which (using parallel processing) allows the user to
   perform an exhaustive grid search on a model. this package has the
   following advantages:
     * validate: flexibly specify the validation set to score on
     * score: flexibly choose the scoring metric
     * visualize: optionally plot the scores over the grid of
       hyper-parameters entered
     * optimize: automatically return the best model, associated
       hyper-parameters, and score of that model
     * easy to use: do all of this with only one function call

   sounds great, how do i use it?

   to install the package, enter the following command in your terminal:
pip install parfit

   then, in your jupyter notebook (or python program) run the following
   line to import the module:
import parfit.parfit as pf

   now, you are ready to rock and roll! in just a couple lines of code,
   you can find the best set of hyper-parameters for your validation set.
   here is an example of how to perform an exhaustive search over a
   parameter grid for randomforestclassifier. notice below that i have
   only specified the randomforestclassifier class, not instantiated the
   object with parentheses ().
import numpy as np
from sklearn.model_selection import parametergrid
from sklearn.ensemble import randomforestclassifier
from sklearn.metrics import roc_auc_score
paramgrid = parametergrid({
    'min_samples_leaf': [1,3,5,10,15,25,50,100,125,150,175,200],
    'max_features': ['sqrt', 'log2', 0.4, 0.5, 0.6, 0.7],
    'n_estimators': [60],
    'n_jobs': [-1],
    'random_state': [42]
})
best_model, best_score, all_models, all_scores = pf.bestfit(randomforestclassifi
er, paramgrid,
     x_train, y_train, x_val, y_val,
     metric=roc_auc_score, bestscore='max', scorelabel='auc')
print(best_model)

   [1*hzfsg4zezfgycqsle4ssqw.png]
   [1*sbhces0jcgkpcdt8tuzjpa.png]

   from inspecting this grid, we can see that on these data, the highest
   scoring models used max_features = 0.4 or 0.6 and min_samples_leaf =
   100. not only that, we   ve automatically returned the model, with the
   single set of parameters which maximized the score on the validation
   set. additionally, we   ve returned the score of that best model, so we
   know how we   re doing.

   this package has support for varying any number of parameters, but only
   plotting support for a grid varying over 1   3 parameters (due to
   visualization limitations).

   the [26]parfit github page has documentation for each function in the
   package and notes on how to use the functions. there is also a
   parfit_ex.ipynb that gives examples of how to use for varying 1, 2, and
   3 parameters on a generated data set.

   my colleague and fellow master   s in analytics candidate at the
   university of san francisco, vinay patlolla, wrote an excellent post in
   which he utilized parfit to show [27]how to make sgd classifier perform
   as well as id28. in that post, there is also brief
   section on how to use parfit.
     __________________________________________________________________

what is the full process?

    1. create an effective validation set
    2. choose your model(s)
    3. create a parameter grid over which to evaluate your model on the
       validation set
    4. use parfit to visualize the scores over the grid and select the
       best model
    5. re-train model on full training set, using best parameters from
       parfit
    6. apply re-trained model to test set
     __________________________________________________________________

   current limitations of parfit:
     * the package is designed to be used with sklearn machine learning
       models
     * need to pass a parametergrid object, as shown in above example

   have suggestions to improve the package?

   let me know or [28]contribute to the package!

   linkedin: [29]https://www.linkedin.com/in/jasonmcarpenter/

     * [30]machine learning
     * [31]sklearn
     * [32]parameterization
     * [33]visualization
     * [34]parallel processing

   (button)
   (button)
   (button) 586 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of jason carpenter

[36]jason carpenter

   machine learning engineer @ manifold.ai

     (button) follow
   [37]ml review

[38]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 586
     * (button)
     *
     *

   [39]ml review
   never miss a story from ml review, when you sign up for medium.
   [40]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/77253e7e175e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_mtjt42qhusrs---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@jmcarpenter2?source=post_header_lockup
  12. https://medium.com/@jmcarpenter2
  13. https://medium.com/@jeremyphoward
  14. https://github.com/fastai/fastai
  15. http://scikit-learn.org/stable/
  16. http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics
  17. https://en.wikipedia.org/wiki/statistical_classification
  18. https://en.wikipedia.org/wiki/regression_analysis
  19. http://www.fast.ai/2017/11/13/validation-sets/
  20. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.randomforestclassifier.html
  21. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.gridsearchcv.html#sklearn.model_selection.gridsearchcv
  22. https://en.wikipedia.org/wiki/cross-validation_(statistics)
  23. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.predefinedsplit.html
  24. https://github.com/jmcarpenter2/parfit
  25. https://github.com/jmcarpenter2/parfit
  26. https://github.com/jmcarpenter2/parfit
  27. https://medium.com/@vinnsvinay/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4
  28. https://github.com/jmcarpenter2/parfit
  29. https://www.linkedin.com/in/jasonmcarpenter/
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/sklearn?source=post
  32. https://medium.com/tag/parameterization?source=post
  33. https://medium.com/tag/visualization?source=post
  34. https://medium.com/tag/parallel-processing?source=post
  35. https://medium.com/@jmcarpenter2?source=footer_card
  36. https://medium.com/@jmcarpenter2
  37. https://medium.com/mlreview?source=footer_card
  38. https://medium.com/mlreview?source=footer_card
  39. https://medium.com/mlreview
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/77253e7e175e/share/twitter
  43. https://medium.com/p/77253e7e175e/share/facebook
  44. https://medium.com/p/77253e7e175e/share/twitter
  45. https://medium.com/p/77253e7e175e/share/facebook
