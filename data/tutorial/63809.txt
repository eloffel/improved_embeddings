   #[1]alternate

[2]steven miller

   engineering manager at [3]segment
   [4]twitter [5]github [6]rss

   [7]follow @stevenmiller888

   [8]home

mind: how to build a neural network (part one)

   monday, 10 august 2015

   [9]id158s are statistical learning models, inspired
   by biological neural networks (central nervous systems, such as the
   brain), that are used in [10]machine learning. these networks are
   represented as systems of interconnected    neurons   , which send messages
   to each other. the connections within the network can be systematically
   adjusted based on inputs and outputs, making them ideal for supervised
   learning.

   neural networks can be intimidating, especially for people with little
   experience in machine learning and cognitive science! however, through
   code, this tutorial will explain how neural networks operate. by the
   end, you will know how to build your own flexible, learning network,
   similar to [11]mind.

   the only prerequisites are having a basic understanding of javascript,
   high-school calculus, and simple matrix operations. other than that,
   you don   t need to know anything. have fun!

understanding the mind

   a neural network is a collection of    neurons    with    synapses   
   connecting them. the collection is organized into three main parts: the
   input layer, the hidden layer, and the output layer. note that you can
   have n hidden layers, with the term    deep    learning implying multiple
   hidden layers.

   screenshot taken from [12]this great introductory video, which trains a
   neural network to predict a test score based on hours spent studying
   and sleeping the night before.

   hidden layers are necessary when the neural network has to make sense
   of something really complicated, contextual, or non obvious, like image
   recognition. the term    deep    learning came from having many hidden
   layers. these layers are known as    hidden   , since they are not visible
   as a network output. read more about hidden layers [13]here and
   [14]here.

   the circles represent neurons and lines represent synapses. synapses
   take the input and multiply it by a    weight    (the    strength    of the
   input in determining the output). neurons add the outputs from all
   synapses and apply an activation function.

   training a neural network basically means calibrating all of the
      weights    by repeating two key steps, forward propagation and back
   propagation.

   since neural networks are great for regression, the best input data are
   numbers (as opposed to discrete values, like colors or movie genres,
   whose data is better for statistical classification models). the output
   data will be a number within a range like 0 and 1 (this ultimately
   depends on the activation function   more on this below).

   in forward propagation, we apply a set of weights to the input data and
   calculate an output. for the first forward propagation, the set of
   weights is selected randomly.

   in back propagation, we measure the margin of error of the output and
   adjust the weights accordingly to decrease the error.

   neural networks repeat both forward and back propagation until the
   weights are calibrated to accurately predict an output.

   next, we   ll walk through a simple example of training a neural network
   to function as an [15]   exclusive or    (   xor   ) operation to illustrate
   each step in the training process.

forward propagation

   note that all calculations will show figures truncated to the
   thousandths place.

   the xor function can be represented by the mapping of the below inputs
   and outputs, which we   ll use as training data. it should provide a
   correct output given any input acceptable by the xor function.
input | output
--------------
0, 0  | 0
0, 1  | 1
1, 0  | 1
1, 1  | 0

   let   s use the last row from the above table, (1, 1) => 0, to
   demonstrate forward propagation:

   note that we use a single hidden layer with only three neurons for this
   example.

   we now assign weights to all of the synapses. note that these weights
   are selected randomly (based on gaussian distribution) since it is the
   first time we   re forward propagating. the initial weights will be
   between 0 and 1, but note that the final weights don   t need to be.

   we sum the product of the inputs with their corresponding set of
   weights to arrive at the first values for the hidden layer. you can
   think of the weights as measures of influence the input nodes have on
   the output.
1 * 0.8 + 1 * 0.2 = 1
1 * 0.4 + 1 * 0.9 = 1.3
1 * 0.3 + 1 * 0.5 = 0.8

   we put these sums smaller in the circle, because they   re not the final
   value:

   to get the final value, we apply the [16]activation function to the
   hidden layer sums. the purpose of the activation function is to
   transform the input signal into an output signal and are necessary for
   neural networks to model complex non-linear patterns that simpler
   models might miss.

   there are many types of id180   linear, sigmoid,
   hyperbolic tangent, even step-wise. to be honest, i don   t know why one
   function is better than another.

   table taken from [17]this paper.

   for our example, let   s use the [18]sigmoid function for activation. the
   sigmoid function looks like this, graphically:

   and applying s(x) to the three hidden layer sums, we get:
s(1.0) = 0.73105857863
s(1.3) = 0.78583498304
s(0.8) = 0.68997448112

   we add that to our neural network as hidden layer results:

   then, we sum the product of the hidden layer results with the second
   set of weights (also determined at random the first time around) to
   determine the output sum.
0.73 * 0.3 + 0.79 * 0.5 + 0.69 * 0.9 = 1.235

   ..finally we apply the activation function to get the final output
   result.
s(1.235) = 0.7746924929149283

   this is our full diagram:

   because we used a random set of initial weights, the value of the
   output neuron is off the mark; in this case by +0.77 (since the target
   is 0). if we stopped here, this set of weights would be a great neural
   network for inaccurately representing the xor operation.

   let   s fix that by using back propagation to adjust the weights to
   improve the network!

back propagation

   to improve our model, we first have to quantify just how wrong our
   predictions are. then, we adjust the weights accordingly so that the
   margin of errors are decreased.

   similar to forward propagation, back propagation calculations occur at
   each    layer   . we begin by changing the weights between the hidden layer
   and the output layer.

   calculating the incremental change to these weights happens in two
   steps: 1) we find the margin of error of the output result (what we get
   after applying the activation function) to back out the necessary
   change in the output sum (we call this delta output sum) and 2) we
   extract the change in weights by multiplying delta output sum by the
   hidden layer results.

   the output sum margin of error is the target output result minus the
   calculated output result:

   and doing the math:
target = 0
calculated = 0.77
target - calculated = -0.77

   to calculate the necessary change in the output sum, or delta output
   sum, we take the derivative of the activation function and apply it to
   the output sum. in our example, the activation function is the sigmoid
   function.

   to refresh your memory, the activation function, sigmoid, takes the sum
   and returns the result:

   so the derivative of sigmoid, also known as sigmoid prime, will give us
   the rate of change (or    slope   ) of the activation function at the
   output sum:

   since the output sum margin of error is the difference in the result,
   we can simply multiply that with the rate of change to give us the
   delta output sum:

   conceptually, this means that the change in the output sum is the same
   as the sigmoid prime of the output result. doing the actual math, we
   get:
delta output sum = s'(sum) * (output sum margin of error)
delta output sum = s'(1.235) * (-0.77)
delta output sum = -0.13439890643886018

   here is a graph of the sigmoid function to give you an idea of how we
   are using the derivative to move the input towards the right direction.
   note that this graph is not to scale.

   now that we have the proposed change in the output layer sum (-0.13),
   let   s use this in the derivative of the output sum function to
   determine the new change in weights.

   as a reminder, the mathematical definition of the output sum is the
   product of the hidden layer result and the weights between the hidden
   and output layer:

   the derivative of the output sum is:

   ..which can also be represented as:

   this relationship suggests that a greater change in output sum yields a
   greater change in the weights; input neurons with the biggest
   contribution (higher weight to output neuron) should experience more
   change in the connecting synapse.

   let   s do the math:
hidden result 1 = 0.73105857863
hidden result 2 = 0.78583498304
hidden result 3 = 0.68997448112

delta weights = delta output sum / hidden layer results
delta weights = -0.1344 / [0.73105, 0.78583, 0.69997]
delta weights = [-0.1838, -0.1710, -0.1920]

old w7 = 0.3
old w8 = 0.5
old w9 = 0.9

new w7 = 0.1162
new w8 = 0.329
new w9 = 0.708

   to determine the change in the weights between the input and hidden
   layers, we perform the similar, but notably different, set of
   calculations. note that in the following calculations, we use the
   initial weights instead of the recently adjusted weights from the first
   part of the backward propagation.

   remember that the relationship between the hidden result, the weights
   between the hidden and output layer, and the output sum is:

   instead of deriving for output sum, let   s derive for hidden result as a
   function of output sum to ultimately find out delta hidden sum:

   also, remember that the change in the hidden result can also be defined
   as:

   let   s multiply both sides by sigmoid prime of the hidden sum:

   all of the pieces in the above equation can be calculated, so we can
   determine the delta hidden sum:
delta hidden sum = delta output sum / hidden-to-outer weights * s'(hidden sum)
delta hidden sum = -0.1344 / [0.3, 0.5, 0.9] * s'([1, 1.3, 0.8])
delta hidden sum = [-0.448, -0.2688, -0.1493] * [0.1966, 0.1683, 0.2139]
delta hidden sum = [-0.088, -0.0452, -0.0319]

   once we get the delta hidden sum, we calculate the change in weights
   between the input and hidden layer by dividing it with the input data,
   (1, 1). the input data here is equivalent to the hidden results in the
   earlier back propagation process to determine the change in the
   hidden-to-output weights. here is the derivation of that relationship,
   similar to the one before:

   let   s do the math:
input 1 = 1
input 2 = 1

delta weights = delta hidden sum / input data
delta weights = [-0.088, -0.0452, -0.0319] / [1, 1]
delta weights = [-0.088, -0.0452, -0.0319, -0.088, -0.0452, -0.0319]

old w1 = 0.8
old w2 = 0.4
old w3 = 0.3
old w4 = 0.2
old w5 = 0.9
old w6 = 0.5

new w1 = 0.712
new w2 = 0.3548
new w3 = 0.2681
new w4 = 0.112
new w5 = 0.8548
new w6 = 0.4681

   here are the new weights, right next to the initial random starting
   weights as comparison:
old         new
-----------------
w1: 0.8     w1: 0.712
w2: 0.4     w2: 0.3548
w3: 0.3     w3: 0.2681
w4: 0.2     w4: 0.112
w5: 0.9     w5: 0.8548
w6: 0.5     w6: 0.4681
w7: 0.3     w7: 0.1162
w8: 0.5     w8: 0.329
w9: 0.9     w9: 0.708

   once we arrive at the adjusted weights, we start again with forward
   propagation. when training a neural network, it is common to repeat
   both these processes thousands of times (by default, mind iterates
   10,000 times).

   and doing a quick forward propagation, we can see that the final output
   here is a little closer to the expected output:

   through just one iteration of forward and back propagation, we   ve
   already improved the network!!

   check out [19]this short video for a great explanation of identifying
   global minima in a cost function as a way to determine necessary weight
   changes.

   if you enjoyed learning about how neural networks work, check out
   [20]part two of this post to learn how to build your own neural
   network.

references

   visible links
   1. http://stevenmiller888.github.io/mind-how-to-build-a-neural-network/atom.xml
   2. http://stevenmiller888.github.io/
   3. https://segment.com/
   4. https://twitter.com/stevenmiller888
   5. https://github.com/stevenmiller888
   6. http://stevenmiller888.github.io/atom.xml
   7. https://twitter.com/stevenmiller888
   8. http://stevenmiller888.github.io/
   9. https://en.wikipedia.org/wiki/artificial_neural_network
  10. https://en.wikipedia.org/wiki/list_of_machine_learning_concepts
  11. https://www.github.com/stevenmiller888/mind
  12. https://www.youtube.com/watch?v=bxe2t-v8xrs
  13. http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute
  14. http://www.cs.cmu.edu/~dst/pubs/byte-hiddenlayer-1989.pdf
  15. https://en.wikipedia.org/wiki/exclusive_or
  16. https://en.wikipedia.org/wiki/activation_function
  17. http://www.asprs.org/a/publications/pers/2003journal/november/2003_nov_1225-1234.pdf
  18. https://en.wikipedia.org/wiki/sigmoid_function
  19. https://www.youtube.com/watch?v=glcnxulrtek
  20. http://stevenmiller888.github.io/mind-how-to-build-a-neural-network-part-2

   hidden links:
  22. http://stevenmiller888.github.io/
