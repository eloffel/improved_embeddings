   #[1]rss feed

   [tr?id=1853843318208763&ev=pageview&noscript=1]

     * [2]home
     * [3]blog
     * [4]research
     * [5]cv
     * [6]photography
          + [7]arches
          + [8]berlin
          + [9]lindau
          + [10]munich
          + [11]people taking pictures

     * menu

[12]brian dolhansky

   ml     code     photography

     * [13]home
     * [14]blog
     * [15]research
     * [16]cv
     * photography
          + [17]arches
          + [18]berlin
          + [19]lindau
          + [20]munich
          + [21]people taking pictures

[22]id158s: id75 (part 1)

   [23]july 10, 2013 in [24]ml primers, [25]neural networks

   id158s (anns) were originally devised in the
   mid-20th century as a computational model of the human brain. their
   used waned because of the limited computational power available at the
   time, and some theoretical issues that weren't solved for several
   decades (which i will detail at the end of this post). however, they
   have experienced a resurgence with the recent interest and hype
   surrounding deep learning. one of the more famous examples of deep
   learning is the "[26]youtube cat" paper by andrew ng et al.

   it is theorized that because of their biological inspiration, ann-based
   learners will be able to emulate how a human learns to recognize
   concepts or objects without the time-consuming [27]feature
   engineering step. whether or not this is true (or even provides an
   advantage in terms of development time) remains to be seen, but
   currently it's important that we machine learning researchers and
   enthusiasts have a familiarity with the basic concepts of neural
   networks.

   this post covers the basics of anns, namely single-layer networks. we
   will cover three applications: id75, two-class
   classification using the id88 algorithm and multi-class
   classification.

theory

   neural network terminology is inspired by the biological operations of
   specialized cells called neurons. a neuron is a cell that has several
   inputs that can be activated by some outside process. depending on the
   amount of activation, the neuron produces its own activity and sends
   this along its outputs. in addition, specific input or output paths may
   be "strengthened" or weighted higher than other paths. the hypothesis
   is that since the human brain is nothing but a network of neurons, we
   can emulate the brain by modeling a neuron and connecting them via a
   weighted graph.

   the artificial equivalent of a neuron is a node (also sometimes called
   neurons, but i will refer to them as nodes to avoid ambiguity) that
   receives a set of weighted inputs, processes their sum with
   its activation function $\phi$, and passes the result of the activation
   function to nodes further down the graph. note that it is simpler to
   represent the input to our activation function as a dot product:
   $$ \phi \left(\sum_i w_i a_i\right) = \phi(\mathbf{w}^t\mathbf a) $$

   visually this looks like the following:
   node.png node.png

   there are several canonical id180. for instance, we can
   use a linear activation function:
   $$ \phi (\mathbf{w}^t\mathbf{a}) = \mathbf{w}^t\mathbf a $$

   this is also called the identity activation function. another example
   is the sigmoid activation function:
   $$ \phi (\mathbf{w}^t\mathbf{a}) =
   \frac{1}{1+\exp(-\mathbf{w}^t\mathbf{a})} $$

   one more example is the tanh activation function:
   $$ \phi(\mathbf{w}^t\mathbf{a}) = \mbox{tanh}(\mathbf{w}^t\mathbf{a})
   $$

   we can then form a network by chaining these nodes together. usually
   this is done in layers - one node layer's outputs are connected to the
   next layer's inputs (we must take care not to introduce cycles in our
   network, for reasons that will become clear in the section on
   id26).

   our goal is to train a network using labelled data so that we can then
   feed it a set of inputs and it produces the appropriate outputs
   for unlabeled data. we can do this because we have both the input $
   \mathbf{x}_i $ and the desired target output $ y_i $ in the form of
   data pairs. training in this case involves learning the correct edge
   weights to produce the target output given the input. the network and
   its trained weights form a function (denoted $ h $) that operates on
   input data. with the trained network, we can make predictions given any
   unlabeled test input.


   training and testing in the neural network context. note that a
   multilayer network is shown here. training a multilayer network is
   covered in parts 3-6 of this primer. training and testing in the neural
   network context. note that a multilayer network is shown here. training
   a multilayer network is covered in parts 3-6 of this primer.

   training and testing in the neural network context. note that a
   multilayer network is shown here. training a multilayer network is
   covered in parts 3-6 of this primer.

   we can train a neural network to perform regression or classification.
   in this part, i will cover id75 with a single-layer
   network. classification and multilayer networks are covered in later
   parts.

id75

   id75 is the simplest form of regression.  we model our
   system with a linear combination of features to produce one output.
   that is,

   $$ y_i = h(\mathbf{x}_i, \mathbf{w}) = \mathbf{w}^t\mathbf{x}_i $$

   our task is then to find the weights the provide the best fit to our
   training data. one way to measure our fit is to calculate the leasts
   squares error (or loss) over our dataset:

   $$ l(\mathbf{w}) = \sum_i \left(h(\mathbf{x}_i, \mathbf{w}) -
   y_i\right)^2 $$

   in order to find the line of best fit, we must minimize $ l(\mathbf{w})
   $. this has a closed-form solution for ordinary least squares, but in
   general we can minimize loss using id119.

training a neural network to perform id75

   so what does this have to do with neural networks? in fact, the
   simplest neural network performs least squares regression. consider the
   following single-layer neural network, with a single node that uses a
   linear activation function:
   linear_1layer.png linear_1layer.png

   this network takes as input a data point with two features $ x_i^{(1)},
   x_i^{(2)} $, weights the features with $ w_1, w_2 $ and sums them, and
   outputs a prediction . we could define a network that takes data with
   more features, but we would have to keep track of more weights, e.g. $
   w_1, \ldots, w_j $ if there are $ j $ features.

   if we use quadratic loss to measure how well our network performs,
   (quadratic loss is a common choice for neural networks), it would be
   identical to the loss defined for least squares regression above:
   $$l(\mathbf{w}) = \sum_i \left(h(\mathbf{x}_i, \mathbf{w}) -
   y_i\right)^2 $$

   this is the sum squared error of our network's predictions over our
   entire training set.

   we will then use id119 on the loss's gradient $
   \nabla_{\mathbf{w}} l(\mathbf{w}) $ in order to minimize the overall
   error on the training data. we first derive the gradient of the loss
   with respect to a particular weight $ w_{j \rightarrow k} $ (which is
   just the weight of the edge connecting node $ j $ to node $k$ [note
   that we treat inputs as "nodes," so there is a weight $ w_{j
   \rightarrow k} $ for each connection from the input to a first-layer
   node]) in the general case:


   $$ \begin{align} \frac{\partial}{\partial w_{j \rightarrow k}}
   l(\mathbf{w}) =& \frac{\partial}{\partial w_{j \rightarrow k}} \sum_i
   \left(h(\mathbf{x}_i, \mathbf{w})-y_i\right)^2\\ =& \sum_i
   \frac{\partial}{\partial w_{j \rightarrow k}} \left(h(\mathbf{x}_i,
   \mathbf{w})-y_i\right)^2\\ =& \sum_i 2\left(h(\mathbf{x}_i,
   \mathbf{w})-y_i\right) \frac{\partial}{\partial w_{j \rightarrow k}}
   h(\mathbf{x}_i, \mathbf{w}) \end{align} $$

   at this point, we must compute the gradient of our network function
   with respect to the weight in question ($ \frac{\partial}{\partial w_{j
   \rightarrow k}} h(\mathbf{x}_i, \mathbf{w}) $). in the case of a single
   layer network, this turns out to be simple.

   recall our simple two input network above. the network function is $
   h(\mathbf{x}_i, \mathbf{w}) = w_1x_i^{(1)} + w_2x_i^{(2)} $. the
   gradient with respect to $ w_1 $ is just $ x_1 $, and the gradient with
   respect to $ w_2 $ is just $ x_2 $. we usually store all the weights of
   our network in a vector or a matrix, so the full gradient is:


   $$ \nabla_{\mathbf{w}}l(\mathbf{w}) = \left(\frac{\partial
   l(\mathbf{w})}{\partial w_1}, \frac{\partial l(\mathbf{w})}{\partial
   w_2}\right) = \left(\sum_i 2x_i^{(1)}h(\mathbf{x}_i, \mathbf{w}),
   \sum_i 2x_i^{(2)}h(\mathbf{x}_i, \mathbf{w})\right) $$

    using this, we then update our weights using standard gradient
   descent:
   $$ \mathbf{w} = \mathbf{w} - \eta \nabla_{\mathbf{w}} l(\mathbf{w}) $$

   as with all id119 methods, care must be taken to select the
   "right" step size $ \eta $, with "right" being application-dependent.
   after a set amount of epochs, the weights we end up with define a line
   of best-fit.

testing

   with our trained network, testing consists of obtaining a prediction
   for each test point $ x_i $ using $ h(\mathbf{x}_i, \mathbf{w}) $. the
   test error is computed with the quadratic loss, exactly as in training:
   $$ l(\mathbf{w}) = \sum_i \left( h(\mathbf{x}_i, \mathbf{w}) -
   y_i\right)^2 = \sum_i \left( \hat{y}_i - y_i\right)^2 $$

implementation

   for this implementation, we will use the weight of a car to predict its
   mpg. the data looks something like this:
   mpg.png mpg.png

   note that this relationship does not appear to be linear - linear
   regression will probably not find the underlying relationship between
   weight and mpg. however, it will  find a line that models the data
   "pretty well."

   as with my other tutorials, i will be using python with numpy (for
   matrix math operations) and matplotlib (for plotting). (all the code
   listed here is located in the file ann_linear_1d_regression.py). to
   begin, let's first load the mpg data from mpg.csv:
x_file = np.genfromtxt('mpg.csv', delimiter=',', skip_header=1)
n = np.shape(x_file)[0]
x = np.hstack((np.ones(n).reshape(n, 1), x_file[:, 4].reshape(n, 1)))
y = x_file[:, 0]

   this loads our data into two matrices, $ x $ (containing the features,
   the weight) and $ y $ (containing the labels). you may haven noticed
   something odd - we are also appending a column of ones to $ x $. it is
   important to have bias weights in our neural network - otherwise, we
   could only fit functions that pass through 0.

   next, we standardize the input. this is another implementation-specific
   detail. although it is not [28]theoretically necessary, it helps
   provide stability to our id119 routine and prevents our
   weights from quickly "blowing up." we standardize the weight features
   by subtracting their mean and normalizing by their standard deviation:

x[:, 1] = (x[:, 1]-np.mean(x[:, 1]))/np.std(x[:, 1])

   then we begin id119. we will run batch id119 for
   100 epochs with a step size $ \eta $ = 0.001:
max_iter = 100
eta = 1e-3
for t in range(0, max_iter):
    # we need to iterate over each data point for one epoch
    grad_t = np.array([0., 0.])
    for i in range(0, n):
        x_i = x[i, :]
        y_i = y[i]
        # dot product, computes h(x_i, w)
        h = np.dot(w, x_i)-y_i
        grad_t += 2*x_i*h

    # update the weights
    w = w - eta*grad_t

   in line 30, we compute the network function $ h(\mathbf{x}_i,
   \mathbf{w}) = \mathbf{w}^t \mathbf{x}_i $. in line 31, we compute the
   actual gradient for both weights simultaneously and add them to the
   gradient of the current epoch. then, in line 34 we perform the gradient
   descent update. finally, to compute the line of best fit, we use the
   following:

tt = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 10)
bf_line = w[0]+w[1]*tt

   this uses the weights to compute the value of the line with the same
   domain spanned by our data. here is an animation of the line of best
   fit at each epoch, which illustrates how the result of our weight
   update at each step:
   weights.gif weights.gif

   you can download the code and data [29]here. for posterity, here is the
   complete source file, complete with plotting functionality.
import matplotlib.pyplot as plt
import numpy as np

# load the data and create the data matrices x and y
# this creates a feature vector x with a column of ones (bias)
# and a column of car weights.
# the target vector y is a column of mpg values for each car.
x_file = np.genfromtxt('mpg.csv', delimiter=',', skip_header=1)
n = np.shape(x_file)[0]
x = np.hstack((np.ones(n).reshape(n, 1), x_file[:, 4].reshape(n, 1)))
y = x_file[:, 0]

# standardize the input
x[:, 1] = (x[:, 1]-np.mean(x[:, 1]))/np.std(x[:, 1])

# there are two weights, the bias weight and the feature weight
w = np.array([0, 0])

# start batch id119, it will run for max_iter epochs and have a step
# size eta
max_iter = 100
eta = 1e-3
for t in range(0, max_iter):
    # we need to iterate over each data point for one epoch
    grad_t = np.array([0., 0.])
    for i in range(0, n):
        x_i = x[i, :]
        y_i = y[i]
        # dot product, computes h(x_i, w)
        h = np.dot(w, x_i)-y_i
        grad_t += 2*x_i*h

    # update the weights
    w = w - eta*grad_t
print "weights found:",w

# plot the data and best fit line
tt = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 10)
bf_line = w[0]+w[1]*tt

plt.plot(x[:, 1], y, 'kx', tt, bf_line, 'r-')
plt.xlabel('weight (normalized)')
plt.ylabel('mpg')
plt.title('ann regression on 1d mpg data')

plt.savefig('mpg.png')

plt.show()

conclusion

   in this post, i detailed how to emulate id75 using a
   simple neural network. using a neural network for this task may seem
   useless, but the concepts covered in this post carry over to more
   complicated networks.

   several questions remain. what if we want to perform classification?
   and how do we implement multilayer networks? stay tuned for more parts
   in this series.

references

   [1] [30]http://www.willamette.edu/~gorr/classes/cs449/intro.html

   [2] [31]http://blog.zabarauskas.com/id26-tutorial/


   tags: [32]neural network, [33]tutorial

   [34]prev / [35]next

references

   1. http://www.briandolhansky.com/blog?format=rss
   2. http://www.briandolhansky.com/
   3. http://www.briandolhansky.com/blog
   4. http://www.briandolhansky.com/research
   5. http://www.briandolhansky.com/cv-online
   6. http://www.briandolhansky.com/photography
   7. http://www.briandolhansky.com/arches
   8. http://www.briandolhansky.com/berlin
   9. http://www.briandolhansky.com/lindau
  10. http://www.briandolhansky.com/munich
  11. http://www.briandolhansky.com/people-taking-pictures
  12. http://www.briandolhansky.com/
  13. http://www.briandolhansky.com/
  14. http://www.briandolhansky.com/blog
  15. http://www.briandolhansky.com/research
  16. http://www.briandolhansky.com/cv-online
  17. http://www.briandolhansky.com/arches
  18. http://www.briandolhansky.com/berlin
  19. http://www.briandolhansky.com/lindau
  20. http://www.briandolhansky.com/munich
  21. http://www.briandolhansky.com/people-taking-pictures
  22. http://www.briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1
  23. http://www.briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1
  24. http://www.briandolhansky.com/blog?category=ml primers
  25. http://www.briandolhansky.com/blog?category=neural networks
  26. http://arxiv.org/abs/1112.6209
  27. http://;http://www.cs.princeton.edu/courses/archive/spring10/cos424/slides/18-feat.pdf
  28. http://www.faqs.org/faqs/ai-faq/neural-nets/part2/
  29. http://www.briandolhansky.com/s/ann_1d_lr.zip
  30. http://www.willamette.edu/~gorr/classes/cs449/intro.html
  31. http://blog.zabarauskas.com/id26-tutorial/
  32. http://www.briandolhansky.com/blog?tag=neural+network#show-archive
  33. http://www.briandolhansky.com/blog?tag=tutorial#show-archive
  34. http://www.briandolhansky.com/blog/2013/7/11/snippets-android-async-progress
  35. http://www.briandolhansky.com/blog/2013/7/8/ml-primers
