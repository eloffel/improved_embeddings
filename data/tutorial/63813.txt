   #[1]rss feed

   [tr?id=1853843318208763&ev=pageview&noscript=1]

     * [2]home
     * [3]blog
     * [4]research
     * [5]cv
     * [6]photography
          + [7]arches
          + [8]berlin
          + [9]lindau
          + [10]munich
          + [11]people taking pictures

     * menu

[12]brian dolhansky

   ml     code     photography

     * [13]home
     * [14]blog
     * [15]research
     * [16]cv
     * photography
          + [17]arches
          + [18]berlin
          + [19]lindau
          + [20]munich
          + [21]people taking pictures

[22]id158s: linear classification (part 2)

   [23]september 23, 2013 in [24]ml primers, [25]neural networks

   so far we've covered using neural networks to perform linear
   regression. what if we want to perform classification using a
   single-layer network?  in this post, i will cover two methods: the
   id88 algorithm and using a sigmoid activation function to
   generate a likelihood. i will not cover the [26]delta rule because it
   is a special case of the more general id26 algorithm, which
   will be covered in detail in part 4.

theory

single-layer id88

   perhaps the simplest neural network we can define for binary
   classification is the single-layer id88. given an input, the
   output neuron fires (produces an output of 1) only if the data point
   belongs to the target class. otherwise, it does not fire (it produces
   an output of -1). the network looks something like this:
   sl_id88.png sl_id88.png
   instead of using a linear activation function like in linear
   regression, we instead use a sign function. recall the definition of
   the sign function:
   $$ \mbox{sign}(\mathbf{w}^t\mathbf{x}_i) = \begin{cases} 1 &\mbox{if
   }\mathbf{w}^t\mathbf{x}_i > 0 \\ 0 &\mbox{if }\mathbf{w}^t\mathbf{x}_i
   = 0 \\ -1 &\mbox{if }\mathbf{w}^t\mathbf{x}_i < 0 \end{cases} $$

   in this, we are computing the dot product of an example with our weight
   vector. points with positive projections will be given a label of 1 and
   points with negative projections will be given a label of -1.
   consequently, our decision boundary will be perpendicular to our weight
   vector. why? consider a 2-dimensional decision problem. the decision
   boundary is the line where it is equally probable that a point on that
   line belongs to either class, i.e. $ h(\mathbf{x}_i, \mathbf{w}) =
   \mbox{sign}(\mathbf{w}^t\mathbf{x}_i) = 0 $, or $
   \mathbf{w}^t\mathbf{x}_i = 0 $. then we have:
   $$ \begin{align} \mathbf{w}^t\mathbf{x}_i =& 0\\ w_1+w_2x_i^{(2)} +
   w_3x_i^{(3)} =& 0\\ x_i^{(3)} =& -\frac{w_2}{w_3}x_i^{(2)}
   -\frac{w_1}{w_3} \end{align} $$

   in this case, $ x_i^{(1)} $ is our bias value and is always equal to 1,
   $ x_i^{(2)}  $ is "$ x $" in the cartesian plane and $ x_i^{(3)} $ is
   "$ y $." the slope of our weight vector in the cartesian plane is $
   \frac{w_3}{w_2} $ (they "$ y $" component of $ \mathbf{w} $ is $ w_3 $,
   and the "$ x $" component is $ w_2 $), while the slope of the decision
   boundary is $ -\frac{w_2}{w_3} $ (thus making them
   perpendicular). graphically, this looks something like this:
   linear_db.png linear_db.png

   the problem we now face is that the step function is not continuously
   differentiable, and we cannot use standard id119 to learn
   the weights. therefore, we will use the appropriately-named id88
   algorithm . this algorithm is an online method used to successively
   update the weights defining a linear boundary only if that boundary
   does not classify a training point correctly.  the algorithm is as
   follows:

     * initialize the weight vector $ \mathbf{w} $ to all zeros.
     * repeat the following:
         1. for each training example $ \mathbf{x}_i $:
               o if $ h(\mathbf{x}_i, \mathbf{w}) \neq y_i $, then update
                 the weights with $ \mathbf{w}' = \mathbf{w}+\eta
                 y_i\mathbf{x}_i $ here, $ \eta $ is the step size.
         2. if the stopping condition $ \frac{1}{n} \sum_{j=0}^m |w_j' -
            w_j| < \delta $ is reached, then accept $ \mathbf{w} $ as the
            final weight vector ($ m $ in this case is the number of
            features in the dataset).

   if our problem is linearly separable, the id88 algorithm is
   [27]guaranteed to converge. therefore, at the algorithm's termination,
   we will end up with a linear decision boundary defined by $ \mathbf{w}
   $. however, this decision boundary is not guaranteed to be a
   [28]maximum margin hyperplane as in the case of id166s.

   finally, if we want to predict the label $ \hat{y}_i $ of a test point
   $ \mathbf{x}_i $, we use $ \hat{y}_i =
   \mbox{sign}(\mathbf{w}^t\mathbf{x}_i) $.

   while it is not strictly necessary to define a neural network to use
   the id88 algorithm, this is a good first step towards
   single-layer classification.

classification with a sigmoid (softmax) activation function

   instead of an all-or-nothing classifier (like the sign function), it is
   helpful to come up with some way to measure the id203 of
   assignment, that is $ p( y = y_i\ |\ x = \mathbf{x}_i, \mathbf{w} ) $.
   if we can calculate this likelihood, we can use as a confidence measure
   of our predictions.

   instead of using a sign activation function, we can instead use a
   sigmoid (usually called softmax in the neural net literature) to output
   a id203:
   $$ p(y = y\ |\ x = \mathbf{x}_i, \mathbf{w}) =
   \frac{1}{1+\exp\left(-y\mathbf{w}^t\mathbf{x}_i\right)}$$

   but how do we assign a class label when given only a id203? we
   can simply "clamp" the id203 using a sign function, so that any $
   p(y = 1\ |\ \mathbf{x}_i, \mathbf{w}) \geq 0.5$ is assigned a class
   label of 1, and any id203 less than 0.5 is given a class label of
   -1. our simple network now looks something like the following:
   linear_sigmoid.png linear_sigmoid.png

   luckily for us, this network function is identical to the likelihood
   used by id28. because the sigmoid is differentiable, we
   can use standard gradient acent to train the weights instead of the
   id88 algorithm. for a derivation of the gradient for logistic
   regression, see the appendix.

implementation

   to implement this theory, we'll be learning a set of weights that
   classify two groups of 2d data using both the id88 algorithm and
   id119. let's start out by defining our 2d data (you can find
   this code in ann_linear_2d_classification_id88.py):
# generate two random clusters of 2d data
n_c = 100
a = 0.3*np.random.randn(n_c, 2)+[1, 1]
b = 0.3*np.random.randn(n_c, 2)+[3, 3]
x = np.hstack((np.ones(2*n_c).reshape(2*n_c, 1), np.vstack((a, b))))
y = np.vstack(((-1*np.ones(n_c)).reshape(n_c, 1), np.ones(n_c).reshape(n_c, 1)))
n = 2*n_c

   this code generates two 2d clusters centered around the points (1, 1)
   and (3, 3). it then appends a 1 to each point, which is our bias
   feature. finally, it assigns the label 1 to one cluster, and the label
   -1 to the other cluster. the features are put in the matrix $ x $ and
   the labels in the vector $ y $.
   next, we run the id88 algorithm to learn the weights:
# run id88
delta = 1e-7
eta = 1e-2
max_iter = 500
w = np.array([0, 0, 0])
w_old = np.array([0, 0, 0])
for t in range(0, max_iter):
    for i in range(0, n):
        x_i = x[i, :]
        y_i = y[i]
        h = np.sign(np.dot(w, x_i))
        if h != y_i:
            w = w+eta*y_i*x_i

    if 1/(float(n))*np.abs(np.sum(w_old-w)) < delta:
        print "converged in", t, "steps."
        break

    w_old = w

    if t==max_iter-1:
        print "warning, did not converge."

print "weights found:",w

   this snippet follows the id88 algorithm directly. in line 15 we
   initialize the weights to 0. in line 16 we also initialize a vector
   that stores the previous weights, which we will use to test the
   stopping condition. note that we only run the algorithm for max_iter
   steps so that it doesn't loop indefinitely if the problem is not
   separable.
   in line 21, we test to see if $ h(\mathbf{x}_i) = y_i $, and if it
   doesn't we update the weights accordingly. finally, we test the
   stopping condition in line 25.
   in one particular test run, you can see that the algorithm converged in
   6 steps:
   two_class.gif two_class.gif
   what if, instead of using the id88, we wanted to learn a sigmoid
   using id119? we can use the same procedure for gradient
   descent as detailed in [29]part 1 of this series. however, in the
   previous section, we ran id119 for a set amount of epochs.
   here we introduce an alternative stopping condition. the heuristic is
   this: if the norm of the gradient is small, we must be nearing the
   minimum of the function. thus, if we set a threshold for the norm of
   the gradient, and it falls below this threshold at a step, we stop. the
   interesting part of the gradient code is as follows:
# run id119
delta = 1e-7
eta = 1e-3
max_iter = 1000
w = np.array([0, 0, 0])
grad_thresh = 5
for t in range(0, max_iter):
    grad_t = np.array([0., 0., 0.])
    for i in range(0, n):
        x_i = x[i, :]
        y_i = y[i]

        grad_t += y_i*x_i*(np.exp(-y_i*np.dot(w, x_i)))/(1+np.exp(-y_i*np.dot(w,
 x_i)))

    w = w + 1/float(n)*eta*grad_t
    grad_norm = np.linalg.norm(grad_t)
    print grad_norm
    if grad_norm < grad_thresh:
        print "converged in ",t+1,"steps."
        break

print "weights found:",w


   in line 24, we compute the gradient according to the derivation in the
   appendix. in lines 28-29, we check to see if we've reached the stopping
   condition. this is a simple heuristic that may not work in all cases,
   but works well enough for a simple problem like this. determining how
   to compute the right parameters for id119 is an active area
   of research.

code download

   [30]python file that learns weights using the id88 algorithm.
   [31]python file that learns weights using gradient ascent.

appendix

derivation of the gradient for id28


   the likelihood of our data defined by id28 is: $$ p(d_y
   | d_x, \mathbf{w}) = \prod_i^n
   \frac{1}{1+\exp(-y_i\mathbf{w}^t\mathbf{x_i})} $$ then the log
   likelihood is: $$ \ell(\mathbf{w}) = \log(p(d_y | d_x, \mathbf{w})) =
   -\sum_i^n \log(1+\exp(-y_i\mathbf{w}^t\mathbf{x}_i) $$ for gradient
   ascent, we are trying to select $ \mathbf{w} $ in order to maximize the
   log likelihood. to do this, we must first calculate the gradient:
   $$\begin{align} \nabla_{\mathbf{w}}\ell(\mathbf{w}) &=
   \frac{\partial}{\partial \mathbf{w}} \left(-\sum_i^n
   \log(1+\exp(-y_i\mathbf{w}^t\mathbf{x}_i))\right)\\ &= \sum_i^n
   \frac{y_i\mathbf{x}_i\exp(-y_i\mathbf{w}^t\mathbf{x_i})}{1+\exp(-y_i\ma
   thbf{w}^t\mathbf{x}_i)} \end{align}$$ because $$ 1 - p(y_i |
   \mathbf{x}_i, \mathbf{w}) =
   \frac{\exp(-y_i\mathbf{w}^t\mathbf{x_i})}{1+\exp(-y_i\mathbf{w}^t\mathb
   f{x}_i)} $$ the gradient is $$ \nabla_{\mathbf{w}}\ell(\mathbf{w}) =
   \sum_i^n y_i\mathbf{x}_i(1-p(y_i | \mathbf{x}_i, \mathbf{w})) $$ to
   avoid having to change our learning rate for different dataset sizes,
   we can scale by the number of examples: $$
   \nabla_{\mathbf{w}}\ell(\mathbf{w}) = \frac{1}{n}\sum_i^n
   y_i\mathbf{x}_i(1-p(y_i | \mathbf{x}_i, \mathbf{w})) $$

   tags: [32]neural network, [33]tutorial

   [34]prev / [35]next

references

   1. http://www.briandolhansky.com/blog?format=rss
   2. http://www.briandolhansky.com/
   3. http://www.briandolhansky.com/blog
   4. http://www.briandolhansky.com/research
   5. http://www.briandolhansky.com/cv-online
   6. http://www.briandolhansky.com/photography
   7. http://www.briandolhansky.com/arches
   8. http://www.briandolhansky.com/berlin
   9. http://www.briandolhansky.com/lindau
  10. http://www.briandolhansky.com/munich
  11. http://www.briandolhansky.com/people-taking-pictures
  12. http://www.briandolhansky.com/
  13. http://www.briandolhansky.com/
  14. http://www.briandolhansky.com/blog
  15. http://www.briandolhansky.com/research
  16. http://www.briandolhansky.com/cv-online
  17. http://www.briandolhansky.com/arches
  18. http://www.briandolhansky.com/berlin
  19. http://www.briandolhansky.com/lindau
  20. http://www.briandolhansky.com/munich
  21. http://www.briandolhansky.com/people-taking-pictures
  22. http://www.briandolhansky.com/blog/2013/7/11/artificial-neural-networks-linear-classification-part-2
  23. http://www.briandolhansky.com/blog/2013/7/11/artificial-neural-networks-linear-classification-part-2
  24. http://www.briandolhansky.com/blog?category=ml primers
  25. http://www.briandolhansky.com/blog?category=neural networks
  26. http://en.wikipedia.org/wiki/delta_rule
  27. http://en.wikipedia.org/wiki/id88#convergence
  28. https://en.wikipedia.org/wiki/maximum-margin_hyperplane
  29. http://briandolhansky.com/blog/artificial-neural-networks-linear-regression-part-1
  30. http://www.briandolhansky.com/s/ann_linear_2d_classification.py
  31. http://www.briandolhansky.com/s/ann_linear_2d_classification_gd.py
  32. http://www.briandolhansky.com/blog?tag=neural+network#show-archive
  33. http://www.briandolhansky.com/blog?tag=tutorial#show-archive
  34. http://www.briandolhansky.com/blog/2013/9/23/artificial-neural-nets-linear-multiclass-part-3
  35. http://www.briandolhansky.com/blog/2013/7/11/snippets-android-async-progress
