   #[1]rss feed

   [tr?id=1853843318208763&ev=pageview&noscript=1]

     * [2]home
     * [3]blog
     * [4]research
     * [5]cv
     * [6]photography
          + [7]arches
          + [8]berlin
          + [9]lindau
          + [10]munich
          + [11]people taking pictures

     * menu

[12]brian dolhansky

   ml     code     photography

     * [13]home
     * [14]blog
     * [15]research
     * [16]cv
     * photography
          + [17]arches
          + [18]berlin
          + [19]lindau
          + [20]munich
          + [21]people taking pictures

[22]id158s: linear multiclass classification (part 3)

   [23]september 27, 2013 in [24]ml primers, [25]neural networks

   in the last section, we went over how to use a linear neural network to
   perform classification. we covered using both the id88 algorithm
   and id119 with a sigmoid activation function to learn the
   placement of the decision boundary in our feature space. however, we
   only covered binary classification. what if we instead want to classify
   a point belonging to one of $k$ classes?

theory

   multiclass classification using a linear neural network is a fairly
   simple extension of the binary classification setup. you may think that
   instead of outputting 0/1 from our second layer node, we could output
   0, 1, ..., $k-1$. however, this is not the case . our labels are not
   necessarily linear, and halfway between $\hat{y}=0$ and $\hat{y}=2$ is
   not necessarily $\hat{y}=1$. they are in fact categorical, and we use
   $k \in \{0, 1, \ldots, k\}$ out of computational convenience.

   consider instead representing a label using a binary vector of length
   $k$. having a 1 in position $k$ corresponds to a label of $k$. then, we
   can extend our linear network (with a sigmoid activation at the output)
   to learn how to output this vector. it would look something like the
   following:
   linear_multiclass.png linear_multiclass.png
   note that instead of $|\mathbf{w}| = m$ (where $m$ is the number of
   features), we instead have $|\mathbf{w}| = mk$. so in this figure, we
   have $k=3$ classes, and 3 features, giving us 9 weights in total. in
   fact, this is the neural network view of multinomial logistic
   regression. recall the previous likelihood used in binary logistic
   regression: $$ p(y = y\ |\ x = \mathbf{x}_i, \mathbf{w}) =
   \frac{1}{1+\exp\left(-y\mathbf{w}^t\mathbf{x}_i\right)}$$ to extend
   this to $k$ classes, we use the following likelihood: $$ p(y = k\ |\ x
   = \mathbf{x}_i, \mathbf{w}) =
   \frac{\exp(\mathbf{w}_k^t\mathbf{x}_i)}{\sum_{k'}^k\exp\left(\mathbf{w}
   _{k'}^t\mathbf{x}_i\right)}$$ the key difference is that there are now
   $k$ sets of $m$ weights, one for each label. these are specified when
   determining the likelihood for a particular $k$. by using this
   formalism, we ensure that the values produced by the output nodes forms
   a valid id203 distribution, as we are normalizing the likelihood
   by summing over all values of $k$.
   our training routine is exactly the same as in part 2, except that the
   gradient of the multinomial id28 objective is slightly
   different: $$ \nabla_{\mathbf{w}_k}\ell(\mathbf{w}) = \frac{1}{n}
   \sum_i^n\left(\mathbf{x_i}(1-p(y = y_i\ |\ x = \mathbf{x}_i,
   \mathbf{w})\right) $$ see the appendix for a full derivation.

implementation

   the implementation of multiclass linear classification doesn't change
   much from the binary case, except for the gradient and how we label our
   data points.  for this toy example, we'll be generating 3 clusters of
   two-dimensional data. to make things more interesting, we won't
   restrict them to be linearly separable.
   let's jump right into it. this code can be found in
   ann_linear_multiclassification.py. first, we generate three clusters of
   data, and give each a label of either 0, 1, or 2:
# generate three random clusters of 2d data
n_c = 200
a = 0.6*np.random.randn(n_c, 2)+[1, 1]
b = 0.6*np.random.randn(n_c, 2)+[3, 3]
c = 0.6*np.random.randn(n_c, 2)+[3, 0]
x = np.hstack((np.ones(3*n_c).reshape(3*n_c, 1), np.vstack((a, b, c))))
y = np.vstack(((np.zeros(n_c)).reshape(n_c, 1),
        np.ones(n_c).reshape(n_c, 1), 2*np.ones(n_c).reshape(n_c, 1)))
k = 3
n = k*n_c

   next we run id119 using the multinomial id28
   gradient:
# run id119
eta = 1e-2
max_iter = 1000
w = np.zeros((3, 3))
grad_thresh = 5
for t in range(0, max_iter):
    grad_t = np.zeros((3, 3))
    for i in range(0, n):
        x_i = x[i, :]
        y_i = y[i]
        exp_vals = np.exp(w.dot(x_i))
        lik = exp_vals[int(y_i)]/np.sum(exp_vals)
        grad_t[int(y_i), :] += x_i*(1-lik)

    w = w + 1/float(n)*eta*grad_t
    grad_norm = np.linalg.norm(grad_t)

    if grad_norm < grad_thresh:
        print "converged in ",t+1,"steps."
        break

    if t == max_iter-1:
        print "warning, did not converge."

   there are a couple of things to note here. first, our weight vector
   $\mathbf{w}$ is now actually a 3x3 matrix. we have 3 classes, so we
   need 3 sets of weights. recall that each set of weights has a bias
   weight and a weight for each feature, giving them a length of $m+1$.
   therefore, each row of $\mathbf{w}$ corresponds to the weight for each
   $k \in \{0, 1, 2\}$. you can see this initialization in line 20.
   in line 27, we calculate the unnormalized likelihood using numpy's dot
   function. dot computes the dot product between the input $\mathbf{x_i}$
   and each of the weights in a row-wise fashion. in line 28, we generate
   the normalized likelihood that our datapoint $\mathbf{x}_i$ belongs to
   class 0, 1, or 2. finally, in line 29, we use these likelihoods to
   calculate the gradient. this code follows the math exactly, so there
   shouldn't be any surprises here.
   running around 1,000 epochs with the given descent parameters will
   generate classification regions in the following manner:
   multiclass.gif multiclass.gif

code download

   [26]2d multi-label classification python code.

appendix

deriving the gradient for multinomial id28


   our likelihood over the data for mlr is the following: $$ p(d_y\ |\
   d_x, \mathbf{w}) = \prod_i^n
   \frac{\exp(\mathbf{w}_{y_i}^t\mathbf{x}_i)}{\sum_{k'}^k\exp\left(\mathb
   f{w}_{k'}^t\mathbf{x}_i\right)}$$ the log likelihood is then: $$
   \ell(\mathbf{w}) = \sum_i^n\left(\mathbf{w}_{y_i}^t\mathbf{x}_i -
   \log\left( \sum_{k'}^k \exp(\mathbf{w}_k^t\mathbf{x}_i) \right)
   \right)$$ we can then determine the gradient for a particular set of
   weights when $y_i = k$: $$ \begin{align}
   \nabla_{\mathbf{w}_k}\ell(\mathbf{w}) =& \frac{\partial}{\partial
   \mathbf{w}_k}\sum_i^n\left(\mathbf{w}_{k}^t\mathbf{x}_i - \log\left(
   \sum_{k'}^k \exp(\mathbf{w}_k^t\mathbf{x}_i) \right) \right)\\ =&
   \sum_i^n\left(\frac{\partial}{\partial
   \mathbf{w}_k}\mathbf{w}_k^t\mathbf{x}_i - \frac{\partial}{\partial
   \mathbf{w}_k}\log\left( \sum_{k'}^k \exp(\mathbf{w}_k^t\mathbf{x}_i)
   \right) \right)\\ =& \sum_i^n\left(\mathbf{x}_i -
   \frac{\mathbf{x}_i\exp(\mathbf{w}_k^t\mathbf{x}_i)}{\sum_{k'}^k
   \exp(\mathbf{w}_k^t\mathbf{x}_i)} \right) \end{align} $$ recall that
   our likelihood is: $$ p(y = k\ |\ x = \mathbf{x}_i, \mathbf{w}) =
   \frac{\exp(\mathbf{w}_k^t\mathbf{x}_i)}{\sum_{k'}^k\exp\left(\mathbf{w}
   _{k'}^t\mathbf{x}_i\right)} $$ so then our gradient is: $$
   \nabla_{\mathbf{w}_k}\ell(\mathbf{w}) =
   \sum_i^n\left(\mathbf{x_i}(1-p(y = y_i\ |\ x = \mathbf{x}_i,
   \mathbf{w})\right) $$ the gradient contribution of a particular example
   for all $\mathbf{w}_{k\neq y_i}$ is zero, so for each example we only
   need to update the particular set of weights corresponding to $k =
   y_i$.

   tags: [27]neural network, [28]multiclass, [29]softmax, [30]tutorial

   [31]prev / [32]next

references

   1. http://www.briandolhansky.com/blog?format=rss
   2. http://www.briandolhansky.com/
   3. http://www.briandolhansky.com/blog
   4. http://www.briandolhansky.com/research
   5. http://www.briandolhansky.com/cv-online
   6. http://www.briandolhansky.com/photography
   7. http://www.briandolhansky.com/arches
   8. http://www.briandolhansky.com/berlin
   9. http://www.briandolhansky.com/lindau
  10. http://www.briandolhansky.com/munich
  11. http://www.briandolhansky.com/people-taking-pictures
  12. http://www.briandolhansky.com/
  13. http://www.briandolhansky.com/
  14. http://www.briandolhansky.com/blog
  15. http://www.briandolhansky.com/research
  16. http://www.briandolhansky.com/cv-online
  17. http://www.briandolhansky.com/arches
  18. http://www.briandolhansky.com/berlin
  19. http://www.briandolhansky.com/lindau
  20. http://www.briandolhansky.com/munich
  21. http://www.briandolhansky.com/people-taking-pictures
  22. http://www.briandolhansky.com/blog/2013/9/23/artificial-neural-nets-linear-multiclass-part-3
  23. http://www.briandolhansky.com/blog/2013/9/23/artificial-neural-nets-linear-multiclass-part-3
  24. http://www.briandolhansky.com/blog?category=ml primers
  25. http://www.briandolhansky.com/blog?category=neural networks
  26. http://www.briandolhansky.com/s/ann_linear_multiclassification.py
  27. http://www.briandolhansky.com/blog?tag=neural+network#show-archive
  28. http://www.briandolhansky.com/blog?tag=multiclass#show-archive
  29. http://www.briandolhansky.com/blog?tag=softmax#show-archive
  30. http://www.briandolhansky.com/blog?tag=tutorial#show-archive
  31. http://www.briandolhansky.com/blog/2014/4/27/search-bash-history-by-first-few-characters-like-matlab
  32. http://www.briandolhansky.com/blog/2013/7/11/artificial-neural-networks-linear-classification-part-2
