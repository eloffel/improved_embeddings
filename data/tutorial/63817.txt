   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

how to find wally with a neural network

a guide on how to train a model to solve where   s wally puzzles

   [16]go to the profile of tadej magajna
   [17]tadej magajna (button) blockedunblock (button) followfollowing
   dec 2, 2017
   [1*kkeiafrp-y9lqsabokeupa.gif]
   process of a neural network learning to find wally from start to finish

   deep learning provides yet another way to solve the where   s wally
   puzzle problem. but unlike traditional image processing id161
   methods, it works using only a handful of labelled examples that
   include the location of wally in an image.

     why look for wally if a neural network can do it for you?

   final trained model with evaluation images and detection scripts is
   published on my [18]github repo.

   this post describes the process of training a neural network using
   tensorflow id164 api and using a python script built around
   it to find wally. it consists of the following steps:
     * preparing the dataset by creating a set of labelled training images
       where labels represent x y locations of wally in an image
     * fetching and configuring the model to use with tensorflow object
       detection api
     * training the model on our dataset
     * testing the model on evaluation images using the exported graph

   before starting, make sure to install tensorflow id164 api
   as per [19]the instructions.

preparing the dataset

   while dealing with neural networks is the most notable process in deep
   learning, it sadly [20]turns out that the step data scientists spend
   the most time on is preparing and formatting training data.

   target values for the most simple machine learning problems are usually
   scalars (like for a digit detector) or a categorical string. tensorflow
   id164 api training data uses a combination of both. it
   consists of a set of images accompanied with labels of desired objects
   and locations of where they appear in an image. locations are defined
   with two points since (in 2d space) two points are enough to draw a
   bounding box around an object.

   so in order to create the training set, we need to come up with a set
   of where   s wally puzzle images with locations of where wally appears.

   while i could spend weeks solving wally puzzles by hand labelling
   images with annotation tools like [21]labelimg, i found an [22]already
   solved training set of where   s wally puzzles.
   [1*vlfjmbgrj-m73tornkpqsg.png]
   where   s wally training dataset with last four columns describing where
   wally appears in an image

   the final step of preparing our dataset involved packing our labels
   (saved as a .csv) and images (.jpeg) into a single binary .tfrecord
   file. the process of doing that is [23]explained here, but you can find
   both train and eval where   s wally .tfecord file on my [24]github repo.

preparing the model

   tensorflow id164 api provides a set of pre trained models
   with different performances (usually a speed-accuracy tradeoff) trained
   on several public datasets.

   while the model could be trained from scratch starting with randomly
   initialised network weights, this process would probably take weeks.
   instead, we used a method called id21.
   it involves taking a model usually trained to solve some general
   problem and retraining it to solve ours. the idea behind transfer
   learning is that instead of reinventing the wheel by training our model
   from scratch we can use the knowledge obtained in the pre trained model
   and transfer it to out new one. this saves us a lot of time so that the
   time spend for training can be invested into obtaining only the
   knowledge specific to our problem.

   we used the [25]rid98 with inception v2 model trained on the coco
   dataset along with [26]it   s pipeline configuration file. the model
   includes a checkpoint .ckpt file which we can use to start the
   training.

     after downloading the configuration file make sure to replace
        path_to_be_configured    fields with paths pointing to your
     checkpoint file, training and eval .tfrecord files and the labels
     map file.

   the final file that needs to be configured is the labels.txt map file
   which includes labels of all our different objects. since we   re only
   looking for one type of object, our labels file looks like this
item {
  id: 1
  name: 'waldo'
}

   finally, we should end up with:
     * a pretrained model with a .ckpt checkpoint file
     * training and evaluation .tfrecord dataset
     * label map file
     * pipeline configuration file pointing to the files above

   now, we   re ready to start training.

training

   tensorflow id164 api provides a simple-to-use python script
   to retrain our model locally. it is located in
   models/research/object_detection and can be ran with:

   python train.py --logtostderr --pipeline_config_path=
   path_to_pipeline_config --train_dir=path_to_train_dir

   where path_to_pipeline_config is the path to our pipeline config file
   and path_to_train_dir is a newly created directory where our new
   checkpoints and model will be stored.

   the output of train.py should look something like this:
   [1*yun-z_cxxl5apgstmrkbhq.png]

   with the most important information to look for being loss. it   s a
   summation of the errors made for each example in training or validation
   sets. you, of course, want it to be as low as possible, meaning that if
   it   s slowly decreasing, that means that your model is learning (   or
   overfitting your training data).

   you can also use tensorboard to display training data in more detail.

   the script will automatically store a checkpoint file after a certain
   number of steps, so that you can restore your saved checkpoints at any
   time in case your computer crashes while learning.

   this means that when you want to finish training the model, you can
   just terminate the script.

   but when to stop learning? the general rule as to when to stop training
   is when the loss on our evaluation set stops decreasing or is generally
   very low (below 0.01 in our example).

testing

   now we can actually use our model in practice by testing it on a few
   example images.

   first we need to export an id136 graph from the stored checkpoint
   (which is located in our train directory) using a script in
   models/research/object_detection :
python export_id136_graph.py     pipeline_config_path path_to_pipeline_config
--trained_checkpoint_prefix path_to_checpoint --output_directory output_path

   the exported id136 graph is now what our python script can use to
   find wally.

   i wrote a few simple python scripts (based on tensorflow object
   detection api) you can use to perform id164 on your models
   and draw boxes around detected objects or expose them.

   both [27]find_wally.py and [28]find_wally_pretty.py can be found in my
   [29]github repo and can simply be ran with
python find_wally.py

   [1*brmnx72p2kqbj5qmjmcepw.png]

   or
python find_wally_pretty.py

   [1*5t5sy-uwer2zraiuma6tea.png]

   when using scripts on your own model or own evaluation images, make
   sure to modify the model_path and image_path variables.
     __________________________________________________________________

final thoughts

   the model published on my [30]github repo performed surprisingly really
   well.

   it managed to find wally in the evaluation images and did decently on
   some extra random examples from the web. it failed to find wally where
   he was really large, which by intuition should be even easier to solve
   as opposed to finding him where he   s really small. this indicates that
   our model probably overfit our training data mostly as a result of
   using only a handful of training images.

   anyone looking to improve the model performance by hand labelling some
   extra images from the web, feel free the submit a pr on my [31]github
   repo and help improve the training set.

   [32]some rights reserved
     * [33]machine learning
     * [34]deep learning
     * [35]tensorflow
     * [36]data science
     * [37]towards data science

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of tadej magajna

[39]tadej magajna

   lead machine learning engineer at opencapacity, uk. more on
   [40]https://www.linkedin.com/in/tadej-magajna/

     (button) follow
   [41]towards data science

[42]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [43]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [44]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/eddbb20b0b90
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-find-wally-neural-network-eddbb20b0b90&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-find-wally-neural-network-eddbb20b0b90&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_7seknbksq0s0---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@tadejmagajna?source=post_header_lockup
  17. https://towardsdatascience.com/@tadejmagajna
  18. https://github.com/tadejmagajna/hereiswally
  19. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md
  20. https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/
  21. https://github.com/tzutalin/labelimg
  22. https://github.com/vc1492a/hey-waldo
  23. http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/
  24. https://github.com/tadejmagajna/hereiswally
  25. http://download.tensorflow.org/models/object_detection/faster_rid98_inception_v2_coco_2017_11_08.tar.gz
  26. https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_coco.config
  27. https://github.com/tadejmagajna/hereiswally/blob/master/find_wally.py
  28. https://github.com/tadejmagajna/hereiswally/blob/master/find_wally_pretty.py
  29. https://github.com/tadejmagajna/hereiswally
  30. https://github.com/tadejmagajna/hereiswally
  31. https://github.com/tadejmagajna/hereiswally
  32. http://creativecommons.org/licenses/by/4.0/
  33. https://towardsdatascience.com/tagged/machine-learning?source=post
  34. https://towardsdatascience.com/tagged/deep-learning?source=post
  35. https://towardsdatascience.com/tagged/tensorflow?source=post
  36. https://towardsdatascience.com/tagged/data-science?source=post
  37. https://towardsdatascience.com/tagged/towards-data-science?source=post
  38. https://towardsdatascience.com/@tadejmagajna?source=footer_card
  39. https://towardsdatascience.com/@tadejmagajna
  40. https://www.linkedin.com/in/tadej-magajna/
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/?source=footer_card
  43. https://towardsdatascience.com/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/eddbb20b0b90/share/twitter
  47. https://medium.com/p/eddbb20b0b90/share/facebook
  48. https://medium.com/p/eddbb20b0b90/share/twitter
  49. https://medium.com/p/eddbb20b0b90/share/facebook
