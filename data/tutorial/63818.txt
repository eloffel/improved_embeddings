      

     * microsoft
     * [1]home
     * [2]updates
     * [3]docs
     * [4]sourcecode
     * [5]support

   distributed machine learning toolkit
   big data, big model, flexibility, efficiency
   [6]fork me on github

   [pic1_v7.jpg]

distributed machine learning toolkit [7]#

   distributed machine learning has become more important than ever in
   this big data era. especially in recent years, practices have
   demonstrated the trend that more training data and bigger models tend
   to generate better accuracies in various applications. however, it
   remains a challenge for common machine learning researchers and
   practitioners to learn big models from huge amount of data, because the
   task usually requires a large number of computation resources. in order
   to tackle this challenge, we release the microsoft distributed machine
   learning toolkit (dmtk), which contains both algorithmic and system
   innovations. these innovations make machine learning tasks on big data
   highly scalable, efficient, and flexible. we will continue to add new
   algorithms to dmtk in a regular basis.

   the current version of dmtk includes the following components (more
   components will be added to the future versions):

       dmtk framework: a flexible framework that supports unified interface
   for data parallelization, hybrid data structure for big model storage,
   model scheduling for big model training, and automatic pipelining for
   high training efficiency.

       lightlda, an extremely fast and scalable topic model algorithm, with
   a o(1) gibbs sampler and an efficient distributed implementation.

       distributed (multisense) id27, a distributed version of
   (multi-sense) id27 algorithm.

       lightgbm: a very high-performance gradient boosting tree framework
   (supporting gbdt, gbrt, gbm, and mart), and its distributed
   implementation.

   machine learning researchers and practitioners can also build their own
   distributed machine learning algorithms on top of our framework with
   small modifications to their existing single-machine algorithms.

   we believe that in order to push the frontier of distributed machine
   learning, we need the collective effort from the entire community, and
   need the organic combination of both machine learning innovations and
   system innovations. this belief strongly motivates us to open source
   the dmtk project.
   [pic2_v2.jpg]

dmtk framework [8]#

   this framework includes the following components:

       a parameter server that supports

   o hybrid data structure for model storage, which uses separate data
   structures for high- and low-frequency parameters, and thus achieves
   outstanding balance between memory capacity and access speed.

   o acceptance and aggregation of updates from local workers, which
   supports several different mechanisms for model sync-up, including bsp,
   asp, ssp, in a unified manner.

       a client sdk that supports

   o pipeline between local training and model communication, which
   enables very high training speed regardless of various conditions of
   computational resources and network connections.

   o scheduling big model training in a round-robin fashion, which allows
   each worker machine to pull the sub-models as needed from the parameter
   server, resulting in a frugal use of limited memory capacity and
   network bandwidth to support very big models.

   o lua and python bindings, which can enable users in various
   communities to leverage our parameter server to parallelize their
   machine learning tasks.
   [pic3_v2.jpg]

lightlda [9]#

   lightlda is a new, highly-efficient o(1) metropolis-hastings sampling
   algorithm, whose running cost is (surprisingly) agnostic of model size,
   and empirically converges nearly an order of magnitude more quickly
   than current state-of-the-art gibbs samplers.

   in the distributed implementation, we leverage the hybrid data
   structure, model scheduling, and automatic pipelining functions
   provided by the dmtk framework, so as to make lightlda capable for
   extremely big data and big model even on a modest computer cluster. in
   particular, on a cluster of as few as 8 machines, we can train a topic
   model with 1 million topics and a 10-million-word vocabulary (for a
   total of 10 trillion parameters), on a document collection with over
   100-billion tokens - a scale not yet reported even with thousands of
   machines in the literature.
   [pic4_v2.jpg]

distributed (multi-sense) id27 [10]#

   id27 has become a very popular tool to compute semantic
   representation of words, which can serve as high-quality word features
   for natural language processing tasks. we provide the distributed
   implementations of two id27 algorithms. one is the standard
   id97 algorithm, and the other is a multi-sense id27
   algorithm that learns multiple embedding vectors for polysemous words.
   by leveraging the model scheduling and automatic pipelining functions
   provided by the dmtk framework, we are able to train 300-d word
   embedding vectors for a 10-million-word vocabulary, on a document
   collection with over 100-billion tokens, on a cluster of 8 machines.
   [pic5_v2.jpg]

lightgbm [11]#

   lightgbm is a new gradient boosting tree framework, which is highly
   efficient and scalable and can support many different algorithms
   including gbdt, gbrt, gbm, and mart. lightgbm is evidenced to be
   several times faster than existing implementations of gradient boosting
   trees, due to its fully greedy tree-growth method and histogram-based
   memory and computation optimization. it also has a complete solution
   for distributed training, based on the dmtk framework. the distributed
   version of lightgbm takes only one or two hours to finish the training
   of a ctr predictor on the criteo dataset, which contains 1.7 billion
   records with 67 features, on a cluster of 16 machines.

   a special note: dmtk is a platform deigned for distributed machine
   learning. deep learning is not our focus, and the algorithms released
   in dmtk are mostly non-deep learning algorithms. if you want to use
   state-of-the-art deep learning tools, you are highly recommended to use
   [12]microsoft cntk. we have close collaborations with cntk and provide
   support to its asynchronous parallel training functionalities.
   [13]contact us [14]privacy and cookies [15]terms of use [16]trademarks
     2015 microsoft [17]microsoft

references

   1. http://www.dmtk.io/./index.html
   2. https://github.com/microsoft/dmtk/wiki/news
   3. https://github.com/microsoft/dmtk/wiki
   4. https://github.com/microsoft/dmtk
   5. http://www.dmtk.io/./support.html
   6. https://github.com/microsoft/dmtk
   7. https://github.com/microsoft/dmtk
   8. https://github.com/microsoft/multiverso/tree/master
   9. https://github.com/microsoft/lightlda/tree/master
  10. https://github.com/microsoft/multiverso/tree/master/applications/wordembedding
  11. https://github.com/microsoft/lightgbm/tree/master
  12. http://cntk.ai/
  13. http://www.dmtk.io/./support.html
  14. http://go.microsoft.com/fwlink/?linkid=286687
  15. http://go.microsoft.com/fwlink/?linkid=386394
  16. http://go.microsoft.com/fwlink/?linkid=506942
  17. http://research.microsoft.com/c/1456
