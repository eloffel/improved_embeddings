   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

id100 and id23

   aug 2, 2015

   in this post, we   ll review id100 and reinforcement
   learning. this material is from chapters 17 and 21 in russell and
   norvig (2010).

id100

   the general idea with this situation is that we are an agent in some
   world, and we have a set of states, actions (for each state), a
   transition id203 (which we may not actually know), and a reward
   function. the goal for a rational agent is to maximize the expected sum
   of (discounted) rewards for a state , where is our discount factor,
   which we usually assume is equal to one for toy cases only.

   this is the setup of a markov decision process (mdp), with the added
   constraint that the id203 of going to another state only depends
   on the current state, which is why we call this a markov decision
   process. since the ultimate goal is to maximize the expected utility,
   we will need to learn a policy function that maps states to actions.
   finally, while not strictly necessary, it is common to    spice up    the
   mdp problem by assuming that actions are non-deterministic. in the
   canonical grid-world example described in the book (and in a lot of
   undergraduate ai classes, for that matter), we assume if we move north,
   then that has an 80 percent chance of succeeding. hence, the transition
   id203 is nontrivial, where and are states, and is the action we
   took at state .

   in order to understand how to get an optimal policy, we first need to
   realize how exactly to define the value of a state, which in other
   words, is the expected utility we will get if we are at state and play
   optimally. (the asterisk here is to indicate optimality.) there are,
   sadly, two common formulations for . the first, from r & n, assumes
   that the reward aspect of the formula is defined in terms of a state
   only:

   the second formulation, which berkeley   s cs 188 class uses^[7]1,
   defines reward in terms of a state-action-successor triple:

   both of these equation sets can be called the bellman equations, which
   characterize the optimal values (but we will generally need some other
   way of computing them, as we show shortly). in general, i will utilize
   the second formulation, but the formulations are not fundamentally
   different.

   it is also common to define a new quantity called a q-value with
   respect to state-action pairs:

   in words, is the expected utility starting at state , taking action ,
   and hereafter, playing optimally. we can therefore relate the s and s
   with the following equation:

   i find it easiest to think of these in terms of expectimax trees with
   chance nodes. the    normal    nodes correspond to s, and the    chance   
   nodes correspond to s. both nodes have multiple successors: the s
   because we have choices of actions, and s because, even if we commit to
   an action, the actual outcome is generally non-deterministic, so we
   will not know what state we end up in. (i guess we can also view states
   as the    normal    nodes here.)

   given these definitions, how do we figure out the optimal policy? there
   are two common tactics:
     * value iteration is an iterative algorithm that computes values of
       states indexed by a , as in , which can also be thought of the best
       value of state assuming the game ends in time-steps. this is not
       the actual policy itself, but these values are used to determine
       the optimal policy^[8]2.
       starting with for all , value iteration performs the following
       update:
       and it repeats until we tell it to stop.
       to make this intuitive, during each step, we have a vector of
       values, and then we do a one-ply expectimax computation to get the
       next vector. note that expectimax could compute all of the values
       we need, but it would take too long due to repeated and infinite
       depth state trees.
       to prove value iteration converges to (and uniquely!), appeal to
       contraction and the fact that , so long as , will go down to zero.
       said another way, the     tree    and the     tree    are different only in
       their last layer, but that last layer   s contribution is reduced
       exponentially.
     * policy iteration is generally an improvement over value iteration
       because policies often converge long before the values do, so we
       alternate between policy evaluation and policy improvement steps.
       in the first step, we assume we are given a policy and have to
       figure out the expected utilities of each state when executing .
       these values are characterized by the following equations:
       note that the max operator is gone because gives us our action.
       thus, we can solve these equations in time with standard matrix
       multiplication methods^[9]3.
       it is actually rather easy to do policy improvement from q-values:
       alternatively, we can use the more complicated expectimax form:
       the two steps would iterate until convergence, and like value
       iteration, policy iteration is optimal. also, these equations are
       really policy extraction, in the sense that given values, we know
       how to get a policy.

   in general, policy iteration seems to be a slightly better bet than
   value iteration, though i guess the latter could be simpler to
   implement in some cases since it only involves the value updating part?

   if we do not know exactly what state we are in, we do not have a notion
   of here, so we need to change our representation of the problem. this
   is the partially observable markov decision process (pomdp) case. we
   augment the mdp with a sensor model and treat states as belief states.
   in a discrete mdp with states, the belief state vector would be an
   -dimensional vector with components representing the probabilities of
   being in a particular state. the belief state update for a particular
   component (state) looks a lot like an id48 update:

   the cycle is: we compute an action from , see evidence , compute a new
   belief state (using the above formula), then repeat. fortunately, the
   optimal action for a pomdp only depends on the current belief state,
   allowing us to have our policy be a mapping from a belief state to an
   action.

   we can also map pomdps to an observable mdp formula, by appropriately
   defining transition and reward functions. for instance, . the optimal
   policy for this new mdp over the belief states is the same optimal
   policy over the original pomdp. this is good, since we   ve reduced this
   problem to an mdp, but we have to modify our earlier algorithms to
   handle continuous-valued state spaces. a key observation that will
   help: the value function over belief states is piecewise linear and
   convex, because it is the maximum of a collection of hyperplanes. what
   a modified value iteration algorithm needs to do is to accumulate a set
   of possible plans, and for a given belief state, compute which of those
   has the optimal hyperplane. but this is too slow so in practice, people
   use online algorithms based on one-step lookahead, inspired by dynamic
   id110s.

   to understand pomdps well, consider simple mdps with two states, so we
   can represent as a scalar.

id23

   now we switch to the id23 case. before diving into
   the details, it is easiest to think of us in the same mdp framework,
   except that we have to learn our policy online, not offline. this is
   harder because we typically assume we do not know the transition
   probabilities ahead of time, or even the rewards. unfortunately, this
   means we will try out different paths, and will sometimes fail (e.g.,
   falling into the -1 pit in the gridworld example).

   first, consider the passive reinforcement case, where we are given a
   fixed (possibly garbage) policy and the only goal is to learn the
   values at each state, according to the bellman equations.

   here   s one way to get the values: learn the transitions and the rewards
   through trials, then immediately apply the policy evaluation step in
   policy iteration. as a reminder, this computes values (as in value
   iteration) but since the policy is fixed, we can solve the set of
   equations quickly. this technique, which russell and norvig seem to
   call adaptive id145, is part of the class of model-based
   learning techniques, because that assumes we have to compute a model of
   the world (i.e., the transitions and rewards). here, our agent could
   play a series of actions from a variety of states (   restarting    when
   necessary). at the end, it will have data about the rewards it   s
   gotten, along with the number of times it transitioned from state to
   state . to compute the transition function, we would convert the counts
   to the averages in a    maximum likelihood estimate   -manner. the rewards
   are easier because we can see immediately when we transition from to .

   in model-free learning, we do not bother to compute the transition
   model, but instead only compute the actual values of each state. there
   is a basic technique called direct utility estimation that falls in
   this category of methods. the idea here is that for each trial, we need
   to record the sum of discounted rewards for each state visited. after a
   series of trials, these values will average out and converge, albeit
   slowly since direct utility estimation does not take into account the
   bellman equations.

   we need a better solution. since we already have a policy, we would
   like to do some form of policy evaluation (explained earlier), but we
   do not have the transitions and rewards. we can approximate those with
   our samples instead, which leads to temporal difference learning. this
   is a model-free tactic where, after each transition , we update the
   values:

   in short, td learning works by adjusting the utility estimates towards
   the ideal equilibrium as stated by the bellman equations. but what if
   we want to get a new policy? td learning does not learn the transition
   probabilities, so we switch to learning q-values, since it is easier to
   extract actions from q-values.

   this now brings us to active id23, where we have to
   learn an optimal policy by choosing actions. clearly, there will be
   some tradeoffs between exploration and exploitation.

   the solution here is an algorithm called id24, which iteratively
   computes q-values:

   notice how the sample here is slightly different than in td learning.
   each sample is computed from a tuple.

   the amazing thing is, so long as certain obvious assumptions hold, such
   as that we visit states sufficiently often, id24 will converge to
   an optimal solution even if the actions we take are repeatedly
   suboptimal^[10]4! in the fire pit example in the cs 188 class, we could
   keep jumping in the pit, but so long as our actions are somewhat
   stochastic, we will eventually have some paths that end up in the lone
   beneficial spot, which will result in higher q-values for those states
   and actions that lead towards that spot. (think of each grid as having
   four q-values.)

   even so, it would probably be wise to have good heuristics for deciding
   on what actions to take, which might make id24 converge faster.
   the simplest (and worst) strategy: act according to a current policy,
   but with some small id203, we choose a random action. to improve
   this, use exploration functions that weigh the importance of actions
   and states based on whether their values are    established    or not. this
   information propagates back so that we end up favoring actions that
   lead to unexplored areas. this means for some exploration function ,
   the action we pick at a step will be where represents counts, and the
   new id24 update is:

   there is a close relative to id24 called sarsa,
   state-action-reward-state-action. the update is, following russell and
   norvig   s notation to have rewards be fixed to a state:

   what this means is that we already have an action chosen for the
   successor state . id24 will choose the best action , but in
   sarsa, it is fixed, and then we can update . sarsa and id24 (the
   first version here, not the second version with ) are the same for a
   greedy agent always picking the best action. when exploration happens,
   id24 will attempt to pick the best action, but sarsa picks
   whatever is actually going to happen. this means id24 does not
   pay attention to the policy, i.e., it is off-policy, but sarsa does, so
   it is on-policy. here   s an example i thought of to clarify: the fire
   pit in cs 188 means that a policy that tells us to go in the pit will
   be bad, but if transitions are stochastic, then at some point, we will
   get to the good exit, hence id24 will eventually learn that best
   value. but with sarsa, we do not get that opportunity because we will
   always have the next action fixed, so it is    more realistic    in some
   sense.

   it is worth pointing out that all this previous discussion about
   learning all the values is really applicable only for the smallest of
   problems, because we cannot tabulate all those in real-life situations.
   thus, we resort to approximate id24. here, we use feature-based
   representations by representing a state as a vector of features:

   and the same for the q-values:

   we usually make things linear in terms of the features for simplicity.
   this has the additional benefit in that we can generalize to states we
   have not visited yet (in addition to the obvious compression benefits).
   note that when we do the id24 update here, we change the weights.
   each weight update looks like the same kind of update we see in
   stochastic id119.

   finally, the last major id23 topic is policy search.
   it actually seems like the policy iteration analogue: we only care
   about the policy, not the actual values themselves. starting with an
   initial feature-based representation of the q-states, we might consider
   tweaking the individual weights up and down and then evaluating the
   result of that representation to see if our rewards are higher. this
   may involve computing an expression for the expected reward, or running
   an agent in the world multiple times.

old cs 188 exam questions

   i found four interesting questions related to mdps and reinforcement
   learning.
     * [11]spring 2011, question 4 (   worst-case markov decision
       processes   ). now we measure the quality of a policy by its
       worst-case utility, or in other words, what we are guaranteed to
       achieve. this question is really intellectually simulating, as we
       have to consider the algorithm from a minimax perspective, rather
       than an expectimax perspective.
       for a state, we have:
       so we pick the action that maximizes the following quantity: it is
       the discounted reward based on the worst possible state to which we
       could transition.
       there is a corresponding q-value version:
       we are guaranteed , but have already committed to action , so the
       adversary is free to send us to the worst possible subsequent state
       for us (as long as that transition has non-zero id203, of
       course), but then we are free to maximize over the next action.
       if we want to do -learning, then it is actually easier than
       standard -learning because the environment has deterministic
       rewards and we only care about the minimax one, so if we ever
       observe a reward, we know that it   s the correct one.
     * [12]fall 2011, question 4 (an un-named question). this one required
       us to actually perform some computations of value iteration and
       id24. the only non-trivial thing about this was the jump from
       c, which could land in either d or e. i personally found it easiest
       to just run value iteration and try to reason about the q-values
       later. the id24 iterative update is:
       for the third part, the tricky thing was to remember that the
       id24 update will involve max-ing over previously computed
       q-values.
     * [13]fall 2012, question 6 (   id23   ). this question
       makes clear the distinction between id24 and sarsa. for part
       (a), they give us the formula for the id24 agent, and ask if
       this will converge to the optimal set of q-values even if the
       policy we   re following is suboptimal. the answer is yes, since they
       also mention in the question several necessary requirements (e.g.,
       that we visit all state-action pairs infinitely often). for part
       (b), the sarsa update, answer is no, we do not converge to the
       optimal q-values, because sarsa will have q-values for whatever
       policy is actually being executed. this policy that we converge to
       is the policy that follows with id203 0.5, and uniformly
       random otherwise.
     * [14]fall 2012, question 7 (   bandits   ). this is a rather long
       question. part (a) gives a nice review of meu and vpi, and the
       remaining parts now deal with pomdps over the two states of having
       bandits versus not having bandits. hence, we represent the belief
       state as a single scalar representing the id203 we believe
       there are bandits. when we have to compute in the tree, we use the
       following formula:
       notice that this is precisely the formula for updating the belief
       state that we discussed earlier in the notes, except that we have
       the transition function as the identity. by the way, it took me a
       while to figure this out, but the normalizing constant here needs
       to iterate through the set of belief states. we have a scalar , but
       we really have to consider the case of there possibly being no
       bandits at all. a bit tricky here. the transition id203 is
       also tricky, so it   s easiest to think of it in terms of the
       problem: what is the id203 of even reaching this node?
       finally, they discuss some utility diagrams. as we discussed
       earlier, to find the overall value function, we take the maximum
       over all the possible lines. for the discounting questions, part
       (i) the point was that agents with larger values should be more
       concerned about the future, so they are more willing to try out
       short-term suboptimal paths. thus, agent a has a higher discount
       factor because it played the sub-optimal land action one more time.
       in part (ii), there are three plots you can easily remove. then
       it   s just assigning three plots to three discount factors. a higher
       discount factor means we need to be more certain that we have
       bandits before we commit to making air shipments.
     __________________________________________________________________

    1. here is where i get confused. why are we having berkeley   s ai class
       use a different formulation than what r & n are using? i mean,
       stuart russell is a faculty member here, so why are we choosing the
       other way? for pedagogical purposes? [15]   
    2. in r & n   s notation, we would use . [16]   
    3. alternatively, if that step is still too expensive, one can perform
       iterative updates as we would in value iteration for some number of
       steps, then switch to policy improvement. in this case, we would
       add s as subscripts. [17]   
    4. but we do need to visit states often enough. in section 21.3 of r &
       n, they make it clear that at each time step (i.e., after each
       sample of state-action-rewards-successors), we could compute a
       model with whatever information we have, and then follow the
       optimal policy for that. but such a greedy agent is not good
       because it will never try out different states whose value may not
       be established. [18]   

   please enable javascript to view the [19]comments powered by disqus.

seita's place

     * seita's place
     * [20]seita@cs.berkeley.edu

     * [21]danieltakeshi
     * [22](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fn:confused
   8. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fn:russell
   9. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fn:alternatively
  10. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fn:visitoften
  11. https://s3-us-west-2.amazonaws.com/cs188websitecontent/exams/sp11_final.pdf
  12. https://s3-us-west-2.amazonaws.com/cs188websitecontent/exams/fa11_final.pdf
  13. https://s3-us-west-2.amazonaws.com/cs188websitecontent/exams/fa12_final.pdf
  14. https://s3-us-west-2.amazonaws.com/cs188websitecontent/exams/fa12_final.pdf
  15. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fnref:confused
  16. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fnref:russell
  17. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fnref:alternatively
  18. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/#fnref:visitoften
  19. https://disqus.com/?ref_noscript
  20. mailto:seita@cs.berkeley.edu
  21. https://github.com/danieltakeshi
  22. https://twitter.com/(never!)

   hidden links:
  24. https://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/
