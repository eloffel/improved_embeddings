   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

stanford's id202 review

   jul 26, 2015

   i   ll take a break from the recent discussion on id114 to go
   over some old id202 concepts. while there are many references
   on id202 designed to help a student re-learn what he or she
   may have forgotten, i found the [7]handout from stanford   s cs 229
   (machine learning) class to be one of the best when considering the
   content, size, and clarity together. and despite being advertised as a
   review, it actually did teach me a lot. in college, i only took one
   id202 class, a basic introductory and proofs-based
   course^[8]1. while i found it difficult when i was taking it, in
   retrospect, we did not go over that much material. two things that
   really tricked me up when i first arrived in berkeley were
     * positive definite matrices (and related to that, singular value
       decomposition)
     * matrix calculus (with gradients, hessians, etc.)

   these two concepts are used all the time in ai, and i really wish i
   could know them better. thus, in this post, i   ll quickly go over some
   of the basic review concepts of id202 as presented in the
   stanford handout, and then spend a little more time on those previously
   mentioned topics.

basic concepts and id127

   a few years ago, i kept getting confused between column and row vectors
   when i was reading machine learning literature, but now i know that all
   vectors are assumed to be columns by default. also, when denoting row
   vectors, be careful that the transpose sign does not necessarily mean
   we are taking the corresponding column and then transposing it (as the
   notes say, the definitions of and are ambiguous).

   id127 is at the heart of id202 and what we do
   in ai. i find it easiest to reason about id127 by
   always checking the number of rows and columns of the matrices
   involved, and making sure they align correctly. but this handout made
   it easier for me to think of other ways to view how multiplication
   works. they describe four interesting ways of looking at matrix
   multiplication, of which the most interesting to me is when the product
   is expressed as the sum of outer products (between vectors).

operations and properties

   some interesting operations and properties of matrices:
     * any square matrix can be represented as a sum of a symmetric matrix
       and an anti-symmetric matrix, since .
     * the trace is the sum of the diagonal elements of a matrix. despite
       its seeming simplicity, the trace operator will actually be very
       useful in matrix calculus (the    trace trick   ) and in the study of
       eigenvalues and eigenvectors, since the sum of the eigenvalues of a
       matrix equals the trace, even though the individual components
       being summed up generally have no relation.
     * the norm of a vector is like an informal measure of the distance.
       for some reason, these kept confusing me when i was trying to work
       on my undergraduate thesis, so it   s nice to see it formally treated
       here. there are several properties of norms, but in general, i tend
       to view norms of vectors as just the norm. also, when working with
       norms, make sure i know the cauchy-schwarz inequality: .
     * the rank of a matrix is equal to the number of its linearly
       independent columns (or rows). there are many implications of the
       rank; for me, the one i remember the best is that in order for an
       inverse to exist, a (square) matrix has to be full rank. if we
       generated a square matrix by filling in its values from a random
       number generator, then it will almost always have full rank.
     * a square matrix is orthogonal if . this requires the columns of to
       be orthonormal^[9]2. a nice property: , which we can prove by
       squaring them, thus considering and .
     * the range, i.e., column space, of a matrix is . the null space is
       ^[10]3. vectors in the former set have length , those in the latter
       have length , and we can get the complement of each of those sets
       by considering and . for instance, considering the sets and , it
       turns out that the intersection of those two sets is empty, and
       that every vector in can be expressed as the sum of two vectors,
       one in each of those two sets. thus, they are orthogonal
       complements. the two column spaces, combined with the two column
       spaces (using and ) form the four fundamental subspaces, a term
       popularized by gilbert strang. i   m not sure i really get it,
       though.
     * the determinant of a square matrix , denoted as or , is a
       mysterious function that maps to the real numbers. the three main
       properties it satisfies are:
          + that the identity has determinant one
          + that if we multiply a single row in by a scalar , the
            determinant of the new matrix is
          + that if we exchange any two (different) rows, then the
            determinant of the new matrix is
       all the other properties of determinants follow from this,
       including the cofactor expansion, which i will not list here.
       determinants have an intuitive interpretation in terms of volume:
       if we take the span of the rows of , and restrict all the
       coefficients to be in , then the determinant is the volume of this
       set. also, determinants are useful for checking if exists. finally,
       the adjoint of a matrix, , is .
     * given a square, symmetric matrix , a quadratic form is a function
       mapping and to the reals. our variables are , so even if the
       dimensions of are large, the highest power of any (i.e., a
       component of ) that can exist in is two, hence the intuitive name.
       something that is related is the concept of a positive semidefinite
       matrix, which is a symmetric matrix such that for all . we can also
       define the obvious analogues for definite, negative definite, etc.,
       matrices. given any, not necessarily square matrix , the matrix is
       always positive semidefinite; prove this by using .
       note: quadratic forms like these are the start of an introduction
       to what really happens in machine learning and ai, where we have to
       understand matrix representations.
     * for determining eigenvalues and their corresponding eigenvectors
       (note: eigenvectors are not unique), refer to if we are going to be
       solving them by hand.
       we tend to use (capital lambda) to represent the matrix of
       eigenvalues, so the matrix equation will result in , implying that
       , hence is diagonalizable.
       some interesting properties: the trace and determinant are the sum
       and product, respectively, of the eigenvalues. the eigenvalues of a
       triangular matrix are just the elements on the diagonal. if is
       nonsingular, then is an eigenvalue of with eigenvector .
       if we have a symmetric matrix , then (1) all eigenvalues are real,
       and (2) its eigenvectors can be scaled to be orthogonal to each
       other, so that above can turn orthogonal, and hence we have , a
       good thing because transposes are easier computationally than
       inverses. unfortunately, i don   t currently know how to prove these
       off the top of my head.

singular value decomposition

   with a symmetric matrix , we can set instead of using . it turns out we
   can generalize this to arbitrary (not even square!) matrices by
   expressing them in singular value decomposition form, . the and
   matrices are square and orthogonal, and represent eigenvectors of and ,
   respectively.

   the matrix is diagonal, possibly non-square, and contains something
   called the singular values (not the eigenvalues!) of matrix . they fill
   the first diagonal elements for a rank matrix , and the rest of the
   elements are zero. the singular values are the square roots of the
   nonzero eigenvalues of both and (which are symmetric, so they have real
   eigenvalues).

   for a positive definite matrix, .

   there are many applications of svd, and the ones i   m familiar with
   happen to be in id161. some of these applications have to do
   with the fact that and give orthonormal bases for all four fundamental
   subspaces. for instance, to use svd to compute the null space of , then
   look at the last columns of .

matrix calculus

   i really wish i had completely digested this section before taking cs
   281a here because i had no idea how matrix calculus worked until i
   reviewed this document. given a function , the gradient of with respect
   to is a matrix (i.e., it   s the same size as the original matrix input
   to ) of partial derivatives of .

   the hessian is the second derivative analogue to the gradient (well,
   almost). here, we assume we are taking the hessian with respect to a
   function , where we take a vector as input (not a general matrix, even
   though that   s possible).

the trace trick

   here   s a neat trick i learned from reading mike jordan   s notes. since
   the trace is such that is the same no matter what the ordering of the
   three matrices, and since (verify this by explicit computation) then we
   can claim:

   intuitively, this makes sense since we just eliminated . in the past,
   when i tried computing derivatives like these, i would convert the
   entire expression to an enormous set of summations, and try to reason
   element by element. that is way too much work.

   there is a related trick involving determinants: the derivative of with
   respect to is .

   one can use these tactics to take the id113 of the covariance matrix of a
   multivariate gaussian distribution, since the log likelihood of that
   term has and in it.
     __________________________________________________________________

    1. when i took the course (with professor elizabeth beazley), it was
       called math 211. now it   s called math 250 since the department put
       the core courses in the    50   s with their much-improved numbering
       system. [11]   
    2. watch out with the terminology! some people might call such
       matrices as orthonormal matrices, but i call them orthogonal. [12]   
    3. this is technically called the right null space, or the kernel,
       since we may wish to refer to the left null space, which uses
       . [13]   

   please enable javascript to view the [14]comments powered by disqus.

seita's place

     * seita's place
     * [15]seita@cs.berkeley.edu

     * [16]danieltakeshi
     * [17](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. http://cs229.stanford.edu/section/cs229-linalg.pdf
   8. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fn:williams
   9. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fn:terminology
  10. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fn:right
  11. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fnref:williams
  12. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fnref:terminology
  13. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/#fnref:right
  14. https://disqus.com/?ref_noscript
  15. mailto:seita@cs.berkeley.edu
  16. https://github.com/danieltakeshi
  17. https://twitter.com/(never!)

   hidden links:
  19. https://danieltakeshi.github.io/2015-07-26-stanfords-linear-algebra-review/
