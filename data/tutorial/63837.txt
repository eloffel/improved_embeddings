   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

id88s, id166s, and kernel methods

   aug 8, 2015

   in this post, we   ll discuss the id88 and the support vector
   machine (id166) classifiers, which are both error-driven methods that
   make direct use of training data to adjust the classification boundary.
   they do not    build a model,    which is what a bayesnet-based algorithm
   such as naive bayes would do, which means we can make fewer assumptions
   about the data.

   we   ll also talk about kernels, which allow us to efficiently compute
   dot products of high-dimensional feature vectors without actually
   computing those feature vectors.

the id88

   the id88 learning algorithm relies on classification via the sign
   of the dot product. given a binary classification problem of vectors in
   , the id88 algorithm computes one parameter vector . given an
   arbitrary sample with features^[7]1 , we classify this as +1 if and -1
   if otherwise. assuming we   re doing supervised data, we will know the
   true label . if , then we don   t do anything. otherwise, we must adjust
   the weight vector . this will change the direction of the vector, thus
   shifting the classification boundary. it   s easiest to understand how
   this works by realizing that represents the decision boundary, which is
   orthogonal to by definition of the dot product and divides up the
   feature vector space into    halves,    where one has dot products with
   positive, and the other negative.

   in the general case, there will be multiple classes, so we will have
   multiple weight vectors for a -way classification problem. in that
   case, whenever we have a training instance , we assign the class based
   on . if was actually in class , we are done; otherwise it should have
   been in class so we need to adjust two weight vectors with and . we add
   to the appropriate class, and subtract from the wrong class.

   what are the problems with the id88 as we just described? well,
   if the data isn   t linearly separable, the algorithm will    thrash   
   around and never converge^[8]2. two other (related) problems: it can
   overfit the data, or not find a suitable boundary. for the latter case,
   think of a linearly separable data, but with one outlier that causes
   the linear boundary to drastically shift. it may be wise to allow one
      error    in order to get a that generalizes better.

   there is a modification of the id88 known as the margin-infused
   relaxed algorithm (mira), which updates in the same direction as the
   id88, but at the minimum magnitude necessary (technically, we add
   one to leave some slack, but whatever) to force the classifier to
   classify the current sample correctly (if it was not already correct).
   this means that the update could be smaller or greater than the
   id88 update, but unlike the id88, mira will always classify
   an example correctly after seeing it. in practice, we cap the amount
   that a single training example can change the weight vector, so the
   scale factor is at most a pre-specified .

   as an alternative to the multiway classification id88, one can
   use the id88 for ranking (e.g., website ranking), which has only
   one weight vector. it   s useful if we want to consider data points and
   classes together in a single vector . the decision rule is

   and the update rule is

   where for a data point , was the predicted class but was the actual
   class. now the weights are interpreted as the importance of each
   feature component to each class.

the kernelized id88

   we can create more complicated classification boundaries with
   id88s by using kernelization^[9]3. suppose starts off as the zero
   vector. then we notice in the general -way classification problem that
   we only add or subtract vectors to . in other words, with samples in
   the training data, where all the variables are integers. this means
   learning all the alphas would be enough to reconstruct the weight
   vectors.

   how do we make a classification decision? for a given training instance
   (or even an entirely new sample) , we would assign it the class based
   on whatever (for weight vector ) that maximizes the following: . we can
   re-express the dot product: , where we have introduced a kernel
   function . kernels allow us to    map    vectors and into a higher
   dimensional space, where we would then    take the dot product,    without
   actually transforming the features into the higher dimensional space.

   here   s an example: if we let , then we have mapped and into a higher
   dimensional space that includes squared components of and , resulting
   in linearly separable boundaries in that space even if the original
   feature space was not, e.g., the positive examples formed a circle and
   were surrounded by the negative examples. as a general rule, the more
   features we have, the more likely we have linearly separable data,
   unless two of the exact same    s have different classes, for whatever
   reason. of course, we will need more examples to learn correctly
   (growth is roughly quadratic in the number of features), and when doing
   classification, we will need to compute all the values. it will be
   further slower if most of the alpha counts are nonzero.

   there are two popular classes of kernels:
     * the polynomial kernel has the form for degree . for vectors of
       dimension , this kernel will map them to an -dimensional space!
       expanding the kernel out for the simple case of , we get
       this is the equivalent of a dot product of features that contain
       elements , , and (not     watch out!).
     * the gaussian kernel, also known as the radial-basis function (rbf)
       kernel maps elements into an infinite-dimensional feature space. it
       is . probably more than any other kernel, classifying with this one
       is a lot like nearest neighbor because it clearly measures a
       similarity function, weighing    closer    examples more in our
       classification decisions. as , the kernel becomes a lookup table,
       and our training accuracy for a id88 trained with this is 100
       percent (except in the weird case of two exact same points getting
       different labels) but our validation and test set accuracy will be
       horrible.

   to test my understanding of kernels in more detail, i looked at (as
   usual) an old cs 188 handout. it had the following image:

   kernels

   (in the first plot, the dotted line is .)

   let   s consider a linear, a shifted linear, a quadratic, and cubic
   kernels (see the handout for details on these), and see if any of them
   can linearly separate the data in the two plots.

   plot (a) requires a third-order polynomial to separate the data, so
   only the cubic kernel will work, because that will map feature vectors
   to have in it. then we   d just adjust the weights to set that to have
   nonzero weight.

   in plot (b), a linear kernel is enough, but there has to be a bias term
   in there! (that actually tricked me.) without a bias, in the 2-d case
   here, the decision rule is , and a 2-d vector must    emanate    out of the
   origin, which means the perpendicular line to it crosses the origin.

kernels, formalized

   the preceding discussion motivates the following question: how do we
   know if a function is a valid kernel? first, the official definition of
   a kernel is that it is a function that performs an inner product in a
   hilbert space. normally, i prefer thinking of the inner product as the
   normal dot product (as i wrote earlier) but more generally, we should
   use the terminology of the inner product. those satisfy properties of
   symmetry, bilinearity, and positive definiteness. a hilbert space is an
   inner product space that is complete and separable with respect to the
   norm defined by the inner product.

   since we are only dealing with real-valued vectors, our hilbert space
   will be and the inner product here is the normal vector dot product. to
   test whether a function is a kernel, we invoke a simplified form of
   mercer   s theorem: let function be given. then for to be a valid mercer
   kernel, it is necessary and sufficient that for any set of points the
   corresponding kernel matrix is (symmetric) positive semi-definite. the
   element of the kernel matrix is the value . (sometimes the kernel
   matrix is called the gram matrix.)

   to prove one direction, that if is a kernel matrix corresponding to a
   feature mapping , it must be symmetric positive semi-definite, we
   proceed as follows. first, it   s going to be symmetric due to the dot
   product (or more accurately, inner product) being commutative. next,
   for any , we have

   where we used the fact we indirectly showed earlier that . it   s a
   little tricky because we are keeping , a component of , fixed, and
   ranging across different vectors.

   this fact about positive semi-definiteness makes it easy to see that
   the following are also valid kernels:
     * addition:
     * multiplication:
     * scalar: for a constant

   we can use kernels for id88s (as previously discussed), support
   vector machines (as we will discuss), principal components analysis,
   and other classifiers such as id75.

   let   s discuss the id75 case. in the general (regularized)
   case, the objective is:

   where is the matrix of training instances, where each training instance
   is a row (this is different from what i usually think of it, but it
   makes more sense in regression). the vector has the true labels. by
   taking derivatives, we see that the optimal is

   in the last step above, i used the clever trick i learned from cs 281a
   that for and a matrix that is , we have . but wait, what does this
   mean? we can express as

   with an appropriately defined since the columns of (not , be careful!)
   are the actual training elements, so is in the space spanned by them
   and thus we can write it as a linear combination.

   when we are faced with a new training instance to do regression, , we
   will perform the following:

   we have kernels again! this is more accurately known as kernelized
   id75. in fact, we even use kernels before we test on new
   examples (i.e., we use it during training). why? the matrix is itself a
   kernel matrix! it consists of dot products between the training
   instances, and since we optimize over that during training, we will use
   kernels during training.

   i may end up reading [10]more of tom mitchell   s slides on this, because
   this was quite illuminating to me.

support vector machines

   we now switch gears to support vector machines (id166s), which are
   possibly the best    off-the-shelf    classifier because they combine the
   kernel trick along with the concept of a maximum margin separator.
   thus, we know immediately that     like in the id88     we must find
   some way to express the optimization problem in terms of dot products.

   to begin the derivation, we define the functional margin of a weight
   vector (note: we keep the intercept term separate now) with respect to
   training instance to be , where the class label , and across the entire
   dataset, is just the minimum of all the functional margins. ideally, we
   would like the functional margin to be relatively large, as that would
   indicate a strong,    robust    boundary between the two classes.

   we can formulate id166s with the following    optimization    problem:

   such that

   along with the restriction that , which prevents the functional margin
   from changing due to invariance of the size of (though might vary, but
   i don   t think it   s a problem).

   unfortunately, this is not really possible with    optimization    easily,
   so we transform the problem into an equivalent one as follows:

   such that for all training instances , we have the constraint . this
   scales so that the functional margin must be one.

   we are done, but it is better to face the problem from the dual
   perspective so that we can take advantage of kernels. since the dual
   solution is less than or equal to the primal solution , it follows that
   we can equivalently solve the problem by maximizing the dual^[11]4. we
   re-write the constraints as and construct the lagrangian as:

   setting the derivative of with respect to and , then after some algebra
   (which took me a while due to lots of indices messing me up, but i
   eventually got it), and then knowing that we need to maximize this, we
   pose the dual optimization problem:

   such that for all , and . fortunately, this is convex, so there is a
   single global minimum.

   notice that we have variables again, though these have a different
   interpretation than the ones in the kernelized id88, though.
   watch out! and yes, we do get kernels to appear once again. nice!

   now let   s see what happens when we have trained and are going to assign
   a class to a new instance . we perform the computation, which can
   equivalently be expressed as

   once again, we have kernels! incidentally, it looks like we might have
   to do a lot of computation for classifying a single point, but in fact,
   most of the s will be zero. the few that are nonzero correspond to the
   training instances called support vectors, and they are the ones
   closest to the margin. this is formally called the karush-kuhn-tucker
   dual complementarity condition. the fact that we may not need to do
   much computation means id166s gain some of the advantages of parametric
   models.

   in the above problem, we     just like in the kernelized id88 and
   kernelized regression     have formulated the problem so that, both
   during training and classification of new examples, the data enter via
   inner products, allowing us to use kernels.

   what happens when we do not have linearly separable data? rather than
   come up with a more complicated or longer feature vector (which might
   risk overfitting), we can reformulate the problem using slack variables
   (for -id173) and an additional, controllable parameter :

   such that for all , we have and . thus, samples are permitted to have a
   (functional) margin less than one.

   rather surprisingly, the only change to the dual is that constraints
   turns into constraints, so we can apply the same principles (roughly
   speaking) as we did in the linearly separable case. in addition, the
   way we find changes, but generally we don   t really worry about the
   intercept too much when going through the derivation. it   s really that
   matters most to me.
     __________________________________________________________________

    1. this is important. when we call things , we usually refer to the
       raw data, but what the classifier needs are a set of features for
       each sample. but some people elide this notation by treating
       directly as features, so be careful. [12]   
    2. even when the data is linearly separable, the id88 is only
       guaranteed to converge in the binary classification case. here   s a
       key theorem: suppose the (binary) data are separable with margin
       and the maximum norm of a training sample is . then the id88
       converges with at most updates. [13]   
    3. here   s some intuition: we   re trying to combine the best of nearest
       neighbor approaches with id88 approaches by using the
       former   s ability to use fancy    similarity    functions along with the
       latter   s ability to explicitly learn from data. [14]   
    4. technically, we need the karush-kuhn-tucker conditions to hold for
       there to be possibly equality. [15]   

   please enable javascript to view the [16]comments powered by disqus.

seita's place

     * seita's place
     * [17]seita@cs.berkeley.edu

     * [18]danieltakeshi
     * [19](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fn:features
   8. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fn:binary
   9. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fn:intuitive
  10. https://www.cs.cmu.edu/~tom/10701_sp11/slides/kernels_id166_04_7_2011-ann.pdf
  11. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fn:kkt
  12. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fnref:features
  13. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fnref:binary
  14. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fnref:intuitive
  15. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/#fnref:kkt
  16. https://disqus.com/?ref_noscript
  17. mailto:seita@cs.berkeley.edu
  18. https://github.com/danieltakeshi
  19. https://twitter.com/(never!)

   hidden links:
  21. https://danieltakeshi.github.io/2015-08-08-id88s-id166s-and-kernel-methods.md/
