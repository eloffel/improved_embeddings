   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

id48 and id143ing

   jul 25, 2015

   in this post, we will continue our discussion of id114 by
   going over a special kind known as a hidden markov model (id48). these
   are appropriate for modeling forms of sequential data, implying that we
   finally relax various forms of    independent identically distributed   
   data or variables. they are a piece of the broader class of models
   known as dynamic id110s, of which kalman filters     which we
   will discuss in a future blog post here     are their continuous
   analogue.

id48

   id48s are one of the most popular id114 in real use (indeed,
   probably the most popular). they are characterized by variables
   representing hidden states and variables representing observations
   (i.e., evidence). the subscript in and represents a discrete slice of
   time. there are three id203 distributions in id48s: a prior
   id203 , a transition id203 and an emission (sometimes
   called observation) id203 . that the transition id203 only
   depends on the previous state means we are effectively invoking the
   markov assumption. whatever information about the state in the last
   time slice is all we need to know to determine the likelihood of the
   current state. our distribution is stationary, which means that we only
   need to specify those three probabilities and we are set, because the
   cpts are    carried over    to all the time slices. this is an instance of
   parameter tying.

   there are several well-defined id136 tasks and algorithms that we
   can conduct on id48s, to which we now turn. true, we could simply take
   an id48 and run variable elimination on it, but the structure of id48s
   provides us with some recursion we can exploit.

filtering, smoothing, and predicting

   these three are all similar. given a series of observations, we want to
   determine the distribution over states at some time stamp. concretely,
   we want to determine . the task is called filtering if , smoothing if ,
   and predicting if . clearly, smoothing will give better estimates, and
   prediction the weakest (or most uncertain) estimates.

   to compute filtering estimates, we require a recursive computation. the
   actual algorithm we use is the forward algorithm. we will talk more
   about it later; here is the update equation for the forward algorithm:

   where capital variables denote unknown variables, and the lowercase
   ones denote known variables. one can see that this equation makes
   intuitive sense. we sum up over all possible state realizations in the
   previous time step by their transition id203 to the current
   state, and for each of those, we weigh the id203 by the
   likelihood of emission from the state (which doesn   t depend on so we
   can move it out of the sum). furthermore, note that this is a recursive
   computation, a theme that will be common when we talk about id136
   in id48s. finally, the normalizing constant here is computed by summing
   up for all possible realizations of . the normalizing is needed only
   because of the addition of the emission model id203 downscaling
   our values.

   given that we know filtering, prediction is not that much more work,
   because we have the value up to the latest evidence variable, then it   s
   like we are ignoring the evidence variable and only using the
   transition id203:

   (here, we have no proportion sign.)

   as we predict further into the distribution, we will eventually end up
   at the stationary distribution of the underlying markov chain governing
   the sequence of state realizations. we can compute this as , where is
   the matrix of transitions, and is the column vector of priors.

   for smoothing, we actually need to compute something called a backward
   id203:

   the first component is what we were able to compute earlier, and we can
   do this recursively from the starting state. the second component is
   new, but in fact, it is not too hard to compute recursively     we just
   have to start backwards from . here is the update:

   notice now that we cannot move anything out of the summation, like we
   did with the forward update.

   we can run something called the forward-backward algorithm, which will
   compute the forward probabilities, then the backward probabilities. we
   will return to these    forward    and    backward    subroutines in the
   following sub-sections, with two helpful figures representing these two
   algorithms.

the likelihood of observations

   in this section, we   ll talk about two other common tasks in id48s. the
   first is computing the likelihood of an observation sequence, . at
   first, it might not seem obvious how to do this, because we don   t even
   know the state sequence that caused the observations! following the
   laws of id203, we have to include the states in our joint
   sequence, and then sum up over them. to do this efficiently, we use a
   id145 algorithm called the forward algorithm, which folds
   together the paths that could have generated the observations.

   i find it easiest to think of this computation just by looking at a
   trellis, like the one in the following image from jurafsky   s book^[7]1:

   forward_trellis

   this example was about determining the observation likelihood of seeing
   three, one, and then three ice creams eaten in a day, and the state
   variables represented the temperature of the day. in the trellis, the
   rows correspond to the possible realizations of each state, along with
   one other row to symbolically represent the
   emission/observation/evidence. the notation of represents the
   id203 of the observation sequence and being at a particular state
   at time , so . the computation of is done recursively in an intuitive
   manner, as shown in the image:

   forward_id203

   intuitively, the likelihood of an observation sequence should be the
   sum of all the possible state sequences that could have led to that
   observation. the fact that we are summing up and storing probabilities
   in the previous terms is an indication of the id48   s independence
   assumptions, and is what lets us do id136 efficiently. oh, and to
   actually compute the observations, we do need to sum out from .

   by the way, did you notice that these    updates    that we are doing are
   exactly what we did for the filtering step earlier? it   s just taking an
   earlier factor (representing whatever id203 we want) and
   multiplying it by the transition and then the emission id203! in
   russell and norvig, the update equation is called , and can represent
   or . while and are clearly different, their updates are the same. and
   this forward subroutine is what we generally mean when we talk about
      the forward algorithm.   

the viterbi algorithm (decoding)

   this is another common id136 task. given a series of observations,
   the viterbi algorithm helps us to determine the most likely sequence of
   states the system went to produce those observations. this is similar
   to the forward algorithm that we used to compute the likelihood of an
   observation sequence, but here we will now take max-es instead of
   summations, and we also have to keep a series of    backtrace pointers   
   so that we can reproduce our path.

   here   s the intuition: suppose we want to find the most likely path to
   some state . to do that, we need to find the most likely state path
   through , and then compare all the possible realizations of along with
   the transition (and emission, but those are the same) probabilities
   going to the current state. then we pick the highest likelihood one,
   and add that particular to the path (note the lowercase here!).
   mathematically, we express it as follows:

   the    messages    here are not forward messages but instead

   the time and space complexity is linear in terms of the length of the
   sequence, , but for filtering, we don   t have the space taking up linear
   time since there are no backpointers.

training the parameters with em

   the purpose of the baum-welch algorithm (which is an example of the
   expectation-maximization algorithm), is to determine the parameters of
   an id48 given observed data. somewhat confusingly, jurafsky calls it the
   forward-backward algorithm. russell calls the forward-backward
   algorithm as the algorithm that computes all the forward and backward
   id203 messages defined earlier^[8]2. but i guess they might be
   doing the same thing, because training id48s will require us to compute
   the backward probabilities anyway. jurafsky defines the backward
   id203 to be exactly the same as what russell defines it, i.e., as
   the id203 of seeing the observations from time to the end, given
   that we are at some state in time , or . the recursion of the backwards
   id203 is shown in the following image, where we now see that we
   had to keep separate emission probabilities for each state, unlike in
   the forward id203 case.

   backward_id203

   to estimate the parameters of the transition id203 matrix, for a
   given element , we must count the expected proportion of times that the
   system undergoes a transition from state realization to state
   realization . the expected counts are what em computes, since those are
   the latent variables.

   the following diagram shows the various things we have to compute to
   get the expected counts. notice that the diagram is computing the
   id203 of being in state then state , jointly with the observation
   sequence. thus, after this step, we would divide by (or in their
   notation). we then sum over all times, and that gives us the expected
   number of transitions from state to state .

   baum_welch

   we follow a similar procedure for estimating the parameters of the
   emission id203.

   the m-step is simple here; it turns expected counts into id113 for the
   corresponding parameters.

id143ing

   i got confused about id143ing earlier. i assumed it broadly
   meant any algorithm that sampled something, but in fact, it has a
   specific meaning in the context of id48s and dynamic id110s
   more generally. it is indeed a sampling algorithm, but has some
   interesting properties that are worth discussing in its own right.

   first, when do we want to do id143ing? we want to do this if
   exact id136 in id48s is intractable, or if the speed of sampling
   (and computing its corresponding probabilities) is more important than
   exact answers.

   a naive way of performing exact id136 in an id48 is to    unroll    it
   into a normal id110. technically, id48s are infinitely long,
   so what we would do is chop off the id48 after the last time step that
   has a query or evidence variable, since by the structure of an id48,
   anything after is not relevant to the query^[9]3. but if we did
   filtering after each observation, we would have to redo a lot of
   computations from the start to the set of variables at the current time
   step. to save on memory costs, we realize that if we do filtering for a
   given time , we only need the relevant id203 distribution from
   the previous time , since the computation is done recursively.
   unfortunately, when we do this    summing out    process a-la variable
   elimination, that will iterate through the set of states, and this
   computation will grow exponentially due to the need to construct an
   exponentially large factor table^[10]4.

   thus, we need to use approximate id136 methods. in a [11]previous
   post, i discussed likelihood weighting, which is a sampling algorithm
   that we can adapt for the id48 setting. we do, however, have to ensure
   that we do not unroll the entire id48, and we have to make sure our
   weights are not being driven to zero.

   likelihood weighting (just like rejection sampling) suffers as the
   number of evidence variables increases, because the weight of each
   sample is based on the product of all the terms, which drives our
   values down to zero. but there is something that might be even worse
   than that. when we sample our non-evidence variables, ideally we would
   like those samples to depend on the evidence variables, so they can
      guide    the sampling correctly. unfortunately, if all the evidence
   variables tend to be downstream, i.e., they occur late in the variable
   ordering, then the sampling for the non-evidence variables will not
   depend on the evidence, even though the actual variables should depend
   on each other^[12]5. sadly, in an id48, all state variables have the
   property such that all the evidence relevant to them is downstream!
   (the ones before it are not downstream, but they are not ancestors
   either!)

   what does id143ing do, then, keeping in mind the weaknesses
   of likelihood weighting? here is how the algorithm works. we initialize
   a population of samples. the quantity determines the time complexity of
   the algorithm, not the number of states. then for each time step, we
   perform the following steps in sequence:
     * we transition the samples according to the transition probabilities
       . this means we do not have to unroll the id48.
     * we then observe the evidence and downscale (i.e., multiply by a
       value in ) each sample according to the evidence . this is where
       the likelihood weighting    adaption    is obvious because now each
       sample is a fraction of a sample with some weight, and the overall
       distribution over the state variable   s realizations is the fraction
       of weights there divided by the total fraction of weights across
       all realizations.
     * then (and this is important), we resample to renormalize the
       distribution, where the resampling process is sampled according to
       the weights of the samples.

   here is a nice diagram from the cs 188 slides that suddenly clarified
   id143ing for me.

   particle_filtering

   here, we assume we have one state variable, with nine possible
   realizations. (or we could have two state variables, each with three
   realizations, etc.) we have ten particles, representing our estimate of
   the distribution . now the goal will be to determine the table of
   values. first, we transition, which means we move each of the ten
   particles over to their next realization and multiply accordingly.
   then, we observe , and downscale the values, resulting in points of
   different sizes, which are a diagram notation for weights. above, we
   assume that whatever was, the id203 is high for that spot at
   coordinate , and lower as we gradually go away from that spot. the last
   step is to resample, so we are resampling based on the id203
   distribution of the weighted samples. so we get five samples in that
   block because divided by the sum of the weights (note: there   s a typo
   on the slides) gives a    high    id203. note now that we have
      normal-sized    samples again.

   at this point, we should ask ourselves why we even bother resampling.
   it   s not completely clear to me, but it has to do with how we
   constantly rescale the weights downwards. in general, it   s not a good
   idea to have such small weights, because then the more evidence we
   have, the more likely we will just have a bunch of zero-id203
   particles. in other words, most particles end up in low-likelihood
   places in the state space. still, i don   t quite buy some of the
   textbook explanations on this because couldn   t we just perform the
   computations in log space?
     __________________________________________________________________

    1. i know it says the words    draft    there, because i copied these
       images from an online version of the book. however, i did buy a
       hard copy of the book, so i think it is fine for me to do this
       (i.e., not illegal). if i did not have that online version, i would
       have taken a picture of the book with a cell phone, but then the
       figure would not have been perfectly flat. [13]   
    2. in the em chapter, russell says that his forward-backward algorithm
       can be slightly modified for the em case. [14]   
    3. in other words, the variable elimination algorithm would be able to
       push the summation sign for those variables and isolate them to the
       right. they would sum up to one and thus are irrelevant. [15]   
    4. i apologize if this is not clear. i think the easiest way to
       understand this is that this is still normal variable elimination,
       so it suffers from the sample computational complexity problems.
       what makes this confusing is that we tend to have one hidden
       variable , but that hidden variable can actually taken on different
       realizations. but what is happening here (and which russel and
       norvig discuss in their book) is that our variable represents a set
       of state variables. as the set increases in size, the process of
       summing out will require as many sums as there are state variables.
       at first, i thought the intractability came from how our one hidden
       variable could take on many values, but the intractability is
       actually when we have multiple hidden variables (and we would just
       use as shorthand to represent all of them). but both views can be
       made compatible if we have one mega-variable that takes on values
       from each state variable into one realization. [16]   
    5. i hope i   m thinking about this correctly. i think the issue is that
       when we sample, we assume we only know the cpts, which only express
       node-parent relationships, so there is no notion of sampling in the
          opposite direction    where we could sample a node based on the
       value of its children, even though those values do depend on each
       other according to the bayes ball algorithm. [17]   

   please enable javascript to view the [18]comments powered by disqus.

seita's place

     * seita's place
     * [19]seita@cs.berkeley.edu

     * [20]danieltakeshi
     * [21](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fn:illegal
   8. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fn:forwardback
   9. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fn:sumtoone
  10. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fn:clarity
  11. http://danieltakeshi.github.io/closing-thoughts-on-graphical-models/
  12. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fn:ithink
  13. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fnref:illegal
  14. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fnref:forwardback
  15. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fnref:sumtoone
  16. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fnref:clarity
  17. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/#fnref:ithink
  18. https://disqus.com/?ref_noscript
  19. mailto:seita@cs.berkeley.edu
  20. https://github.com/danieltakeshi
  21. https://twitter.com/(never!)

   hidden links:
  23. https://danieltakeshi.github.io/2015-07-25-hidden-markov-models-and-particle-filtering/
