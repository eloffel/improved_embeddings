   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

mathematical tricks commonly used in machine learning and statistics

   may 6, 2017

   i have passionately studied various machine learning and statistical
   concepts over the last few years. one thing i   ve learned from all this
   is that there are many mathematical    tricks    involved, whether or not
   they are explicitly stated. (in research papers, such tricks are often
   used without acknowledgment since it is assumed that anyone who can
   benefit from reading the paper has the mathematical maturity to fill in
   the details.) i thought it would be useful for me, and hopefully for a
   few interested readers, to catalogue a set of the common tricks here,
   and to see them applied in a few examples.

   the following list, in alphabetical order, is a non-exhaustive set of
   tricks that i   ve seen:
     * cauchy-schwarz
     * integrating probabilities into expectations
     * introducing an independent copy
     * jensen   s inequality
     * law of iterated expectation
     * lipschitz functions
     * markov   s inequality
     * norm properties
     * series expansions (e.g. taylor   s)
     * stirling   s approximation
     * symmetrization
     * take a derivative
     * union bound
     * variational representations

   if the names are unclear or vague, the examples below should clarify.
   all the tricks are used except for the law of iterated expectation,
   i.e. . (no particular reason for that omission; it just turns out the
   exercises i   m interested in didn   t require it.)

example 1: maximum of (not necessarily independent!) sub-gaussians

   i covered this problem in [7]my last post here so i will not repeat the
   details. however, there are two extensions to that exercise which i
   thought would be worth noting.

   first, to prove an upper bound for the random variable , it suffices to
   proceed as we did earlier in the non-absolute value case, but augment
   our sub-gaussian variables with the set . it   s ok to do this because no
   independence assumptions are needed. then it turns out that an upper
   bound can be derived as

   this is the same as what we had earlier, except the    2    is now outside
   the square root. it   s quite intuitive.

   second, consider how we can prove the following bound:

   we start by applying the standard technique of multiplying by ,
   exponentiating and then applying markov   s inequality with our
   non-negative random variable :

   where in (i) we used a bound previously determined in our bound on (it
   came out of an intermediate step), and then used the fact that the term
   in the exponential is a convex quadratic to find the minimizer value
   via differentiation in (ii).

   at this point, to satisfy the desired inequality, we compare terms in
   the exponentials and claim that with ,

   this will result in our desired bound. it therefore remains to prove
   this, but it reduces to checking that

   and the left hand side is non-negative. hence, the desired bound holds.

   tricks used:
     * jensen   s inequality
     * markov   s inequality
     * take a derivative
     * union bound

   comments: my earlier blog post (along with this one) shows what i mean
   when i say    take a derivative.    it happens when there is an upper bound
   on the right hand side and we have a free parameter (or ) which we can
   optimize to get the tighest possible bound. often times, such a is
   explicitly introduced via markov   s inequality, as we have here. just
   make sure to double check that when taking a derivative, you   re getting
   a minimum, not a maximum. in addition, markov   s inequality can only be
   applied to non-negative random variables, which is why we often have to
   exponentiate the terms inside a id203 statement first.

   note the use of convexity of the exponential function. it is very
   common to see jensen   s inequality applied with the exponential
   function. always remember that !!

   the procedure that i refer to as the    union bound    when i bound a
   maximum by a sum isn   t exactly the canonical way of doing it, since
   that typically involves probabilities, but it has a similar flavor.
   more formally, the [8]union bound states that

   for countable sets of events . when we define a set of events based on
   a maximum of certain variables, that   s the same as taking the union of
   the individual events.

   on a final note, be on the look-out for applications of this type
   whenever a    maximum    operation is seen with something that resembles
   gaussians. sometimes this can be a bit subtle. for instance, it   s not
   uncommon to use a bound of the form above when dealing with , the
   expectation of the -norm of a standard gaussian vector. in addition,
   when dealing with sparsity, often our        or        is actually something
   like or another combinatorics-style value. seeing a    log    accompanied
   by a square root is a good clue and may help identify such cases.

example 2: bounded random variables are sub-gaussian

   this example is really split into two parts. the first is as follows:

     prove that rademacher random variables are sub-gaussian with
     parameter .

   the next is:

     prove that if is a zero-mean and has support , then is sub-gaussian
     with parameter (at most) .

   to prove the first part, let be a rademacher random variable. for , we
   have

   and thus the claim is satisfied by the definition of a sub-gaussian
   random variable. in (i), we removed the expectation by using facts from
   rademacher random variables, in (ii) we used the series expansions of
   the exponential function, in (iii) we simplified by removing the odd
   powers, in (iv) we used the clever trick that , and in (v) we again
   used the exponential function   s power series.

   to prove the next part, observe that for any , we have

   which shows by definition that is sub-gaussian with parameter . in (i),
   we cleverly introduce an extra independent copy inside the exponent.
   it   s zero-mean, so we can insert it there without issues.^[9]1 in (ii),
   we use jensen   s inequality, and note that we can do this with respect
   to just the random variable . (if this is confusing, just think of the
   expression as a function of and ignore the outer expectation.) in (iii)
   we apply a clever symmetrization trick by multiplying a rademacher
   random variable to . the reason why we can do this is that is already
   symmetric about zero. hence, inserting the rademacher factor will
   maintain that symmetry (since rademachers are only +1 or -1). in (iv),
   we applied the rademacher sub-gaussian bound with held fixed, and then
   in (v), we finally use the fact that .

   tricks used:
     * introducing an independent copy
     * jensen   s inequality
     * series expansions (twice!!)
     * symmetrization

   comments: the first part is a classic exercise in theoretical
   statistics, one which tests your ability to understand how to use the
   power series of exponential functions. the first part involved
   converting an exponential function to a power series, and then later
   doing the reverse. when i was doing this problem, i found it easiest to
   start by stating the conclusion     that we would have somehow     and then
   i worked backwards. obviously, this only works when the problem gives
   us the solution!

   the next part is also    classic    in the sense that it   s often how
   students (such as myself) are introduced to the symmetrization trick.
   the takeaway is that one should be on the lookout for anything that
   seems symmetric. or, failing that, perhaps introduce symmetry by adding
   in an extra independent copy, as we did above. but make sure that your
   random variables are zero-mean!!

example 3: concentration around median and means

   here   s the question:

     given a scalar random variable , suppose that there are positive
     constants such that

     for all .

     (a) prove that

     (b) prove that for any median , we have

     for all , where and .

   to prove the first part, note that

   where (i) follows from definition, (ii) follows from the    integrating
   probabilities into expectations    trick (which i will describe shortly),
   (iii) follows from the provided bound, and (iv) follows from standard
   calculus (note the multiplication of for mathematical convenience).
   this proves the first claim.

   this second part requires some clever insights to get this to work. one
   way to start is by noting that:

   and where the last inequality follows from the bound provided in the
   question. for us to be able to apply that bound, assume without loss of
   generality that , meaning that our term is positive and that we can
   increase the id203 by inserting in absolute values. the above
   also shows that

   we next tackle the core of the question. starting from the left hand
   side of the desired bound, we get

   where step (i) follows from adding zero, step (ii) follows from the
   triangle inequality, and (iii) follows from the provided bound based on
   the expectation. and yes, this is supposed to work only for when . the
   way to get around this is that we need to assume is greater than some
   quantity. after some algebra, it turns out a nice condition for us to
   enforce is that , which in turn will make . if , then the desired bound
   is attained because

   a fact which can be derived through some algebra. thus, the remainder
   of the proof boils down to checking the case that when , we have

   and this is proved by analyzing roots of the quadratic and solving for
   .

   tricks used:
     * integrating probabilities into expectations
     * triangle inequality

   comments: the trick    integrating probabilities into expectations    is
   one which i only recently learned about, though one can easily find it
   (along with the derivation) on the [10]wikipedia page for the expected
   values. in particular, note that for a positive real number , we have

   and in the above, i use this trick with . it   s quite useful to convert
   between probabilities and expectations!

   the other trick above is using the triangle inequality in a clever way.
   the key is to observe that when we have something like , if we increase
   the value of , then we increase that id203. this is another
   common trick used in proving various bounds.

   finally, the above also shows that when we have constants , it pays to
   be clever in how we assign those values. then the remainder is some
   brute-force computation. i suppose it also helps to think about
   inserting s whenever we have a id203 and a median.

example 4: upper bounds for    balls   

   consider the set

   we often write the number of nonzeros in as like this even though is
   not technically a norm. this exercise consists of three parts:

     (a) show that where consists of all subsets of of size , and is a
     sub-vector of (of size ) indexed by those components. note that by
     this definition, the cardinality of is equal to .

     (b) show that for any fixed subset of cardinality , we have .

     (c) establish the claim that .

   to be clear on the notation, and refers to the gaussian complexity of
   that set. it is, roughly speaking, a way to measure the    size    of a
   set.

   to prove (a), let and let indicate the support of (i.e. where its
   nonzeros occur). for any (which we later treat to be sampled from ,
   though the immediate analysis below does not require that fact) we have

   where refers to the vector taking only the nonzero components from .
   the first inequality follows from cauchy-schwarz. in addition, by
   standard norm properties, taking results in the case when equality is
   attained. the claim thus follows. (there are some technical details
   needed regarding which of the maximums     over the set sizes or over the
   vector selection     should come first, but i don   t think the details are
   critical for me to know.)

   for (b), we first claim that the function defined as is lipschitz with
   respect to the euclidean norm with lipschitz constant . to see this,
   observe that when and are both -dimensional vectors, we have

   where (i) follows from the reverse triangle inequality for normed
   spaces and (ii) follows from how the vector cannot have more nonzero
   terms than but must otherwise match it for indices lying in the subset
   .

   the fact that is lipschitz means that we can apply a theorem regarding
   tail bounds of lipschitz functions of gaussian variables. the function
   here doesn   t require its input to consist of vectors with iid standard
   gaussian components, but we have to assume that the input is like that
   for the purposes of the theorem/bound to follow. more formally, for all
   we have

   where (i) follows from how and thus we are just decreasing the
   threshold for the event (hence making it more likely) and (ii) follows
   from the theorem, which provides an in the denominator of the
   exponential, but here.

   finally, to prove (c), we first note that the previous part   s theorem
   guaranteed that the function is sub-gaussian with parameter . using
   this, we have

   where (i) applies the bound for a maximum over sub-gaussian random
   variables for all the sets (see example 1 earlier), each with parameter
   , and (ii) applies an approximate bound due to stirling   s approximation
   and ignores the constants of and . the careful reader will note that
   example 1 required zero-mean sub-gaussian random variables, but we can
   generally get around this by, i believe, subtracting away a mean and
   then re-adding later.

   tricks used:
     * cauchy-schwarz
     * jensen   s inequality
     * lipschitz functions
     * norm properties
     * stirling   s approximation
     * triangle inequality

   comments: this exercise involves a number of tricks. the fact that
   follows from how

   due to jensen   s inequality and how for . fiddling with norms,
   expectations, and square roots is another common way to utilize
   jensen   s inequality (in addition to using jensen   s inequality with the
   exponential function, as explained earlier). moreover, if you see norms
   in a probabilistic bound statement, you should immediately be thinking
   of the possibility of using a theorem related to lipschitz functions.

   the example also uses the (reverse!) triangle inequality for norms:

   this can come up quite often and is the non-canonical way of viewing
   the triangle inequality, so watch out!

   finally, don   t forget the trick where we have . this comes from [11]an
   application of stirling   s approximation and is seen frequently in cases
   involving sparsity, where components are    selected    out of total. the
   maximum over a finite set should also provide a big hint regarding the
   use of a sub-gaussian bound over maximums of (sub-gaussian) variables.

example 5: gaussian complexity of ellipsoids

     recall that the space consists of all real sequences such that .
     given a strictly positive sequence , consider the associated ellipse

     (a) prove that the gaussian complexity satisfies the bounds

     (b) for a given radius , consider the truncated set

     obtain upper and lower bounds on its gaussian complexity that are
     tight up to universal constants independent of and .

   to prove (a), we first start with the upper bound. letting indicate a
   sequence of iid standard gaussians , we have

   where (i) follows from definition, (ii) follows from multiplying by
   one, (iii) follows from a clever application of the [12]cauchy-schwarz
   inequality for sequences (or more generally, [13]holder   s inequality),
   (iv) follows from the definition of , (v) follows from jensen   s
   inequality, and (vi) follows from linearity of expectation and how .

   we next prove the lower bound. first, we note a well-known result that
   where indicates the rademacher complexity of the set. thus, our task
   now boils down to showing that . letting be iid rademachers, we first
   begin by proving the upper bound

   where (i) follows from definition, (ii) follows from the symmetric
   nature of the class of (meaning that wlog we can pick for all ) and
   then multiplying by one, (iii), follows from cauchy-schwarz again, and
   (iv) follows from the provided bound in the definition of .

   we   re not done yet: we actually need to show equality for this, or at
   the very least prove a lower bound instead of an upper bound. however,
   if one chooses the valid sequence such that , then equality is attained
   since we get

   in one of our steps above. this proves part (a).

   for part (b), we construct two ellipses, one that contains and one
   which is contained inside it. let . then we claim that the ellipse
   defined out of this sequence (i.e. treating        as our       ) will be
   contained in . we moreover claim that the ellipse defined out of the
   sequence for all contains , i.e. . if this is true, it then follows
   that

   because the definition of gaussian complexity requires taking a maximum
   of over a set, and if the set grows larger via set containment, then
   the gaussian complexity can only grow larger. in addition, the fact
   that the upper and lower bounds are related by a constant suggests that
   there should be extra lower and upper bounds utilizing universal
   constants independent of and .

   let us prove the two set inclusions previously described, as well as
   develop the desired upper and lower bounds. suppose . then we have

   and

   in both cases, the first inequality is because we can only decrease the
   value in the denominator.^[14]2 the last inequality follows by
   assumption of membership in . both requirements for membership in are
   satisfied, and therefore, implies and thus the first set containment.
   moving on to the second set containment, suppose . we have

   where (i) follows from a    union bound   -style argument, which to be
   clear, happens because for every term in the summation, we have either
   or added to the summation (both positive quantities). thus, to make the
   value larger, just add both terms! step (ii) follows from the
   assumption of membership in . thus, we conclude that , and we have
   proved that

   the final step of this exercise is to develop a lower bound on the left
   hand side and an upper bound on the right hand side that are close up
   to universal constants. but we have reduced this to an instance of part
   (a)! thus, we simply apply the lower bound for and the upper bound for
   and obtain

   as our final bounds on . (note that as a sanity check, the constant
   offset is less than one.) this proves part (b).

   tricks used:
     * cauchy-schwarz
     * jensen   s inequality
     * union bound

   comments: this exercise on the surface looks extremely challenging. how
   does one reason about multiple infinite sequences, which furthermore
   may or may not involve squared terms? i believe the key to tackling
   these problems is to understand how to apply cauchy-schwarz (or more
   generally, holder   s inequality) for infinite sequences. more precisely,
   holder   s inequality for sequences spaces states that

   (it   s actually more general for this, since we can assume arbitrary
   positive powers and so long as , but the easiest case to understand is
   when .)

   holder   s inequality is enormously helpful when dealing with sums
   (whether infinite or not), and especially when dealing with two sums if
   one does not square its terms, but the other one does.

   finally, again, think about jensen   s inequality whenever we have
   expectations and a square root!

example 6: pairwise incoherence

     given a matrix , suppose it has normalized columns ( for all ) and
     pairwise incoherence upper bounded as .

     (a) let be any subset of size . show that there is a function such
     that as long as is sufficiently small, where is the matrix formed by
     extracting the columns of whose indices are in .

     (b) prove, from first principles, that satisfies the restricted
     nullspace property with respect to as long as .

   to clarify, the pairwise incoherence of a matrix is defined as

   where denotes the -th column of . intuitively, it measures the
   correlation between any columns, though it subtracts an indicator at
   the end so that the maximal case does not always correspond to the case
   when . in addition, the matrix as defined in the problem looks like:

   where the 1s in the diagonal are due to the assumption of having
   normalized columns.

   first, we prove part (a). starting from the variational representation
   of the minimum eigenvalue, we consider any possible with euclidean norm
   one (and thus this analysis will apply for the minimizer which induces
   the minimum eigenvalue) and observe that

   where (i) follows from the definition of a quadratic form (less
   formally, by id127), (ii) follows from the assumption,
   (iii) follows from noting that

   which in turn follows from the pairwise incoherence assumption that .
   step (iv) follows from definition, and (v) follows from how for
   -dimensional vectors.

   the above applies for any satisfactory . putting together the pieces,
   we conclude that

   which follows if is sufficiently small.

   to prove the restricted [15]nullspace property in (b), we first suppose
   that and . define -dimensional vectors and which match components of
   for the indices within their respective sets or , and which are zero
   otherwise.^[16]3 supposing that corresponds to the subset of indices of
   of the largest elements in absolute value, it suffices to show that ,
   because then we can never violate this inequality (and thus the
   restricted nullspace property holds).

   we first show a few facts which we then piece together to get the final
   result. the first is that

   where (i) follows from the assumption that is in the kernel of , (ii)
   follows from how , (iii) follows from expanding the term, and (iv)
   follows from carefully noting that

   where in the inequality, we have simply chosen as our , which can only
   make the bound worse. then step (iv) follows immediately. don   t forget
   that , because the latter involves a vector that (while longer) only
   has extra zeros. incidentally, the above uses the variational
   representation for eigenvalues in a way that   s more convenient if we
   don   t want to restrict our vectors to have euclidean norm one.

   we conclude from the above that

   next, let us upper bound the rhs. we see that

   where (i) follows from a little thought about how id127
   and quadratic forms work. in particular, if we expanded out the lhs, we
   would get a sum with lots of terms that are zero since or would cancel
   them out. (to be clear, and .) step (ii) follows from definition, step
   (iii) follows from the provided pairwise incoherence bound (note the
   need to multiply by ), and step (iv) follows from how

   and thus it is clear that the product of the norms consists of the sum
   of all possible combination of indices with nonzero values.

   the last thing we note is that from part (a), if we assumed that , then
   a lower bound on is . putting the pieces together, we get the following
   three inequalities

   we can provide a lower bound for the first term above. using the fact
   that , we get . the final step is to tie the lower bound here with the
   upper bound from the set of three inequalities above. this results in

   under the same assumption earlier (that ) it follows directly that , as
   claimed. whew!

   tricks used:
     * cauchy-schwarz
     * norm properties
     * variational representation (of eigenvalues)

   comments: actually, for part (a), one can prove this more directly by
   using the [17]gershgorin circle theorem, a very useful theorem with a
   surprisingly simple proof. but i chose this way above so that we can
   make use of the variational representation for eigenvalues. there are
   also variational representations for singular values.

   the above uses a lot of norm properties. one example was the use of ,
   which can be proved via cauchy-schwarz. the extension to this is that .
   these are quite handy. another example, which is useful when dealing
   with specific subsets, is to understand how the and norms behave.
   admittedly, getting all the steps right for part (b) takes a lot of
   hassle and attention to details, but it is certainly satisfying to see
   it work.

closing thoughts

   i hope this post serves as a useful reference for me and to anyone else
   who might need to use one of these tricks to understand some machine
   learning and statistics-related math.
     __________________________________________________________________

    1. one of my undergraduate mathematics professors, [18]steven j.
       miller, would love this trick, as his two favorite tricks in
       mathematics are adding zero (along with, of course, multiplying by
       one). [19]   
    2. or    downstairs    as professor [20]michael i. jordan often puts it
       (and obviously,    upstairs    for the numerator). [21]   
    3. it can take some time and effort to visualize and process all this
       information. i find it helpful to draw some of these out with
       pencil and paper, and also to assume without loss of generality
       that corresponds to the first    block    of , and therefore
       corresponds to the second (and last)    block.    please contact me if
       you spot typos; they   re really easy to make here. [22]   

   please enable javascript to view the [23]comments powered by disqus.

seita's place

     * seita's place
     * [24]seita@cs.berkeley.edu

     * [25]danieltakeshi
     * [26](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. https://danieltakeshi.github.io/2017/04/22/following-professor-michael-jordans-advice-your-brain-needs-exercise
   8. https://en.wikipedia.org/wiki/boole's_inequality
   9. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fn:miller
  10. https://en.wikipedia.org/wiki/expected_value
  11. https://math.stackexchange.com/questions/132625/n-choose-k-leq-left-fracen-k-rightk
  12. https://en.wikipedia.org/wiki/cauchy   schwarz_inequality
  13. https://en.wikipedia.org/wiki/h  lder's_inequality
  14. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fn:downstairs
  15. https://en.wikipedia.org/wiki/nullspace_property
  16. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fn:time
  17. https://en.wikipedia.org/wiki/gershgorin_circle_theorem
  18. https://web.williams.edu/mathematics/sjmiller/public_html/williams/welcome.html
  19. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fnref:miller
  20. https://people.eecs.berkeley.edu/~jordan/
  21. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fnref:downstairs
  22. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics#fnref:time
  23. https://disqus.com/?ref_noscript
  24. mailto:seita@cs.berkeley.edu
  25. https://github.com/danieltakeshi
  26. https://twitter.com/(never!)

   hidden links:
  28. https://danieltakeshi.github.io/2017/05/06/mathematical-tricks-commonly-used-in-machine-learning-and-statistics
