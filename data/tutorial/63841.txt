   #[1]seita's place

   [2]seita's place

   [3]about [4]archive [5]new? start here [6]subscribe

going deeper into id23: understanding id24 and linear
function approximation

   oct 31, 2016

   as i mentioned in my [7]review on berkeley   s deep reinforcement
   learning class, i have been wanting to write more about reinforcement
   learning, so in this post, i will provide some comments on id24
   and linear function approximation. i   m hoping these posts can serve as
   another set of brief rl introductions, similar to [8]andrej   s excellent
   post on deep rl.

   last year, i [9]wrote an earlier post on the basics of mdps and rl.
   this current one serves as the continuation of that by discussing how
   to scale rl into settings that are more complicated than simple tabular
   scenarios but nowhere near as challenging as, say, learning how to play
   atari games from high-dimensional input. i won   t spend too much time
   trying to labor though the topics in this post, though. i want to save
   my effort for the deep variety, which is all the rage nowadays and a
   topic which i hope to eventually write a lot about for this blog.

   to hep write this post, here are two references i used to quickly
   review id24 with function approximation.
    1. [10]tutorial on rl and linear function approximators
    2. [11]barto and sutton   s rl book, in html

   i read (1) entirely, and (2) only partly, since it is, after all, a
   full book (note that the authors are in the process of making a newer
   edition!). now let   s move on to discuss an important concept: value
   iteration with function approximation.

value iteration with function approximation

   value iteration is probably the first rl-associated algorithm that
   students learn. it lets us assign values to states , which can then be
   used to determine optimal policies. i explained the algorithm in my
   earlier post, but just to be explicit, here   s a slide from my [12]cs
   287 class last fall which describes the procedure:

   value_iteration

   looks pretty simple, right? for each iteration, we perform updates on
   our values until convergence. each iteration, we can also update the
   policy for each state, if desired, but this is not the crucial part of
   the algorithm. in fact, since the policy updates don   t impact the
   max-operations in the algorithm (as shown above), we can forget about
   updating and do that just once after the algorithm has converged. this
   is what barto and sutton do in [13]their value iteration description.

   unfortunately, as suggested by the slide, (tabular) value iteration
   this is not something we can do in practice beyond the simplest of
   scenarios due to the curse of dimensionality in . note that is not
   usually a problem. think of go, for instance: according to reference
   (1) above, there are states, but just actions to play. in atari games,
   the disparity is even greater, with far more states but perhaps just
   two to five actions per game.

   the solution is to obtain features and then represent states with a
   simple function of those features. we might use a linear function,
   meaning that

   where is a bias term with 1    absorbed    into for simplicity. this
   function approximation dramatically reduces the size of our state space
   from into        where is the domain for . rather than determining for all
   , each iteration our goal is to instead update the weights .

   how do we do that? the most straightforward way is to reformulate it as
   a supervised learning problem. from the tabular version above, we know
   that the weights should satisfy the bellman optimality conditions. we
   can   t use all the states for this update, but perhaps we can use a
   small subset of them?

   this motivates the following algorithm: each iteration in our algorithm
   with current weight vector , we sample a very small subset of the
   states and compute the official one-step bellman backup updates:

   then we find the next set of weights:

   which can be done using standard supervised learning techniques. this
   is, in fact, an example of stochastic id119.

   before moving on, it   s worth mentioning the obvious: the performance of
   value iteration with function approximation is going to depend almost
   entirely on the quality of the features (along with the function
   representation, i.e. linear, neural network, etc.). if you   re
   programming an ai to play pacman, the states will be the game board,
   which is far too high-dimensional for tabular representations. features
   will ideally represent something relevant to pacman   s performance in
   the game, such as the distance to the nearest ghost, the distance to
   the nearest pellet, whether pacman is trapped, and so on. don   t neglect
   the art and engineering of feature selection!

id24 with function approximation

   value iteration with function approximation is nice, but as i mentioned
   in my last post, what we really want in practice are values, due to the
   key fact that

   which avoids a costly sum over the states. this will cost us extra in
   the sense that we have to now use a function with respect to in
   addition to , but again, this is not usually a problem since the set of
   actions is typically much smaller than the set of states.

   to use q-values with function approximation, we need to find features
   that are functions of states and actions. this means in the linear
   function regime, we have

   what   s tricky about this, however, is that it   s usually a lot easier to
   reason about features that are only functions of the states. think of
   the pacman example from earlier: it   s relatively easy to think about
   features by just looking at what   s on the game grid, but it   s harder to
   wonder about what happens to the value of a state assuming that action
   is taking place.

   for this reason, i tend to use the following    dimension scaling trick   
   as recommended by reference (1) above, which makes the distinction
   between different actions explicit. to make this clear, image an mdp
   with two features and four actions. the features for state-action pair
   can be encoded as:

   why the need to use different feature for different actions?
   intuitively, if we didn   t do this (and kept with just two non-bias
   features) then the action would have no effect! but actions do have
   impacts on the game. to use the pacman example again, imagine that the
   pacman agent has a pellet to its left, so the agent is one move away
   from being invincible. at the same time, however, there could be a
   ghost to pacman   s right! the action that pacman takes in that state
   (left or right) will have a dramatic impact on the resulting reward
   obtained! it is therefore important to take the actions into account
   when featurizing q-values.

   as a reminder before moving on, don   t forget the bias term! they are
   necessary to properly scale the function values independently of the
   features.

online least squares justification for id24

   i now want to sketch the derivation for id24 updates to provide
   intuition for why it works. knowing this is also important to
   understand how the training process works for the more advanced
   deep-q-network algorithm/architecture.

   recall the standard id24 update without function approximation:

   why does this work? imagine that you have some weight vector at
   iteration of the algorithm and you want to figure out how to update it.
   the standard way to do so is with stochastic id119, as
   mentioned earlier in the value iteration case. but what is the loss
   function here? we need that so we can compute the gradient.

   in id24, when we approximate a state-action value , we want it to
   be equal to the reward obtained, plus the (appropriately discounted)
   value of playing optimally from thereafter. denote this    target    value
   as . in the tabular version, we set the target as

   it is infeasible to sum over all states, but we can get a minibatch
   estimate of this by simply considering the samples we get: . averaged
   over the entire run, these will average out to the true value.

   assuming we   re in the function approximation case (though this is not
   actually needed), and that our estimate of the target is , we can
   therefore define our loss as:

   thus, our gradient update procedure with step size is

   this is how we update the weights for id24. notice that and are
   vectors, while is a scalar.

concluding thoughts

   well, there you have it. we covered:
     * value iteration with (linear) function approximation, a relatively
       easy-to-understand algorithm that should serve as your first choice
       if you need to scale up tabular value iteration for a simple
       id23 problem.
     * id24 with (linear) function approximation, which approximates
       values with a linear function, i.e. . from my experience, i prefer
       to use the states-only formulation , and then to apply the
          dimension scaling trick    from above to make id24 work in
       practice.
     * id24 as a consequence of online least squares, which provides
       a more rigorous rationale for why it makes sense, rather than
       relying on hand-waving arguments.

   the scalability benefit of id24 (or value iteration) with linear
   function approximation may sound great over the tabular versions, but
   here is the critical question: is a linear function approximator
   appropriate for the problem?

   this is important because a lot of current research on reinforcement
   learning is applied towards complicated and high dimensional data. a
   great example, and probably the most popular one, is learning how to
   play atari games from raw (210,160,3)-dimensional arrays. a linear
   function is simply not effective at learning values for these problems,
   because the problems are inherently non-linear! a further discussion of
   this is out of the scope of this post, but if you are interested,
   andrew ng addressed this in his talk at the bay area deep learning
   school a month ago. [14]kevin zakka has a nice blog post which
   summarizes ng   s talk.^[15]1 in the small data regime, many algorithms
   can achieve    good    performance, with differences arising in large part
   due to who tuned his/her algorithm the most, but in the
   high-dimensional big-data regime, the complexity of the model matters.
   neural networks can learn and represent far more complicated functions.

   therefore, in my next post, i will introduce and discuss id24
   with neural networks as the function approximation. it will use the
   atari games as a running example. stay tuned!
     __________________________________________________________________

    1. it   s great that he wrote that, because all the youtube videos with
       ng   s talk (as of now) do not have the auto-captions option enabled.
       i   m not sure why, and despite the flaws of auto-captions, it would
       have helped a lot. i tried watching ng   s talk and gave up after a
       few minutes since i was unable to understand the words he was
       saying. [16]   

   please enable javascript to view the [17]comments powered by disqus.

seita's place

     * seita's place
     * [18]seita@cs.berkeley.edu

     * [19]danieltakeshi
     * [20](never!)

   this is my blog, where i have written over 300 articles on a variety of
   topics. recent posts tend to focus on computer science, my area of
   specialty as a ph.d. student at uc berkeley.

references

   visible links
   1. https://danieltakeshi.github.io/feed.xml
   2. https://danieltakeshi.github.io/
   3. https://danieltakeshi.github.io/about.html
   4. https://danieltakeshi.github.io/archive.html
   5. https://danieltakeshi.github.io/new-start-here.html
   6. https://danieltakeshi.github.io/subscribe.html
   7. http://danieltakeshi.github.io/2015-12-17-review-of-deep-reinforcement-learning-cs-294-112-at-berkeley/
   8. http://karpathy.github.io/2016/05/31/rl/
   9. http://danieltakeshi.github.io/2015-08-02-markov-decision-processes-and-reinforcement-learning/
  10. http://www.research.rutgers.edu/~thomaswa/pub/geramifard13tutorial.pdf
  11. http://incompleteideas.net/book/the-book-2nd.html
  12. https://danieltakeshi.github.io/2015-12-21-review-of-advanced-robotics-cs-287-at-berkeley/
  13. https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node44.html
  14. https://kevinzakka.github.io/2016/09/26/applying-deep-learning/
  15. https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-id24-and-linear-function-approximation/#fn:ng
  16. https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-id24-and-linear-function-approximation/#fnref:ng
  17. https://disqus.com/?ref_noscript
  18. mailto:seita@cs.berkeley.edu
  19. https://github.com/danieltakeshi
  20. https://twitter.com/(never!)

   hidden links:
  22. https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-id24-and-linear-function-approximation/
