   #[1]tensorflow

   (button)
   [2]tensorflow
     *

   [3]install [4]learn
     * [5]introduction
       new to tensorflow?
     * [6]tensorflow
       the core open source ml library
     * [7]for javascript
       tensorflow.js for ml using javascript
     * [8]for mobile & iot
       tensorflow lite for mobile and embedded devices
     * [9]for production
       tensorflow extended for end-to-end ml components
     * [10]swift for tensorflow (in beta)

   [11]api
     * api r1
     * [12]r1.13 (stable)
     * [13]r1.12
     * [14]r1.11
     * [15]r1.10
     * [16]r1.9
     * [17]more   

     * api r2
     * [18]r2.0 (preview)

   [19]resources
     * [20]models & datasets
       pre-trained models and datasets built by google and the community
     * [21]tools
       ecosystem of tools to help you use tensorflow
     * [22]libraries & extensions
       libraries and extensions built on tensorflow

   [23]community [24]why tensorflow
     * [25]about
     * [26]case studies

   ____________________
   (button)
   (button)
   [27]github
     * [28]tensorflow core

   [29]overview [30]tutorials [31]guide [32]tf 2.0 alpha

   (button)
     * [33]install
     * [34]learn
          + more
          + [35]overview
          + [36]tutorials
          + [37]guide
          + [38]tf 2.0 alpha
     * [39]api
          + more
     * [40]resources
          + more
     * [41]community
     * [42]why tensorflow
          + more
     * [43]github

     * [44]tensorflow guide
     * high level apis
     * [45]keras
     * [46]eager execution
     * [47]importing data
     * [48]introduction to estimators
     * estimators
     * [49]premade estimators
     * [50]checkpoints
     * [51]feature columns
     * [52]datasets for estimators
     * [53]creating custom estimators
     * accelerators
     * [54]distribute strategy
     * [55]using gpus
     * [56]using tpus
     * low level apis
     * [57]introduction
     * [58]tensors
     * [59]variables
     * [60]graphs and sessions
     * [61]save and restore
     * [62]control flow
     * [63]ragged tensors
     * ml concepts
     * [64]embeddings
     * debugging
     * [65]tensorflow debugger
     * performance
     * [66]overview
     * [67]data input pipeline
     * [68]benchmarks
     * extend
     * [69]tensorflow architecture
     * [70]create an op
     * [71]custom filesystem plugin
     * [72]custom file and record formats
     * [73]model files
     * [74]language bindings
     * [75]c++ guide
     * misc
     * [76]tensorflow version compatibility

     * [77]introduction
     * [78]tensorflow
     * [79]for javascript
     * [80]for mobile & iot
     * [81]for production
     * [82]swift for tensorflow (in beta)

     * api r1
     * [83]r1.13 (stable)
     * [84]r1.12
     * [85]r1.11
     * [86]r1.10
     * [87]r1.9
     * [88]more   
     * api r2
     * [89]r2.0 (preview)

     * [90]models & datasets
     * [91]tools
     * [92]libraries & extensions

     * [93]about
     * [94]case studies

   watch talks from the 2019 tensorflow dev summit [95]watch now
     * [96]tensorflow
     * [97]learn
     * [98]tensorflow core
     * [99]guide

graphs and sessions

   tensorflow uses a dataflow graph to represent your computation in terms
   of the dependencies between individual operations. this leads to a
   low-level programming model in which you first define the dataflow
   graph, then create a tensorflow session to run parts of the graph
   across a set of local and remote devices.

   this guide will be most useful if you intend to use the low-level
   programming model directly. higher-level apis such as
   [100]tf.estimator.estimator and keras hide the details of graphs and
   sessions from the end user, but this guide may also be useful if you
   want to understand how these apis are implemented.

why dataflow graphs?

   [101]dataflow is a common programming model for parallel computing. in
   a dataflow graph, the nodes represent units of computation, and the
   edges represent the data consumed or produced by a computation. for
   example, in a tensorflow graph, the [102]tf.matmul operation would
   correspond to a single node with two incoming edges (the matrices to be
   multiplied) and one outgoing edge (the result of the multiplication).

   dataflow has several advantages that tensorflow leverages when
   executing your programs:
     * parallelism. by using explicit edges to represent dependencies
       between operations, it is easy for the system to identify
       operations that can execute in parallel.
     * distributed execution. by using explicit edges to represent the
       values that flow between operations, it is possible for tensorflow
       to partition your program across multiple devices (cpus, gpus, and
       tpus) attached to different machines. tensorflow inserts the
       necessary communication and coordination between devices.
     * compilation. tensorflow's [103]xla compiler can use the information
       in your dataflow graph to generate faster code, for example, by
       fusing together adjacent operations.
     * portability. the dataflow graph is a language-independent
       representation of the code in your model. you can build a dataflow
       graph in python, store it in a [104]savedmodel, and restore it in a
       c++ program for low-latency id136.

what is a [105]tf.graph?

   a [106]tf.graph contains two relevant kinds of information:
     * graph structure. the nodes and edges of the graph, indicating how
       individual operations are composed together, but not prescribing
       how they should be used. the graph structure is like assembly code:
       inspecting it can convey some useful information, but it does not
       contain all of the useful context that source code conveys.
     * graph collections. tensorflow provides a general mechanism for
       storing collections of metadata in a [107]tf.graph. the
       [108]tf.add_to_collection function enables you to associate a list
       of objects with a key (where [109]tf.graphkeys defines some of the
       standard keys), and [110]tf.get_collection enables you to look up
       all objects associated with a key. many parts of the tensorflow
       library use this facility: for example, when you create a
       [111]tf.variable, it is added by default to collections
       representing "global variables" and "trainable variables". when you
       later come to create a [112]tf.train.saver or
       [113]tf.train.optimizer, the variables in these collections are
       used as the default arguments.

building a [114]tf.graph

   most tensorflow programs start with a dataflow graph construction
   phase. in this phase, you invoke tensorflow api functions that
   construct new [115]tf.operation (node) and [116]tf.tensor (edge)
   objects and add them to a [117]tf.graph instance. tensorflow provides a
   default graph that is an implicit argument to all api functions in the
   same context. for example:
     * calling tf.constant(42.0) creates a single [118]tf.operation that
       produces the value 42.0, adds it to the default graph, and returns
       a [119]tf.tensor that represents the value of the constant.
     * calling tf.matmul(x, y) creates a single [120]tf.operation that
       multiplies the values of [121]tf.tensor objects x and y, adds it to
       the default graph, and returns a [122]tf.tensor that represents the
       result of the multiplication.
     * executing v = tf.variable(0) adds to the graph a [123]tf.operation
       that will store a writeable tensor value that persists between
       [124]tf.session.run calls. the [125]tf.variable object wraps this
       operation, and can be used [126]like a tensor, which will read the
       current value of the stored value. the [127]tf.variable object also
       has methods such as [128]tf.variable.assign and
       [129]tf.variable.assign_add that create [130]tf.operation objects
       that, when executed, update the stored value. (see [131]variables
       for more information about variables.)
     * calling [132]tf.train.optimizer.minimize will add operations and
       tensors to the default graph that calculates gradients, and return
       a [133]tf.operation that, when run, will apply those gradients to a
       set of variables.

   most programs rely solely on the default graph. however, see
   [134]dealing with multiple graphs for more advanced use cases.
   high-level apis such as the [135]tf.estimator.estimator api manage the
   default graph on your behalf, and--for example--may create different
   graphs for training and evaluation.

   note: calling most functions in the tensorflow api merely adds
   operations and tensors to the default graph, but does not perform the
   actual computation. instead, you compose these functions until you have
   a [136]tf.tensor or [137]tf.operation that represents the overall
   computation--such as performing one step of id119--and then
   pass that object to a [138]tf.session to perform the computation. see
   the section "executing a graph in a [139]tf.session" for more details.

naming operations

   a [140]tf.graph object defines a namespace for the [141]tf.operation
   objects it contains. tensorflow automatically chooses a unique name for
   each operation in your graph, but giving operations descriptive names
   can make your program easier to read and debug. the tensorflow api
   provides two ways to override the name of an operation:
     * each api function that creates a new [142]tf.operation or returns a
       new [143]tf.tensor accepts an optional name argument. for example,
       tf.constant(42.0, name="answer") creates a new [144]tf.operation
       named "answer" and returns a [145]tf.tensor named "answer:0". if
       the default graph already contains an operation named "answer",
       then tensorflow would append "_1", "_2", and so on to the name, in
       order to make it unique.
     * the [146]tf.name_scope function makes it possible to add a name
       scope prefix to all operations created in a particular context. the
       current name scope prefix is a "/"-delimited list of the names of
       all active [147]tf.name_scope context managers. if a name scope has
       already been used in the current context, tensorflow appends "_1",
       "_2", and so on. for example:

c_0 = tf.constant(0, name="c")  # => operation named "c"

# already-used names will be "uniquified".
c_1 = tf.constant(2, name="c")  # => operation named "c_1"

# name scopes add a prefix to all operations created in the same context.
with tf.name_scope("outer"):
  c_2 = tf.constant(2, name="c")  # => operation named "outer/c"

  # name scopes nest like paths in a hierarchical file system.
  with tf.name_scope("inner"):
    c_3 = tf.constant(3, name="c")  # => operation named "outer/inner/c"

  # exiting a name scope context will return to the previous prefix.
  c_4 = tf.constant(4, name="c")  # => operation named "outer/c_1"

  # already-used name scopes will be "uniquified".
  with tf.name_scope("inner"):
    c_5 = tf.constant(5, name="c")  # => operation named "outer/inner_1/c"

   the graph visualizer uses name scopes to group operations and reduce
   the visual complexity of a graph. see [148]visualizing your graph for
   more information.

   note that [149]tf.tensor objects are implicitly named after the
   [150]tf.operation that produces the tensor as output. a tensor name has
   the form "<op_name>:<i>" where:
     * "<op_name>" is the name of the operation that produces it.
     * "<i>" is an integer representing the index of that tensor among the
       operation's outputs.

placing operations on different devices

   if you want your tensorflow program to use multiple different devices,
   the [151]tf.device function provides a convenient way to request that
   all operations created in a particular context are placed on the same
   device (or type of device).

   a device specification has the following form:
/job:<job_name>/task:<task_index>/device:<device_type>:<device_index>

   where:
     * <job_name> is an alpha-numeric string that does not start with a
       number.
     * <device_type> is a registered device type (such as gpu or cpu).
     * <task_index> is a non-negative integer representing the index of
       the task in the job named <job_name>. see [152]tf.train.clusterspec
       for an explanation of jobs and tasks.
     * <device_index> is a non-negative integer representing the index of
       the device, for example, to distinguish between different gpu
       devices used in the same process.

   you do not need to specify every part of a device specification. for
   example, if you are running in a single-machine configuration with a
   single gpu, you might use [153]tf.device to pin some operations to the
   cpu and gpu:
# operations created outside either context will run on the "best possible"
# device. for example, if you have a gpu and a cpu available, and the operation
# has a gpu implementation, tensorflow will choose the gpu.
weights = tf.random_normal(...)

with tf.device("/device:cpu:0"):
  # operations created in this context will be pinned to the cpu.
  img = tf.decode_jpeg(tf.read_file("img.jpg"))

with tf.device("/device:gpu:0"):
  # operations created in this context will be pinned to the gpu.
  result = tf.matmul(weights, img)

   if you are deploying tensorflow in a [154]typical distributed
   configuration, you might specify the job name and task id to place
   variables on a task in the parameter server job ("/job:ps"), and the
   other operations on task in the worker job ("/job:worker"):
with tf.device("/job:ps/task:0"):
  weights_1 = tf.variable(tf.truncated_normal([784, 100]))
  biases_1 = tf.variable(tf.zeros([100]))

with tf.device("/job:ps/task:1"):
  weights_2 = tf.variable(tf.truncated_normal([100, 10]))
  biases_2 = tf.variable(tf.zeros([10]))

with tf.device("/job:worker"):
  layer_1 = tf.matmul(train_batch, weights_1) + biases_1
  layer_2 = tf.matmul(train_batch, weights_2) + biases_2

   [155]tf.device gives you a lot of flexibility to choose placements for
   individual operations or broad regions of a tensorflow graph. in many
   cases, there are simple heuristics that work well. for example, the
   [156]tf.train.replica_device_setter api can be used with [157]tf.device
   to place operations for data-parallel distributed training. for
   example, the following code fragment shows how
   [158]tf.train.replica_device_setter applies different placement
   policies to [159]tf.variable objects and other operations:
with tf.device(tf.train.replica_device_setter(ps_tasks=3)):
  # tf.variable objects are, by default, placed on tasks in "/job:ps" in a
  # round-robin fashion.
  w_0 = tf.variable(...)  # placed on "/job:ps/task:0"
  b_0 = tf.variable(...)  # placed on "/job:ps/task:1"
  w_1 = tf.variable(...)  # placed on "/job:ps/task:2"
  b_1 = tf.variable(...)  # placed on "/job:ps/task:0"

  input_data = tf.placeholder(tf.float32)     # placed on "/job:worker"
  layer_0 = tf.matmul(input_data, w_0) + b_0  # placed on "/job:worker"
  layer_1 = tf.matmul(layer_0, w_1) + b_1     # placed on "/job:worker"

tensor-like objects

   many tensorflow operations take one or more [160]tf.tensor objects as
   arguments. for example, [161]tf.matmul takes two [162]tf.tensor
   objects, and [163]tf.add_n takes a list of n [164]tf.tensor objects.
   for convenience, these functions will accept a tensor-like object in
   place of a [165]tf.tensor, and implicitly convert it to a
   [166]tf.tensor using the [167]tf.convert_to_tensor method. tensor-like
   objects include elements of the following types:
     * [168]tf.tensor
     * [169]tf.variable
     * [170]numpy.ndarray
     * list (and lists of tensor-like objects)
     * scalar python types: bool, float, int, str

   you can register additional tensor-like types using
   [171]tf.register_tensor_conversion_function.

   note: by default, tensorflow will create a new [172]tf.tensor each time
   you use the same tensor-like object. if the tensor-like object is large
   (e.g. a numpy.ndarray containing a set of training examples) and you
   use it multiple times, you may run out of memory. to avoid this,
   manually call [173]tf.convert_to_tensor on the tensor-like object once
   and use the returned [174]tf.tensor instead.

executing a graph in a [175]tf.session

   tensorflow uses the [176]tf.session class to represent a connection
   between the client program---typically a python program, although a
   similar interface is available in other languages---and the c++
   runtime. a [177]tf.session object provides access to devices in the
   local machine, and remote devices using the distributed tensorflow
   runtime. it also caches information about your [178]tf.graph so that
   you can efficiently run the same computation multiple times.

creating a [179]tf.session

   if you are using the low-level tensorflow api, you can create a
   [180]tf.session for the current default graph as follows:
# create a default in-process session.
with tf.session() as sess:
  # ...

# create a remote session.
with tf.session("grpc://example.org:2222"):
  # ...

   since a [181]tf.session owns physical resources (such as gpus and
   network connections), it is typically used as a context manager (in a
   with block) that automatically closes the session when you exit the
   block. it is also possible to create a session without using a with
   block, but you should explicitly call [182]tf.session.close when you
   are finished with it to free the resources.

   note: higher-level apis such as [183]tf.train.monitoredtrainingsession
   or [184]tf.estimator.estimator will create and manage a [185]tf.session
   for you. these apis accept optional target and config arguments (either
   directly, or as part of a [186]tf.estimator.runconfig object), with the
   same meaning as described below.

   [187]tf.session.init accepts three optional arguments:
     * target. if this argument is left empty (the default), the session
       will only use devices in the local machine. however, you may also
       specify a grpc:// url to specify the address of a tensorflow
       server, which gives the session access to all devices on machines
       that this server controls. see [188]tf.train.server for details of
       how to create a tensorflow server. for example, in the common
       between-graph replication configuration, the [189]tf.session
       connects to a [190]tf.train.server in the same process as the
       client. the [191]distributed tensorflow deployment guide describes
       other common scenarios.
     * graph. by default, a new [192]tf.session will be bound to---and
       only able to run operations in---the current default graph. if you
       are using multiple graphs in your program (see [193]programming
       with multiple graphs for more details), you can specify an explicit
       [194]tf.graph when you construct the session.
     * config. this argument allows you to specify a [195]tf.configproto
       that controls the behavior of the session. for example, some of the
       configuration options include:
          + allow_soft_placement. set this to true to enable a "soft"
            device placement algorithm, which ignores [196]tf.device
            annotations that attempt to place cpu-only operations on a gpu
            device, and places them on the cpu instead.
          + cluster_def. when using distributed tensorflow, this option
            allows you to specify what machines to use in the computation,
            and provide a mapping between job names, task indices, and
            network addresses. see
            [197]tf.train.clusterspec.as_cluster_def for details.
          + graph_options.optimizer_options. provides control over the
            optimizations that tensorflow performs on your graph before
            executing it.
          + gpu_options.allow_growth. set this to true to change the gpu
            memory allocator so that it gradually increases the amount of
            memory allocated, rather than allocating most of the memory at
            startup.

using [198]tf.session.run to execute operations

   the [199]tf.session.run method is the main mechanism for running a
   [200]tf.operation or evaluating a [201]tf.tensor. you can pass one or
   more [202]tf.operation or [203]tf.tensor objects to
   [204]tf.session.run, and tensorflow will execute the operations that
   are needed to compute the result.

   [205]tf.session.run requires you to specify a list of fetches, which
   determine the return values, and may be a [206]tf.operation, a
   [207]tf.tensor, or a [208]tensor-like type such as [209]tf.variable.
   these fetches determine what subgraph of the overall [210]tf.graph must
   be executed to produce the result: this is the subgraph that contains
   all operations named in the fetch list, plus all operations whose
   outputs are used to compute the value of the fetches. for example, the
   following code fragment shows how different arguments to
   [211]tf.session.run cause different subgraphs to be executed:
x = tf.constant([[37.0, -23.0], [1.0, 4.0]])
w = tf.variable(tf.random_uniform([2, 2]))
y = tf.matmul(x, w)
output = tf.nn.softmax(y)
init_op = w.initializer

with tf.session() as sess:
  # run the initializer on `w`.
  sess.run(init_op)

  # evaluate `output`. `sess.run(output)` will return a numpy array containing
  # the result of the computation.
  print(sess.run(output))

  # evaluate `y` and `output`. note that `y` will only be computed once, and its
  # result used both to return `y_val` and as an input to the `tf.nn.softmax()`
  # op. both `y_val` and `output_val` will be numpy arrays.
  y_val, output_val = sess.run([y, output])

   [212]tf.session.run also optionally takes a dictionary of feeds, which
   is a mapping from [213]tf.tensor objects (typically [214]tf.placeholder
   tensors) to values (typically python scalars, lists, or numpy arrays)
   that will be substituted for those tensors in the execution. for
   example:
# define a placeholder that expects a vector of three floating-point values,
# and a computation that depends on it.
x = tf.placeholder(tf.float32, shape=[3])
y = tf.square(x)

with tf.session() as sess:
  # feeding a value changes the result that is returned when you evaluate `y`.
  print(sess.run(y, {x: [1.0, 2.0, 3.0]}))  # => "[1.0, 4.0, 9.0]"
  print(sess.run(y, {x: [0.0, 0.0, 5.0]}))  # => "[0.0, 0.0, 25.0]"

  # raises <a href="./../api_docs/python/tf/errors/invalidargumenterror"><code>t
f.errors.invalidargumenterror</code></a>, because you must feed a value for
  # a `tf.placeholder()` when evaluating a tensor that depends on it.
  sess.run(y)

  # raises `valueerror`, because the shape of `37.0` does not match the shape
  # of placeholder `x`.
  sess.run(y, {x: 37.0})

   [215]tf.session.run also accepts an optional options argument that
   enables you to specify options about the call, and an optional
   run_metadata argument that enables you to collect metadata about the
   execution. for example, you can use these options together to collect
   tracing information about the execution:
y = tf.matmul([[37.0, -23.0], [1.0, 4.0]], tf.random_uniform([2, 2]))

with tf.session() as sess:
  # define options for the `sess.run()` call.
  options = tf.runoptions()
  options.output_partition_graphs = true
  options.trace_level = tf.runoptions.full_trace

  # define a container for the returned metadata.
  metadata = tf.runmetadata()

  sess.run(y, options=options, run_metadata=metadata)

  # print the subgraphs that executed on each device.
  print(metadata.partition_graphs)

  # print the timings of each operation that executed.
  print(metadata.step_stats)

visualizing your graph

   tensorflow includes tools that can help you to understand the code in a
   graph. the graph visualizer is a component of tensorboard that renders
   the structure of your graph visually in a browser. the easiest way to
   create a visualization is to pass a [216]tf.graph when creating the
   [217]tf.summary.filewriter:
# build your graph.
x = tf.constant([[37.0, -23.0], [1.0, 4.0]])
w = tf.variable(tf.random_uniform([2, 2]))
y = tf.matmul(x, w)
# ...
loss = ...
train_op = tf.train.adagradoptimizer(0.01).minimize(loss)

with tf.session() as sess:
  # `sess.graph` provides access to the graph used in a <a href="./../api_docs/p
ython/tf/session"><code>tf.session</code></a>.
  writer = tf.summary.filewriter("/tmp/log/...", sess.graph)

  # perform your computation...
  for i in range(1000):
    sess.run(train_op)
    # ...

  writer.close()

   note: if you are using a [218]tf.estimator.estimator, the graph (and
   any summaries) will be logged automatically to the model_dir that you
   specified when creating the estimator.

   you can then open the log in tensorboard, navigate to the "graph" tab,
   and see a high-level visualization of your graph's structure. note that
   a typical tensorflow graph---especially training graphs with
   automatically computed gradients---has too many nodes to visualize at
   once. the graph visualizer makes use of name scopes to group related
   operations into "super" nodes. you can click on the orange "+" button
   on any of these super nodes to expand the subgraph inside.

   for more information about visualizing your tensorflow application with
   tensorboard, see the [219]tensorboard guide.

programming with multiple graphs

   note: when training a model, a common way of organizing your code is to
   use one graph for training your model, and a separate graph for
   evaluating or performing id136 with a trained model. in many cases,
   the id136 graph will be different from the training graph: for
   example, techniques like dropout and batch id172 use different
   operations in each case. furthermore, by default utilities like
   [220]tf.train.saver use the names of [221]tf.variable objects (which
   have names based on an underlying [222]tf.operation) to identify each
   variable in a saved checkpoint. when programming this way, you can
   either use completely separate python processes to build and execute
   the graphs, or you can use multiple graphs in the same process. this
   section describes how to use multiple graphs in the same process.

   as noted above, tensorflow provides a "default graph" that is
   implicitly passed to all api functions in the same context. for many
   applications, a single graph is sufficient. however, tensorflow also
   provides methods for manipulating the default graph, which can be
   useful in more advanced use cases. for example:
     * a [223]tf.graph defines the namespace for [224]tf.operation
       objects: each operation in a single graph must have a unique name.
       tensorflow will "uniquify" the names of operations by appending
       "_1", "_2", and so on to their names if the requested name is
       already taken. using multiple explicitly created graphs gives you
       more control over what name is given to each operation.
     * the default graph stores information about every [225]tf.operation
       and [226]tf.tensor that was ever added to it. if your program
       creates a large number of unconnected subgraphs, it may be more
       efficient to use a different [227]tf.graph to build each subgraph,
       so that unrelated state can be garbage collected.

   you can install a different [228]tf.graph as the default graph, using
   the [229]tf.graph.as_default context manager:
g_1 = tf.graph()
with g_1.as_default():
  # operations created in this scope will be added to `g_1`.
  c = tf.constant("node in g_1")

  # sessions created in this scope will run operations from `g_1`.
  sess_1 = tf.session()

g_2 = tf.graph()
with g_2.as_default():
  # operations created in this scope will be added to `g_2`.
  d = tf.constant("node in g_2")

# alternatively, you can pass a graph when constructing a <a href="./../api_docs
/python/tf/session"><code>tf.session</code></a>:
# `sess_2` will run operations from `g_2`.
sess_2 = tf.session(graph=g_2)

assert c.graph is g_1
assert sess_1.graph is g_1

assert d.graph is g_2
assert sess_2.graph is g_2

   to inspect the current default graph, call [230]tf.get_default_graph,
   which returns a [231]tf.graph object:
# print all of the operations in the default graph.
g = tf.get_default_graph()
print(g.get_operations())

   except as otherwise noted, the content of this page is licensed under
   the [232]creative commons attribution 3.0 license, and code samples are
   licensed under the [233]apache 2.0 license. for details, see the
   [234]google developers site policies. java is a registered trademark of
   oracle and/or its affiliates.

     * stay connected
          + [235]blog
          + [236]github
          + [237]twitter
          + [238]youtube
     * support
          + [239]issue tracker
          + [240]release notes
          + [241]stack overflow
          + [242]brand guidelines

     * [243]terms
     * [244]privacy

   [english_____]

references

   visible links
   1. https://www.tensorflow.org/s/opensearch.xml
   2. https://www.tensorflow.org/
   3. https://www.tensorflow.org/install
   4. https://www.tensorflow.org/learn
   5. https://www.tensorflow.org/learn
   6. https://www.tensorflow.org/overview
   7. https://www.tensorflow.org/js
   8. https://www.tensorflow.org/lite
   9. https://www.tensorflow.org/tfx
  10. https://www.tensorflow.org/swift
  11. https://www.tensorflow.org/api_docs/python/tf
  12. https://www.tensorflow.org/api_docs/python/tf
  13. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  14. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  15. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  16. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  17. https://www.tensorflow.org/versions
  18. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  19. https://www.tensorflow.org/resources/models-datasets
  20. https://www.tensorflow.org/resources/models-datasets
  21. https://www.tensorflow.org/resources/tools
  22. https://www.tensorflow.org/resources/libraries-extensions
  23. https://www.tensorflow.org/community
  24. https://www.tensorflow.org/about
  25. https://www.tensorflow.org/about
  26. https://www.tensorflow.org/about/case-studies
  27. https://github.com/tensorflow
  28. https://www.tensorflow.org/overview
  29. https://www.tensorflow.org/overview
  30. https://www.tensorflow.org/tutorials
  31. https://www.tensorflow.org/guide
  32. https://www.tensorflow.org/alpha
  33. https://www.tensorflow.org/install
  34. https://www.tensorflow.org/learn
  35. https://www.tensorflow.org/overview
  36. https://www.tensorflow.org/tutorials
  37. https://www.tensorflow.org/guide
  38. https://www.tensorflow.org/alpha
  39. https://www.tensorflow.org/api_docs/python/tf
  40. https://www.tensorflow.org/resources/models-datasets
  41. https://www.tensorflow.org/community
  42. https://www.tensorflow.org/about
  43. https://github.com/tensorflow
  44. https://www.tensorflow.org/guide
  45. https://www.tensorflow.org/guide/keras
  46. https://www.tensorflow.org/guide/eager
  47. https://www.tensorflow.org/guide/datasets
  48. https://www.tensorflow.org/guide/estimators
  49. https://www.tensorflow.org/guide/premade_estimators
  50. https://www.tensorflow.org/guide/checkpoints
  51. https://www.tensorflow.org/guide/feature_columns
  52. https://www.tensorflow.org/guide/datasets_for_estimators
  53. https://www.tensorflow.org/guide/custom_estimators
  54. https://www.tensorflow.org/guide/distribute_strategy
  55. https://www.tensorflow.org/guide/using_gpu
  56. https://www.tensorflow.org/guide/using_tpu
  57. https://www.tensorflow.org/guide/low_level_intro
  58. https://www.tensorflow.org/guide/tensors
  59. https://www.tensorflow.org/guide/variables
  60. https://www.tensorflow.org/guide/graphs
  61. https://www.tensorflow.org/guide/saved_model
  62. https://www.tensorflow.org/guide/autograph
  63. https://www.tensorflow.org/guide/ragged_tensors
  64. https://www.tensorflow.org/guide/embedding
  65. https://www.tensorflow.org/guide/debugger
  66. https://www.tensorflow.org/guide/performance/overview
  67. https://www.tensorflow.org/guide/performance/datasets
  68. https://www.tensorflow.org/guide/performance/benchmarks
  69. https://www.tensorflow.org/guide/extend/architecture
  70. https://www.tensorflow.org/guide/extend/op
  71. https://www.tensorflow.org/guide/extend/filesystem
  72. https://www.tensorflow.org/guide/extend/formats
  73. https://www.tensorflow.org/guide/extend/model_files
  74. https://www.tensorflow.org/guide/extend/bindings
  75. https://www.tensorflow.org/guide/extend/cc
  76. https://www.tensorflow.org/guide/version_compat
  77. https://www.tensorflow.org/learn
  78. https://www.tensorflow.org/overview
  79. https://www.tensorflow.org/js
  80. https://www.tensorflow.org/lite
  81. https://www.tensorflow.org/tfx
  82. https://www.tensorflow.org/swift
  83. https://www.tensorflow.org/api_docs/python/tf
  84. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  85. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  86. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  87. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  88. https://www.tensorflow.org/versions
  89. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  90. https://www.tensorflow.org/resources/models-datasets
  91. https://www.tensorflow.org/resources/tools
  92. https://www.tensorflow.org/resources/libraries-extensions
  93. https://www.tensorflow.org/about
  94. https://www.tensorflow.org/about/case-studies
  95. https://www.youtube.com/playlist?list=plqy2h8rroyvzouyi26khmksjbedn3squb
  96. https://www.tensorflow.org/
  97. https://www.tensorflow.org/learn
  98. https://www.tensorflow.org/overview
  99. https://www.tensorflow.org/guide
 100. https://www.tensorflow.org/api_docs/python/tf/estimator/estimator
 101. https://en.wikipedia.org/wiki/dataflow_programming
 102. https://www.tensorflow.org/api_docs/python/tf/linalg/matmul
 103. https://www.tensorflow.org/performance/xla/index
 104. https://www.tensorflow.org/guide/saved_model
 105. https://www.tensorflow.org/api_docs/python/tf/graph
 106. https://www.tensorflow.org/api_docs/python/tf/graph
 107. https://www.tensorflow.org/api_docs/python/tf/graph
 108. https://www.tensorflow.org/api_docs/python/tf/add_to_collection
 109. https://www.tensorflow.org/api_docs/python/tf/graphkeys
 110. https://www.tensorflow.org/api_docs/python/tf/get_collection
 111. https://www.tensorflow.org/api_docs/python/tf/variable
 112. https://www.tensorflow.org/api_docs/python/tf/train/saver
 113. https://www.tensorflow.org/api_docs/python/tf/train/optimizer
 114. https://www.tensorflow.org/api_docs/python/tf/graph
 115. https://www.tensorflow.org/api_docs/python/tf/operation
 116. https://www.tensorflow.org/api_docs/python/tf/tensor
 117. https://www.tensorflow.org/api_docs/python/tf/graph
 118. https://www.tensorflow.org/api_docs/python/tf/operation
 119. https://www.tensorflow.org/api_docs/python/tf/tensor
 120. https://www.tensorflow.org/api_docs/python/tf/operation
 121. https://www.tensorflow.org/api_docs/python/tf/tensor
 122. https://www.tensorflow.org/api_docs/python/tf/tensor
 123. https://www.tensorflow.org/api_docs/python/tf/operation
 124. https://www.tensorflow.org/api_docs/python/tf/session#run
 125. https://www.tensorflow.org/api_docs/python/tf/variable
 126. https://www.tensorflow.org/guide/graphs#tensor-like-objects
 127. https://www.tensorflow.org/api_docs/python/tf/variable
 128. https://www.tensorflow.org/api_docs/python/tf/variable#assign
 129. https://www.tensorflow.org/api_docs/python/tf/variable#assign_add
 130. https://www.tensorflow.org/api_docs/python/tf/operation
 131. https://www.tensorflow.org/guide/variables
 132. https://www.tensorflow.org/api_docs/python/tf/train/optimizer#minimize
 133. https://www.tensorflow.org/api_docs/python/tf/operation
 134. https://www.tensorflow.org/guide/graphs#programming_with_multiple_graphs
 135. https://www.tensorflow.org/api_docs/python/tf/estimator/estimator
 136. https://www.tensorflow.org/api_docs/python/tf/tensor
 137. https://www.tensorflow.org/api_docs/python/tf/operation
 138. https://www.tensorflow.org/api_docs/python/tf/session
 139. https://www.tensorflow.org/api_docs/python/tf/session
 140. https://www.tensorflow.org/api_docs/python/tf/graph
 141. https://www.tensorflow.org/api_docs/python/tf/operation
 142. https://www.tensorflow.org/api_docs/python/tf/operation
 143. https://www.tensorflow.org/api_docs/python/tf/tensor
 144. https://www.tensorflow.org/api_docs/python/tf/operation
 145. https://www.tensorflow.org/api_docs/python/tf/tensor
 146. https://www.tensorflow.org/api_docs/python/tf/name_scope
 147. https://www.tensorflow.org/api_docs/python/tf/name_scope
 148. https://www.tensorflow.org/guide/graphs#visualizing_your_graph
 149. https://www.tensorflow.org/api_docs/python/tf/tensor
 150. https://www.tensorflow.org/api_docs/python/tf/operation
 151. https://www.tensorflow.org/api_docs/python/tf/device
 152. https://www.tensorflow.org/api_docs/python/tf/train/clusterspec
 153. https://www.tensorflow.org/api_docs/python/tf/device
 154. https://www.tensorflow.org/deploy/distributed
 155. https://www.tensorflow.org/api_docs/python/tf/device
 156. https://www.tensorflow.org/api_docs/python/tf/train/replica_device_setter
 157. https://www.tensorflow.org/api_docs/python/tf/device
 158. https://www.tensorflow.org/api_docs/python/tf/train/replica_device_setter
 159. https://www.tensorflow.org/api_docs/python/tf/variable
 160. https://www.tensorflow.org/api_docs/python/tf/tensor
 161. https://www.tensorflow.org/api_docs/python/tf/linalg/matmul
 162. https://www.tensorflow.org/api_docs/python/tf/tensor
 163. https://www.tensorflow.org/api_docs/python/tf/math/add_n
 164. https://www.tensorflow.org/api_docs/python/tf/tensor
 165. https://www.tensorflow.org/api_docs/python/tf/tensor
 166. https://www.tensorflow.org/api_docs/python/tf/tensor
 167. https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor
 168. https://www.tensorflow.org/api_docs/python/tf/tensor
 169. https://www.tensorflow.org/api_docs/python/tf/variable
 170. https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html
 171. https://www.tensorflow.org/api_docs/python/tf/register_tensor_conversion_function
 172. https://www.tensorflow.org/api_docs/python/tf/tensor
 173. https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor
 174. https://www.tensorflow.org/api_docs/python/tf/tensor
 175. https://www.tensorflow.org/api_docs/python/tf/session
 176. https://www.tensorflow.org/api_docs/python/tf/session
 177. https://www.tensorflow.org/api_docs/python/tf/session
 178. https://www.tensorflow.org/api_docs/python/tf/graph
 179. https://www.tensorflow.org/api_docs/python/tf/session
 180. https://www.tensorflow.org/api_docs/python/tf/session
 181. https://www.tensorflow.org/api_docs/python/tf/session
 182. https://www.tensorflow.org/api_docs/python/tf/session#close
 183. https://www.tensorflow.org/api_docs/python/tf/train/monitoredtrainingsession
 184. https://www.tensorflow.org/api_docs/python/tf/estimator/estimator
 185. https://www.tensorflow.org/api_docs/python/tf/session
 186. https://www.tensorflow.org/api_docs/python/tf/estimator/runconfig
 187. https://www.tensorflow.org/api_docs/python/tf/session#__init__
 188. https://www.tensorflow.org/api_docs/python/tf/distribute/server
 189. https://www.tensorflow.org/api_docs/python/tf/session
 190. https://www.tensorflow.org/api_docs/python/tf/distribute/server
 191. https://www.tensorflow.org/deploy/distributed
 192. https://www.tensorflow.org/api_docs/python/tf/session
 193. https://www.tensorflow.org/guide/graphs#programming_with_multiple_graphs
 194. https://www.tensorflow.org/api_docs/python/tf/graph
 195. https://www.tensorflow.org/api_docs/python/tf/configproto
 196. https://www.tensorflow.org/api_docs/python/tf/device
 197. https://www.tensorflow.org/api_docs/python/tf/train/clusterspec#as_cluster_def
 198. https://www.tensorflow.org/api_docs/python/tf/session#run
 199. https://www.tensorflow.org/api_docs/python/tf/session#run
 200. https://www.tensorflow.org/api_docs/python/tf/operation
 201. https://www.tensorflow.org/api_docs/python/tf/tensor
 202. https://www.tensorflow.org/api_docs/python/tf/operation
 203. https://www.tensorflow.org/api_docs/python/tf/tensor
 204. https://www.tensorflow.org/api_docs/python/tf/session#run
 205. https://www.tensorflow.org/api_docs/python/tf/session#run
 206. https://www.tensorflow.org/api_docs/python/tf/operation
 207. https://www.tensorflow.org/api_docs/python/tf/tensor
 208. https://www.tensorflow.org/guide/graphs#tensor_like_objects
 209. https://www.tensorflow.org/api_docs/python/tf/variable
 210. https://www.tensorflow.org/api_docs/python/tf/graph
 211. https://www.tensorflow.org/api_docs/python/tf/session#run
 212. https://www.tensorflow.org/api_docs/python/tf/session#run
 213. https://www.tensorflow.org/api_docs/python/tf/tensor
 214. https://www.tensorflow.org/api_docs/python/tf/placeholder
 215. https://www.tensorflow.org/api_docs/python/tf/session#run
 216. https://www.tensorflow.org/api_docs/python/tf/graph
 217. https://www.tensorflow.org/api_docs/python/tf/summary/filewriter
 218. https://www.tensorflow.org/api_docs/python/tf/estimator/estimator
 219. https://www.tensorflow.org/guide/summaries_and_tensorboard
 220. https://www.tensorflow.org/api_docs/python/tf/train/saver
 221. https://www.tensorflow.org/api_docs/python/tf/variable
 222. https://www.tensorflow.org/api_docs/python/tf/operation
 223. https://www.tensorflow.org/api_docs/python/tf/graph
 224. https://www.tensorflow.org/api_docs/python/tf/operation
 225. https://www.tensorflow.org/api_docs/python/tf/operation
 226. https://www.tensorflow.org/api_docs/python/tf/tensor
 227. https://www.tensorflow.org/api_docs/python/tf/graph
 228. https://www.tensorflow.org/api_docs/python/tf/graph
 229. https://www.tensorflow.org/api_docs/python/tf/graph#as_default
 230. https://www.tensorflow.org/api_docs/python/tf/get_default_graph
 231. https://www.tensorflow.org/api_docs/python/tf/graph
 232. https://creativecommons.org/licenses/by/3.0/
 233. https://www.apache.org/licenses/license-2.0
 234. https://developers.google.com/site-policies
 235. https://medium.com/tensorflow
 236. https://github.com/tensorflow/
 237. https://twitter.com/tensorflow
 238. https://youtube.com/tensorflow
 239. https://github.com/tensorflow/tensorflow/issues
 240. https://github.com/tensorflow/tensorflow/blob/master/release.md
 241. https://stackoverflow.com/questions/tagged/tensorflow
 242. https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf
 243. https://policies.google.com/terms
 244. https://policies.google.com/privacy

   hidden links:
 246. https://www.tensorflow.org/guide/graphs
 247. https://www.tensorflow.org/guide/graphs
 248. https://www.tensorflow.org/guide/graphs
 249. https://www.tensorflow.org/guide/graphs
