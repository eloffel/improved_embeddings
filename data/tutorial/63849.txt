   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]praemineo
   [7]praemineo
   [8]sign in[9]get started
     __________________________________________________________________

id38 using recurrent neural networks part - 1

   [10]go to the profile of tushar pawar
   [11]tushar pawar (button) blockedunblock (button) followfollowing
   dec 3, 2017

   this is a 3 part series where i will cover
    1. introduction to id56 and lstm. (this post)
    2. [12]building a character by character language model using
       tensorflow.
    3. building a word by word language model using keras.

   first of all, lets get motivated to learn recurrent neural
   networks(id56s) by knowing what they can do and how robust and sometimes
   surprisingly effective they can be. [13]this amazing blog by andrej
   karpathy will help you get started.

language modelling

   our aim here is to make a neural network that can learn the structure
   and syntax of language. we   ll provide a huge set of dialogues from the
   scripts of 2 of my favorite shows f.r.i.e.n.d.s and south park as
   training data and hope that our model learns to talk like the
   characters.

id56s

   basic layered neural networks are used when there are a set of distinct
   inputs and we expect a class or a real number etc.. assuming that all
   inputs are independent of each other. id56s on the other hand, are used
   when the inputs are sequential. this is perfect for modelling languages
   because language is a sequence of words and each next word is dependent
   on the words that come before it. if we want to predict the next word
   in a sentence, we should know what were the previous words. this is
   analogues to the fact that the human brain does not start thinking from
   scratch for every word we say. our thoughts have persistence. we   ll
   give this property of persistence to the neural network with the use of
   an id56.

   a very famous example is    i had a good time in france. i also learned
   to speak some _________   . if we want to predict what word will go in
   the blank, we have to go all the way to the word france and then
   conclude that the most likely word will be french. in other words, we
   have to have some memory of our previous outputs and calculate new
   outputs based on our past outputs.

   another advantage of using id56s for language modelling is that due to
   memory constraints, they are limited to remembering only a few steps
   back. this is ideal because the context of the word can be captured in
   the 8   10 words before it. we don   t have to remember 50 words of
   context. although id56s can be used for arbitrarily long sequences if
   you have memory at your disposal.

   the general structure of a id56 looks like..
   [1*gqiyk---rzwucp3nxzketa.png]
   structure of basic id56

   while designing the id56, we only have to care about the top structure.
   the structure on the bottom is the unrolled version of an id56 through
   time. it means that if we want to remember 8 words in the past, it will
   become a 8 layer neural network. x    is input to the network at time
   step t. x    is the input at index 1 in the sequence. s    hidden state of
   the network at time t. it is the memory of the network. this
   corresponds to the weights in a normal neural network which are learned
   at the time of training. it is calculated using u, the current input
   and w, the output of previous time step. s    = f(ux    + wx         ). where f
   is a non-linearity function, typically, tanh or relu. the fact that
   u,v, and w are same for each time step denotes that the network is
   sharing the same parameters for each layer. therefore, it is performing
   the same operations, but with different values of u,v and w at each
   time step.

id56 cons

   till now we discussed all the things that id56s can do. but they are not
   perfect. as the context length increases, ie. our dependencies become
   longer, layers in the unrolled id56 also increase. consequently, the
   network suffers from vanishing gradient problem. as the network becomes
   deeper, the gradients flowing back in the id26 step become
   smaller. as a result, the learning rate becomes really slow and makes
   it infeasible to expect long term dependencies of the language. lstms
   to the rescue!

long-short term memory (lstm)

   id137 are just an advanced version of plain id56s that we
   discussed above. these networks are capable of remembering long-term
   dependencies. they are designed to remember information for long
   periods of time without having to deal with the vanishing gradient
   problem. the structural difference between plain id56 and lstm is as
   follows.
   [1*0hs3gxs9thblny81wt9gha.png]
   basic id56 structure

   in the basic id56 network, each layer consists of a simple function
   f(ux   +wx         ), where f is an activation function such as tanh or relu.
   while training the network, it backpropagates through all the layers
   and consequently suffers vanishing gradient problem.
   [1*h7zd0o4vw9lfazeuqhycfw.png]

   an lstm cell consists of some additional layers to basic id56 cell.
   these additional layers form what is called as a memory and forget
   gates.

   in the lstm cell, the vanishing gradient problem is solved by writing
   the current state as memory of the network. this writing process is
   regulated by memory and forget gates. if you want to remember
   something, you write it down. similarly, lstms also have the same
   intuition. however, writing down every step is tedious and can
   certainly go out of hands if not regulated. it will blow up the memory
   constraints if the    writing down    process is not regulated and
   everything is written.

   to tackle this problem, lstms are designed to be selective in three
   things.
     * writing selectively: only important things should be written down.
       the network learns which features are important and writes them
       down.
     * reading selectively: read only the features that are relevant to
       the current state.
     * forget selectively: we need to throw away the stuff that we don   t
       need anymore to make room for new stuff.

   all these actions are performed by the memory and forget gates.

   checkout this deeper explanation of structure of lstm by [14]r2rt.

   in the next part we   ll use tensorflow to make our own language model.

references:

    1. colah   s blog         [15]understanding id137
    2. andrej karpathy blog         [16]the unreasonable effectiveness of
       recurrent neural networks
    3. r2rt         [17]written memories: understanding, deriving and extending
       the lstm
    4. wildml         [18]recurrent neural networks tutorial

   thanks to [19]rohit parab.
     * [20]machine learning
     * [21]recurrent neural network
     * [22]neural networks
     * [23]lstm
     * [24]id56

   (button)
   (button)
   (button) 47 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of tushar pawar

[26]tushar pawar

   research engineer at [27]@preeminence, doer [28]https://tusharpawar.com

     (button) follow
   [29]praemineo

[30]praemineo

   praemineo

     * (button)
       (button) 47
     * (button)
     *
     *

   [31]praemineo
   never miss a story from praemineo, when you sign up for medium.
   [32]learn more
   never miss a story from praemineo
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/427b165576c2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/praemineo?source=avatar-lo_oxchhco8medk-43af8de1fd5c
   7. https://medium.com/praemineo?source=logo-lo_oxchhco8medk---43af8de1fd5c
   8. https://medium.com/m/signin?redirect=https://medium.com/praemineo/language-modeling-using-recurrent-neural-networks-part-1-427b165576c2&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/praemineo/language-modeling-using-recurrent-neural-networks-part-1-427b165576c2&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@backalla?source=post_header_lockup
  11. https://medium.com/@backalla
  12. https://medium.com/preeminence/language-modelling-using-recurrent-neural-networks-part-2-c13f5e07b6e
  13. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  14. https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
  15. http://colah.github.io/posts/2015-08-understanding-lstms/
  16. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  17. https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html
  18. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  19. https://medium.com/@rohitparab?source=post_page
  20. https://medium.com/tag/machine-learning?source=post
  21. https://medium.com/tag/recurrent-neural-network?source=post
  22. https://medium.com/tag/neural-networks?source=post
  23. https://medium.com/tag/lstm?source=post
  24. https://medium.com/tag/id56?source=post
  25. https://medium.com/@backalla?source=footer_card
  26. https://medium.com/@backalla
  27. http://twitter.com/preeminence
  28. https://tusharpawar.com/
  29. https://medium.com/praemineo?source=footer_card
  30. https://medium.com/praemineo?source=footer_card
  31. https://medium.com/praemineo
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/427b165576c2/share/twitter
  35. https://medium.com/p/427b165576c2/share/facebook
  36. https://medium.com/p/427b165576c2/share/twitter
  37. https://medium.com/p/427b165576c2/share/facebook
