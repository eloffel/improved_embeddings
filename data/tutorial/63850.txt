   #[1]articles from distill

          using artificial intelligence to augment human intelligence

   by creating user interfaces which let us work with the representations
   inside machine learning models, we can give people new tools for
   reasoning.

authors

affiliations

   [2]shan carter

   [3]google brain team

   [4]michael nielsen

   [5]yc research

published

   dec. 4, 2017

doi

   [6]10.23915/distill.00009

what are computers for?

   historically, different answers to this question     that is, different
   visions of computing     have helped inspire and determine the computing
   systems humanity has ultimately built. consider the early electronic
   computers. eniac, the world   s first general-purpose electronic
   computer, was commissioned to compute artillery firing tables for the
   united states army. other early computers were also used to solve
   numerical problems, such as simulating nuclear explosions, predicting
   the weather, and planning the motion of rockets. the machines operated
   in a batch mode, using crude input and output devices, and without any
   real-time interaction. it was a vision of computers as number-crunching
   machines, used to speed up calculations that would formerly have taken
   weeks, months, or more for a team of humans.

   in the 1950s a different vision of what computers are for began to
   develop. that vision was crystallized in 1962, when douglas engelbart
   proposed that computers could be used as a way of augmenting human
   intellect. in this view, computers weren   t primarily tools for solving
   number-crunching problems. rather, they were real-time interactive
   systems, with rich inputs and outputs, that humans could work with to
   support and expand their own problem-solving process. this vision of
   intelligence augmentation (ia) deeply influenced many others, including
   researchers such as alan kay at xerox parc, entrepreneurs such as steve
   jobs at apple, and led to many of the key ideas of modern computing
   systems. its ideas have also deeply influenced digital art and music,
   and fields such as interaction design, data visualization,
   computational creativity, and human-computer interaction.

   research on ia has often been in competition with research on
   artificial intelligence (ai): competition for funding, competition for
   the interest of talented researchers. although there has always been
   overlap between the fields, ia has typically focused on building
   systems which put humans and machines to work together, while ai has
   focused on complete outsourcing of intellectual tasks to machines. in
   particular, problems in ai are often framed in terms of matching or
   surpassing human performance: beating humans at chess or go; learning
   to recognize speech and images or translating language as well as
   humans; and so on.

   this essay describes a new field, emerging today out of a synthesis of
   ai and ia. for this field, we suggest the name artificial intelligence
   augmentation (aia): the use of ai systems to help develop new methods
   for intelligence augmentation. this new field introduces important new
   fundamental questions, questions not associated to either parent field.
   we believe the principles and systems of aia will be radically
   different to most existing systems.

   our essay begins with a survey of recent technical work hinting at
   artificial intelligence augmentation, including work on generative
   interfaces     that is, interfaces which can be used to explore and
   visualize generative machine learning models. such interfaces develop a
   kind of cartography of generative models, ways for humans to explore
   and make meaning from those models, and to incorporate what those
   models    know    into their creative work.

   our essay is not just a survey of technical work. we believe now is a
   good time to identify some of the broad, fundamental questions at the
   foundation of this emerging field. to what extent are these new tools
   enabling creativity? can they be used to generate ideas which are truly
   surprising and new, or are the ideas cliches, based on trivial
   recombinations of existing ideas? can such systems be used to develop
   fundamental new interface primitives? how will those new primitives
   change and expand the way humans think?

using generative models to invent meaningful creative operations

   let   s look at an example where a machine learning model makes a new
   type of interface possible. to understand the interface, imagine you   re
   a type designer, working on creating a new fontwe shall egregiously
   abuse the distinction between a font and a typeface. apologies to any
   type designers who may be reading.. after sketching some initial
   designs, you wish to experiment with bold, italic, and condensed
   variations. let   s examine a tool to generate and explore such
   variations, from any initial design. for reasons that will soon be
   explained the quality of results is quite crude; please bear with us.

   of course, varying the bolding (i.e., the weight), italicization and
   width are just three ways you can vary a font. imagine that instead of
   building specialized tools, users could build their own tool merely by
   choosing examples of existing fonts. for instance, suppose you wanted
   to vary the degree of serifing on a font. in the following, please
   select 5 to 10 sans-serif fonts from the top box, and drag them to the
   box on the left. select 5 to 10 serif fonts and drag them to the box on
   the right. as you do this, a machine learning model running in your
   browser will automatically infer from these examples how to interpolate
   your starting font in either the serif or sans-serif direction:

   in fact, we used this same technique to build the earlier bolding
   italicization, and condensing tool. to do so, we used the following
   examples of bold and non-bold fonts, of italic and non-italic fonts,
   and of condensed and non-condensed fonts:

   to build these tools, we used what   s called a generative model; the
   particular model we use was trained by james wexler. to understand
   generative models, consider that a priori describing a font appears to
   require a lot of data. for instance, if the font is
   [math: <semantics><mrow><mn>6</mn><mn>4</mn></mrow><annotation
   encoding="application/x-tex">64</annotation></semantics> :math]
   64 by
   [math: <semantics><mrow><mn>6</mn><mn>4</mn></mrow><annotation
   encoding="application/x-tex">64</annotation></semantics> :math]
   64 pixels, then we   d expect to need
   [math:
   <semantics><mrow><mn>6</mn><mn>4</mn><mo>  </mo><mn>6</mn><mn>4</mn><mo>
   =</mo><mn>4</mn><mo
   separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation
   encoding="application/x-tex">64 \times 64 =
   4,096</annotation></semantics> :math]
   64  64=4,096 parameters to describe a single glyph. but we can use a
   generative model to find a much simpler description.

   we do this by building a neural network which takes a small number of
   input variables, called latent variables, and produces as output the
   entire glyph. for the particular model we use, we have
   [math: <semantics><mrow><mn>4</mn><mn>0</mn></mrow><annotation
   encoding="application/x-tex">40</annotation></semantics> :math]
   40 latent space dimensions, and map that into the
   [math: <semantics><mrow><mn>4</mn><mo
   separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation
   encoding="application/x-tex">4,096</annotation></semantics> :math]
   4,096-dimensional space describing all the pixels in the glyph. in
   other words, the idea is to map a low-dimensional space into a
   higher-dimensional space:

   the generative model we use is a type of neural network known as a
   variational autoencoder (vae). for our purposes, the details of the
   generative model aren   t so important. the important thing is that by
   changing the latent variables used as input, it   s possible to get
   different fonts as output. so one choice of latent variables will give
   one font, while another choice will give a different font:

   you can think of the latent variables as a compact, high-level
   representation of the font. the neural network takes that high-level
   representation and converts it into the full pixel data. it   s
   remarkable that just
   [math: <semantics><mrow><mn>4</mn><mn>0</mn></mrow><annotation
   encoding="application/x-tex">40</annotation></semantics> :math]
   40 numbers can capture the apparent complexity in a glyph, which
   originally required
   [math: <semantics><mrow><mn>4</mn><mo
   separator="true">,</mo><mn>0</mn><mn>9</mn><mn>6</mn></mrow><annotation
   encoding="application/x-tex">4,096</annotation></semantics> :math]
   4,096 variables.

   the generative model we use is learnt from a training set of more than
   [math: <semantics><mrow><mn>5</mn><mn>0</mn></mrow><annotation
   encoding="application/x-tex">50</annotation></semantics> :math]
   50 thousand fonts bernhardsson scraped from the open web. during
   training, the weights and biases in the network are adjusted so that
   the network can output a close approximation to any desired font from
   the training set, provided a suitable choice of latent variables is
   made. in some sense, the model is learning a highly compressed
   representation of all the training fonts.

   in fact, the model doesn   t just reproduce the training fonts. it can
   also generalize, producing fonts not seen in training. by being forced
   to find a compact description of the training examples, the neural net
   learns an abstract, higher-level model of what a font is. that
   higher-level model makes it possible to generalize beyond the training
   examples already seen, to produce realistic-looking fonts.

   ideally, a good generative model would be exposed to a relatively small
   number of training examples, and use that exposure to generalize to the
   space of all possible human-readable fonts. that is, for any
   conceivable font     whether existing or perhaps even imagined in the
   future     it would be possible to find latent variables corresponding
   exactly to that font. of course, the model we   re using falls far short
   of this ideal     a particularly egregious failure is that many fonts
   generated by the model omit the tail on the capital    q    (you can see
   this in the examples above). still, it   s useful to keep in mind what an
   ideal generative model would do.

   such generative models are similar in some ways to how scientific
   theories work. scientific theories often greatly simplify the
   description of what appear to be complex phenomena, reducing large
   numbers of variables to just a few variables from which many aspects of
   system behavior can be deduced. furthermore, good scientific theories
   sometimes enable us to generalize to discover new phenomena.

   as an example, consider ordinary material objects. such objects have
   what physicists call a phase     they may be a liquid, a solid, a gas, or
   perhaps something more exotic, like a superconductor or
   [7]bose-einstein condensate. a priori, such systems seem immensely
   complex, involving perhaps
   [math:
   <semantics><mrow><mn>1</mn><msup><mn>0</mn><mrow><mn>2</mn><mn>3</mn></
   mrow></msup></mrow><annotation
   encoding="application/x-tex">10^{23}</annotation></semantics> :math]
   1023 or so molecules. but the laws of thermodynamics and statistical
   mechanics enable us to find a simpler description, reducing that
   complexity to just a few variables (temperature, pressure, and so on),
   which encompass much of the behavior of the system. furthermore,
   sometimes it   s possible to generalize, predicting unexpected new phases
   of matter. for example, in 1924, physicists used thermodynamics and
   statistical mechanics to predict a remarkable new phase of matter,
   bose-einstein condensation, in which a collection of atoms may all
   occupy identical quantum states, leading to surprising large-scale
   quantum interference effects. we   ll come back to this predictive
   ability in our later discussion of creativity and generative models.

   returning to the nuts and bolts of generative models, how can we use
   such models to do example-based reasoning like that in the tool shown
   above? let   s consider the case of the bolding tool. in that instance,
   we take the average of all the latent vectors for the user-specified
   bold fonts, and the average for all the user-specified non-bold fonts.
   we then compute the difference between these two average vectors:

   we   ll refer to this as the bolding vector. to make some given font
   bolder, we simply add a little of the bolding vector to the
   corresponding latent vector, with the amount of bolding vector added
   controlling the boldness of the resultin practice, sometimes a slightly
   different procedure is used. in some generative models the latent
   vectors satisfy some constraints     for instance, they may all be of the
   same length. when that   s the case, as in our model, a more
   sophisticated    adding    operation must be used, to ensure the length
   remains the same. but conceptually, the picture of adding the bolding
   vector is the right way to think.:

   this technique was introduced by larsen et al, and vectors like the
   bolding vector are sometimes called attribute vectors. the same idea is
   use to implement all the tools we   ve shown. that is, we use example
   fonts to creating a bolding vector, an italicizing vector, a condensing
   vector, and a user-defined serif vector. the interface thus provides a
   way of exploring the latent space in those four directions.

   the tools we   ve shown have many drawbacks. consider the following
   example, where we start with an example glyph, in the middle, and
   either increase or decrease the bolding (on the right and left,
   respectively):

   examining the glyphs on the left and right we see many unfortunate
   artifacts. particularly for the rightmost glyph, the edges start to get
   rough, and the serifs begin to disappear. a better generative model
   would reduce those artifacts. that   s a good long-term research program,
   posing many intriguing problems. but even with the model we have, there
   are also some striking benefits to the use of the generative model.

   to understand these benefits, consider a naive approach to bolding, in
   which we simply add some extra pixels around a glyph   s edges,
   thickening it up. while this thickening perhaps matches a non-expert   s
   way of thinking about type design, an expert does something much more
   involved. in the following we show the results of this naive thickening
   procedure versus what is actually done, for georgia and helvetica:

   as you can see, the naive bolding procedure produces quite different
   results, in both cases. for example, in georgia, the left stroke is
   only changed slightly by bolding, while the right stroke is greatly
   enlarged, but only on one side. in both fonts, bolding doesn   t change
   the height of the font, while the naive approach does.

   as these examples show, good bolding is not a trivial process of
   thickening up a font. expert type designers have many heuristics for
   bolding, heuristics inferred from much previous experimentation, and
   careful study of historical examples. capturing all those heuristics in
   a conventional program would involve immense work. the benefit of using
   the generative model is that it automatically learns many such
   heuristics.

   for example, a naive bolding tool would rapidly fill in the enclosed
   negative space in the enclosed upper region of the letter    a   . the font
   tool doesn   t do this. instead, it goes to some trouble to preserve the
   enclosed negative space, moving the a   s bar down, and filling out the
   interior strokes more slowly than the exterior. this principle is
   evident in the examples shown above, especially helvetica, and it can
   also be seen in the operation of the font tool:

   the heuristic of preserving enclosed negative space is not a priori
   obvious. however, it   s done in many professionally designed fonts. if
   you examine examples like those shown above it   s easy to see why: it
   improves legibility. during training, our generative model has
   automatically inferred this principle from the examples it   s seen. and
   our bolding interface then makes this available to the user.

   in fact, the model captures many other heuristics. for instance, in the
   above examples the heights of the fonts are (roughly) preserved, which
   is the norm in professional font design. again, what   s going on isn   t
   just a thickening of the font, but rather the application of a more
   subtle heuristic inferred by the generative model. such heuristics can
   be used to create fonts with properties which would otherwise be
   unlikely to occur to users. thus, the tool expands ordinary people   s
   ability to explore the space of meaningful fonts.

   the font tool is an example of a kind of cognitive technology. in
   particular, the primitive operations it contains can be internalized as
   part of how a user thinks. in this it resembles a program such as
   photoshop or a spreadsheet or 3d graphics programs. each provides a
   novel set of interface primitives, primitives which can be internalized
   by the user as fundamental new elements in their thinking. this act of
   internalization of new primitives is fundamental to much work on
   intelligence augmentation.

   the ideas shown in the font tool can be extended to other domains.
   using the same interface, we can use a generative model to manipulate
   images of human faces using qualities such as expression, gender, or
   hair color. or to manipulate sentences using length, sarcasm, or tone.
   or to manipulate molecules using chemical properties:

   images from sampling generative networks by white.
   sentence from pride and prejudice by jane austen. interpolated by the
   authors. inspired by experiments done by the novelist robin sloan
   images from automatic chemical design using a data-driven continuous
   representation of molecules by g  mez-bombarelli et al.

   such generative interfaces provide a kind of cartography of generative
   models, ways for humans to explore and make meaning using those models.

   we saw earlier that the font model automatically infers relatively deep
   principles about font design, and makes them available to users. while
   it   s great that such deep principles can be inferred, sometimes such
   models infer other things that are wrong, or undesirable. for example,
   white points out the addition of a smile vector in some face models
   will make faces not just smile more, but also appear more feminine.
   why? because in the training data more women than men were smiling. so
   these models may not just learn deep facts about the world, they may
   also internalize prejudices or erroneous beliefs. once such a bias is
   known, it is often possible to make corrections. but to find those
   biases requires careful auditing of the models, and it is not yet clear
   how we can ensure such audits are exhaustive.

   more broadly, we can ask why attribute vectors work, when they work,
   and when they fail? at the moment, the answers to these questions are
   poorly understood.

   for the attribute vector to work requires that taking any starting
   font, we can construct the corresponding bold version by adding the
   same vector in the latent space. however, a priori there is no reason
   using a single constant vector to displace will work. it may be that we
   should displace in many different ways. for instance, the heuristics
   used to bold serif and sans-serif fonts are quite different, and so it
   seems likely that very different displacements would be involved:

   of course, we could do something more sophisticated than using a single
   constant attribute vector. given pairs of example fonts (unbold, bold)
   we could train a machine learning algorithm to take as input the latent
   vector for the unbolded version and output the latent vector for the
   bolded version. with additional training data about font weights, the
   machine learning algorithm could learn to generate fonts of arbitrary
   weight. attribute vectors are just an extremely simple approach to
   doing these kinds of operation.

   for these reasons, it seems unlikely that attribute vectors will last
   as an approach to manipulating high-level features. over the next few
   years much better approaches will be developed. however, we can still
   expect interfaces offering operations broadly similar to those sketched
   above, allowing access to high-level and potentially user-defined
   concepts. that interface pattern doesn   t depend on the technical
   details of attribute vectors.

interactive generative adversarial models

   let   s look at another example using machine learning models to augment
   human creativity. it   s the interactive id3,
   or igans, introduced by zhu et al in 2016.

   one of the examples of zhu et al is the use of igans in an interface to
   generate images of consumer products such as shoes. conventionally,
   such an interface would require the programmer to write a program
   containing a great deal of knowledge about shoes: soles, laces, heels,
   and so on. instead of doing this, zhu et al train a generative model
   using
   [math: <semantics><mrow><mn>5</mn><mn>0</mn></mrow><annotation
   encoding="application/x-tex">50</annotation></semantics> :math]
   50 thousand images of shoes, downloaded from zappos. they then use that
   generative model to build an interface that lets a user roughly sketch
   the shape of a shoe, the sole, the laces, and so on:
   excerpted from zhu et al.

   the visual quality is low, in part because the generative model zhu et
   al used is outdated by modern (2017) standards     with more modern
   models, the visual quality would be much higher.

   but the visual quality is not the point. many interesting things are
   going on in this prototype. for instance, notice how the overall shape
   of the shoe changes considerably when the sole is filled in     it
   becomes narrower and sleeker. many small details are filled in, like
   the black piping on the top of the white sole, and the red coloring
   filled in everywhere on the shoe   s upper. these and other facts are
   automatically deduced from the underlying generative model, in a way
   we   ll describe shortly.

   the same interface may be used to sketch landscapes. the only
   difference is that the underlying generative model has been trained on
   landscape images rather than images of shoes. in this case it becomes
   possible to sketch in just the colors associated to a landscape. for
   example, here   s a user sketching in some green grass, the outline of a
   mountain, some blue sky, and snow on the mountain:
   excerpted from zhu et al.

   the generative models used in these interfaces are different than for
   our font model. rather than using id5, they   re
   based on id3 (gans). but the underlying
   idea is still to find a low-dimensional latent space which can be used
   to represent (say) all landscape images, and map that latent space to a
   corresponding image. again, we can think of points in the latent space
   as a compact way of describing landscape images.

   roughly speaking, the way the igans works is as follows. whatever the
   current image is, it corresponds to some point in the latent space:

   suppose, as happened in the earlier video, the user now sketches in a
   stroke outlining the mountain shape. we can think of the stroke as a
   constraint on the image, picking out a subspace of the latent space,
   consisting of all points in the latent space whose image matches that
   outline:

   the way the interface works is to find a point in the latent space
   which is near to the current image, so the image is not changed too
   much, but also coming close to satisfying the imposed constraints. this
   is done by optimizing an objective function which combines the distance
   to each of the imposed constraints, as well as the distance moved from
   the current point. if there   s just a single constraint, say,
   corresponding to the mountain stroke, this looks something like the
   following:

   we can think of this, then, as a way of applying constraints to the
   latent space to move the image around in meaningful ways.

   the igans have much in common with the font tool we showed earlier.
   both make available operations that encode much subtle knowledge about
   the world, whether it be learning to understand what a mountain looks
   like, or inferring that enclosed negative space should be preserved
   when bolding a font. both the igans and the font tool provide ways of
   understanding and navigating a high-dimensional space, keeping us on
   the natural space of fonts or shoes or landscapes. as zhu et al remark:

     [f]or most of us, even a simple image manipulation in photoshop
     presents insurmountable difficulties    any less-than-perfect edit
     immediately makes the image look completely unrealistic. to put
     another way, classic visual manipulation paradigm does not prevent
     the user from    falling off    the manifold of natural images.

   like the font tool, the igans is a cognitive technology. users can
   internalize the interface operations as new primitive elements in their
   thinking. in the case of shoes, for example, they can learn to think in
   terms of the difference they want to apply, adding a heel, or a higher
   top, or a special highlight. this is richer than the traditional way
   non-experts think about shoes (   size 11, black    etc). to the extent
   that non-experts do think in more sophisticated ways        make the top a
   little higher and sleeker        they get little practice in thinking this
   way, or seeing the consequences of their choices. having an interface
   like this enables easier exploration, the ability to develop idioms and
   the ability to plan, to swap ideas with friends, and so on.

two models of computation

   let   s revisit the question we began the essay with, the question of
   what computers are for, and how this relates to intelligence
   augmentation.

   one common conception of computers is that they   re problem-solving
   machines:    computer, what is the result of firing this artillery shell
   in such-and-such a wind [and so on]?   ;    computer, what will the maximum
   temperature in tokyo be in 5 days?   ;    computer, what is the best move
   to take when the go board is in this position?   ;    computer, how should
   this image be classified?   ; and so on.

   this is a conception common to both the early view of computers as
   number-crunchers, and also in much work on ai, both historically and
   today. it   s a model of a computer as a way of outsourcing cognition. in
   speculative depictions of possible future ai, this cognitive
   outsourcing model often shows up in the view of an ai as an oracle,
   able to solve some large class of problems with better-than-human
   performance.

   but a very different conception of what computers are for is possible,
   a conception much more congruent with work on intelligence
   augmentation.

   to understand this alternate view, consider our subjective experience
   of thought. for many people, that experience is verbal: they think
   using language, forming chains of words in their heads, similar to
   sentences in speech or written on a page. for other people, thinking is
   a more visual experience, incorporating representations such as graphs
   and maps. still other people mix mathematics into their thinking, using
   algebraic expressions or diagrammatic techniques, such as feynman
   diagrams and penrose diagrams.

   in each case, we   re thinking using representations invented by other
   people: words, graphs, maps, algebra, mathematical diagrams, and so on.
   we internalize these cognitive technologies as we grow up, and come to
   use them as a kind of substrate for our thinking.

   for most of history, the range of available cognitive technologies has
   changed slowly and incrementally. a new word will be introduced, or a
   new mathematical symbol. more rarely, a radical new cognitive
   technology will be developed. for example, in 1637 descartes published
   his    discourse on method   , explaining how to represent geometric ideas
   using algebra, and vice versa:

   this enabled a radical change and expansion in how we think about both
   geometry and algebra.

   historically, lasting cognitive technologies have been invented only
   rarely. but modern computers are a meta-medium enabling the rapid
   invention of many new cognitive technologies. consider a relatively
   banal example, such as photoshop. adept photoshop users routinely have
   formerly impossible thoughts such as:    let   s apply the clone stamp to
   the such-and-such layer.   . that   s an instance of a more general class
   of thought:    computer, [new type of action] this [new type of
   representation for a newly imagined class of object]   . when that
   happens, we   re using computers to expand the range of thoughts we can
   think.

   it   s this kind of cognitive transformation model which underlies much
   of the deepest work on intelligence augmentation. rather than
   outsourcing cognition, it   s about changing the operations and
   representations we use to think; it   s about changing the substrate of
   thought itself. and so while cognitive outsourcing is important, this
   cognitive transformation view offers a much more profound model of
   intelligence augmentation. it   s a view in which computers are a means
   to change and expand human thought itself.

   historically, cognitive technologies were developed by human inventors,
   ranging from the invention of writing in sumeria and mesoamerica, to
   the modern interfaces of designers such as douglas engelbart, alan kay,
   and others.

   examples such as those described in this essay suggest that ai systems
   can enable the creation of new cognitive technologies. things like the
   font tool aren   t just oracles to be consulted when you want a new font.
   rather, they can be used to explore and discover, to provide new
   representations and operations, which can be internalized as part of
   the user   s own thinking. and while these examples are in their early
   stages, they suggest ai is not just about cognitive outsourcing. a
   different view of ai is possible, one where it helps us invent new
   cognitive technologies which transform the way we think.

   in this essay we   ve focused on a small number of examples, mostly
   involving exploration of the latent space. there are many other
   examples of artificial intelligence augmentation. to give some flavor,
   without being comprehensive: the sketch-id56 system, for neural network
   assisted drawing; the wekinator, which enables users to rapidly build
   new musical instruments and artistic systems; toposketch, for
   developing animations by exploring latent spaces; machine learning
   models for designing overall typographic layout; and a generative model
   which enables interpolation between musical phrases. in each case, the
   systems use machine learning to enable new primitives which can be
   integrated into the user   s thinking. more broadly, artificial
   intelligence augmentation will draw on fields such as computational
   creativity and interactive machine learning.

finding powerful new primitives of thought

   we   ve argued that machine learning systems can help create
   representations and operations which serve as new primitives in human
   thought. what properties should we look for in such new primitives?
   this is too large a question to be answered comprehensively in a short
   essay. but we will explore it briefly.

   historically, important new media forms often seem strange when
   introduced. many such stories have passed into popular culture: the
   near riot at the premiere of stravinsky and nijinksy   s    rite of
   spring   ; the consternation caused by the early cubist paintings,
   leading the new york times to comment:    what do they mean? have those
   responsible for them taken leave of their senses? is it art or madness?
   who knows?   

   another example comes from physics. in the 1940s, different
   formulations of the theory of quantum electrodynamics were developed
   independently by the physicists julian schwinger, shin   ichir   tomonaga,
   and richard feynman. in their work, schwinger and tomonaga used a
   conventional algebraic approach, along lines similar to the rest of
   physics. feynman used a more radical approach, based on what are now
   known as feynman diagrams, for depicting the interaction of light and
   matter:
   [feynmann-diagram.svg] image by [8]joel holdsworth), licensed under a
   creative commons attribution-share alike 3.0 unported license

   initially, the schwinger-tomonaga approach was easier for other
   physicists to understand. when feynman and schwinger presented their
   work at a 1948 workshop, schwinger was immediately acclaimed. by
   contrast, feynman left his audience mystified. as james gleick put it
   in his biography of feynman:

     it struck feynman that everyone had a favorite principle or theorem
     and he was violating them all    feynman knew he had failed. at the
     time, he was in anguish. later he said simply:    i had too much
     stuff. my machines came from too far away.   

   of course, strangeness for strangeness   s sake alone is not useful. but
   these examples suggest that breakthroughs in representation often
   appear strange at first. is there any underlying reason that is true?

   part of the reason is because if some representation is truly new, then
   it will appear different than anything you   ve ever seen before.
   feynman   s diagrams, picasso   s paintings, stravinsky   s music: all
   revealed genuinely new ways of making meaning. good representations
   sharpen up such insights, eliding the familiar to show that which is
   new as vividly as possible. but because of that emphasis on
   unfamiliarity, the representation will seem strange: it shows
   relationships you   ve never seen before. in some sense, the task of the
   designer is to identify that core strangeness, and to amplify it as
   much as possible.

   strange representations are often difficult to understand. at first,
   physicists preferred schwinger-tomonaga to feynman. but as feynman   s
   approach was slowly understood by physicists, they realized that
   although schwinger-tomonaga and feynman were mathematically equivalent,
   feynman was more powerful. as gleick puts it:

     schwinger   s students at harvard were put at a competitive
     disadvantage, or so it seemed to their fellows elsewhere, who
     suspected them of surreptitiously using the diagrams anyway. this
     was sometimes true    murray gell-mann later spent a semester staying
     in schwinger   s house and loved to say afterward that he had searched
     everywhere for the feynman diagrams. he had not found any, but one
     room had been locked   

   these ideas are true not just of historical representations, but also
   of computer interfaces. however, our advocacy of strangeness in
   representation contradicts much conventional wisdom about interfaces,
   especially the widely-held belief that they should be    user friendly   ,
   i.e., simple and immediately useable by novices. that most often means
   the interface is cliched, built from conventional elements combined in
   standard ways. but while using a cliched interface may be easy and fun,
   it   s an ease similar to reading a formulaic romance novel. it means the
   interface does not reveal anything truly surprising about its subject
   area. and so it will do little to deepen the user   s understanding, or
   to change the way they think. for mundane tasks that is fine, but for
   deeper tasks, and for the longer term, you want a better interface.

   ideally, an interface will surface the deepest principles underlying a
   subject, revealing a new world to the user. when you learn such an
   interface, you internalize those principles, giving you more powerful
   ways of reasoning about that world. those principles are the diffs in
   your understanding. they   re all you really want to see, everything else
   is at best support, at worst unimportant dross. the purpose of the best
   interfaces isn   t to be user-friendly in some shallow sense. it   s to be
   user-friendly in a much stronger sense, reifying deep principles about
   the world, making them the working conditions in which users live and
   create. at that point what once appeared strange can instead becomes
   comfortable and familiar, part of the pattern of thoughta powerful
   instance of these ideas is when an interface reifies general-purpose
   principles. an example is an interface one of us developed based on the
   principle of conservation of energy. such general-purpose principles
   generate multiple unexpected relationships between the entities of a
   subject, and so are a particularly rich source of insights when reified
   in an interface..

   what does this mean for the use of ai models for intelligence
   augmentation?

   aspirationally, as we   ve seen, our machine learning models will help us
   build interfaces which reify deep principles in ways meaningful to the
   user. for that to happen, the models have to discover deep principles
   about the world, recognize those principles, and then surface them as
   vividly as possible in an interface, in a way comprehensible by the
   user.

   of course, this is a tall order! the examples we   ve shown are just
   barely beginning to do this. it   s true that our models do sometimes
   discover relatively deep principles, like the preservation of enclosed
   negative space when bolding a font. but this is merely implicit in the
   model. and while we   ve built a tool which takes advantage of such
   principles, it   d be better if the model automatically inferred the
   important principles learned, and found ways of explicitly surfacing
   them through the interface. (encouraging progress toward this has been
   made by infogans, which use information-theoretic ideas to find
   structure in the latent space.) ideally, such models would start to get
   at true explanations, not just in a static form, but in a dynamic form,
   manipulable by the user. but we   re a long way from that point.

do these interfaces inhibit creativity?

   it   s tempting to be skeptical of the expressiveness of the interfaces
   we   ve described. if an interface constrains us to explore only the
   natural space of images, does that mean we   re merely doing the
   expected? does it mean these interfaces can only be used to generate
   visual cliches? does it prevent us from generating anything truly new,
   from doing truly creative work?

   to answer these questions, it   s helpful to identify two different modes
   of creativity. this two-mode model is over-simplified: creativity
   doesn   t fit so neatly into two distinct categories. yet the model
   nonetheless clarifies the role of new interfaces in creative work.

   the first mode of creativity is the everyday creativity of a
   craftsperson engaged in their craft. much of the work of a font
   designer, for example, consists of competent recombination of the best
   existing practices. such work typically involves many creative choices
   to meet the intended design goals, but not developing key new
   underlying principles.

   for such work, the generative interfaces we   ve been discussing are
   promising. while they currently have many limitations, future research
   will identity and fix many deficiencies. this is happening rapidly with
   gans: the original gans had many limitations, but models soon appeared
   that were better adapted to images, improved the resolution, reduced
   artifactsso much work has been done on improving resolution and
   reducing artifacts it seems unfair to single out any small set of
   papers, and to omit the many others., and so on. with enough iterations
   it   s plausible these generative interfaces will become powerful tools
   for craft work.

   the second mode of creativity aims toward developing new principles
   that fundamentally change the range of creative expression. one sees
   this in the work of artists such as picasso or monet, who violated
   existing principles of painting, developing new principles which
   enabled people to see in new ways.

   is it possible to do such creative work, while using a generative
   interface? don   t such interfaces constrain us to the space of natural
   images, or natural fonts, and thus actively prevent us from exploring
   the most interesting new directions in creative work?

   the situation is more complex than this.

   in part, this is a question about the power of our generative models.
   in some cases, the model can only generate recombinations of existing
   ideas. this is a limitation of an ideal gan, since a perfectly trained
   gan generator will reproduce the training distribution. such a model
   can   t directly generate an image based on new fundamental principles,
   because such an image wouldn   t look anything like it   s seen in its
   training data.

   artists such as [9]mario klingemann and [10]mike tyka are now using
   gans to create interesting artwork. they   re doing that using
      imperfect    gan models, which they seem to be able to use to explore
   interesting new principles; it   s perhaps the case that bad gans may be
   more artistically interesting than ideal gans. furthermore, nothing
   says an interface must only help us explore the latent space. perhaps
   operations can be added which deliberately take us out of the latent
   space, or to less probable (and so more surprising) parts of the space
   of natural images.

   of course, gans are not the only generative models. in a sufficiently
   powerful generative model, the generalizations discovered by the model
   may contain ideas going beyond what humans have discovered. in that
   case, exploration of the latent space may enable us to discover new
   fundamental principles. the model would have discovered stronger
   abstractions than human experts. imagine a generative model trained on
   paintings up until just before the time of the cubists; might it be
   that by exploring that model it would be possible to discover cubism?
   it would be an analogue to something like the prediction of
   bose-einstein condensation, as discussed earlier in the essay. such
   invention is beyond today   s generative models, but seems a worthwhile
   aspiration for future models.

   our examples so far have all been based on generative models. but there
   are some illuminating examples which are not based on generative
   models. consider the pix2pix system developed by isola et al. this
   system is trained on pairs of images, e.g., pairs showing the edges of
   a cat, and the actual corresponding cat. once trained, it can be shown
   a set of edges and asked to generate an image for an actual
   corresponding cat. it often does this quite well:

input

output

   [11]live demo by christopher hesse

   when supplied with unusual constraints, pix2pix can produce striking
   images:

   [12]bread cat by ivy tsai

   [13]cat beholder by marc hesse

   spiral cat

   this is perhaps not high creativity of a picasso-esque level. but it is
   still surprising. it   s certainly unlike images most of us have ever
   seen before. how does pix2pix and its human user achieve this kind of
   result?

   unlike our earlier examples, pix2pix is not a generative model. this
   means it does not have a latent space or a corresponding space of
   natural images. instead, there is a neural network, called,
   confusingly, a generator     this is not meant in the same sense as our
   earlier generative models     that takes as input the constraint image,
   and produces as output the filled-in image.

   the generator is trained adversarially against a discriminator network,
   whose job is to distinguish between pairs of images generated from real
   data, and pairs of images generated by the generator.

   while this sounds similar to a conventional gan, there is a crucial
   difference: there is no latent vector input to the generatoractually,
   isola et al experimented with adding such a latent vector to the
   generator, but found it made little difference to the resulting
   images.. rather, there is simply an input constraint. when a human
   inputs a constraint unlike anything seen in training, the network is
   forced to improvise, doing the best it can to interpret that constraint
   according to the rules it has previously learned. the creativity is the
   result of a forced merger of knowledge inferred from the training data,
   together with novel constraints provided by the user. as a result, even
   relatively simple ideas     like the bread- and beholder-cats     can
   result in striking new types of images, images not within what we would
   previously have considered the space of natural images.

conclusion

   it is conventional wisdom that ai will change how we interact with
   computers. unfortunately, many in the ai community greatly
   underestimate the depth of interface design, often regarding it as a
   simple problem, mostly about making things pretty or easy-to-use. in
   this view, interface design is a problem to be handed off to others,
   while the hard work is to train some machine learning system.

   this view is incorrect. at its deepest, interface design means
   developing the fundamental primitives human beings think and create
   with. this is a problem whose intellectual genesis goes back to the
   inventors of the alphabet, of cartography, and of musical notation, as
   well as modern giants such as descartes, playfair, feynman, engelbart,
   and kay. it is one of the hardest, most important and most fundamental
   problems humanity grapples with.

   as discussed earlier, in one common view of ai our computers will
   continue to get better at solving problems, but human beings will
   remain largely unchanged. in a second common view, human beings will be
   modified at the hardware level, perhaps directly through neural
   interfaces, or indirectly through whole brain emulation.

   we   ve described a third view, in which ais actually change humanity,
   helping us invent new cognitive technologies, which expand the range of
   human thought. perhaps one day those cognitive technologies will, in
   turn, speed up the development of ai, in a virtuous feedback cycle:
   [cycle.svg]

   it would not be a singularity in machines. rather, it would be a
   singularity in humanity   s range of thought. of course, this loop is at
   present extremely speculative. the systems we   ve described can help
   develop more powerful ways of thinking, but there   s at most an indirect
   sense in which those ways of thinking are being used in turn to develop
   new ai systems.

   of course, over the long run it   s possible that machines will exceed
   humans on all or most cognitive tasks. even if that   s the case,
   cognitive transformation will still be a valuable end, worth pursuing
   in its own right. there is pleasure and value involved in learning to
   play chess or go well, even if machines do it better. and in activities
   such as story-telling the benefit often isn   t so much the artifact
   produced as the process of construction itself, and the relationships
   forged. there is intrinsic value in personal change and growth, apart
   from instrumental benefits.

   the interface-oriented work we   ve discussed is outside the narrative
   used to judge most existing work in artificial intelligence. it doesn   t
   involve beating some benchmark for a classification or regression
   problem. it doesn   t involve impressive feats like beating human
   champions at games such as go. rather, it involves a much more
   subjective and difficult-to-measure criterion: is it helping humans
   think and create in new ways?

   this creates difficulties for doing this kind of work, particularly in
   a research setting. where should one publish? what community does one
   belong to? what standards should be applied to judge such work? what
   distinguishes good work from bad?

   we believe that over the next few years a community will emerge which
   answers these questions. it will run workshops and conferences. it will
   publish work in venues such as distill. its standards will draw from
   many different communities: from the artistic and design and musical
   communities; from the mathematical community   s taste in abstraction and
   good definition; as well as from the existing ai and ia communities,
   including work on computational creativity and human-computer
   interaction. the long-term test of success will be the development of
   tools which are widely used by creators. are artists using these tools
   to develop remarkable new styles? are scientists in other fields using
   them to develop understanding in ways not otherwise possible? these are
   great aspirations, and require an approach that builds on conventional
   ai work, but also incorporates very different norms.

  acknowledgments

   we are extremely grateful to james wexler who built and trained the
   model on which the font demos are based. nikhil thorat and daniel
   smilkov also generously optimized the browser implementation of the
   model, which runs using [14]deeplearn.js. thanks to daniel dewey, andy
   matuschak, robert ochshorn, and bret victor for valuable conversations
   and feedback.

  author contribution

   authors are listed alphabetically in the byline. michael drafted the
   text of the essay, but the ideas of the essay were developed jointly in
   conversation with shan. shan developed the interactive diagrams, with
   considerable input from michael.

  references

    1. augmenting human intellect: a conceptual framework
       engelbart, d.c., 1962.
    2. deeplearn.js font demo    [15][link]
       wexler, j., 2017.
    3. auto-encoding id58
       kingma, d.p. and welling, m., 2014. iclr.
    4. analyzing 50k fonts using deep neural networks    [16][html]
       bernhardsson, e., 2016.
    5. autoencoding beyond pixels using a learned similarity metric
       larsen, a.b.l., s  nderby, s.k., larochelle, h. and winther, o.,
       2016. icml.
    6. sampling generative networks    [17][pdf]
       white, t., 2016.
    7. writing with the machine    [18][link]
       sloan, r., 2017. eyeo.
    8. automatic chemical design using a data-driven continuous
       representation of molecules    [19][pdf]
       g  mez-bombarelli, r., duvenaud, d., hern  ndez-lobato, j.m.,
       aguilera-iparraguirre, j., hirzel, t.d., adams, r.p. and
       aspuru-guzik, a., 2016.
    9. generative visual manipulation on the natural image manifold
       zhu, j., kr  henb  hl, p., schechtman, e. and efros, a.a., 2016.
       european conference on id161 (eccv).
   10. generative adversarial nets
       goodfellow, i.j., pouget-abadie, j., mirza, m., xu, b.,
       warde-farley, d., ozair, s., courville, a. and bengio, y., 2014.
       advances in neural information processing systems (nips), pp.
       2672-2680.
   11. a neural representation of sketch drawings    [20][pdf]
       ha, d. and eck, d., 2017.
   12. real-time human interaction with supervised learning algorithms for
       music composition and performance
       fiebrink, r., 2011. princeton university phd thesis.
   13. toposketch: drawing in latent space
       loh, i. and white, t., 2017. nips workshop on machine learning for
       creativity and design.
   14. taking the robots to design school, part 1    [21][link]
       gold, j., 2016.
   15. hierarchical id5 for music    [22][pdf]
       roberts, a., engel, j. and eck, d., 2017. nips workshop on machine
       learning for creativity and design.
   16. computational creativity: the final frontier?
       colton, s. and wiggins, g.a., 2012. ecai.
   17. interactive machine learning: letting users build classifiers
       ware, m., frank, e., holmes, g., hall, m. and witten, i.h., 2001.
       international journal of human-computer studies, vol 55, pp.
       281-292.
   18. eccentric school of painting increased its vogue in the current art
       exhibition     what its followers attempt to do    [23][link]
       1911. the new york times.
   19. genius: the life and science of richard feynman
       gleick, j., 1992. vintage books.
   20. thought as a technology    [24][html]
       nielsen, m., 2016.
   21. infogan: interpretable representation learning by information
       maximizing generative adversarial nets
       chen, x., duan, y., houthooft, r., schulman, j., sutskever, i. and
       abbeel, p., 2016. nips.
   22. unsupervised representation learning with deep convolutional
       id3    [25][pdf]
       radford, a., metz, l. and chintala, s., 2016. iclr.
   23. image-to-image translation with conditional adversarial networks
          [26][pdf]
       isola, p., zhu, j., zhou, t. and efros, a.a., 2017.

  updates and corrections

   if you see mistakes or want to suggest changes, please [27]create an
   issue on github.

  reuse

   diagrams and text are licensed under creative commons attribution
   [28]cc-by 4.0 with the [29]source available on github, unless noted
   otherwise. the figures that have been reused from other sources don   t
   fall under this license and can be recognized by a note in their
   caption:    figure from       .

  citation

   for attribution in academic contexts, please cite this work as
carter & nielsen, "using artificial intelligence to augment human intelligence",
 distill, 2017.

   bibtex citation
@article{carter2017using,
  author = {carter, shan and nielsen, michael},
  title = {using artificial intelligence to augment human intelligence},
  journal = {distill},
  year = {2017},
  note = {https://distill.pub/2017/aia},
  doi = {10.23915/distill.00009}
}

   [30]distill is dedicated to clear explanations of machine learning
   [31]about [32]submit [33]prize [34]archive [35]rss [36]github
   [37]twitter      issn 2476-0757

references

   1. https://distill.pub/rss.xml
   2. http://shancarter.com/
   3. https://g.co/brain
   4. http://michaelnielsen.org/
   5. https://ycr.org/
   6. https://doi.org/10.23915/distill.00009
   7. https://en.wikipedia.org/wiki/bose   einstein_condensate
   8. https://commons.wikimedia.org/w/index.php?curid=1764161
   9. http://quasimondo.com/
  10. http://www.miketyka.com/
  11. https://affinelayer.com/pixsrv/
  12. https://twitter.com/ivymyt/status/834174687282241537
  13. https://affinelayer.com/pixsrv/beholder.jpg
  14. https://deeplearnjs.org/
  15. https://pair-code.github.io/font-explorer/
  16. https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks.html
  17. http://arxiv.org/pdf/1609.04468.pdf
  18. https://vimeo.com/232545219
  19. http://arxiv.org/pdf/1610.02415.pdf
  20. http://arxiv.org/pdf/1704.03477.pdf
  21. http://www.jon.gold/2016/05/robot-design-school/
  22. https://nips2017creativity.github.io/doc/hierarchical_variational_autoencoders_for_music.pdf
  23. http://query.nytimes.com/mem/archive-free/pdf?res=9d02e2d71131e233a2575bc0a9669d946096d6cf&mcubz=3
  24. http://cognitivemedium.com/tat/index.html
  25. http://arxiv.org/pdf/1511.06434.pdf
  26. http://arxiv.org/pdf/1611.07004.pdf
  27. https://github.com/distillpub/post--aia/issues/new
  28. https://creativecommons.org/licenses/by/4.0/
  29. https://github.com/distillpub/post--aia
  30. https://distill.pub/
  31. https://distill.pub/about/
  32. https://distill.pub/journal/
  33. https://distill.pub/prize/
  34. https://distill.pub/archive/
  35. https://distill.pub/rss.xml
  36. https://github.com/distillpub
  37. https://twitter.com/distillpub
