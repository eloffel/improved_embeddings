   #[1]todo epsilon suma

   [2]todo epsilon suma

   [3]about

id91 the mnist dataset via semidefinite programming

   jul 5, 2016

   posted with : [4]id91, [5]data science, [6]id76
   (button) run id116 sdp (button) show original data (button) hide
   centers (button) show centers (button) random projection

the dataset

   the [7]mnist dataset consists of thousands of images of handwritten
   digits. (place the mouse on top of the points in order to see how they
   look like). the objective is to cluster them by similarity, the
   previous step for classifying them.

preprocessing using tensorflow

   id91 the raw data gives poor results (due to 4   s and 9   s being
   similar, for example), so we first learn meaningful features, and then
   cluster the data in feature space. we used the first example from the
   [8]tensorflow tutorial. we train a simple one-layer softmax neural
   network on images from a training set. and we run the trained neural
   network on the first 1000 elements of the testing set. as a results,
   each image gets mapped to a vector, representing the probabilities of
   being each digit. since the entries of the vector sum to 1, the feature
   space is actually 9-dimensional. note that spectral id91 would be
   useless in this case since all its    denoising    comes from projecting
   the points into the dimensional space spanned by the top eigenvectors
   of the laplacian.

   these feature vectors are the ones we represent in our plot. since 10
   is too many dimensions for a plot, we depict the feature vectors
   projected onto a random plane.

the id116 semidefinite relaxation

   given a set of points , the -means objective is to find a partition in
   clusters that minimize the sum of the squared distances of the points
   to the centroid of their respective cluster. in other words,

   the [9]id116 sdp is based on the following observation:

   therefore, if is a matrix such that , the -means problem can be written
   as

   where is the indicator vector, a vector such that if and 0 otherwise.

   since optimizing on all possible partitions is np-hard, we are going to
   relax the constraint to a convex one. in particular, the semidefinite
   relaxation:

   where means that all entries of are non-negative, and means that is
   symmetric and positive semidefinite. now may not represent a partition
   anymore, so we need a rounding step.

interpreting the sdp result

   let be the matrix which -th row is the coordinates (in ) of . note that
   when for some partition (i.e.: the relaxation is tight), we have that
   the -th row of the matrix is the coordinates of the corresponding
   center (i.e.: ).

   let   s arrange the points such that the first points are the zeros, the
   second points are the ones, and so on. then, the matrix that
   corresponds to the ground truth id91 is a block diagonal matrix
   which -block is constant .

   the sdp, however, finds this low rank matrix with many repeated columns
   and rows instead:

                               [sdp_sol2.png]

   so when we apply it we get:

   we exploit the fact that has many repeated rows in our rounding step.

approximation guarantees and tightness

   a future post will focus on the mathematical analysis of this
   algorithm. in particular, the sdp is known to exactly recover the
   clusters when the points come from a data model called the stochastic
   ball model (see [10]this and [11]this). and it is known to
   [12]approximate the centers when the points come from a mixture of
   subgaussian distributions.

about the interactive plots and id91 implementation

   an implementation of the -means sdp is available [13]here. the
   implementation uses [14]sdpnal+ which is an amazing sdp solver for
   problems with non-negative constraints.

   the first plot i wrote in javascript using [15]d3. the second
   interactive plot i wrote in python using [16]bokeh. if someone asks i
   can write a tutorial on how to do these plots, but many tutorials and
   examples are available on the internet already.

   note that the fact that has many repeated columns imply that many
   points are mapped to the same point after computing . if you place the
   mouse on top of the points in the second plot you   ll see many images.
   but if you do it in the first one you   ll see one one (i added a little
   bit of noise so you can move the mouse and see a different image).
   that   s because i haven   t figured out how to replicate that behavior in
   javascript. i   ll be happy to hear suggestions.
   please enable javascript to view the [17]comments powered by disqus.

todo epsilon suma

     * by [18]soledad villar
     * mvillar at math utexas edu
     * [19]solevillar

   a research blog on math, data, coding and more.

   follow blog via email
   email address ____________________
   ____________________
   subscribe

references

   visible links
   1. https://www.ma.utexas.edu/users/mvillar//feed.xml
   2. http://solevillar.github.io/
   3. http://solevillar.github.io/about/
   4. http://solevillar.github.io/blog/tag/id91/
   5. http://solevillar.github.io/blog/tag/data-science/
   6. http://solevillar.github.io/blog/tag/convex-optimization/
   7. http://yann.lecun.com/exdb/mnist/
   8. https://www.tensorflow.org/versions/r0.8/tutorials/mnist/beginners/index.html#softmax-regressions
   9. http://www.optimization-online.org/db_file/2005/04/1114.pdf
  10. https://arxiv.org/abs/1408.4045
  11. https://arxiv.org/abs/1505.04778
  12. http://arxiv.org/pdf/1602.06612v2.pdf
  13. https://github.com/solevillar/kmeans_sdp
  14. http://www.math.nus.edu.sg/~mattohkc/sdpnalplus.html
  15. https://d3js.org/
  16. http://bokeh.pydata.org/en/latest/
  17. https://disqus.com/?ref_noscript
  18. https://www.ma.utexas.edu/users/mvillar/
  19. https://github.com/solevillar

   hidden links:
  21. http://solevillar.github.io/2016/07/05/id91-mnist-sdp.html
