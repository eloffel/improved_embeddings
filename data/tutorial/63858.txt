   #[1]github [2]recent commits to qb:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]20
     * [35]star [36]142
     * [37]fork [38]43

[39]pinafore/[40]qb

   [41]code [42]issues 0 [43]pull requests 1 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   qanta quiz bowl ai
   [48]artificial-intelligence [49]machine-learning
   [50]natural-language-processing
     * [51]1,462 commits
     * [52]13 branches
     * [53]3 releases
     * [54]fetching contributors
     * [55]mit

    1. [56]python 96.7%
    2. [57]shell 2.2%
    3. [58]hcl 1.1%

   (button) python shell hcl
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/p
   [60]download zip

downloading...

   want to be notified of new releases in pinafore/qb?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   [67]@entilzha
   [68]entilzha [69]moving
   latest commit [70]9454d68 apr 4, 2019
   [71]permalink
   type             name             latest commit message commit time
        failed to load latest commit information.
        [72]bin
        [73]data
        [74]packer
        [75]qanta
        [76]terraform
        [77].gitignore
        [78].rsyncignore
        [79]license
        [80]readme.md
        [81]aws.md
        [82]aws.tf
        [83]checkpoint.py
        [84]cli.py
        [85]dataset.py
        [86]environment.yaml
        [87]expo.md
        [88]figures.py
        [89]generate_guesses.py
        [90]jmlr.py
        [91]log4j2.properties
        [92]luigi.id18
        [93]mypy.ini
        [94]protobowl_user.py
        [95]qanta-defaults.yaml
        [96]qanta-hyper.yaml
        [97]security_groups.py
        [98]setup.id18
        [99]setup.py
        [100]slack.py
        [101]slurm-config.yaml
        [102]validate_annotations.py

readme.md

qanta

downloading data

   whether you would like to use our system or use only our dataset, the
   easiest way to do so is use our dataset.py script. it is a standalone
   script whose only dependencies are python 3.6 and the package click
   which can be installed via pip install click.

   the following commands can be used to download our dataset, or datasets
   we use in either the system or paper plots. data will be downloaded to
   data/external/datasets by default, but can be changed with the
   --local-qanta-prefix option
     * ./dataset.py download: download only the qanta dataset
     * ./dataset.py download wikidata: download our preprocessed
       wikidata.org instance of attributes
     * ./dataset.py download plotting: download the squad, simple
       questions, jeopardy, and triviaqa datasets we compare against in
       our paper plots and tables

file description:

     * qanta.unmapped.2018.04.18.json: all questions in our dataset,
       without mapped wikipedia answers. sourced from protobowl and
       quizdb. light preprocessing has been applied to remove quiz bowl
       specific syntax such as instructions to moderators
     * qanta.processed.2018.04.18.json: prior dataset with added fields
       extracting the first sentence, and sentence id121s of the
       question paragraph for convenience.
     * qanta.mapped.2018.04.18.json: the processed dataset with wikipedia
       pages matched to the answer where possible. this includes all
       questions, even those without matched pages.
     * qanta.2018.04.18.sqlite3: equivalent to
       qanta.mapped.2018.04.18.json but in sqlite3 format
     * qanta.train.2018.04.18.json: training data which is the mapped
       dataset filtered down to only questions with non-null page matches
     * qanta.dev.2018.04.18.json: dev data which is the mapped dataset
       filtered down to only questions with non-null page matches
     * qanta.test.2018.04.18.json: test data which is the mapped dataset
       filtered down to only questions with non-null page matches

dependencies

   the recommended way to run our system is to use the anaconda python
   distribution. the environment.yaml can be used to create a conda
   environment with all the necessary software versions installed.

   the qanta system has the following dependencies. depending on your
   objective however not all are necessary. the python packages are
   generally required so that imports resolve, apache spark is required
   for many id174 steps, vowpal wabbit is not needed for
   anything except training a linear model, spacy is required for
   preprocessing, elastic search is required for the ir based models, and
   lz4 and the aws cli are necessary for downloading data not part of the
   dataset.py script.
     * anaconda distributed python 3.6
     * pytorch 0.3.x
     * apache spark 2.2.0 with scala/jvm
     * vowpal wabbit 8.4
     * spacy 2.0
     * elastic search 5.6.x
     * lz4
     * aws cli
     * all python packages listed in environment.yaml

installing apache spark

   packer/bin/install-spark.sh

   you can test is spark is installed property by running something like
   the following:
> python
python 3.6.1 |anaconda 4.4.0 (64-bit)| (default, may 11 2017, 13:09:58)
[gcc 4.4.7 20120313 (red hat 4.4.7-1)] on linux
type "help", "copyright", "credits" or "license" for more information.
>>> from qanta.spark import create_spark_context
>>> sc = create_spark_context()
using spark's default log4j profile: org/apache/spark/log4j-defaults.properties
setting default log level to "warn".
to adjust logging level use sc.setloglevel(newlevel). for sparkr, use setlogleve
l(newlevel).
17/07/25 10:04:01 warn nativecodeloader: unable to load native-hadoop library fo
r your platform... using builtin-java classes where applicable
17/07/25 10:04:01 warn utils: your hostname, hongwu resolves to a loopback addre
ss: 127.0.0.2; using 192.168.2.2 instead (on interface eth0)
17/07/25 10:04:01 warn utils: set spark_local_ip if you need to bind to another
address

installing elastic search 5.6

   packer/bin/install-elasticsearch.sh

   install version 5.6.x, do not use 6.x. also be sure that the directory
   bin/ within the extracted files is in your $path as it contains the
   necessary binary elasticsearch.

installing python packages

   either use environment.yaml or:
pip install -r packer/requirements.txt

nltk models

# download nltk data
$ python3 setup.py download

qanta on path

   in addition to these steps you need to either run python setup.py
   develop or include the qanta directory in your pythonpath environment
   variable. we intend to fix path issues in the future by fixing
   absolute/relative paths.

configuration

   qanta configuration is done through a combination of environment
   variables and the qanta-defaults.yaml/qanta.yaml files. qanta will read
   a qanta.yaml first if it exists, otherwise it will fall back to reading
   qanta-defaults.yaml. this is meant to allow for custom configuration of
   qanta.yaml after copying it via cp qanta-defaults.yaml qanta.yaml.

   the configuration of most interest is how to enable or disable specific
   guesser implementations. in the guesser config the keys such as
   qanta.guesser.dan.danguesser correspond to the fully qualified paths of
   each guesser. each of these keys contain an array of configurations
   (this is signified in yaml by the -). our code will inspect all of
   these configurations looking for those that have enabled: true, and
   only run those guessers. by default we have enabled: false for all
   models. if you simply want to perform a sanity check we recommend
   enabling qanta.guesser.tfidf.tfidfguesser. if you are looking for our
   best model and configuration you should use enable
   qanta.guesser.id56.id56guesser.

running qanta

   running qanta is managed primarily by two methods: ./cli.py and
   [103]luigi. the former is used to run specific commands such as
   starting/stopping elastic search, but in general luigi is the primary
   method for running our system.

luigi pipelines

   luigi is a pure python make-like framework for running data pipelines.
   below we give sample commands for running different parts of our
   pipeline. in general, you should either append --local-scheduler to all
   commands or learn about using the [104]luigi central scheduler.

   for these common tasks you can use command luigi --local-scheduler
   followed by:
     * --module qanta.pipeline.preprocess downloaddata: this downloads any
       necessary data and preprocesses it. this will download a copy of
       our preprocessed wikipedia stored in aws s3 and turn it into the
       format used by our code. this step requires the aws cli, lz4,
       apache spark, and may require a decent amount of ram.
     * --module qanta.pipeline.guesser allguesserreports: train all
       enabled guessers, generate guesses for them, and produce a report
       of their performance into output/guesser.

qanta cli

   you can start/stop elastic search with
     * ./cli.py elasticsearch start
     * ./cli.py elasticsearch stop

aws s3 checkpoint/restore

   to provide and easy way to version, checkpoint, and restore runs of
   qanta we provide a script to manage that at aws_checkpoint.py. we
   assume that you set an environment variable qb_aws_s3_bucket to where
   you want to checkpoint to and restore from. we assume that we have full
   access to all the contents of the bucket so we suggest creating a
   dedicated bucket.

information on our data sources

wikipedia dumps

   as part of our ingestion pipeline we access raw wikipedia dumps. the
   current code is based on the english wikipedia dumps created on
   2017/04/01 available at
   [105]https://dumps.wikimedia.org/enwiki/20170401/

   of these we use the following (you may need to use more recent dumps)
     * [106]wikipedia page text: this is used to get the text, title, and
       id of wikipedia pages
     * [107]wikipedia titles: this is used for more convenient access to
       wikipedia page titles
     * [108]wikipedia redirects: db dump for wikipedia redirects, used for
       resolving different ways of referencing the same wikipedia entity
     * [109]wikipedia page to ids: contains a mapping of wikipedia page
       and ids, necessary for making the redirect table useful

   to process wikipedia we use
   [110]https://github.com/attardi/wikiextractor with the following
   command:
$ wikiextractor.py --processes 15 -o parsed-wiki --json enwiki-20170401-pages-ar
ticles-multistream.xml.bz2

   do not use the flag to filter disambiguation pages. it uses a simple
   string regex to check the title and articles contents. this introduces
   both false positives and false negatives. we handle the problem of
   filtering these out by using the wikipedia categories dump

   afterwards we use the following command to tar it, compress it with
   lz4, and upload the archive to s3
tar cvf - parsed-wiki | lz4 - parsed-wiki.tar.lz4

wikipedia redirect mapping creation

   the output of this process is stored in
   s3://pinafore-us-west-2/public/wiki_redirects.csv

   all the wikipedia database dumps are provided in mysql sql files. this
   guide has a good explanation of how to install mysql which is necessary
   to use sql dumps. for this task we will need these tables:
     * redirect table:
       [111]https://www.mediawiki.org/wiki/manual:redirect_table
     * page table: [112]https://www.mediawiki.org/wiki/manual:page_table
     * the namespace page is also helpful:
       [113]https://www.mediawiki.org/wiki/manual:namespace

   to install, prepare mysql, and read in the wikipedia sql dumps execute
   the following:
    1. install mysql sudo apt-get install mysql-server and sudo
       mysql_secure_installation
    2. login with something like mysql --user=root --password=something
    3. create a database and use it with create database wikipedia; and
       use wikipedia;
    4. source enwiki-20170401-redirect.sql; (in mysql session)
    5. source enwiki-20170401-page.sql; (in mysql session)
    6. this will take quite a long time, so wait it out...
    7. finally run the query to fetch the redirect mapping and write it to
       a csv by executing bin/redirect.sql with source bin/redirect.sql.
       the file will be located in /var/lib/mysql/redirect.csv which
       requires sudo access to copy
    8. the result of that query is csv file containing a source page id,
       source page title, and target page title. this can be interpretted
       as the source page redirecting to the target page. we filter
       namespace=0 to keep only redirects/pages that are main pages and
       trash things like list/category pages

wikipedia category links creation

   the purpose of this step is to use wikipedia category links to filter
   out disambiguation pages. every wikipedia page has a list of categories
   it belongs to. we filter out any pages which have a category which
   includes the string disambiguation in its name. the output of this
   process is a json file containing a list of page_ids that correspond to
   known disambiguation pages. these are then used downstream to filter
   down to only non-disambiguation wikipedia pages.

   the output of this process is stored in
   s3://pinafore-us-west-2/public/disambiguation_pages.json with the csv
   also saved at s3://pinafore-us-west-2/public/categorylinks.csv

   the process for this is similar to redirects, except that you should
   instead source a file named similar to
   enwiki-20170401-categorylinks.sql, run the script bin/categories.sql,
   and copy categorylinks.csv. afterwards run ./cli.py categories
   disambiguate categorylinks.csv
   data/external/wikipedia/disambiguation_pages.json. this file is
   automatically downloaded by the pipeline code like the redirects file
   so unless you would like to change this or inspect the results, you
   shouldn't need to worry about this.

sql references

   these references may be useful and are the source for these
   instructions:
     * [114]https://www.digitalocean.com/community/tutorials/how-to-instal
       l-mysql-on-ubuntu-16-04
     * [115]https://dev.mysql.com/doc/refman/5.7/en/mysql-batch-commands.h
       tml
     * [116]http://stackoverflow.com/questions/356578/how-to-output-mysql-
       query-results-in-csv-format

debugging faq and solutions

     pyspark uses the wrong version of python

   set pyspark_python to be python3

     importerror: no module named 'pyspark'

   export
   pythonpath=$spark_home/python:$spark_home/python/build:$pythonpath

     valueerror: unknown locale: utf-8

   export lc_all=en_us.utf-8 export lang=en_us.utf-8

     typeerror: namedtuple() missing 3 required keyword-only arguments:
     'verbose', 'rename', and 'module'

   python 3.6 needs spark 2.1.1

qanta id numbering

     * default dataset starts near 0
     * pace adversarial writing event may 2018 starts at 1,000,000
     * december 15 2018 event starts at 2,000,000
     * dataset for hs student of acf 2018 regionals starts at 3,000,000

     *    2019 github, inc.
     * [117]terms
     * [118]privacy
     * [119]security
     * [120]status
     * [121]help

     * [122]contact github
     * [123]pricing
     * [124]api
     * [125]training
     * [126]blog
     * [127]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [128]reload to refresh your
   session. you signed out in another tab or window. [129]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/pinafore/qb/commits/master.atom
   3. https://github.com/pinafore/qb#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/pinafore/qb
  32. https://github.com/join
  33. https://github.com/login?return_to=/pinafore/qb
  34. https://github.com/pinafore/qb/watchers
  35. https://github.com/login?return_to=/pinafore/qb
  36. https://github.com/pinafore/qb/stargazers
  37. https://github.com/login?return_to=/pinafore/qb
  38. https://github.com/pinafore/qb/network/members
  39. https://github.com/pinafore
  40. https://github.com/pinafore/qb
  41. https://github.com/pinafore/qb
  42. https://github.com/pinafore/qb/issues
  43. https://github.com/pinafore/qb/pulls
  44. https://github.com/pinafore/qb/projects
  45. https://github.com/pinafore/qb/wiki
  46. https://github.com/pinafore/qb/pulse
  47. https://github.com/join?source=prompt-code
  48. https://github.com/topics/artificial-intelligence
  49. https://github.com/topics/machine-learning
  50. https://github.com/topics/natural-language-processing
  51. https://github.com/pinafore/qb/commits/master
  52. https://github.com/pinafore/qb/branches
  53. https://github.com/pinafore/qb/releases
  54. https://github.com/pinafore/qb/graphs/contributors
  55. https://github.com/pinafore/qb/blob/master/license
  56. https://github.com/pinafore/qb/search?l=python
  57. https://github.com/pinafore/qb/search?l=shell
  58. https://github.com/pinafore/qb/search?l=hcl
  59. https://github.com/pinafore/qb/find/master
  60. https://github.com/pinafore/qb/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/pinafore/qb
  62. https://github.com/join?return_to=/pinafore/qb
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/entilzha
  68. https://github.com/pinafore/qb/commits?author=entilzha
  69. https://github.com/pinafore/qb/commit/9454d68c55e8305e4448708484dced8f739dbc39
  70. https://github.com/pinafore/qb/commit/9454d68c55e8305e4448708484dced8f739dbc39
  71. https://github.com/pinafore/qb/tree/9454d68c55e8305e4448708484dced8f739dbc39
  72. https://github.com/pinafore/qb/tree/master/bin
  73. https://github.com/pinafore/qb/tree/master/data
  74. https://github.com/pinafore/qb/tree/master/packer
  75. https://github.com/pinafore/qb/tree/master/qanta
  76. https://github.com/pinafore/qb/tree/master/terraform
  77. https://github.com/pinafore/qb/blob/master/.gitignore
  78. https://github.com/pinafore/qb/blob/master/.rsyncignore
  79. https://github.com/pinafore/qb/blob/master/license
  80. https://github.com/pinafore/qb/blob/master/readme.md
  81. https://github.com/pinafore/qb/blob/master/aws.md
  82. https://github.com/pinafore/qb/blob/master/aws.tf
  83. https://github.com/pinafore/qb/blob/master/checkpoint.py
  84. https://github.com/pinafore/qb/blob/master/cli.py
  85. https://github.com/pinafore/qb/blob/master/dataset.py
  86. https://github.com/pinafore/qb/blob/master/environment.yaml
  87. https://github.com/pinafore/qb/blob/master/expo.md
  88. https://github.com/pinafore/qb/blob/master/figures.py
  89. https://github.com/pinafore/qb/blob/master/generate_guesses.py
  90. https://github.com/pinafore/qb/blob/master/jmlr.py
  91. https://github.com/pinafore/qb/blob/master/log4j2.properties
  92. https://github.com/pinafore/qb/blob/master/luigi.id18
  93. https://github.com/pinafore/qb/blob/master/mypy.ini
  94. https://github.com/pinafore/qb/blob/master/protobowl_user.py
  95. https://github.com/pinafore/qb/blob/master/qanta-defaults.yaml
  96. https://github.com/pinafore/qb/blob/master/qanta-hyper.yaml
  97. https://github.com/pinafore/qb/blob/master/security_groups.py
  98. https://github.com/pinafore/qb/blob/master/setup.id18
  99. https://github.com/pinafore/qb/blob/master/setup.py
 100. https://github.com/pinafore/qb/blob/master/slack.py
 101. https://github.com/pinafore/qb/blob/master/slurm-config.yaml
 102. https://github.com/pinafore/qb/blob/master/validate_annotations.py
 103. https://github.com/spotify/luigi
 104. https://luigi.readthedocs.io/en/stable/central_scheduler.html
 105. https://dumps.wikimedia.org/enwiki/20170401/
 106. https://dumps.wikimedia.org/enwiki/20170401/enwiki-20170401-pages-articles-multistream.xml.bz2
 107. https://dumps.wikimedia.org/enwiki/20170401/enwiki-20170401-all-titles.gz
 108. https://dumps.wikimedia.org/enwiki/20170401/enwiki-20170401-redirect.sql.gz
 109. https://dumps.wikimedia.org/enwiki/20170401/enwiki-20170401-page.sql.gz
 110. https://github.com/attardi/wikiextractor
 111. https://www.mediawiki.org/wiki/manual:redirect_table
 112. https://www.mediawiki.org/wiki/manual:page_table
 113. https://www.mediawiki.org/wiki/manual:namespace
 114. https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04
 115. https://dev.mysql.com/doc/refman/5.7/en/mysql-batch-commands.html
 116. http://stackoverflow.com/questions/356578/how-to-output-mysql-query-results-in-csv-format
 117. https://github.com/site/terms
 118. https://github.com/site/privacy
 119. https://github.com/security
 120. https://githubstatus.com/
 121. https://help.github.com/
 122. https://github.com/contact
 123. https://github.com/pricing
 124. https://developer.github.com/
 125. https://training.github.com/
 126. https://github.blog/
 127. https://github.com/about
 128. https://github.com/pinafore/qb
 129. https://github.com/pinafore/qb

   hidden links:
 131. https://github.com/
 132. https://github.com/pinafore/qb
 133. https://github.com/pinafore/qb
 134. https://github.com/pinafore/qb
 135. https://help.github.com/articles/which-remote-url-should-i-use
 136. https://github.com/pinafore/qb#qanta
 137. https://github.com/pinafore/qb#downloading-data
 138. https://github.com/pinafore/qb#file-description
 139. https://github.com/pinafore/qb#dependencies
 140. https://github.com/pinafore/qb#installing-apache-spark
 141. https://github.com/pinafore/qb#installing-elastic-search-56
 142. https://github.com/pinafore/qb#installing-python-packages
 143. https://github.com/pinafore/qb#nltk-models
 144. https://github.com/pinafore/qb#qanta-on-path
 145. https://github.com/pinafore/qb#configuration
 146. https://github.com/pinafore/qb#running-qanta
 147. https://github.com/pinafore/qb#luigi-pipelines
 148. https://github.com/pinafore/qb#qanta-cli
 149. https://github.com/pinafore/qb#aws-s3-checkpointrestore
 150. https://github.com/pinafore/qb#information-on-our-data-sources
 151. https://github.com/pinafore/qb#wikipedia-dumps
 152. https://github.com/pinafore/qb#wikipedia-redirect-mapping-creation
 153. https://github.com/pinafore/qb#wikipedia-category-links-creation
 154. https://github.com/pinafore/qb#sql-references
 155. https://github.com/pinafore/qb#debugging-faq-and-solutions
 156. https://github.com/pinafore/qb#qanta-id-numbering
 157. https://github.com/
