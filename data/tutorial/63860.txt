   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

pca using python (scikit-learn)

   [16]go to the profile of michael galarnyk
   [17]michael galarnyk (button) blockedunblock (button) followfollowing
   dec 4, 2017
   [1*xl0fr9pgmu0gmzmylcuyzw.png]
   original image (left) with different amounts of variance retained

   my last tutorial went over [18]id28 using python. one of
   the things learned was that you can speed up the fitting of a machine
   learning algorithm by changing the optimization algorithm. a more
   common way of speeding up a machine learning algorithm is by using
   principal component analysis (pca). if your learning algorithm is too
   slow because the input dimension is too high, then using pca to speed
   it up can be a reasonable choice. this is probably the most common
   application of pca. another common application of pca is for data
   visualization.

   to understand the value of using pca for data visualization, the first
   part of this tutorial post goes over a basic visualization of the iris
   dataset after applying pca. the second part uses pca to speed up a
   machine learning algorithm (id28) on the mnist dataset.

   with that, let   s get started! if you get lost, i recommend opening the
   [19]video below in a separate tab.

   iframe: [20]/media/19288be5f5cdddd7fb78476c7bbfa84f?postid=e653f8989e60

   pca using python video

   the code used in this tutorial is available below

   [21]pca for data visualization

   [22]pca to speed-up machine learning algorithms

pca for data visualization

   for a lot of machine learning applications it helps to be able to
   visualize your data. visualizing 2 or 3 dimensional data is not that
   challenging. however, even the iris dataset used in this part of the
   tutorial is 4 dimensional. you can use pca to reduce that 4 dimensional
   data into 2 or 3 dimensions so that you can plot and hopefully
   understand the data better.

load iris dataset

   the iris dataset is one of datasets scikit-learn comes with that do not
   require the downloading of any file from some external website. the
   code below will load the iris dataset.
import pandas as pd
url = "[23]https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.d
ata"
# load dataset into pandas dataframe
df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal
width','target'])

   [1*qt_pylwbehttewnedksykq.png]
   original pandas df (features + target)

standardize the data

   pca is effected by scale so you need to scale the features in your data
   before applying pca. use standardscaler to help you standardize the
   dataset   s features onto unit scale (mean = 0 and variance = 1) which is
   a requirement for the optimal performance of many machine learning
   algorithms. if you want to see the negative effect not scaling your
   data can have, scikit-learn has a section on the [24]effects of not
   standardizing your data.
from sklearn.preprocessing import standardscaler
features = ['sepal length', 'sepal width', 'petal length', 'petal width']
# separating out the features
x = df.loc[:, features].values
# separating out the target
y = df.loc[:,['target']].values
# standardizing the features
x = standardscaler().fit_transform(x)

   [1*qxyo-udrmsuzdxie7nnsmg.png]
   the array x (visualized by a pandas dataframe) before and after
   standardization

pca projection to 2d

   the original data has 4 columns (sepal length, sepal width, petal
   length, and petal width). in this section, the code projects the
   original data which is 4 dimensional into 2 dimensions. i should note
   that after id84, there usually isn   t a particular
   meaning assigned to each principal component. the new components are
   just the two main dimensions of variation.
from sklearn.decomposition import pca
pca = pca(n_components=2)
principalcomponents = pca.fit_transform(x)
principaldf = pd.dataframe(data = principalcomponents
             , columns = ['principal component 1', 'principal component 2'])

   [1*7jucr36yguamknhtn4gt8a.png]
   pca and keeping the top 2 principal components
finaldf = pd.concat([principaldf, df[['target']]], axis = 1)

   concatenating dataframe along axis = 1. finaldf is the final dataframe
   before plotting the data.
   [1*4q1kh0zkehrnhf7eg_yhtq.png]
   concatenating dataframes along columns to make finaldf before graphing

visualize 2d projection

   this section is just plotting 2 dimensional data. notice on the graph
   below that the classes seem well separated from each other.
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('principal component 1', fontsize = 15)
ax.set_ylabel('principal component 2', fontsize = 15)
ax.set_title('2 component pca', fontsize = 20)
targets = ['iris-setosa', 'iris-versicolor', 'iris-virginica']
colors = ['r', 'g', 'b']
for target, color in zip(targets,colors):
    indicestokeep = finaldf['target'] == target
    ax.scatter(finaldf.loc[indicestokeep, 'principal component 1']
               , finaldf.loc[indicestokeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()

   [1*duz0mens6vfc35xtyr88bg.png]
   2 component pca graph

explained variance

   the explained variance tells you how much information (variance) can be
   attributed to each of the principal components. this is important as
   while you can convert 4 dimensional space to 2 dimensional space, you
   lose some of the variance (information) when you do this. by using the
   attribute explained_variance_ratio_, you can see that the first
   principal component contains 72.77% of the variance and the second
   principal component contains 23.03% of the variance. together, the two
   components contain 95.80% of the information.
pca.explained_variance_ratio_

pca to speed-up machine learning algorithms

   one of the most important applications of pca is for speeding up
   machine learning algorithms. using the iris dataset would be
   impractical here as the dataset only has 150 rows and only 4 feature
   columns. the mnist database of handwritten digits is more suitable as
   it has 784 feature columns (784 dimensions), a training set of 60,000
   examples, and a test set of 10,000 examples.

download and load the data

   you can also add a data_home parameter to fetch_mldata to change where
   you download the data.
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('mnist original')

   the images that you downloaded are contained in mnist.data and has a
   shape of (70000, 784) meaning there are 70,000 images with 784
   dimensions (784 features).

   the labels (the integers 0   9) are contained in mnist.target. the
   features are 784 dimensional (28 x 28 images) and the labels are simply
   numbers from 0   9.

split data into training and test sets

   typically the train test split is 80% training and 20% test. in this
   case, i chose 6/7th of the data to be training and 1/7th of the data to
   be in the test set.
from sklearn.model_selection import train_test_split
# test_size: what proportion of original data is used for test set
train_img, test_img, train_lbl, test_lbl = train_test_split( mnist.data, mnist.t
arget, test_size=1/7.0, random_state=0)

standardize the data

   the text in this paragraph is almost an exact copy of what was written
   earlier. pca is effected by scale so you need to scale the features in
   the data before applying pca. you can transform the data onto unit
   scale (mean = 0 and variance = 1) which is a requirement for the
   optimal performance of many machine learning algorithms. standardscaler
   helps standardize the dataset   s features. note you fit on the training
   set and transform on the training and test set. if you want to see the
   negative effect not scaling your data can have, scikit-learn has a
   section on the [25]effects of not standardizing your data.
from sklearn.preprocessing import standardscaler
scaler = standardscaler()
# fit on training set only.
scaler.fit(train_img)
# apply transform to both the training set and the test set.
train_img = scaler.transform(train_img)
test_img = scaler.transform(test_img)

import and apply pca

   notice the code below has .95 for the number of components parameter.
   it means that scikit-learn choose the minimum number of principal
   components such that 95% of the variance is retained.
from sklearn.decomposition import pca
# make an instance of the model
pca = pca(.95)

   fit pca on training set. note: you are fitting pca on the training set
   only.
pca.fit(train_img)

   note: you can find out how many components pca choose after fitting the
   model using pca.n_components_ . in this case, 95% of the variance
   amounts to 330 principal components.

apply the mapping (transform) to both the training set and the test set.

train_img = pca.transform(train_img)
test_img = pca.transform(test_img)

apply id28 to the transformed data

   step 1: import the model you want to use

   in sklearn, all machine learning models are implemented as python
   classes
from sklearn.linear_model import logisticregression

   step 2: make an instance of the model.
# all parameters not specified are set to their defaults
# default solver is incredibly slow which is why it was changed to 'lbfgs'
logisticregr = logisticregression(solver = 'lbfgs')

   step 3: training the model on the data, storing the information learned
   from the data

   model is learning the relationship between digits and labels
logisticregr.fit(train_img, train_lbl)

   step 4: predict the labels of new data (new images)

   uses the information the model learned during the model training
   process

   the code below predicts for one observation
# predict for one observation (image)
logisticregr.predict(test_img[0].reshape(1,-1))

   the code below predicts for multiple observations at once
# predict for one observation (image)
logisticregr.predict(test_img[0:10])

   measuring model performance

   while accuracy is not always the best metric for machine learning
   algorithms (precision, recall, f1 score, [26]roc curve, etc would be
   better), it is used here for simplicity.
logisticregr.score(test_img, test_lbl)

timing of fitting id28 after pca

   the whole point of this section of the tutorial was to show that you
   can use pca to speed up the fitting of machine learning algorithms. the
   table below shows how long it took to fit id28 on my
   macbook after using pca (retaining different amounts of variance each
   time).
   [1*xkuk0wlnlhajys1zbt-7wa.png]
   time it took to fit id28 after pca with different
   fractions of variance retained

image reconstruction from compressed representation

   the earlier parts of the tutorial have demonstrated using pca to
   compress high dimensional data to lower dimensional data. i wanted to
   briefly mention that pca can also take the compressed representation of
   the data (lower dimensional data) back to an approximation of the
   original high dimensional data. if you are interested in the code that
   produces the image below, check out my [27]github.
   [1*gob8zbscym7hhuhjvrmjyg.png]
   original image (left) and approximations (right) of the original data
   after pca

closing thoughts

   this is a post that i could have written on for a lot longer as pca has
   many different uses. i hope this post helps you with whatever you are
   working on. if you any questions or thoughts on the tutorial, feel free
   to reach out in the comments below or through [28]twitter.

     * [29]machine learning
     * [30]data science
     * [31]python
     * [32]scikit learn
     * [33]pca

   (button)
   (button)
   (button) 3.5k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of michael galarnyk

[35]michael galarnyk

   data scientist at scripps research institute

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.5k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e653f8989e60
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_uujenwazmbob---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@galarnykmichael?source=post_header_lockup
  17. https://towardsdatascience.com/@galarnykmichael
  18. https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a
  19. https://www.youtube.com/watch?v=kappbm1ysqu
  20. https://towardsdatascience.com/media/19288be5f5cdddd7fb78476c7bbfa84f?postid=e653f8989e60
  21. https://github.com/mgalarnyk/python_tutorials/blob/master/sklearn/pca/pca_data_visualization_iris_dataset_blog.ipynb
  22. https://github.com/mgalarnyk/python_tutorials/blob/master/sklearn/pca/pca_to_speed-up_machine_learning_algorithms.ipynb
  23. https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
  24. http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py
  25. http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py
  26. https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0
  27. https://github.com/mgalarnyk/python_tutorials/blob/master/sklearn/pca/pca_image_reconstruction_and_such.ipynb
  28. https://twitter.com/galarnykmichael
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/data-science?source=post
  31. https://towardsdatascience.com/tagged/python?source=post
  32. https://towardsdatascience.com/tagged/scikit-learn?source=post
  33. https://towardsdatascience.com/tagged/pca?source=post
  34. https://towardsdatascience.com/@galarnykmichael?source=footer_card
  35. https://towardsdatascience.com/@galarnykmichael
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/e653f8989e60/share/twitter
  42. https://medium.com/p/e653f8989e60/share/facebook
  43. https://medium.com/p/e653f8989e60/share/twitter
  44. https://medium.com/p/e653f8989e60/share/facebook
