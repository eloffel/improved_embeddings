   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

solving a simple classification problem with python         fruits lovers    edition

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   dec 3, 2017
   [1*gnnvxoh_h8n7z2d7cecbuw.jpeg]
   photo credit: pixabay

   in this post, we   ll implement several machine learning algorithms in
   python using [18]scikit-learn, the most popular machine learning tool
   for python. using a simple dataset for the task of training a
   classifier to distinguish between different types of fruits.

   the purpose of this post is to identify the machine learning algorithm
   that is best-suited for the problem at hand; thus, we want to compare
   different algorithms, selecting the best-performing one. let   s get
   started!

data

   the fruits dataset was created by [19]dr. iain murray from university
   of edinburgh. he bought a few dozen oranges, lemons and apples of
   different varieties, and recorded their measurements in a table. and
   then the professors at university of michigan formatted the fruits data
   slightly and it can be downloaded from [20]here.

   let   s have a look the first a few rows of the data.
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
fruits = pd.read_table('fruit_data_with_colors.txt')
fruits.head()

   [1*fexfrlszj7h7tuxm3ewx1a.png]
   figure 1

   each row of the dataset represents one piece of the fruit as
   represented by several features that are in the table   s columns.

   we have 59 pieces of fruits and 7 features in the dataset:
print(fruits.shape)

   (59, 7)

   we have four types of fruits in the dataset:
print(fruits['fruit_name'].unique())

   [   apple       mandarin       orange       lemon   ]

   the data is pretty balanced except mandarin. we will just have to go
   with it.
print(fruits.groupby('fruit_name').size())

   [1*ntwprhtwbemqlqmpztqtyw.png]
   figure 2
import seaborn as sns
sns.countplot(fruits['fruit_name'],label="count")
plt.show()

   [1*kevibv4hf6ejbh5xup2ilg.png]
   figure 3

visualization

     * box plot for each numeric variable will give us a clearer idea of
       the distribution of the input variables:

fruits.drop('fruit_label', axis=1).plot(kind='box', subplots=true, layout=(2,2),
 sharex=false, sharey=false, figsize=(9,9),
                                        title='box plot for each input variable'
)
plt.savefig('fruits_box')
plt.show()

   [1*kodjdtia0uj-beupp-ziva.png]
   figure 4
     * it looks like perhaps color score has a near gaussian distribution.

import pylab as pl
fruits.drop('fruit_label' ,axis=1).hist(bins=30, figsize=(9,9))
pl.suptitle("histogram for each numeric input variable")
plt.savefig('fruits_hist')
plt.show()

   [1*qsbl9jybuugmg5kmg30rxg.png]
   figure 5
     * some pairs of attributes are correlated (mass and width). this
       suggests a high correlation and a predictable relationship.

from pandas.tools.plotting import scatter_matrix
from matplotlib import cm
feature_names = ['mass', 'width', 'height', 'color_score']
x = fruits[feature_names]
y = fruits['fruit_label']
cmap = cm.get_cmap('gnuplot')
scatter = pd.scatter_matrix(x, c = y, marker = 'o', s=40, hist_kwds={'bins':15},
 figsize=(9,9), cmap = cmap)
plt.suptitle('scatter-matrix for each input variable')
plt.savefig('fruits_scatter_matrix')

   [1*d4jrcyebgfzqh-8bronwew.png]
   figure 6

statistical summary

   [1*oaihqfz0dvyxuvlnwbv7cw.png]
   figure 7

   we can see that the numerical values do not have the same scale. we
   will need to apply scaling to the test set that we computed for the
   training set.

create training and test sets and apply scaling

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
from sklearn.preprocessing import minmaxscaler
scaler = minmaxscaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

build models

id28

from sklearn.linear_model import logisticregression
logreg = logisticregression()
logreg.fit(x_train, y_train)
print('accuracy of id28 classifier on training set: {:.2f}'
     .format(logreg.score(x_train, y_train)))
print('accuracy of id28 classifier on test set: {:.2f}'
     .format(logreg.score(x_test, y_test)))

   accuracy of id28 classifier on training set: 0.70
   accuracy of id28 classifier on test set: 0.40

decision tree

from sklearn.tree import decisiontreeclassifier
clf = decisiontreeclassifier().fit(x_train, y_train)
print('accuracy of decision tree classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('accuracy of decision tree classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))

   accuracy of decision tree classifier on training set: 1.00
   accuracy of decision tree classifier on test set: 0.73

k-nearest neighbors

from sklearn.neighbors import kneighborsclassifier
knn = kneighborsclassifier()
knn.fit(x_train, y_train)
print('accuracy of id92 classifier on training set: {:.2f}'
     .format(knn.score(x_train, y_train)))
print('accuracy of id92 classifier on test set: {:.2f}'
     .format(knn.score(x_test, y_test)))

   accuracy of id92 classifier on training set: 0.95
   accuracy of id92 classifier on test set: 1.00

id156

from sklearn.discriminant_analysis import lineardiscriminantanalysis
lda = lineardiscriminantanalysis()
lda.fit(x_train, y_train)
print('accuracy of lda classifier on training set: {:.2f}'
     .format(lda.score(x_train, y_train)))
print('accuracy of lda classifier on test set: {:.2f}'
     .format(lda.score(x_test, y_test)))

   accuracy of lda classifier on training set: 0.86
   accuracy of lda classifier on test set: 0.67

gaussian naive bayes

from sklearn.naive_bayes import gaussiannb
gnb = gaussiannb()
gnb.fit(x_train, y_train)
print('accuracy of gnb classifier on training set: {:.2f}'
     .format(gnb.score(x_train, y_train)))
print('accuracy of gnb classifier on test set: {:.2f}'
     .format(gnb.score(x_test, y_test)))

   accuracy of gnb classifier on training set: 0.86
   accuracy of gnb classifier on test set: 0.67

support vector machine

from sklearn.id166 import svc
id166 = svc()
id166.fit(x_train, y_train)
print('accuracy of id166 classifier on training set: {:.2f}'
     .format(id166.score(x_train, y_train)))
print('accuracy of id166 classifier on test set: {:.2f}'
     .format(id166.score(x_test, y_test)))

   accuracy of id166 classifier on training set: 0.61
   accuracy of id166 classifier on test set: 0.33

   the knn algorithm was the most accurate model that we tried. the
   confusion matrix provides an indication of no error made on the test
   set. however, the test set was very small.
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
pred = knn.predict(x_test)
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

   [1*z4myp0mnbspqgacd9x4oug.png]
   figure 7

plot the decision boundary of the id92 classifier

import matplotlib.cm as cm
from matplotlib.colors import listedcolormap, boundarynorm
import matplotlib.patches as mpatches
import matplotlib.patches as mpatches
x = fruits[['mass', 'width', 'height', 'color_score']]
y = fruits['fruit_label']
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)
def plot_fruit_knn(x, y, n_neighbors, weights):
    x_mat = x[['height', 'width']].as_matrix()
    y_mat = y.as_matrix()
# create color maps
    cmap_light = listedcolormap(['#ffaaaa', '#aaffaa', '#aaaaff','#afafaf'])
    cmap_bold  = listedcolormap(['#ff0000', '#00ff00', '#0000ff','#afafaf'])
clf = neighbors.kneighborsclassifier(n_neighbors, weights=weights)
    clf.fit(x_mat, y_mat)
# plot the decision boundary by assigning a color in the color map
    # to each mesh point.

    mesh_step_size = .01  # step size in the mesh
    plot_symbol_size = 50

    x_min, x_max = x_mat[:, 0].min() - 1, x_mat[:, 0].max() + 1
    y_min, y_max = x_mat[:, 1].min() - 1, x_mat[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),
                         np.arange(y_min, y_max, mesh_step_size))
    z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
# put the result into a color plot
    z = z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, z, cmap=cmap_light)
# plot training points
    plt.scatter(x_mat[:, 0], x_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bol
d, edgecolor = 'black')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
patch0 = mpatches.patch(color='#ff0000', label='apple')
    patch1 = mpatches.patch(color='#00ff00', label='mandarin')
    patch2 = mpatches.patch(color='#0000ff', label='orange')
    patch3 = mpatches.patch(color='#afafaf', label='lemon')
    plt.legend(handles=[patch0, patch1, patch2, patch3])
plt.xlabel('height (cm)')
plt.ylabel('width (cm)')
plt.title("4-class classification (k = %i, weights = '%s')"
           % (n_neighbors, weights))
plt.show()
plot_fruit_knn(x_train, y_train, 5, 'uniform')

   [1*ph9h_p2w29i6raxr3a_uva.png]
   figure 8
k_range = range(1, 20)
scores = []
for k in k_range:
    knn = kneighborsclassifier(n_neighbors = k)
    knn.fit(x_train, y_train)
    scores.append(knn.score(x_test, y_test))
plt.figure()
plt.xlabel('k')
plt.ylabel('accuracy')
plt.scatter(k_range, scores)
plt.xticks([0,5,10,15,20])

   [1*bepn_3r5yvckexada3rmmw.png]
   figure 9

   for this particular dateset, we obtain the highest accuracy when k=5.

summary

   in this post, we focused on the prediction accuracy. our objective is
   to learn a model that has a good generalization performance. such a
   model maximizes the prediction accuracy. we identified the machine
   learning algorithm that is best-suited for the problem at hand (i.e.
   fruit types classification); therefore, we compared different
   algorithms and selected the best-performing one.

   source code that created this post can be found [21]here. i would be
   pleased to receive feedback or questions on any of the above.

     * [22]machine learning
     * [23]data science
     * [24]supervised learning
     * [25]data visualization
     * [26]towards data science

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 19 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of susan li

[28]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [29]https://www.linkedin.com/in/susanli/

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d20ab6b071d2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_t1pu8gbdf8ur---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. http://scikit-learn.org/stable/
  19. http://homepages.inf.ed.ac.uk/imurray2/
  20. https://github.com/susanli2016/machine-learning-with-python/blob/master/fruit_data_with_colors.txt
  21. https://github.com/susanli2016/machine-learning-with-python/blob/master/solving a simple classification problem with python.ipynb
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/data-science?source=post
  24. https://towardsdatascience.com/tagged/supervised-learning?source=post
  25. https://towardsdatascience.com/tagged/data-visualization?source=post
  26. https://towardsdatascience.com/tagged/towards-data-science?source=post
  27. https://towardsdatascience.com/@actsusanli?source=footer_card
  28. https://towardsdatascience.com/@actsusanli
  29. https://www.linkedin.com/in/susanli/
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/d20ab6b071d2/share/twitter
  36. https://medium.com/p/d20ab6b071d2/share/facebook
  37. https://medium.com/p/d20ab6b071d2/share/twitter
  38. https://medium.com/p/d20ab6b071d2/share/facebook
