   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*bskyc6qe4lk2jdmyzsmhoa.jpeg]

stochastic id119 with momentum

   [16]go to the profile of vitaly bushaev
   [17]vitaly bushaev (button) blockedunblock (button) followfollowing
   dec 4, 2017

   this is part 2 of my series on optimization algorithms used for
   training neural networks and machine learning models. [18]part 1 was
   about stochastic id119. in this post i presume basic
   knowledge about neural networks and id119 algorithm. if you
   don   t know anything about neural networks or how to train them, feel
   free to read my first post before reading this one.

   in this post i   ll talk about simple addition to classic sgd algorithm,
   called momentum which almost always works better and faster than
   stochastic id119. momentum [1] or sgd with momentum is
   method which helps accelerate gradients vectors in the right
   directions, thus leading to faster converging. it is one of the most
   popular optimization algorithms and many state-of-the-art models are
   trained using it. before jumping over to the update equations of the
   algorithm, let   s look at some math that underlies the work of momentum.

exponentially weighed averages

   exponentially weighed averages deal with sequences of numbers. suppose,
   we have some sequence s which is noisy. for this example i plotted
   cosine function and added some gaussian noise. it looks like this:
   [1*u5g-mnikrzjvni12eptblw.png]
   out sequence.

   note, that even though these dots seem very close to each over, none of
   them share x coordinate. it is a unique number for each point. that   s
   the number the defines the index of each point in our sequence s.

   what we want to do with this data is, instead of using it, we want some
   kind of    moving    average which would    denoise    the data and bring it
   closer to the original function. exponentially weighed averages can
   give us a pictures which looks like this:
   [1*fhhakq1nwn7hk1kbndarqw.png]
   momentum         data from exponentially weighed averages.

   as you can see, that   s a pretty good result. instead of having data
   with a lot of noise, we got much smoother line, which is closer to the
   original function than data we had. exponentially weighed averages
   define a new sequence v with the following equation:
   [1*kqc1uiyuxdza5isseg4gow.png]

   that sequence v is the one plotted yellow above. beta is another
   hyper-parameter which takes values from 0 to one. i used beta = 0.9
   above. it is a good value and most often used in sgd with momentum.
   intuitively, you can think of beta as follows. we   re approximately
   averaging over last 1 / (1- beta) points of sequence. let   s see how the
   choice of beta affects our new sequence v.
   [1*buj-rjg3ww6rsclnpczkza.png]
   exponentially weighed averages for different values of beta.

   as you can see, with smaller numbers of beta, the new sequence turns
   out to be fluctuating a lot, because we   re averaging over smaller
   number of examples and therefore are    closer    to the noisy data. with
   bigger values of beta, like beta=0.98, we get much smother curve, but
   it   s a little bit shifted to the right, because we average over larger
   number of example(around 50 for beta=0.98). beta = 0.9 provides a good
   balance between these two extremes.

math section

   this section isn   t necessary to use momentum in your projects, so feel
   free to skip it. but it provides a little bit more intuition of how
   momentum works.

   let   s expand our definition of exponentially weighed averages for three
   consecutive elements of the new sequence v.
   [1*fg2t0oqyafidx0exevty3g.png]
   v         new sequence. s         original sequence.

   combining all of them together we get:
   [1*exyss1dsx9onryglm3pkjq.png]

   and then simplifying it a bit:
   [1*cdsuuj5wmaq2ajrj1acxra.png]

   from this equation we see, that the value of tth number of the new
   sequence is dependent on all the previous values 1..t of the original
   sequence s. all of the values from s are assigned some weight. this
   weight is beta to power of i multiplied by (1- beta) for (t - i)th
   value of s. because beta is less than 1, it becomes even smaller when
   we take beta to the power of some positive number. so the older values
   of s get much smaller weight and therefore contribute less for overall
   value of the current point of v. at some point the weight is going to
   be so small that we can almost say that we    forget    that value because
   its contribution becomes too small to notice. a good value to use for
   this approximation is when weight becomes smaller than 1 / e. bigger
   values of beta require larger values of the power to be smaller than 1
   / e. this is why with bigger values of beta we   re averaging over bigger
   numbers of points. the following graph shows how fast weights get
   smaller with older values of s compared to threshold = 1 / e, where we
   mostly    forget    values that are older.
   [1*ptkdnvkpzixrbrdskxy4pq.png]

   the last thing to notice is that the first couple of iterations will
   provide a pretty bad averages because we don   t have enough values yet
   to average over. the solution is instead of using v, we can use what   s
   called bias-corrected version of v.
   [1*ysq5noczr0zsuyi-9kzk8a.png]

   where b = beta. with large values of t, b to the power of t will be
   indistinguishable from zero, thus not changing our values of v at all.
   but for small values of t, this equation will produce a little bit
   better results. with momentum, people usually don   t bother to implement
   this part, because learning stabilizes pretty fast.

sgd with momentum

   we   ve defined a way to get    moving    average of some sequence, which
   changes alongside with data. how can we apply this for training neural
   networks ? the can average over our gradients. i   ll define how it   s
   done in momentum and then move on to explain why it might work better.

   i   ll provide two definitions of sgd with momentum, which are pretty
   much just two different ways of writing the same equations. first, is
   how [19]andrew ng, defines it in his [20]deep learning specialization
   on coursera. the way he explains it, we define a momentum, which is a
   moving average of our gradients. we then use it to update the weight of
   the network. this could written as follows:
   [1*v5fnciao4ypml0of_8v2yw.png]

   where l         is id168, triangular thing         gradient w.r.t weight
   and alpha         learning rate. the other way, which is most popular way to
   write momentum update rules, is less intuitive and just omits (1 -
   beta) term.
   [1*3ykczwuuu6xybpa2hvtclw.png]

   this is pretty much identical to the first pair of equation, the only
   difference is that you need to scale learning rate by (1 - beta)
   factor.

nesterov accelerated gradient

   nesterov momentum is a slightly different version of the momentum
   update that has recently been gaining popularity. in this version we   re
   first looking at a point where current momentum is pointing to and
   computing gradients from that point. it becomes much clearer when you
   look at the picture.
   [1*hjslxzmjyvzgf5a_moqevq.jpeg]
   source ([21]stanford cs231n class)

   nesterov momentum can be define with the following formulas:
   [1*pgq7p480s764tsqdp8gjfw.png]

why momentum works

   in this section i   d like to talk a little bit about why momentum most
   of the times will be better than classic sgd.

   with stochastic id119 we don   t compute the exact derivate of
   our id168. instead, we   re estimating it on a small batch. which
   means we   re not always going in the optimal direction, because our
   derivatives are    noisy   . just like in my graphs above. so,
   exponentially weighed averages can provide us a better estimate which
   is closer to the actual derivate than our noisy calculations. this is
   one reason why momentum might work better than classic sgd.

   the other reason lies in ravines. ravine is an area, where the surface
   curves much more steeply in one dimension than in another. ravines are
   common near local minimas in deep learning and sgd has troubles
   navigating them. sgd will tend to oscillate across the narrow ravine
   since the negative gradient will point down one of the steep sides
   rather than along the ravine towards the optimum. momentum helps
   accelerate gradients in the right direction. this is expressed in the
   following pictures:
   [1*jhyidkzf1imuzk487q_kiw.gif]
   [1*utip1url2cahaa-dfu3nkw.gif]
   left         sgd without momentum, right    sgd with momentum. (source:
   [22]genevieve b. orr)

conclusion

   i hope this post gave you some intuition on how and why sgd with
   momentum works. it   s actually one of the most popular optimization
   algorithms in deep learning and used even more frequently than more
   advanced ones.

   below i provide some references, where you can learn more about
   optimization in deep learning.

references

   [1] ning qian. [23]on the momentum term in id119 learning
   algorithms. neural networks : the official journal of the international
   neural network society, 12(1):145   151, 1999

   [2] distill, [24]why momentum really works

   [3] deeplearning.ai

   [4] ruder (2016). [25]an overview of id119 optimization
   algorithms. arxiv preprint arxiv:1609.04747

   [5] ruder (2017) [26]optimization for deep learning highlights in 2017.

   [6] [27]stanford cs231n lecture notes.

   [7] [28]fast.ai

     * [29]machine learning
     * [30]deep learning
     * [31]neural networks
     * [32]optimization
     * [33]towards data science

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of vitaly bushaev

[35]vitaly bushaev

   c++, python developer

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a84097641a5d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_g9jrlioqutff---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@bushaev?source=post_header_lockup
  17. https://towardsdatascience.com/@bushaev
  18. https://towardsdatascience.com/how-do-we-train-neural-networks-edd985562b73
  19. http://www.andrewng.org/
  20. https://www.deeplearning.ai/
  21. http://cs231n.github.io/neural-networks-3/
  22. https://www.willamette.edu/~gorr/classes/cs449/momrate.html
  23. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.5612&rep=rep1&type=pdf
  24. https://distill.pub/2017/momentum/
  25. https://arxiv.org/abs/1609.04747
  26. http://ruder.io/deep-learning-optimization-2017/index.html
  27. http://cs231n.github.io/neural-networks-3/
  28. http://fast.ai/
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/deep-learning?source=post
  31. https://towardsdatascience.com/tagged/neural-networks?source=post
  32. https://towardsdatascience.com/tagged/optimization?source=post
  33. https://towardsdatascience.com/tagged/towards-data-science?source=post
  34. https://towardsdatascience.com/@bushaev?source=footer_card
  35. https://towardsdatascience.com/@bushaev
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/a84097641a5d/share/twitter
  42. https://medium.com/p/a84097641a5d/share/facebook
  43. https://medium.com/p/a84097641a5d/share/twitter
  44. https://medium.com/p/a84097641a5d/share/facebook
