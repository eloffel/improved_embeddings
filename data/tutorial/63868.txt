   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

dl01: neural networks theory

   [14]go to the profile of sarthak gupta
   [15]sarthak gupta (button) blockedunblock (button) followfollowing
   oct 16, 2017
   [0*-q_ot1bnm4bqaoks.]

   hello, hackers! time for a small coffee break.

   this is the first in a series of blog posts in which i   ll implement
   various deep learning algorithms and research papers. before
   implementing anything new, i   ll explain the basic concept behind that.
   knowledge of [16]python and numpy are pre-requisites for this post.

     the accompanying code can be found [17]here. in the next post, i   ll
     do a line-by-line explanation of the code. this post covers the
     theory of a basic neural network.

the brain and id158s

biological neuron

   brain consists of a number of brain cells (neurons) connected
   end-to-end. a neuron ([18]figure 2) takes electric impulse as signal,
   do some processing on the message, and send it to another neuron. so, a
   brain cell mainly consists of four parts:

   1. dendrites: accepts inputs (electric impulses)

   2. soma: process the inputs

   3. axon: turn the processed inputs into a form that can be accepted by
   the next neuron i.e. converts processed inputs into output.

   4. synapses: the electrochemical contact between neurons. using
   synapses, a neuron can transfer the outputs of that neuron to the
   inputs of the next neuron.

   artificial neuron

   to try to imitate the brain,    artificial neurons    were created with a
   similar structure.

   artificial neurons are extremely simplified versions of biological
   neurons. the inputs are represented by xi. each input is    processed   
   i.e., multiplied by some weight wi, and all wixi products are added.
   after that, the processed input is passed through a function, known as
   the    activation function   , which converts the processed input to
   output. the value at the output of an artificial neuron is called the
      activation    value. finally, there is an output path.
   [1*dwuzfaamopfqjamhd9zvvg.png]
   figure 1: architecture of ann

   these artificial neurons can be connected in many ways to give
      id158s   . the example shown above is a shallow
   neural network. more layers of neurons can be added to make the network
      deep   . adding more layers (usually) increases the accuracy of the
   network. in such a network, there is one input layer, one or more
   hidden layers, and one output layer, as shown in [19]figure 1. the
   layers in between are called hidden layers because they are hidden from
   the user. figure 2 shows the similarities between artificial and
   biological neurons.
   [1*pverrxotrzq0mck6m0g5aw.png]
   figure 2: similarity between artificial and biological neurons

working of neural networks

components

   neural networks can (accurately) predict an output upon receiving some
   input. this section explores how it is done.

   broadly, a neural network consists of four components:

   1. artificial neurons

   2. topology         how the neurons are connected

   3. weights

   4. learning algorithm

forward pass

   before the neural network can accurately predict the output, it needs
   to be trained on some data. data usually consists of input-output
   pairs. the outputs in this dataset are known as    labels    or    targets   .

   the    training    phase starts by randomly initialising all weights (i.e.
   weights associated with each artificial neuron). then, inputs are fed
   to the network, activations of all nodes in hidden layers are
   calculated, and finally, we get the activation of the output layer
   (which is the actual output). the process described above is known as
      forward pass   . initially, the output is extremely inaccurate due to
   the random initialisation of the weights. the goal here is to finally
   arrive at an optimum value of the weights, such that the neural network
   can predict the output (given some input) with reasonable accuracy. the
   algorithm used to do this is known as    id26   .

id26

   after the forward pass, we have some activations at the output layer.
   the output should ideally be equal to the label. however, that is not
   the case in the beginning, since the weights are randomly initisalized.
   so, error is calculated, and    backpropagated    through the network. the
   function that measures the error is known as the    cost function   . the
   aim is to adjust the weights to minimize this cost function. the
   technique used to minimize the cost function is called    gradient
   descent   .

id119

   the plot of cost function vs weight is more or less convex and looks
   something like [20]figure 3:
   [0*dqzrp6zmszemvzhm.png]
   figure 3: id119

   the idea is that we make small steps in the direction of the gradient,
   and we hope that eventually, we   ll be at the global minima. however,
   that would be the case only if the plot is convex. usually, the plot is
   not perfectly convex, resulting in a few local minima. for now, we   ll
   assume that the local minima are good approximations of the global
   minimum (which is usually the case).

   so, we make small updates on the weights, each time moving them in the
   direction of the gradient. we multiply the updates with a parameter,
   known as    learning rate   .

   this post gave a brief overview of the elements involved in making a
   neural network. detailed analysis(and maths) of the algorithms will be
   done in separate posts.

     want to read more? go to [21]dl02!

     * [22]machine learning
     * [23]neural networks
     * [24]theory
     * [25]deep learning
     * [26]python

   (button)
   (button)
   (button) 276 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of sarthak gupta

[28]sarthak gupta

   [29]www.github.com/sar-gupta [30]https://sar-gupta.github.io

     (button) follow
   [31]hacker noon

[32]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 276
     * (button)
     *
     *

   [33]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [34]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c02ccc897864
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_ndnpv1wuppe8---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@sargupta?source=post_header_lockup
  15. https://hackernoon.com/@sargupta
  16. https://www.datacamp.com/courses/intro-to-python-for-data-science
  17. https://github.com/thesemicolonguy/neural-network-from-scratch
  18. https://www.wikiwand.com/en/neuron#/media/file:blausen_0657_multipolarneuron.png
  19. http://cs231n.stanford.edu/
  20. https://www.youtube.com/watch?v=5u4g23_oohi
  21. https://medium.com/@thesemicolonguy/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257
  22. https://hackernoon.com/tagged/machine-learning?source=post
  23. https://hackernoon.com/tagged/neural-networks?source=post
  24. https://hackernoon.com/tagged/theory?source=post
  25. https://hackernoon.com/tagged/deep-learning?source=post
  26. https://hackernoon.com/tagged/python?source=post
  27. https://hackernoon.com/@sargupta?source=footer_card
  28. https://hackernoon.com/@sargupta
  29. http://www.github.com/sar-gupta
  30. https://sar-gupta.github.io/
  31. https://hackernoon.com/?source=footer_card
  32. https://hackernoon.com/?source=footer_card
  33. https://hackernoon.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/c02ccc897864/share/twitter
  37. https://medium.com/p/c02ccc897864/share/facebook
  38. https://medium.com/p/c02ccc897864/share/twitter
  39. https://medium.com/p/c02ccc897864/share/facebook
