   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

dl02: writing a neural network from scratch (code)

   [14]go to the profile of sarthak gupta
   [15]sarthak gupta (button) blockedunblock (button) followfollowing
   oct 16, 2017
   [0*ncak9vkwtvouxulr.jpg]

   check out the previous tutorial:

     [16]dl01: writing a neural network from scratch (theory)

   hello hackers, time for another coffee break!

   this time, let   s start coding.
   [0*febvmpbnrs7_xwzs.gif]

     the accompanying code can be found [17]here.

   iframe: [18]/media/b7b578f88910da28f544875f77968e19?postid=b32f4877c257

   numpy is used for mathematical calculations in python.

   dill is used to store all variables in a python file, so that they can
   be loaded later. install using pip3 install dill.

   now, we make a class for the neural_network:

   iframe: [19]/media/df777cc8b0e03be696adc36a2098cae5?postid=b32f4877c257

   and a class for layer:

   iframe: [20]/media/8094164d36c4cb09873dbae078b25613?postid=b32f4877c257

   okay, so let   s dive in!

   class neural_network has the following functions:
     * __init__ : it takes 4 things as inputs:
       1. num_layers: number of layers in the network.
       2. num_nodes: it is a list of size num_layers, specifying the
       number of nodes in each layer.
       3. activation_function: it is also a list, specifying, the
       activation function for each layer (activation function for first
       layer will usually be none. it can take values sigmoid, tanh, relu,
       softmax.
       4. cost_function: function to calculate error between predicted
       output and actual label/target. it can take values mean_squared,
       cross_id178.
                layers are initialized with the given number of nodes in each
       layer. weights associated with every layer are initialized.
     * train : it takes 6 inputs:
       1. batch_size: mini batch size for id119.
       2. inputs: inputs to be given to the network.
       3. labels: target values.
       4. num_epochs: number of epochs i.e. how many times the program
       should iterate over all training .
       5. learning_rate: learning rate for the algorithm, as discussed in
       dl01.
       6. filename: the name of the file that will finally store all
       variables after training. (filename must have the extension .pkl).
                first, there is a loop for iterating over number of epochs.
       then, there is a nested loop for iterating over all mini batches.
       after that, forward_pass, calculate_error and back_pass are called
       over the mini batch (which is consistent with what we learnt in
       dl01).
     * forward_pass : it takes just 1 input:
       1. inputs: mini batch of inputs.
                this function multiplies inputs with weights, applies the
       activation function and stores the output as activations of next
       layer. this process is repeated for all layers, until we have some
       activations in the output layer.
     * calculate_error : it also takes 1 input:
       1. labels: mini batch of labels.
                this function calculates the error between the predicted output
       (i.e. activations at the output layer after a forward pass) and the
       target values. this error is then backpropagated through the
       network in back_pass function.
     * back_pass : it takes 1 input:
       1. labels: mini batch of labels.
                this function implements the id26 algorithm. this
       algorithm will be discussed in detail in a separate post. basically
       what it does is, it calculates the gradient, multiplies it with a
       learning rate and subtracts the product from the existing weights.
       this is done for all layers, from the last to the first.
     * predict : it takes 2 inputs:
       1. filename: the file from which trained model is to be loaded.
       2. input: the input for which we want the prediction.
                it does a forward pass, and then converts the output into
       one-hot encoding i.e. the maximum element of array is 1 and all
       others are 0.
     * check_accuracy : it takes 3 inputs:
       1. filename: the file from which trained model is to be loaded.
       2. inputs: input test data.
       3. labels: target test data.
                this function does pretty much the same thing as predict. but
       instead of returning the predicted output, it commpares the
       predictions with the labels, and then calculates accuracy as
       correct*100/total.

   class layer has the following functions:
     * __init__ : it takes 3 arguments:
       1. num_nodes_in_layer: number of nodes in that layer.
       2. num_nodes_in_next_layer: number of nodes in the next layer.
       3. activation_function: activation function for that layer.
                this function is called from the constructor of neural_network
       class. it initializes one layer at a time. the weights of the last
       layer are set to none.

   this post explained the code in detail. although convolutional neural
   networks (id98s) perform much better on images, i trained a neural
   network on mnist just for the feel of it. id98s will be covered in a
   later blog post.
   [0*d2zc0qfzx3k6vgoi.]

   i hope you can now implement a neural network from scratch yourself.
   happy coding!

     * [21]machine learning
     * [22]deep learning
     * [23]neural networks
     * [24]code
     * [25]python

   (button)
   (button)
   (button) 299 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of sarthak gupta

[27]sarthak gupta

   [28]www.github.com/sar-gupta [29]https://sar-gupta.github.io

     (button) follow
   [30]hacker noon

[31]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 299
     * (button)
     *
     *

   [32]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [33]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b32f4877c257
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_g9azjjqaoxmt---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@sargupta?source=post_header_lockup
  15. https://hackernoon.com/@sargupta
  16. https://medium.com/@thesemicolonguy/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864
  17. https://github.com/thesemicolonguy/neural-network-from-scratch
  18. https://hackernoon.com/media/b7b578f88910da28f544875f77968e19?postid=b32f4877c257
  19. https://hackernoon.com/media/df777cc8b0e03be696adc36a2098cae5?postid=b32f4877c257
  20. https://hackernoon.com/media/8094164d36c4cb09873dbae078b25613?postid=b32f4877c257
  21. https://hackernoon.com/tagged/machine-learning?source=post
  22. https://hackernoon.com/tagged/deep-learning?source=post
  23. https://hackernoon.com/tagged/neural-networks?source=post
  24. https://hackernoon.com/tagged/code?source=post
  25. https://hackernoon.com/tagged/python?source=post
  26. https://hackernoon.com/@sargupta?source=footer_card
  27. https://hackernoon.com/@sargupta
  28. http://www.github.com/sar-gupta
  29. https://sar-gupta.github.io/
  30. https://hackernoon.com/?source=footer_card
  31. https://hackernoon.com/?source=footer_card
  32. https://hackernoon.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/b32f4877c257/share/twitter
  36. https://medium.com/p/b32f4877c257/share/facebook
  37. https://medium.com/p/b32f4877c257/share/twitter
  38. https://medium.com/p/b32f4877c257/share/facebook
