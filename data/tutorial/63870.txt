   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

dl03: id119

   [14]go to the profile of sarthak gupta
   [15]sarthak gupta (button) blockedunblock (button) followfollowing
   oct 20, 2017

   the previous two posts can be found here:
   [16]dl01: neural networks theory
   [17]dl02: writing a neural network from scratch (code)
   [1*5y-mqiykmizfhdi-x8peaw.jpeg]

   time for a break, coders! let   s dive into a little bit of maths, shall
   we? *insert nerdy smile*
   [0*vw4luaf5ncxhoyko.gif]

   to understand id119, let   s conisder id75.
   id75 is a technique, where given some data points, we try
   to fit a line through those points and then make predictions by
   extrapolating that line. the challenge is to find the best fit for the
   line. for the sake of simplicity, we   ll assume that the output(y)
   depends on just one input variable(x) i.e. the data points given to us
   are of the form (x, y).

   now we want to fit a line through these data points. the line will
   obviously be of the form y = mx + b, and we want to find the best
   values for m and b. this line is called our    hypothesis   .

   to get started with, m and b are randomly initialized. hence, we get a
   random line in the beginning. our goal is to update these values so
   that the resulting line gives least error. we   ll use mean squared error
   as the cost function(j), that calculates the error between the actual
   value output, and prediction from the hypothesis.

   j(m, b) = (1/n) *     (y    - (mx    + b))  

   where n represents the total number of data points and i represents the
   i      data point (i takes values from 1 to n).

   this error function is typically convex. in simple terms, we can say
   that this error function typically has just one minimum (the global
   minimum).
   [0*x09h19ylqmw_yjw9.png]
   a convex function

   the horizontal axes represent the variables on which the function
   depends, and the vertical dimension represents the output. in our case,
   the horizontal axes are represented by m and b, while the vertical axis
   is represented by y.

   when we start with random values of m and b, we get some value of y
   correspondingly. error is minimum at the lowest point on this graph, so
   our goal is to move down the slope to finally reach the bottom-most
   point. the question is, how?

   before moving on, i   d like you to recall some of your high school
   maths.
   [0*p4y5qm3nkuxweo6u.jpg]

   the slope of the tangent at any point on a graph is equal to the
   derivative of the graph w.r.t. input variables.
   [0*v-1bzwjkjzm6k4vv.png]
   source:
   [18]https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-re
   gression/

   now, as you can see, the slope of tangent at the bottom-most point on
   the graph is 0 i.e. partial derivatives of j at the bottom-most point
   are 0. to get to the bottom-most point, we have to move in the
   direction of the slope i.e. we   ll update values of m and b, such that
   we eventually get to the optimum values, where error function is
   minimum.

   the update equations would be as follows:
   [1*juys82bhn0qastd0vkqbsa.gif]
   [1*2qrbbz4epxlwlwxwst9jew.gif]

   here,    is called the    learning rate   , and it decides how large steps
   to take in the direction of the gradient. if the value of    is very
   small, reaching the optimum value is guaranteed, but it   ll take a lot
   of time to converge. if    is very large, then the values of m and b
   might overshoot the optimal values, and then the error will start to
   increase instead of decreasing. hence, learning rate plays an important
   part in id76.

   a visual representation of id119 in action has been shown
   below:
   [0*d7zg46wrdkx54pbu.gif]
   source:
   [19]https://alykhantejani.github.io/images/gradient_descent_line_graph.
   gif

how does it work intuitively?

   for the sake of simplicity, consider an error function j that depends
   on only one variable (or weight) w.
   [0*yzbz_qja1v8_dise.png]

   consider the situation as shown in figure i.e. initial weight is more
   than the optimal value. here, initially the slope is large and
   positive. so, in the update equation, w is reduced. as w keeps getting
   reduced, notice that the gradient also reduces, and hence the updates
   become smaller and smaller and eventually, it converges to the minimum.

   now consider the situation when the initial weight is less than the
   optimal value. here, the slope is negative. so, in the update equation,
   the value of w increases after every update. initially, the slope is
   large, so the updates are large. as w gets increased, the slope keeps
   getting smaller, and hence the updates keep getting smaller and
   eventually converge to the minimum.

   so, in any case, the weights will finally converge to a value such that
   the derivative of the cost function is 0 (given that the value of alpha
   is sufficiently less).

   in this post, i covered id119 for id75.
   however, this is the basic idea behind neural networks too. all the
   weights in the neural network are updated according to this algorithm.
   id119 is used for optimization in the [20]id26
   algorithm, which has been covered in the [21]next post.

     id119 101:
     sometimes the smallest step in the right direction ends up being the
     biggest step of your life.

     * [22]machine learning
     * [23]deep learning
     * [24]id119
     * [25]neural networks
     * [26]algorithms

   (button)
   (button)
   (button) 131 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of sarthak gupta

[28]sarthak gupta

   [29]www.github.com/sar-gupta [30]https://sar-gupta.github.io

     (button) follow
   [31]hacker noon

[32]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 131
     * (button)
     *
     *

   [33]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [34]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/719aff91c7d6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/dl03-gradient-descent-719aff91c7d6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/dl03-gradient-descent-719aff91c7d6&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_xsilb1en9dw2---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@sargupta?source=post_header_lockup
  15. https://hackernoon.com/@sargupta
  16. https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864
  17. https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257
  18. https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/
  19. https://alykhantejani.github.io/images/gradient_descent_line_graph.gif
  20. https://hackernoon.com/dl04-id26-bbcfbf2528d6
  21. https://hackernoon.com/dl04-id26-bbcfbf2528d6
  22. https://hackernoon.com/tagged/machine-learning?source=post
  23. https://hackernoon.com/tagged/deep-learning?source=post
  24. https://hackernoon.com/tagged/gradient-descent?source=post
  25. https://hackernoon.com/tagged/neural-networks?source=post
  26. https://hackernoon.com/tagged/algorithms?source=post
  27. https://hackernoon.com/@sargupta?source=footer_card
  28. https://hackernoon.com/@sargupta
  29. http://www.github.com/sar-gupta
  30. https://sar-gupta.github.io/
  31. https://hackernoon.com/?source=footer_card
  32. https://hackernoon.com/?source=footer_card
  33. https://hackernoon.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/719aff91c7d6/share/twitter
  37. https://medium.com/p/719aff91c7d6/share/facebook
  38. https://medium.com/p/719aff91c7d6/share/twitter
  39. https://medium.com/p/719aff91c7d6/share/facebook
