   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

dl04: id26

   [14]go to the profile of sarthak gupta
   [15]sarthak gupta (button) blockedunblock (button) followfollowing
   dec 6, 2017

   the previous three posts can be found here:
   [16]dl01: neural networks theory
   [17]dl02: writing a neural network from scratch (code)
   [18]dl03: id119

   so, welcome to part 4 of this series! this would require a little bit
   of maths, so basic calculus is a pre-requisite.

   in this post, i   ll try to explain backprop with a very simple neural
   network, shown below:
   [1*c0zju-mxn8qdznsrhsoclw.jpeg]

   l1, l2 and l3 represent the layers in the neural net. the numbers in
   square bracket represent the layer number. i   ve numbered every node on
   each layer. e.g. second node of first layer is numbered as 2[1], and so
   on.
   i   ve labelled every weight too. e.g. the weight connecting second node
   of second layer (2[2]) to first node of third layer (1[3]) is w21[2].

   i   ll assume that we   re using activation function g(z).

   the basic concept behind id26 is to calculate error
   derivatives. after the forward pass through the net, we calculate the
   error, and then update the weights through [19]id119 (using
   the error derivatives calculated by backprop).

   if you understand [20]chain rule in differentiation, you   ll easily
   understand id26.

   first, let us write the equations for the forward pass.

   let x1and x2 be the inputs at l1.

   i   m denoting z as the weighted sum of previous layer, and a as the
   output of a node after applying non-linearity/activation function g.

     z1[2] = w11[1]*x1 + w21[1]*x2
     a1[2] = g(z1[2])
     z2[2] = w12[1]*x1 + w22[1]*x2
     a2[2] = g(z2[2])
     z3[2] = w13[1]*x1 + w23[1]*x2
     a3[2] = g(z3[2])

     z1[3] = w11[2]*a1[2] + w21[2]*a2[2] + w31[2]*a3[2]
     a1[3] = g(z1[3])

   now i   ll use mse as a id168.

     e = (1/2)  (a1[3]   t1)  , where t1 is the target label.

   to use id119 to backpropagate through the net, we   ll have to
   calculate the derivative of this error w.r.t. every weight, and then
   perform weight updates.

   we need to find de/dwij[k], where, wij is a weight at layer k.

   by chain rule, we have
   [1*uuomsvg0oahuucvqql0gbg.jpeg]

   now, we can calculate these three terms on the rhs as follows,
   [1*n_6gvkugnzzgs4gpufylog.jpeg]

   therefore, we have
   [1*vreb_nahot0k057 qqbg.jpeg]

   similarly,
   [1*tbbylbnwyctehazhjbcngg.png]

     let   1[3] = (a1[3]         t) * (g   (z1[3])).

   hence, we have
   [1*wbfkxtg5vontfj5i0ijibw.png]

   here, we   ll call   1[3] as the error propagated by node 1 of layer 3.

   now, we want to go one layer back.
   [1*wccimtwvqxdqemlllprzda.jpeg]

   on simplifying, we get
   [1*jrhokyqyaudrzr82ezvfrq.jpeg]
   note: the value of weight w11[2] is to be used before the update was
   performed on it. this applies to all equations and weights below.

   similarly,
   [1*7oefji_l2edba6-bglksmw.jpeg]

   similarly backpropagating through other nodes in l2, we get
   [1*l_2601rncyk__rrywhsuda.jpeg]
   [1*4a0hgmndngmawgb333btyg.jpeg]

   in terms of   , these can be written as
   [1*fxp3v3kel0rieil6nurm7g.png]

   when we have all the error derivatives, weights are updated as:
   [1*pushvu4zxwvkkgztkkwo1q.png]
   (wij[k] refers to weight wij at layer k)

   where    is called the    learning rate   ,

   so, this was backprop for you! it is completely understandable if your
   brain is like this right now:
   [1*w1u-zvev1iwd9szbwsvd5w.jpeg]

   to get a better grasp of this, you can try deriving it yourself.

   you can even do it for some other error functions like cross-id178
   loss (with softmax). or for a particular activation function like
   sigmoid, tanh, relu or leaky rely.

   i hope i was able to clear the basics of id26 through this
   post. a lot of time and effort was put into this, so feedback would be
   appreciated!

     * [21]machine learning
     * [22]id26
     * [23]deep learning
     * [24]mathematics
     * [25]artificial intelligence

   (button)
   (button)
   (button) 91 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of sarthak gupta

[27]sarthak gupta

   [28]www.github.com/sar-gupta [29]https://sar-gupta.github.io

     (button) follow
   [30]hacker noon

[31]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 91
     * (button)
     *
     *

   [32]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [33]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bbcfbf2528d6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/dl04-id26-bbcfbf2528d6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/dl04-id26-bbcfbf2528d6&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_grrbvfcw41s6---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@sargupta?source=post_header_lockup
  15. https://hackernoon.com/@sargupta
  16. https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864
  17. https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257
  18. https://hackernoon.com/dl03-gradient-descent-719aff91c7d6
  19. https://hackernoon.com/dl03-gradient-descent-719aff91c7d6
  20. http://www.mathcentre.ac.uk/resources/uploaded/mc-ty-chain-2009-1.pdf
  21. https://hackernoon.com/tagged/machine-learning?source=post
  22. https://hackernoon.com/tagged/id26?source=post
  23. https://hackernoon.com/tagged/deep-learning?source=post
  24. https://hackernoon.com/tagged/mathematics?source=post
  25. https://hackernoon.com/tagged/artificial-intelligence?source=post
  26. https://hackernoon.com/@sargupta?source=footer_card
  27. https://hackernoon.com/@sargupta
  28. http://www.github.com/sar-gupta
  29. https://sar-gupta.github.io/
  30. https://hackernoon.com/?source=footer_card
  31. https://hackernoon.com/?source=footer_card
  32. https://hackernoon.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/bbcfbf2528d6/share/twitter
  36. https://medium.com/p/bbcfbf2528d6/share/facebook
  37. https://medium.com/p/bbcfbf2528d6/share/twitter
  38. https://medium.com/p/bbcfbf2528d6/share/facebook
