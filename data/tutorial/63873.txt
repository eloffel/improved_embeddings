community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   87
   87
   aditya sharma
   december 5th, 2017
   python
   +4

convolutional neural networks in python with keras

   in this tutorial, you   ll learn how to implement convolutional neural
   networks (id98s) in python with keras, and how to overcome overfitting
   with dropout.

   you might have already heard of image or facial recognition or
   self-driving cars. these are real-life implementations of convolutional
   neural networks (id98s). in this blog post, you will learn and
   understand how to implement these deep, feed-forward artificial neural
   networks in keras and also learn how to overcome overfitting with the
   id173 technique called "dropout".

   more specifically, you'll tackle the following topics in today's
   tutorial:
     * you will be introduced to [3]convolutional neural networks;
     * then, you'll first try to [4]understand the data. you'll use python
       and its libraries to load, [5]explore and analyze your data,
     * after that, you'll [6]preprocess your data: you'll learn how to
       resize, rescale, convert your labels into one-hot encoding vectors
       and split up your data in training and validation sets;
     * with all of this done, you can [7]construct the neural network
       model: you'll learn how to model the data and form the network.
       next, you'll compile, train and evaluate the model, visualizing the
       accuracy and loss plots;
     * then, you will learn about the concept of overfitting and how you
       can overcome it by [8]adding a dropout layer;
     * with this information, you can revisit your original model and
       re-train the model. you'll also [9]re-evaluate your new model and
       compare the results of both the models;
     * next, you'll make [10]predictions on the test data, convert the
       probabilities into class labels and plot few test samples that your
       model correctly classified and incorrectly classified;
     * finally, you will visualize the [11]classification report which
       will give you more in-depth intuition about which class was
       (in)correctly classified by your model.

   would you like to take a course on keras and deep learning in python?
   consider taking datacamp's [12]deep learning in python course!

   also, don't miss our [13]keras cheat sheet, which shows you the six
   steps that you need to go through to build neural networks in python
   with code examples!

convolutional neural network: introduction

   by now, you might already know about machine learning and deep
   learning, a computer science branch that studies the design of
   algorithms that can learn. deep learning is a subfield of machine
   learning that is inspired by id158s, which in turn
   are inspired by biological neural networks.

   a specific kind of such a deep neural network is the convolutional
   network, which is commonly referred to as id98 or convnet. it's a deep,
   feed-forward id158. remember that feed-forward
   neural networks are also called multi-layer id88s(mlps), which
   are the quintessential deep learning models. the models are called
   "feed-forward" because information fl   ows right through the model.
   there are no feedback connections in which outputs of the model are fed
   back into itself.

   id98s specifically are inspired by the biological visual cortex. the
   cortex has small regions of cells that are sensitive to the specific
   areas of the visual field. this idea was expanded by a captivating
   experiment done by hubel and wiesel in 1962 (if you want to know more,
   here's a [14]video). in this experiment, the researchers showed that
   some individual neurons in the brain activated or fired only in the
   presence of edges of a particular orientation like vertical or
   horizontal edges. for example, some neurons fired when exposed to
   vertical sides and some when shown a horizontal edge. hubel and wiesel
   found that all of these neurons were well ordered in a columnar fashion
   and that together they were able to produce visual perception. this
   idea of specialized components inside of a system having specific tasks
   is one that machines use as well and one that you can also find back in
   id98s.

   convolutional neural networks have been one of the most influential
   innovations in the field of id161. they have performed a lot
   better than traditional id161 and have produced
   state-of-the-art results. these neural networks have proven to be
   successful in many different real-life case studies and applications,
   like:
     * image classification, id164, segmentation, face
       recognition;
     * self driving cars that leverage id98 based vision systems;
     * classification of crystal structure using a convolutional neural
       network;
     * and many more, of course!

   to understand this success, you'll have to go back to 2012, the year in
   which alex krizhevsky used convolutional neural networks to win that
   year's id163 competition, reducing the classification error from 26%
   to 15%.

   note that id163 large scale visual recognition challenge (ilsvrc)
   began in the year 2010 is an annual competition where research teams
   assess their algorithms on the given data set and compete to achieve
   higher accuracy on several visual recognition tasks.

   this was the time when neural networks [15]regained prominence after
   quite some time. this is often called the "third wave of neural
   networks". the other two waves were in the 1940s until the 1960s and in
   the 1970s to 1980s.

   alright, you know that you'll be working with feed-forward networks
   that are inspired by the biological visual cortex, but what does that
   actually mean?

   take a look at the picture below:

   convolutional neural networks python

   figure: convolutional neural network from [16]wikimedia

   the image shows you that you feed an image as an input to the network,
   which goes through multiple convolutions, subsampling, a fully
   connected layer and finally outputs something.

   but what are all these concepts?
    1. the convolution layer computes the output of neurons that are
       connected to local regions or receptive fields in the input, each
       computing a dot product between their weights and a small receptive
       field to which they are connected to in the input volume. each
       computation leads to extraction of a feature map from the input
       image. in other words, imagine you have an image represented as a
       5x5 matrix of values, and you take a 3x3 matrix and slide that 3x3
       window or kernel around the image. at each position of that matrix,
       you multiply the values of your 3x3 window by the values in the
       image that are currently being covered by the window. as a result,
       you'll get a single number that represents all the values in that
       window of the images. you use this layer to filtering: as the
       window moves over the image, you check for patterns in that section
       of the image. this works because of filters, which are multiplied
       by the values outputted by the convolution.
    2. the objective of subsampling is to get an input representation by
       reducing its dimensions, which helps in reducing overfitting. one
       of the techniques of subsampling is max pooling. with this
       technique, you select the highest pixel value from a region
       depending on its size. in other words, max pooling takes the
       largest value from the window of the image currently covered by the
       kernel. for example, you can have a max-pooling layer of size 2 x 2
       will select the maximum pixel intensity value from 2 x 2 region.
       you're right to think that the pooling layer then works a lot like
       the convolution layer! you also take a kernel or a window and move
       it over the image; the only difference is the function that is
       applied to the kernel and the image window isn't linear.
       convolutional neural network in python
       figure: max-pooling from [17]wikipedia
    3. the objective of the fully connected layer is to flatten the
       high-level features that are learned by convolutional layers and
       combining all the features. it passes the flattened output to the
       output layer where you use a softmax classifier or a sigmoid to
       predict the input class label.

   for more information, you can go [18]here.

the fashion-mnist data set

   before you go ahead and load in the data, it's good to take a look at
   what you'll exactly be working with! the [19]fashion-mnist dataset is a
   dataset of zalando's article images, with 28x28 grayscale images of
   70,000 fashion products from 10 categories, and 7,000 images per
   category. the training set has 60,000 images, and the test set has
   10,000 images. you can double check this later when you have loaded in
   your data! ;)

   fashion-mnist is similar to the mnist dataset that you might already
   know, which you use to classify handwritten digits. that means that the
   image dimensions, training and test splits are similar to the mnist
   dataset. tip: if you want to learn how to implement an multi-layer
   id88 (mlp) for classification tasks with this latter dataset, go
   to [20]this tutorial.

   you can find the fashion-mnist dataset [21]here, but you can also load
   it with the help of specific tensorflow and keras modules. you'll see
   how this works in the next section!

load the data

   keras comes with a library called datasets, which you can use to load
   datasets out of the box: you download the data from the server and
   speeds up the process since you no longer have to download the data to
   your computer. the train and test images along with the labels are
   loaded and stored in variables train_x, train_y, test_x, test_y,
   respectively.
from keras.datasets import fashion_mnist
(train_x,train_y), (test_x,test_y) = fashion_mnist.load_data()

using tensorflow backend.

   great! that was pretty simple, wasn't it?

   you have probably done this a million times by now, but it's always an
   essential step to get started. now you're completely set to start
   analyzing, processing and modeling your data!

analyze the data

   let's now analyze how images in the dataset look like. even though you
   know the dimension of the images by now, it's still worth the effort to
   analyze it programmatically: you might have to rescale the image pixels
   and resize the images.
import numpy as np
from keras.utils import to_categorical
import matplotlib.pyplot as plt
%matplotlib inline

print('training data shape : ', train_x.shape, train_y.shape)

print('testing data shape : ', test_x.shape, test_y.shape)

('training data shape : ', (60000, 28, 28), (60000,))
('testing data shape : ', (10000, 28, 28), (10000,))

   from the above output, you can see that the training data has a shape
   of 60000 x 28 x 28 since there are 60,000 training samples each of 28 x
   28 dimension. similarly, the test data has a shape of 10000 x 28 x 28
   since there are 10,000 testing samples.
# find the unique numbers from the train labels
classes = np.unique(train_y)
nclasses = len(classes)
print('total number of outputs : ', nclasses)
print('output classes : ', classes)

('total number of outputs : ', 10)
('output classes : ', array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8))

   there's also a total of ten output classes that range from 0 to 9.

   also, don't forget to take a look at what the images in your dataset:
plt.figure(figsize=[5,5])

# display the first image in training data
plt.subplot(121)
plt.imshow(train_x[0,:,:], cmap='gray')
plt.title("ground truth : {}".format(train_y[0]))

# display the first image in testing data
plt.subplot(122)
plt.imshow(test_x[0,:,:], cmap='gray')
plt.title("ground truth : {}".format(test_y[0]))

text(0.5,1,u'ground truth : 9')

   [output_19_1_udnwlg.png]

   the output of above two plots looks like an ankle boot, and this class
   is assigned a class label of 9. similarly, other fashion products will
   have different labels, but similar products will have same labels. this
   means that all the 7,000 ankle boot images will have a class label of
   9.

id174

   as you could see in the above plot, the images are grayscale images
   have pixel values that range from 0 to 255. also, these images have a
   dimension of 28 x 28. as a result, you'll need to preprocess the data
   before you feed it into the model.
     * as a first step, convert each 28 x 28 image of the train and test
       set into a matrix of size 28 x 28 x 1 which is fed into the
       network.

train_x = train_x.reshape(-1, 28,28, 1)
test_x = test_x.reshape(-1, 28,28, 1)
train_x.shape, test_x.shape

((60000, 28, 28, 1), (10000, 28, 28, 1))

     * the data right now is in an int8 format, so before you feed it into
       the network you need to convert its type to float32, and you also
       have to rescale the pixel values in range 0 - 1 inclusive. so let's
       do that!

train_x = train_x.astype('float32')
test_x = test_x.astype('float32')
train_x = train_x / 255.
test_x = test_x / 255.

     * now you need to convert the class labels into a one-hot encoding
       vector.

   in one-hot encoding, you convert the categorical data into a vector of
   numbers. the reason why you convert the categorical data in one hot
   encoding is that machine learning algorithms cannot work with
   categorical data directly. you generate one boolean column for each
   category or class. only one of these columns could take on the value 1
   for each sample. hence, the term one-hot encoding.

   for your problem statement, the one hot encoding will be a row vector,
   and for each image, it will have a dimension of 1 x 10. the important
   thing to note here is that the vector consists of all zeros except for
   the class that it represents, and for that, it is 1. for example, the
   ankle boot image that you plotted above has a label of 9, so for all
   the ankle boot images, the one hot encoding vector would be [0 0 0 0 0
   0 0 0 1 0].

   so let's convert the training and testing labels into one-hot encoding
   vectors:
# change the labels from categorical to one-hot encoding
train_y_one_hot = to_categorical(train_y)
test_y_one_hot = to_categorical(test_y)

# display the change for category label using one-hot encoding
print('original label:', train_y[0])
print('after conversion to one-hot:', train_y_one_hot[0])

('original label:', 9)
('after conversion to one-hot:', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
 0.,  1.]))

   that's pretty clear, right? note that you can also print the
   train_y_one_hot, which will display a matrix of size 60000 x 10 in
   which each row depicts one-hot encoding of an image.
     * this last step is a crucial one. in machine learning or any data
       specific task, you should partition the data correctly. for the
       model to generalize well, you split the training data into two
       parts, one designed for training and another one for validation. in
       this case, you will train the model on 80\% of the training data
       and validate it on 20\% of the remaining training data. this will
       also help to reduce overfitting since you will be validating the
       model on the data it would not have seen in training phase, which
       will help in boosting the test performance.

from sklearn.model_selection import train_test_split
train_x,valid_x,train_label,valid_label = train_test_split(train_x, train_y_one_
hot, test_size=0.2, random_state=13)

   for one last time let's check the shape of training and validation set.
train_x.shape,valid_x.shape,train_label.shape,valid_label.shape

((48000, 28, 28, 1), (12000, 28, 28, 1), (48000, 10), (12000, 10))

the network

   the images are of size 28 x 28. you convert the image matrix to an
   array, rescale it between 0 and 1, reshape it so that it's of size 28 x
   28 x 1, and feed this as an input to the network.

   you'll use three convolutional layers:
     * the first layer will have 32-3 x 3 filters,
     * the second layer will have 64-3 x 3 filters and
     * the third layer will have 128-3 x 3 filters.

   in addition, there are three max-pooling layers each of size 2 x 2.

   [fashion-mnist-architecture_htbpsz.png]

   figure: architecture of the model

model the data

   first, let's import all the necessary modules required to train the
   model.
import keras
from keras.models import sequential,input,model
from keras.layers import dense, dropout, flatten
from keras.layers import conv2d, maxpooling2d
from keras.layers.id172 import batchid172
from keras.layers.advanced_activations import leakyrelu

   you will use a batch size of 64 using a higher batch size of 128 or 256
   is also preferable it all depends on the memory. it contributes
   massively to determining the learning parameters and affects the
   prediction accuracy. you will train the network for 20 epochs.
batch_size = 64
epochs = 20
num_classes = 10

neural network architecture

   in keras, you can just stack up layers by adding the desired layer one
   by one. that's exactly what you'll do here: you'll first add a first
   convolutional layer with conv2d(). note that you use this function
   because you're working with images! next, you add the leaky relu
   activation function which helps the network learn non-linear decision
   boundaries. since you have ten different classes, you'll need a
   non-linear decision boundary that could separate these ten classes
   which are not linearly separable.

   more specifically, you add leaky relus because they attempt to fix the
   problem of dying rectified linear units (relus). the relu activation
   function is used a lot in neural network architectures and more
   specifically in convolutional networks, where it has proven to be more
   effective than the widely used logistic sigmoid function. as of 2017,
   this activation function is the most popular one for deep neural
   networks. the relu function allows the activation to be thresholded at
   zero. however, during the training, relu units can "die". this can
   happen when a large gradient flows through a relu neuron: it can cause
   the weights to update in such a way that the neuron will never activate
   on any data point again. if this happens, then the gradient flowing
   through the unit will forever be zero from that point on. leaky relus
   attempt to solve this: the function will not be zero but will instead
   have a small negative slope.

   next, you'll add the max-pooling layer with maxpooling2d() and so on.
   the last layer is a dense layer that has a softmax activation function
   with 10 units, which is needed for this multi-class classification
   problem.
fashion_model = sequential()
fashion_model.add(conv2d(32, kernel_size=(3, 3),activation='linear',input_shape=
(28,28,1),padding='same'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d((2, 2),padding='same'))
fashion_model.add(conv2d(64, (3, 3), activation='linear',padding='same'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d(pool_size=(2, 2),padding='same'))
fashion_model.add(conv2d(128, (3, 3), activation='linear',padding='same'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d(pool_size=(2, 2),padding='same'))
fashion_model.add(flatten())
fashion_model.add(dense(128, activation='linear'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(dense(num_classes, activation='softmax'))

compile the model

   after the model is created, you compile it using the adam optimizer,
   one of the most popular optimization algorithms. you can read more
   about this optimizer [22]here. additionally, you specify the loss type
   which is categorical cross id178 which is used for multi-class
   classification, you can also use binary cross-id178 as the loss
   function. lastly, you specify the metrics as accuracy which you want to
   analyze while the model is training.
fashion_model.compile(loss=keras.losses.categorical_crossid178, optimizer=kera
s.optimizers.adam(),metrics=['accuracy'])

   let's visualize the layers that you created in the above step by using
   the summary function. this will show some parameters (weights and
   biases) in each layer and also the total parameters in your model.
fashion_model.summary()

_________________________________________________________________
layer (type)                 output shape              param #
=================================================================
conv2d_51 (conv2d)           (none, 28, 28, 32)        320
_________________________________________________________________
leaky_re_lu_57 (leakyrelu)   (none, 28, 28, 32)        0
_________________________________________________________________
max_pooling2d_49 (maxpooling (none, 14, 14, 32)        0
_________________________________________________________________
conv2d_52 (conv2d)           (none, 14, 14, 64)        18496
_________________________________________________________________
leaky_re_lu_58 (leakyrelu)   (none, 14, 14, 64)        0
_________________________________________________________________
max_pooling2d_50 (maxpooling (none, 7, 7, 64)          0
_________________________________________________________________
conv2d_53 (conv2d)           (none, 7, 7, 128)         73856
_________________________________________________________________
leaky_re_lu_59 (leakyrelu)   (none, 7, 7, 128)         0
_________________________________________________________________
max_pooling2d_51 (maxpooling (none, 4, 4, 128)         0
_________________________________________________________________
flatten_17 (flatten)         (none, 2048)              0
_________________________________________________________________
dense_33 (dense)             (none, 128)               262272
_________________________________________________________________
leaky_re_lu_60 (leakyrelu)   (none, 128)               0
_________________________________________________________________
dense_34 (dense)             (none, 10)                1290
=================================================================
total params: 356,234
trainable params: 356,234
non-trainable params: 0
_________________________________________________________________

train the model

   it's finally time to train the model with keras' fit() function! the
   model trains for 20 epochs. the fit() function will return a history
   object; by storying the result of this function in fashion_train, you
   can use it later to plot the accuracy and id168 plots between
   training and validation which will help you to analyze your model's
   performance visually.
fashion_train = fashion_model.fit(train_x, train_label, batch_size=batch_size,ep
ochs=epochs,verbose=1,validation_data=(valid_x, valid_label))

train on 48000 samples, validate on 12000 samples
epoch 1/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.4661 - acc
: 0.8311 - val_loss: 0.3320 - val_acc: 0.8809
epoch 2/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.2874 - acc
: 0.8951 - val_loss: 0.2781 - val_acc: 0.8963
epoch 3/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.2420 - acc
: 0.9111 - val_loss: 0.2501 - val_acc: 0.9077
epoch 4/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.2088 - acc
: 0.9226 - val_loss: 0.2369 - val_acc: 0.9147
epoch 5/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.1838 - acc
: 0.9324 - val_loss: 0.2602 - val_acc: 0.9070
epoch 6/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.1605 - acc
: 0.9396 - val_loss: 0.2264 - val_acc: 0.9193
epoch 7/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.1356 - acc
: 0.9488 - val_loss: 0.2566 - val_acc: 0.9180
epoch 8/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.1186 - acc
: 0.9553 - val_loss: 0.2556 - val_acc: 0.9149
epoch 9/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.0985 - acc
: 0.9634 - val_loss: 0.2681 - val_acc: 0.9204
epoch 10/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.0873 - acc
: 0.9670 - val_loss: 0.2712 - val_acc: 0.9221
epoch 11/20
48000/48000 [==============================] - 59s 1ms/step - loss: 0.0739 - acc
: 0.9721 - val_loss: 0.2757 - val_acc: 0.9202
epoch 12/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0628 - acc
: 0.9767 - val_loss: 0.3126 - val_acc: 0.9132
epoch 13/20
48000/48000 [==============================] - 61s 1ms/step - loss: 0.0569 - acc
: 0.9789 - val_loss: 0.3556 - val_acc: 0.9081
epoch 14/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0452 - acc
: 0.9833 - val_loss: 0.3441 - val_acc: 0.9189
epoch 15/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0421 - acc
: 0.9847 - val_loss: 0.3400 - val_acc: 0.9165
epoch 16/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0379 - acc
: 0.9861 - val_loss: 0.3876 - val_acc: 0.9195
epoch 17/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0405 - acc
: 0.9855 - val_loss: 0.4112 - val_acc: 0.9164
epoch 18/20
48000/48000 [==============================] - 60s 1ms/step - loss: 0.0285 - acc
: 0.9897 - val_loss: 0.4150 - val_acc: 0.9181
epoch 19/20
48000/48000 [==============================] - 61s 1ms/step - loss: 0.0322 - acc
: 0.9877 - val_loss: 0.4584 - val_acc: 0.9196
epoch 20/20
48000/48000 [==============================] - 61s 1ms/step - loss: 0.0262 - acc
: 0.9906 - val_loss: 0.4396 - val_acc: 0.9205

   finally! you trained the model on fashion-mnist for 20 epochs, and by
   observing the training accuracy and loss, you can say that the model
   did a good job since after 20 epochs the training accuracy is 99% and
   the training loss is quite low.

   however, it looks like the model is overfitting, as the validation loss
   is 0.4396 and the validation accuracy is 92%. overfitting gives an
   intuition that the network has memorized the training data very well
   but is not guaranteed to work on unseen data, and that is why there is
   a difference in the training and validation accuracy.

   you probably need to handle this. in next sections, you'll learn how
   you can make your model perform much better by adding a dropout layer
   into the network and keeping all the other layers unchanged.

   but first, let's evaluate the performance of your model on the test set
   before you come on to a conclusion.

model evaluation on the test set

test_eval = fashion_model.evaluate(test_x, test_y_one_hot, verbose=0)

print('test loss:', test_eval[0])
print('test accuracy:', test_eval[1])

('test loss:', 0.46366268818555401)
('test accuracy:', 0.91839999999999999)

   the test accuracy looks impressive. it turns out that your classifier
   does better than the benchmark that was reported [23]here, which is an
   id166 classifier with mean accuracy of 0.897. also, the model does well
   compared to some of the deep learning models mentioned on the
   [24]github profile of the creators of fashion-mnist dataset.

   however, you saw that the model looked like it was overfitting. are
   these results really all that good?

   let's put your model evaluation into perspective and plot the accuracy
   and loss plots between training and validation data:
accuracy = fashion_train.history['acc']
val_accuracy = fashion_train.history['val_acc']
loss = fashion_train.history['loss']
val_loss = fashion_train.history['val_loss']
epochs = range(len(accuracy))
plt.plot(epochs, accuracy, 'bo', label='training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='validation accuracy')
plt.title('training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('training and validation loss')
plt.legend()
plt.show()

   [output_56_0_st6ods.png]

   [output_56_1_ie0rbw.png]

   from the above two plots, you can see that the validation accuracy
   almost became stagnant after 4-5 epochs and rarely increased at certain
   epochs. in the beginning, the validation accuracy was linearly
   increasing with loss, but then it did not increase much.

   the validation loss shows that this is the sign of overfitting, similar
   to validation accuracy it linearly decreased but after 4-5 epochs, it
   started to increase. this means that the model tried to memorize the
   data and succeeded.

   with this in mind, it's time to introduce some dropout into our model
   and see if it helps in reducing overfitting.

adding dropout into the network

   you can add a dropout layer to overcome the problem of overfitting to
   some extent. dropout randomly turns off a fraction of neurons during
   the training process, reducing the dependency on the training set by
   some amount. how many fractions of neurons you want to turn off is
   decided by a hyperparameter, which can be tuned accordingly. this way,
   turning off some neurons will not allow the network to memorize the
   training data since not all the neurons will be active at the same time
   and the inactive neurons will not be able to learn anything.

   so let's create, compile and train the network again but this time with
   dropout. and run it for 20 epochs with a batch size of 64.
batch_size = 64
epochs = 20
num_classes = 10

fashion_model = sequential()
fashion_model.add(conv2d(32, kernel_size=(3, 3),activation='linear',padding='sam
e',input_shape=(28,28,1)))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d((2, 2),padding='same'))
fashion_model.add(dropout(0.25))
fashion_model.add(conv2d(64, (3, 3), activation='linear',padding='same'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d(pool_size=(2, 2),padding='same'))
fashion_model.add(dropout(0.25))
fashion_model.add(conv2d(128, (3, 3), activation='linear',padding='same'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(maxpooling2d(pool_size=(2, 2),padding='same'))
fashion_model.add(dropout(0.4))
fashion_model.add(flatten())
fashion_model.add(dense(128, activation='linear'))
fashion_model.add(leakyrelu(alpha=0.1))
fashion_model.add(dropout(0.3))
fashion_model.add(dense(num_classes, activation='softmax'))

fashion_model.summary()

_________________________________________________________________
layer (type)                 output shape              param #
=================================================================
conv2d_54 (conv2d)           (none, 28, 28, 32)        320
_________________________________________________________________
leaky_re_lu_61 (leakyrelu)   (none, 28, 28, 32)        0
_________________________________________________________________
max_pooling2d_52 (maxpooling (none, 14, 14, 32)        0
_________________________________________________________________
dropout_29 (dropout)         (none, 14, 14, 32)        0
_________________________________________________________________
conv2d_55 (conv2d)           (none, 14, 14, 64)        18496
_________________________________________________________________
leaky_re_lu_62 (leakyrelu)   (none, 14, 14, 64)        0
_________________________________________________________________
max_pooling2d_53 (maxpooling (none, 7, 7, 64)          0
_________________________________________________________________
dropout_30 (dropout)         (none, 7, 7, 64)          0
_________________________________________________________________
conv2d_56 (conv2d)           (none, 7, 7, 128)         73856
_________________________________________________________________
leaky_re_lu_63 (leakyrelu)   (none, 7, 7, 128)         0
_________________________________________________________________
max_pooling2d_54 (maxpooling (none, 4, 4, 128)         0
_________________________________________________________________
dropout_31 (dropout)         (none, 4, 4, 128)         0
_________________________________________________________________
flatten_18 (flatten)         (none, 2048)              0
_________________________________________________________________
dense_35 (dense)             (none, 128)               262272
_________________________________________________________________
leaky_re_lu_64 (leakyrelu)   (none, 128)               0
_________________________________________________________________
dropout_32 (dropout)         (none, 128)               0
_________________________________________________________________
dense_36 (dense)             (none, 10)                1290
=================================================================
total params: 356,234
trainable params: 356,234
non-trainable params: 0
_________________________________________________________________

fashion_model.compile(loss=keras.losses.categorical_crossid178, optimizer=kera
s.optimizers.adam(),metrics=['accuracy'])

fashion_train_dropout = fashion_model.fit(train_x, train_label, batch_size=batch
_size,epochs=epochs,verbose=1,validation_data=(valid_x, valid_label))

train on 48000 samples, validate on 12000 samples
epoch 1/20
48000/48000 [==============================] - 66s 1ms/step - loss: 0.5954 - acc
: 0.7789 - val_loss: 0.3788 - val_acc: 0.8586
epoch 2/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.3797 - acc
: 0.8591 - val_loss: 0.3150 - val_acc: 0.8832
epoch 3/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.3302 - acc
: 0.8787 - val_loss: 0.2836 - val_acc: 0.8961
epoch 4/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.3034 - acc
: 0.8868 - val_loss: 0.2663 - val_acc: 0.9002
epoch 5/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.2843 - acc
: 0.8936 - val_loss: 0.2481 - val_acc: 0.9083
epoch 6/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.2699 - acc
: 0.9002 - val_loss: 0.2469 - val_acc: 0.9032
epoch 7/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2561 - acc
: 0.9049 - val_loss: 0.2422 - val_acc: 0.9095
epoch 8/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2503 - acc
: 0.9068 - val_loss: 0.2429 - val_acc: 0.9098
epoch 9/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2437 - acc
: 0.9096 - val_loss: 0.2230 - val_acc: 0.9173
epoch 10/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2307 - acc
: 0.9126 - val_loss: 0.2170 - val_acc: 0.9187
epoch 11/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2307 - acc
: 0.9135 - val_loss: 0.2265 - val_acc: 0.9193
epoch 12/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2229 - acc
: 0.9160 - val_loss: 0.2136 - val_acc: 0.9229
epoch 13/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2202 - acc
: 0.9162 - val_loss: 0.2173 - val_acc: 0.9187
epoch 14/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.2161 - acc
: 0.9188 - val_loss: 0.2142 - val_acc: 0.9211
epoch 15/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2119 - acc
: 0.9196 - val_loss: 0.2133 - val_acc: 0.9233
epoch 16/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2073 - acc
: 0.9222 - val_loss: 0.2159 - val_acc: 0.9213
epoch 17/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2050 - acc
: 0.9231 - val_loss: 0.2123 - val_acc: 0.9233
epoch 18/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.2016 - acc
: 0.9238 - val_loss: 0.2191 - val_acc: 0.9235
epoch 19/20
48000/48000 [==============================] - 65s 1ms/step - loss: 0.2001 - acc
: 0.9244 - val_loss: 0.2110 - val_acc: 0.9258
epoch 20/20
48000/48000 [==============================] - 64s 1ms/step - loss: 0.1972 - acc
: 0.9255 - val_loss: 0.2092 - val_acc: 0.9269

   let's save the model so that you can directly load it and not have to
   train it again for 20 epochs. this way, you can load the model later on
   if you need it and modify the architecture; alternatively, you can
   start the training process on this saved model. it is always a good
   idea to save the model -and even the model's weights!- because it saves
   you time. note that you can also save the model after every epoch so
   that, if some issue occurs that stops the training at an epoch, you
   will not have to start the training from the beginning.
fashion_model.save("fashion_model_dropout.h5py")

model evaluation on the test set

   finally, let's also evaluate your new model and see how it performs!
test_eval = fashion_model.evaluate(test_x, test_y_one_hot, verbose=1)

10000/10000 [==============================] - 5s 461us/step

print('test loss:', test_eval[0])
print('test accuracy:', test_eval[1])

('test loss:', 0.21460009642243386)
('test accuracy:', 0.92300000000000004)

   wow! looks like adding dropout in our model worked, even though the
   test accuracy did not improve significantly but the test loss decreased
   compared to the previous results.
   now, let's plot the accuracy and loss plots between training and
   validation data for the one last time.
accuracy = fashion_train_dropout.history['acc']
val_accuracy = fashion_train_dropout.history['val_acc']
loss = fashion_train_dropout.history['loss']
val_loss = fashion_train_dropout.history['val_loss']
epochs = range(len(accuracy))
plt.plot(epochs, accuracy, 'bo', label='training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='validation accuracy')
plt.title('training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('training and validation loss')
plt.legend()
plt.show()

   [output_71_0_dq7xtu.png]

   [output_71_1_rr3sym.png]

   finally, you can see that the validation loss and validation accuracy
   both are in sync with the training loss and training accuracy. even
   though the validation loss and accuracy line are not linear, but it
   shows that your model is not overfitting: the validation loss is
   decreasing and not increasing, and there is not much gap between
   training and validation accuracy.

   therefore, you can say that your model's generalization capability
   became much better since the loss on both test set and validation set
   was only slightly more compared to the training loss.

predict labels

predicted_classes = fashion_model.predict(test_x)

   since the predictions you get are floating point values, it will not be
   feasible to compare the predicted labels with true test labels. so, you
   will round off the output which will convert the float values into an
   integer. further, you will use np.argmax() to select the index number
   which has a higher value in a row.

   for example, let's assume a prediction for one test image to be 0 1 0 0
   0 0 0 0 0 0, the output for this should be a class label 1.
predicted_classes = np.argmax(np.round(predicted_classes),axis=1)

predicted_classes.shape, test_y.shape

((10000,), (10000,))

correct = np.where(predicted_classes==test_y)[0]
print "found %d correct labels" % len(correct)
for i, correct in enumerate(correct[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(test_x[correct].reshape(28,28), cmap='gray', interpolation='none'
)
    plt.title("predicted {}, class {}".format(predicted_classes[correct], test_y
[correct]))
    plt.tight_layout()

found 9188 correct labels

   [output_78_1_djnisy.png]
incorrect = np.where(predicted_classes!=test_y)[0]
print "found %d incorrect labels" % len(incorrect)
for i, incorrect in enumerate(incorrect[:9]):
    plt.subplot(3,3,i+1)
    plt.imshow(test_x[incorrect].reshape(28,28), cmap='gray', interpolation='non
e')
    plt.title("predicted {}, class {}".format(predicted_classes[incorrect], test
_y[incorrect]))
    plt.tight_layout()

found 812 incorrect labels

   [output_79_1_mu5esp.png]

   by looking at a few images, you cannot be sure as to why your model is
   not able to classify the above images correctly, but it seems like a
   variety of the similar patterns present on multiple classes affect the
   performance of the classifier although id98 is a robust architecture.
   for example, images 5 and 6 both belong to different classes but look
   kind of similar maybe a jacket or perhaps a long sleeve shirt.

classification report

   classification report will help us in identifying the misclassified
   classes in more detail. you will be able to observe for which class the
   model performed bad out of the given ten classes.
from sklearn.metrics import classification_report
target_names = ["class {}".format(i) for i in range(num_classes)]
print(classification_report(test_y, predicted_classes, target_names=target_names
))

             precision    recall  f1-score   support

    class 0       0.77      0.90      0.83      1000
    class 1       0.99      0.98      0.99      1000
    class 2       0.88      0.88      0.88      1000
    class 3       0.94      0.92      0.93      1000
    class 4       0.88      0.87      0.88      1000
    class 5       0.99      0.98      0.98      1000
    class 6       0.82      0.72      0.77      1000
    class 7       0.94      0.99      0.97      1000
    class 8       0.99      0.98      0.99      1000
    class 9       0.98      0.96      0.97      1000

avg / total       0.92      0.92      0.92     10000

   you can see that the classifier is underperforming for class 6
   regarding both precision and recall. for class 0 and class 2, the
   classifier is lacking precision. also, for class 4, the classifier is
   slightly lacking both precision and recall.

go further!

   this tutorial was good start to convolutional neural networks in python
   with keras. if you were able to follow along easily or even with little
   more efforts, well done! try doing some experiments maybe with same
   model architecture but using different types of public datasets
   available.

   there is still a lot to cover, so why not take datacamp's [25]deep
   learning in python course? in the meantime, also make sure to check out
   the [26]keras documentation, if you haven't done so already. you will
   find more examples and information on all functions, arguments, more
   layers, etc. it will undoubtedly be an indispensable resource when
   you're learning how to work with neural networks in python!

   if you rather feel like reading a book that explains the fundamentals
   of deep learning (with keras) together with how it's used in practice,
   you should definitely read fran  ois chollet's [27]deep learning in
   python book.
   87
   87
   [28]0
   related posts
   must read
   python
   +1

[29]python machine learning: scikit-learn tutorial

   karlijn willems
   february 25th, 2019
   must read
   python
   +4

[30]keras tutorial: deep learning in python

   karlijn willems
   february 4th, 2019
   must read
   r programming
   +4

[31]keras: deep learning in r

   karlijn willems
   february 12th, 2019
   (button)
   post a comment

   [32]subscribe to rss
   [33]about[34]terms[35]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#comments
   3. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#id98
   4. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#understand_data
   5. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#explore
   6. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#preprocess
   7. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#network
   8. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#dropout
   9. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#dropout_evaluate
  10. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#predictions
  11. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#classification
  12. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  13. https://www.datacamp.com/community/blog/keras-cheat-sheet
  14. https://www.youtube.com/watch?v=cw5pkv9rj3o
  15. https://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  16. https://commons.wikimedia.org/wiki/file:typical_id98.png
  17. https://en.wikipedia.org/wiki/convolutional_neural_network
  18. http://cs231n.github.io/convolutional-networks/
  19. https://arxiv.org/abs/1708.07747
  20. https://www.datacamp.com/community/tutorials/deep-learning-python
  21. https://github.com/zalandoresearch/fashion-mnist
  22. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
  23. http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/
  24. https://github.com/zalandoresearch/fashion-mnist
  25. https://www.datacamp.com/courses/deep-learning-in-python
  26. https://keras.io/
  27. https://www.manning.com/books/deep-learning-with-python
  28. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python#comments
  29. https://www.datacamp.com/community/tutorials/machine-learning-python
  30. https://www.datacamp.com/community/tutorials/deep-learning-python
  31. https://www.datacamp.com/community/tutorials/keras-r-deep-learning
  32. https://www.datacamp.com/community/rss.xml
  33. https://www.datacamp.com/about
  34. https://www.datacamp.com/terms-of-use
  35. https://www.datacamp.com/privacy-policy

   hidden links:
  37. https://www.datacamp.com/
  38. https://www.datacamp.com/community
  39. https://www.datacamp.com/community/tutorials
  40. https://www.datacamp.com/community/data-science-cheatsheets
  41. https://www.datacamp.com/community/open-courses
  42. https://www.datacamp.com/community/podcast
  43. https://www.datacamp.com/community/chat
  44. https://www.datacamp.com/community/blog
  45. https://www.datacamp.com/community/tech
  46. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  47. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  48. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  49. https://www.datacamp.com/profile/adityasharma101993
  50. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  51. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  52. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  53. https://www.datacamp.com/profile/karlijn
  54. https://www.datacamp.com/profile/karlijn
  55. https://www.datacamp.com/profile/karlijn
  56. https://www.facebook.com/pages/datacamp/726282547396228
  57. https://twitter.com/datacamp
  58. https://www.linkedin.com/company/datamind-org
  59. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
