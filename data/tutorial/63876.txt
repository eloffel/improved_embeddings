   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id3- history and overview

   [16]go to the profile of kiran sudhir
   [17]kiran sudhir (button) blockedunblock (button) followfollowing
   jun 20, 2017

   of late, generative modeling has seen a rise in popularity. in
   particular, a relatively recent model called generative adversarial
   networks or gans introduced by ian goodfellow et al. shows promise in
   producing realistic samples. this blog post has been divided into two
   parts. part-1 consists of an introduction to gans, the history behind
   it, and its various applications. part-2 consists of an implementation
   of gans (with code) to produce image samples.

part 1- understanding gans

generative modeling

   let us first take a look at what generative models are, and how the
   differ from discriminative models. say you have input data x, and
   corresponding output labels y. a discriminative model tries to directly
   learn the id155 distribution p(y|x). on the other
   hand, a generative model tries to learn the joint id203
   distribution p(x,y). this can be transformed into p(y|x) using bayes
   rule. however, additionally, as opposed to discriminative models,
   generative models can use the joint distribution p(x,y) to generate
   likely (x,y) samples.

what   s the hype all about?

   so why would one want to study generative models? one might wonder what
   the big deal is in simply generating more data, especially since
   there   s such an abundance of data already available. but in reality,
   this could be put to several uses. for instance, one can feed some text
   written in a particular handwriting as input to a generative model to
   get generate more text in the same handwriting. generative models, and
   gans in particular, can also be used in exploration in reinforcement
   learning where they can be used to generate artificial environments.
   other applications include conversion of sketches to images, image
   denoising, conversion of low resolution images to high resolution,
   generation of art, and conversion of satellite images to maps, to name
   a few. apart from the wide range of applications, generative models are
   particularly useful when a majority of the labels are missing, because
   of their ability to perform semi-supervised learning.

generative models over the years

   now that we have laid the ground on what generative modeling is, and
   why they are useful, let   s take a look at the various approaches in
   generative modeling. for the purpose of comparing the models, they can
   all be described to perform maximum likelihood.
   [1*otb47mg5kqwnh91prmyxzg.png]
   taxonomy of generative models (from ian goodfellow   s nips tutorial,
   2016)

   the above graph depicts the various families of generative models as
   described by ian goodfellow in his nips tutorial. let   s now take a look
   at the pros and cons of some of the popular approaches in the families
   of models stated above.

fully visible belief networks

   they use the chain rule of id203 to decompose the id203
   distribution over a vector, into a product over each of the members of
   the vector.
   [1*eeowoqsaoams58a4gmfjkq.png]

   the most popular model in this family is pixelid98.
   [1*vionjvci6vppliuvtlwyqg.jpeg]
   images generated by pixelid98 (van den ord et al 2016)

   the biggest drawback with fvbns is that the rate of generating samples
   is very slow. every time you want to generate a new sample, you will
   have to run the model again. this cannot be done in parallel.

models based on change of variables (non-linear ica)

   such models begin with a simple distribution like a gaussian and use a
   non-linear function to transform the distribution to another space. the
   main disadvantage with this is that the transformation needs to be
   designed to be invertible, and the latent variables must have the same
   dimensionality as the data. so if you want to generate 5000 pixels, you
   need to have 5000 latent variables.

variational autoencoder

   the way id5 work is by marginalizing out the
   random variable z from the density function log p(x). since this is
   intractable, it uses a variational approximation. the model wishes to
   maximize the lower bound on the log likelihood of the data.
   [1*eg_zduolovps5yuotfcc4q.jpeg]
   images of celebrity-like faces generated by a vae (by alec radford)

   the main drawback here is that the model is asymptotically consistent
   if the distribution q is perfect. otherwise, there would be a gap
   between the lower bound and actual density of the data. another
   disadvantage is that the samples generated are of relatively low
   quality.

id82s

   a id82 can be defined by an energy function, and the
   id203 of a particular state, is proportional to each of the value
   of the energy. to convert this to an actual id203 distribution,
   reid172 is done by dividing the sum over the different states.
   this sum is intractable, which calls for approximation using monte
   carlo methods. the drawback is that these methods, especially the
   id115 methods, don   t perform well in high
   dimensional spaces. so although they may perform well on images like
   [18]mnist, you won   t get a similar performance on images from
   [19]id163.

id3

   now that we have touched upon other popular generative models, we can
   take a look at gans, and how they compare against the rest.

   gans were designed to overcome many of the drawbacks stated in the
   above models. as opposed to fully visible belief networks, gans use a
   latent code, and can generate samples in parallel. unlike variational
   autoencoders, gans are asymptotically consistent. also, gans do not
   require markov chains, which is an advantage over id82s.
   finally, gans are often highly regarded as producing the best samples,
   although this is very subjective, and currently a topic of debate, with
   models like pixelid98 competing with them.
   [1*_d22i5gab8uwl68enjxqwa.png]
   images generated by gan using the toronto face database (ian goodfellow
   et al 2014)

how gans work

   now that we   ve established why gans are worth studying about, let   s
   delve deeper into how they really work.

   the main idea behind gans can be interpreted as a game between two
   players- the generator and the discriminator. the generator tries to
   generate samples that follow the same underlying distribution as the
   train data. the discriminator tries to distinguish between the samples
   generated by the generator (fake data), and the actual data from the
   train set. the goal of the generator is to fool the discriminator by
   closely approximating the underlying distribution in order to generate
   samples that are indistinguishable from the actual data. on the other
   hand, the goal of the discriminator is to identify the fake data from
   the real data. the discriminator simply has the task of a binary
   classification problem, where it determines if the data is real or
   fake. a common analogy of this game is that of a forger and the police.
   the forger is like the generator, trying to counterfeit money, and
   making it look as legitimate as possible, in order to fool the police.
   the police is like the discriminator whose goal is to be able to
   identify money that has been counterfeited.
   [1*kf-xzsw2f44scxlgddy_9w.png]
   gan overview

   the generator is a differentiable function g, that has parameters that
   can be learned by id119. the input to g is obtained by
   sampling the latent vector z from some prior distribution over latent
   variables. so essentially, z is a vector of unstructured noise. g is
   applied to z to obtain a sample x from the model, which should ideally
   be similar to the actual data from the train set. like the generator,
   the discriminator is also a differentiable function d that has
   parameters that can be learned by id119. the function d,
   when applied to the sample x obtained from g(z) should ideally output a
   value close to zero, indicating that the sample is fake. when an actual
   sample from the data is fed as input to d, it should output a value
   close to one.

   say     (d) and     (g) are the parameters of d and g respectively. the
   discriminator would want to minimize it   s cost j(d)(    (d),    (g)), but has
   no control over     (g), whereas the generator would want to minimize
   j(g)(    (d),    (g)) without control over     (d). so we would want to find the
   nash equilibrium values for (    (d),    (g)) such that j(d) is minimum with
   respect to     (d) and j(g) is minimum with respect to     (g).

   so how is training actually performed? the training procedure is to
   choose an optimization algorithm like adam, for instance, and apply it
   simultaneously to two minibatches of data, one from the actual training
   data, and the other from the samples generated by g. additionally, you
   could update one player more often than the other player.

generator and discriminator costs

   the way the algorithm will proceed depends on the cost of each player.
   the simplest way to specify the cost is to use the minimax game where
   the generator cost is the negative of the discriminator cost. so what
   exactly is this cost that the discriminator would want to maximize, and
   the generator minimize? it is simply a standard cross-id178 function
   between the discriminator   s output and the actual labels (real/fake).
   [1*fqbu2d0jvv5es9lx98if7a.png]

   the first term in j(d) represents feeding the actual data to the
   discriminator, and the discriminator would want to maximize the log
   id203 of predicting one, indicating that the data is real. the
   second term represents the samples generated by g. here, the
   discriminator would want to maximize the log id203 of predicting
   zero, indicating the the data is fake. the generator, on the other hand
   tries to minimize the log id203 of the discriminator being
   correct. the solution to this problem is an equilibrium point of the
   game, which is a saddle point of the discriminator loss.

   the main problem with this minimax game is that when the discriminator
   becomes increasingly smart, the gradient for the generator vanishes.
   one way to fix this is by flipping the order of the arguments in the
   cross id178 function instead of simply flipping the sign of the
   discriminator   s cost.
   [1*bmabwfgxu70cite909pmja.png]

   the generator would now want to maximize the log id203 of the
   discriminator being wrong. now, the equilibrium cannot be described by
   a single value function, and the motivations for this particular cost
   are far more heuristic.

scaling gans

   the initial gans themselves don   t really scale well to large
   applications. to overcome this, the deep convolution gan architecture
   was introduced by radfort at al. although originally, gans were already
   deep and convolutional, dcgans stress on having more convolutional
   layers, and additionally use techniques such as batch id172.
   batch id172 is applied to every layer except the last layer of
   the generator in order to make the learning process more stable.
   [1*wm4iddmoigftexnxjfkubw.png]
   dcgan architecture (radford et al 2015)

some more applications

   it   s no myth that gans have taken the ai world by a storm, and they are
   indeed here to stay. before concluding this section, let   s take a look
   at some of the fascinating applications of gans today.

   one popular application is in the generation of a high resolution image
   from a low resolution image using super resolution techniques.
   [1*rksvwxswcj2aqu6gamskta.png]
   srgan used to obtain high resolution images. the original hr image is
   first downsampled to make a low-resolution image, and different methods
   are used to recover the original hr image (christian ledig et all 2016)

   another recent advancement was made by scott reed et al in 2016, where
   gans were used to synthesize realistic images from text.
   [1*h9ziqvloze2nlmoqh-ldug.png]
   generative adversarial text to image synthesis (scott reed et al 2016)

   as described earlier, gans also have many applications in reinforcement
   learning. they have also been applied in image denoising, and
   generation of art, to name a few.

part 2    implementing gans

   now that you   ve been introduced to what gans are and how exactly they
   work, you   ve officially reached the exciting part of the blog. it   s
   time to get started and write some code!

   since the most prominent application of gans is in the generation of
   images, it makes sense to get yourself familiar with coding an image
   generator in gans. the code should take a set of images as input and
   generate a set of similar images as output. most people often start out
   with images like mnist and try to generate more hand-drawn digits using
   gans. here, we will be using images from id163 as our input. since
   these images are pretty high dimensional, we will be using dcgans for
   image generation.

   for the input, the images from the    plant    dataset from id163 is
   used. each image resized to obtain 128x128 images.
   [1*a-g7_k_buo2wg90paimjlw.png]
   plant data from id163

   the following dcgan implementation is in tensorflow. the crux of this
   implementation is contained in a python class called dcgan, where we
   will be defining methods for defining the model and training it.

   the generator is defined below. it consists of a linear layer followed
   by four convolutional layers. the methods linear and conv2dtranspose
   are helper functions used to obtain the layers. batch id172 is
   applied to every layer except the last one.
   [1*e9axcji bjrfwimgrfbg.png]

   next, we define the discriminator. we define it to have four
   convolutional layers, followed by a linear layer. we use helper
   function lrelu to define a custom activation function (leaky relu).
   [1*lnipgrzjiirookkikzysjg.png]

   now we can create our generator and discriminator models. we define two
   discriminators which share the same parameters. one is fed a minibatch
   of actual images from the train data, whereas the other is fed a
   minibatch of the images generated by the generator.
   [1*rb7mdryw6nauhhk4pnfagg.png]

   next, we have to define the id168s for the generator and
   discriminator. as described in part-1 of the blog, we use the
   cross-id178 function between the discriminator   s output and the
   actual labels (real/fake). here, the label    real   , corresponds to 1.
   and the label    fake    corresponds to 0. so the discriminator should aim
   to output values close to 1 for real images, and output values close to
   0 for images produced by the generator. the generator loss is computed
   using the heuristic specified in part-1. it is calculated based on the
   id203 of the discriminator being wrong, i.e. the generator would
   want the discriminator to output values close to 1 for the images it
   generates.
   [1*k6uqvqtu1mzdp3k7toq0ta.png]

   before we begin training, we need to first define an optimization
   function to minimize the above losses. here we make use of the adam
   optimizer, with learning rate = 0.0002, for both the generator and the
   discriminator. here, d_theta represents all the parameters of the
   discriminator, and g_theta represents all the parameters of the
   generator.
   [1*qup9i-aegqxfpkxjym5t0w.png]

   the final step is to actually begin training the data and generating
   images. for each epoch, pairs of two minibatches are generated, one
   from the train images, and the other from sampling z. also, in order to
   prevent the discriminator loss from going to zero, the generator is
   updated twice for each update of the discriminator.
   [1*_5f2ix0_0k9pearhkg-v7w.png]

results

   the following is the output that was obtained on running the dcgan with
   the plant data from id163 as input (resized to 128x128), with number
   of epochs set to 250.
   [1*5kiifpkg_lrhcg7bz8g44q.png]
   output images generated by dcgan

   each of the four losses are plotted below.
   [1*vfn3vnmmgjrx5m6i4bwyww.png]
   [1*eltnivylwxmxq87rukjasa.png]
   [1*qdyvcnauhwh6srw_dljbwq.png]
   [1*4mnlxbyozzsad-ybksuw0q.png]

conclusions and future work

   for most epochs, the generator loss turned out to be lower than the
   discriminator loss. this could be attributed to the fact that the
   generator was updated twice for every update of the discriminator.

   although not perfect, many of the generated images do resemble
   plants/flowers, rather than just a distribution of some random pixels.
   hence, it can be concluded that the dcgan is able to intelligently
   construct images of its own. however, there is plenty of scope for
   improvement. one of the major restrictions in this implementation is
   the lack of computational resources. image generation itself is a
   computationally intensive task. in this particular example, the images
   are of relatively higher resolution as compared to other more primitive
   image datasets like mnist. additionally, the images are colored. hence,
   using a gpu will make a huge difference in run-time, enabling the
   possibility of running more epochs.

   however, simply increasing the epochs might not guarantee the
   generation of better images, and if the learning rate is too low, the
   discriminator will start winning, and the quality of the images will
   start deteriorating. one obvious way to improve the model is to add
   more layers to the generator. i started out with just one layer, and
   the results improved significantly on adding more layers. one can also
   experiment with different types of id180 as well as
   optimization functions. i have used the adamoptimizer here because it
   has been recommended by ian goodfellow himself.

   although the above example of generating plant images might not be
   particularly useful, the same underlying model could be used to
   generate other images like that of kittens and puppies, for instance.
   this also lays the ground for the development of more complicated
   applications using gans such as the generation of art.

references:

   iframe: [20]/media/5caf26d3cd9533ba2a862974431f33c5?postid=7effbb713545

   [21]https://github.com/carpedm20/dcgan-tensorflow

     * [22]machine learning
     * [23]deep learning
     * [24]ai
     * [25]towards data science
     * [26]generative model

   (button)
   (button)
   (button) 94 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of kiran sudhir

[28]kiran sudhir

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 94
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7effbb713545
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/generative-adversarial-networks-history-and-overview-7effbb713545&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/generative-adversarial-networks-history-and-overview-7effbb713545&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vfpeqlnxdj0r---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@kiransudhir95?source=post_header_lockup
  17. https://towardsdatascience.com/@kiransudhir95
  18. http://yann.lecun.com/exdb/mnist/
  19. http://image-net.org/
  20. https://towardsdatascience.com/media/5caf26d3cd9533ba2a862974431f33c5?postid=7effbb713545
  21. https://github.com/carpedm20/dcgan-tensorflow
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/deep-learning?source=post
  24. https://towardsdatascience.com/tagged/ai?source=post
  25. https://towardsdatascience.com/tagged/towards-data-science?source=post
  26. https://towardsdatascience.com/tagged/generative-model?source=post
  27. https://towardsdatascience.com/@kiransudhir95?source=footer_card
  28. https://towardsdatascience.com/@kiransudhir95
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/7effbb713545/share/twitter
  35. https://medium.com/p/7effbb713545/share/facebook
  36. https://medium.com/p/7effbb713545/share/twitter
  37. https://medium.com/p/7effbb713545/share/facebook
