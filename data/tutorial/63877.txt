   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]twentybn
     * [9]frontier
     * [10]embodied ai
     * [11]tutorials
     * [12]videos
     * [13]news
     __________________________________________________________________

learning about the world through video

at [14]twentybn, we build ai systems that enable a human-like visual
understanding of the world. today, we are releasing two large-scale video
datasets (256,591 labeled videos) to teach machines visual common sense. the
first dataset allows machines to develop a fine-grained understanding of
basic actions that occur in the physical world. the second dataset of dynamic
hand gestures enables robust cognition models for human-computer interaction.
for more, visit our [15]dataset page, [16]research paper or [17]contact us.

   [18]go to the profile of moritz mueller-freitag
   [19]moritz mueller-freitag (button) blockedunblock (button)
   followfollowing
   jun 21, 2017
   [1*hzgadqz08nqwrl2bqzzkoq.png]
   example videos from our datasets

video is becoming ubiquitous

   video plays an increasingly important role in our lives. as consumers,
   we collectively spend hundreds of millions of hours every day watching
   and sharing videos on services like youtube, facebook or snapchat. when
   we are not busy gobbling up video on social media, we produce more of
   it with our smartphones, gopro cameras and (soon) ar goggles. as a
   growing fraction of the planet   s population is documenting their lives
   in video format, we are transitioning from starring in our own magazine
   (the still image era) to starring in our own reality tv show.
   [1*z44hxb5u0qyi489ltqzmpq.png]

   all that is arguably just the beginning. the next few years will see a
   proliferation of connected devices, ranging from smart, always-on home
   cameras to autonomous vehicles. many of these devices will rely on a
   camera as the primary sensory input for understanding and navigating
   the world. as the technological evolution marches on, video
   intelligence will be crucial. it is quite clear that there are not
   enough eyeballs in the world to process all video data and
   human-powered visual understanding unfortunately does not scale. what
   we need is a software layer that can analyze and extract meaning from
   video. that requires learning algorithms that understand the physical
   world and the myriad of actions that are carried out by the actors
   within it.

video is the next frontier in id161

   deep learning has made historic progress in recent years by producing
   systems that rival         and in some cases exceed         human performance in
   tasks such as recognizing objects in still images. despite this
   progress, enabling computers to understand both the spatial and
   temporal aspects of video remains an unsolved problem. the reason is
   sheer complexity. while a photo is just one static image, a video shows
   narrative in motion. video is time-consuming to annotate manually, and
   it is computationally expensive to store and process.

   the main obstacle that prevents neural networks from reasoning more
   fundamentally about complex scenes and situations is their lack of
   [20]common sense knowledge about the physical world. video data
   contains a wealth of fine-grained information about the world as it
   shows how objects behave by virtue of their properties. for example,
   videos implicitly encode physical information like three-dimensional
   geometry, [21]material properties, [22]object permanence,
   [23]affordance or gravity. while we humans intuitively grasp these
   concepts, a detailed understanding of the physical world is still
   largely missing from current applications in artificial intelligence
   (ai) and robotics.
   [1*a_xo5qtkv-vrybrsejidca.png]
   existing id161 systems produce (at best) descriptions of the
   world that are not robust. here are a couple of examples produced by a
   model that generates natural language descriptions of images (source:
   [24]karpathy & fei-fei)

   at [25]twentybn, we believe that intelligent software for handling
   video is a prerequisite for enabling the most promising applications of
   ai in the real world. one field of applications that we are excited
   about is health care, and in particular elderly care. when it comes to
   eldercare, changes in the [26]activities of basic living (adls) often
   precede physiological changes and can predict poor clinical outcomes.
   imagine how much we could improve the care for our aged loved ones if
   it were possible to install a handful of smart camera devices in fixed
   locations to monitor changes in activities of seniors, aid their
   memory, and ultimately improve their health?

   to enable applications like these, we need a technology step change. we
   need systems that can understand the context and actions occurring in
   visual scenes. state-of-the-art image recognition just won   t cut it.
   this is because life is more than a sequence of snapshots, and
   perceiving the world is more than recognizing cats and dogs in images.
   it   s about what is actually happening in the physical world as time
   unfolds. it   s about verbs, not just nouns.

a novel approach to video understanding

   one of the most important rate limiting factors for advancing video
   understanding is the lack of large and diverse real-world video
   datasets. many video datasets that have been published to date suffer
   from a number of shortcomings: they are often weakly labeled, lack
   variety, or underwent a high degree of editing and post-processing. a
   few notable exceptions, like deepmind   s recently released [27]kinetics
   dataset, try to alleviate this by focusing on shorter clips, but since
   they show high-level human activities taken from youtube videos, they
   fall short of representing the simplest [28]physical object
   interactions that will be needed for modeling visual common sense.

   at twentybn, we have spent the past year building a foundational data
   layer for the understanding of physical actions. our approach is based
   on a single, rather straightforward idea: why not leverage the
   amazingly precise and cultivated motor skills that human beings possess
   to generate fine-grained, complex and varied data at scale? after all,
   the vast majority of motion patterns that we observe day-to-day are
   actually caused by other humans.

   to generate the complex, labelled videos that neural networks need to
   learn, we use what we call    crowd acting   . we instruct crowd workers to
   record short video clips based on carefully predefined and highly
   specific descriptions. for example,    pushing something until it falls
   of the table   ,    moving object a closer to object b   , or    sliding two
   fingers of your left hand up   . while we collect data for many different
   kinds of human actions, we naturally stress dexterous manipulation of
   objects using one or both hands. this is because our hands are best at
   generating the highly controlled, complex motion patterns needed for
   training networks. instead of painstakingly labeling existing video
   data, crowd acting allows us to generate large amounts of densely
   labeled, meaningful video segments at low cost.

   today, we are excited to announce the release of two substantial
   snapshots from our data collection campaign: a database of human-object
   interactions ([29]something-something) and the world   s largest video
   dataset for classifying dynamic hand gestures ([30]jester). the two
   datasets are    snapshots    because data collection is an ongoing process.
   in total, we are releasing 256,591 labeled video clips for supervised
   training of deep learning models. both datasets are made available
   under a creative commons attribution 4.0 international license (cc
   by-nc-nd 4.0) and are freely available for academic use. if can want to
   license our datasets for commercial use, please [31]contact us.

1. [32]   something-something    dataset

   this snapshot contains 108,499 annotated video clips, each between 2
   and 6 seconds in duration. the videos show objects and the actions
   performed on them across 175 classes. the captions are textual
   descriptions based on templates, such as    dropping something into
   something   . the templates contain slots of    something    that serve as
   placeholders for objects. this provides added structure between the
   text-to-video encoding for the network to improve learning.
   [1*cvkf6bof61a94gx6-i-lcq.png]

   the goal of this dataset is not only to detect or track objects in
   videos but to decipher the behavior of human actors as well as the
   direct and indirect manipulations of the objects they interact with.
   predicting the textual labels from the videos therefore requires strong
   visual features that are capable of representing a wealth of physical
   properties of the objects and the world. this includes information
   about properties like spatial relations and material properties.

2. [33]   jester    dataset

   this snapshot contains 148,092 annotated video clips, each about 3
   seconds long. the videos cover 25 classes of human hand gestures as
   well as two    no gesture    classes to help the network distinguish
   between specific gestures and unknown hand movements. the videos show
   human actors performing generic hand gestures in front of a webcam,
   such as    swiping left/right,       sliding two fingers up/down,    or
      rolling hand forward/backward.    predicting these textual labels from
   the videos requires a network that is capable of grasping such concepts
   as the degrees of freedom in three-dimensional space (surging, swaying,
   heaving, etc).
   [1*v3wo3cwsbgmsfdxzyaewew.png]

   traditional gesture recognition systems require special hardware like
   [34]stereo cameras or depth sensors such as [35]time-of-flight cameras.
   using our jester dataset, we were able to train a neural network that
   can detect and classify all 25 gestures from raw rgb input with a test
   accuracy of 82%. the system runs in real-time on a variety of embedded
   platforms using video input from a webcam.

key characteristics of both datasets

     * supervised learning: in contrast to other methods that seek to
       acquire common sense through the use of [36]predictive unsupervised
       learning, we phrase the task as a supervised learning problem. this
       makes the representation learning task more tractable and defined.
     * dense captioning: the labels describe video content that is
       restricted to a short time interval. this ensures there is a tight
       synchronization between the video content and the corresponding
       caption.
     * crowd-acted videos: in contrast to other academic datasets that
       source and annotate video clips from youtube, we created our
       datasets using crowd acting. our proprietary crowd acting platform
       allows us to ask crowd workers to provide videos given caption
       templates instead of the other way around. this facilitates the
       generation of labeled recordings rather than just the labeling of
       existing videos.
     * human focused: with the exception of motion    textures    like ocean
       waves or leaves in the wind, most complex motion patterns that we
       ever see are caused by humans. our datasets are human-centered to
       have the complex spatio-temporal patterns that encode features of
       articulation, degrees of freedom, etc.
     * natural video scenes: our videos were captured with many different
       devices and varying zoom factors. the datasets feature scenes with
       natural lighting, partial occlusions, motion blur and background
       noise. this assures that the datasets can transfer to real-world
       use cases with minimal domain shift.

   the videos clips are challenging because they capture the messiness of
   the real world. to give you a flavor, take a look at this video clip
   from the jester dataset, showing a person performing a hand gesture:
   [1*zkkeltonrwme6zmxedwnoq.png]

   while the hand gesture is visible to the human eye, it is difficult to
   recognize for a computer because the video footage contains sub-optimal
   lighting conditions and background noise (cat walking through the
   scene). training on jester forces the neural network to learn the
   relevant visual cues, or    [37]hierarchical features   , to separate the
   signal (hand motion) from the noise (background motion). basic
   [38]motion detection will not be sufficient.

the practical use of visual common sense

   how do we go from an understanding of physical concepts to offering
   practical, real-world solutions? we believe the answer is to be found
   in a technical concept called id21.

   we humans are good at thinking by analogy. taking an idea from one
   domain and applying it to another is    the fuel and fire of our
   thinking,    as [39]douglas hofstadter puts it. in ai, a step towards
   reasoning by analogy is id21. with [40]id21,
   we can take a neural network trained on something-something and jester,
   and transfer its capabilities to contribute to specific business
   applications. specifically, networks that have internalized
   representations of how objects interact in the real world can transfer
   this intrinsic knowledge to solve problems of higher order complexity
   that are predicated on these fundamental concepts.

   id21 has already achieved astounding results on a wide
   array of image-based vision tasks (see [41]here, [42]here and
   [43]here). we believe that similar breakthroughs are imminent for deep
   video understanding. the prerequisite for leveraging id21
   in the video domain is, however, the availability of high-quality,
   labeled video data that allows neural networks to model visual common
   sense. this is the mission we signed up for at twentybn. data
   collection at our company spans a spectrum from hard to recognize, but
   solvable (with existing proof points, like hand gesture recognition),
   to very hard and still not solvable. the ultimate endpoint of this
   spectrum is general ai.

how to get the data and where to benchmark your results

   the two datasets are available for download on [44]our website. you can
   find more information about the datasets and the technical specifics of
   our research in this [45]technical report. if you want to benchmark the
   accuracy that your very own model achieves on the datasets, you will be
   able to upload your results to our website to be ranked in a
   leaderboard. if can want to license our datasets for commercial use,
   please [46]reach out to us.

   our goal in releasing these two datasets is to catalyze the development
   of machines that can perceive the world like humans. our work builds on
   the foundations of researchers past and present. we are committed to
   give back for the benefit of this vibrant community.

     * [47]machine learning
     * [48]artificial intelligence
     * [49]deep learning
     * [50]id161
     * [51]technology

   (button)
   (button)
   (button) 216 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [52]go to the profile of moritz mueller-freitag

[53]moritz mueller-freitag

   [54]@twentybn

     (button) follow
   [55]twentybn

[56]twentybn

   we teach machines to perceive the world like humans.

     * (button)
       (button) 216
     * (button)
     *
     *

   [57]twentybn
   never miss a story from twentybn, when you sign up for medium.
   [58]learn more
   never miss a story from twentybn
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4db73785ac02
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/twentybn/learning-about-the-world-through-video-4db73785ac02&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/twentybn/learning-about-the-world-through-video-4db73785ac02&source=--------------------------nav_reg&operation=register
   8. https://medium.com/twentybn?source=logo-lo_lklxdo4dodhb---6b97bd931b7e
   9. https://medium.com/twentybn/tagged/computer-vision
  10. https://medium.com/twentybn/tagged/ai-avatars
  11. https://medium.com/twentybn/tagged/software-development
  12. https://medium.com/twentybn/tagged/videos
  13. https://medium.com/twentybn/tagged/news
  14. https://www.twentybn.com/
  15. https://www.twentybn.com/datasets
  16. https://arxiv.org/abs/1706.04261
  17. mailto:moritz.mueller-freitag@twentybn.com
  18. https://medium.com/@muellerfreitag?source=post_header_lockup
  19. https://medium.com/@muellerfreitag
  20. https://en.wikipedia.org/wiki/common_sense
  21. https://en.wikipedia.org/wiki/physical_property
  22. https://en.wikipedia.org/wiki/object_permanence
  23. https://en.wikipedia.org/wiki/affordance
  24. http://cs.stanford.edu/people/karpathy/deepimagesent/
  25. https://www.twentybn.com/
  26. https://en.wikipedia.org/wiki/activities_of_daily_living
  27. https://deepmind.com/research/open-source/open-source-datasets/kinetics/
  28. http://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/deepmind-shows-ai-has-trouble-seeing-homer-simpson-actions
  29. https://www.twentybn.com/datasets/something-something
  30. https://www.twentybn.com/datasets/jester
  31. mailto:moritz.mueller-freitag@twentybn.com
  32. https://www.twentybn.com/datasets/something-something
  33. https://www.twentybn.com/datasets/jester
  34. https://en.wikipedia.org/wiki/stereo_cameras
  35. https://en.wikipedia.org/wiki/time-of-flight_camera
  36. https://drive.google.com/file/d/0bxkbnd5y2m8nrezod0tvdw5fltq/view
  37. https://en.wikipedia.org/wiki/feature_(machine_learning)
  38. https://en.wikipedia.org/wiki/motion_detection
  39. https://www.amazon.com/surfaces-essences-analogy-fuel-thinking/dp/0465018475
  40. http://cs231n.github.io/transfer-learning/
  41. https://arxiv.org/abs/1403.6382
  42. https://arxiv.org/abs/1411.1792
  43. http://cs.stanford.edu/people/karpathy/deepimagesent/
  44. https://www.twentybn.com/datasets
  45. https://arxiv.org/abs/1706.04261
  46. mailto:moritz.mueller-freitag@twentybn.com
  47. https://medium.com/tag/machine-learning?source=post
  48. https://medium.com/tag/artificial-intelligence?source=post
  49. https://medium.com/tag/deep-learning?source=post
  50. https://medium.com/tag/computer-vision?source=post
  51. https://medium.com/tag/technology?source=post
  52. https://medium.com/@muellerfreitag?source=footer_card
  53. https://medium.com/@muellerfreitag
  54. http://twitter.com/twentybn
  55. https://medium.com/twentybn?source=footer_card
  56. https://medium.com/twentybn?source=footer_card
  57. https://medium.com/twentybn
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/p/4db73785ac02/share/twitter
  61. https://medium.com/p/4db73785ac02/share/facebook
  62. https://medium.com/p/4db73785ac02/share/twitter
  63. https://medium.com/p/4db73785ac02/share/facebook
