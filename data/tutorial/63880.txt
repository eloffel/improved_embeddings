   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*bwbrpwwrrqrpxnvap4r13q.jpeg]

kegra: deep learning on id13s with keras

   go to the profile of daniel shapiro, phd
   [16]daniel shapiro, phd (button) blockedunblock (button)
   followfollowing
   dec 4, 2017

   hi there. i mentioned in [17]past articles that i am working intensely
   on cognitive computing for enterprise datasets. this is that.

   this article will require some understanding of deep learning, but you
   should be able to follow along with just a minimal understanding of
   data science.

   i have been working on detecting patterns in graphs with deep learning
   on gpus. thomas kipf [18]wrote a nice library on classifying graph
   nodes with keras. this article is based on his work
      [19]semi-supervised classification with id197   .
   let   s have a look.

   firstly, what is a graph?

   well, i am concerned with id13s in my work. these graphs
   represent entities like    white house    and    donald trump    as nodes, and
   relationships like    works at    are edges. how we build these graphs is a
   story for another time. in this article i   m looking at transaction data
   to train a classifier to recognize fraudulent transactions. if you
   prefer vertex and arc instead of nodes and edges, then [20]read this
   article.

   i feel at home in the weird world of graphs. my work on graphs goes
   back to my master   s thesis. in that work, i was interested in finding
   common elements (convex subgraphs) within a directed acyclic graph. i
   was identifying what custom instructions to add to a processor based on
   the software it runs. i used integer id135 to solve the
   problem. on large graphs the solver could take hours or even days.

   links to that line of research:
     * [21]a case study on hardware/software codesign in embedded
       id158s
     * [22]tunable instruction set extension identification
     * [23]asips for id158s
     * [24]id158 acceleration on fpga using custom
       instruction
     * [25]improved ise identification under hardware constraint
     * [26]parallel instruction set extension identification
     * [27]static task scheduling for configurable multiprocessors
     * [28]design and implementation of instruction set extension
       identification for a multiprocessor system-on-chip
       hardware/software co-design toolchain
     * [29]sing: a multiprocessor system-on-chip design and system
       generation tool

   here is an example of a id13 ontology in orientdb:
   [0*eygcktvkmhll8txa.png]
   source: [30]orientdb demo page

   second, what is a pattern we can detect?

   we want to label nodes. each entity in the graph has some feature we
   want to classify, and we only have labels for some of the nodes. there
   are simple [31]boolean labels we can predict, like    person    or    not
   person   , and more interesting labels like classification of nodes into
   one of several categories. and then there are more complex regressions
   we can do, like predicting the risk posed by an entity based on the
   data we have on the entity from the graph. this includes the connection
   of nodes to other nodes. let   s stick to the boolean node
   labeling/classification problem in this article, to keep it simple. we
   want to label 594,643 transactions by about 4,000 bank accounts as
   either suspicious or not. and we want to do it in less than a minute.
   not hours or days.

   third, how can we define a graph that kegra understands?

   we need to specify two files. the first has the node ids with node
   descriptions, and the second says how the nodes connect. in the cora
   example provided with kegra, there are 2,708 node   s descriptions and
   labels, with 5,429 edges (pairs of nodes), defining connections of
   nodes to each other.

   here is a view of a few lines of each file:
   [1*my1t7qc55jw5mxwoxy8sqg.png]
   links between nodes
   [1*kpzgayly5bjgwvejjgwaaw.png]
   each node id is followed by features (mostly 0s) and finally there is a
   node label (e.g. neural_networks, case_based). the features are mostly
   0s and wrap around for many lines in the screenshot above. each feature
   represents the use in the document (node) of a certain word. more info
   in the [32]kegra readme here.

   let   s try it out

   firstly, you need keras 2, so do this:
pip install keras --upgrade

   assuming you have keras and tensorflow installed, keras-id197 depends on
   id197, so let   s git clone and install them one by one.
#install id197
git clone [33]https://github.com/tkipf/id197.git
cd id197/
python setup.py install
cd ..
#install keras-id197
git clone [34]https://github.com/tkipf/keras-id197.git
cd keras-id197/
python setup.py install

   first let   s run the code on an off the shelf example [35]provided with
   kegra called cora. we see in the output that cora has detected and
   printed the expected number of nodes and edges from the raw data.
   [1*eqgr97jtjqhg2brboh9lfa.png]
   training run on the cora dataset: 36% accuracy and rising.
   [1*hpygi4g52sknjipmqnavjw.png]
   testing result on the cora dataset: 77.6% accuracy.

   we now make a slight change to the way kegra understands input files,
   just to make the names nicer. in the current version on github, the
   input files are    *.cites    for describing arcs between nodes, and
      *.content    for describing nodes. instead i have changed kegra to read
   in    *.link    and    *.node    files. your data folder should look like this
   now:
~/kegra/keras-id197/kegra$ ls -l data/cora/
total 7720
-rwxrwxr-x 1 ubuntu ubuntu 69928 dec 3 02:52 cora.link (was cora.cites)
-rwxrwxr-x 1 ubuntu ubuntu 7823427 dec 3 02:52 cora.node (was cora.content)
-rwxrwxr-x 1 ubuntu ubuntu 1560 dec 3 02:52 readme
~/kegra/keras-id197/kegra$ ls -l data/customertx/
total 7720
-rwxrwxr-x 1 ubuntu ubuntu 7823427 dec 3 05:20 customertx.node
-rwxrwxr-x 1 ubuntu ubuntu 1560 dec 3 05:20 readme
-rwxrwxr-x 1 ubuntu ubuntu 69928 dec 3 05:20 customertx.link

   let us now fill in customertx.node and customertx.link with transaction
   data. the first file is the list of the the bank customers and their
   features. the format is:
   [1*bificnd75ae_xmvyq6rqqa.png]
   quick view of some transaction records. in this scenario, there is a
   sender and recipient of money, and a record of the amount sent (amount
   column), and the label applied by a human analyst that reviewed the
   transaction (fraud column). we can ignore the first two columns (the
   index and step columns).
   [1*8w7iho8k-s2mxnpbzg5jtg.png]
   the edges file (customertx.link) records who the two parties are in
   each transaction.
   [1*yyqmqfup2qjzlvjqcky3ua.png]
   the nodes file (customertx.node) records information on each node in
   the graph as a sender of funds on each transaction. the txcount column
   lists the number of transactions (edges) leaving the node. the
   amountmean column specifies the mean transaction size. the fraudmean
   column is the mean of flagged transactions on the sender account during
   the period this data covers. note that the vast majority of
   transactions are ok and not fraud, which is a [36]dataset imbalance.
   [1*xw9esaupnv0nz2mzmzelig.png]
   there are 4112 nodes in the graph. on average 2.3% have been flagged as
   problematic by an analyst.

   we can now use kegra to analyze the graph with various levels of
   analyst accuracy.

   if the system is trained on data by a perfect analyst, it should learn
   perfectly how to analyze the graph. however, if the human analyst is
   wrong 20% of the time, then the predictive power of the kegra model
   should similarly be limited to 80%. to try this out, i added different
   amounts of random noise to the graph labels, to see how kegra would do
   as the quality of the training data gets worse and worse.

   here are the results in table and graph forms:
   [1*2p4drmhlvgvum3etfvsdha.png]
   raw results for the transaction labeling experiments, using deep
   learning on id13s
   [1*a1dzakjtox73hev5biticw.png]
   this is the same data as the table above, but in an easier to
   understand graphic

   there is a lot to digest here. first, we see that early stopping
   (labels on x-axis) kicks in earlier and earlier in the training, as the
   noise in the data (blue) increases. this tells us that the number of
   features being so small (few columns) is leading to increasing
   overfitting on the training data. second, we see that the test accuracy
   is generally lower than the training accuracy. this is expected, as the
   training data is familiar to the classifier while the testing data is
   not. third, the test accuracy is not zero. good! that implies the
   classifier can regenerate the ok/fraud labels using only the graph and
   the features of each node (txcount, amountmean, and fraudmean). fourth,
   the accuracy of the classifier (orange) goes down as the injected noise
   (blue) goes up. this implies that the results are not random. fifth, we
   see that the training accuracy (red) plus the added noise (blue) adds
   to around 100%, which implies that the classifier is as good/bad as the
   analyst who labels the dataset, but not much worse.

   in summary, kegra performed very well on id13
   classification. compared to the results in their paper, these results
   are maybe too good. i will check if perhaps the fraud label column from
   the transaction file is too explanatory, and substitute it with
   features that are harder to use to predict from the broader dataset
   like country of origin, city, zip, and more columns.

   my next action is to regenerate the transaction dataset from the source
   files with more columns, and see if kegra still performs so well. on
   the cora dataset there was no early stopping, and so i suspect the
   transaction data was not as challenging for kegra, for one of the
   reasons i mentioned above. maybe if i embed more semantic features into
   the generated graph    there are lots of fun things i can do as next
   steps.

   special thanks to [37]thomas kipf for eyeballing this article before
   publication. this was a really complicated article to prepare (and to
   read) compared to my usual high-level articles. if you enjoyed this
   article on deep learning on graphs, then please let me know to write
   more [38]researchy content like this. i   m also happy to hear your
   feedback in the comments. what do you think?

   try out the clap tool. tap that. follow us on medium. share a link to
   this article. go for it.

   happy coding!

   -daniel
   [39]daniel@lemay.ai     this actually works. say hi.

   [40]lemay.ai
   [41]1(855)lemay-ai

   other articles you may enjoy:
     * [42]artificial intelligence and bad data
     * [43]artificial intelligence: hyperparameters
     * [44]artificial intelligence: get your users to label your data

     * [45]machine learning
     * [46]deep learning
     * [47]data science
     * [48]artificial intelligence
     * [49]towards data science

   (button)
   (button)
   (button) 841 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of daniel shapiro, phd

[50]daniel shapiro, phd

   medium member since oct 2017

   passionate about machine learning r&d and value creation.    
   [51]daniel@lemay.ai     [52]https://lemay.ai

     (button) follow
   [53]towards data science

[54]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 841
     * (button)
     *
     *

   [55]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [56]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/98e340488b93
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/kegra-deep-learning-on-knowledge-graphs-with-keras-98e340488b93&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/kegra-deep-learning-on-knowledge-graphs-with-keras-98e340488b93&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_hixlso5n8czp---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@lemaysolutions
  17. https://towardsdatascience.com/machine-learning-use-small-words-5cc8f34a5964
  18. https://github.com/tkipf/keras-id197?files=1
  19. https://arxiv.org/abs/1609.02907
  20. https://math.stackexchange.com/questions/31207/graph-terminology-vertex-node-edge-arc
  21. https://link.springer.com/chapter/10.1007/978-3-642-28305-5_18?lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  22. https://ruor.uottawa.ca/handle/10393/20716?lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  23. http://ieeexplore.ieee.org/document/5873060/?arnumber=5873060&lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  24. http://ieeexplore.ieee.org/document/6030491/?arnumber=6030491&searchwithin="authors":.qt.daniel shapiro.qt.&newsearch=true&lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  25. https://ruor.uottawa.ca/handle/10393/19799
  26. http://ieeexplore.ieee.org/document/5662163/?arnumber=5662163&lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  27. https://ruor.uottawa.ca/handle/10393/12896?lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  28. https://search.proquest.com/docview/305138347?lipi=urn:li:page:d_flagship3_profile_view_base;hljc+sj0th+ahnjsotolrw==
  29. https://ruor.uottawa.ca/handle/10393/12898
  30. https://orientdb.com/docs/3.0.x/gettingstarted/demodb/queries/demodb-queries-shortest-paths.html
  31. https://english.stackexchange.com/questions/4481/should-the-word-boolean-be-capitalized
  32. https://github.com/tkipf/keras-id197/blob/master/kegra/data/cora/readme
  33. https://github.com/tkipf/id197.git
  34. https://github.com/tkipf/keras-id197.git
  35. https://github.com/tkipf/keras-id197/tree/master/kegra/data/cora
  36. https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
  37. https://github.com/tkipf
  38. https://en.wiktionary.org/wiki/researchy
  39. mailto:daniel@lemay.ai
  40. https://lemay.ai/
  41. about:invalid#zsoyz
  42. https://towardsdatascience.com/artificial-intelligence-and-bad-data-fbf2564c541a
  43. https://towardsdatascience.com/artificial-intelligence-hyperparameters-48fa29daa516
  44. https://medium.com/towards-data-science/artificial-intelligence-get-your-users-to-label-your-data-b5fa7c0c9e00
  45. https://towardsdatascience.com/tagged/machine-learning?source=post
  46. https://towardsdatascience.com/tagged/deep-learning?source=post
  47. https://towardsdatascience.com/tagged/data-science?source=post
  48. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  49. https://towardsdatascience.com/tagged/towards-data-science?source=post
  50. https://towardsdatascience.com/@lemaysolutions
  51. mailto:daniel@lemay.ai
  52. https://lemay.ai/
  53. https://towardsdatascience.com/?source=footer_card
  54. https://towardsdatascience.com/?source=footer_card
  55. https://towardsdatascience.com/
  56. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  58. https://towardsdatascience.com/@lemaysolutions?source=post_header_lockup
  59. https://medium.com/p/98e340488b93/share/twitter
  60. https://medium.com/p/98e340488b93/share/facebook
  61. https://towardsdatascience.com/@lemaysolutions?source=footer_card
  62. https://medium.com/p/98e340488b93/share/twitter
  63. https://medium.com/p/98e340488b93/share/facebook
