   [1]tryolabs blog
     * tryolabs

     * [2]blog
     * [3]subscribe
     * [4]get in touch
     * [5]search

     * [6]all
     * [7]backend
     * [8]frontend
     * [9]machine-learning
     * [10]mobile
     * [11]news
     * [12]others
     * [13]resources

   read time: 19 minutes

deep learning for nlp: advancements & trends

   [14]javier tue, dec 12, 2017 in [15]machine learning
     * [16]deep learning
     * [17]nlp
     * [18]recap

   over the past few years, [19]deep learning (dl) architectures and
   algorithms have made impressive advances in fields such as [20]image
   recognition and [21]speech processing.

   their application to [22]natural language processing (nlp) was less
   impressive at first, but has now proven to make significant
   contributions, yielding state-of-the-art results for some common nlp
   tasks. [23]id39 (ner), [24]part of speech (pos)
   tagging or [25]id31 are some of the problems where
   [26]neural network models have outperformed traditional approaches. the
   progress in [27]machine translation is perhaps the most remarkable
   among all.

   in this article i will go through some recent advancements for nlp that
   rely on dl techniques. i do not pretend to be exhaustive: it would
   simply be impossible given the vast amount of scientific papers,
   frameworks and tools available. i just want to share with you some of
   the works that i liked the most. i think the last months have been
   great for our field. the use of dl in nlp keeps widening, yielding
   amazing results in some cases, and all signs point to the fact that
   this trend will not stop.

from training id97 to using pre-trained models

   arguably, [28]id27s is the most known technique related to dl
   for nlp. they follow the [29]distributional hypothesis, by harris
   (1954), according to which words with similar meaning usually occur in
   comparable contexts. for a detailed explanation of id27s, i
   suggest you to read [30]this great post by gabriel mordecki.
   image with an example of distributional vectors of words

    example of distributional vectors of words.

   [31]image source

   algorithms such as [32]id97 (mikolov et al., 2013) and [33]glove
   (pennington et al., 2014) have been pioneers in the field, and although
   they cannot be considered as dl (neural network in id97 is shallow
   and glove implements a count-based method), the models trained with
   them are used as input data in a lot of dl for nlp approaches. this is
   so true that using id27s in our field is now generally
   considered a good practice.

   at the beginning, for a given nlp problem that required word
   embeddings, we tended to train our own model from a big corpus that was
   domain related. this is not, of course, the best way to democratize the
   use of id27s, so pre-trained models slowly began to arrive.
   trained on wikipedia, twitter, google news, web crawls, and more, these
   models allow you to easily integrate id27s to your dl
   algorithms.

   the latest developments confirmed that pre-trained id27
   models is still a key issue in nlp. for example [34]fasttext, from the
   [35]facebook ai research (fair) lab, released [36]pre-trained vectors
   in 294 languages, which represents a great work and contribution to our
   community. besides the wide number of languages, this is very useful
   because fasttext uses character id165s as features. this allows
   fasttext to avoid the oov (out of vocabulary) problem, since even a
   very rare word (e.g. specific domain terminology) will probably share
   some character id165s with more common words. in this sense, fasttext
   behaves better than id97 and glove, and outperforms them for small
   datasets.

   however, although we can see some progress, there is still a lot to do
   in this area. the great nlp framework [37]spacy, for example,
   integrates id27s and dl models for tasks such as ner and
   id33 in a native way, allowing the users to update the
   models or use their own models.

   i think this is the way to go. in the future, it would be great to have
   pre-trained models for specific domains (e.g. biology, literature,
   economy, etc.) that are easy to use in a nlp framework. the icing on
   the cake would be the capability of fine tuning them, in the simplest
   way possible, for our use case. in the meantime, methods for adapting
   id27s are beginning to appear.

adapting generic embeddings to specific use cases

   maybe the major downside of using pre-trained id27s is the
   word distributional gap existing between the training data and the
   actual data of our problem. let   s say you have a corpus of biology
   papers, food recipes or research papers in economics. it is more than
   likely that generic id27s are going to help you improve your
   results, since you probably don   t have a corpus big enough to train
   good embeddings. but what if you could adapt generic embeddings to your
   specific use case?

   these kinds of adaptations are usually called cross-domain or
   [38]id20 techniques in nlp, and are very close to
   [39]id21. a very interesting work in this vein was
   proposed by [40]yang et al.. they have presented a regularized
   skip-gram model for learning embeddings for a target domain, given the
   embeddings of a source domain.

   the key idea is simple yet effective. imagine that we know the word
   embedding ws for the word _w
   in the source domain. to compute the embedding for w_t (target domain),
   the authors add to w_s a certain amount of transfer between both
   domains. basically, if the word is frequent in both domains, that means
   that its semantics is not domain dependent. in this case, the amount of
   transfer is high, so the resulting embeddings tend to be similar in
   both domains. but since domain-specific words are more frequent in one
   domain than the other, the amount of transfer is small.

   this is a research topic for id27s has not been widely
   explored, and i think that it is going to get more attention in the
   near future.

id31 as an incredible side effect

   the penicillin, the x-ray or even the post-it were unexpected
   discoveries. [41]radford et al. were exploring the properties of
   byte-level recurrent language models, with the goal of predicting the
   next character in the text of amazon reviews, when they found that one
   single neuron in the trained model was highly predictive of the
   sentiment value. yes, this single    sentiment neuron    was capable of
   classifying the reviews as positive or negative, in a pretty accurate
   way.
   graph showing the relation between the number of reviews and the value
   of the senitment neuron

    review polarity vs value of the neuron.

   [42]image source

   having noted that behavior, the authors have decided to test the model
   on the [43]stanford sentiment treebank, and found that its accuracy was
   of 91.8%, versus the previous best of 90.2%. this means that using
   significantly less examples, their model, trained in an unsupervised
   manner, achieves state-of-the-art id31, at least on one
   specific but extensively-studied dataset.

  the sentiment neuron at work

   since the model works at the character level, the neuron changes its
   state for each character in a text, and it is pretty striking to see
   how it behaves.

    behavior of the sentiment neuron.

   [44]image source

   after the word best, for example, the value of the neuron becomes
   strongly positive. this effect, however, disappears with the word
   horrendous, which makes sense.

  generating polarity biased text

   of course, the trained model is still a valid generative model, so it
   can be used to generate texts similar to the amazon reviews. but what i
   find great about it, is that you can choose the polarity of the
   generated text by simply overwriting the value of the sentiment neuron.

   sentiment fixed to positive sentiment fixed to negative
   best hammock ever! stays in place and holds its shape. comfy (i love
   the deep neon pictures on it), and looks so cute. they didn   t fit
   either. straight high sticks at the end. on par with other buds i have.
   lesson learned to avoid.
   just what i was looking for. nice fitted pants, exactly matched seam to
   color contrast with other pants i own. highly recommended and also very
   happy! the package received was blank and has no barcode. a waste of
   time and money.

   examples of generated texts ([45]source).

   the nn model chosen by the authors is a [46]multiplicative lstm,
   presented by [47]krause et al. (2016), mainly because they observed
   that it converged faster than normal lstms for the hyperparameter
   settings they were exploring. it has 4,096 units and was trained on a
   corpus of 82 million amazon reviews.

   why the trained model captures in such a precise way the notion of
   sentiment is still an open and fascinating question. meanwhile, you can
   try to train you own model and experiment with it. if you have time and
   gpus available, of course: the training of this particular model took
   one month to the authors of the work, across four nvidia pascal gpus.

id31 in twitter

   whether to know what people say about your business   s brand, to analyze
   the impact of marketing campaigns or to gauge the global feeling about
   hillary clinton and donald trump during the last campaign, sentiment
   analysis in twitter is a very powerful tool.
   id31 in twitter, taking the example of donald trump and
   hillary clinton

    donald trump vs hillary clinton: id31 on twitter.

   [48]image source

  semeval 2017

   id31 in twitter has drawn a lot of attention from
   researchers in nlp, but also in political and social sciences. that is
   why since 2013, [49]semeval proposes a specific task.

   in 2017, a total of 48 teams participated in the evaluation, which
   shows how much interest it generates. to give you an idea of [50]what
   exactly semeval evaluates in the case of twitter, let   s take a look at
   the five subtasks proposed this year.
    1. subtask a: given a tweet, decide whether it expresses positive,
       negative or neutral sentiment.
    2. subtask b: given a tweet and a topic, classify the sentiment
       conveyed towards that topic on a two-point scale: positive vs.
       negative.
    3. subtask c: given a tweet and a topic, classify the sentiment
       conveyed in the tweet towards that topic on a five-point scale:
       stronglypositive, weaklypositive, neutral, weaklynegative, and
       stronglynegative.
    4. subtask d: given a set of tweets about a topic, estimate the
       distribution of tweets across the positive and negative classes.
    5. subtask e: given a set of tweets about a topic, estimate the
       distribution of tweets across the five classes: stronglypositive,
       weaklypositive, neutral, weaklynegative, and stronglynegative.

   as you can see, the subtask a is the most common task, and 38 teams
   participated in it, but the others are more challenging. the organizers
   note that the use of dl methods stands out and keeps increasing, with
   20 teams this year using models such as [51]convolutional neural
   networks (id98) and [52]long short term memory (lstm). moreover,
   although [53]id166 models are still very popular, several participants
   combined them with neural network methods or used id27
   features.

  the bb_twtr system

   what i find remarkable is that one pure dl system, the
   _[54]bb_twtr_[55] system (cliche, 2017), was ranked first in the 5
   subtasks for english. the author combines an ensemble of 10 id98s and 10
   bilstms trained with different hyper-parameters and different
   pre-training strategies. you can see the details of the architectures
   of the networks in the paper.

   to train the models, the author uses the human labeled tweets (to give
   you an order of magnitude, there are 49,693 for the subtask a), and
   builds an unlabeled dataset of 100 million tweets, from which he
   extracts a distant dataset by simply labeling a tweet as positive in
   presence of positive emoticons such as :-) and vice versa for negative
   tweets. the tweets are lowercased, tokenized, urls and emoticons are
   replaced by specific tokens (, , etc.) and character repetitions are
   unified, so, for example,    niiice    and    niiiiiiiice    become both
      niice   .

   to pre-train the id27s used as input for the id98s and the
   bilstms, the author uses id97, glove and fasttext, all with the
   default settings, over the unlabeled dataset. then he refines the
   embeddings using the distant dataset to add polarity information, and
   he refines them again using the human labeled dataset.

   the experimentations using the previous semeval datasets, show that
   using glove gives lower performances and that there is not one unique
   best model for all the gold standard datasets. the author then combines
   all the models with a soft voting strategy. the resulting model
   outperforms the previous best historical scores for 2014 and 2016 and
   is very close for the other years. finally, it is ranked first in the 5
   semeval 2017 subtasks for english.

   even if the combination is not performed in an organic way but with a
   simple soft voting strategy, this work shows the potential of combining
   dl models, and also the fact that an almost end-to-end method (the
   input must be pre-processed) can outperform supervised methods in
   id31 in twitter.

an exciting abstractive summarization system

   [56]id54 was, with [57]automatic translation, one of
   the first nlp tasks. there are two main families of approaches:
   extraction-based, were the summary is built by extracting the most
   important segments from the source text, and abstraction-based, were
   the summary is built by generating the text. historically,
   extraction-based approaches have been the most frequent because of
   their simplicity over the abstraction-based approaches.

   in the last years, id56-based models have achieved amazing results in
   text generation. they perform really well for short inputs and output
   texts, but tend to be incoherent and repetitive for long texts. in
   their work, [58]paulus et al. propose a new neural network model to
   overcome this limitation. the results are exciting, as you can see in
   the image below.

    illustration of the model generating a summary.

   [59]image source

   the authors use a bilstm encoder to read the input and an lstm decoder
   to generate the output. their main contributions are a new
   intra-attention strategy that attends over the input and the
   continuously generated output separately, and a new training method
   that combines standard supervised word prediction and reinforcement
   learning.

  intra-attention strategy

   the goal of the proposed intra-attention strategy is to avoid the
   repetitions in the output. to achieve this, they use temporal attention
   when decoding to look at previous segments of the input text, before
   deciding which word will be generated next. this forces the model to
   use different parts of the input in the generation process. they also
   allow the model to access the previous hidden states from the decoder.
   these two functions are then combined to choose the best next word for
   the output summary.

  id23

   to create a summary, two different people will use different words and
   sentence orders, and both summaries will probably be considered as
   valid. thus, a good summary does not necessarily have to be a sequence
   of words that match a sequence in the training dataset as much as
   possible. knowing this, the authors avoid the standard teacher forcing
   algorithm, which minimizes the loss at each decoding step (i.e. for
   each generated word), and they rely on a id23
   strategy that proves to be an excellent choice.

  great results for an almost end-to-end model

   the model was tested on the [60]id98/daily mail dataset and achieved
   state-of-the-art results. a specific experimentation with human
   evaluators showed, in addition, an increase in human readability and
   quality. these results are impressive given such basic pre-processing:
   the input texts are tokenized, lowercased, the numbers are replaced
   with    0    and some specific entities of the dataset are removed.

a first step towards fully unsupervised machine translation?

   bilingual lexicon induction, that is, identifying word translation
   pairs using source and target monolingual corpora in two languages, is
   an old nlp task. automatically induced bilingual lexicons help in other
   nlp tasks such as [61]information retrieval and [62]statistical machine
   translation. however, the approaches rely most of the time on some kind
   of resource, typically an initial bilingual lexicon, which is not
   always available or easy to build.

   with the success of id27s, the idea of cross-lingual word
   embeddings appeared, were the goal is to align embedding spaces instead
   of lexicons. unfortunately, the first approaches also rely on bilingual
   lexicons or parallel corpora. in their work, [63]conneau et al. (2018)
   present a very promising approach that does not rely on any specific
   resource, and outperforms state-of-the-art supervised approaches on
   several language pairs for the tasks of word translation, sentence
   translation retrieval, and cross-lingual word similarity.

   the method developed by the authors takes as input two sets of word
   embeddings trained independently on monolingual data and learns a
   mapping between them such that translations are close in the common
   space. they use unsupervised word vectors trained on wikipedia
   documents with fasttext. the following image illustrates the key idea.
   illustration showing the mapping between two id27 spaces

    building the mapping between two id27 spaces.

   [64]image source

   the x distributions in red are the embeddings for english words and the
   y distributions in blue the ones for italian words.

   first, they use [65]adversarial learning to learn a rotation matrix w
   that is going to perform a first raw alignment. they basically train a
   [66]generative adversarial network (gan), following the proposition
   made by [67]goodfellow et al. (2014). to have an intuitive idea of how
   gans work, i recommend you [68]this excellent post by pablo soto.

   to model the problem in terms of adversarial learning, they define the
   discriminator to have the role of determining, given some elements
   randomly sampled from wx and y (see second column in the picture
   above), to which language each one of them belongs. then, they train w
   to prevent the discriminator from making good predictions. this seems
   to me very clever and elegant, and the direct results are pretty nice.

   after that, they apply two more steps to refine the mapping. one to
   avoid the noise that rare words introduce in the mapping computation.
   the other one to build the actual translations, mainly using the
   learned mapping and a distance measure.

   the results in some cases are impressive regarding the
   state-of-the-art. for example, in the case of english-italian word
   translation, they outperform the best average precision for 1.500
   source words by nearly 17% in the p@10 case.
   table showing the english-italian word translation average precisions

    english-italian word translation average precisions.

   [69]image source

   the authors claim that their method can be used as a first step towards
   unsupervised machine translation. it would be great if that is the
   case. in the meantime, let   s see how far this new promising method can
   go.

specialized frameworks and tools

   there are a lot of generic dl frameworks and tools, some of them widely
   used, such as [70]tensorflow, [71]keras or [72]pytorch. however,
   specific open source nlp oriented dl frameworks and tools are just
   emerging. this has been a good year for us because some very useful
   open source frameworks have been made available to the community. three
   of them caught my attention in particular, that you might also find
   interesting.

  allennlp

   the [73]allennlp framework is a platform built on top of pytorch,
   designed to easily use dl methods in semantic nlp tasks. its goal is to
   allow researchers to design and evaluate new models. it includes
   reference implementations of models for common semantic nlp tasks such
   as id14, id123 and coreference
   resolution.

  parlai

   the [74]parlai framework is an open-source software platform for dialog
   research. it is implemented in python and its goal is to provide a
   unified framework for sharing, training and testing of dialog models.
   parlai provides a mechanism for easy integration with amazon mechanical
   turk. it also provides popular datasets in the field and supports
   several models, including neural models such as memory networks,
   id195 and attentive lstms.

  openid4

   the [75]openid4 toolkit is a generic framework specialized in
   sequence-to-sequence models. it can be used in particular to perform
   tasks such as machine translation, summarization, image to text, and
   id103.

final thoughts

   the constant increment of dl techniques used for nlp problems is
   undeniable. a good indicator of this is the variation of the percentage
   of deep learning papers in key nlp conferences such as [76]acl,
   [77]emnlp, [78]eacl and [79]naacl, over the last years.
   graph showing the development of the percentage of deep learning papers
   in key nlp conferences

    percentage of deep learning papers.

   [80]image source

   however, works on true end-to-end learning are just beginning to
   emerge. we are still dealing with some classic nlp tasks to prepare the
   dataset, such as cleaning, id121 or unification of some entities
   (e.g. urls, numbers, e-mail addresses, etc.). we also use generic
   embeddings, with the drawback that they fail to capture the importance
   of specific domain terms, and also that they perform poorly for
   multi-word expressions, a key issue that i find over and over in the
   projects that i work on.

   the latest advancements have been great for dl applied to nlp. i hope
   that the future brings more end-to-end learning works and that the
   specific open source frameworks get more developed. please feel free to
   share with us in the comments section your opinion about these works
   and frameworks, and also those that you liked this year and that i do
   not mention here.

further reading

   for more information about the deep learning methods in nlp research
   today, i strongly recommend you the excellent paper    [81]recent trends
   in deep learning based natural language processing    by young et al.
   (2017).

   another interesting reading is the report from the seminar    [82]from
   characters to understanding natural language (c2nlu): robust end-to-end
   deep learning for nlp    by blunsom et al. (2017), were researchers on
   nlp, computational linguistics, deep learning and general machine
   learning have discussed about the advantages and challenges of using
   characters as input for deep learning models instead of
   language-specific tokens.

   to have a comparative perspective between models, i can recommend you a
   very interesting [83]comparative study of id98 and id56 for nlp, by yin
   et al. (2017).

   to have an intuitive idea of how gans work, you can read[84] this
   excellent post by pablo soto, that presents the major advancements in
   deep learning in 2016.

   for a detailed explanation of id27s, i suggest you to read
   [85]this great post by gabriel mordecki. it is written in a didactic
   and entertaining way, and explains the different methods and even some
   myths about id27s.

   finally, sebastian ruder wrote a pretty nice and exhaustive post about
   id27s in 2017, that you might find useful: [86]about word
   embeddings in 2017: trends and future directions.

   oh    and if you liked this post, you should [87]subscribe to our
   newsletter so you don   t miss the ones to come :)

bibliography

     * [88]from characters to understanding natural language (c2nlu):
       robust end-to-end deep learning for nlp phil blunsom, kyunghyun
       cho, chris dyer and hinrich sch  tze (2017)
     * [89]bb_twtr at semeval-2017 task 4: twitter id31 with
       id98s and lstms mathieu cliche (2017)
     * [90]word translation without parallel data alexis conneau,
       guillaume lample, marc   aurelio ranzato, ludovic denoyer, herv  
       j  gou (2018)
     * [91]generative adversarial nets ian goodfellow, jean pouget-abadie,
       mehdi mirza, bing xu, david warde-farley, sherjil ozair, aaron
       courville and yoshua bengio (2014)
     * [92]distributional structure zellig harris (1954)
     * [93]openid4: open-source toolkit for id4
       guillaume klein, yoon kim, yuntian deng, jean senellart and
       alexander m rush. (2017)
     * [94]multiplicative lstm for sequence modelling ben krause, liang
       lu, iain murray and steve renals (2016)
     * [95]parlai: a dialog research software platform alexander h miller,
       will feng, adam fisch, jiasen lu, dhruv batra, antoine bordes, devi
       parikh and jason weston (2017)
     * [96]linguistic regularities in continuous space word
       representations tomas mikolov, scott wen-tau yih and geoffrey zweig
       (2013)
     * [97]glove: global vectors for word representation jeffrey
       pennington, richard socher and christopher d. manning (2014)
     * [98]learning to generate reviews and discovering sentiment alec
       radford, rafal jozefowicz and ilya sutskever (2017)
     * [99]a simple id173-based algorithm for learning
       cross-domain id27s wei yang, wei lu, vincent zheng (2017)
     * [100]comparative study of id98 and id56 for natural language
       processing wenpeng yin, katharina kann, mo yu and hinrich sch  tze
       (2017)
     * [101]recent trends in deep learning based natural language
       processing tom younga, devamanyu hazarikab, soujanya poriac and
       erik cambriad (2017)

   [102]twitter [103]linkedin [104]facebook [105]reddit
     __________________________________________________________________

like what you read?

   subscribe to our newsletter and get updates on deep learning, nlp,
   id161 & python.
   ____________________ subscribe
   error
   success!
   sending...
   ____________________
   no spam, ever. we'll never share your email address and you can opt out
   at any time.

   please enable javascript to view the [106]comments powered by disqus.
   [107]comments powered by disqus

what to read next

     *

5 actionable steps to get your data ready for price optimization with ml
     *

11 questions to ask before starting a successful machine learning project
     *

the major advancements in deep learning in 2018

get in touch

do you have a project in mind?
we'd love to e-meet you!

   ____________________
   ____________________
   ____________________
   ____________________

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   [ ] subscribe to receive news and blog updates
   contact us
   thanks for getting in touch! we will reply shortly.
   there was an error :(

   tryolabs

   about
     * [108]about us
     * [109]what we do
     * [110]ml consulting

   our work
     * [111]clients
     * [112]products
     * [113]brochure

   community
     * [114]blog
     * [115]open source
     * [116]careers

   contact
     * uy phone: (598) 2716 8997
     * [117]hello@tryolabs.com

   offices
     * us: 156 2nd street, sf.
     * uy: rambla gandhi 655/701, mvd.

   social
   twitter
   linkedin
   github
   facebook
   instagram
   tryolabs    2019. all rights reserved.

references

   visible links
   1. https://tryolabs.com/
   2. https://tryolabs.com/blog/
   3. https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/#subscribe
   4. https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/#contact-form
   5. https://tryolabs.com/blog/search/
   6. https://tryolabs.com/blog/
   7. https://tryolabs.com/blog/categories/backend/
   8. https://tryolabs.com/blog/categories/frontend/
   9. https://tryolabs.com/blog/categories/machine-learning/
  10. https://tryolabs.com/blog/categories/mobile/
  11. https://tryolabs.com/blog/categories/news/
  12. https://tryolabs.com/blog/categories/others/
  13. https://tryolabs.com/blog/categories/resources/
  14. https://tryolabs.com/blog/authors/javier-couto/
  15. https://tryolabs.com/blog/categories/machine-learning/
  16. https://tryolabs.com/blog/tags/deep-learning/
  17. https://tryolabs.com/blog/tags/nlp/
  18. https://tryolabs.com/blog/tags/recap/
  19. https://en.wikipedia.org/wiki/deep_learning
  20. https://en.wikipedia.org/wiki/computer_vision#recognition
  21. https://en.wikipedia.org/wiki/speech_processing
  22. https://en.wikipedia.org/wiki/natural_language_processing
  23. https://en.wikipedia.org/wiki/named-entity_recognition
  24. https://en.wikipedia.org/wiki/part-of-speech_tagging
  25. https://en.wikipedia.org/wiki/sentiment_analysis
  26. https://en.wikipedia.org/wiki/artificial_neural_network
  27. https://en.wikipedia.org/wiki/machine_translation
  28. https://en.wikipedia.org/wiki/word_embedding
  29. https://en.wikipedia.org/wiki/distributional_semantics
  30. https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/
  31. https://veredshwartz.blogspot.com.uy/2016/01/representing-words.html
  32. https://www.aclweb.org/anthology/n13-1090
  33. http://www.aclweb.org/anthology/d14-1162
  34. https://fasttext.cc/
  35. https://research.fb.com/category/facebook-ai-research-fair/
  36. https://fasttext.cc/blog/2017/05/02/blog-post.html
  37. https://spacy.io/
  38. https://en.wikipedia.org/wiki/domain_adaptation
  39. https://en.wikipedia.org/wiki/transfer_learning
  40. http://aclweb.org/anthology/d17-1312
  41. https://arxiv.org/abs/1704.01444
  42. https://blog.openai.com/unsupervised-sentiment-neuron/
  43. https://nlp.stanford.edu/sentiment/treebank.html
  44. https://blog.openai.com/unsupervised-sentiment-neuron/
  45. https://blog.openai.com/unsupervised-sentiment-neuron/
  46. https://arxiv.org/abs/1609.07959
  47. https://arxiv.org/abs/1609.07959
  48. https://monkeylearn.com/blog/donald-trump-vs-hillary-clinton-sentiment-analysis-twitter-mentions/
  49. https://en.wikipedia.org/wiki/semeval
  50. http://www.aclweb.org/anthology/s17-2088
  51. https://en.wikipedia.org/wiki/convolutional_neural_network
  52. https://en.wikipedia.org/wiki/long_short-term_memory
  53. https://en.wikipedia.org/wiki/support_vector_machine
  54. https://arxiv.org/abs/1704.06125
  55. https://arxiv.org/abs/1704.06125
  56. https://en.wikipedia.org/wiki/automatic_summarization
  57. https://en.wikipedia.org/wiki/machine_translation
  58. https://arxiv.org/abs/1705.04304
  59. https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization
  60. https://github.com/abisee/id98-dailymail
  61. https://en.wikipedia.org/wiki/information_retrieval
  62. https://en.wikipedia.org/wiki/statistical_machine_translation
  63. https://arxiv.org/abs/1710.04087
  64. https://arxiv.org/abs/1710.04087
  65. https://en.wikipedia.org/wiki/adversarial_machine_learning
  66. https://en.wikipedia.org/wiki/generative_adversarial_network
  67. https://arxiv.org/abs/1406.2661
  68. https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/
  69. https://arxiv.org/abs/1710.04087
  70. https://github.com/tensorflow/tensorflow
  71. https://github.com/fchollet/keras
  72. http://pytorch.org/
  73. http://allennlp.org/papers/allennlp_white_paper.pdf
  74. https://arxiv.org/abs/1705.06476
  75. https://arxiv.org/abs/1701.02810
  76. https://www.aclweb.org/
  77. https://www.aclweb.org/portal/emnlp
  78. http://www.eacl.org/
  79. http://naacl.org/
  80. https://arxiv.org/abs/1708.02709
  81. https://arxiv.org/abs/1708.02709
  82. http://drops.dagstuhl.de/opus/volltexte/2017/7248/pdf/dagrep_v007_i001_p129_s17042.pdf
  83. https://arxiv.org/abs/1702.01923
  84. https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/
  85. https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/
  86. http://ruder.io/word-embeddings-2017/
  87. https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/#subscribe
  88. http://drops.dagstuhl.de/opus/volltexte/2017/7248/pdf/dagrep_v007_i001_p129_s17042.pdf
  89. https://arxiv.org/abs/1704.06125
  90. https://arxiv.org/abs/1710.04087
  91. https://arxiv.org/abs/1406.2661
  92. https://doi.org/10.1080/00437956.1954.11659520
  93. https://arxiv.org/abs/1701.02810
  94. https://arxiv.org/abs/1609.07959
  95. http://www.aclweb.org/anthology/d17-2014
  96. https://www.aclweb.org/anthology/n13-1090
  97. https://www.aclweb.org/anthology/d14-1162
  98. https://arxiv.org/abs/1704.01444
  99. http://aclweb.org/anthology/d17-1312
 100. https://arxiv.org/abs/1702.01923
 101. https://arxiv.org/abs/1708.02709
 102. http://twitter.com/share?url=https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/&text=deep learning for nlp: advancements &amp; trends
 103. https://www.linkedin.com/sharearticle?mini=true&url=https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/&title=deep learning for nlp: advancements &amp; trends&source=tryolabs&summary=the use of deep learning for nlp (natural language processing) is widening and yielding amazing results. this overview covers some major advancements & recent trends.
 104. https://www.facebook.com/sharer.php?u=https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/&title=deep learning for nlp: advancements &amp; trends
 105. http://www.reddit.com/submit?url=https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/&title=deep learning for nlp: advancements &amp; trends
 106. http://disqus.com/?ref_noscript
 107. http://disqus.com/
 108. https://tryolabs.com/about/
 109. https://tryolabs.com/what-we-do/
 110. https://tryolabs.com/what-we-do/machine-learning-consulting/
 111. https://tryolabs.com/work/
 112. https://tryolabs.com/work/#products
 113. https://tryolabs.com/downloads/tryolabs-brochure-2017-11-01.pdf
 114. https://tryolabs.com/blog/
 115. https://github.com/tryolabs
 116. https://tryolabs.com/careers/
 117. mailto:hello@tryolabs.com

   hidden links:
 119. https://tryolabs.com/blog/2019/03/27/data-preparation-price-optimization-machine-learning/
 120. https://tryolabs.com/blog/2019/02/13/11-questions-to-ask-before-starting-a-successful-machine-learning-project/
 121. https://tryolabs.com/blog/2018/12/19/major-advancements-deep-learning-2018/
 122. https://twitter.com/tryolabs
 123. https://www.linkedin.com/company/tryolabs
 124. https://github.com/tryolabs
 125. https://www.facebook.com/tryolabs
 126. https://www.instagram.com/tryolabsteam
