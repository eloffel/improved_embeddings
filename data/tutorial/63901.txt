   #[1]rss feed for arg min blog

   arg min[blog]

   [2]about

reflections on random kitchen sinks

   ali rahimi and ben recht       dec 5, 2017

   ed. note: ali rahimi and i won the test of time award at nips 2017 for
   our paper    random features for large-scale kernel machines   . this post
   is the text of the acceptance speech we wrote. an addendum with some
   reflections on this talk appears in the [3]following post.

   video of the talk can be found [4]here.

   it feels great to get an award. thank you. but i have to say, nothing
   makes you feel old like an award called a    test of time   . it   s forcing
   me to accept my age. ben and i are both old now, and we   ve decided to
   name this talk accordingly.

back when we were kids

   we   re getting this award for [5]this paper. but this paper was the
   beginning of a trilogy of sorts. and like all stories worth telling,
   the good stuff happens in the middle, not at the beginning. if you   ll
   put up with my old man ways, i   d like to tell you the story of these
   papers, and take you way back to nips 2006, when ben and i were young
   spry men and dinosaurs roamed the earth.

   deep learning had just made a splash at nips 2006. the training
   algorithms were complicated, and results were competitive with linear
   models like pca and linear id166s. in a hallway conversation, people were
   speculating how they   d fare against kernel id166s. but at the time, there
   was no easy way to train kernel id166s on large datasets. ben and i had
   both been working on randomized algorithms (me for bipartite graph
   matching, ben for compressed sensing), so after we got home, it took us
   just two emails to nail down the idea for how to train large kernel
   id166s. these emails became the first paper:

   but her emails

everything about the kitchen sink

   to fit a kernel id166, you normally fit a weighted sum of radial basis
   functions to data:

   we showed how to approximate each of these basis functions in turn as a
   sum of some random functions that did not depend on the data:

   a linear combination of a linear combination is another linear
   combination, but with this new linear combination has many fewer ($d$)
   parameters:

   we showed how to approximate a variety of radial basis functions and
   gave bounds for how many random functions you need to approximate them
   each of them well.

   and the trick worked amazingly well in practice. but when it came time
   to compare it against deep nets, we couldn   t find a dataset or code
   base that we could train or compare against. machine learning was just
   beginning to become reproducible. so even though we   d started all this
   work to curb the deep net hype in its tracks, we couldn   t find anything
   to quash. instead, we compared against boosting and various accelerated
   id166 techniques, because that   s what was reproducible at the time. to
   show off how simple this algorithm was, during nips, we handed out
   these leaflets, that explained how to train kernel id166s with three
   lines of matlab code. a bit of guerilla algorithm marketing:

   post no bills

   but there was something shady in this story. according to this bound,
   which is tight, to approximate a kernel well, say 1% uniform error, you
   need tens of thousands of random features. but in all of our
   experiments, we were getting great results with a only few hundred
   random features. sometimes, random feature produced predictors that had
   better test error than the id166 we were trying to approximate! in other
   words, it wasn   t necessary to approximate kernels to get good test
   errors at all.

   there seems to be something dodgy with the random features story i   ve
   been telling you so far.

the nips rigor police

   in those days, nips had finished transitioning away from what [6]sam
   roweis used to call an    ideas conference   . around that time, we lived
   in fear of the tough questions from the nips rigor police who   d patrol
   the poster sessions: i   m talking about [7]nati srebro, [8]ofer dekel,
   or [9]michael jordan   s students, or god forbid, if you   re unlucky,
   [10]shai ben-david, or, if you   re really unlucky, [11]manfred warmuth.

   we decided to submit the paper with this bit of dodginess. but the nips
   rigor police kept us honest. we developed a solid understanding of what
   was happening. we draw a set of random functions iid, then linearly
   combine them into a predictor that minimize a training loss.

   [12]our second paper said this: in a similar way fourier functions form
   a basis for the space of l2 functions, or similar to how wide three
   layer neural nets can represent any smooth function, random sets of
   smooth functions, with very high id203, form a basis set for a
   ball of functions in l2. you don   t need to talk about random features
   as eigenfunctions for any famous rbf kernels to be sensible. random
   functions are a basis set of a hilbert space that was legitimate by
   itself. in our [13]third paper, we finally analyzed the test error of
   this algorithm when it   s trained on set of samples.

   by this third paper, we   d entirely stopped thinking in terms of
   kernels, and just fitting random basis function to data. we put on
   solid foundation the idea of linearly combining random kitchen sinks
   into a predictor. which meant that it didn   t really bother us if we
   used more features than data points.

   our original goal had been to compare deep nets against kernel id166s. we
   couldn   t do it back then. but code and benchmarks are abundant now, and
   direct comparisons are easy. for example, [14]avner may and his
   collaborators have refined the technique and achieved results
   comparable to deep nets on speech benchmarks. again, not bad for just
   four lines of matlab code.

   i sometimes use random features in my job. i like to get creative with
   special-purpose random features. it   s such an easy thing to try. when
   they work and i   m feeling good about life, i say    wow, random features
   are so powerful! they solved this problem!    or if i   m in a more somber
   mood, i say    that problem was trivial. even random features cracked
   it.    it   s the same way i think about nearest neighbors. when nearest
   neighbors cracks a dataset, you either marvel at the power of nearest
   neighbors, or you conclude your problem wasn   t hard at all. regardless,
   it   s an easy trick to try.

kids these days

   it   s now 2017. i find myself overwhelmed with the field   s progress.
   we   ve become reproducible. we share code freely and use common task
   benchmarks.

   we produce stunningly impressive results: self-driving cars seem to be
   around the corner, artificial intelligence tags faces in photos,
   transcribes voicemails, translates documents, and feeds us ads.
   billion-dollar companies are built on machine learning. in many ways,
   we   re in a better spot than we were 10 years ago. in some ways, we   re
   in a worse spot.

   there   s a self-congratulatory feeling in the air. [15]we say things
   like    machine learning is the new electricity   . i   d like to offer an
   alternative metaphor: machine learning has become alchemy.

   alchemy works

   alchemy   s ok. alchemy   s not bad. there   s a place for alchemy. alchemy
   worked.

alchemy worked

   alchemists invented metallurgy, ways to make medication, dying
   techniques for textiles, and our modern glass-making processes.

   then again, alchemists also believed they could transmute base metals
   into gold and that leeches were a fine way to cure diseases. to reach
   the sea change in our understanding of the universe that the physics
   and chemistry of the 1700s ushered in, most of the theories alchemists
   developed had to be abandoned.

   if you   re building photo sharing services, alchemy is fine. but we   re
   now building systems that govern health care and our participation in
   civil debate. i would like to live in a world whose systems are build
   on rigorous, reliable, verifiable knowledge, and not on alchemy. as
   aggravating as the nips rigor police was, i wish it would come back.

   i   ll give you an example of where this hurts us.

sgd (and variants) are all you need    because machine learning noise floor   

   i bet a lot of you have tried training a deep net of your own from
   scratch and walked away feeling bad about yourself because you couldn   t
   get it to perform.

   i don   t think it   s your fault. i think it   s id119   s fault.
   i   m going to run id119 on the simplest deep net you can
   imagine, a two layer deep net with linear activations and where the
   labels are a badly-conditioned linear function of the input.

   not all algorithms are created equal

   here, the condition number of a is $10^{20}$. id119 makes
   great progress early on, then spends the rest of the time making almost
   no progress at all. you might think this it   s hit a local minimum. it
   hasn   t. the gradients aren   t decaying to 0. you might say it   s hitting
   a statistical noise floor of the dataset. that   s not it either. i can
   compute the expectation of the loss and minimize it directly with
   id119. same thing happens. id119 just slows down
   the closer it gets to a good answer. if you   ve ever trained inception
   on id163, you   ll know that id119 gets through this regime
   in a few hours, and takes days to crawl through this regime.

   the black line is what a better descent direction would do. this is
   levenberg-marquardt.

   if you haven   t tried optimizing this problem with id119,
   please spend 10 minutes coding this up or [16]try out this jupyter
   notebook. this is the algorithm we use as our workhorse, and it fails
   on a completely benign non-contrived problem. you might say    this is a
   toy problem, id119 fits large models well.    first, everyone
   who raised their hands a minute ago would say otherwise. secondly, this
   is how we build knowledge, we apply our tools to simple problems we can
   analyze, and work our way up in complexity. we seem to have just jumped
   our way up.

   this pain is real. here   s an email that landed in my inbox two weeks
   ago from my friend boris:

   0.999 = / = 1

   this experience is not unique. there are [17]numerous [18]bugs [19]like
   [20]this out there on various forums. this happens because we run the
   wrong optimizers on loss surfaces we don   t understand. our solution is
   to add more mystery to an already mysterious scaffolding. like batch
   norm.

reducing internal covariate shift

   batch norm is a technique that speeds up id119 on deep nets.
   you sprinkle it between your layers and id119 goes faster. i
   think it   s ok to use techniques we don   t understand. i only vaguely
   understand how an airplane works, and i was fine taking one to this
   conference. but it   s always better if we build systems on top of things
   we do understand deeply? this is what we know about why batch norm
   works well. but don   t you want to understand why reducing internal
   covariate shift speeds up id119? don   t you want to see
   evidence that batch norm reduces internal covariate shift? don   t you
   want to know what internal covariate shift is? batch norm has become a
   foundational operation for machine learning. it works amazingly well.
   but we know almost nothing about it.

what   d your mother say?

   our community has a new place in society. if any of what i   ve been
   saying resonates with you, let me suggest some just two ways we can
   assume our new place responsibly.

   think about how many experiments you   ve run in the past year to crack a
   dataset for sport, or to see if a technique would give you a boost. now
   think about the experiments you ran to help you find an explanation for
   a puzzling phenomenon you observed. we do a lot of the former. we could
   use a lot more of the latter. simple experiments and simple theorems
   are the building blocks that help understand complicated larger
   phenomena.

   it seems easier to train a bi-directional lstm with attention than to
compute the svd of a large matrix   . - chris re

   for now, most of our mature large scale computational workhorses are
   variants of id119. imagine the kinds of models and
   optimization algorithms we could explore if we had commodity large
   scale linear system solvers or id105 engines. we don   t
   know how to solve this problem yet, but one worth solving. we are the
   group who can solve it.

   over the years, some of my dearest friends and strongest relationships
   have emerged from this community. my gratitude and love for this group
   are sincere, and that   s why i   m up here asking us to be rigorous, less
   alchemical. ben and i are grateful for the award, and the opportunity
   to have gotten to know many of you. and we hope that you   ll join us to
   grow machine learning beyond alchemy into electricity.

comments

   please enable javascript to view the [21]comments powered by disqus.

   theme available on [22]github.

references

   visible links
   1. http://www.argmin.net/feed.xml
   2. http://www.argmin.net/about/
   3. http://www.argmin.net/2017/12/11/alchemy-addendum/
   4. https://www.youtube.com/watch?v=qi1yry33tqe
   5. https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines
   6. https://cs.nyu.edu/~roweis/
   7. http://ttic.uchicago.edu/~nati/
   8. xxhttps://www.microsoft.com/en-us/research/people/oferd/x
   9. https://en.wikipedia.org/wiki/michael_jordan
  10. https://cs.uwaterloo.ca/~shai/
  11. https://users.soe.ucsc.edu/~manfred/
  12. https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.allerton.pdf
  13. https://cs.nyu.edu/~roweis/
  14. https://arxiv.org/abs/1701.03577
  15. https://medium.com/@synced/artificial-intelligence-is-the-new-electricity-andrew-ng-cc132ea6264
  16. https://nbviewer.jupyter.org/url/argmin.net/code/twolayerlinearnets.ipynb
  17. https://github.com/tensorflow/tensorflow/issues/2732
  18. https://github.com/tensorflow/tensorflow/issues/2226
  19. https://github.com/fchollet/keras/issues/1244
  20. https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow
  21. http://disqus.com/?ref_noscript
  22. https://github.com/johnotander/pixyll

   hidden links:
  24. http://benjamin-recht.github.io/
