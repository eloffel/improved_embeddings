   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]praemineo
   [7]praemineo
   [8]sign in[9]get started
     __________________________________________________________________

language modelling using recurrent neural networks - part 2

   [10]go to the profile of tushar pawar
   [11]tushar pawar (button) blockedunblock (button) followfollowing
   dec 12, 2017

   this is a 3 part series where i will cover
    1. [12]introduction to id56 and lstm.
    2. building a character by character language model using tensorflow.
       (this post)
    3. building a word by word language model using keras.

   in the previous part we have covered the basics of id56s and lstms. in
   this part we   ll use tensorflow to make our own language model using
   lstms. all the code has been shared on [13]github.

the training data

   huge thanks to the folks at [14]imsdb for maintaining the scripts of a
   huge number of tv shows and movies. i used [15]this code to download
   all the scripts. thanks to [16]jeremy kun on github. also, i have
   provided the scripts of my favourite tv show friends. i scraped
   [17]this site to download all the scripts. thanks to nikki at
   [18]livesinabox.

   after downloading all the scripts, i concatenated them into a single
   file which turned out to be 2.6mb. this is a small snippet of the
   training data.

   iframe: [19]/media/21b5d6aa15fea549206891eb089517b1?postid=c13f5e07b6e

the model

   the size of the model depends on various factors such as how much
   computing power do you have, how much memory you have and also the size
   of your training data. we   ll be making a model with a single layer of
   1024 lstm cells.

   iframe: [20]/media/70bcff22c07a7c7a8e8525563e597693?postid=c13f5e07b6e

   preeminence_utils is a library we have built as a collection of utility
   functions that are helpful in saving time on common tasks such as
   saving, restoring a model, model visualisation etc. you can check it
   out at our [21]github repo.

   iframe: [22]/media/b5d7dc063c3d75bcd9ced959fdd124a3?postid=c13f5e07b6e

   vocab is a variable that has all the letters that appear in the
   training data. this includes all letters, numbers and punctuation
   marks. characters2id and  characters are the dictionaries to map the
   characters in vocab to an index and index to characters respectively.
   these are helpful in converting the training data into one-hot encoding
   for training and converting the predictions back to characters for
   testing. we   re choosing section_length to be 50, so the model will take
   50 characters as an input and predict the next character that is most
   likely to come. this is because the context can be captured to an
   acceptable extent in 50 characters. a higher number can also be used
   but this comes at a cost of memory.

   from line 11   15, the corpus is divided into chunks of 50 characters
   each and is appended to the sections list. this is the input text and
   the next character after the 50 character section is its output label.
   this label is appended to section_labels list. also, notice that i   m
   skipping 10(step) characters between each chunk. this is done to save
   memory. you can use step size of 1 which will give more training data
   to the model and will perform better.

   from line 17   22, the data is one-hot encoded. it means that we are
   replacing each character by an array of zeros of size of our vocabulary
   and making the index of that character 1. the shape of our input data
   and label data is (260539, 50, 111) (260539, 111) respectively. here
   260539 is the number of samples. 111 is the size of vocabulary. the
   shape of input sequence is (50,111) and its corresponding label is
   (1,111)
   [1*bim7ojcqnzua42cekpmc9q.png]
   inputs: red and labels: green

   iframe: [23]/media/d0442a7c7b0f61022647f344bff26c14?postid=c13f5e07b6e

   we initialise a new model using the [24]tf_utils library which will
   return a model object. we get the graph for that object using
   model.init() function and set it as default graph so that we can add
   tensors to it. we also initialise some hyperparameters such as
   batch_size and hidden_nodes ie number of lstm cells.

   iframe: [25]/media/a6b9e98ceb8694efb38b66378d3031f0?postid=c13f5e07b6e

   here we define our input and labels placeholders. notice the shape of x
   is [none,section_length,vocab_length] which is (batch_size,50,111) and
   y is [none,vocab_length] which is (batch_size,111).
   [0*q6eo2lqydqbo9kwq.png]
   lstm model ([26]r2rt.com)

   here w and b are the output weights and biases that will be used to get
   a prediction from the lstm model. we define an lstm function that takes
   input data, weights and biases and returns a prediction for those
   inputs. in the image above, n is our hidden_nodes variable. the list on
   the bottom ie id56_inputs is our x placeholder and the list on the top
   ie predictions is the collection of outputs of every cell in our model.
   this corresponds to the outputs variable on line 11. the states
   variable on line 11 is the list in the middle ie id56_outputs. we only
   care about the last output of our model since it captures the context
   of the input till now. on line 12 we   ll return the final prediction by
   multiplying the last output by weights and adding the bias. checkout
   this article by [27]r2rt to get an in depth explanation of the
   structure.

   now that we have defined the structure of our model, we need to define
   some training and id74.

   iframe: [28]/media/36925422faa334b0741cf144c8257852?postid=c13f5e07b6e

   on line 1, we   ll call the lstm function to get our logits and pass it
   through a softmax function to get the predictions. we   ll be using cross
   id178 loss to compute our gradients and id119 optimizer to
   train our network. you can learn more about these in the previous
   series [29]neural networks.

   let   s train our model.

   iframe: [30]/media/b9cce99e686204e258b2e43e63a50a8f?postid=c13f5e07b6e

   call to the model.session() function will return a tensorflow session.
   if this is the first run, we   ll use line 2 to initialise all our
   variables. if we   re continuing the training from previous run,
   model.restore_weights() function will take care of detecting the latest
   weights and restoring the models with the weights. similarly,
   model.train() function will run the training epoch on the data and will
   take care of batching and printing the progress. model.save() function
   will save the current weights to the given location.

   using [31]preeminence_utils.tf_utils library is really helpful in
   reducing the hassle of restoring weights, printing training progress
   and saving weights. it cuts down the training code from 20   25 lines to
   just 6 lines. its really easy to get started and use. checkout the
   library at our [32]github repo.

   iframe: [33]/media/6238ab7fb13079cc67866a9d303f7fac?postid=c13f5e07b6e

   this function is used to get a random character from the prediction
   according to its probabilities. the output of our network is the array
   of probabilities of a character in the vocabulary being the next
   character. so this function will add a randomisation factor to these
   probabilities defined by the temperature and return new probabilities.

   it took me around 6   7 hours to train this network on aws for 310 epochs
   with step_size=1. now that we have our trained model, lets test its
   output.

   iframe: [34]/media/53a2972bcdb284fd418056ad47243945?postid=c13f5e07b6e

   on line 3, we   ll restore the weights into our model. start_index is a
   random index in the text that we   ll use as our starting point. we   ll
   feed this sentence into our model and continuously append the output to
   the input text and generate new characters. here   s what the output
   looks like throughout the training process.
   [1*hs4apox8sg_utiofpaki5w.png]
   [1*wbhvz0hh1l_50ft7ca7llq.png]
   [1*-_dpkjdmoovtdtp2zahykg.png]
   [1*-bhkwstgdl4xiebi1inyaq.png]
   [1*b6zqnojxndeb-aaehufliw.png]
   [1*6efmademiclyycng2qme1g.png]
   [1*c-twfktjh3ixwdkoanx68q.png]
   [1*f7jpymwikirqrniqlltr0q.png]
   [1*iri97us2zrnh6thd02w0kq.png]
   [1*fjstw3jkhzhmk3c52lzfra.png]
   [1*nhqbdbmpsythzhbibcpbmg.png]
   [1*y_wkdywkme7phxptl_xzig.png]
   [1*egje3z-e54kmwxzrtjtz8q.png]
   [1*e6vozr58_hwl4khkjan5da.png]
   [1*-itx3rnjejro-khkdy7ria.png]
   [1*wzd1wi5r8qmbrwpkdolzpq.png]
   [1*rvzjjezutmuo4ntabo2iaq.png]
   [1*sn6ipn03jmfg6sydcjjlbq.png]
   [1*tjttmmmwo4qsk3s0jnyxpa.png]
   [1*bk2d98yr05kpm3g_q7xxfq.png]
   [1*hfdyvqxnlhhdgjm1acuiua.png]
   [1*uixbbzwar2eg4iqevzccka.png]
   [1*9hh3bjc9y-mh1ycpt7xvrg.png]
   [1*t9v9mf6nfmdx5lcyjq2njg.png]

   as you can see, the model learned the structure of the script and
   remembered to start a new line after a couple of words instead of one
   huge line. it also learned the names of the characters and also that
   every dialogue starts with a capitalised name and then the dialogue
   from next line.

     * [35]machine learning
     * [36]artificial intelligence
     * [37]lstm
     * [38]id56

   (button)
   (button)
   (button) 25 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of tushar pawar

[40]tushar pawar

   research engineer at [41]@preeminence, doer [42]https://tusharpawar.com

     (button) follow
   [43]praemineo

[44]praemineo

   praemineo

     * (button)
       (button) 25
     * (button)
     *
     *

   [45]praemineo
   never miss a story from praemineo, when you sign up for medium.
   [46]learn more
   never miss a story from praemineo
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c13f5e07b6e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/praemineo?source=avatar-lo_dhz3oothu58c-43af8de1fd5c
   7. https://medium.com/praemineo?source=logo-lo_dhz3oothu58c---43af8de1fd5c
   8. https://medium.com/m/signin?redirect=https://medium.com/praemineo/language-modelling-using-recurrent-neural-networks-part-2-c13f5e07b6e&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/praemineo/language-modelling-using-recurrent-neural-networks-part-2-c13f5e07b6e&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@backalla?source=post_header_lockup
  11. https://medium.com/@backalla
  12. https://medium.com/preeminence/language-modeling-using-recurrent-neural-networks-part-1-427b165576c2
  13. https://github.com/preeminence/lstm-lm
  14. http://www.imsdb.com/tv/south park.html
  15. https://github.com/j2kun/imsdb_download_all_scripts
  16. https://github.com/j2kun
  17. http://www.livesinabox.com/friends/scripts.shtml
  18. http://www.livesinabox.com/
  19. https://medium.com/media/21b5d6aa15fea549206891eb089517b1?postid=c13f5e07b6e
  20. https://medium.com/media/70bcff22c07a7c7a8e8525563e597693?postid=c13f5e07b6e
  21. https://github.com/preeminence/py-utils
  22. https://medium.com/media/b5d7dc063c3d75bcd9ced959fdd124a3?postid=c13f5e07b6e
  23. https://medium.com/media/d0442a7c7b0f61022647f344bff26c14?postid=c13f5e07b6e
  24. https://github.com/preeminence/py-utils
  25. https://medium.com/media/a6b9e98ceb8694efb38b66378d3031f0?postid=c13f5e07b6e
  26. https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html
  27. https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html
  28. https://medium.com/media/36925422faa334b0741cf144c8257852?postid=c13f5e07b6e
  29. https://medium.com/preeminence/neural-networks-part-1-af2444ea4dc0
  30. https://medium.com/media/b9cce99e686204e258b2e43e63a50a8f?postid=c13f5e07b6e
  31. https://github.com/preeminence/py-utils
  32. https://github.com/preeminence/py-utils
  33. https://medium.com/media/6238ab7fb13079cc67866a9d303f7fac?postid=c13f5e07b6e
  34. https://medium.com/media/53a2972bcdb284fd418056ad47243945?postid=c13f5e07b6e
  35. https://medium.com/tag/machine-learning?source=post
  36. https://medium.com/tag/artificial-intelligence?source=post
  37. https://medium.com/tag/lstm?source=post
  38. https://medium.com/tag/id56?source=post
  39. https://medium.com/@backalla?source=footer_card
  40. https://medium.com/@backalla
  41. http://twitter.com/preeminence
  42. https://tusharpawar.com/
  43. https://medium.com/praemineo?source=footer_card
  44. https://medium.com/praemineo?source=footer_card
  45. https://medium.com/praemineo
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/p/c13f5e07b6e/share/twitter
  49. https://medium.com/p/c13f5e07b6e/share/facebook
  50. https://medium.com/p/c13f5e07b6e/share/twitter
  51. https://medium.com/p/c13f5e07b6e/share/facebook
