   [1]

linkedin

     * [2]sign in
     * [3]join now

   is learning rate useful in id158s?

   updating weights of a simple id158

             is learning rate useful in id158s?

   published on december 16, 2017december 16, 2017     85 likes     13
   comments

   [4]ahmed gad

[5]ahmed gad[6]follow

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

     * (button) like85
     * (button) comment13
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       13

   this article will help you understand why we need the learning rate and
   whether it is useful or not for training an id158.
   using a very simple python code for a single layer id88, the
   learning rate value will get changed to catch its idea.

   introduction

   an obstacle for newbies in id158s is the learning
   rate. i was asked many times about the effect of the learning rate in
   the training of the id158s (anns). why we use
   learning rate? what is the best value for the learning rate? in this
   article, i will try to make things simpler by providing an example that
   shows how learning rate is useful in order to train an ann. i will
   start by explaining our example with python code before working with
   the learning rate.

   example

   a very very simple example is used to get us out of complexity and
   allow us to just focus on the learning rate. a single numerical input
   will get applied to a single layer id88. if the input is 250 or
   smaller, its value will get returned as the output of the network. if
   the input is larger than 250, then it will be clipped to just 250. the
   following table shows the 6 samples used for training.

   [:0]

   ann architecture

   the architecture of the ann used is shown in the next figure. there are
   just input and output layers. the input layer has just a single neuron
   for our single input. the output layer has just a single neuron for
   generating the output. the output layer neuron is responsible for
   mapping the input to the correct output. there is also a bias applied
   to the output layer neuron with weight b and value +1. there is also a
   weight w for the input.

   [:0]

   activation function

   the equation and the graph of the activation function used in this
   example are as shown in the next figure. when the input is below or
   equal to 250, the output will be the same as the input. otherwise, it
   will be clipped to 250.

   [:0]

   implementation using python

   the python code implementing the entire network is shown below. we will
   discuss all of it until making it easy as much as possible then focus
   on changing the learning rate to find out how it affects the network
   training.
1.  import numpy

2.

3.  def activation_function(inpt):

4.      if(inpt > 250):

5.          return 250 # clip the result to 250

6.      else:

7.          return inpt # just return the input

8.

9.  def prediction_error(desired, expected):

10.     return numpy.abs(numpy.mean(desired-expected)) # absolute error

11.

12. def update_weights(weights, predicted, idx):

13.     weights = weights + .00001*(desired_output[idx] - predicted)*inputs[idx]
 # updating weights

14.     return weights # new updated weights

15.

16. weights = numpy.array([0.05, .1]) #bias & weight of input

17. inputs = numpy.array([60, 40, 100, 300, -50, 310]) # training inputs

18. desired_output = numpy.array([60, 40, 150, 250, -50, 250]) # training output
s

19.

20. def training_loop(inpt, weights):

21.     error = 1

22.     idx = 0 # start by the first training sample

23.     iteration = 0 #loop iteration variable

24.     while(iteration < 2000 or error >= 0.01): #while(error >= 0.1):

25.         predicted = activation_function(weights[0]*1+weights[1]*inputs[idx])


26.         error = prediction_error(desired_output[idx], predicted)

27.         weights = update_weights(weights, predicted, idx)

28.         idx = idx + 1 # go to the next sample

29.         idx = idx % inputs.shape[0] # restricts the index to the range of ou
r samples

30.         iteration = iteration + 1 # next iteration

31.     return error, weights

32.

33. error, new_weights = training_loop(inputs, weights)

34. print('--------------final results----------------')

35. print('learned weights : ', new_weights)

36. new_inputs = numpy.array([10, 240, 550, -160])

37. new_outputs = numpy.array([10, 240, 250, -160])

38. for i in range(new_inputs.shape[0]):

39.     print('sample ', i+1, '. expected = ', new_outputs[i], ' , predicted = '
, activation_function(new_weights[0]*1+new_weights[1]*new_inputs[i]))


   lines 17 and 18 are responsible for creating two arrays (inputs and
   desired_output) holding the training input and output data presented in
   the previous table. each input will have an output according to the
   activation function used.

   line 16 creates an array of the network weights. there are just two
   weights: one for the input and another for the bias. they were randomly
   initialized to 0.05 for the bias and 0.1 for the input.

   the activation function itself is implemented using the
   activation_function(inpt) method from line 3 to 7. it accepts a single
   argument which is the input and returns a single value which is the
   expected output.

   because there may be an error in the prediction, we need to measure
   that error to know how far we are from the correct prediction. for that
   reason, there is a method implemented from line 9 to 10 called
   prediction_error(desired, expected) that accepts two inputs: the
   desired and expected outputs. that method just calculates the absolute
   difference between each desired and expected outputs. the best value
   for any error is for sure 0. this is the optimal value.

   what if there was a prediction error? in this case, we must make a
   change to the network. but what exactly to change? it is the network
   weights. for updating the network weights, there is a method called
   update_weights(weights, predicted, idx) defined from line 13 to 14. it
   accepts three inputs: old weights, predicted output, and the index of
   the input that has a false prediction. this method applies the
   following equation to update the weights:

   [:0]

   the equation uses the weights of the current step (n) to generate the
   weights of the next step (n+1). this equation is what we will use for
   knowing how the learning rate affects the learning process.

   finally, we need to concatenate all of these together to make the
   network learn. this is done using the training_loop(inpt, weights)
   method defined from line 20 to 31. it goes into a training loop. the
   loop is used to map the inputs to their outputs with the least possible
   prediction error.

   the loop does three operations:

   1.      output prediction.

   2.      error calculation.

   3.      updating weights.

   after getting the idea of the example and its python code, let us start
   showing how the learning rate is useful in order to get the best
   results.

   learning rate

   in the previously discussed example, line 13 has the weights update
   equation in which the learning rate is used. at first, let us assume
   that we have not used the learning rate completely. the equation will
   as follows:
weights = weights + (desired_output[idx] - predicted)*inputs[idx]

   let us see the effect of removing the learning rate. in the iteration
   of the training loop, the network has the following inputs (b=0.05 and
   w=0.1, input = 60, and desired output=60).

   the expected output which is the result of the activation function as
   in line 25 will be activation_function(0.05(+1) + 0.1(60)). the
   predicted output will be 6.05.

   in line 26, the prediction error will be calculated by getting the
   difference between the desired and the predicted output. the error will
   be abs(60-6.05)=53.95.

   then in line 27 the weights will get updated according to the above
   equation. the new weights will be [0.05, 0.1] + (53.95)*60 = [0.05,
   0.1] + 3237 = [3237.05, 3237.1].

   it seems that the new weights are too different from the previous
   weights. each weight got increased by 3,237 which is too large. but let
   us continue making the next prediction.

   in the next iteration, the network will have these inputs applied:
   (b=3237.05 and w=3237.1, input = 40, and desired output=40). the
   expected output will be activation_function((3237.05 + 3237.1(40)) =
   250. the prediction error will be abs(40 - 250) = 210. the error is
   very large. it is larger than the previous error. thus we have to
   update the weights again. according to the above equation, the new
   weights will be [3237.05, 3237.1] + (-210)*40 = [3237.05, 3237.1] +
   -8400 = [-5162.95, -5162.9].

   the next table summarizes the results of the first three iterations:

   [:0]

   as we go into more iterations, the results get worse. the magnitude of
   the weights is changing rapidly and sometimes with changing its signs.
   they are moving from very large positive value to very large negative
   value. how can we stop this large and abrupt changes in the weights?
   how to scale down the value by which the weights are updated?

   if we looked at the value by which the weights are changing by from the
   previous table, it seems that the value is very large. this means that
   the network changes its weights with large speed. it is like someone
   that makes large moves within small times. at one time, the person is
   in the far east and after a very short time, that person will be in the
   far west. we just need to make it slower.

   if we are able to scale down this value to get smaller then everything
   will be alright. but how?

   getting back to the part of the code that generates this value, it
   looks that the update equation is what generates it. specifically this
   part:
(desired_output[idx] - predicted)*inputs[idx]

   we can scale this part by multiplying it by a small value such as 0.1.
   so, rather than generating 3237.0 as the updated value in the first
   iteration, it will be reduced to just 323.7. we can even scale this
   value to a smaller value by decreasing the scale value to say 0.001.
   using 0.001, the value will be just 3.327.

   we can catch it now. this scaling value is the learning rate. choosing
   small values for the learning rate makes the rate of weights update
   smaller and avoids abrupt changes. as the value gets larger as the
   changes are faster and as a result bad results.

   but what is the best value for the learning rate?

   there is no value we can say it is the best value for the learning
   rate. the learning rate is a hyperparameter. a hyperparameter has its
   value determined by experiments. we try different values and use the
   value that gives best results. there are some ways that just helps you
   select values of hyperparameters.

   testing network

   for our problem, i deduced that a value of .00001 works fine. after
   training the network with that learning rate, we can make a test. the
   following table shows the results of prediction of 4 new testing
   samples. it seems that results are now much better after using the
   learning rate.

   [:0]

   [7]ahmed gad

[8]ahmed gad

job seeker. fritz/kdnuggets/tds contributor, t.a. & deep learning | machine
learning | id161 researcher

   [9]follow

   13 comments
   article-comment__guest-image
   [10]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from ahmed gad

   [11]27 articles
   from y=x to building a complete id158

[12]from y=x to building a complete artificial   

   march 29, 2019

   feature reduction using genetic algorithm with python

[13]feature reduction using genetic algorithm   

   january 29, 2019

   id158s optimization using genetic algorithm with
   python

[14]id158s optimization   

   january 24, 2019

     *    2019
     * [15]about
     * [16]user agreement
     * [17]privacy policy
     * [18]cookie policy
     * [19]copyright policy
     * [20]brand policy
     * [21]manage subscription
     * [22]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   5. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad&trk=author-info__follow-button
   7. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_image
   8. https://eg.linkedin.com/in/ahmedfgad?trk=author_mini-profile_title
   9. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad&trk=author-info__follow-button-bottom
  10. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad&trk=article-reader_leave-comment
  11. https://www.linkedin.com/today/author/ahmedfgad
  12. https://www.linkedin.com/pulse/from-yx-building-complete-artificial-neural-network-ahmed-gad?trk=related_artice_from y=x to building a complete id158_article-card_title
  13. https://www.linkedin.com/pulse/feature-reduction-using-genetic-algorithm-ahmed-gad?trk=related_artice_feature reduction using genetic algorithm with python_article-card_title
  14. https://www.linkedin.com/pulse/artificial-neural-networks-optimization-using-genetic-ahmed-gad?trk=related_artice_id158s optimization using genetic algorithm with python_article-card_title
  15. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  16. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  17. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  18. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  19. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  20. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  21. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  22. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
