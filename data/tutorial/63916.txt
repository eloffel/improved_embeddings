   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*iukc2ifbomxl5dj3krajrw.png]
   [16]https://pixabay.com

understanding gru networks

   [17]go to the profile of simeon kostadinov
   [18]simeon kostadinov (button) blockedunblock (button) followfollowing
   dec 16, 2017

   in this article, i will try to give a fairly simple and understandable
   explanation of one really fascinating type of neural network.
   introduced by [19]cho, et al. in 2014, gru (gated recurrent unit) aims
   to solve the vanishing gradient problem which comes with a standard
   recurrent neural network. gru can also be considered as a variation on
   the lstm because both are designed similarly and, in some cases,
   produce equally excellent results. if you are not familiar with
   recurrent neural networks, i recommend reading [20]my brief
   introduction. for better understanding of lstm, many people recommend
   [21]christopher olah   s article. i would also add [22]this paper which
   gives a clear distinction between gru and lstm.

how grus work?

   as mentioned above, grus are improved version of standard recurrent
   neural network. but what makes them so special and effective?

   to solve the vanishing gradient problem of a standard id56, gru uses, so
   called, update gate and reset gate. basically, these are two vectors
   which decide what information should be passed to the output. the
   special thing about them is that they can be trained to keep
   information from long ago, without washing it through time or remove
   information which is irrelevant to the prediction.

   to explain the mathematics behind that process we will examine a single
   unit from the following recurrent neural network:
   [1*7oe-4wg6bz7u8ydf5cjjpa.png]
   recurrent neural network with gated recurrent unit

   here is a more detailed version of the that single gru:
   [1*6entqlzq08aabo-stfnibw.png]
   gated recurrent unit

   first, let   s introduce the notations:
   [1*qx5uusvgl_qcvsj_ym2pma.png]

   if you are not familiar with the above terminology, i recommend
   watching these tutorials about [23]   sigmoid    and    tanh    function and
   [24]   hadamard product    operation.

#1. update gate

   we start with calculating the update gate z_t for time step t using the
   formula:
   [1*o7nzuf8w0h7qybg8fn-shw.png]

   when x_t is plugged into the network unit, it is multiplied by its own
   weight w(z). the same goes for h_(t-1) which holds the information for
   the previous t-1 units and is multiplied by its own weight u(z). both
   results are added together and a sigmoid activation function is applied
   to squash the result between 0 and 1. following the above schema, we
   have:
   [1*gslr_jlneuzbscakyjmada.png]

   the update gate helps the model to determine how much of the past
   information (from previous time steps) needs to be passed along to the
   future. that is really powerful because the model can decide to copy
   all the information from the past and eliminate the risk of vanishing
   gradient problem. we will see the usage of the update gate later on.
   for now remember the formula for z_t.

#2. reset gate

   essentially, this gate is used from the model to decide how much of the
   past information to forget. to calculate it, we use:
   [1*j1j1mliytm97hcay4grc_q.png]

   this formula is the same as the one for the update gate. the difference
   comes in the weights and the gate   s usage, which will see in a bit. the
   schema below shows where the reset gate is:
   [1*5m6lyj544ukkhkfkdmdq8a.png]

   as before, we plug in h_(t-1)         blue line and x_t         purple line,
   multiply them with their corresponding weights, sum the results and
   apply the sigmoid function.

#3. current memory content

   let   s see how exactly the gates will affect the final output. first, we
   start with the usage of the reset gate. we introduce a new memory
   content which will use the reset gate to store the relevant information
   from the past. it is calculated as follows:
   [1*cxqbmqy8dvgjnjejcur6pq.png]
    1. multiply the input x_t with a weight w and h_(t-1) with a weight u.
    2. calculate the hadamard (element-wise) product between the reset
       gate r_t and uh_(t-1). that will determine what to remove from the
       previous time steps. let   s say we have a id31 problem
       for determining one   s opinion about a book from a review he wrote.
       the text starts with    this is a fantasy book which illustrates      
       and after a couple paragraphs ends with    i didn   t quite enjoy the
       book because i think it captures too many details.    to determine
       the overall level of satisfaction from the book we only need the
       last part of the review. in that case as the neural network
       approaches to the end of the text it will learn to assign r_t
       vector close to 0, washing out the past and focusing only on the
       last sentences.
    3. sum up the results of step 1 and 2.
    4. apply the nonlinear activation function tanh.

   you can clearly see the steps here:
   [1*azobvz2gxsdykj2iv28maq.png]

   we do an element-wise multiplication of h_(t-1)         blue line and
   r_t         orange line and then sum the result         pink line with the input
   x_t         purple line. finally, tanh is used to produce h   _t         bright green
   line.

#4. final memory at current time step

   as a last step, the network needs to calculate h_t         vector which holds
   information for the current unit and passes it down to the network. in
   order to do that the update gate is needed. it determines what to
   collect from the current memory content         h   _t and what from the
   previous steps         h_(t-1). that is done as follows:
   [1*zxstnqedwlroicghkyksvq.png]
    1. apply element-wise multiplication to the update gate z_t and
       h_(t-1).
    2. apply element-wise multiplication to (1-z_t) and h   _t.
    3. sum the results from step 1 and 2.

   let   s bring up the example about the book review. this time, the most
   relevant information is positioned in the beginning of the text. the
   model can learn to set the vector z_t close to 1 and keep a majority of
   the previous information. since z_t will be close to 1 at this time
   step, 1-z_t will be close to 0 which will ignore big portion of the
   current content (in this case the last part of the review which
   explains the book plot) which is irrelevant for our prediction.

   here is an illustration which emphasises on the above equation:
   [1*uxz0ptqw8kofl9bzpvyv1w.png]

   following through, you can see how z_t         green line is used to
   calculate 1-z_t which, combined with h   _t         bright green line, produces
   a result in the dark red line. z_t is also used with h_(t-1)         blue
   line in an element-wise multiplication. finally, h_t         blue line is a
   result of the summation of the outputs corresponding to the bright and
   dark red lines.
     __________________________________________________________________

   now, you can see how grus are able to store and filter information
   using their update and reset gates. that eliminates the vanishing
   gradient problem since the model is not washing out the new input every
   single time but keeps the relevant information and passes it down to
   the next time steps of the network. if carefully trained, they can
   perform extremely well even in complex scenarios.

   i hope the article is leaving you armed with a better understanding of
   this state-of-the-art deep learning model called gru.

thank you for the reading. if you enjoyed the article, give it some claps      .
hope you have a great day!

     * [25]machine learning
     * [26]recurrent neural network
     * [27]artificial intelligence
     * [28]neural networks
     * [29]lstm

   (button)
   (button)
   (button) 3k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of simeon kostadinov

[31]simeon kostadinov

   software engineer @ speechify. lives in san francisco. compsci student
   @ university of birmingham. [32]https://www.linkedin.com/in/simonnoff/

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2ef37df6c9be
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_7wmlctwpmnht---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://pixabay.com/
  17. https://towardsdatascience.com/@simonnoff?source=post_header_lockup
  18. https://towardsdatascience.com/@simonnoff
  19. https://arxiv.org/pdf/1406.1078v3.pdf
  20. https://medium.com/@simeon.kostadinoff/learn-how-recurrent-neural-networks-work-84e975feaaf7
  21. http://colah.github.io/posts/2015-08-understanding-lstms/
  22. https://arxiv.org/pdf/1412.3555v1.pdf
  23. https://www.youtube.com/watch?v=9vb5nzrl4hy
  24. https://www.youtube.com/watch?v=2gpzlrvhqwy
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/tagged/recurrent-neural-network?source=post
  27. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  28. https://towardsdatascience.com/tagged/neural-networks?source=post
  29. https://towardsdatascience.com/tagged/lstm?source=post
  30. https://towardsdatascience.com/@simonnoff?source=footer_card
  31. https://towardsdatascience.com/@simonnoff
  32. https://www.linkedin.com/in/simonnoff/
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/2ef37df6c9be/share/twitter
  39. https://medium.com/p/2ef37df6c9be/share/facebook
  40. https://medium.com/p/2ef37df6c9be/share/twitter
  41. https://medium.com/p/2ef37df6c9be/share/facebook
