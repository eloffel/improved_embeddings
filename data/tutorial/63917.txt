   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

gradient boosting from scratch

   [11]go to the profile of prince grover
   [12]prince grover (button) blockedunblock (button) followfollowing
   dec 8, 2017

   simplifying a complex algorithm

motivation

   although most of the [13]kaggle competition winners use stack/ensemble
   of various models, one particular model that is part of most of the
   ensembles is some variant of gradient boosting (gbm) algorithm. take
   for an example the winner of latest kaggle competition: [14]michael
   jahrer   s solution with representation learning in [15]safe driver
   prediction. his solution was a blend of 6 models. 1 [16]lightgbm (a
   variant of gbm) and 5 neural nets. although his success is attributed
   to the semi-supervised learning that he used for the structured data,
   but gradient boosting model has done the useful part too.

   even though gbm is being used widely, many practitioners still treat it
   as complex black-box algorithm and just run the models using pre-built
   libraries. the purpose of this post is to simplify a supposedly complex
   algorithm and to help the reader to understand the algorithm
   intuitively. i am going to explain the pure vanilla version of the
   gradient boosting algorithm and will share links for its different
   variants at the end. i have taken base decisiontree code from
   [17]fast.ai library
   (fastai/courses/ml1/[18]lesson3-rf_foundations.ipynb) and on top of
   that, i have built my own simple version of basic gradient boosting
   model.

brief description for ensemble, id112 and boosting

   when we try to predict the target variable using any machine learning
   technique, the main causes of difference in actual and predicted values
   are noise, variance, and bias. ensemble helps to reduce these factors
   (except noise, which is irreducible error)

   an ensemble is just a collection of predictors which come together
   (e.g. mean of all predictions) to give a final prediction. the reason
   we use ensembles is that many different predictors trying to predict
   same target variable will perform a better job than any single
   predictor alone. ensembling techniques are further classified into
   id112 and boosting.
     * id112 is a simple ensembling technique in which we build many
       independent predictors/models/learners and combine them using some
       model averaging techniques. (e.g. weighted average, majority vote
       or normal average)

   we typically take random sub-sample/bootstrap of data for each model,
   so that all the models are little different from each other. each
   observation is chosen with replacement to be used as input for each of
   the model. so, each model will have different observations based on the
   bootstrap process. because this technique takes many uncorrelated
   learners to make a final model, it reduces error by reducing variance.
   example of id112 ensemble is id79 models.
     * boosting is an ensemble technique in which the predictors are not
       made independently, but sequentially.

   this technique employs the logic in which the subsequent predictors
   learn from the mistakes of the previous predictors. therefore, the
   observations have an unequal id203 of appearing in subsequent
   models and ones with the highest error appear most. (so the
   observations are not chosen based on the bootstrap process, but based
   on the error). the predictors can be chosen from a range of models like
   id90, regressors, classifiers etc. because new predictors are
   learning from mistakes committed by previous predictors, it takes less
   time/iterations to reach close to actual predictions. but we have to
   choose the stopping criteria carefully or it could lead to overfitting
   on training data. gradient boosting is an example of boosting
   algorithm.
   [1*8t4hejzhto_v8preflkd9a.png]
   fig 1. ensembling
   [1*paxj8hcye9r2mgiz32tq2a.png]
   fig 2. id112 (independent models) & boosting (sequential models).
   reference:
   [19]https://quantdare.com/what-is-the-difference-between-id112-and-bo
   osting/

gradient boosting algorithm

     gradient boosting is a machine learning technique for regression and
     classification problems, which produces a prediction model in the
     form of an ensemble of weak prediction models, typically decision
     trees. (wikipedia definition)

   the objective of any supervised learning algorithm is to define a loss
   function and minimize it. let   s see how maths work out for gradient
   boosting algorithm. say we have mean squared error (mse) as loss
   defined as:
   [1*fhenn7nvqcwvw25d3-zriq.png]

   we want our predictions, such that our id168 (mse) is minimum.
   by using id119 and updating our predictions based on a
   learning rate, we can find the values where mse is minimum.
   [1*llbc4tstqzxq3hza8wcmeg.png]

   so, we are basically updating the predictions such that the sum of our
   residuals is close to 0 (or minimum) and predicted values are
   sufficiently close to actual values.

intuition behind gradient boosting

   the logic behind gradient boosting is simple, (can be understood
   intuitively, without using mathematical notation). i expect that
   whoever is reading this post might be familiar with [20]simple linear
   regression modeling.

   a basic assumption of id75 is that sum of its residuals is
   0, i.e. the residuals should be spread randomly around zero.
   [1*mbstjwvk-ylvpvgyjw-1da.png]
   fig 3. sample random normally distributed residuals with mean around 0

   now think of these residuals as mistakes committed by our predictor
   model. although, tree-based models (considering decision tree as base
   models for our gradient boosting here) are not based on such
   assumptions, but if we think logically (not statistically) about this
   assumption, we might argue that, if we are able to see some pattern of
   residuals around 0, we can leverage that pattern to fit a model.

   so, the intuition behind gradient boosting algorithm is to repetitively
   leverage the patterns in residuals and strengthen a model with weak
   predictions and make it better. once we reach a stage that residuals do
   not have any pattern that could be modeled, we can stop modeling
   residuals (otherwise it might lead to overfitting). algorithmically, we
   are minimizing our id168, such that test loss reach its minima.

     in summary,
         we first model data with simple models and analyze data for
     errors.
         these errors signify data points that are difficult to fit by a
     simple model.
         then for later models, we particularly focus on those hard to fit
     data to get them right.
         in the end, we combine all the predictors by giving some weights
     to each predictor.

   a more technical quotation of the same logic is written in [21]probably
   approximately correct: nature   s algorithms for learning and prospering
   in a complex world,

      the idea is to use the weak learning method several times to get a
   succession of hypotheses, each one refocused on the examples that the
   previous ones found difficult and misclassified.     note, however, it is
   not obvious at all how this can be done   

steps to fit a gradient boosting model

   let   s consider simulated data as shown in scatter plot below with 1
   input (x) and 1 output (y) variables.
   [1*7epvsm_80fyh-29g8q6yrq.png]
   fig 4. simulated data (x: input, y: output)

   data for above shown plot is generated using below python code:

   iframe: [22]/media/8600b093f59a4c9a2f7363156df3e4a6?postid=1e317ae4587d

   code chunk 1. data simulation

     1. fit a simple linear regressor or decision tree on data (i have
     chosen decision tree in my code) [call x as input and y as output]

   iframe: [23]/media/fec010ff500612b58a18cfea96457264?postid=1e317ae4587d

   code chunk 2. (step 1)using decision tree to find best split (here
   depth of our tree is 1)

     2. calculate error residuals. actual target value, minus predicted
     target value [e1= y - y_predicted1 ]

     3. fit a new model on error residuals as target variable with same
     input variables [call it e1_predicted]

     4. add the predicted residuals to the previous predictions
     [y_predicted2 = y_predicted1 + e1_predicted]

     5. fit another model on residuals that is still left. i.e. [e2 = y -
     y_predicted2] and repeat steps 2 to 5 until it starts overfitting or
     the sum of residuals become constant. overfitting can be controlled
     by consistently checking accuracy on validation data.

   iframe: [24]/media/12c8129e2e9309b6b896f511bbd4a71d?postid=1e317ae4587d

   code chunk 3. (steps 2 to 5) calculate residuals and update new target
   variable and new predictions

   to aid the understanding of the underlying concepts, here is the link
   with complete implementation of a simple gradient boosting model from
   scratch. [25][link: gradient boosting from scratch]

   shared code is a non-optimized vanilla implementation of gradient
   boosting. most of the gradient boosting models available in libraries
   are well optimized and have many hyper-parameters.

visualization of working gradient boosting tree

   blue dots (left) plots are input (x) vs. output (y)     red line (left)
   shows values predicted by decision tree     green dots (right) shows
   residuals vs. input (x) for ith iteration     iteration represent
   sequential order of fitting gradient boosting tree
   [1*2fgb3jtf85xyhtnpjya8ug.png]
   fig 5. visualization of gradient boosting predictions (first 4
   iterations)
   [1*ram0yhpcwxwz23hzun1qwa.png]
   fig 6. visualization of gradient boosting predictions (18th to 20th
   iterations)

   we observe that after 20th iteration , residuals are randomly
   distributed (i am not saying random normal here) around 0 and our
   predictions are very close to true values. (iterations are called
   n_estimators in sklearn implementation). this would be a good point to
   stop or our model will start overfitting.

   let   s see how our model look like for 50th iteration.
   [1*tnyxuuu23kcoiww26uh6jw.png]
   fig 7. visualization of gradient boosting prediction (iteration 50th)

   we see that even after 50th iteration, residuals vs. x plot look
   similar to what we see at 20th iteration. but the model is becoming
   more complex and predictions are overfitting on the training data and
   are trying to learn each training data. so, it would have been better
   to stop at 20th iteration.

   python code snippet used for plotting all the above figures.

   iframe: [26]/media/f5bca4f4459ab7fcfbf4c150c40cc315?postid=1e317ae4587d

   code chunk 4. plotting predictions and residuals (fed in 1st code
   chunk   s loop)
     __________________________________________________________________

   i hope that this blog helped you to get basic intuition behind how
   gradient boosting works. to understand gradient boosting for regression
   in detail, i would strongly recommend you to read this amazing article
   by the faculty at university of san francisco, terence parr (creator of
   the [27]antlr parser generator) and jeremy howard (founding researcher
   at [28]fast.ai) : [29]how to explain gradient boosting.

   more useful resources
    1. my github repo and kaggle kernel link for gbm from scratch:
       [30]https://www.kaggle.com/grroverpr/gradient-boosting-simplified/
       [31]https://nbviewer.jupyter.org/github/groverpr/machine-learning/b
       lob/master/notebooks/01_gradient_boosting_scratch.ipynb
    2. a detailed and intuitive explanation of gradient boosting: [32]how
       to explain gradient boosting by terence parr and jeremy howard
    3. fast.ai github repo link for decisiontree from scratch (massive
       ml/dl related resources):
       [33]https://github.com/fastai/fastai
    4. video by [34]alexander ihler. this video really helped me to build
       my understanding.

   iframe: [35]/media/a5db75e69630fe7ebe8716bd8c4cbd1a?postid=1e317ae4587d

   4. a kaggle master explains gradient boosting: ben gorman
   [36]a kaggle master explains gradient boosting
   if id75 was a toyota camry, then gradient boosting would
   be a uh-60 blackhawk helicopter. a particular   blog.kaggle.com

   5. widely used gbm algorithms:
   [37]xgboost || [38]lightgbm || [39]catboost ||
   [40]sklearn.ensemble.gradientboostingclassifier

     * [41]machine learning
     * [42]gradient boosting
     * [43]artificial intelligence

   (button)
   (button)
   (button) 7.3k claps
   (button) (button) (button) 23 (button) (button)

     (button) blockedunblock (button) followfollowing
   [44]go to the profile of prince grover

[45]prince grover

   machine learning engineer [46]@manifold.ai, usf-msds and iit-roorkee
   alumnus (twitter: [47]@groverpr4)

     (button) follow
   [48]ml review

[49]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 7.3k
     * (button)
     *
     *

   [50]ml review
   never miss a story from ml review, when you sign up for medium.
   [51]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1e317ae4587d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_zfyaseteusp2---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@pgrover3?source=post_header_lockup
  12. https://medium.com/@pgrover3
  13. https://www.kaggle.com/datasets
  14. https://www.kaggle.com/mjahrer
  15. https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927
  16. https://github.com/microsoft/lightgbm
  17. https://github.com/fastai/fastai
  18. http://localhost:8888/notebooks/fastai/courses/ml1/lesson3-rf_foundations.ipynb
  19. https://quantdare.com/what-is-the-difference-between-id112-and-boosting/
  20. https://en.wikipedia.org/wiki/simple_linear_regression
  21. http://www.amazon.com/dp/0465060722?tag=inspiredalgor-20
  22. https://medium.com/media/8600b093f59a4c9a2f7363156df3e4a6?postid=1e317ae4587d
  23. https://medium.com/media/fec010ff500612b58a18cfea96457264?postid=1e317ae4587d
  24. https://medium.com/media/12c8129e2e9309b6b896f511bbd4a71d?postid=1e317ae4587d
  25. https://www.kaggle.com/grroverpr/gradient-boosting-simplified/
  26. https://medium.com/media/f5bca4f4459ab7fcfbf4c150c40cc315?postid=1e317ae4587d
  27. http://www.antlr.org/
  28. http://www.fast.ai/
  29. http://explained.ai/gradient-boosting/index.html
  30. https://www.kaggle.com/grroverpr/gradient-boosting-simplified/
  31. https://nbviewer.jupyter.org/github/groverpr/machine-learning/blob/master/notebooks/01_gradient_boosting_scratch.ipynb
  32. http://explained.ai/gradient-boosting/index.html
  33. https://github.com/fastai/fastai
  34. https://www.youtube.com/watch?v=srktkszfmsk&t=311s
  35. https://medium.com/media/a5db75e69630fe7ebe8716bd8c4cbd1a?postid=1e317ae4587d
  36. http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/
  37. https://github.com/dmlc/xgboost
  38. https://github.com/microsoft/lightgbm
  39. https://github.com/catboost/catboost
  40. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.gradientboostingclassifier.html
  41. https://medium.com/tag/machine-learning?source=post
  42. https://medium.com/tag/gradient-boosting?source=post
  43. https://medium.com/tag/artificial-intelligence?source=post
  44. https://medium.com/@pgrover3?source=footer_card
  45. https://medium.com/@pgrover3
  46. http://twitter.com/manifold
  47. http://twitter.com/groverpr4
  48. https://medium.com/mlreview?source=footer_card
  49. https://medium.com/mlreview?source=footer_card
  50. https://medium.com/mlreview
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/
  54. https://medium.com/p/1e317ae4587d/share/twitter
  55. https://medium.com/p/1e317ae4587d/share/facebook
  56. https://medium.com/p/1e317ae4587d/share/twitter
  57. https://medium.com/p/1e317ae4587d/share/facebook
