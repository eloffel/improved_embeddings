   #[1]vincent granville's posts - data science central [2]comments -
   comparison of deepnet & neuralnet - data science central

   iframe: [3]https://www.googletagmanager.com/ns.html?id=gtm-t5w4wq

   ____________________ [4]search

     * [5]sign up
     * [6]sign in

[7]data science central

     * [8]home
          + [9]top content
          + [10]editorial guidelines
          + [11]user agreement
          + [12]cookie policy
     * [13]ai
     * [14]ml
     * [15]dl
     * [16]analytics
     * [17]statistics
     * [18]big data
     * [19]dataviz
     * [20]hadoop
     * [21]podcasts
     * [22]webinars
     * [23]forums
     * [24]jobs
     * [25]membership
          + [26]top content
          + [27]archives
     * [28]groups
     * [29]search
     * [30]contact

                       [31]subscribe to dsc newsletter

     * [32]all blog posts
     * [33]my blog
     * [34]add

comparison of deepnet & neuralnet

     * posted by [35]vincent granville on december 15, 2017 at 8:00am
     * [36]view blog

   guest blog post by [37]blaine batman. based on two r packages for
   neural networks.

   in this article, i compare two available r packages for using neural
   networks to model data: neuralnet and deepnet. through the comparisons
   i highlight various challenges in finding good hyperparameter values. i
   show that some needed hyperparameters differ when using these two
   packages, even with the same underlying algorithmic approach. neuralnet
   was developed by stefan fritsch and frauke guenther with contributors
   marc suling and sebastian m. mueller. deepnet was created by xiao
   rong. both packages can be obtained via the r cran repository (see
   links at the end). i will focus on a simple time series example,
   composed of two predictors and the performance of the packages to
   predict future data after being trained on past data using a simple
   5-neuron network. note that most of what you read about in deep
   learning with neural networks are    classification    problems (more
   later); nonetheless such networks have promise for predicting
   continuous data including time series.

   brief overview of simple neural networks

   briefly, a neural network (also called a multilayer-id88 etc.) is
   a connected network of neurons as shown here.
   [aaiaaqdgaagaaqaaaaaaaaynaaaajgqznwnhmzmxlwe5zgitngjlzi1hnzuzltezntdhnt
   y0ytvlma.jpg]

   figure 1. an example neural network (generated using neuralnet).

   note that except for the input layer (where the predictor values are
   fed in), the inputs to a neuron have weights specific to that neuron,
   so the output of a neuron is    re-used    as input to all neurons in the
   next layer, with unique weights. before moving on to a brief
   description of how neural networks compute predictions, it is worth
   reflecting on the number of independent parameters in neural network
   models as compared to, for example, id75.

   if we applied id75 to the problem of figure 1, the model
   would have the form:

              m0 + m1*x1 + m2*x2 + m3*x3 + m4*x4 + m5*x5 = prediction

   where m0 is the constant or bias term, and the mi are the coefficients
   for the input values. the id75 algorithm is used to
   determine the values of m that provide the best prediction, a total of
   six parameters. id75 models sometimes can be solved with a
   closed form method called the normal equation, as well as using linear
   algebraic approaches. most machine learning languages have very
   optimized packages for these types of problems, so id75 is
   generally very fast and computationally efficient.

   looking back at figure 1, for the same 5 input model, with two layers
   each having 3 hidden neurons and one bias node, and the output layer
   and its bias node, there are a total of 34 weights to be determined by
   the model, or more than 5 time as many as the id75
   model. it is evident that increasing hidden layers and their size
   rapidly increases the total number of coefficients. surprisingly, the
   neural network model is not over specified, yet it is also evident that
   since each node impacts all the other nodes, very complex behavior
   could be modeled. we could try to add terms to the id75
   model, such as all the second order terms (e.g. mi11*x1^2, mi12*x1*x2,
   etc.); using all the second order terms increases the number of
   parameters to 21. however, doing so is not guaranteed to lead to a good
   predictive model; if you fit polynomial terms in a linear model, it may
   extrapolate very badly. neural networks may be a better choice.

   neuron function and equations

   neurons are computational nodes   the inputs into a neuron are summed,
   then an activation function is used to generate the output. figure 2
   shows how this works.
   [aaiaaqdgaagaaqaaaaaaaa1gaaaajgvkywyyzme1ltmyzmutngrhzc04zwjjlwi2owe0nj
   kyzmq4na.jpg]

   figure 2. how neuron output is computed. the output of neuron 2,1 is a
   function of all the neurons in the previous layer, as well as the bias
   value. the weights of all those inputs are unique to neuron 2,1; the
   same inputs are fed to other neurons with different weights. in neuron
   2,1 the inputs are summed, then fed to fact.

   the fact is called the activation function, and can be a nonlinear
   function of the summed inputs. a common activation function is the
   sigmoid (also known as logistic function) function:
   [aaiaaqdgaagaaqaaaaaaaan3aaaajdm3odk5otc1ltgzytatngqyys04mjaylwjmzgm5ot
   yyyjhlna.jpg]

   where x is the sum of the inputs. this function mimics biological
   neurons in that the neuron doesn   t    fire    until the input reaches a
   threshold, by approximating a 0 to 1 step change. there are other
   id180 that can be used; generally it is desirable that
   the function be differentiable (see below for why), but in principal
   nearly any function can be used, even a distribution (like a normal
   distribution). the sigmoid function output vs. inputs is shown in
   figure 3.
   [aaiaaqdgaagaaqaaaaaaaan3aaaajddmzjhkmtazlwq4ywutndm1zc04njllltm3zdm2m2
   u2yjbknw.jpg]

   figure 3. a typical activation function, the sigmoid function varies in
   a highly non-linear way with respect to the input.

   linear output neurons

   a small but important point is that the output layer, which has just
   one neuron in figure 2, may use a different activation function. note
   that the layer can have more than one node, such as if we train a
   network to recognize digits (such as reading zip codes for the postal
   service sort), there would be 10 node, one for each digit. in
   classification problems (again, when the target has a finite number of
   values, like digits or letters or cats vs. non-cats) usually use a
   non-linear activation function in the output layer, because we want the
   correct choice to be near 1 and the rest near zero. however, it is
   possible to use neural networks to model continuous data, including
   time series. in those cases, which i explore below, the output layer
   has a single neuron with a linear activation function.

   the way the best weights are found is to calculate the error on each
   iteration, where the errors are the difference between the outputs and
   target values for the entire data set, or a sample of the data, then
   adjusting the weights to reduce the error. when using very large data
   sets and/or very complex networks, the computational cost of using the
   full data set on every update can be too high, depending on the compute
   resources available. using a sample of the data instead of the entire
   set is referred to as batch id119. the batch size is a
   hyperparameter. deepnet has a default batch size of 100, which i did
   not adjust in this work, while neuralnet does not use this
   parameter. as the data sets used here are small, this doesn   t make much
   difference. interestingly, a new paper from google brain (see
   [38]https://arxiv.org/abs/1711.00489v1) suggests that tuning the batch
   size is more effective than adjusting the learning rate to achieve
   fastest convergence on large data.

   in practice the squared errors are used in the error function, hence
   the very common error function root mean squared error (rmse). rmse is
   calculated by squaring each error term (the difference between the
   output and the target for a single instance in the data), summing,
   dividing by the number of training instances, then taking the square
   root. using rmse penalizes errors regardless if they are positive or
   negative, and there are statistical arguments for using it as the error
   function. here, i will use only the error function rmse.

   training vs. test data

   an important nuance is to distinguish between training data and test
   data. in any machine learning approach, it is important to subset data
   into training data, for which the algorithm is given the    correct   
   (here i call the target values) values, and test data. the test data
   are not used in determining the best weights, so they are often
   referred to as    unseen data   . for classification problems, the
   train/test data are often taken as a split, usually done randomly, of
   the available data. for instance if we have one million instances of a
   metric along with the million sets of the predictor data, we might
   select at random 70% of the data an use it for training, and use the
   remaining, randomly selected 30% of the data to test the result. at the
   end of the process, it is the performance on the test data we are
   concerned with. it is common in machine learning competitions that
   there is a set of data provided for training, and another set for which
   the dependent values are not provided, and the competitor is to submit
   their best predictions of the dependent values in the test set. in such
   cases, it is common to then further split the provided data with known
   values into a training and test set to find the best solution, then use
   that on the unseen data to submit to the competition. another subtlety
   is that if we are trying to model a time series, typically the test
   data are taken as a subset of data of the most recent times. there are
   many suggested approaches to train test spits for both classification
   and time series data, but this is outside our scope in this article.

   a common misconception is that we cannot    understand    how deep learning
   with neural networks    works   . this misconception arises from the
   observation that we can train huge networks with millions of neurons to
   recognize images or determine factors that can be used to classify
   other inputs. in such applications, it has been surprising to some how
   successful these methods are, lending the approach an air of
   mystery. in fact, the output of a trained network is deterministic, and
   the training process is mathematically represented by a closed set of
   equations. i think the challenge is to understand why deep learning
   works, rather than how.

   if we follow one path backward from the output, the impact on the
   output of the prior layer parameters can be described by partial
   derivative terms, allowing computation of the partial impact of one
   weight on the output as a function of that weight. this in turn allows
   updating the weights to move in the direction of lower error.

   as this weight adjustment phase is working backwards, it is called back
   propagation   the error is propagated back through the network and used
   to update the weights. in general, all the weights are updated at once
   in an iteration. this mathematical process can be represented very
   compactly using matrix methods (called id202 in the
   mathematical sciences). there are very efficient matrix mathematics
   algorithms in most computer programming languages, such as r, matlab  ,
   c++, etc. neuralnet and deepnet use features in the r language to do
   the updates. the process of updating the weights is often referred to
   as id119. the idea is that the partial derivatives are the
   local slope, or gradient, of the error function with respect to a
   weight. since we want to minimize the error function, we want to walk
   in a downhill direction, hence the descent.

   deep learning flavors and approaches

   having described the process as deterministic and simple
   mathematically, it turns out that like most problems, there are methods
   of solution that are better than others. standard back propagation
   proceeds more or less identically to the description above. there are
   more sophisticated algorithms that have been found to work better in
   many cases, where better means either more likely convergence and/or
   faster convergence. i won   t review here the range of advanced
   approaches to improve performance of neural networks. a useful
   reference article (although heavy in math!) by sebastien ruder
   is [39]here. (note that the number of labels can be distracting. you
   may see any of the following and all are at least partially under the
   umbrella of neural networks: back propagation (backprop), gradient
   descent, batch id119, stochastic id119 (sgd),
   deep neural networks (dnns), recurrent neural networks (id56s),
   convolutional neural networks (id98s), partially connected networks,
   fully connected networks, momentum, decay, adaptive learning rate,
   resilient id26, weight backtracking, , and a range of
   specific advanced algorithms including nag, adam, rmsprop, rprop, etc.)

   package feature and differences

   neuralnet has many attractive options, including selection of several
   algorithms to update weights, and a nice network visualization function
   that can be called once a model is built (used to generate the network
   in figure 1). deepnet has fewer options, but three important options
   not present in neuralnet   the first is the ability to use dropout to
   reduce over-training. dropout is a method that    turns off    a sample of
   neurons in a given update; typically the fraction of neurons turned off
   is a hyperparameter (see below for a working definition of
   hyperparameter). use of dropout has been shown to reduce over-training
   (or over-fitting), which is a behavior of many machine learning models
   wherein using more iterations reduces the error of matching the
   training data but degrades performance of predicting using new (aka
   unseen) data.

   the second valuable feature available in deepnet is the ability to
   exclude some neurons from being included in the calculation of the
   updates. the ability to exclude some neurons allows simulating more
   complex networks that are not fully connected networks. different
   network topologies than fully connected neural networks are an ongoing
   area of research, and may be very important for certain types of
   problems.

   lastly, deepnet allows a common hyperparameter to adjust the    batch
   size    used to update the weights. if i define an    epoch    as the process
   to use all the available data in one forward pass and one backward pass
   to update the weights, then a batch describes using a subset of the
   data instead of all the data. there are a number of approaches in the
   literature ranging from using as few as one instance per update (which
   may be called stochastic id119), or a small subset (which
   may be called mini-batch). there are papers describing advantages of
   using different batch sizes, including arguing for small batches to
   speed up convergence (especially for large datasets), and another
   showing the opposite in some cases
   (see: [40]https://arxiv.org/abs/1711.00489). for the purposes here, i
   use the default value (100 instances) for most comparisons, then
   compare performance a function of batch size for one set of parameters.

   here, i use only the standard id26 algorithm in both
   packages, no dropout, and only fully connected networks to focus on the
   hyperparameter questions of learning rate and decay. (look for future
   articles on the benefits of advanced neural network algorithms and
   advanced features like dropout and partially connected networks.) also,
   i do not address momentum in this article (see below).

   from this brief description, i can segue to some practical matters
   involved in actually training such networks. as already noted, there
   are a set of factors that are used to tweak the training process,
   commonly called hyperparameters. perhaps the most important is the
   learning rate. the learning rate is simply a factor used to reduce the
   size of the weight updates. below i will show why this is important,
   but in general, since the updates are calculated using partial
   derivatives, the impact of changes in the other weights isn   t included
   when estimating the weights for a given neuron. shrinking the update
   helps avoid    chasing our tail    where the changes are too large and
   introduce more error when combined with all the other updates. another
   hyperparameter is momentum, which carries forward gradient information
   from the prior update to improve the convergence rate when moving along
   an extended narrow    canyon    in the hypersurface. i don   t address
   momentum in this article. a third important hyperparameter is learning
   rate decay. this factor is used to reduce the learning rate on each
   iteration; i will show why this matters below.

   standard id26 is the method most often taught in
   introductory machine learning regarding deep learning or neural
   networks. it is also very likely to suffer from challenges in
   converging to a global optimum. a full discussion of these issues is
   outside the scope here, but essentially standard id26 easily
   can converge to local minimum of the error function, or not
   converge. figure 4 idealizes the n-dimensional surface along which we
   search for weights that converge on the minimum of the error function.
   [aaiaaqdgaagaaqaaaaaaaa51aaaajgvlyzq5zwfiltjmmjytndnlni1iodrklwizmtqxzg
   ezoge5nq.jpg]

   figure 4. simplified id119 diagram.

   the grey line in figure 4 is the    surface    of the error function across
   the space of all possible neuron weights. in reality, this surface is a
   hypersurface of many dimensions (hundreds to millions). here, you can
   view the grey curve as a cross section in one dimension of the
   multidimensional surface. our task in training the network is to find
   the set of weights which produces the global minimum error function
   value. if for illustration we assume this cross section is taken to
   include the global minimum, then the pink path is the only one that
   will converge to the global minimum. the green path won   t converge at
   all, and the red path is hard to predict. only by using learning rate
   decay (reducing the rate on each iteration) do the pink and blue paths
   converge. the blue path converges quickly, but to a local minimum, not
   the global one.

   most implementations of neural networks use a randomized starting
   point. it is easy to see that given a starting point, it is possible
   (maybe likely) to take such large steps (changes in the weights) that
   we randomly jump to another local feature of the hypersurface. on the
   other hand, if our steps are very small, either the network converges
   to a local minimum, or it converges very slowly, or both. also, if we
   find ourselves in a concave feature of the hypersurface, it is possible
   to get stuck, bouncing back and forth. these issues are often addressed
   by reducing the learning rate at each iteration. the learning rate is
   essentially the size of the step we take in the hyperspace; this is
   represented by the length of the arrows in figure 4. it is easy to see
   the intuition that decaying the learning rate over time can help to
   ensure convergence.

   figure 4 and the descriptions are only a few of the possible
   behaviors. because the system is multi-dimensional, the progress
   towards convergence can move at different rates over the iterations,
   and it is possible to move between local minima and other complex
   behaviors. below we   ll show a case that may appear non-intuitive but is
   an actual outcome of trying to train a network.

   all of this discussion about learning rate and convergence has so far
   avoided the question: how we detect convergence and stop training? the
   simplest approach is to define a fixed number of iterations, run that
   many, and inspect the results. the deepnet algorithm is designed in
   this way. if we don   t like the results, we can change the iterations
   limit and try again. another approach is to define the value of the
   error function we would like to reach or get below, and iterate until
   that criterion is met. interestingly, it has been found that in some
   cases, just doing more and more iterations may not be the best
   approach. if we calculate the error of the trained network predicting
   new data, sometimes the error gets better (smaller) as we increase
   iterations, up to a point, then gets worse. note that the error on the
   training data continues to improve, so that is an incomplete
   criterion. the idea of stopping before the error on the training data
   is minimized, but rather when the error on the test data is minimized,
   is called early stopping.

   neuralnet uses a stopping criteria not based on a fixed number of
   epochs, and not based on the error function, but the values of the
   partial derivative terms. an interesting aspect of setting the stopping
   criteria this way is that the resulting rmse on the test data tracks
   the threshold, so using this method is a nice way to set the accuracy
   you want to achieve and let the algorithm automatically run until that
   is met.

   a value named threshold in neuralnet is the maximum value of all the
   derivative terms that we will accept as convergence, and the algorithm
   continues until this criterion is met, or until the max steps is
   reached. in the latter case, the algorithm does not return a
   model. neuralnet does not report the error function on each iteration,
   instead reporting the maximum threshold value. the frequency of
   reporting is determined by a variable that can be adjusted. because
   neuralnet does not report error function values, it can be confusing
   when first using the package. the reason is that while the error
   function should monotonically decrease on each iteration (with properly
   selected hyperparameters), the maximum derivative may not
   decrease. this can give the appearance of non-convergence, but that may
   not be the case. for this article, i tweaked the threshold values in
   neuralnet and the max iterations in deepnet to compare apples to
   apples.

   the problem i use to illustrate convergence behavior in this article is
   shown in figure 5.
   [aaiaaqdgaagaaqaaaaaaaaujaaaajdjmnjgxode4lwnjn2utngi2zi04mdc4lta2ymu3nt
   fhndflmq.jpg]

   figure 5. a time series is observed as shown in the black curve,
   representing, for example, sales over time.  assume we believe the main
   contributors to sales are a long-term upward trend in the economy
   (green symbols), and our sales pipeline in the past (the orange
   symbols).

   for this example, i synthesized the sales history as a non-linear
   combination of the two trends, as shown in the equation on the
   figure. i then added noise to the two predictors, to challenge the
   model by including some other unknown factors, incomplete description
   of the relationships, etc. noise was added as random values to both the
   training data and the test data after calculating the values to be
   predicted.

   as a baseline, figure 6 shows predicting 500 future days using a linear
   model with the training data shown in figure 5.
   [aaiaaqdgaagaaqaaaaaaaayzaaaajdy2otm0zjuwlte2yzytnda5nc05mdjmltayzjbimj
   vkotlimw.jpg]

   figure 6. results of using a id75 model on the training
   data and predicting 500 days into the future. there are periods of
   significant deviation as well as an apparent phase shift (the peaks and
   valleys seem shifted from the target data).

   figure 7 shows some results using deepnet. it is evident even at this
   stage we have improved on the predictive power by using the neural
   network vs. id75.
   [aaiaaqdgaagaaqaaaaaaaarpaaaajduxn2iymde4lthhztetngnhoc05owy2lthiytq4zj
   rlmwq1zg.jpg]
   [aaiaaqdgaagaaqaaaaaaaa7iaaaajdnhodkzm2fklwjlmzgtngixmy1hnwi4lwizzwvjym
   mwzdjjmg.jpg]

   figure 7. baseline convergence with a simple network having one hidden
   layer with 5 neurons, an initial learning rate of 0.5 and a learning
   rate decay of 0.999986.

   at the top of figure 7 is the resulting prediction for future dates
   overlaid on the actual (calculated using the same functions as before,
   projected into the future). it is evident that the network learns major
   features of the time series behavior, as well as that it learns some of
   the noise. in the lower portion of figure 7, on the left is the raw
   error function reported by the algorithm vs. iterations; on the right
   is the resulting rmse on the test data (the times > 500). it can be
   seen that there are some anomalies in the convergence where the error
   function increases; these are due to the complex hypersurface the
   algorithm is navigating. in terms of the rmse, the behavior is more
   monotonic, but with these starting parameters, the prediction initially
   gets worse before declining monotonically. this could be taken to
   indicate less than ideal hyperparameters (learning rate 0.5 and decay
   0.999986).

   if we explore the learning rate and decay space in more detail, we see
   the behaviors shown in figure 8.
   [aaiaaqdgaagaaqaaaaaaaaylaaaajda1y2vmy2m1ltrhowutndvioc04zdblltg0zmu5zd
   rlyjdlma.jpg]

   figure 8. behavior of deepnet convergence as a function of initial
   learning rate, decay, and iterations. the curves are mean log trends
   across the learning rates in each group.

   here we can see the sensitivity of the solution to parameter
   choices. in particular, although in most cases the convergence is
   smoothly downward in error function, in the upper left series we can
   see that an unfortunate choice of initial learning rate and decay can
   result in non-convergence. although small, the two upper charts also
   show that there may be an optimum in decay in some cases. keep in mind
   that with hundreds of thousands of iterations, even the decay of
   0.99999 is very significant compared to no decay.

   as noted earlier, although when the weight values converge, to either a
   local or global minimum, the error function will continue to decrease
   as we increase iterations. however, what we really want is the best
   prediction on new/unseen data. figure 9 shows that if we monitor the
   performance as rmse on the test data, a different picture emerges.
   [aaiaaqdgaagaaqaaaaaaaataaaaajge5n2rkyjdmlwyxzmitndvjms05mge1lwvkn2u3mm
   qxzti0mq.jpg]

   figure 9. trend of root mean squared error (rmse) on the test data vs.
   initial learning rate and decay.

   we can see that significant differences can occur in results by small
   changes in the learning rate decay. in addition, the rmse tends to be
   minimized by careful choice of the initial learning rate. making the
   right choices can improve the prediction error by more than a factor of
   two. as noted earlier, decreasing error function does not guarantee
   decreasing rmse and could even lead to worse rmse. for deepnet, figure
   10 shows results of extending iterations to 600,000.
   [aaiaaqdgaagaaqaaaaaaaawuaaaajda1ztmyngvjlwi4mtatndixyi05njuxlwuxmjbmmd
   u1ogjima.jpg]

   figure 10. deepnet convergence behavior at higher iterations.

   what we see in figure 10 is that the error function is still decreasing
   at 600,000 iterations, but the rmse on the test data largely levels off
   around 350,000 iterations. although the error function is decreasing,
   the performance on the training data varies only slightly. figure 11
   shows that the main impact is a very slight reduction in bias in some
   ranges of the data. this illustrates the recommended practice of early
   stopping, where the test rmse is monitored and training stops when it
   ceases to significantly improve or begins to get worse (see, for
   instance,    deep learning   , a [41]nips 2015 tutorial by hinton, bengio,
   & lecun). a challenge is that many of the available r packages don   t
   facilitate such monitoring. for this work, i wrapped a loop around the
   function calls to the training algorithms, and stepped through csv
   files with the run parameters, so that i could stop and capture the
   intermediate values. this method is highly inefficient, leading to the
   likelihood that source code must be modified to efficiently use the
   packages.
   [aaiaaqdgaagaaqaaaaaaaaslaaaajgriyzbhngy2lwezy2ytndaxys1hodi4ltziyju5md
   e0ywewyg.jpg]

   figure 11. when iterations are increased from 300,000 to 600,000 using
   deepnet,

   a similar conclusion can be drawn by viewing the histograms of the
   residuals. the residuals are the difference between the model predicted
   values and the actual values. it is always a good idea to review the
   residuals to see if something is going wrong in the model training; in
   general a good model should have minimal bias which means the
   distribution of the residuals should have a maximum at zero. figure 12
   shows the corresponding histograms to figure 11.
   [aaiaaqdgaagaaqaaaaaaaawnaaaajdexndk5nta0lwuzmmutngvins05ztviltc0nzazmm
   vlymq4ng.jpg]

   figure 12. increasing iterations shifts the histogram by an rmse of
   about 0.25.

   now that i have completed the parameter investigation using deepnet, i
   can compare to neuralnet very easily. since neuralnet does not offer
   the learning rate decay hyperparameter, the task is to investigate the
   learning rate dependence, then compare the performance of the model
   training algorithm to deepnet.

   a quick initial comparison is to look at the ability to fit the
   training data. we expect this should be similar for similar levels of
   convergence (resulting rmse). figure 13 shows that this is roughly
   true, but it appears deepnet takes more iterations to achieve the same
   level of fit error.
   [aaiaaqdgaagaaqaaaaaaaa1vaaaajdqyzjg0ztg5lwi3mjgtndg3zi05m2ewlwnkotaxnj
   yyngm2nq.jpg]

   figure 13. fit by deepnet at 600,000 iterations results in a similar
   rmse on the training data as achieved by neuralnet in 300,000
   iterations. the fit behaviors are more or less identical.

   however, recall that deepnet uses a batch size hyperparameter, which i
   kept at the default of 100. the way deepnet reports iterations, an
   iteration is one forward and backward pass of a batch. the dataset was
   500 points long, which means five of these iterations is required to
   complete an epoch, which is one training cycle through all the
   data. neuralnet does not provide the batch size hyperparameter, which
   implies an epoch of neuralnet is using about 5 times the calculations
   that deepnet does with a batch of 100. i took all the data of rmse
   values vs. iterations for both algorithms, and plotted the rmse of
   deepnet vs. the rmse of neuralnet, when both are at the same
   epoch. figure 14 shows the results.
   [aaiaaqdgaagaaqaaaaaaaaptaaaajgqxn2vlnmqwltbkmzetndzjzi1izjk1ltu2ogyynd
   himmrhzq.jpg]

   figure 14. when compared on equivalent epochs, the two algorithms have
   a constant proportion to one another. deepnet tends to start higher
   (about 0.5 units in rmse), and it moves a bit more slowly (about 75% of
   the convergence rate, in epochs, of neuralnet).

   it is satisfying that the same underlying math generates
   self-consistent results. nonetheless, deepnet takes more epochs than
   neuralnet. this leads to the next comparison, which is the time each
   algorithm takes.
   [aaiaaqdgaagaaqaaaaaaaauiaaaajde5ywezm2mwlwmxyjqtndrkmi05mmi4ltnmmtk3nt
   hhmjlhyg.jpg]

   figure 15. deepnet epochs take much longer than neuralnet.

   what we see is that neuralnet is much faster than deepnet, and the
   difference grows  it turns out that neuralnet time is linear with
   epochs, while deepnet is roughly cubic time in epochs. this difference
   was quite noticeable on the system used to do the comparisons. (caveat:
   the system isn   t very high performance; it is possible the differences
   could be less on a faster machine [intel core i7 @2.4 ghz, 8 gb ram, no
   gpu used]) figure 16 shows another view of neuralnet behavior, looking
   at rmse on the test set vs. the threshold parameter. recall that
   earlier i noted that as neuralnet uses the threshold, which is the
   largest partial derivative value on an update pass, as the stopping
   parameter, and said that the rmse of the test data is linear with the
   threshold.
   [aaiaaqdgaagaaqaaaaaaaa6iaaaajgq5odblywewltfmm2utndg5zc1injvjltq3zgyynd
   u4odyynq.jpg]

   figure 16. neuralnet converges to higher rmse on the test data as the
   threshold stopping parameter increases. at very low values of the
   threshold, it appears to reach diminishing returns, and below about
   0.005 no further improvement in the predictive performance is noted. on
   the other hand, at high enough values the performance becomes again
   constant; this is an artifact of the particular data, and network
   topology such that above a certain threshold, the derivatives drop
   below the limit after relatively few iterations, but may not represent
   a properly trained model.

   based on the behavior in figure 16, if we started a parameter search at
   too high a threshold, and began lowering it, we could conclude,
   incorrectly, that we were at an optimum.

   in the bulk of the comparisons, i found a learning rate of 0.001 to
   0.0015 was needed to get good performance. it is evident that the
   definition of the learning rate differs in the two packages, which is
   another key point when using various open source packages   you need to
   explore enough to understand typical parameters. using a learning rate
   similar to that for deepnet diverged and produced no results.

   the convergence behavior of neuralnet on this problem has some
   interesting aspects. in particular, measured both as the error function
   and as rmse, the algorithm initially seemed to plateau, then jumped to
   lower error values and continued to converge slowly, as shown in figure
   17.
   [aaiaaqdgaagaaqaaaaaaaazraaaajgvhodzlotuwlwjjmwutngm4os1hzja2ltkwogy5yj
   i3mzdlma.jpg]

   figure 17. neuralnet seemed to be converge around 30,000 epochs, then
   jumped to another path and converged slowly. as noted already,
   neuralnet converges in fewer epochs and much less time than deepnet, so
   it was not a penalty to test higher numbers of epochs. not shown
   explicitly, neuralnet also tended to converge to lower rmse on the test
   data at the limit of high epochs (neuralnet achieved rmse around 0.6 as
   compared to 0.8 for deepnet).

   most of the results shown so far for neuralnet were obtained using a
   learning rate of 0.001. neuralnet has a clear optimum as compared to
   deepnet. figure 18 shows the epochs and time needed to reach a fixed
   level of convergence close to the results shown so far at around
   300,000 epochs.
   [aaiaaqdgaagaaqaaaaaaaaxnaaaajda0ndm1nwe0ltgxyjctnde4zc05zgi0lwm5zjvkmd
   ywndzknq.jpg]

   figure 18. neuralnet shows clear optimum for learning rate when
   progress is stopped well past when improvement has leveled off (around
   300,000 epochs in this case).

   neuralnet was more robust, in general, to changes in learning rate. at
   any given target convergence rmse, it would converge to a solution
   across a range of learning rates, and outside the acceptable range
   simply would not converge. as another way to look at the convergence
   optimization, figure 19 shows a 3d surface of the time to converge for
   neuralnet as a function of the stopping threshold and the learning
   rate. in the same figure, a similar surface is shown for deepnet, but
   with slightly different parameters.
   [aaiaaqdgaagaaqaaaaaaaarnaaaajda4zdewzdg0lwy3owmtndq0zi1hyzy1ltdmmtdimw
   ewyzkxyw.jpg]

   figure 19. convergence behavior of neuralnet (left) and deepnet
   (right).

   on the left of figure 19, the surface shows the time for neuralnet to
   converge as a function of the learning rate and the threshold value of
   the maximum partial derivative term. red/orange signifies larger time,
   and purple/grey signifies smaller time. recalling that the threshold is
   roughly proportional to the rmse achieved (using neuralnet neuralnet)
   for the test data (the unseen data that was not used for training),
   this chart shows that if the learning rate is too low for a given
   target, it takes longer to converge; however this is a broad region
   where convergence is fast (the large grey basin). as the threshold is
   increased, which means allowing a higher final rmse, the time to
   converge generally is reduced, except at extremely large values of the
   learning rate.

   it is important to note that there are some regions of the neuralnet
   parameters where the starting parameters can land in a complicated part
   of the surface, and can lead to confusing results. this exemplifies the
   challenge of finding parameters that    work   .

   on the right of figure 19 i show a surface that is roughly the
   equivalent of the neuralnet example, but for deepnet. learning rate has
   the same meaning, and as deepnet uses only the steps (epochs) as the
   stopping criteria, i replace the threshold axis for neuralnet with the
   steps axis for deepnet. as long as we are in a region where deepnet
   converges, these are roughly equivalent. the vertical dimension is the
   resulting rmse on the test data. i chose this metric because of the
   observed non-linear (approximately cubic in steps) growth of the time
   vs. steps using deepnet. in general, as steps increase, accuracy
   improves, but as already shown, there are diminishing
   returns. similarly, as the learning rate increases, convergence to a
   given rmse is faster. however, this surface is more complicated as
   there are regions where the rmse can fluctuate for a time as steps
   increase. depending on the stopping method and where search was
   started, it is very easy to end in a local pseudo-optimum when using
   deepnet. like neuralnet, if the starting learning rate is very small,
   there are regions where it can take a long time to converge.

   summary

   i have shown the behavior of two available r packages used to train and
   predict with a simple neural network applied to time series data. a
   synthesized data set was used with a single hidden layer of 5 neurons
   to train a model, and the model was used to predict unseen data in the
   test data set. the goal was to illustrate the challenges that can arise
   with the choice of hyperparameters to achieve an acceptable solution in
   an acceptable time. the non-linearity of the error function
   hypersurface requires choices of convergence control parameters that
   are    good enough    to allow convergence to an acceptable solution. i
   focused on only a few parameters: learning rate, learning rate decay,
   and number of epochs. nonetheless, the behavior is fairly complex and
   can be counterintuitive if starting in a    bad    location and with    poor   
   values of hyperparameters.

   there are many strategies to deal with these challenges. my hope is
   that these simple approaches pique your interest to dig deeper and
   understand the practical behavior of neural networks and that such
   understanding leads you to consider practical applications of neural
   networks to your predictive use cases. in my work i have used rprop in
   the neuralnet package. if you are concerned that neural networks are
   too complex to use, be encouraged that approaches like rprop and others
   achieve very significant improvements which also make practical
   applications easier.

   neuralnet can be obtained
   at: [42]https://cran.r-project.org/package=neuralnet

   deepnet can be obtained
   at [43]https://cran.r-project.org/package=deepnet

   originally posted [44]here.

   views: 8474

   tags:
   [45]like
   [46]8 members like this

   [47]share [48]tweet [49]facebook
     * [50]< previous post
     * [51]next post >

   comment

you need to be a member of data science central to add comments!

   [52]join data science central

   comment by [53]patrick stroh on december 20, 2017 at 5:06am
          well done, good thinking and systematic.  thanks so much.

   [54]rss

   welcome to
   data science central

   [55]sign up
   or [56]sign in

resources

     * [57]join dsc
     * [58]free books
     * [59]forum discussions
     * [60]cheat sheets
     * [61]jobs
     * [62]search dsc
     * [63]dsc on twitter
     * [64]dsc on facebook

videos

     * [65]dsc webinar series: predictive analytics: practical
       applications

[66]dsc webinar series: predictive analytics: practical applications
       added by [67]tim matteson [68]0 comments [69]0 likes

     * [70]dsc webinar series: patterns for successful data science
       projects

[71]dsc webinar series: patterns for successful data science projects
       added by [72]tim matteson [73]0 comments [74]0 likes

     * [75]dsc webinar series: advanced mapping with tableau

[76]dsc webinar series: advanced mapping with tableau
       added by [77]tim matteson [78]0 comments [79]0 likes

     * [80]add videos
     * [81]view all
     * [82]facebook

      2019   data science central      powered by[83] website builder |
   create website | ning.com

   [84]badges  |  [85]report an issue  |  [86]privacy policy  |  [87]terms
   of service

hello, you need to enable javascript to use data science central.

   please check your browser settings or contact your system
   administrator.
     __________________________________________________________________

   most popular content on dsc

   to not miss this type of content in the future, [88]subscribe to our
   newsletter.

   technical
     * [89]free books and resources for dsc members
     * [90]learn machine learning coding basics in a weekend
     * [91]new machine learning cheat sheet | [92]old one
     * [93]advanced machine learning with basic excel
     * [94]12 algorithms every data scientist should know
     * [95]hitchhiker's guide to data science, machine learning, r, python
     * [96]visualizations: comparing tableau, spss, r, excel, matlab, js,
       pyth...
     * [97]how to automatically determine the number of clusters in your
       data
     * [98]new perspectives on statistical distributions and deep learning
     * [99]fascinating new results in the theory of randomness
     * [100]long-range correlations in time series: modeling, testing,
       case study
     * [101]fast combinatorial feature selection with new definition of
       predict...
     * [102]10 types of regressions. which one to use?
     * [103]40 techniques used by data scientists
     * [104]15 deep learning tutorials
     * [105]r: a survival guide to data science with r

   non technical

     * [106]advanced analytic platforms - incumbents fall - challengers
       rise
     * [107]difference between ml, data science, ai, deep learning, and
       statistics
     * [108]how to become a data scientist - on your own
     * [109]16 analytic disciplines compared to data science
     * [110]six categories of data scientists
     * [111]21 data science systems used by amazon to operate its business
     * [112]24 uses of statistical modeling
     * [113]33 unusual problems that can be solved with data science
     * [114]22 differences between junior and senior data scientists
     * [115]why you should be a data science generalist - and how to
       become one
     * [116]becoming a billionaire data scientist vs struggling to get a
       $100k job
     * [117]why do people with no experience want to become data
       scientists?

   articles from top bloggers

     * [118]kirk borne | [119]stephanie glen | [120]vincent granville
     * [121]ajit jaokar | [122]ronald van loon | [123]bernard marr
     * [124]steve miller | [125]bill schmarzo | [126]bill vorhies

   other popular resources

     * [127]comprehensive repository of data science and ml resources
     * [128]statistical concepts explained in simple english
     * [129]machine learning concepts explained in one picture
     * [130]100 data science interview questions and answers
     * [131]cheat sheets | [132]curated
       articles | [133]search | [134]jobs | [135]courses
     * [136]post a blog | [137]forum
       questions | [138]books | [139]salaries | [140]news

   archives: [141]2008-2014 | [142]2015-2016 | [143]2017-2019 | [144]book
   1 | [145]book 2 | [146]more

   follow us: [147]twitter | [148]facebook
     __________________________________________________________________

   most popular articles

     * [149]free book and resources for dsc members
     * [150]new perspectives on statistical distributions and deep
       learning
     * [151]time series, growth modeling and data science wizardy
     * [152]statistical concepts explained in simple english
     * [153]machine learning concepts explained in one picture
     * [154]comprehensive repository of data science and ml resources
     * [155]advanced machine learning with basic excel
     * [156]difference between ml, data science, ai, deep learning, and
       statistics
     * [157]selected business analytics, data science and ml articles
     * [158]how to automatically determine the number of clusters in your
       data
     * [159]fascinating new results in the theory of randomness
     * [160]hire a data scientist | [161]search dsc | [162]find a job
     * [163]post a blog | [164]forum questions

   [8fa427bf6de170faefe32330e3b4b102?n_seg=_technology&n_name=data science
   central]

references

   visible links
   1. https://www.datasciencecentral.com/profiles/blog/feed?user=3v6n5b6g08kgn&xn_auth=no
   2. https://www.datasciencecentral.com/profiles/comment/feed?attachedto=6448529:blogpost:667452&xn_auth=no
   3. https://www.googletagmanager.com/ns.html?id=gtm-t5w4wq
   4. https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
   5. https://www.datasciencecentral.com/main/authorization/signup?target=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
   6. https://www.datasciencecentral.com/main/authorization/signin?target=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
   7. https://www.datasciencecentral.com/
   8. https://www.datasciencecentral.com/
   9. https://www.datasciencecentral.com/page/most-popular-content-on-dsc
  10. https://www.datasciencecentral.com/page/editorial-guidelines
  11. https://www.datasciencecentral.com/page/user-agreement
  12. https://www.datasciencecentral.com/page/cookie-policy
  13. https://www.datasciencecentral.com/page/search?q=ai
  14. https://www.datasciencecentral.com/page/search?q=machine+learning
  15. https://www.datasciencecentral.com/page/search?q=deep+learning
  16. https://www.analyticbridge.datasciencecentral.com/
  17. https://www.statisticshowto.datasciencecentral.com/
  18. https://www.bigdatanews.datasciencecentral.com/
  19. https://www.datavizualization.datasciencecentral.com/
  20. https://www.hadoop360.datasciencecentral.com/
  21. https://www.datasciencecentral.com/video/video/listtagged?tag=dsc+podcast+series
  22. https://www.datasciencecentral.com/video/video/listfeatured
  23. https://www.datasciencecentral.com/forum
  24. https://www.analytictalent.datasciencecentral.com/
  25. https://www.datasciencecentral.com/profiles/blogs/check-out-our-dsc-newsletter
  26. https://www.datasciencecentral.com/page/most-popular-content-on-dsc
  27. https://www.datasciencecentral.com/group/resources/forum/topics/selection-of-best-articles-from-our-past-weekly-digests
  28. https://www.datasciencecentral.com/groups/group/list
  29. https://www.datasciencecentral.com/page/search
  30. https://www.datasciencecentral.com/page/contact-us
  31. https://www.datasciencecentral.com/profiles/blogs/check-out-our-dsc-newsletter
  32. https://www.datasciencecentral.com/profiles/blog/list
  33. https://www.datasciencecentral.com/profiles/blog/list?my=1
  34. https://www.datasciencecentral.com/profiles/blog/new
  35. https://www.datasciencecentral.com/profile/vincentgranville
  36. https://www.datasciencecentral.com/profiles/blog/list?user=3v6n5b6g08kgn
  37. https://www.linkedin.com/in/blainebateman/
  38. https://arxiv.org/abs/1711.00489v1
  39. http://ruder.io/optimizing-gradient-descent/index.html
  40. https://arxiv.org/abs/1711.00489
  41. http://www.iro.umontreal.ca/~bengioy/talks/dl-tutorial-nips2015.pdf
  42. https://cran.r-project.org/package=neuralnet
  43. https://cran.r-project.org/package=deepnet
  44. https://www.linkedin.com/pulse/comparison-deepnet-neuralnet-two-r-packages-neural-bateman-eaf-llc/
  45. https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  46. https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  47. https://www.datasciencecentral.com/main/sharing/share?id=6448529%3ablogpost%3a667452
  48. https://twitter.com/share
  49. https://www.facebook.com/share.php?u=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet?xg_source=facebook&t=comparison of deepnet & neuralnet on data science central
  50. https://www.datasciencecentral.com/profiles/blogs/thursday-news-big-data-ml-ai-tensorflow-privacy-slim-databases
  51. https://www.datasciencecentral.com/profiles/blogs/weekly-digest-december-18
  52. https://www.datasciencecentral.com/main/authorization/signup?target=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  53. https://www.datasciencecentral.com/profile/patrickstroh
  54. https://www.datasciencecentral.com/profiles/comment/feed?attachedto=6448529:blogpost:667452&xn_auth=no
  55. https://www.datasciencecentral.com/main/authorization/signup?target=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  56. https://www.datasciencecentral.com/main/authorization/signin?target=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  57. https://www.datasciencecentral.com/profiles/blogs/check-out-our-dsc-newsletter
  58. https://www.datasciencecentral.com/profiles/blogs/new-books-and-resources-for-dsc-members
  59. https://www.datasciencecentral.com/forum
  60. https://www.datasciencecentral.com/page/search?q=cheat+sheets
  61. https://www.analytictalent.datasciencecentral.com/
  62. https://www.datasciencecentral.com/page/search?q=one+picture
  63. https://twitter.com/datasciencectrl
  64. https://www.facebook.com/datasciencecentralcommunity/
  65. https://www.datasciencecentral.com/video/dsc-webinar-series-predictive-analytics-practical-applications
  66. https://www.datasciencecentral.com/video/dsc-webinar-series-predictive-analytics-practical-applications
  67. https://www.datasciencecentral.com/profile/2edcolrgc4o4b
  68. https://www.datasciencecentral.com/video/dsc-webinar-series-predictive-analytics-practical-applications#comments
  69. https://www.datasciencecentral.com/video/dsc-webinar-series-predictive-analytics-practical-applications
  70. https://www.datasciencecentral.com/video/dsc-webinar-series-patterns-for-successful-data-science-projects
  71. https://www.datasciencecentral.com/video/dsc-webinar-series-patterns-for-successful-data-science-projects
  72. https://www.datasciencecentral.com/profile/2edcolrgc4o4b
  73. https://www.datasciencecentral.com/video/dsc-webinar-series-patterns-for-successful-data-science-projects#comments
  74. https://www.datasciencecentral.com/video/dsc-webinar-series-patterns-for-successful-data-science-projects
  75. https://www.datasciencecentral.com/video/dsc-webinar-series-advanced-mapping-with-tableau
  76. https://www.datasciencecentral.com/video/dsc-webinar-series-advanced-mapping-with-tableau
  77. https://www.datasciencecentral.com/profile/2edcolrgc4o4b
  78. https://www.datasciencecentral.com/video/dsc-webinar-series-advanced-mapping-with-tableau#comments
  79. https://www.datasciencecentral.com/video/dsc-webinar-series-advanced-mapping-with-tableau
  80. https://www.datasciencecentral.com/video/video/chooseuploader
  81. https://www.datasciencecentral.com/video/video
  82. https://www.facebook.com/share.php?u=https://www.datasciencecentral.com/video/video?from=fb
  83. https://www.ning.com/
  84. https://www.datasciencecentral.com/main/embeddable/list
  85. https://www.datasciencecentral.com/main/authorization/signup?target=https://www.datasciencecentral.com/main/index/report
  86. https://www.datasciencecentral.com/main/authorization/privacypolicy?previousurl=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  87. https://www.datasciencecentral.com/main/authorization/termsofservice?previousurl=https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet
  88. https://www.datasciencecentral.com/profiles/blogs/check-out-our-dsc-newsletter
  89. https://www.datasciencecentral.com/profiles/blogs/new-books-and-resources-for-dsc-members
  90. https://www.datasciencecentral.com/profiles/blogs/learn-machinelearning-coding-basics-in-a-weekend-a-new-approach
  91. https://www.datasciencecentral.com/profiles/blogs/new-data-science-cheat-sheet
  92. https://www.datasciencecentral.com/profiles/blogs/data-science-cheat-sheet
  93. https://www.datasciencecentral.com/profiles/blogs/advanced-machine-learning-with-basic-excel
  94. https://www.datasciencecentral.com/profiles/blogs/12-algorithms-every-data-scientist-should-know
  95. https://www.datasciencecentral.com/profiles/blogs/hitchhiker-s-guide-to-data-science-machine-learning-r-python
  96. https://www.datasciencecentral.com/profiles/blogs/visualizations-comparing-tableau-spss-r-excel-matlab
  97. https://www.datasciencecentral.com/profiles/blogs/how-to-automatically-determine-the-number-of-clusters-in-your-dat
  98. https://www.datasciencecentral.com/profiles/blogs/decomposition-of-statistical-distributions-using-mixture-models-a
  99. https://www.datasciencecentral.com/profiles/blogs/fascinating-new-results-in-the-theory-of-randomness
 100. https://www.datasciencecentral.com/profiles/blogs/long-range-correlation-in-time-series-tutorial-and-case-study
 101. https://www.datasciencecentral.com/profiles/blogs/feature-selection-based-on-predictive-power
 102. https://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use
 103. https://www.datasciencecentral.com/profiles/blogs/40-techniques-used-by-data-scientists
 104. https://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials
 105. https://www.datasciencecentral.com/profiles/blogs/one-page-r-a-survival-guide-to-data-science-with-rone-page
 106. https://www.datasciencecentral.com/profiles/blogs/advanced-analytic-platforms-incumbents-fall-challengers-rise
 107. https://www.datasciencecentral.com/profiles/blogs/difference-between-machine-learning-data-science-ai-deep-learning
 108. https://www.datasciencecentral.com/profiles/blogs/how-to-become-a-data-scientist-for-free
 109. https://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared
 110. https://www.datasciencecentral.com/profiles/blogs/six-categories-of-data-scientists
 111. https://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business
 112. https://www.datasciencecentral.com/profiles/blogs/24-uses-of-statistical-modeling-part-ii
 113. https://www.datasciencecentral.com/profiles/blogs/33-unusual-problems-that-can-be-solved-with-data-science
 114. https://www.datasciencecentral.com/profiles/blogs/10-differences-between-junior-and-senior-data-scientist
 115. https://www.datasciencecentral.com/profiles/blogs/why-you-should-be-a-data-science-generalist
 116. https://www.datasciencecentral.com/profiles/blogs/becoming-a-billionaire-data-scientist-vs-struggling-to-get-a-100k
 117. https://www.datasciencecentral.com/profiles/blogs/why-do-people-with-no-experience-want-to-become-data-scientists
 118. https://www.analyticbridge.datasciencecentral.com/profiles/blog/list?user=1zo63k80n1dij
 119. https://www.datasciencecentral.com/profiles/blog/list?user=0lahn4b4odglr
 120. https://www.datasciencecentral.com/profiles/blogs/my-data-science-machine-learning-and-related-articles
 121. https://www.datasciencecentral.com/profiles/blog/list?user=32ac9fc41n4f4
 122. https://www.datasciencecentral.com/profiles/blog/list?user=3f0kgbtc91mum
 123. https://www.datasciencecentral.com/profiles/blog/list?user=00t05rv0ehb3k
 124. https://www.datasciencecentral.com/profiles/blog/list?user=00u6blr1dk4fz
 125. https://www.datasciencecentral.com/profiles/blog/list?user=0do9dajam14h1
 126. https://www.datasciencecentral.com/profiles/blog/list?user=0h5qapp2gbuf8
 127. https://www.datasciencecentral.com/profiles/blogs/comprehensive-repository-of-data-science-and-ml-resources
 128. https://www.datasciencecentral.com/page/search?q=statistical+concepts
 129. https://www.datasciencecentral.com/page/search?q=in+one+pictures
 130. https://www.datasciencecentral.com/profiles/blogs/100-data-science-interview-questions-and-answers
 131. https://www.datasciencecentral.com/page/search?q=cheat+sheets
 132. https://www.datasciencecentral.com/profiles/blogs/21-curated-blogs-about-deep-learning-and-data-science
 133. https://www.datasciencecentral.com/page/search?q=python
 134. http://www.analytictalent.com/
 135. https://www.datasciencecentral.com/page/search?q=courses
 136. https://www.datasciencecentral.com/profiles/blog/new
 137. https://www.datasciencecentral.com/forum/topic/new
 138. https://www.datasciencecentral.com/page/search?q=books
 139. https://www.datasciencecentral.com/page/search?q=salary
 140. https://www.bigdatanews.datasciencecentral.com/group/bdn-daily-press-releases/forum
 141. https://www.analyticbridge.datasciencecentral.com/page/links
 142. https://www.datasciencecentral.com/group/resources/forum/topics/selection-of-best-articles-from-our-past-weekly-digests
 143. https://www.datasciencecentral.com/page/previous-digests
 144. https://www.analyticbridge.datasciencecentral.com/group/data-science/forum/topics/data-science-e-book-first-draft-available-for-download
 145. https://www.datasciencecentral.com/profiles/blogs/my-data-science-book
 146. https://www.datasciencecentral.com/profiles/blogs/older-data-science-articles-still-of-great-value-today
 147. https://twitter.com/datasciencectrl
 148. https://www.facebook.com/datasciencecentralcommunity/
 149. https://www.datasciencecentral.com/profiles/blogs/new-books-and-resources-for-dsc-members
 150. https://www.datasciencecentral.com/profiles/blogs/decomposition-of-statistical-distributions-using-mixture-models-a
 151. https://www.datasciencecentral.com/profiles/blogs/data-science-wizardry
 152. https://www.datasciencecentral.com/page/search?q=statistical+concepts
 153. https://www.datasciencecentral.com/page/search?q=in+one+pictures
 154. https://www.datasciencecentral.com/profiles/blogs/comprehensive-repository-of-data-science-and-ml-resources
 155. https://www.datasciencecentral.com/profiles/blogs/advanced-machine-learning-with-basic-excel
 156. https://www.datasciencecentral.com/profiles/blogs/difference-between-machine-learning-data-science-ai-deep-learning
 157. https://www.datasciencecentral.com/profiles/blogs/my-data-science-machine-learning-and-related-articles
 158. https://www.datasciencecentral.com/profiles/blogs/how-to-automatically-determine-the-number-of-clusters-in-your-dat
 159. https://www.datasciencecentral.com/profiles/blogs/fascinating-new-results-in-the-theory-of-randomness
 160. http://careers.analytictalent.com/jobs/products
 161. https://www.datasciencecentral.com/page/search?q=python
 162. http://www.analytictalent.com/
 163. https://www.datasciencecentral.com/profiles/blog/new
 164. https://www.datasciencecentral.com/forum/topic/new

   hidden links:
 166. https://www.datasciencecentral.com/profile/vincentgranville
 167. https://www.datasciencecentral.com/profile/patrickstroh
 168. https://www.datasciencecentral.com/forum
 169. https://www.datasciencecentral.com/page/search?q=cheat+sheets
 170. https://analytictalent.com/
