   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

playing strategy games with the minimax algorithm

   [11]go to the profile of grant bartel
   [12]grant bartel (button) blockedunblock (button) followfollowing
   dec 10, 2017
   [1*jeazgnr-tcr60iwxlwwpdg.jpeg]
   [13]https://www.engadget.com/2017/05/31/ai-is-already-beating-us-at-our
   -own-game/

   in this lesson, we   ll explore a popular algorithm called minimax. we   ll
   also learn some of its friendly neighborhood add-on features like
   heuristic scores, iterative deepening, and alpha-beta pruning. using
   these techniques, we can create a more flexible and powerful game
   playing agent. it   ll be able to compete in many challenges, including
   the strategy game isolation.

   in my previous post [14]how to win sudoku, we learned how to teach
   computers to solve the puzzle sudoku. if you haven   t read it, go ahead
   and give it a quick read. but that was really just a way to get our
   feet wet, before diving into more sophisticated methods of game playing
   agents. especially those methods that can make strategic moves against
   an opponent!
   [1*himj55zzd3g9rpyuza0thw.jpeg]
   [15]https://boardgamegeek.com/image/784001/isolation

don   t get stranded

   isolation (or isola) is a turn-based strategy board game where two
   players try to confine their opponent on a 7x7 checker-like board.
   eventually, they can no longer make a move (thus isolating them).

   each player has one piece, which they can move around like a queen in
   chess         up-down, left-right, and diagonal. there are three conditions
   under which the pieces can be moved    
    1. they cannot place their piece on an already visited square.
    2. they cannot cross over already visited squares (squeezing through
       them diagonally is ok).
    3. they cannot cross over each other   s piece.

   [1*mdxmwkoerqzzmx-j4fmhpa.jpeg]
   [16]https://www.cs.umb.edu/~yunxu/isola/final.jpg

   in the above picture, you can see from the black squares that both
   players have placed their pieces on various parts of the board. but as
   the game progressed, it shows that the yellow player still has three
   possible moves. up and to the right, right one square, and right two
   squares. but the blue player is out of options. hence, the yellow
   player is the winner here.

   now this may seem like a simple game         and to be honest, it is. it   s
   not like we   re playing [17]poker or [18]starcraft. yet, there is still
   an enormous amount of possible moves either player can make at any
   point during the game.

   in puzzles like sudoku, there   s an    answer    which we want to solve for.
   but there is no answer when it comes to strategy games.

   we   re playing against another opponent         like a person, computer, or a
   cat [19]detective. this requires strategy and some thought into how the
   game may turn out as it rolls along.

   such games can evolve and produce an absurd amount of possible
   outcomes. so we need to think of how we can choose the best possible
   move, without spending the amount of time it took for cats to populate
   the earth.
   [1*9uuiulvx4_xtpsf8-buoya.jpeg]

   okay, no more cats!

mighty minimax and friends

   now that you know how to play isolation, let   s take a look at how we
   can use the minimax algorithm; a staple in the ai community. we   ll also
   look at heuristic scores, iterative deepening, and alpha-beta pruning.
   together with these, we can build a competitive ai agent.

minimax

   the [20]minimax algorithm is very popular for teaching ai agents how to
   play turn-based strategy games. the reason being is that it takes into
   account all the possible moves that players can take at any given time
   during the game. with this information, it then attempts to minimize
   the opponent player   s advantage while maximizing the agent   s at every
   turn the ai agent gets to play.

   now, how does this look?

   well, similar to how an ai agent would play a game like sudoku, we can
   model the next possible moves either player can make via a search tree.
   however, we   ll need to use a search tree with variable breadths         or in
   other words, a tree level   s width. the reason being is that there are a
   variable number of moves each player can make at any given time during
   the game.
   [1*y_josvkybzb63uzblbcgrq.png]
   udacity ai nanodegree program

   the tree shown above represents the next moves available during a game
   of isolation. it has a 2x3 grid, with the bottom right square being
   unreachable. as you can see, the two players are a blue circle and a
   red cross.

   the top of the tree (the root node) illustrates a move made by the red
   player. the middle level illustrates the next possible moves by the
   blue player. and the third level illustrates the possible moves by the
   red player, given the previous move made by the blue player.

     each game state or node in the tree has information about which
     player has the most to gain from any potential move.

   now you might be wondering, what the heck are those triangles below
   each move?

   the downward triangle represents a location in the tree where minimax
   will minimize the opponent   s advantage. whereas, the upward triangles
   are the locations where minimax maximizes the agent   s advantage.

   but minimax can only know either players    advantage if it knows the
   paths in the tree that lead to a victory for either player. this means
   minimax must traverse to the very bottom of the tree for every possible
   series of moves. next, it has to assign some score (e.g., +1 for a win
   and -1 for a loss), and propagate those numbers up through the tree.
   this way, each game state or node in the tree has information about
   which player has the most to gain from any potential move.
   [1*p-4ljaoeghyp16qxpzraiw.png]
   udacity ai nanodegree program

   in this picture, we can make a couple of observations. first minimax
   assigns a number to the final game outcomes at the leaf nodes. then it
   propagates them upward through the tree, performing minimizations and
   maximizations on the way. once minimax finishes filling in the tree,
   whenever it   s the ai agent   s turn, it   ll know which moves will likely
   lead to a win or loss.

   the second level after the root node shows the next possible moves for
   the blue player (our ai agent). our agent wants to maximize the
   available scores during its turn. so it would choose the move
   represented in the right-most node following the root node. super cool!

   but does it make sense to simply assign a +1 or -1 to game outcomes?
   shouldn   t this score take into consideration how the game is won or
   lost?

   spoiler alert: the answer is yes!

heuristic scores

   in the world of strategy games, a heuristic score is essentially a
   subjective value we assign to some game state. this value is based on
   our understanding of how the game is won and lost. by choosing a
   well-thought-out heuristic score, we   re able to teach our ai agent how
   to best select its next moves while playing the game isolation.

   now there   s probably an unlimited number of heuristic scores we could
   come up with. but here we   ll only look at a few of them, apart from the
   na  ve score (ns) of +1 and -1.

   one idea could be to count all the next possible moves each player has
   at any given time, since more possible moves mean less chance of being
   isolated. we   ll call this the open move score (oms).

   another idea could be to use the value obtained from oms and
   subtracting the number of next possible moves the opponent has. the
   reason being that each player wants to increase their amount of moves
   while decreasing their opponent   s. we   ll call this the improved score
   (is).
   [1*vfozxcjwl2dybexymi2btq.jpeg]

   the above figure shows the win rates over many simulated isolation
   games played between ai agents using different heuristic scores. now
   you can see how different our scores did during actual game play. but
   there were some heuristic scores that outperformed the ones we came up
   with

   interestingly, the top two are almost exactly the same as improved
   score. we   ll call them aggressive improved score (ais) and super
   aggressive improved score (sais). but there   s a slight difference
   between these scores and the original. the top two scores apply a
   factor of two and three to the value you subtract with (the number of
   moves available to the opponent) when computing the improved score.

   you can discover an optimal    aggressive factor    to apply when computing
   this score!

   another spoiler alert         better values exist.

   but what if we come up with a heuristic score that takes a lot of time
   to compute? what if the tree is huge? will our ai agent have enough
   time to find its next best moves, while still being responsive enough
   during game play?

iterative deepening

   now we know that our ai agent can model all possible moves using a
   search tree and its nodes    corresponding heuristic score. but
   unfortunately, when playing isolation our tree will be massive. it
   would take more time to search the tree and compute these values than
   there are years since the big bang!
   [1*irycnaruyxwnk56oloag1a.jpeg]

   enter [21]iterative deepening         the go-to time management strategy for
   game playing agents. by using this method, we can cut the compute and
   search time down to a maximum time of our choosing. this way our ai
   agent can respond at least as fast as a human could.

   but how does iterative deepening work?

   it allows minimax to move level by level and compute heuristic scores
   until a certain time limit. once this time limit is reached, the ai
   agent is forced to use the best move it discovered while having moved
   deeper and deeper down the tree.
   [1*_b32br_bjglof8spnsenbg.jpeg]
   [22]https://chessprogramming.wikispaces.com/iterative+deepening/

   now this provides some insight into how difficult it can be. creating
   an ai agent that is smart and responsive enough for strategy games, can
   be quite tricky, even for ai wizards. especially if such games contain
   a world of possibilities.

   unfortunately, the number of moves the ai agent can    imagine    going
   forward is limited. so it   s possible it could make a decision that
   leads to its demise. this is a well-known phenomenon called the
   [23]horizon effect. but we still need to look at arguably the most
   effective time slashing algorithm used when searching trees.

alpha-beta pruning

   [1*4jiw0zgtf2ebooctxz1wmq.jpeg]
   [24]http://desperateexes.com/wp-content/uploads/2016/09/the-california-
   raisins.jpg

   okay, those are raisins and not prunes, but still         how were these ever
   a thing? i mean, seriously, a raisin blues group?

   you might have already guessed [25]alpha-beta pruning has nothing to do
   with prunes, and more about reducing the size (pruning) of our search
   tree. when we have a very large search tree, it turns out that it   s not
   always necessary to traverse every node when using minimax.

   we need to give minimax the ability to stop searching a particular
   region of the tree when it finds the guaranteed minimum or maximum of
   that particular level.

   if we can do that, this can greatly reduce our ai agent   s response time
   and improve performance.

   how does alpha-beta pruning work?

   the minimax algorithm moves through the tree using [26]depth-first
   search. meaning it traverses through the tree going from left to right,
   and always going the deepest it can go. it then discovers values that
   must be assigned to nodes directly above it, without ever looking at
   other branches of the tree.

   alpha-beta pruning allows minimax to make just as good of decisions as
   minimax could do alone, but with a higher level of performance.

   consider the following image, in which we have a tree with various
   scores assigned to each node. some nodes are shaded in red, indicating
   there   s no need to review them.
   [1*ksh_tocchh41khwsu6dywa.jpeg]
   [27]https://en.wikipedia.org/wiki/alpha-beta_pruning

   at the bottom left of the tree, minimax looks at the values 5 and 6 on
   the bottom max level. it determines that 5 must be assigned to the min
   level right above it. makes sense.

   but, after looking at 7 and 4 of the right max level branch, it
   realizes that the above min level node must be assigned a maximum value
   of 4. since the second max level right above the first min level will
   take the maximum between 5 and at most 4, it   s clear that it   ll choose
   5. following this, it would continue traversing the tree to perform the
   same exact set of operations within the tree   s other branches.

   below is the algorithmic representation of minimax with alpha-beta
   pruning.
   [1*gkjyorisoiy1xrwid113xdya.png]
   udacity ai nanadegree program

   using this method provides an easy way to cut down on our ai agent   s
   search space. this way, alpha-beta pruning allows minimax to make good
   decisions that minimax could do alone, but with a higher level of
   performance.

isola-ter

   we   ve explored how to build our own ai agent that can play the game
   isolation at a fairly competitive level. by using the minmax algorithm,
   we saw how the ai agent can model the game and can make decisions based
   on a heuristic score. we also learned how to determine a well-defined
   heuristic for our given task (isolation).

   but we also discovered that it would be far too computationally intense
   to let minimax run wild. so we had to use techniques like iterative
   deepening and alpha-beta pruning. these would force our ai agent to
   come up with the next move within a reasonable amount of time. but what
   if we want our ai agent to have a higher win rate while at least being
   as responsive as a human?

   well, there are other techniques we could explore to increase our
   agent   s win rate as well as response time. we touched on the idea of
   tweaking our heuristic score   s parameters (remember the    aggressive
   factor   ?). we could even come up with a heuristic score better tailored
   for playing isolation.

   there are also reflective properties related to the possible moves on
   the isolation board. these become evident when we analyze the fully
   populated search tree, which would allow us to potentially slash a lot
   of branches from the search tree. also, if we upgraded our hardware,
   our ai agent would be faster         and thus be able to explore more
   possibilities.

   if you want to get into the nitty-gritty details of how to implement
   this yourself, take a look at the code i wrote to solve this problem
   for my [28]udacity artificial intelligence nanodegree. you can find it
   on [29]my github repo.
     __________________________________________________________________

   hi, i   m [30]grant! i work as an ai engineer at a startup in barcelona,
   spain and currently participate in udacity   s artificial intelligence
   nanodegree program. follow me on [31]twitter and connect with me on
   [32]linkedin!

   this article was originally posted on [33]my blog.

     * [34]artificial intelligence
     * [35]tech
     * [36]programming
     * [37]startup
     * [38]self improvement

   (button)
   (button)
   (button) 751 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of grant bartel

[40]grant bartel

   technical recruiter | developer | 100% remote anywhere | check out my
   website: [41]http://grantbartel.com

     (button) follow
   [42]freecodecamp.org

[43]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 751
     * (button)
     *
     *

   [44]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [45]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4ecb83b39b4b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/playing-strategy-games-with-minimax-4ecb83b39b4b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/playing-strategy-games-with-minimax-4ecb83b39b4b&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_t7q1m6wenpz7---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@grantbartel?source=post_header_lockup
  12. https://medium.freecodecamp.org/@grantbartel
  13. https://www.engadget.com/2017/05/31/ai-is-already-beating-us-at-our-own-game/
  14. https://towardsdatascience.com/how-to-win-sudoku-3a82d05a57d
  15. https://boardgamegeek.com/image/784001/isolation
  16. https://www.cs.umb.edu/~yunxu/isola/final.jpg
  17. http://www.sciencemag.org/news/2017/03/artificial-intelligence-goes-deep-beat-humans-poker
  18. https://www.wired.com/story/googles-ai-declares-galactic-war-on-starcraft-/
  19. http://theoatmeal.com/comics/scrambles
  20. https://en.wikipedia.org/wiki/minimax
  21. http://www.geeksforgeeks.org/iterative-deepening-searchids-iterative-deepening-depth-first-searchiddfs/
  22. https://chessprogramming.wikispaces.com/iterative+deepening/
  23. https://en.wikipedia.org/wiki/horizon_effect
  24. http://desperateexes.com/wp-content/uploads/2016/09/the-california-raisins.jpg
  25. https://en.wikipedia.org/wiki/alpha   beta_pruning
  26. https://en.wikipedia.org/wiki/depth-first_search
  27. https://en.wikipedia.org/wiki/alpha-beta_pruning
  28. https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889
  29. https://github.com/grantathon/aind-isolation
  30. http://grantbartel.com/
  31. https://twitter.com/grantbartel
  32. https://www.linkedin.com/in/grantbartel/
  33. http://grantbartel.com/blog/playing-strategy-games-minimax/
  34. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  35. https://medium.freecodecamp.org/tagged/tech?source=post
  36. https://medium.freecodecamp.org/tagged/programming?source=post
  37. https://medium.freecodecamp.org/tagged/startup?source=post
  38. https://medium.freecodecamp.org/tagged/self-improvement?source=post
  39. https://medium.freecodecamp.org/@grantbartel?source=footer_card
  40. https://medium.freecodecamp.org/@grantbartel
  41. http://grantbartel.com/
  42. https://medium.freecodecamp.org/?source=footer_card
  43. https://medium.freecodecamp.org/?source=footer_card
  44. https://medium.freecodecamp.org/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://medium.com/p/4ecb83b39b4b/share/twitter
  48. https://medium.com/p/4ecb83b39b4b/share/facebook
  49. https://medium.com/p/4ecb83b39b4b/share/twitter
  50. https://medium.com/p/4ecb83b39b4b/share/facebook
