   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*4bdmlxpxmj7hfd37b2d3fw.jpeg]

automatic speaker recognition using id21

   go to the profile of christopher gill
   [16]christopher gill (button) blockedunblock (button) followfollowing
   dec 12, 2017

   project by christopher gill, hamza ghani, yousef abdelrazzaq, minkoo
   park

     when tasked with the challenge of creating a dynamic voice
     identifier, naturally the tool of choice for our team would be an
     image classifier

   even with today   s frequent technological breakthroughs in
   speech-interactive devices (think siri and alexa), few companies have
   tried their hand at enabling multi-user profiles. google home has been
   the most ambitious in this area, allowing up to six user profiles. the
   recent boom of this technology is what made the potential for this
   project very exciting to our team. we also wanted to engage in a
   project that is still a hot topic in deep-learning research, create
   interesting tools, learn more about neural network architectures, and
   make original contributions where possible.

   we sought to create a system able to quickly add user profiles and
   accurately identify their voices with very little training data, a few
   sentences as most! this learning from one to only a few samples is
   known as [17]one shot learning. this article will outline the phases of
   our project in detail.

i. project summary

   goal: classify speakers with minimal training such that only a few
   words or sentences are needed to achieve high levels of accuracy.

   data: training data was scraped from librivox, a source of open domain
   audiobooks. testing data was either scraped off youtube or collected
   live.

   method: in summary, we converted all of our audio data to spectrogram
   form. we then trained a id98 derived from cifar-10 on many speakers as a
   feature extractor to feed into an id166 for final classification. this
   approach is known as [18]id21. this approach enabled us to
   reap the small sample high performance of id166 and id171 of
   id98s.

   applications: the potential applications for our proposed systems are
   plentiful. they range from home assistant needs (think alexa and google
   home) to biometric security, marketing tools, and even spying
   (identifying high profile targets). it could also be used as a tool for
   [19]speaker diarisation in speech data collection. given some small
   previous exposure to included voices, an audio file with multiple
   speakers could be accurately separated. this opens the door for a lot a
   more potential    clean data    to be used to create more sophisticated
   speech-specific models.

   performance: results were largely positive. with 20   35 seconds of
   training audio, our model was able to distinguish between three
   speakers of with 63   95% accuracy in our tests. however, performance
   drops severely with 5+ speakers or in uniform gender test groups.

   github link: [20]https://github.com/hamzag95/voice-classification

ii. data collection

background

   one of the greatest challenges in the field of speaker and speech
   recognition is the lack of open source data. most speech data is either
   proprietary, hard to access, insufficiently labeled, insufficient in
   amount per speaker, or noisy. in many related research papers,
   insufficient data has been cited as reasoning for not pursuing further,
   more complex, models and applications.

   we saw this is an opportunity to make a novel contribution to research
   in this field. many hours of googling later, our team concluded the
   best sources of potential audio for our project would be [21]librivox,
   an extensive source of open domain audiobooks, and [22]youtube. these
   sources were selected upon best satisfying our criteria shown below.

audio source criteria

     * enough unique speakers for a model to learn generalizable
       differences in speech
     * male and female speakers, preferably in a range of languages
     * at least 1 hour of audio per speaker available
     * audio can be automatically labeled with meta-data
     * audio has minimal noise (little background noise/music, decent
       quality, few extraneous sounds)
     * open domain or creative commons license for legal use and data set
       usage

scraping audio from librivox

   we wrote scripts using beautifulsoup and selenium to parse the website
   librivox and download the audio books we wanted. beautifulsoup by
   itself was not enough, as parts of the website took time (1   2 seconds)
   to load. thus we used selenium to wait until certain elements on the
   webpage appeared and became scrapable.
   [0*d4rtqgxgwdfgp_pz.]

   our first attempt used a script that moves starts at librivox   s default
   home page and downloads all the audio in the page within a certain size
   file size boundaries. we later realized this was flawed as many
   audiobooks are actually a collaboration of multiple narrators that
   would be difficult to automatically separate. thus we had to find a way
   to get audiobooks with unique speakers. unfortunately, the librivox api
   didn   t contain a field to filter by project type (solo or
   collaboration) or narrator name.

   instead, we used advanced search to include only    solo    narrator books.
   soon we realized it was problematic to assume each book had a unique
   speaker as librivox has many repeat narrators. to fix this, we had to
   read the meta-data of each book to maintain a list of scraped narrators
   to ensure our data was correctly labeled. in the end, we had 6000+
   unique speakers and links to 24000+ hours of audio. however, due to
   time constraints, we sampled 162 unique speakers for spectrogram
   conversion. the full list of download links can be found on our project
   github [23]here.

scraping audio from youtube

   from youtube, we scraped video links from 7 youtube stars and their
   tutorial/informative videos. we found that tutorials tended to best fit
   our standards as they contain mostly clean speech. selenium was needed
   to automate the process as scraping youtube requires scrolling. this
   process can be seen in real-time in the video below.

   iframe: [24]/media/c3d12b802ab60f9b2cc0ee600290ce06?postid=6fab63e34e74

   scraping videos on youtube using selenium

   we didn   t scrape more profiles because it was inefficient to manually
   filter and verify videos based on their inclusion of guest speakers,
   music, etc   although tutorial channels generally fit the bill as far as
   speech cleanliness, they were heavily skewed in gender toward males.
   videos would also have varying qualities of audio and background noise.
   we decided against using what we collected to train the neural net, but
   thought they would be useful for testing purposes. youtube remains a
   source of audio with a lot of potential for data collection but very
   demanding in terms of dating verification and cleaning.

iii. data processing

background

   ultimately, all collected audio had to be converted to 503x800(x3)
   spectrogram images that captured 5 seconds of audio. the steps for
   converting the data collected from librivox and youtube were slightly
   different as a result of differing download formats.

   for our various processing needs, we were very fortunate to have tools
   such as [25]ffmpeg, [26]sox, and [27]mp3splt at our disposal that sped
   up the process while minimizing loss of audio quality.

   youttube audio processing

   once the youtube video links were collected, we were fortunate to find
   the [28]youtube-dl library which allowed us to easily download our
   desired videos in wav format. when attempting to convert this data to
   spectrograms, we came to find that each file was producing two
   spectrograms because it was stereo audio. this is the major difference
   we encountered versus processing of librivox audio.
   [1*ndobueeep1qaby07xuahiq.png]

   thus, the process can be summarized in the points below:
    1. manually check scraped youtube links to verify usability
    2. download in all verified links in wav format and automatically
       label/sort audio
    3. split mono wav files into 5 seconds segments
    4. convert all stereo wav files to mono wav
    5. convert all audio segments to spectrograms

   [0*-gewmohwci7kir58.]
   youtube data processing phases.

   a video of the youtube audio processing can be seen below:

   iframe: [29]/media/cc49bf9390dc2845700469c53face3fa?postid=6fab63e34e74

   librivox audio processing

   we processed the librivox audio using a single [30]script that placed
   the data in various levels of processing into different directories so
   that potential future users could change the segments lengths or
   conversion types as they please.

   the processing can be summarized in the points below:
    1. combine all downloaded chapters for a single speaker
    2. trim combined audio to desired length
    3. convert trimmed audio to 16bit 16khz mono wav using ffmpeg
    4. remove silences longer than .5 seconds
    5. split wav file in 5 second segments
    6. convert each segment to a spectrogram

   [0*4br80dovo-0ixvu2.]
   librivox data processing stages

iv. learning

our model

   we create a id98 by modifying an existing [31]cifar-10 architecture and
   train it on spectrograms from 57 unique speakers. using this trained
   neural network, we extract features by removing the last fully
   connected layer and feeding outputs of the flatten layer into an id166 in
   a process known as id21. there was no publicly available
   pre-trained model for voice classification so we create and train our
   own neural network.
   [0*ooq2kyyw-_ekqng1.]

id98 architecture

   the green layers in our architecture are convolutional layers whereas
   the blue layers max pooling. for all convolutional layers we use a 3x3
   kernel. for the max pooling we use a pool size of 2x2. we use relu
   id180 between each layer and a softmax activation
   function for the last layer. our id168 is categorical
   cross-id178.
   [0*lmullnsmswtsys2k.]
   modified cifar-10 architecture

id98 training

   when trained on 6 different people, the neural network is 97% accurate.
   each of the 6 people had about an hour of audio that the id98 was
   trained on. after our data scraping and processing is done for a larger
   dataset of 162 different speakers, we trained our neural network on 57
   different speakers due to time constraints on training and storage
   space on aws. we trained each of the 57 speakers on 45 min worth of
   audio ( ~2700 seconds). after one epoch our id98 is 97% accurate. the
   id98 took about an and hour and a half to train on ~24000 spectrogram
   images.
   [0*imeyd10uzzi8jnu8.]

id166 and id21

   we now have have a decent neural network at identifying 57 different
   people. we cut off the last layer which is a dense layer classifying 57
   people, and use the flatten layer to feed into an id166. id166s are
   supposed to perform well with smaller amounts of data (compared to a
   neural network) and with high dimensions. using our id98 as a feature
   extractor we have data in ~400,000 dimensions. we use a radial basis
   function as the kernel for the id166.

   using 35 seconds of audio for training on 3 different speakers and
   testing on 35 seconds results in 95% accuracy. feeding into the id166 we
   see that with 15 seconds training for each of 3 different speakers and
   testing on 15 seconds for each speaker, our id166 results in an accuracy
   of 83%. we see that we are able to learn someone   s voice in 15   20
   seconds now as opposed to 45 minutes of audio.

more examples and results

   all our examples will try to differentiate between three new voices.

   when we first tested the id166 we tested it on three youtubers with 7
   samples for each the training and test set. we had an accuracy of 95%.

   the indices correspond to a specific person. the array in this example
   is set up such;

   [ [32]christen dominique, [33]tushar, [34]sriraj raval]

   an output of 0 is christen (female) , 1 is tushar (male) and 2 is
   sriraj (male).
   [0*uxp8pzoezgyuuzmg.]

   we see here that the model never misclassifies christen, but classified
   sriraj as tushar for one sample. for future testing we tried to
   minimize number of samples for training to about 5 samples. the number
   of times the a person name occurs in the array is how many samples
   there were for the test set.

   for testing our program we made an interface where we record speakers
   and create a test and train set. we use this program to run live demos
   and test on real people, not just scraped data.

   additionally our classifier is language agnostic; it can recognize your
   voice independent of language. the order a name appears in the array is
   the index the classifier predicts as the person speaking. the first
   array seen is the prediction and the second array is the true speaker.

   below are the results of a few live tests of our model.

   when doing the live demo in our class, learning three people   s voices,
   two males and one female, we reached 63% accuracy. this was based on 5
   samples for training and 3 samples for testing. below is an example of
   the in class demo we did. (0     caramanis, 1     dimakis, 2     monica)
   [0*qxbfowoz3w0nkdxu.]
   in class live demo with our professors and teaching assistant with some
   mixed language

   another example consisted of two males and one female voice, where all
   switched between english and a respective different foreign language
   (spanish, arabic and urdu). our model was 90% accurate.
   [0*ayjhta9bcssyhfz0.]
   demo among friends mixing english and their native languages

   here is another example with 86% accuracy.
   [0*qcqg3o9pjykaa70t.]
   demo among friends in purely english

   results were generally positive! testing on groups of 3 with differing
   genders generally yields accuracy of 60   90%. however, our model does
   have limitations. performance declines when tested on groups of
   entirely the same gender or as group size increases. testing of groups
   of uniform genders generally yields accuracy of 40   60%. accuracy nears
   that of a random guess as group size surpasses 6.

v. conclusion

summary

   we wanted to create a model to identify speakers with only a few
   sentences of training data. we chose to approach this by using existing
   image classifying architectures, representing audio using spectrograms.
   this included a significant data collection component, leading us to
   create a dataset of 162 speakers included segmented audio files and
   segmented spectrograms. we chose to use take a id21
   approach, training a derivative of the cifar-10 id98, and extracting
   features to feed an id166 to classify new speakers. we restricted our
   training data to 20   35 seconds per person (4   7 samples). this method
   brought surprising levels of accuracy (60   90%) for groups of 3 with
   differing genders. results were less impressive for uniformly gendered
   groups, but consistently much better than random guesses.

contributions

   following the goals we set upon starting this project, our team
   successfully managed to make original contributions to this area of
   research. we managed to create an extremely large set of audio download
   links for unique speakers in a field where lack of open source data is
   a common hurdle to research projects. again, this list of links can be
   found [35]here. additionally, our approach of using image recognition
   in conjunction with id21 with an id166 for audio data has
   not been heavily explored. it is our hope that our architecture and
   methods may be useful to future research.

   note: link to full data set including audio and spectrograms coming
   soon   

future changes/improvements

   a lot of roadblocks for this project consisted of time and computer
   resources such as storage and computing power. below are possible
   future steps for our team or someone else wanting to improve what we
   have worked on

   first would be access to more storage so we can train our neural
   network on 4 hours of audio per person and use all 162 speakers we
   scraped. we believe that this will make for an even better feature
   extractor to feed into the id166.

   second, do some feature selection before feeding the features into the
   id166. even though id166s with nonlinear kernels are resistant to
   over-fitting, having so many features with so few samples may have
   resulted in over-fitting, which may explain the high variance in
   accuracy scores. with more time, we would have done more
   experimentation with the architecture of the neural network,
   specifically adding another dense layer before the classification
   layer. this would reduce the input to the id166 from ~400,000 features to
   whatever we set as the number of nodes in the new dense layer. another
   architecture to explore would be basing a model on vgg19.

   third, would be to scrape more speakers. we are able to scrape 6000+
   unique speakers from librivox, although this takes time to download and
   preprocess data and a ridiculous amount of storage. we would try to
   scrape about 500   1000 and use about 30   45 min. of audio per person to
   see how this improves our feature extractor. this would take a lot of
   time to train. for reference 57 speakers with 45 min. of audio took an
   hour and a half to train.

   thanks to [36]isabel cachola.
     * [37]machine learning
     * [38]id21
     * [39]id103
     * [40]neural networks
     * [41]towards data science

   (button)
   (button)
   (button) 2.2k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of christopher gill

[42]christopher gill

   medium member since jan 2019

   undergraduate researcher at the university of texas at austin with a
   deep interest in machine learning, software, and math.

     (button) follow
   [43]towards data science

[44]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.2k
     * (button)
     *
     *

   [45]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [46]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6fab63e34e74
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/automatic-speaker-recognition-using-transfer-learning-6fab63e34e74&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/automatic-speaker-recognition-using-transfer-learning-6fab63e34e74&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_qct1pu34oify---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@christophergill_91238
  17. https://en.wikipedia.org/wiki/one-shot_learning
  18. http://ruder.io/transfer-learning/
  19. https://en.wikipedia.org/wiki/speaker_diarisation
  20. https://github.com/hamzag95/voice-classification
  21. https://librivox.org/search?primary_key=0&search_category=author&search_page=1&search_form=get_results
  22. https://www.youtube.com/
  23. https://github.com/hamzag95/voice-classification/blob/master/data_collection/download_links.txt
  24. https://towardsdatascience.com/media/c3d12b802ab60f9b2cc0ee600290ce06?postid=6fab63e34e74
  25. https://www.ffmpeg.org/
  26. https://github.com/chirlu/sox
  27. http://mp3splt.sourceforge.net/mp3splt_page/home.php
  28. https://github.com/rg3/youtube-dl
  29. https://towardsdatascience.com/media/cc49bf9390dc2845700469c53face3fa?postid=6fab63e34e74
  30. https://github.com/hamzag95/voice-classification/blob/master/data_collection/audiobook_dataprocessing.ipynb
  31. https://github.com/hamzag95/keras/blob/master/examples/cifar10_id98.py
  32. https://www.youtube.com/user/christendominique
  33. https://www.youtube.com/user/tusharroy2525
  34. https://www.youtube.com/channel/ucwn3xxrkmtpmbkwht9fue5a
  35. https://github.com/hamzag95/voice-classification/blob/master/data_collection/download_links.txt
  36. https://medium.com/@isabelcachola?source=post_page
  37. https://towardsdatascience.com/tagged/machine-learning?source=post
  38. https://towardsdatascience.com/tagged/transfer-learning?source=post
  39. https://towardsdatascience.com/tagged/speech-recognition?source=post
  40. https://towardsdatascience.com/tagged/neural-networks?source=post
  41. https://towardsdatascience.com/tagged/towards-data-science?source=post
  42. https://towardsdatascience.com/@christophergill_91238
  43. https://towardsdatascience.com/?source=footer_card
  44. https://towardsdatascience.com/?source=footer_card
  45. https://towardsdatascience.com/
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://towardsdatascience.com/@christophergill_91238?source=post_header_lockup
  49. https://medium.com/p/6fab63e34e74/share/twitter
  50. https://medium.com/p/6fab63e34e74/share/facebook
  51. https://towardsdatascience.com/@christophergill_91238?source=footer_card
  52. https://medium.com/p/6fab63e34e74/share/twitter
  53. https://medium.com/p/6fab63e34e74/share/facebook
