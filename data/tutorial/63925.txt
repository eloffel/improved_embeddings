   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]ml review
     * [9]       contribute
     * [10]            review.com newsletter
     __________________________________________________________________

   [1*xmgio5lq90ejrogopcujfw.png]

modern theory of deep learning: why does it work so well

what can we learn from the latest research on the paradoxical effectiveness
of deep learning    alchemy   .

   [11]go to the profile of dmytrii s. shchadei
   [12]dmytrii s. shchadei (button) blockedunblock (button)
   followfollowing
   dec 20, 2017

motivation

   deep learning is currently being used for a variety of different
   applications. but is frequently criticised for lacking a fundamental
   theory that can fully answer why does it work so well. it   s only
   recently that the winner of the test-of-time award at the conference on
   neural information processing (nips) [13]compared deep learning to
   alchemy.

   although generalization theory that explains why deep learning
   generalizes so well is an open problem, in this article we would
   discuss most recent theoretical and empirical advances in the field
   that attempt to explain it.

the paradox of deep learning

   an    apparent paradox    with deep learning is that it can generalize well
   in practice despite its large capacity, numerical instability, sharp
   minima, and nonrobustness.

   in a recent paper [14]   understanding deep learning requires rethinking
   generalization    it was shown that deep neural networks (dnn) has enough
   capacity to memorize id163 and cifar10 datasets with random labels.
   and it is unclear why they find generalizable solutions on real data.

   another important issue with deep architectures is numerical
   instabilities. numerical instabilities in derivative-based learning
   algorithms are commonly called exploding or [15]vanishing gradients.
   additional difficulties stem from instabilities of the underlying
   forward model. that is the output of some networks can be unstable with
   respect to small perturbations in the original features. in machine
   learning it is called [16]non-robustness. an example of it is [17]an
   adversarial attack illustrated in figure 1.
   [1*d39by8gonrgyg9blyhkonq.png]
   figure 1: source: [18]attacking machine learning with adversarial
   examples.

   several studies based their arguments for generalization in deep
   learning on the flatness of minima of the id168 found by
   stochastic id119 (sgd). however, recently it was shown that
      [19]sharp minima can generalize for deep nets    as well. more
   specifically, flat minima can be turned into sharp minima via
   re-parameterization without changing the generalization. consequently,
   generalization can not be explained merely by the robustness of
   parameter space only.

generalization theory

   the goal of generalization theory is to explain and justify why and how
   improving accuracy on a training set improves accuracy on a test set.
   the difference between these two accuracies is called generalization
   error or    generalization gap   . more rigorously generalization gap can
   be defined as a difference between the non-computable expected risk and
   the computable empirical risk of a function f on a dataset sm given a
   learning algorithm a:
   [1*usthe2pxszoatkpwnbr4qq.png]

   essentially if we bound the generalization gap with a small value it
   would guarantee that deep learning algorithm f generalizes well in
   practice. multiple theoretical bounds exist for generalization gap
   based on model complexity, stability, robustness etc.

   two measures of model complexity are [20]rademacher complexity and
   [21]vapnik   chervonenkis (vc) dimension. unfortunately, for the deep
   learning function f known bounds based on radamacher complexity grow
   exponentially in depth of dnn. which contradicts practical observation
   that deeper nets fit better to the training data and achieve smaller
   empirical errors. similarly, the generalization gap bound based on vc
   dimension grows linearly in the number of trainable parameters and
   cannot account for the practical observations with deep learning. in
   other words, these two bounds are too conservative.

   a much more useful approach was recently proposed by [22]k kawaguchi,
   lp kaelbling, and y bengio. unlike others, they embrace the fact that
   usually deep learning models are trained using train-validation
   paradigm. instead of bounding the non-computable expected risk with
   train error, they use validation error. in this view, they propose the
   following intuition on why deep learning generalizes well in practice:
      we can generalize well because we can obtain a good model via model
   search with validation errors   . and prove the following bound for any   
   > 0, with id203 at least 1       :
   [1*480uxt83upl8hriobivxkg.png]

   importantly |fval| is the number of times we use the validation dataset
   in our decision making to choose a final model, m is the validation set
   size. this bound explains why deep learning had the ability to
   generalize well, despite possible nonstability, nonrobustness and sharp
   minimas. somewhat an open question still remains why we are able to
   find architectures and parameters that result in low validation errors.
   usually, architectures are inspired by real-world observations and good
   parameters are searched using sgd that we would discuss next.

stochastic id119

   sgd is an inherent part of modern deep learning and apparently one of
   the main reasons behind its generalization. so we would discuss its
   generalization properties next.

   in a recent paper    [23]data-dependent stability of stochastic gradient
   descent    authors were able to prove that sgd is on-average stable
   algorithm under some additional loss conditions. these conditions are
   fulfilled in commonly used id168s such as logistic/softmax
   losses in neural nets with sigmoid activations. stability, in this
   case, means how sensitive is sgd to small perturbations in the training
   set. they take it further to prove a data-dependant on-average bound
   for generalization gap of sgd in non-convex functions such as deep
   neural nets:
   [1*zv96ln8wmrswiwvybymb6a.png]

   where m is a training set size, t number of training steps and   
   characterizes how the curvature at the initialization point affects
   stability. which lead to at-least two conclusions. first, the curvature
   of the objective function around the initialization point has crucial
   influence. starting from a point in a less curved region with low risk
   should yield higher stability, i.e faster generalization. in practice
   it can be a good pre-screen strategy for picking good initialization
   params. second, considering the full pass, that is m = o(t), we
   simplify the bound to o(m     ). that is the bigger training set, the
   smaller generalization gap.

   interestingly, there is a number of studies investigating learning
   curves. most of them show power-law generalization error, scaling as
     (m)     m    with exponent    =    0.5 or    1. which is also aligned with the
   previously discussed paper. however, it   s important to mention [24]the
   large-scale research by baidu that was able to empirically observe this
   power-law (see figure 2). however, the exponent    in real applications
   was between   0.07 and    0.35. which has to be still explained by theory.
   [1*ouvxgl7wkyv4sloaviqg9g.png]
   figure 2. source: [25]deep learning scaling is predictable, empirically

   additionally, there is both theoretical and empirical evidence of batch
   size effect on generalization of sgd. intuitively a small batch
   training introduces noise to the gradients, and this noise drives the
   sgd away from sharp minima, thus enhancing generalization. in a
   [26]recent paper from google it was shown that the optimum batch size
   is proportional to the learning rate and the training set size. or
   simply put in other words, [27]   don   t decay the learning rate, increase
   the batch size   . similarly scaling rules were derived for sgd with
   momentum: bopt ~1/(1     m), where bopt is an optimal batch size and m is
   momentum. alternatively, all conclusions can be summarised by the
   following equation:
   [1*gp2qjvuhg2aie0ye9cznyq.png]

   where      is learning rate, n is training set size, m is momentum and b
   is batch size.

conclusions

   in the last few years, there has been an increasingly growing interest
   in the fundamental theory behind the paradoxical effectiveness of deep
   learning. and although there are still open research problems, modern
   deep learning by any means is far from being called alchemy. in this
   article we discussed the generalization perspective on the problem
   which leads us to a number of practical conclusions:
     * select initialization params in a less curved region and lower
       risk. curvature can be efficiently estimated by [28]hessian-vector
       multiplication method.
     * rescale the batch size, when changing momentum.
     * don   t decay the learning rate, increase the batch size.

   don   t forget to clap and share the article if you found it interesting.
   you may also reach me on [29]linkedin and [30]twitter.

     * [31]machine learning
     * [32]artificial intelligence
     * [33]neural networks
     * [34]deep learning

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of dmytrii s. shchadei

[36]dmytrii s. shchadei

   machine learning engineer at [37]@amazon, former [38]@here [39]@nokia,
   [40]@yandex. creator of telefile.me mlreview.com superwised.com

     (button) follow
   [41]ml review

[42]ml review

   highlights from machine learning research, projects and learning
   materials. from and for ml scientists, engineers an enthusiasts.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [43]ml review
   never miss a story from ml review, when you sign up for medium.
   [44]learn more
   never miss a story from ml review
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9ee1f7fb2808
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/mlreview/modern-theory-of-deep-learning-why-does-it-works-so-well-9ee1f7fb2808&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/mlreview/modern-theory-of-deep-learning-why-does-it-works-so-well-9ee1f7fb2808&source=--------------------------nav_reg&operation=register
   8. https://medium.com/mlreview?source=logo-lo_tmnd0u4nagyw---33272dbbd858
   9. https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d
  10. http://mlreview.com/?medium
  11. https://medium.com/@metrofun?source=post_header_lockup
  12. https://medium.com/@metrofun
  13. https://youtu.be/qi1yry33tqe?t=11m59s
  14. https://arxiv.org/abs/1611.03530
  15. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  16. https://en.wikipedia.org/wiki/robustness
  17. https://blog.openai.com/adversarial-example-research/
  18. https://blog.openai.com/adversarial-example-research/
  19. https://arxiv.org/abs/1703.04933
  20. https://en.wikipedia.org/wiki/rademacher_complexity
  21. https://en.wikipedia.org/wiki/vc_dimension
  22. https://arxiv.org/abs/1710.05468
  23. https://arxiv.org/abs/1703.01678
  24. https://arxiv.org/abs/1712.00409
  25. https://arxiv.org/abs/1712.00409
  26. https://arxiv.org/abs/1710.06451
  27. https://arxiv.org/abs/1711.00489
  28. http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf
  29. https://www.linkedin.com/in/dmytrii
  30. https://twitter.com/metrofun
  31. https://medium.com/tag/machine-learning?source=post
  32. https://medium.com/tag/artificial-intelligence?source=post
  33. https://medium.com/tag/neural-networks?source=post
  34. https://medium.com/tag/deep-learning?source=post
  35. https://medium.com/@metrofun?source=footer_card
  36. https://medium.com/@metrofun
  37. http://twitter.com/amazon
  38. http://twitter.com/here
  39. http://twitter.com/nokia
  40. http://twitter.com/yandex
  41. https://medium.com/mlreview?source=footer_card
  42. https://medium.com/mlreview?source=footer_card
  43. https://medium.com/mlreview
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/9ee1f7fb2808/share/twitter
  47. https://medium.com/p/9ee1f7fb2808/share/facebook
  48. https://medium.com/p/9ee1f7fb2808/share/twitter
  49. https://medium.com/p/9ee1f7fb2808/share/facebook
