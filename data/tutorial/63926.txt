   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    id21 & the art of using pre-trained
   models in deep learning comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]id21 & the art of using
   pre-trained models in deep learning

   [95]deep learning[96]python[97]uncategorized

id21 & the art of using pre-trained models in deep learning

   [98]dishashree gupta, june 1, 2017

introduction

   neural networks are a different breed of models compared to the
   supervised machine learning algorithms. why do i say so? there are
   multiple reasons for that, but the most prominent is the cost of
   running algorithms on the hardware.

   in today   s world, ram on a machine is cheap and is available in plenty.
   you need hundreds of gbs of ram to run a super complex supervised
   machine learning problem     it can be yours for a little investment /
   rent. on the other hand, access to gpus is not that cheap. you need
   access to hundred gb vram on gpus     it won   t be straight forward and
   would involve significant costs.

   now, that may change in future. but for now, it means that we have to
   be smarter about the way we use our resources in solving deep learning
   problems. especially so, when we try to solve complex real life
   problems on areas like image and voice recognition. once you have a few
   hidden layers in your model, adding another layer of hidden layer would
   need immense resources.

   thankfully, there is something called    id21    which enables
   us to use pre-trained models from other people by making small changes.
   in this article, i am going to tell how we can use pre-trained models
   to accelerate our solutions.

   note     this article assumes basic familiarity with neural networks and
   deep learning. if you are new to deep learning, i would strongly
   recommend that you read the following articles first:
    1. [99]what is deep learning and why is it getting so much attention?
    2. [100]deep learning vs. machine learning     the essential differences
       you need to know!
    3. [101]25 must know terms & concepts for beginners in deep learning
    4. [102]why are gpus necessary for training deep learning models?


table of contents

    1. what is id21?
    2. what is a pre-trained model?
    3. why would we use pre-trained models?     a real life example
    4. how can i use pre-trained models?
          + extract features
          + fine tune the model
    5. ways to fine tune your model
    6. use the pre-trained model for identifying digits
          + retraining the output dense layers only
          + freeze the weights of first few layers


what is id21?

   let us start with developing an intuition for id21. let us
   understand from a simple teacher     student analogy.

   a teacher has years of experience in the particular topic he/she
   teaches. with all this accumulated information, the lectures that
   students get is a concise and brief overview of the topic. so it can be
   seen as a    transfer    of information from the learned to a novice.

   keeping in mind this analogy, we compare this to neural network. a
   neural network is trained on a data. this network gains knowledge from
   this data, which is compiled as    weights    of the network. these weights
   can be extracted and then transferred to any other neural network.
   instead of training the other neural network from scratch, we
      transfer    the learned features.

   now, let us reflect on the importance of id21 by relating
   to our evolution. and what better way than to use id21 for
   this! so i am picking on a concept touched on by tim urban from one of
   his recent articles on waitbutwhy.com

   tim explains that before language was invented, every generation of
   humans had to re-invent the knowledge for themselves and this is how
   knowledge growth was happening from one generation to other:

   then, we invented language! a way to id21 from one
   generation to another and this is what happened over same time frame:

   isn   t it phenomenal and super empowering? so, id21 by
   passing on weights is equivalent of language used to disseminate
   knowledge over generations in human evolution.


what is a pre-trained model?

   simply put, a pre-trained model is a model created by some one else to
   solve a similar problem. instead of building a model from scratch to
   solve a similar problem, you use the model trained on other problem as
   a starting point.

   for example, if you want to build a self learning car. you can spend
   years to build a decent image recognition algorithm from scratch or you
   can take inception model (a pre-trained model) from google which was
   built on id163 data to identify images in those pictures.

   a pre-trained model may not be 100% accurate in your application, but
   it saves huge efforts required to re-invent the wheel. let me show this
   to you with a recent example.


why would we use pre-trained models?

   i spent my last week working on a problem at [103]crowdanalytix
   platform     identifying themes from mobile case images. this was an
   image classification problem where we were given 4591 images in the
   training dataset and 1200 images in the test dataset. the objective was
   to classify the images into one of the 16 categories. after the basic
   pre-processing steps, i started off with a simple [104]mlp model with
   the following architecture-

   to simplify the above architecture after flattening the input image
   [224 x 224 x 3] into [150528], i used three hidden layers with 500, 500
   and 500 neurons respectively. the output layer had 16 neurons which
   correspond to the number of categories in which we need to classify the
   input image.

   i barely managed a training accuracy of 6.8 % which turned out to be
   very bad. even experimenting with hidden layers, number of neurons in
   hidden layers and drop out rates. i could not manage to substantially
   increase my training accuracy. increasing the hidden layers and the
   number of neurons, caused 20 seconds to run a single epoch on my titan
   x gpu with 12 gb vram.

   below is an output of the training using the mlp model with the above
   architecture.

   epoch 10/10

   50/50 [==============================]     21s     loss: 15.0100     acc:
   0.0688

   as, you can see mlp was not going to give me any better results without
   exponentially increasing my training time. so i switched to
   convolutional neural network to see how they perform on this dataset
   and whether i would be able to increase my training accuracy.

   the id98 had the below architecture    

   i used 3 convolutional blocks with each block following the below
   architecture-
    1. 32 filters of size 5 x 5
    2. activation function     relu
    3. max pooling layer of size 4 x 4

   the result obtained after the final convolutional block was flattened
   into a size [256] and passed into a single hidden layer of with 64
   neurons. the output of the hidden layer was passed onto the output
   layer after a drop out rate of 0.5.

   the result obtained with the above architecture is summarized below-

   epoch 10/10

   50/50 [==============================]     21s     loss: 13.5733     acc:
   0.1575


   though my accuracy increased in comparison to the mlp output, it also
   increased the time taken to run a single epoch     21 seconds.

   but the major point to note was that the majority class in the dataset
   was around 17.6%. so, even if we had predicted the class of every image
   in the train dataset to be the majority class, we would have performed
   better than mlp and id98 respectively. addition of more convolutional
   blocks substantially increased my training time. this led me to switch
   onto using pre-trained models where i would not have to train my entire
   architecture but only a few layers.

   so, i used vgg16 model which is pre-trained on the id163 dataset and
   provided in the keras library for use. below is the architecture of the
   vgg16 model which i used.

   the only change that i made to the vgg16 existing architecture is
   changing the softmax layer with 1000 outputs to 16 categories suitable
   for our problem and re-training the dense layer.

   this architecture gave me an accuracy of 70% much better than mlp and
   id98. also, the biggest benefit of using the vgg16 pre-trained model was
   almost negligible time to train the dense layer with greater accuracy.

   so, i moved forward with this approach of using a pre-trained model and
   the next step was to fine tune my vgg16 model to suit this problem.


how can i use pre-trained models?

   what is our objective when we train a neural network? we wish to
   identify the correct weights for the network by multiple forward and
   backward iterations. by using pre-trained models which have been
   previously trained on large datasets, we can directly use the weights
   and architecture obtained and apply the learning on our problem
   statement. this is known as id21. we    transfer the
   learning    of the pre-trained model to our specific problem statement.

   you should be very careful while choosing what pre-trained model you
   should use in your case. if the problem statement we have at hand is
   very different from the one on which the pre-trained model was trained
       the prediction we would get would be very inaccurate. for example, a
   model previously trained for id103 would work horribly if
   we try to use it to identify objects using it.

   we are lucky that many pre-trained architectures are directly available
   for us in the keras library. id163 data set has been widely used to
   build various architectures since it is large enough (1.2m images) to
   create a generalized model. the problem statement is to train a model
   that can correctly classify the images into 1,000 separate object
   categories. these 1,000 image categories represent object classes that
   we come across in our day-to-day lives, such as species of dogs, cats,
   various household objects, vehicle types etc.

   these pre-trained networks demonstrate a strong ability to generalize
   to images outside the id163 dataset via id21. we make
   modifications in the pre-existing model by fine-tuning the model. since
   we assume that the pre-trained network has been trained quite well, we
   would not want to modify the weights too soon and too much. while
   modifying we generally use a learning rate smaller than the one used
   for initially training the model.


ways to fine tune the model

    1. feature extraction     we can use a pre-trained model as a feature
       extraction mechanism. what we can do is that we can remove the
       output layer( the one which gives the probabilities for being in
       each of the 1000 classes) and then use the entire network as a
       fixed feature extractor for the new data set.
    2. use the architecture of the pre-trained model     what we can do is
       that we use architecture of the model while we initialize all the
       weights randomly and train the model according to our dataset
       again.
    3. train some layers while freeze others     another way to use a
       pre-trained model is to train is partially. what we can do is
       we keep the weights of initial layers of the model frozen while we
       retrain only the higher layers. we can try and test as to how many
       layers to be frozen and how many to be trained.

   the below diagram should help you decide on how to proceed on using the
   pre trained model in your case    

   scenario 1     size of the data set is small while the data similarity is
   very high     in this case, since the data similarity is very high, we do
   not need to retrain the model. all we need to do is to customize and
   modify the output layers according to our problem statement. we use the
   pretrained model as a feature extractor. suppose we decide to use
   models trained on id163 to identify if the new set of images have
   cats or dogs. here the images we need to identify would be similar to
   id163, however we just need two categories as my output     cats or
   dogs. in this case all we do is just modify the dense layers and the
   final softmax layer to output 2 categories instead of a 1000.

   scenario 2     size of the data is small as well as data similarity is
   very low     in this case we can freeze the initial (let   s say k) layers
   of the pretrained model and train just the remaining(n-k) layers again.
   the top layers would then be customized to the new data set. since the
   new data set has low similarity it is significant to retrain and
   customize the higher layers according to the new dataset.  the small
   size of the data set is compensated by the fact that the initial layers
   are kept pretrained(which have been trained on a large dataset
   previously) and the weights for those layers are frozen.

   scenario 3     size of the data set is large however the data similarity
   is very low     in this case, since we have a large dataset, our neural
   network training would be effective. however, since the data we have is
   very different as compared to the data used for training our pretrained
   models. the predictions made using pretrained models would not be
   effective. hence, its best to train the neural network from scratch
   according to your data.

   scenario 4     size of the data is large as well as there is high data
   similarity     this is the ideal situation. in this case the pretrained
   model should be most effective. the best way to use the model is to
   retain the architecture of the model and the initial weights of the
   model. then we can retrain this model using the weights as initialized
   in the pre-trained model.


use the pre-trained models to identify handwritten digits

   let   s now try to use a pretrained model for a simple problem. there are
   various architectures that have been trained on the id163 data set.
   you can go through various architectures [105]here. i have used vgg16
   as pretrained model architecture and have tried to identify handwritten
   digits using it. let   s see in which of the above scenarios would this
   problem fall into. we have around 60,000 training images of handwritten
   digits. this data set is definitely small. so the situation would
   either fall into scenario 1 or scenario 2. we shall try to solve the
   problem using both these scenarios. the data set can be downloaded from
   [106]here.
    1. retrain the output dense layers only     here we use vgg16 as a
       feature extractor. we then use these features and send them to
       dense layers which are trained according to our data set. the
       output layer is also replaced with our new softmax layer relevant
       to our problem. the output layer in a vgg16 is a softmax activation
       with 1000 categories. we remove this layer and replace it with a
       softmax layer of 10 categories. we just train the weights of these
       layers and try to identify the digits.

   # importing required libraries

   from keras.models import sequential
   from scipy.misc import imread
   get_ipython().magic('matplotlib inline')
   import matplotlib.pyplot as plt
   import numpy as np
   import keras
   from keras.layers import dense
   import pandas as pd

   from keras.applications.vgg16 import vgg16
   from keras.preprocessing import image
   from keras.applications.vgg16 import preprocess_input
   import numpy as np
   from keras.applications.vgg16 import decode_predictions
   train=pd.read_csv("r/data/train/train.csv")
   test=pd.read_csv("r/data/test.csv")
   train_path="r/data/train/images/train/"
   test_path="r/data/train/images/test/"

   from scipy.misc import imresize
   # preparing the train dataset

   train_img=[]
   for i in range(len(train)):


   temp_img=image.load_img(train_path+train['filename'][i],target_size=(22
   4,224))

       temp_img=image.img_to_array(temp_img)

       train_img.append(temp_img)

   #converting train images to array and applying mean subtraction
   processing

   train_img=np.array(train_img)
   train_img=preprocess_input(train_img)
   # applying the same procedure with the test dataset

   test_img=[]
   for i in range(len(test)):


   temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,
   224))

       temp_img=image.img_to_array(temp_img)

       test_img.append(temp_img)

   test_img=np.array(test_img)
   test_img=preprocess_input(test_img)

   # loading vgg16 model weights
   model = vgg16(weights='id163', include_top=false)
   # extracting features from the train dataset using the vgg16
   pre-trained model

   features_train=model.predict(train_img)
   # extracting features from the train dataset using the vgg16
   pre-trained model

   features_test=model.predict(test_img)

   # flattening the layers to conform to mlp input

   train_x=features_train.reshape(49000,25088)
   # converting target variable to array

   train_y=np.asarray(train['label'])
   # performing one-hot encoding for the target variable

   train_y=pd.get_dummies(train_y)
   train_y=np.array(train_y)
   # creating training and validation set

   from sklearn.model_selection import train_test_split
   x_train, x_valid, y_train,
   y_valid=train_test_split(train_x,train_y,test_size=0.3,
   random_state=42)


   # creating a mlp model
   from keras.layers import dense, activation
   model=sequential()

   model.add(dense(1000, input_dim=25088,
   activation='relu',kernel_initializer='uniform'))
   keras.layers.core.dropout(0.3, noise_shape=none, seed=none)

   model.add(dense(500,input_dim=1000,activation='sigmoid'))
   keras.layers.core.dropout(0.4, noise_shape=none, seed=none)

   model.add(dense(150,input_dim=500,activation='sigmoid'))
   keras.layers.core.dropout(0.2, noise_shape=none, seed=none)

   model.add(dense(units=10))
   model.add(activation('softmax'))

   model.compile(loss='categorical_crossid178', optimizer="adam",
   metrics=['accuracy'])

   # fitting the model

   model.fit(x_train, y_train, epochs=20,
   batch_size=128,validation_data=(x_valid,y_valid))


   2. freeze the weights of first few layers     here what we do is we
   freeze the weights of the first 8 layers of the vgg16 network, while we
   retrain the subsequent layers. this is because the first few layers
   capture universal features like curves and edges that are also relevant
   to our new problem. we want to keep those weights intact and we will
   get the network to focus on learning dataset-specific features in the
   subsequent layers.

   code for freezing the weights of first few layers.

   from keras.models import sequential
   from scipy.misc import imread
   get_ipython().magic('matplotlib inline')
   import matplotlib.pyplot as plt
   import numpy as np
   import keras
   from keras.layers import dense
   import pandas as pd

   from keras.applications.vgg16 import vgg16
   from keras.preprocessing import image
   from keras.applications.vgg16 import preprocess_input
   import numpy as np
   from keras.applications.vgg16 import decode_predictions
   from keras.utils.np_utils import to_categorical

   from sklearn.preprocessing import labelencoder
   from keras.models import sequential
   from keras.optimizers import sgd
   from keras.layers import input, dense, convolution2d, maxpooling2d,
   averagepooling2d, zeropadding2d, dropout, flatten, merge, reshape,
   activation

   from sklearn.metrics import log_loss

   train=pd.read_csv("r/data/train/train.csv")
   test=pd.read_csv("r/data/test.csv")
   train_path="r/data/train/images/train/"
   test_path="r/data/train/images/test/"

   from scipy.misc import imresize

   train_img=[]
   for i in range(len(train)):


   temp_img=image.load_img(train_path+train['filename'][i],target_size=(22
   4,224))

       temp_img=image.img_to_array(temp_img)

       train_img.append(temp_img)

   train_img=np.array(train_img)
   train_img=preprocess_input(train_img)

   test_img=[]
   for i in range(len(test)):

   temp_img=image.load_img(test_path+test['filename'][i],target_size=(224,
   224))

       temp_img=image.img_to_array(temp_img)

       test_img.append(temp_img)

   test_img=np.array(test_img)
   test_img=preprocess_input(test_img)

   from keras.models import model

   def vgg16_model(img_rows, img_cols, channel=1, num_classes=none):

       model = vgg16(weights='id163', include_top=true)

       model.layers.pop()

       model.outputs = [model.layers[-1].output]

       model.layers[-1].outbound_nodes = []

             x=dense(num_classes, activation='softmax')(model.output)

       model=model(model.input,x)

   #to set the first 8 layers to non-trainable (weights will not be
   updated)

             for layer in model.layers[:8]:

          layer.trainable = false

   # learning rate is changed to 0.001
       sgd = sgd(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=true)
       model.compile(optimizer=sgd, loss='categorical_crossid178',
   metrics=['accuracy'])

       return model

   train_y=np.asarray(train['label'])

   le = labelencoder()

   train_y = le.fit_transform(train_y)

   train_y=to_categorical(train_y)

   train_y=np.array(train_y)

   from sklearn.model_selection import train_test_split
   x_train, x_valid, y_train,
   y_valid=train_test_split(train_img,train_y,test_size=0.2,
   random_state=42)

   # example to fine-tune on 3000 samples from cifar10

   img_rows, img_cols = 224, 224 # resolution of inputs
   channel = 3
   num_classes = 10
   batch_size = 16
   nb_epoch = 10

   # load our model
   model = vgg16_model(img_rows, img_cols, channel, num_classes)

   model.summary()
   # start fine-tuning
   model.fit(x_train,
   y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=true,verbose=1,va
   lidation_data=(x_valid, y_valid))

   # make predictions
   predictions_valid = model.predict(x_valid, batch_size=batch_size,
   verbose=1)

   # cross-id178 loss score
   score = log_loss(y_valid, predictions_valid)


projects

   now, its time to take the plunge and actually play with some other real
   datasets. so are you ready to take on the challenge? accelerate your
   deep learning journey with the following practice problems:
   [107]practice problem: identify the apparels identify the type of
   apparel for given images
   [108]practice problem: identify the digits identify the digit in given
   images

end notes

   i hope that you would now be able to apply pre-trained models to your
   problem statements. be sure that the pre-trained model you have
   selected has been trained on a similar data set as the one that you
   wish to use it on. there are various architectures people have tried on
   different types of data sets and i strongly encourage you to go through
   these architectures and apply them on your own problem statements.
   please feel free to discuss your doubts and concerns in the comments
   section.
   you can also read this article on analytics vidhya's android app
   [109]get it on google play

share this:

     * [110]click to share on linkedin (opens in new window)
     * [111]click to share on facebook (opens in new window)
     * [112]click to share on twitter (opens in new window)
     * [113]click to share on pocket (opens in new window)
     * [114]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [115]deep learning, [116]deep learning in python,
   [117]inception, [118]keras, [119]parameter tuning, [120]pre-trained
   models, [121]vgg16
   next article

data science evangelist- gurgaon (2 to 3 years of experience)

   previous article

data scientist -mumbai (1-4 years of experience)

[122]dishashree gupta

   dishashree is passionate about statistics and is a machine learning
   enthusiast. she has an experience of 1.5 years of market research using
   r, advanced excel, azure ml.

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [123]discussion portal to get your queries resolved

26 comments

     * ashish mokalkar says:
       [124]june 1, 2017 at 11:33 am
       hey! you havent given folder structure and format in which training
       data is stored. people wishing to try out this example may not be
       having idea where to download training data from and where to place
       it. it would elucidate them if you update the same.
       [125]reply
          + dishashree gupta says:
            [126]june 3, 2017 at 10:59 am
            i have added the link for the download of mnist data.
            [127]reply
     * shamane says:
       [128]june 1, 2017 at 11:44 pm
       perfect
       [129]reply
     * gutchapa says:
       [130]june 3, 2017 at 6:31 am
       same here, what does the csv files contain? please explain why the
       train and test data was loaded twice esp. why it is required to
       load while freezing the weights?
       appreciate your response..
       [131]reply
          + dishashree gupta says:
            [132]june 3, 2017 at 11:03 am
            so i have added the link of mnist data. the csv file for train
            consists the name of images and the corresponding digits for
            them.
            the test and train files have been loaded just because i
            wanted to keep the two codes exclusive.
            [133]reply
               o [134][email protected] says:
                 [135]july 7, 2017 at 6:14 pm
                 hi,
                 your presentation is very nice. but my case is that i
                 just begin to deal with image classification with id98. i
                 know how to convert mnist data to csv file since it   s a
                 1-d image.
                 i really need help to know how to convert a 3-dimensional
                 images into csv file.
                 thanks,
                 [136]reply
                    # sujit nalawade says:
                      [137]may 17, 2018 at 7:35 am
                      @b.diaby248 : basically each colored image consist
                      of 3 channels( red, green and blue). in case of
                      grayscale image it consist of only one channel. we
                      put values of all pixels per channel basis in single
                      row of csv. for ex: if i have 28x28x3 image where 28
                      are height and width and 3 represents channels(rgb)
                      then we would put (28x28) red, (28x28)green and
                      (28x28) blue channel values along a single row of
                      csv file for single image. hope it helps.
                      [138]reply
     * kai says:
       [139]june 6, 2017 at 4:00 pm
       thanks for your sharing.
       quick question:
       1. can we substract the data from the mean of our data for each
       channel instead of from the mean of vgg16 data? what is the
       difference for this two preprocess?
       2. what if i want to train by grayscale image?
       [140]reply
     * kai says:
       [141]june 8, 2017 at 10:25 am
       hi!
       should we apply the same preprocess function from the network we
       transfer on our data?
       i.e. substract training data from the mean of our data or the mean
       of transfer network   s data?
       [142]reply
          + dishashree gupta says:
            [143]june 9, 2017 at 6:58 pm
            hi kai,
            so the preprocess function is subtracting the mean of your own
            dataset rather than the data of the pretrained.
            it doesn   t mean much to actually subtract the mean of the
            network   s data since it might be very different than your own.
            [144]reply
     * rico says:
       [145]june 8, 2017 at 4:25 pm
       dear dishashree.
       when trying to apply this to my own dataset, the program throws the
       error valueerror: cannot reshape array of size 5143040 into shape
       (49000,25088).
       i guess this is because of the size difference in the datasets.
       can you explain what the numbers 49000 and 25088 mean?
       best regards
       [146]reply
          + dishashree gupta says:
            [147]june 9, 2017 at 6:56 pm
            hi rico,
            the numbers 49000 and 25088 are the dimensions of your data
            set. so you would have 49000 records with each record having
            25088 features. you should check your reshaping function.
            since the error lies there !
            [148]reply
     * sudipto banerjee says:
       [149]june 14, 2017 at 11:25 am
       hi, i would like to know what should i enter in
       (   r/data/train/train.csv   ), [   filename   ] and [   label   ]?
       and my image size is 850*600. where should i mention the size and
       what other specifications should i consider to accord with my image
       size?
       train=pd.read_csv(   r/data/train/train.csv   )
       test=pd.read_csv(   r/data/test.csv   )
       train_path=   r/data/train/images/train/   
       test_path=   r/data/train/images/test/   
       temp_img=image.load_img(train_path+train[   filename   ][i],target_size
       =(224,224))
       train_y=np.asarray(train[   label   ])
       [150]reply
     * robert m says:
       [151]june 14, 2017 at 4:56 pm
       hello,
       thank you for posting this. i have though some problems in
       implementing your algorithm:
       1) what is train[   filename   ][i] ? where is defined in the code? i
       just downloaded the csv files from your ink. did i missed
       something?
       2) why target_size=(224,224) when the images are 28  28?
       could you help me with this?
       thank you!
       [152]reply
     * harish says:
       [153]june 17, 2017 at 7:57 am
       awesome explanation
       [154]reply
     * [155]akram khan says:
       [156]june 21, 2017 at 12:35 am
       this is a very informative article. i also agree with your post
       title and your really well explain your point of view. i am very
       happy to see this post. thanks for share with us. keep it up and
       share the more most related post.
       quiz online [157]programming test
       [158]reply
     * abrar says:
       [159]june 27, 2017 at 4:46 pm
       ma   am can you help me to solve the identifying themes from mobile
       case images problem.
       this problem has been assigned to me as project.
       my [160][email protected]
       [161]reply
     * fadh says:
       [162]july 20, 2017 at 7:20 am
       thank you for this amazing blog,
       in the link of the mnsit data. i did not find train.csv.
       could you provide us with some explanation for this?
       [163]reply
     * kusha says:
       [164]november 1, 2017 at 12:35 am
       good stuff! thanks.
       could you provide some paper references which speak about the
       relation between data size and data similarity?
       [165]reply
     * mrt says:
       [166]november 2, 2017 at 1:03 pm
       i rarely leave any comment, but this post is fantastic! exactly
       what i was looking for at least. appreciated it!
       [167]reply
     * mezo says:
       [168]november 16, 2017 at 7:54 am
       thanks for your sharing and it is very valuable
       but
       please, could you provide matlab code ?
       or
       abbreviate explanation for each step
       [169]reply
     * tavish says:
       [170]march 19, 2018 at 3:01 am
       hey dishashree,
       great analogy to explain id21. very well written
       article.
       thanks,
       tavish
       [171]reply
     * pforet says:
       [172]march 23, 2018 at 11:05 pm
       great article! very well detailed.
       if i may add something, i like to replace the last layers with a
       different classifier, like support vector machines for instance. i
       find it somehow more easy to implement and in my experience, it
       leads to better results. if you   re interested, i   ve written a post
       here [173]http://www.ml-hack.com/neural-networks-specialization/ on
       how to perform id21 using support vector classifiers
       to replace the last layers.
       [174]reply
          + faizan shaikh says:
            [175]march 24, 2018 at 12:37 pm
            thanks for sharing pierre
            [176]reply
     * tom says:
       [177]march 29, 2018 at 1:57 am
       do you have code in github?
       [178]reply
          + aishwarya singh says:
            [179]march 29, 2018 at 2:32 pm
            hi,
            the code is added in the article itself.
            [180]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [181]srk       3924
   2    [2.jpg?date=2019-04-06] [182]mark12    3510
   3    [3.jpg?date=2019-04-06] [183]nilabha   3261
   4    [4.jpg?date=2019-04-06] [184]nitish007 3237
   5    [5.jpg?date=2019-04-06] [185]tezdhar   3082
   [186]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [187]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [188]understanding support vector machine algorithm from examples
       (along with code)
     * [189]essentials of machine learning algorithms (with python and r
       codes)
     * [190]a complete tutorial to learn data science with python from
       scratch
     * [191]7 types of regression techniques you should know!
     * [192]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [193]a simple introduction to anova (with applications in excel)
     * [194]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [195]top 5 machine learning github repositories and reddit discussions
   from march 2019

[196]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [197]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[198]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [199]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[200]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [201]16 opencv functions to start your id161 journey (with
   python code)

[202]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [203][ds-finhack.jpg]

   [204][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [205]about us
     * [206]our team
     * [207]career
     * [208]contact us
     * [209]write for us

   [210]about us
   [211]   
   [212]our team
   [213]   
   [214]careers
   [215]   
   [216]contact us

data scientists

     * [217]blog
     * [218]hackathon
     * [219]discussions
     * [220]apply jobs
     * [221]leaderboard

companies

     * [222]post jobs
     * [223]trainings
     * [224]hiring hackathons
     * [225]advertising
     * [226]reach us

   don't have an account? [227]sign up here.

join our community :

   [228]46336 [229]followers
   [230]20222 [231]followers
   [232]followers
   [233]7513 [234]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [235]privacy policy
     * [236]terms of use
     * [237]refund policy

   don't have an account? [238]sign up here

   iframe: [239]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [240](button) join now

   subscribe!

   iframe: [241]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [242](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/category/uncategorized/
  98. https://www.analyticsvidhya.com/blog/author/dishashree26/
  99. https://www.analyticsvidhya.com/blog/2014/06/deep-learning-attention/
 100. https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/
 101. https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/
 102. https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/
 103. https://www.crowdanalytix.com/contests/identifying-themes-from-mobile-case-images
 104. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
 105. http://www.pyimagesearch.com/2017/03/20/id163-vggnet-resnet-inception-xception-keras/
 106. http://yann.lecun.com/exdb/mnist/
 107. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=transfer-learning-the-art-of-fine-tuning-a-pre-trained-model&utm_medium=blog
 108. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=transfer-learning-the-art-of-fine-tuning-a-pre-trained-model&utm_medium=blog
 109. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 110. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?share=linkedin
 111. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?share=facebook
 112. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?share=twitter
 113. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?share=pocket
 114. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?share=reddit
 115. https://www.analyticsvidhya.com/blog/tag/deep-learning/
 116. https://www.analyticsvidhya.com/blog/tag/deep-learning-in-python/
 117. https://www.analyticsvidhya.com/blog/tag/inception/
 118. https://www.analyticsvidhya.com/blog/tag/keras/
 119. https://www.analyticsvidhya.com/blog/tag/parameter-tuning/
 120. https://www.analyticsvidhya.com/blog/tag/pre-trained-models/
 121. https://www.analyticsvidhya.com/blog/tag/vgg16/
 122. https://www.analyticsvidhya.com/blog/author/dishashree26/
 123. https://discuss.analyticsvidhya.com/
 124. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129584
 125. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129584
 126. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129686
 127. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129686
 128. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129611
 129. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129611
 130. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129675
 131. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129675
 132. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129687
 133. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129687
 134. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection
 135. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-131676
 136. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-131676
 137. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-153324
 138. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-153324
 139. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129884
 140. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129884
 141. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129994
 142. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-129994
 143. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130150
 144. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130150
 145. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130029
 146. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130029
 147. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130149
 148. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130149
 149. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130462
 150. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130462
 151. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130485
 152. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130485
 153. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130625
 154. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130625
 155. http://www.codeverb.com/
 156. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130845
 157. http://www.codeverb.com/
 158. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-130845
 159. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-131197
 160. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#791c54141810155414161d10404f494e391e14181015571a1614
 161. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-131197
 162. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-132426
 163. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-132426
 164. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-141686
 165. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-141686
 166. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-141895
 167. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-141895
 168. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-143963
 169. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-143963
 170. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-151971
 171. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-151971
 172. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152131
 173. http://www.ml-hack.com/neural-networks-specialization/
 174. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152131
 175. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152145
 176. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152145
 177. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152247
 178. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152247
 179. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152257
 180. https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/#comment-152257
 181. https://datahack.analyticsvidhya.com/user/profile/srk
 182. https://datahack.analyticsvidhya.com/user/profile/mark12
 183. https://datahack.analyticsvidhya.com/user/profile/nilabha
 184. https://datahack.analyticsvidhya.com/user/profile/nitish007
 185. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 186. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 187. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 188. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 189. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 190. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 191. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 192. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 193. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 194. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 195. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 196. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 197. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 198. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 199. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 200. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 201. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 202. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 203. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 204. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 205. http://www.analyticsvidhya.com/about-me/
 206. https://www.analyticsvidhya.com/about-me/team/
 207. https://www.analyticsvidhya.com/career-analytics-vidhya/
 208. https://www.analyticsvidhya.com/contact/
 209. https://www.analyticsvidhya.com/about-me/write/
 210. http://www.analyticsvidhya.com/about-me/
 211. https://www.analyticsvidhya.com/about-me/team/
 212. https://www.analyticsvidhya.com/about-me/team/
 213. https://www.analyticsvidhya.com/about-me/team/
 214. https://www.analyticsvidhya.com/career-analytics-vidhya/
 215. https://www.analyticsvidhya.com/about-me/team/
 216. https://www.analyticsvidhya.com/contact/
 217. https://www.analyticsvidhya.com/blog
 218. https://datahack.analyticsvidhya.com/
 219. https://discuss.analyticsvidhya.com/
 220. https://www.analyticsvidhya.com/jobs/
 221. https://datahack.analyticsvidhya.com/users/
 222. https://www.analyticsvidhya.com/corporate/
 223. https://trainings.analyticsvidhya.com/
 224. https://datahack.analyticsvidhya.com/
 225. https://www.analyticsvidhya.com/contact/
 226. https://www.analyticsvidhya.com/contact/
 227. https://datahack.analyticsvidhya.com/signup/
 228. https://www.facebook.com/analyticsvidhya/
 229. https://www.facebook.com/analyticsvidhya/
 230. https://twitter.com/analyticsvidhya
 231. https://twitter.com/analyticsvidhya
 232. https://plus.google.com/+analyticsvidhya
 233. https://in.linkedin.com/company/analytics-vidhya
 234. https://in.linkedin.com/company/analytics-vidhya
 235. https://www.analyticsvidhya.com/privacy-policy/
 236. https://www.analyticsvidhya.com/terms/
 237. https://www.analyticsvidhya.com/refund-policy/
 238. https://id.analyticsvidhya.com/accounts/signup/
 239. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 240. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 241. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 242. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 244. https://www.facebook.com/analyticsvidhya
 245. https://twitter.com/analyticsvidhya
 246. https://plus.google.com/+analyticsvidhya/posts
 247. https://in.linkedin.com/company/analytics-vidhya
 248. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=transfer-learning-the-art-of-fine-tuning-a-pre-trained-model&utm_medium=blog
 249. https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/?utm_source=transfer-learning-the-art-of-fine-tuning-a-pre-trained-model&utm_medium=blog
 250. https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/
 251. https://www.analyticsvidhya.com/blog/2017/06/data-scientist-mumbai-1-4-years-of-experience/
 252. https://www.analyticsvidhya.com/blog/author/dishashree26/
 253. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 254. https://www.facebook.com/analyticsvidhya/
 255. https://twitter.com/analyticsvidhya
 256. https://plus.google.com/+analyticsvidhya
 257. https://plus.google.com/+analyticsvidhya
 258. https://in.linkedin.com/company/analytics-vidhya
 259. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 260. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 261. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 262. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 263. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 264. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 265. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 266. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 267. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 268. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 269. javascript:void(0);
 270. javascript:void(0);
 271. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 272. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 273. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 274. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 275. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 276. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 277. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 278. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 279. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 280. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f06%2ftransfer-learning-the-art-of-fine-tuning-a-pre-trained-model%2f&linkname=transfer%20learning%20%26amp%3b%20the%20art%20of%20using%20pre-trained%20models%20in%20deep%20learning
 281. javascript:void(0);
 282. javascript:void(0);
