   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

deep learning achievements over the past year

great developments in text, voice, and id161 technologies

   [16]go to the profile of eduard tyantov
   [17]eduard tyantov (button) blockedunblock (button) followfollowing
   dec 21, 2017
   [18][1*jdqusr0c9xkln4vqpc3fzq.png]

   at [19]statsbot, we   re constantly reviewing the deep learning
   achievements to improve our models and product. around christmas time,
   our team decided to take stock of the recent achievements in deep
   learning over the past year (and a bit longer). we translated the
   article by a data scientist, ed tyantov, to tell you about the most
   significant developments that can affect our future.
   [1*zdvsgoxawk7vyy354fxsua.jpeg]

1. text

1.1. google id4

   almost a year ago, [20]google announced the launch of a new model for
   google translate. the company [21]described in detail the network
   architecture         recurrent neural network (id56).
   [22]machine learning translation and the google translate algorithm
   the basic principles of machine translation enginesblog.statsbot.co

   the key outcome: closing down the gap with humans in accuracy of the
   translation by 55   85% (estimated by people on a 6-point scale). it is
   difficult to reproduce good results with this model without the huge
   dataset that google has.
   [0*ex_bxxf_odfrwh9y.]

1.2. negotiations. will there be a deal?

   you probably heard the silly news that [23]facebook turned off its
   chatbot, which went out of control and made up its own language. this
   chatbot was created by the company for negotiations. its purpose is to
   conduct text negotiations with another agent and reach a deal: how to
   divide items (books, hats, etc.) by two. each agent has his own goal in
   the negotiations that the other does not know about. it   s impossible to
   leave the negotiations without a deal.

   for training, they collected a dataset of human negotiations and
   trained a supervised recurrent network. then, they took a reinforcement
   learning trained agent and trained it to talk with itself, setting a
   limit         the similarity of the language to human.

   the bot has learned one of the real negotiation strategies         showing a
   fake interest in certain aspects of the deal, only to give up on them
   later and benefit from its real goals. it has been the first attempt to
   create such an interactive bot, and it was quite successful.

   full story is in this [24]article, and the [25]code is publicly
   available.

   certainly, the news that the bot has allegedly invented a language was
   inflated from scratch. when training (in negotiations with the same
   agent), they disabled the restriction of the similarity of the text to
   human, and the algorithm modified the language of interaction. nothing
   unusual.
   [26]creepiest stories in artificial intelligence development
   scary things about ai: from virtual cannibalism to racist
   monstersblog.statsbot.co

   over the past year, recurrent networks have been actively developed and
   used in many tasks and applications. the architecture of id56s has
   become much more complicated, but in some areas similar results were
   achieved by simple feedforward-networks         [27]dssm. for example,
   [28]google has reached the same quality, as with lstm previously, for
   its mail feature smart reply. in addition, yandex launched a new search
   engine based on such networks.

2. voice

2.1. wavenet: a generative model for raw audio

   employees of deepmind [29]reported in their article about generating
   audio. briefly, researchers made an autoregressive full-convolution
   wavenet model based on previous approaches to image generation
   ([30]pixelid56 and [31]pixelid98).
   [0*3tm4xequodve7nrl.]

   the network was trained end-to-end: text for the input, audio for the
   output. the researches got an excellent result as the difference
   compared to human has been reduced by 50%.
   [0*g4mtbnvg7w02kfxp.]

   the main disadvantage of the network is a low productivity as, because
   of the autoregression, sounds are generated sequentially and it takes
   about 1   2 minutes to create one second of audio.

   look at    sorry, hear [32]this example.

   if you remove the dependence of the network on the input text and leave
   only the dependence on the previously generated phoneme, then the
   network will generate phonemes similar to the human language, but they
   will be meaningless.

   hear the example of [33]the generated voice.

   this same model can be applied not only to speech, but also, for
   example, to creating music. imagine [34]audio generated by the model,
   which was taught using the dataset of a piano game (again without any
   dependence on the input data).

   read a full version of [35]deepmind research if you   re interested.

2.2. lip reading

   lip reading is another deep learning achievement and victory over
   humans.

   google deepmind, in collaboration with oxford university, reported in
   the article, [36]   lip reading sentences in the wild    on how their
   model, which had been trained on a television dataset, was able to
   surpass the professional lip reader from the bbc channel.
   [0*xayel2qlpevq27mm.]

   there are 100,000 sentences with audio and video in the dataset. model:
   lstm on audio, and id98 + lstm on video. these two state vectors are fed
   to the final lstm, which generates the result (characters).
   [0*05m8s8ru1wwc6fpl.]

   different types of input data were used during training: audio, video,
   and audio + video. in other words, it is an    omnichannel    model.
   [0*ay4k5juoresodfow.]

2.3. synthesizing obama: synchronization of the lip movement from audio

   the university of washington [37]has done a serious job of generating
   the lip movements of former us president obama. the choice fell on him
   due to the huge number of his performance recordings online (17 hours
   of hd video).
   [0*lngrl6s8y6jhzmyz.]

   they couldn   t get along with just the network as they got too many
   artifacts. therefore, the authors of the article made several crutches
   (or tricks, if you like) to improve the texture and timings.
   [0*p038qo5bisv6thbv.]

   you can see that [38]the results are amazing. soon, you couldn   t trust
   even the video with the president.

3. id161

3.1. ocr: google maps and street view

   in their [39]post and [40]article, google brain team reported on how
   they introduced a new ocr (id42) engine into
   its maps, through which street signs and store signs are recognized.
   [0*-hrmlr5h0izuhlkc.]
   [0*gzytwv1wecxzyzda.]

   in the process of technology development, the company compiled a new
   [41]fsns (french street name signs), which contains many complex cases.

   to recognize each sign, the network uses up to four of its photos. the
   features are extracted with the id98, scaled with the help of the
   spatial attention (pixel coordinates are taken into account), and the
   result is fed to the lstm.
   [0*6i76xjpxodvtwcfy.]

   the same approach is applied to the task of recognizing store names on
   signboards (there can be a lot of    noise    data, and the network itself
   must    focus    in the right places). this algorithm was applied to 80
   billion photos.

3.2. visual reasoning

   there is a type of task called visual reasoning, where a neural network
   is asked to answer a question using a photo. for example:    is there a
   same size rubber thing in the picture as a yellow metal cylinder?    the
   question is truly nontrivial, and until recently, the problem was
   solved with an accuracy of only 68.5%.
   [0*yqwczworvgshnlz7.]

   and again the breakthrough was achieved by the team from deepmind:
   [42]on the clevr dataset they reached a super-human accuracy of 95.5%.

   the network architecture is very interesting:
    1. using the pre-trained lstm on the text question, we get the
       embedding of the question.
    2. using the id98 (just four layers) with the picture, we get feature
       maps (features that characterize the picture).
    3. next, we form pairwise combinations of coordinatewise slices on the
       feature maps (yellow, blue, red in the picture below), adding
       coordinates and text embedding to each of them.
    4. we drive all these triples through another network and sum up.
    5. the resulting presentation is run through another feedforward
       network, which provides the answer on the softmax.

   [0*yr6nrlajf8q4ulrb.]

3.3. pix2code

   an interesting application of neural networks [43]was created by the
   company uizard: generating a layout code according to a screenshot from
   the interface designer.
   [0*vnrkyzxqfb3xcbf6.]

   this is an extremely useful application of neural networks, which can
   make life easier when developing software. the authors claim that they
   reached 77% accuracy. however, this is still under research and there
   is no talk on real usage yet.

   there is no code or dataset in open source, but they promise to upload
   it.

   iframe: [44]/media/dc382c83709908a0d481743aa95cb01f?postid=4c563e034257

3.4. sketchid56: teaching a machine to draw

   perhaps you   ve seen [45]quick, draw! from google, where the goal is to
   draw sketches of various objects in 20 seconds. the corporation
   collected this dataset in order to teach the neural network to draw, as
   google described in their [46]blog and[47] article.
   [0*kh2zkbsesz14e_zq.]

   the collected dataset consists of 70 thousand sketches, which
   eventually [48]became publicly available. sketches are not pictures,
   but detailed vector representations of drawings (at which point the
   user pressed the    pencil,    released where the line was drawn, and so
   on).

   researchers have trained the sequence-to-sequence variational
   autoencoder (vae) using id56 as a coding/decoding mechanism.
   [0*6lg03jdvu0b5haq0.]

   eventually, as befits the auto-encoder, the model received a latent
   vector that characterizes the original picture.
   [0*egm0glz3ivm8aclm.]

   whereas the decoder can extract a drawing from this vector, you can
   change it and get new sketches.
   [0*tykblbtkdv7vkpbj.]

   and even perform vector arithmetic to create a catpig:
   [0*i0ujknocpsq-zzu2.]

3.5. gans

   one of the hottest topics in deep learning is generative adversarial
   networks (gans). most often, this idea is used to work with images, so
   i will explain the concept using them.
   [49]id3 (gans): engine and applications
   how generative adversarial nets are used to make our life
   betterblog.statsbot.co

   the idea is in the competition of two networks         the generator and the
   discriminator. the first network creates a picture, and the second one
   tries to understand whether the picture is real or generated.

   schematically it looks like this:
   [0*ru6mtivceet8lxlv.]

   during training, the generator from a random vector (noise) generates
   an image and feeds it to the input of the discriminator, which says
   whether it is fake or not. the discriminator is also given real images
   from the dataset.

   it is difficult to train such construction, as it is hard to find the
   equilibrium point of two networks. most often the discriminator wins
   and the training stagnates. however, the advantage of the system is
   that we can solve problems in which it is difficult for us to set the
   loss-function (for example, improving the quality of the photo)         we
   give it to the discriminator.

   a classic example of the gan training result is pictures of bedrooms or
   people
   [0*yj-kmizd2wekmqvn.]
   [0*hpiwpx5opnvwxsju.]

   previously, we considered the auto-coding (sketch-id56), which encodes
   the original data into a latent representation. the same thing happens
   with the generator.

   the idea of generating an image using a vector is clearly shown in
   [50]this project in the example of faces. you can change the vector and
   see how the faces change.
   [0*rbd0gj3eeqtlth-r.]

   the same arithmetic works over the latent space:    a man in glasses   
   minus    a man    plus a    woman    is equal to    a woman with glasses.   
   [0*qanzrt_1w3vrbxum.]

3.6. changing face age with gans

   if you teach a controlled parameter to the latent vector during
   training, when you generate it, you can change it and so manage the
   necessary image in the picture. this approach is called conditional
   gan.

   so did the authors of the article, [51]   face aging with conditional
   id3.    having trained the engine on the imdb
   dataset with a known age of actors, the researchers were given the
   opportunity to change the face age of the person.
   [0*pmgrxixfw7htynff.]

3.7. professional photos

   google [52]has found another interesting application to gan         the
   choice and improvement of photos. gan was trained on a professional
   photo dataset: the generator is trying to improve bad photos
   (professionally shot and degraded with the help of special filters),
   and the discriminator         to distinguish    improved    photos and real
   professional ones.

   a trained algorithm went through google street view panoramas in search
   of the best composition and received some pictures of professional and
   semi-professional quality (as per photographers    rating).
   [0*hvddmatythu0xp41.]
   [0*dvuojwokmwu_78yg.]

3.8. synthesization of an image from a text description

   an impressive example of gans is generating images using text.
   [0*1-oyozetnljyauqf.]

   [53]the authors of this research suggest embedding text into the input
   of not only a generator (conditional gan), but also a discriminator, so
   that it verifies the correspondence of the text to the picture. in
   order to make sure the discriminator learned to perform his function,
   in addition to training they added pairs with an incorrect text for the
   real pictures.
   [0*lgsuho5dkpzfl26w.]

3.9. pix2pix

   one of the eye-catching articles of 2016 is, [54]   image-to-image
   translation with conditional adversarial networks    by berkeley ai
   research (bair). researchers solved the problem of image-to-image
   generation, when, for example, it was required to create a map using a
   satellite image, or realistic texture of the objects using their
   sketch.
   [0*cmffpkyyo1qs5ufl.]

   here is another example of the successful performance of conditional
   gans. in this case, the condition goes to the whole picture. popular in
   image segmentation, unet was used as the architecture of the generator,
   and a new patchgan classifier was used as a discriminator for combating
   blurred images (the picture is cut into n patches, and the prediction
   of fake/real goes for each of them separately).

   [55]christopher hesse made the nightmare cat demo, which attracted
   great interest from the users.
   [0*d4nsmn1zb1w34t_o.]

   [56]you can find a source code here.

3.10. cyclegan

   in order to apply pix2pix, you need a dataset with the corresponding
   pairs of pictures from different domains. in the case, for example,
   with cards, it is not a problem to assemble such a dataset. however, if
   you want to do something more complicated like    transfiguring    objects
   or styling, then pairs of objects cannot be found in principle.

   therefore, authors of pix2pix decided to develop their idea and came up
   with cyclegan for transfer between different domains of images without
   specific pairs         [57]   unpaired image-to-image translation.   
   [0*rdtadzojz7b4kxa2.]

   the idea is to teach two pairs of generator-discriminators to transfer
   the image from one domain to another and back, while we require a cycle
   consistency         after a sequential application of the generators, we
   should get an image similar to the original l1 loss. a cyclic loss is
   required to ensure that the generator did not just begin to transfer
   pictures of one domain to pictures from another domain, which are
   completely unrelated to the original image.
   [0*vgmrfqssyygbm2h6.]

   this approach allows you to learn the mapping of horses -> zebras.
   [0*m1gkpxxo6nv-pnze.]

   such transformations are unstable and often create unsuccessful
   options:
   [0*2_5evutpswlf5_fr.]

   [58]you can find a source code here.

3.11. development of molecules in oncology

   machine learning is now coming to medicine. in addition to recognizing
   ultrasound, mri, and diagnosis, it can be used to find new drugs to
   fight cancer.

   we already [59]reported in detail about this research. briefly, with
   the help of adversarial autoencoder (aae), you can learn the latent
   representation of molecules and then use it to search for new ones. as
   a result, 69 molecules were found, half of which are used to fight
   cancer, and the others have serious potential.
   [0*0zli_creps3ejh7k.]

3.12. adversarial-attacks

   topics with adversarial-attacks are actively explored. what are
   adversarial-attacks? standard networks trained, for example, on
   id163, are completely unstable when adding special noise to the
   classified picture. in the example below, we see that the picture with
   noise for the human eye is practically unchanged, but the model goes
   crazy and predicts a completely different class.
   [0*lpelttdyv0pk5tce.]

   stability is achieved with, for example, the fast gradient sign method
   (fgsm): having access to the parameters of the model, you can [60]make
   one or several gradient steps towards the desired class and change the
   original picture.

   one of the tasks on [61]kaggle is related to this: the participants are
   encouraged to create universal attacks/defenses, which are all
   eventually run against each other to determine the best.

   why should we even investigate these attacks? first, if we want to
   protect our products, we can add noise to the captcha to prevent
   spammers from recognizing it automatically. secondly, algorithms are
   more and more involved in our lives         face recognition systems and
   self-driving cars. in this case, attackers can use the shortcomings of
   the algorithms.

   here is an example of when special glasses allow you to deceive the
   face recognition system and    pass yourself off as another person.    so,
   we need to take possible attacks into account when teaching models.
   [0*_djk3ntnlc2fdkjm.]

   such manipulations with signs also do not allow them to be recognized
   correctly.
   [0*bxqmk7qr5u6kn8ts.]

       [62]a set of articles from the organizers of the contest.
       already written libraries for attacks: [63]cleverhans and
   [64]foolbox.

4. id23

   id23 (rl), or learning with reinforcement is also one
   of the most interesting and actively developing approaches in machine
   learning.

   the essence of the approach is to learn the successful behavior of the
   agent in an environment that gives a reward through experience         just
   as people learn throughout their lives.
   [0*eblcchvgddtsgw7n.]

   rl is actively used in games, robots, and system management (traffic,
   for example).

   of course, everyone has heard about [65]alphago   s victories in the game
   over the best professionals. researchers were using rl for training:
   the bot played with itself to improve its strategies.

4.1. reinforcement training with uncontrolled auxiliary tasks

   in previous years, deepmind had learned using [66]id25 to play arcade
   games better than humans. currently, algorithms are being taught to
   play more complex games like [67]doom.

   much of the attention is paid to learning acceleration because
   experience of the agent in interaction with the environment requires
   many hours of training on modern gpus.

   in his blog, [68]deepmind reported that the introduction of additional
   losses (auxiliary tasks), such as the prediction of a frame change
   (pixel control) so that the agent better understands the consequences
   of the actions, significantly speeds up learning.
   [0*m-xmqfqnsbtve1pd.]

   learning results:

   iframe: [69]/media/7f4275b9e626acdc583e352c7e3a46fd?postid=4c563e034257

   4.2. learning robots
   in openai, they have been actively studying an agent   s training by
   humans in a virtual environment, which is safer for experiments than in
   real life.

   in one of the [70]studies, the team showed that id62 is
   possible: a person shows in vr how to perform a certain task, and one
   demonstration is enough for the algorithm to learn it and then
   reproduce it in real conditions.
   [0*zu-lcyjnvzbdsrfl.]

   if only it was so easy with people. :)

4.3. learning on human preferences

   here is [71]the work of openai and deepmind on the same topic. the
   bottom line is that an agent has a task, the algorithm provides two
   possible solutions for the human and indicates which one is better. the
   process is repeated iteratively and the algorithm for 900 bits of
   feedback (binary markup) from the person learned how to solve the
   problem.
   [0*apxlqjcwyqxui8mu.]
   [0*pjrknqdxffb0z4zy.]

   as always, the human must be careful and think of what he is teaching
   the machine. for example, the evaluator decided that the algorithm
   really wanted to take the object, but in fact, he just simulated this
   action.
   [0*at4xr9yckiirebvq.]

4.4. movement in complex environments

   there is [72]another study from deepmind. to teach the robot complex
   behavior (walk, jump, etc.), and even do it similar to the human, you
   have to be heavily involved with the choice of the id168, which
   will encourage the desired behavior. however, it would be preferable
   that the algorithm learned complex behavior itself by leaning with
   simple rewards.

   researchers managed to achieve this: they taught agents (body
   emulators) to perform complex actions by constructing a complex
   environment with obstacles and with a simple reward for progress in
   movement.
   [0*7srcpchu-ooeurh4.]

   you can watch the [73]impressive video with results. however, it   s much
   more fun to watch it with a superimposed sound!

   iframe: [74]/media/33933aa8759229e7eb730be3d62f1254?postid=4c563e034257

   finally, i will give a link to the [75]recently published algorithms
   for learning rl from openai. now you can use more advanced solutions
   than the standard id25.

5. other

5.1. cooling the data center

   in july 2017, google [76]reported that it took advantage of deepmind   s
   development in machine learning to reduce the energy costs of its data
   center.

   based on the information from thousands of sensors in the data center,
   google developers trained a neural network ensemble to predict pue
   (power usage effectiveness) and more efficient data center management.
   this is an impressive and significant example of the practical
   application of ml.
   [0*hfa16m78unox_bdn.]

5.2. one model for all tasks

   as you know, trained models are poorly transferred from task to task,
   as each task has to be trained for a specific model. a small step
   towards the universality of the models was done by google brain in his
   article [77]   one model to learn the all.   

   researchers have trained a model that performs eight tasks from
   different domains (text, speech, and images). for example, translation
   from different languages, text parsing, and image and sound
   recognition.
   [0*bykgrpwqmhogocld.]

   in order to achieve this, they built a complex network architecture
   with various blocks to process different input data and generate a
   result. the blocks for the encoder/decoder fall into three types:
   convolution, attention, and [78]gated mixture of experts (moe).
   [0*ekurr1udxnbrff-7.]
   [0*q8xdvc5fvsi7sncq.]

   main results of learning:
     * almost perfect models were obtained (the authors did not fine tune
       the hyperparameters).
     * there is a transfer of knowledge between different domains, that
       is, on tasks with a lot of data, the performance will be almost the
       same. and it is better on small problems (for example, on parsing).
     * blocks needed for different tasks do not interfere with each other
       and even sometimes help, for example, moe         for the id163 task.

   by the way, this model is [79]present in tensor2tensor.

5.3. learning on id163 in one hour

   in their post, facebook staff told us how their engineers were able to
   teach the resnet-50 model on id163 in just one hour. truth be told,
   this required a cluster of 256 gpus (tesla p100).

   [80]they used gloo and caffe2 for distributed learning. to make the
   process effective, it was necessary to adapt the learning strategy with
   a huge batch (8192 elements): gradient averaging, warm-up phase,
   special learning rate, etc.

   as a result, it was possible to achieve an efficiency of 90% when
   scaling from 8 to 256 gpu. now researchers from facebook can experiment
   even faster, unlike mere mortals without such a cluster.

6. news

6.1. self-driving cars

   the self-driving car sphere is intensively developing, and the cars are
   actively tested. from the relatively recent events, we can note the
   purchase of intel mobileye, [81]the scandals around uber and google
   technologies stolen by their former employee, [82]the first death when
   using an autopilot, and much more.

   i will note one thing: [83]google waymo is launching a beta program.
   google is a pioneer in this field, and it is assumed that their
   technology is very good because cars have been driven more than 3
   million miles.

   as to more recent events, self-driving cars have been allowed to travel
   across all us states.

6.2. healthcare

   as i said, modern ml is beginning to be introduced into medicine. for
   example, [84]google collaborates with a medical center to help with
   diagnosis.
   [0*bdrsapmvfpnoh0o8.]

   deepmind has even [85]established a separate unit.
   [0*ledrc2nzqmq31c_g.]

   this year, under the program of the data science bowl, there was [86]a
   competition held to predict lung cancer in a year on the basis of
   detailed images with a prize fund of one million dollars.

6.3. investments

   currently, there are heavy investments in ml as it was before with
   bigdata.

   china invested $150 billion in ai to become the world leader in the
   industry.

   for comparison, baidu research employs 1,300 people, and in the same
   fair (facebook)         80. at the last kdd, alibaba employees talked about
   their parameter server [87]kungpeng, which runs on 100 billion samples
   with a trillion parameters, which    becomes a common task      .
   [0*l5arlp1lcgwmiznk.]

   you can draw your own conclusions, it   s never too late [88]to study
   machine learning. in one way or another, over time, all developers will
   use machine learning, which will become one of the common skills, as it
   is today         the ability to work with databases.

   [89]link to the original post.

   iframe: [90]/media/02231cd5403151a2a463e36cc3b88462?postid=4c563e034257

you   d also like:

   [91]etl vs elt: considering the advancement of data warehouses |
   statsbot blog
   with the advent of modern cloud-based data warehouses, such as bigquery
   or redshift, the traditional concept of etl is   statsbot.co
   [92]sql window functions tutorial for business analysis
   the most popular business problems solved with sqlblog.statsbot.co
   [93]a guide for customer retention analysis with sql
   how to make customer retention curves and cohort analysis the right
   wayblog.statsbot.co
   [94]sql queries for funnel analysis
   a template for building sql funnel queriesblog.statsbot.co

     * [95]machine learning
     * [96]deep learning
     * [97]data science
     * [98]ai
     * [99]id161

   (button)
   (button)
   (button) 5.4k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [100]go to the profile of eduard tyantov

[101]eduard tyantov

   mail.ru group, head of machine learning team

     (button) follow
   [102]stats and bots

[103]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 5.4k
     * (button)
     *
     *

   [104]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [105]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4c563e034257
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/deep-learning-achievements-4c563e034257&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/deep-learning-achievements-4c563e034257&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_a10sobf87yxv---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@tyantovev?source=post_header_lockup
  17. https://blog.statsbot.co/@tyantovev
  18. https://cube.dev/
  19. https://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=dl_achievements
  20. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  21. https://arxiv.org/abs/1609.08144
  22. https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4
  23. http://www.telegraph.co.uk/technology/2017/08/01/facebook-shuts-robots-invent-language/
  24. https://code.facebook.com/posts/1686672014972296/deal-or-no-deal-training-ai-bots-to-negotiate
  25. https://github.com/facebookresearch/end-to-end-negotiator
  26. https://blog.statsbot.co/creepy-artificial-intelligence-ebc3f76179a8
  27. https://www.microsoft.com/en-us/research/project/dssm/
  28. https://arxiv.org/pdf/1705.00652.pdf
  29. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  30. https://arxiv.org/abs/1601.06759
  31. https://arxiv.org/abs/1606.05328
  32. http://storage.googleapis.com/deepmind-media/pixie/us-english/wavenet-1.wav
  33. https://cloud.mail.ru/public/fkg2/hjj2xnyhn
  34. https://cloud.mail.ru/public/dn2s/yppfhf3qj
  35. https://arxiv.org/pdf/1609.03499.pdf
  36. https://arxiv.org/pdf/1611.05358v1.pdf
  37. http://grail.cs.washington.edu/projects/audiotoobama/siggraph17_obama.pdf
  38. https://www.youtube.com/watch?v=9yq67cjdqvw&t=0m25s
  39. https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html
  40. https://arxiv.org/abs/1704.03549
  41. https://github.com/tensorflow/models/tree/master/research/street
  42. https://arxiv.org/pdf/1706.01427.pdf
  43. https://arxiv.org/abs/1705.07962
  44. https://blog.statsbot.co/media/dc382c83709908a0d481743aa95cb01f?postid=4c563e034257
  45. https://quickdraw.withgoogle.com/
  46. https://research.googleblog.com/2017/04/teaching-machines-to-draw.html
  47. https://arxiv.org/pdf/1704.03477.pdf
  48. https://github.com/googlecreativelab/quickdraw-dataset
  49. https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47
  50. http://carpedm20.github.io/faces/
  51. https://arxiv.org/pdf/1702.01983.pdf
  52. https://research.googleblog.com/2017/07/using-deep-learning-to-create.html
  53. https://arxiv.org/pdf/1605.05396.pdf
  54. https://arxiv.org/pdf/1611.07004.pdf
  55. https://twitter.com/christophrhesse
  56. https://github.com/phillipi/pix2pix
  57. https://arxiv.org/pdf/1703.10593.pdf
  58. https://github.com/junyanz/cyclegan
  59. https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47
  60. https://arxiv.org/abs/1412.6572
  61. https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack
  62. https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack/discussion/35840
  63. https://github.com/tensorflow/cleverhans
  64. https://github.com/bethgelab/foolbox
  65. http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html
  66. https://deepmind.com/research/id25/
  67. http://vizdoom.cs.put.edu.pl/
  68. https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/
  69. https://blog.statsbot.co/media/7f4275b9e626acdc583e352c7e3a46fd?postid=4c563e034257
  70. https://blog.openai.com/robots-that-learn/
  71. https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/
  72. https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/
  73. https://www.youtube.com/watch?v=hx_bgotf7bs
  74. https://blog.statsbot.co/media/33933aa8759229e7eb730be3d62f1254?postid=4c563e034257
  75. https://github.com/openai/baselines
  76. https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/
  77. https://arxiv.org/abs/1706.05137
  78. https://arxiv.org/abs/1701.06538
  79. https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/multimodel.py
  80. https://research.fb.com/wp-content/uploads/2017/06/id1631kin1h5.pdf
  81. https://www.theguardian.com/technology/2017/feb/25/uber-google-lawsuit-self-driving-car-threat-anthony-levandowski
  82. https://www.theguardian.com/technology/2016/jun/30/tesla-autopilot-death-self-driving-car-elon-musk
  83. https://waymo.com/
  84. https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html
  85. https://deepmind.com/applied/deepmind-health/
  86. https://www.kaggle.com/c/data-science-bowl-2017
  87. http://www.kdd.org/kdd2017/papers/view/kunpeng-parameter-server-based-distributed-learning-systems-and-its-applica
  88. https://blog.statsbot.co/machine-learning-algorithms-183cc73197c
  89. https://habrahabr.ru/company/mailru/blog/338248/
  90. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=4c563e034257
  91. https://statsbot.co/blog/etl-vs-elt/
  92. https://blog.statsbot.co/sql-window-functions-tutorial-b5075b87d129
  93. https://blog.statsbot.co/customer-retention-analysis-93af9daee46b
  94. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
  95. https://blog.statsbot.co/tagged/machine-learning?source=post
  96. https://blog.statsbot.co/tagged/deep-learning?source=post
  97. https://blog.statsbot.co/tagged/data-science?source=post
  98. https://blog.statsbot.co/tagged/ai?source=post
  99. https://blog.statsbot.co/tagged/computer-vision?source=post
 100. https://blog.statsbot.co/@tyantovev?source=footer_card
 101. https://blog.statsbot.co/@tyantovev
 102. https://blog.statsbot.co/?source=footer_card
 103. https://blog.statsbot.co/?source=footer_card
 104. https://blog.statsbot.co/
 105. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 107. https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4
 108. https://blog.statsbot.co/creepy-artificial-intelligence-ebc3f76179a8
 109. https://blog.statsbot.co/generative-adversarial-networks-gans-engine-and-applications-f96291965b47
 110. https://statsbot.co/blog/etl-vs-elt/
 111. https://blog.statsbot.co/sql-window-functions-tutorial-b5075b87d129
 112. https://blog.statsbot.co/customer-retention-analysis-93af9daee46b
 113. https://blog.statsbot.co/sql-queries-for-funnel-analysis-35d5e456371d
 114. https://medium.com/p/4c563e034257/share/twitter
 115. https://medium.com/p/4c563e034257/share/facebook
 116. https://medium.com/p/4c563e034257/share/twitter
 117. https://medium.com/p/4c563e034257/share/facebook
