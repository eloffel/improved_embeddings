   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a zero-math introduction to id115 methods

   [16]go to the profile of ben shaver
   [17]ben shaver (button) blockedunblock (button) followfollowing
   dec 22, 2017

   for many of us, bayesian statistics is voodoo magic at best, or
   completely subjective nonsense at worst. among the trademarks of the
   bayesian approach, id115 methods are especially
   mysterious. they   re math-heavy and computationally expensive procedures
   for sure, but the basic reasoning behind them, like so much else in
   data science, can be made intuitive. that is my goal here.
     __________________________________________________________________

   so, what are id115 (mcmc) methods? the short answer
   is:

     mcmc methods are used to approximate the posterior distribution of a
     parameter of interest by random sampling in a probabilistic space.

   in this article, i will explain that short answer, without any math.

   first, some terminology. a parameter of interest is just some number
   that summarizes a phenomenon we   re interested in. in general we use
   statistics to estimate parameters. for example, if we want to learn
   about the height of human adults, our parameter of interest might be
   average height in in inches. a distribution is a mathematical
   representation of every possible value of our parameter and how likely
   we are to observe each one. the most famous example is a bell curve:
   [0*znhfx8oue0biqrr6.]
   courtesy [18]m. w. toews

   in the bayesian way of doing statistics, distributions have an
   additional interpretation. instead of just representing the values of a
   parameter and how likely each one is to be the true value, a bayesian
   thinks of a distribution as describing our beliefs about a parameter.
   therefore, the bell curve above shows we   re pretty sure the value of
   the parameter is quite near zero, but we think there   s an equal
   likelihood of the true value being above or below that value, up to a
   point.

   as it happens, human heights do follow a normal curve, so let   s say we
   believe the true value of average human height follows a bell curve
   like this:
   [0*tlqbginnayyi3dgx.]

   clearly, the person with beliefs represented by this graph has been
   living among giants for years, because as far as they know, the most
   likely average adult height is 6'2" (but they   re not super confident
   one way or another).

   lets imagine this person went and collected some data, and they
   observed a range of people between 5' and 6'. we can represent that
   data below, along with another normal curve that shows which values of
   average human height best explain the data:
   [0*kkao7qpzegog9drf.]

   in bayesian statistics, the distribution representing our beliefs about
   a parameter is called the prior distribution, because it captures our
   beliefs prior to seeing any data. the likelihood distribution
   summarizes what the observed data are telling us, by representing a
   range of parameter values accompanied by the likelihood that each each
   parameter explains the data we are observing. estimating the parameter
   value that maximizes the likelihood distribution is just answering the
   question: what parameter value would make it most likely to observe the
   data we have observed? in the absence of prior beliefs, we might stop
   there.

   the key to bayesian analysis, however, is to combine the prior and the
   likelihood distributions to determine the posterior distribution. this
   tells us which parameter values maximize the chance of observing the
   particular data that we did, taking into account our prior beliefs. in
   our case, the posterior distribution looks like this:
   [0*3m3hcm8gojjls-kx.]

   above, the red line represents the posterior distribution. you can
   think of it as a kind of average of the prior and the likelihood
   distributions. since the prior distribution is shorter and more spread
   out, it represents a set of belief that is    less sure    about the true
   value of average human height. meanwhile, the likelihood summarizes the
   data within a relatively narrow range, so it represents a    more sure   
   guess about the true parameter value.

   when the prior the likelihood are combined, the data (represented by
   the likelihood) dominate the weak prior beliefs of the hypothetical
   individual who had grown up among giants. although that individual
   still believes the average human height is slightly higher than just
   what the data is telling him, he is mostly convinced by the data.

   in the case of two bell curves, solving for the posterior distribution
   is very easy. there is a simple equation for combining the two. but
   what if our prior and likelihood distributions weren   t so well-behaved?
   sometimes it is most accurate to model our data or our prior beliefs
   using distributions which don   t have convenient shapes. what if our
   likelihood were best represented by a distribution with two peaks, and
   for some reason we wanted to account for some really wacky prior
   distribution? i   ve visualized that scenario below, by hand drawing an
   ugly prior distribution:
   [0*pcwai087hhpjtbkm.]
   visualizations rendered in matplotlib, enhanced using ms paint

   as before, there exists some posterior distribution that gives the
   likelihood for each parameter value. but its a little hard to see what
   it might look like, and it is impossible to solve for analytically.
   enter mcmc methods.

   mcmc methods allow us to estimate the shape of a posterior distribution
   in case we can   t compute it directly. recall that mcmc stands for
   id115 methods. to understand how they work, i   m
   going to introduce monte carlo simulations first, then discuss markov
   chains.
     __________________________________________________________________

   monte carlo simulations are just a way of estimating a fixed parameter
   by repeatedly generating random numbers. by taking the random numbers
   generated and doing some computation on them, monte carlo simulations
   provide an approximation of a parameter where calculating it directly
   is impossible or prohibitively expensive.

   suppose that we   d like to estimate the area of the follow circle:
   [0*ovzsv_mw2ogi1mq4.]

   since the circle is inside a square with 10 inch sides, the area can be
   easily calculated as 78.5 square inches. instead, however, we can drop
   20 points randomly inside the square. then we count the proportion of
   points that fell within the circle, and multiply that by the area of
   the square. that number is a pretty good approximation of the area of
   the circle.
   [0*oulzbu6hpdhignvb.]

   since 15 of the 20 points lay inside the circle, it looks like the
   circle is approximately 75 square inches. not too bad for a monte carlo
   simulation with only 20 random points.

   now, imagine we   d like to calculate the area of the shape plotted by
   the [19]batman equation:
   [0*5ahw9htojbmk0tmo.]

   here   s a shape we never learned an equation for! therefore, finding the
   area of the bat signal is very hard. nevertheless, by dropping points
   randomly inside a rectangle containing the shape, monte carlo
   simulations can provide an approximation of the area quite easily!

   monte carlo simulations aren   t only used for estimating the area of
   difficult shapes. by generating a lot of random numbers, they can be
   used to model very complicated processes. in practice, they   re used to
   forecast the weather, or estimate the id203 of winning an
   election.
     __________________________________________________________________

   the second element to understanding mcmc methods are markov chains.
   these are simply sequences of events that are probabilistically related
   to one another. each event comes from a set of outcomes, and each
   outcome determines which outcome occurs next, according to a fixed set
   of probabilities.

   an important feature of markov chains is that they are memoryless:
   everything that you would possibly need to predict the next event is
   available in the current state, and no new information comes from
   knowing the history of events. a game like [20]chutes and ladders
   exhibits this memorylessness, or markov property, but few things in the
   real world actually work this way. nevertheless, markov chains are
   powerful ways of understanding the world.

   in the 19th century, the bell curve was observed as a common pattern in
   nature. (we   ve noted, for example, that human heights follow a bell
   curve.) galton boards, which simulate the average values of repeated
   random events by dropping marbles through a board fitted with pegs,
   reproduce the normal curve in their distribution of marbles:
   [0*hdefoqpfgs9ueui0.]

   pavel nekrasov, a russian mathematician and theologian, [21]argued that
   the bell curve and, more generally, the law of large numbers, were
   simply artifacts of children   s games and trivial puzzles, where every
   event was completely independent. he thought that interdependent events
   in the real world, such as human actions, did not conform to nice
   mathematical patterns or distributions.

   andrey markov, for whom markov chains are named, sought to prove that
   non-independent events may also conform to patterns. one of his best
   known examples required counting thousands of two-character pairs from
   a work of russian poetry. using those pairs, he computed the
   id155 of each character. that is, given a certain
   preceding letter or white space, there was a certain chance that the
   next letter would be an a, or a t, or a whitespace. using those
   probabilities, markov was ability to simulate an arbitrarily long
   sequence of characters. this was a markov chain. although the first few
   characters are largely determined by the choice of starting character,
   markov showed that in the long run, the distribution of characters
   settled into a pattern. thus, even interdependent events, if they are
   subject to fixed probabilities, conform to an average.

   for a more useful example, imagine you live in a house with five rooms.
   you have a bedroom, bathroom, living room, dining room, and kitchen.
   lets collect some data, assuming that what room you are in at any given
   point in time is all we need to say what room you are likely to enter
   next. for instance, if you are in the kitchen, you have a 30% chance to
   stay in the kitchen, a 30% chance to go into the dining room, a 20%
   chance to go into the living room, a 10% chance to go into the
   bathroom, and a 10% chance to go into the bedroom. using a set of
   probabilities for each room, we can construct a chain of predictions of
   which rooms you are likely to occupy next.

   making predictions a few states out might be useful, if we want to
   predict where someone in the house will be a little while after being
   in the kitchen. but since our predictions are just based on one
   observation of where a person is in the house, its reasonable to think
   they won   t be very good. if someone went from the bedroom to the
   bathroom, for example, its more likely they   ll go right back to the
   bedroom than if they had come from the kitchen. so the markov property
   doesn   t usually apply to the real world.

   running the markov chain for thousands of iterations, however, does
   give the long-run prediction of what room you   re likely to be in. more
   importantly, this prediction isn   t affected at all by which room the
   person began in! intuitively, this makes sense: it doesn   t matter where
   someone is in the house at one point in time in order to simulate and
   describe where they are likely to be in the long-term, or in general.
   so markov chains, which seem like an unreasonable way to model a random
   variable over a few periods, can be used to compute the long-run
   tendency of that variable if we understand the probabilities that
   govern its behavior.
     __________________________________________________________________

   with some knowledge of monte carlo simulations and markov chains, i
   hope the math-free explanation of how mcmc methods work is pretty
   intuitive.

   recall that we are trying to estimate the posterior distribution for
   the parameter we   re interested in, average human height:
   [0*xeun0u_-eph6mmcv.]
   i am not a visualization expert, nor apparently am i any good at
   keeping my example within the bounds of common sense: my example of the
   posterior distribution seriously overestimates average human height.

   we know that the posterior distribution is somewhere in the range of
   our prior distribution and our likelihood distribution, but for
   whatever reason, we can   t compute it directly. using mcmc methods,
   we   ll effectively draw samples from the posterior distribution, and
   then compute statistics like the average on the samples drawn.

   to begin, mcmc methods pick a random parameter value to consider. the
   simulation will continue to generate random values (this is the monte
   carlo part), but subject to some rule for determining what makes a good
   parameter value. the trick is that, for a pair of parameter values, it
   is possible to compute which is a better parameter value, by computing
   how likely each value is to explain the data, given our prior beliefs.
   if a randomly generated parameter value is better than the last one, it
   is added to the chain of parameter values with a certain id203
   determined by how much better it is (this is the markov chain part).

   to explain this visually, lets recall that the height of a distribution
   at a certain value represents the id203 of observing that value.
   therefore, we can think of our parameter values (the x-axis) exhibiting
   areas of high and low id203, shown on the y-axis. for a single
   parameter, mcmc methods begin by randomly sampling along the x-axis:
   [0*fqwsnmwaxnadwxdh.]
   red points are random parameter samples

   since the random samples are subject to fixed probabilities, they tend
   to converge after a period of time in the region of highest id203
   for the parameter we   re interested in:
   [0*aye_6ewzdwfvcosq.]
   blue points just represent random samples after an arbitrary point in
   time, when convergence is expected to have occurred. note: i   m stacking
   point vertically purely for illustrative purposes.

   after convergence has occurred, mcmc sampling yields a set of points
   which are samples from the posterior distribution. draw a histogram
   around those points, and compute whatever statistics you like:
   [0*fmevhsn-xlvzlw1h.]

   any statistic calculated on the set of samples generated by mcmc
   simulations is our best guess of that statistic on the true posterior
   distribution.

   mcmc methods can also be used to estimate the posterior distribution of
   more than one parameter (human height and weight, say). for n
   parameters, there exist regions of high id203 in n-dimensional
   space where certain sets of parameter values better explain observed
   data. therefore, i think of mcmc methods as randomly sampling inside a
   probabilistic space to approximate the posterior distribution.
     __________________________________________________________________

   recall the short answer to the question    what are markov chain monte
   carlo methods?    here it is again as a tl;dr:

     mcmc methods are used to approximate the posterior distribution of a
     parameter of interest by random sampling in a probabilistic space.

   i hope i   ve explained that short answer, why you would use mcmc
   methods, and how they work. the inspiration for this post was a talk i
   gave as part of general assembly   s data science immersive course in
   washington, dc. the goals of that talk were to explain markov chain
   monte carlo methods to a non-technical audience, and i   ve tried to do
   the same here. leave a comment if you think this explanation is off the
   mark in some way, or if it could be made more intuitive.

   thanks to [22]matt brems.
     * [23]data science
     * [24]bayesian statistics
     * [25]markov chains
     * [26]monte carlo
     * [27]towards data science

   (button)
   (button)
   (button) 9.1k claps
   (button) (button) (button) 38 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of ben shaver

[29]ben shaver

   data scientist in residence and local instructor at general assembly

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 9.1k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/dcba889e0c50
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_l3mgm2gy0z6q---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@benpshaver?source=post_header_lockup
  17. https://towardsdatascience.com/@benpshaver
  18. https://commons.wikimedia.org/wiki/file:standard_deviation_diagram.svg
  19. http://mathworld.wolfram.com/batmancurve.html
  20. http://jakevdp.github.io/blog/2017/12/18/simulating-chutes-and-ladders/
  21. https://www.americanscientist.org/article/first-links-in-the-markov-chain
  22. https://medium.com/@matthew.w.brems?source=post_page
  23. https://towardsdatascience.com/tagged/data-science?source=post
  24. https://towardsdatascience.com/tagged/bayesian-statistics?source=post
  25. https://towardsdatascience.com/tagged/markov-chains?source=post
  26. https://towardsdatascience.com/tagged/monte-carlo?source=post
  27. https://towardsdatascience.com/tagged/towards-data-science?source=post
  28. https://towardsdatascience.com/@benpshaver?source=footer_card
  29. https://towardsdatascience.com/@benpshaver
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/dcba889e0c50/share/twitter
  36. https://medium.com/p/dcba889e0c50/share/facebook
  37. https://medium.com/p/dcba889e0c50/share/twitter
  38. https://medium.com/p/dcba889e0c50/share/facebook
