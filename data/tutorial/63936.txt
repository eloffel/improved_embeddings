   #[1]image segmentation using deconvolution layer in tensorflow
   [2]freeze tensorflow models and serve on web

   ____________________

   [3]cv-tricks.com learn machine learning, ai & id161
   [4]login

     * [5]home
     * [6]tensorflow tutorials
     * [7]about

   [8]search

resnet, alexnet, vggnet, inception: understanding various architectures of
convolutional networks

   by koustubh

convolutional neural networks are fantastic for visual recognition tasks.

   good convnets are beasts with millions of parameters and many hidden
   layers. in fact, a bad rule of thumb is:    higher the number of hidden
   layers, better the network   . alexnet, vgg, inception, resnet are some
   of the popular networks. why do these networks work so well? how are
   they designed? why do they have the structures they have? one wonders.
   the answer to these questions is not trivial and certainly, can   t be
   covered in one blog post. however, in this blog, i shall try to discuss
   some of these questions. network architecture design is a complicated
   process and will take a while to learn and even longer to experiment
   designing on your own. but first, let   s put things in perspective:

why are convnets beating traditional id161?

   image classification is the task of classifying a given image into one
   of the pre-defined categories. traditional pipeline for image
   classification involves two modules: viz. feature extraction and
   classification.

   feature extraction involves extracting a higher level of information
   from raw pixel values that can capture the distinction among the
   categories involved. this feature extraction is done in an unsupervised
   manner wherein the classes of the image have nothing to do with
   information extracted from pixels. some of the traditional and widely
   used features are gist, hog, sift, lbp etc. after the feature is
   extracted, a classification module is trained with the images and their
   associated labels. a few examples of this module are id166, logistic
   regression, id79, id90 etc.

   the problem with this pipeline is that feature extraction cannot be
   tweaked according to the classes and images. so if the chosen feature
   lacks the representation required to distinguish the categories, the
   accuracy of the classification model suffers a lot, irrespective of the
   type of classification strategy employed. a common theme among the
   state of the art following the traditional pipeline has been, to pick
   multiple feature extractors and club them inventively to get a better
   feature. but this involves too many heuristics as well as manual labor
   to tweak parameters according to the domain to reach a decent level of
   accuracy. by decent i mean, reaching close to human level accuracy.
   that   s why it took years to build a good id161 system(like
   ocr, face verification, image classifiers, object detectors etc), that
   can work with a wide variety of data encountered during practical
   application, using traditional id161. we once produced better
   results using convnets for a company(a client of my start-up) in 6
   weeks, which took them close to a year to achieve using traditional
   id161.

   another problem with this method is that it is completely different
   from how we humans learn to recognize things. just after birth, a child
   is incapable of perceiving his surroundings, but as he progresses and
   processes data, he learns to identify things. this is the philosophy
   behind deep learning, wherein no hard-coded feature extractor is built
   in. it combines the extraction and classification modules into one
   integrated system and it learns to extract, by discriminating
   representations from the images and classify them based on supervised
   data.

   one such system is multilayer id88s aka neural networks which are
   multiple layers of neurons densely connected to each other. a deep
   vanilla neural network has such a large number of parameters involved
   that it is impossible to train such a system without overfitting the
   model due to the lack of a sufficient number of training examples. but
   with convolutional neural networks(convnets), the task of training the
   whole network from the scratch can be carried out using a large dataset
   like id163. the reason behind this is, sharing of parameters between
   the neurons and sparse connections in convolutional layers. it can be
   seen in this figure 2. in the convolution operation, the neurons in one
   layer are only locally connected to the input neurons and the set of
   parameters are shared across the 2-d feature map.

   in order to understand the design philosophy of convnets, one must ask:
   what is the objective here?

a. accuracy:

   if you are building an intelligent machine, it is absolutely critical
   that it must be as accurate as possible. one fair question to ask here
   is that    accuracy not only depends on the network but also on the
   amount of data available for training   . hence, these networks are
   compared on a standard dataset called id163.

   id163 project is an ongoing effort and currently has 14,197,122
   images from 21841 different categories. since 2010, id163 has been
   running an annual competition in visual recognition where participants
   are provided with 1.2 million images belonging to 1000 different
   classes from id163 data-set. so, each network architecture reports
   accuracy using these 1.2 million images of 1000 classes.

b. computation:

   most convnets have huge memory and computation requirements, especially
   while training. hence, this becomes an important concern. similarly,
   the size of the final trained model becomes important to consider if
   you are looking to deploy a model to run locally on mobile. as you can
   guess, it takes a more computationally intensive network to produce
   more accuracy. so, there is always a trade-off between accuracy and
   computation.

   apart from these, there are many other factors like ease of training,
   the ability of a network to generalize well etc. the networks described
   below are the most popular ones and are presented in the order that
   they were published and also had increasingly better accuracy from the
   earlier ones.

alexnet

   this architecture was one of the first deep networks to push id163
   classification accuracy by a significant stride in comparison to
   traditional methodologies. it is composed of 5 convolutional layers
   followed by 3 fully connected layers, as depicted in figure 1.
   alexnet network architecture: tensorflow tutorial

   alexnet, proposed by alex krizhevsky, uses relu(rectified linear unit)
   for the non-linear part, instead of a tanh or sigmoid function which
   was the earlier standard for traditional neural networks. relu is given
   by

   f(x) = max(0,x)

   the advantage of the relu over sigmoid is that it trains much faster
   than the latter because the derivative of sigmoid becomes very small in
   the saturating region and therefore the updates to the weights almost
   vanish(figure 4). this is called vanishing gradient problem.

   in the network, relu layer is put after each and every convolutional
   and fully-connected layers(fc).

   another problem that this architecture solved was reducing the
   over-fitting by using a dropout layer after every fc layer. dropout
   layer has a id203,(p), associated with it and is applied at every
   neuron of the response map separately. it randomly switches off the
   activation with the id203 p, as can be seen in figure 5.

why does dropout work?

   the idea behind the dropout is similar to the model ensembles. due to
   the dropout layer, different sets of neurons which are switched off,
   represent a different architecture and all these different
   architectures are trained in parallel with weight given to each subset
   and the summation of weights being one. for n neurons attached to
   dropout, the number of subset architectures formed is 2^n. so it
   amounts to prediction being averaged over these ensembles of models.
   this provides a structured model id173 which helps in avoiding
   the over-fitting. another view of dropout being helpful is that since
   neurons are randomly chosen, they tend to avoid developing
   co-adaptations among themselves thereby enabling them to develop
   meaningful features, independent of others.

vgg16

   this architecture is from vgg group, oxford. it makes the improvement
   over alexnet by replacing large kernel-sized filters(11 and 5 in the
   first and second convolutional layer, respectively) with multiple 3x3
   kernel-sized filters one after another. with a given receptive
   field(the effective area size of input image on which output depends),
   multiple stacked smaller size kernel is better than the one with a
   larger size kernel because multiple non-linear layers increases the
   depth of the network which enables it to learn more complex features,
   and that too at a lower cost.

   for example, three 3x3 filters on top of each other with stride 1 ha a
   receptive size of 7, but the number of parameters involved is 3*(9c^2)
   in comparison to 49c^2 parameters of kernels with a size of 7. here, it
   is assumed that the number of input and output channel of layers is
   c.also, 3x3 kernels help in retaining finer level properties of the
   image. the network architecture is given in the table.
   [s_8c760a111a4204fb24ffc30e04e069bd755c4eefd62acba4b54bba2a78e13e8c_149
   1022251600_vggnet.png]

   you can see that in vgg-d, there are blocks with same filter size
   applied multiple times to extract more complex and representative
   features. this concept of blocks/modules became a common theme in the
   networks after vgg.

   the vgg convolutional layers are followed by 3 fully connected layers.
   the width of the network starts at a small value of 64 and increases by
   a factor of 2 after every sub-sampling/pooling layer. it achieves the
   top-5 accuracy of 92.3 % on id163.

googlenet/inception:

   while vgg achieves a phenomenal accuracy on id163 dataset, its
   deployment on even the most modest sized gpus is a problem because of
   huge computational requirements, both in terms of memory and time. it
   becomes inefficient due to large width of convolutional layers.

   for instance, a convolutional layer with 3x3 kernel size which takes
   512 channels as input and outputs 512 channels, the order of
   calculations is 9x512x512.

   in a convolutional operation at one location, every output channel (512
   in the example above), is connected to every input channel, and so we
   call it a dense connection architecture. the googlenet builds on the
   idea that most of the activations in a deep network are either
   unnecessary(value of zero) or redundant because of correlations between
   them. therefore the most efficient architecture of a deep network will
   have a sparse connection between the activations, which implies that
   all 512 output channels will not have a connection with all the 512
   input channels. there are techniques to prune out such connections
   which would result in a sparse weight/connection. but kernels for
   sparse id127 are not optimized in blas or cublas(cuda
   for gpu) packages which render them to be even slower than their dense
   counterparts.

   so googlenet devised a module called inception module that approximates
   a sparse id98 with a normal dense construction(shown in the figure).
   since only a small number of neurons are effective as mentioned
   earlier, the width/number of the convolutional filters of a particular
   kernel size is kept small. also, it uses convolutions of different
   sizes to capture details at varied scales(5x5, 3x3, 1x1).

   another salient point about the module is that it has a so-called
   bottleneck layer(1x1 convolutions in the figure). it helps in the
   massive reduction of the computation requirement as explained below.

   let us take the first inception module of googlenet as an example which
   has 192 channels as input. it has just 128 filters of 3x3 kernel size
   and 32 filters of 5x5 size. the order of computation for 5x5 filters is
   25x32x192 which can blow up as we go deeper into the network when the
   width of the network and the number of 5x5 filter further increases. in
   order to avoid this, the inception module uses 1x1 convolutions before
   applying larger sized kernels to reduce the dimension of the input
   channels, before feeding into those convolutions. so in the first
   inception module, the input to the module is first fed into 1x1
   convolutions with just 16 filters before it is fed into 5x5
   convolutions. this reduces the computations to 16x192 +  25x32x16. all
   these changes allow the network to have a large width and depth.

   another change that googlenet made, was to replace the fully-connected
   layers at the end with a simple global average pooling which averages
   out the channel values across the 2d feature map, after the last
   convolutional layer. this drastically reduces the total number of
   parameters. this can be understood from alexnet, where fc layers
   contain approx. 90% of parameters. use of a large network width and
   depth allows googlenet to remove the fc layers without affecting the
   accuracy. it achieves 93.3% top-5 accuracy on id163 and is much
   faster than vgg.
   [s_8c760a111a4204fb24ffc30e04e069bd755c4eefd62acba4b54bba2a78e13e8c_149
   0879611424_inception_module.png]

residual networks

   as per what we have seen so far, increasing the depth should increase
   the accuracy of the network, as long as over-fitting is taken care of.
   but the problem with increased depth is that the signal required to
   change the weights, which arises from the end of the network by
   comparing ground-truth and prediction becomes very small at the earlier
   layers, because of increased depth. it essentially means that earlier
   layers are almost negligible learned. this is called vanishing
   gradient. the second problem with training the deeper networks is,
   performing the optimization on huge parameter space and therefore
   naively adding the layers leading to higher training error. residual
   networks allow training of such deep networks by constructing the
   network through modules called residual models as shown in the figure.
   this is called degradation problem. the intuition around why it works
   can be seen as follows:
   [s_8c760a111a4204fb24ffc30e04e069bd755c4eefd62acba4b54bba2a78e13e8c_149
   0999744884_main-qimg-b1fcbef975924b2ec4ad3a851e9f3934.png]


   imagine a network, a which produces x amount of training error.
   construct a network b by adding few layers on top of a and put
   parameter values in those layers in such a way that they do nothing to
   the outputs from a. let   s call the additional layer as c. this would
   mean the same x amount of training error for the new network. so while
   training network b, the training error should not be above the training
   error of a. and since it does happen, the only reason is that learning
   the identity mapping(doing nothing to inputs and just copying as it is)
   with the added layers-c is not a trivial problem, which the solver does
   not achieve. to solve this, the module shown above creates a direct
   path between the input and output to the module implying an identity
   mapping and the added layer-c just need to learn the features on top of
   already available input. since c is learning only the residual, the
   whole module is called residual module.

   also, similar to googlenet, it uses a global average pooling followed
   by the classification layer. through the changes mentioned, resnets
   were learned with network depth of as large as 152. it achieves better
   accuracy than vggnet and googlenet while being computationally more
   efficient than vggnet. resnet-152 achieves 95.51 top-5 accuracies.

   the architecture is similar to the vggnet consisting mostly of 3x3
   filters. from the vggnet, shortcut connection as described above is
   inserted to form a residual network. this can be seen in the figure
   which shows a small snippet of earlier layer synthesis from vgg-19.

   the power of the residual networks can be judged from one of the
   experiments in paper 4. the plain 34 layer network had higher
   validation error than the 18 layers plain network. this is where we
   realize the degradation problem. and the same 34 layer network when
   converted into the residual network has much lesser training error than
   the 18 layer residual network.
   finally, here is a table that shows the key figures around these
   networks:
   as we design more and more sophisticated architectures, some of the
   networks may not stay relevant a few years down the line but the core
   principles that led to their design must be understood. hope this
   articles offered you a good perspective on the design of neural network
   architectures.
   update: 3rd august, 2018. added a comparison table.

most popular
     __________________________________________________________________

     * [9]tensorflow tutorial 2: image classifier using convolutional
       neural network
     * [10]tensorflow tutorials [11]a quick complete tutorial to save and
       restore tensorflow models
     * resnet, alexnet, vggnet, inception: understanding various
       architectures of convolutional networks
     * [12]zero to hero: guide to id164 using deep learning:
       ...
     * [13]keras tensorflow tutorial [14]keras tutorial: practical guide
       from getting started to developing complex ...

[15]rss [16]cv-tricks rss feed
     __________________________________________________________________

     * [17]human pose estimation using deep learning in opencv
     * [18]deep learning based image colorization with opencv
     * [19]deep learning based edge detection in opencv
     __________________________________________________________________

share this article

   [20]share on facebook [21]share on twitter [22]share on pinterest

   copyright    2017 cv-tricks.com

references

   visible links
   1. https://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/
   2. https://cv-tricks.com/how-to/freeze-tensorflow-models/
   3. https://cv-tricks.com/
   4. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
   5. https://cv-tricks.com/
   6. https://cv-tricks.com/category/tensorflow-tutorial
   7. https://cv-tricks.com/about-us/
   8. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
   9. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  10. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  11. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  12. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  13. https://cv-tricks.com/tensorflow-tutorial/keras/
  14. https://cv-tricks.com/tensorflow-tutorial/keras/
  15. https://cv-tricks.com/feed/
  16. https://cv-tricks.com/
  17. https://cv-tricks.com/pose-estimation/using-deep-learning-in-opencv/
  18. https://cv-tricks.com/opencv/deep-learning-image-colorization/
  19. https://cv-tricks.com/opencv-dnn/edge-detection-hed/
  20. http://www.facebook.com/share.php?u=https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  21. https://twitter.com/share?url=https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  22. http://pinterest.com/pin/create/button/?url=https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/

   hidden links:
  24. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  25. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  26. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  27. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/#top
