   #[1]a quick complete tutorial to save and restore tensorflow models
   [2]accelerating convolutional neural networks on raspberry pi

   ____________________

   [3]cv-tricks.com learn machine learning, ai & id161
   [4]login

     * [5]home
     * [6]tensorflow tutorials
     * [7]about

   [8]search

   tensorflow tutorial for deep learning

quick complete tensorflow tutorial to understand and run alexnet, vgg,
inceptionv3, resnet and squeezenet networks

   by ankit sachan
    this tensorflow tutorial for convolutional neural networks has three
   parts:
   1. we shall look at some of the most successful convolutional neural
   network architectures like inception, alexnet, resnet etc.
   2. in the second part, we shall take a quick tutorial of a popular
   high-level and light-weight tensorflow library called
   tensorflow-slim(tf-slim).
   3. finally, using tf-slim, we shall take pre-trained models of some of
   these networks and use them for the prediction on some images.
       in the end, i shall provide the code to run prediction/id136,
   so that you can run it on your own images. so, after finishing this
   quick tutorial, you shall have a fairly good understanding of a running
   image classification and you could run it on your own images.

1.popular convolutional neural networks:

   in this section, we shall talk about some of the most successful
   convolutional neural networks. before that, let   s talk about id163.
   id163 is a project, started by stanford professor fei fei li where
   she created a large dataset of labeled images belonging to commonly
   seen real-world objects like dogs, cars, aeroplanes etc. (her [9]ted
   talk is a recommended watch). id163 project is an ongoing effort and
   currently has 14,197,122 images from 21841 different categories. since
   2010, id163 runs an annual competition in visual recognition where
   participants are provided with 1.2 million images belonging to 1000
   different classes from id163 data-set. each participating team then
   builds id161 algorithms to solve the classification problem
   for these 1000 classes. this competition is called id163 large scale
   visual recognition challenge (ilsvrc) and is considered an annual
   olympics of id161 with participants from across the globe
   including the finest of academia and industry. most of these networks
   have come out of ilsvrc which works as a global benchmark.

   let   s look at the network architectures:

i) alexnet:

   alex krizhevsky changed the world when he first won id163 challenged
   in 2012 using a convolutional neural network for image classification
   task. alexnet achieved top-5 accuracy of 84.6% in the classification
   task while the team that stood second had top-5 accuracy of 73.8% which
   was a record breaking and unprecedented difference. before this, id98s
   (and the people who were working on it) were not so popular among
   id161 community. however, the tables were turned after this.
   soon, most of the id161 researchers started working on id98
   and the accuracy has improved significantly over last 4-5 years.

   alexnet has 5 convolutional layers which are followed by 3 fully
   connected layers. alex trained it on 2 gpus as computational capacity
   was quite limited back in 2012.
   alexnet architecture in convolutional neural network

   architecture of alexnet which won 2012 id163 challenge.


   alexnet used 11*11 sized filter in the first convolution layer which
   later turned out to be too large and was modified by following networks
   in coming years. alex also used drop out, relu and a lot of data
   augmentation to improve the results.

   also, before i forget, alexnet was very similar to lenet which was
   proposed by yann lecun in 1998 based on his understanding but didn   t
   have enough training data and computation capacity(yes, he   s genius!
   you can [10]follow him on twitter here).

ii) vgg:

   vgg was proposed by a reasearch group at oxford in 2014. this network
   was once very popular due to its simplicity and some nice properties
   like it worked well on both image classification as well as detection
   tasks. however, if you are looking to build production systems in 2017
   and someone suggests vgg, run. this is because the size of vgg network
   is too large(id163 pre-trained model is more than 500 mb) due to
   large fully connected layers. there are more accurate and less
   computationally efficient networks available.

   vgg uses 3*3 convolution, in place of 11*11 convolution in alexnet
   which works better as 11*11 in the first layer leaves out a lot of
   original information. one of the slightly crude analogy for filter size
   is: think of it as if you are breaking and examining image into sized
   11*11 at one time. if you are    looking    at big chunks then you might
   miss finer details while with smaller chunks, you miss the context and
   spatial information. don   t worry if it doesn   t make too much sense,
   network architecture design is much more complicated which i shall
   cover in more details in a future post. vgg achieved 92.3% top-5
   accuracy in ilsvrc 2014 but was not the winner.
   vgg convolutional neural network: tensorflow tutorial

   vgg and its variants: d and e were the most accurate and popular ones.
   they didn   t win id163 challenge in 2014 but were widely adopted due
   to simplicity


iii) inception:

   alexnet was only 8 layers deep network, while vgg, zfnet and other more
   accurate networks that followed had more layers. this proved that one
   needs to go deep to get higher accuracy, this is why this field got the
   name    deep learning   . however, proposed by a team at google, inception
   was the first architecture which improved results by design not by
   simply going deep.
   the idea of inception was to use different sized filters on the same
   image(i.e. 1*1, 3*3, 5*5) and then concatenating the feature to
   generate a more robust representation. so, this is how it was initially
   proposed.
   naive inception module: convolutional neural network

   naive inception module which was replaced by a sophisticated one later
   however, 1*1 convolutions were added later and inception module that
   worked was:
   inception network: tensorflow tutorial

   refined inception module
    you can imagine this better with this image below:
   inception module shown in tensorflow tutorial

   inception module visualized: on the left is input which is    looked    in
   difference chunks and final output is a concatenation of all 3
   the first version of inception network was 22 layer network and was
   called googlenet(to honor yann lecun   s lenet) and it won 2014 id163
   challenge with 93.3% top-5 accuracy. however, later versions are
   referred as inceptionvn where n is the version number so inceptionv1,
   inceptionv2 etc. size of an id163 pre-trained model on inceptionv3
   is 104 mb.
   as mentioned earlier, all the network architectures before inception
   simply stacked layers on top of each other. inception was the first
   network that got creative with placement and proved that it   s possible
   to improve the accuracy and save on computation by doing that. you can
   think of inception module as a micro network inside another network.
   so, this also encouraged the whole community to be more creative with
   network designs.

iv) resnet:

   microsoft won id163 challenge in 2015 by using 152 layer resnet
   network which used a resnet module:
   resnet module: convoltional neural network tutorial

   resnet module proposed by microsoft


   they achieved 96.4% top-5 accuracy on id163 2015. they also
   experimented with much deeper models like 1000 layer networks but
   accuracy droped with too deep models probably due to overfitting.

v) squeezenet:

   squeezenet is remarkable not for its accuracy but for how less
   computation does it need. squeezenet has accuracy levels close to that
   of alexnet however, the pre-trained model on id163 has a size of
   less than 5 mb which is great for using id98s in a real world
   application. squeezenet introduced a fire module which is made of
   alternate squeeze and expand modules.
   squeezenet fire module

   squeezenet fire module

   with networks like squeezenet, it   s possible to reduce the amount of
   computation(hence, energy) required for similar kind of accuracy.
      given current power consumption by electronic computers, a computer
   with the storage and processing capability of the human mind would
   require in excess of 10 terawatts of power, while human mind takes on
   10 watts.    so, a lot of improvement is required in the efficiency of
   current neural networks and squeezenet is one step in that direction.

   great! you are now familiar with most of the popular and useful
   convolutional neural networks. in the next two sections, you shall run
   them on your own images.


2. tensorflow-slim tutorial:

    in order to build convolutional neural networks using tensorflow
   python api, we need to write a lot of(boilerplate) code. so, there are
   multiple high- level apis that are written on top of it to make it
   simple. tf-slim, written by nathan silberman and sergio guadarrama is
   one of the most prominent light-weight such an api which allows you to
   build, train and evaluate complex models easily.
    in this tensorflow tutorial, we shall learn the basics of tf-slim and
   build inceptionv1 network using it.
   2.1) installing tf-slim:

a) from tensorflow version 1.0 and above:

   tf-slim is available with versions 1.0 and above as part of
   tf.contrib.slim. to check if it   s working properly, you can run
   following line on the shell, it should not throw any errors:
   python

   python -c "import tensorflow.contrib.slim as slim; eval = sl
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   python -c "import tensorflow.contrib.slim as slim; eval =
   slim.evaluation.evaluate_once"

b) for older versions:

   if you happen to be running older version of tensorflow and can   t
   upgrade for whatever reasons, install using following steps:

   go to the [11]jenkins page for tensorflow project (where status of all
   the latest versions of tensorflow are managed by tensorflow team) and
   take the latest nightly(if you are not familiar with jenkins, just
   google continuous integration).
   tf-slim installation for tensorflow tutorial

   jenkins for tensorflow

   click on a relevant link, for example if you run ubuntu with gpu click
   on    nightly-matrix-linux-gpu   . on this page, you shall see a matrix for
   various python versions for pip and no_pip versions. choose no_pip with
   relevant python which will look like:

https://ci.tensorflow.org/view/all/job/nightly-matrix-linux-gpu/tf_build_is_o
pt=opt,tf_build_is_pip=no_pip,tf_build_python_version=python2,label=gpu-linux
/

   finally, append that with your relevant tensorflow version .whl file.
   now, install using:
   python

   export tf_binary_url=https://ci.tensorflow.org/view/nightly/
   ____________________________________________________________
   sudo pip install --upgrade $tf_binary_url___________________
   ____________________________________________________________
   1
   2
   3
   4

   export
   tf_binary_url=https://ci.tensorflow.org/view/nightly/job/nightly-matrix
   -linux-gpu/tf_build_is_opt=opt,tf_build_is_pip=pip,tf_build_python_vers
   ion=python2,label=gpu-linux/lastsuccessfulbuild/artifact/pip_test/whl/t
   ensorflow-0.11.0-cp27-none-linux_x86_64.whl

   sudo pip install --upgrade $tf_binary_url

   congratulations! this will install tf-slim on your machine. to test
   your installation:
   python

   python______________________________________________________
   >>import tensorflow as tf___________________________________
   >>slim= tf.contrib.slim_____________________________________
   ____________________________________________________________
   1
   2
   3
   4

   python
   >>import tensorflow as tf
   >>slim= tf.contrib.slim

   this should not throw any errors.

2.2) tensorflow-slim tutorial:

   in order to import tf-slim just do this:
   python

   slim = tf.contrib.slim______________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   slim = tf.contrib.slim

   there are the 3 most important features of tf-slim which we are going
   to cover now:

   2.2.1. quick slim variables

   2.2.2. higher level layers

   2.2.3. arg scope and using it

   let   s get started. shall we?

2.2.1. quick slim variables:

   let   s say you want to create a variable named    weight    using tf-slim,
   with a shape of [10 10 3 3] with initial values generated from a random
   distribution of standard deviation of 0.1 and a l2_regularizer with
   scale 0.5 and want to put it on gpu 0, you can do all this with a
   single line code using tf-slim:
   python

   weights = slim.variable('weights',__________________________
                            shape=[10, 10, 3 , 3],_____________
                                       initializer=tf.truncated
                                      regularizer=slim.l2_regul
                            device='/gpu:0')___________________
   1
   2
   3
   4
   5
   6

   weights = slim.variable('weights',
                            shape=[10, 10, 3 , 3],
                                       initializer=tf.truncated_normal_ini
   tializer(stddev=0.1),

   regularizer=slim.l2_regularizer(0.05),
                            device='/gpu:0')

   while the same will take multiple lines in vanilla tensorflow code.
   all the trainable variables in the model are called model variables in
   tf-slim. examples are variables created with tf.con2d and
   tf.fully_connected. using slim, this is how you create model variables:
   python

   weights = slim.model_variable('weights',____________________
                                 shape=[10, 10, 3 , 3],________
                                 initializer=tf.truncated_norma
                                 device='/cpu:0')______________
   1
   2
   3
   4
   5

   weights = slim.model_variable('weights',
                                 shape=[10, 10, 3 , 3],
                                 initializer=tf.truncated_normal_initializ
   er(stddev=0.1,                              regularizer=slim.l2_regular
   izer(0.05),
                                 device='/cpu:0')
   to access model variables:

   model_variables = slim.get_model_variables()
   all the other variables are non-model variable like global_step
   variable. to access all the slim variables:

   regular_variables_and_model_variables = slim.get_variables()
   when you create a model variable via tf-slim   s layers or directly via
   the slim.model_variable function, tf-slim adds the variable to a the
   tf.graphkeys.model_variables collection.

2.2.2. higher level layers:

   in order to create a convolutional layer using tf-slim which has 128
   filters of size 3*3, this single line of code is sufficient.

   python

   input=...___________________________________________________
   net = slim.conv2d(input, 128,[3, 3], scope='conv1_1')_______
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   input=...
   net = slim.conv2d(input, 128,[3, 3], scope='conv1_1')

   while the same with pure tensorflow code will be very long:

   python

   input = ..._________________________________________________
   with tf.name_scope('conv1_1') as scope:_____________________
       kernel = tf.variable(tf.truncated_normal([3, 3,  64,128]
     conv = tf.nn.conv2d(input, kernel,[1, 1, 1, 1], padding='s
   ____________________________________________________________
     biases = tf.variable(tf.constant(0.0, shape=[128], dtype=t
   ____________________________________________________________
     bias = tf.nn.bias_add(conv, biases)_______________________
   ____________________________________________________________
     conv1 = tf.nn.relu(bias, name=scope)______________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11

   input = ...
   with tf.name_scope('conv1_1') as scope:
       kernel = tf.variable(tf.truncated_normal([3, 3,  64,128],
   dtype=tf.float32,                     stddev=1e-1), name='weights')
     conv = tf.nn.conv2d(input, kernel,[1, 1, 1, 1], padding='same')

     biases = tf.variable(tf.constant(0.0, shape=[128], dtype=tf.float32),
   trainable=true, name='biases')

     bias = tf.nn.bias_add(conv, biases)

     conv1 = tf.nn.relu(bias, name=scope)


   so, that   s simple. now, if you have same layer many times in neural
   network like this, the code is cleaner but can it get even simpler.
   python

   net = ...___________________________________________________
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_1')_________
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_2')_________
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_3')_________
   1
   2
   3
   4
   5

   net = ...
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_1')
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_2')
   net = slim.conv2d(net, 256,[3, 3], scope='conv3_3')
   in this case, you can use repeat to do the same thing in a single line
   of code:

   net = slim.repeat(net, 3, slim.conv2d, 256,[3, 3], scope='conv3')
   what if your layers are almost the same except one or few parameters?

   python

   x = slim.fully_connected(x, 32, scope='fc/fc_1')____________
   x = slim.fully_connected(x, 64, scope='fc/fc_2')____________
   x = slim.fully_connected(x, 128, scope='fc/fc_3')___________
   ____________________________________________________________
   1
   2
   3
   4

   x = slim.fully_connected(x, 32, scope='fc/fc_1')
   x = slim.fully_connected(x, 64, scope='fc/fc_2')
   x = slim.fully_connected(x, 128, scope='fc/fc_3')

   here, we had to build 3 fully connected layers with only 1 different
   parameter.  you can use stack like this:

   python

   slim.stack(x, slim.fully_connected,[32, 64, 128], scope='fc'
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   slim.stack(x, slim.fully_connected,[32, 64, 128], scope='fc')

   similarly for conv layers:

   python

   # without stack: verbose way:_______________________________
   x = slim.conv2d(x, 32,[3, 3], scope='core/core_1')__________
   x = slim.conv2d(x, 32,[1, 1], scope='core/core_2')__________
   x = slim.conv2d(x, 64,[3, 3], scope='core/core_3')__________
   x = slim.conv2d(x, 64,[1, 1], scope='core/core_4')__________
    ___________________________________________________________
   # using stack:______________________________________________
   slim.stack(x, slim.conv2d,[(32,[3, 3]),(32,[1, 1]),(64,[3, 3
   1
   2
   3
   4
   5
   6
   7
   8
   9

   # without stack: verbose way:
   x = slim.conv2d(x, 32,[3, 3], scope='core/core_1')
   x = slim.conv2d(x, 32,[1, 1], scope='core/core_2')
   x = slim.conv2d(x, 64,[3, 3], scope='core/core_3')
   x = slim.conv2d(x, 64,[1, 1], scope='core/core_4')
   # using stack:
   slim.stack(x, slim.conv2d,[(32,[3, 3]),(32,[1, 1]),(64,[3, 3]),(64,[1,
   1])], scope='core')

2.2.3. arg scope and using it

   in the following example, look at the part where it says    with
   tf.name_scope(   conv1_1   ) as scope:   , this is tensorflow using
   name_scope to keep all the variables/ops organized. here, we are
   creating 1st convolutional layer so we have added     conv1_1    as a
   prefix in front of all the variables. this helps us in keeping things
   organized as all the weights in this layer are referred as
      conv1_1/weights   .
   python

   input = ..._________________________________________________
   with tf.name_scope('conv1_1') as scope:_____________________
       kernel = tf.variable(tf.truncated_normal([3, 3, 64, 128]
                                              stddev=1e-1), nam
       conv = tf.nn.conv2d(input, kernel,[1, 1, 1, 1], padding=
       biases = tf.variable(tf.constant(0.0, shape=[128], dtype
                          trainable=true, name='biases')_______
       bias = tf.nn.bias_add(conv, biases)_____________________
       conv1 = tf.nn.relu(bias, name=scope)____________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10

   input = ...
   with tf.name_scope('conv1_1') as scope:
       kernel = tf.variable(tf.truncated_normal([3, 3, 64, 128],
   dtype=tf.float32,
                                              stddev=1e-1),
   name='weights')
       conv = tf.nn.conv2d(input, kernel,[1, 1, 1, 1], padding='same')
       biases = tf.variable(tf.constant(0.0, shape=[128],
   dtype=tf.float32),
                          trainable=true, name='biases')
       bias = tf.nn.bias_add(conv, biases)
       conv1 = tf.nn.relu(bias, name=scope)
   building on top of this, tf-slim uses arg-scope which allows you to
   specify arguments or operations that will be applied to everything
   defined within arg-scope.
   look at the code below, we are creating 3 convolutional layers which
   have different number of filters i.e. 64, 128, 256 respectively with
   padding of same, valid, same and we use the same weights initializer
   and regularizers.
   python

   net = slim.conv2d(inputs, 64,[11, 11], 4, padding='same',___
                     weights_initializer=tf.truncated_normal_in
                     weights_regularizer=slim.l2_regularizer(0.
   net = slim.conv2d(net, 128,[11, 11], padding='valid',_______
                     weights_initializer=tf.truncated_normal_in
                     weights_regularizer=slim.l2_regularizer(0.
   net = slim.conv2d(net, 256,[11, 11], padding='same',________
                     weights_initializer=tf.truncated_normal_in
                     weights_regularizer=slim.l2_regularizer(0.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10

   net = slim.conv2d(inputs, 64,[11, 11], 4, padding='same',
                     weights_initializer=tf.truncated_normal_initializer(s
   tddev=0.01),
                     weights_regularizer=slim.l2_regularizer(0.0005),
   scope='conv1')
   net = slim.conv2d(net, 128,[11, 11], padding='valid',
                     weights_initializer=tf.truncated_normal_initializer(s
   tddev=0.01),
                     weights_regularizer=slim.l2_regularizer(0.0005),
   scope='conv2')
   net = slim.conv2d(net, 256,[11, 11], padding='same',
                     weights_initializer=tf.truncated_normal_initializer(s
   tddev=0.01),
                     weights_regularizer=slim.l2_regularizer(0.0005),
   scope='conv3')
   let   s redefine these using tf-slim arg-scope:
   python

   with slim.arg_scope([slim.conv2d], padding='same',__________
                      weights_initializer=tf.truncated_normal_i
                      weights_regularizer=slim.l2_regularizer(0
   ____________________________________________________________
      net = slim.conv2d(inputs, 64,[11, 11], scope='conv1')____
      net = slim.conv2d(net, 128,[11, 11], padding='valid', sco
      net = slim.conv2d(net, 256,[11, 11], scope='conv3')______
   1
   2
   3
   4
   5
   6
   7
   8

   with slim.arg_scope([slim.conv2d], padding='same',

   weights_initializer=tf.truncated_normal_initializer(stddev=0.01)
                      weights_regularizer=slim.l2_regularizer(0.0005)):

      net = slim.conv2d(inputs, 64,[11, 11], scope='conv1')
      net = slim.conv2d(net, 128,[11, 11], padding='valid', scope='conv2')
      net = slim.conv2d(net, 256,[11, 11], scope='conv3')
   note that, weights initializers,padding and regularizers have been
   defined within arg_scope for slim.conv2d. so, these operations will be
   passed to conv2d layers created using slim within the scope block.
   however, individual layers do have the option of overriding the default
   by declaring them locally, as in this example, second layer uses
   padding=   valid   .
   now, we have covered all the important concepts of tf-slim to build
   neural network. let   s use this learning to run some of the id163
   pre-trained models in the next section.

3. running id163 pretrained models:

    in this section of tensorflow tutorial, i shall demonstrate how easy
   it is to use trained models for prediction. let   s take inception_v1 and
   inception_v3 networks trained on id163 dataset. you can find
   [12]more id163 models here.  without changing anything in the
   network, we will run prediction on few images and you can find the
   [13]code here. both the pretrained models are saved in slim_pretrained
   folder:

   powershell

   inception_v1.ckpt    26 mb__________________________________
   inception_v3.ckpt    104 mb_________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   inception_v1.ckpt    26 mb
   inception_v3.ckpt    104 mb

   the complete networks have been kept in nets folder. inception_v1.py
   and inception_v3.py are the files which define inception_v1 and
   inception_v3 networks respectively and we can build a network like
   this:
   python

   with slim.arg_scope(inception.inception_v1_arg_scope()):____
           logits, _ = inception.inception_v1(processed_images,
   ____________________________________________________________
   ____________________________________________________________
   1
   2
   3

   with slim.arg_scope(inception.inception_v1_arg_scope()):
           logits, _ = inception.inception_v1(processed_images,
   num_classes=1001, is_training=false)
   this will give the output of the last layer which can be converted to
   probabilities using softmax. probabilities = tf.nn.softmax(logits)

   now, we have built our tensorflow graph, the second step is to load the
   saved parameters in the network. for this, we shall
   use assign_from_checkpoint_fn function in which we shall specify the
   complete path of saved pre-trained model and all the variables we want
   to save. for inception_v1 network which has inceptionv1 as scope, we
   shall get all the model variables using
   slim.get_model_variables(   inceptionv1   ). now we have created the graph
   and initialized the network parameters from the saved model, we can run
   the network inside a session:
   python

   with tf.session() as sess:__________________________________
           init_fn(sess)_______________________________________
           np_image, probabilities = sess.run([image, probabili
   ____________________________________________________________
   1
   2
   3
   4

   with tf.session() as sess:
           init_fn(sess)
           np_image, probabilities = sess.run([image, probabilities])

   finally, this will give out the index of labels, so we use id163.py
   inside dataset folder to get the mapping to print out the probabilities
   and show the results. let   s look at some of the results:

   a). inception_v1 result: let   s run the prediction on this image of
   coffee:
   python

   python run_id136_on_v1.py https://cv-tricks.com/wp-conte
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   python run_id136_on_v1.py
   http://cv-tricks.com/wp-content/uploads/2017/03/pexels-photo-362042.jpe
   g

   results of id136 on inception_v1 during tensorflow tutorial

   inception_v1 identifies this image of expresso, however it is only 36%
   confident


   look at other results, some of them are are wrong but close like soup
   bowl.

   b. inception_v3 result: let   s run inception_v3 which is much larger in
   size and more accurate.
   python

   python run_id136_on_v3.py https://cv-tricks.com/wp-conte
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   python run_id136_on_v3.py
   http://cv-tricks.com/wp-content/uploads/2017/03/pexels-photo-280207.jpe
   g


   tensorflow tutorial

   prediction of inception_v3 on this image is spot-on and the model is
   very confident about it

   this seems to be pretty good. let   s run inception_v3 on the earlier
   image:
   python

   python run_id136_on_v3.py https://cv-tricks.com/wp-conte
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   2

   python run_id136_on_v3.py
   http://cv-tricks.com/wp-content/uploads/2017/03/pexels-photo-362042.jpe
   g

    look at the results now. the model is very confident this time with
   99% accuracy.
     tensorflow tutorial
   reference:
   1. http://aleph.se/andart2/neuroscience/energetics-of-the-brain-and-ai/
   [14]tensorflow tutorial

most popular
     __________________________________________________________________

     * [15]tensorflow tutorial 2: image classifier using convolutional
       neural network
     * [16]tensorflow tutorials [17]a quick complete tutorial to save and
       restore tensorflow models
     * [18]resnet, alexnet, vggnet, inception: understanding various
       architectures of convolutional networks
     * [19]zero to hero: guide to id164 using deep learning:
       ...
     * [20]keras tensorflow tutorial [21]keras tutorial: practical guide
       from getting started to developing complex ...

[22]rss [23]cv-tricks rss feed
     __________________________________________________________________

     * [24]human pose estimation using deep learning in opencv
     * [25]deep learning based image colorization with opencv
     * [26]deep learning based edge detection in opencv
     __________________________________________________________________

share this article

   [27]share on facebook [28]share on twitter [29]share on pinterest

   copyright    2017 cv-tricks.com

references

   visible links
   1. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
   2. https://cv-tricks.com/artificial-intelligence/deep-learning/accelerating-convolutional-neural-networks-on-raspberry-pi/
   3. https://cv-tricks.com/
   4. https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/
   5. https://cv-tricks.com/
   6. https://cv-tricks.com/category/tensorflow-tutorial
   7. https://cv-tricks.com/about-us/
   8. https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/
   9. https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures
  10. https://twitter.com/ylecun
  11. https://ci.tensorflow.org/view/nightly/
  12. https://github.com/tensorflow/models/tree/master/slim
  13. https://github.com/sankit1/cv-tricks.com
  14. https://cv-tricks.com/tag/tensorflow-tutorial/
  15. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  16. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  17. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  18. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  19. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  20. https://cv-tricks.com/tensorflow-tutorial/keras/
  21. https://cv-tricks.com/tensorflow-tutorial/keras/
  22. https://cv-tricks.com/feed/
  23. https://cv-tricks.com/
  24. https://cv-tricks.com/pose-estimation/using-deep-learning-in-opencv/
  25. https://cv-tricks.com/opencv/deep-learning-image-colorization/
  26. https://cv-tricks.com/opencv-dnn/edge-detection-hed/
  27. http://www.facebook.com/share.php?u=https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/
  28. https://twitter.com/share?url=https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/
  29. http://pinterest.com/pin/create/button/?url=https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/

   hidden links:
  31. https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/
  32. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  33. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  34. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  35. https://cv-tricks.com/tensorflow-tutorial/understanding-alexnet-resnet-squeezenetand-running-on-tensorflow/#top
