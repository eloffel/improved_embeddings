   #[1]case-study: better haar feature-based eye detector using opencv
   [2]install tensorflow-1.0 with gpu on ubuntu 14.04 on aws p2.xlarge

   ____________________

   [3]cv-tricks.com learn machine learning, ai & id161
   [4]login

     * [5]home
     * [6]tensorflow tutorials
     * [7]about

   [8]search

   show attend and tell, a communication system

show, attend and tell: fine details of how state-of-the-art deep learning
systems generate image captions

   by koustubh

   how would you communicate with aliens ? how would you teach them your
   language ? the way we teach children right ? by showing them pictures
   of various objects and their names.

   basically associating the image and the text. that   s the goal of
   automatic image captioning.

   it is an extensive topic and decades of research has gone into cracking
   this problem. a blog post can never do justice to this field just as
   reading the caption is not the same as looking at the picture! but one
   can try.

   id98

   if you just want to use a general image captioning system, you should
   be happy to know that google has open sourced their system and made it
   available under [9]im2txt repo in tensorflow. if you use torch/lua,
   then neuraltalk2 by karpathy would be a good starting point as the code
   is available on [10]github. you must know however, that google has done
   much more engineering to make it suitable for real world usage, so
   tensorflow is more recommended way to go.


   in this post, we will describe the foundation upon which these current
   soas    in image captioning are based. while we are reaching human level
   performance in many problem domains( e.g. [11]image classification,
   [12]id103, [13]face verification). however, automatic
   image captioning, even after decades of research hasn   t come close to
   human level performance. but the surge of deep learning in object
   recognition, localization and fine-grained attribute identification has
   led to an unprecedented boost in the accuracy of relatively hard
   id161 problem of image captioning. earlier methods involved
   transferring the most relevant description from indexed database by
   finding the semantically similar images through id162.
   obviously, their applicability was limited.

   current image captioning methodologies can be divided into three
   categories:
    1. the first set of methods detect objects and attributes and then
       piece together a description for the image from the phrases
       containing those objects.
    2. the second set of methods embed images and corresponding captions
       in the same vector space. for a given query, captions nearest to
       the image in the embedding space are retrieved and those captions
       are modified to generate a new caption for the given image.
       however, these methods do not generate a novel description of a
       given test image as the descriptions from the most similar images
       are used to spur out the caption.
    3. the third set of methods directly generate the sequence of the
       words most relevant to the image conditioned on the image and
       previously generated words. therefore, they can produce novel
       combinations of words that might never have occurred in the
       training data. this has become the standard pipeline in most of the
       state of the art algorithms for image captioning and is described
       in a greater detail below.let   s deep dive:

recurrent neural networks(id56s) are the key

   for the task of image captioning, a model is required that can predict
   the words of the caption in a correct sequence given the image. this
   can be modeled as finding the caption that maximizes the following log
   id203.

   $$\log p(s|i) = \sum_{t=0}^{n} \log p(s_{t}|i,s_{0},s_{1}      s_{t-1})
   \hspace{2cm} (eqn : 1)$$

   where:
     * \(s\) is the caption,

     * \(s_{t}\) is the word in the caption at location t
       (\(t_{th}\) word)

   the id203 of a word depends on the previously generated words and
   the image, and hence the conditioning on these variables in the
   equation. the training data consists of thousands of images and a
   caption associated with each image. the captions in the training data
   is written by humans. the training phase involves finding the
   parameters in the model that maximizes the id203 of captions
   given the image in the training set.

   in probabilistic id114, for this type of densely connected
   node variables wherein every node is dependent on all other nodes,
   training becomes computationally prohibitive because of an exponential
   number of possible configurations. recurrent neural networks (id56)
   provide a smooth way to perform the conditioning on previous variables
   using a fixed sized hidden vector.

   this hidden vector is then used to predict the next word just like a
   feed forward neural network.

   $$p(s_{t}|i,s_{0},s_{1}      ..s_{t-1}) \approx p(s_{t}|i,h_{t})
   \hspace{2cm} (eqn : 2)$$

   so at the step t, the complex conditioning on a variable number of
   nodes is replaced by a simple vector(\(h_{t}\)). then for making the
   prediction at step t, eqn:2 is used to model the id203
   distribution over all the words using a fully connected layer with
   number of outputs as number of words in vocabulary followed by softmax
   function. so \(p(s_{t}|i,h_{t})\) becomes

   $$p(s_{t}|i,h_{t}) = softmax(l_{h}h_{t}+l_{i}i)$$

   where \(l_{h}\) and \(l_{i}\) are the weight matrices of the fully
   connected layer with inputs taken as one concatenated vector from
   \(h_{t}\) and i.
   according to this distribution, the word with maximum id203 is
   taken as the next word in the caption. now at the step t+1, the
   conditioning on the previously generated words should also involve this
   newly generated word(\(s_{t}\)). but the id56 vector(\(h_{t}\)) is
   conditioned on the words \(s_{1}, s_{2}    . s_{t-1}\). so \(s_{t}\) is
   then combined with \(h_{t}\) through a linear layer followed by a
   non-linearity to produce \(h_{t+1}\) which stands for conditioning on
   \(s_{1}, s_{2}    . s_{t-1}, s_{t}\) . this is called an id56 unit which
   comprises of this calculation:
   $$h_{t+1}=tanh(w_{h}h_{t}+w_{s}s_{t}) \hspace{2cm} (eqn : 3)$$

   figure : 2

   the overall flow of the algorithm has been shown in figure 2.

   since id56 is basically like the conventional feed forward neural
   comprising of linear and non-linear layers, the back-propagation of
   loss during training is straight-forward without performing heavy
   id136 computations. the only difference with a normal neural
   network is that the clubbing of the previous hidden vector and newly
   generated word is done through the same set of parameters at each step.
   this is equivalent to feeding the output to the same network as input
   and hence the name recurrent neural network. this avoids blowing up the
   size of the network which otherwise would require a new set of
   parameters at each step. the id56 unit can be represented as shown in
   figure 3.

   figure : 3


   before we move forward, let me explain a very important concept in
   machine learning. typically, we want to represent everything ( images,
   sounds, words etc. ) as a vector ( i.e. an array of numbers ). you can
   also think of this vector of length n as a point in an n-dimensional
   space. but why represent images and words as vectors ? because we
   understand vector spaces and we understand distances in vector spaces.
   e.g. we know that the point (1,0,0) is closer to the point (0,0,0) than
   the point (0,5,0) in three dimensional space. if we could represent two
   words by points in a higher dimensional space, we could easily
   calculate how close they are in meaning or context by just calculating
   the distance of these points in higher dimensional space.

   conceptually, representing images and words as points in higher
   dimensional space sounds wonderful. the hard part is to figure out how
   to do this.

   the image is represented as a vector by taking the penultimate layer
   (before the classification layer) output from any of the standard
   convolutional networks viz. vggnet, googlenet etc.

   for words, we want the representation should be such that the vectors
   for the words similar in meaning or context should lie close to each
   other in the vector space. an algorithm that converts a word to a
   vector is called id97 and it is arguably the most popular one.
   id97 draws inspiration from the famous quote    you shall know the
   word from the company it keeps    to learn the representation for words.

   learning of id97 is done using a corpus ( psst    this is a fancy
   word for    collection of text    ) of sentences related to the domain in
   which we are working. let   s use  the domain    id161    as an
   example. let \(w_{1},w_{2}       w_{m}\) be the unique words in the corpus
   and \(s_{w_{1}},s_{w_{2}}    ..s_{w_{m}}\) be the corresponding word
   vector that we need to find. in order to determine the word vectors,
   id97 trains a system that can predict the surrounding words of a
   given word. surrounding words are defined as words appearing around a
   given word in a small context window of a given size (let   s say 2 here)
   on both sides of that word. let these three sentences be our corpus. (
   this is just for demonstration purposes; in real world applications,
   the size of the corpus should be very large to get a meaningful
   representation.)

            this blog is about deep learning and id161.

            we love deep learning.

            we love id161.

   given a word    computer   , the task is to predict the context words
      learning   ,    and    and     vision    from first sentence and    we   ,    love   
   and    vision    from last sentence. therefore the training objective
   becomes maximising the log id203 of these context words given the
   word    computer   . the formulation is:

   $$objective = maximize \sum_{t=1}^{t} \sum_{-m\geq j \geq m} log p
   (s_{w_{t+j}} | s_{w_{t}})$$

   where:

   m  : is the context window size (here 2) and t runs over the length of
   the corpus (i.e. every word in the collection of sentences)

   \( p(s_{w_{t+j}}|s_{w_{t}}) \) is modelled by the similarity or inner
   product between the context word vector and center word vector. for
   each word there are two vectors associated with it viz. when it appears
   as context and when it is the center word represented by r and s
   respectively.

   so \(p(s_{w_{t+j}}|s_{w_{t}})\) is taken as
   \(\frac{e^{r_{w_{t+j}}^{t}}}{\sum_{i=1}^{m}
   e^{r_{i}^{t}s_{w_{t}}}}\). here denominator is the id172 term
   that takes the similarity of center word vector with the context
   vectors of every other word in vocabulary so that id203 sums to
   one.

   the center vector for a word is then taken as the vector representation
   and is used with the id56 in eqn: 3.

dealing with noise in real world images: attention mechanism

   when there is clutter in the scene it becomes very difficult for
   simpler systems to generate a complex description of the image. we,
   humans, do it by getting a holistic picture of the image at first
   glance. our focus then shifts to different regions in the image as we
   go on describing the image. for machines, a similar attention mechanism
   has been proposed to mimic human behavior. this attention mechanism
   allows only important features from an image to come to the forefront
   when needed. at each step, the salient region of the image is
   determined and is fed into the id56 instead of using features from the
   whole image. the system gets a focused view from the image and predicts
   the word relevant to that region. the region where attention is
   focussed needs to be determined on the basis of previously generated
   words. otherwise, newly generated words may be coherent within the
   region but not in the description being generated.


   until now the output of the fully connected layer was used as input to
   the id56. this output corresponds to the entire image. so, how do we get
   a feature corresponding to only a small subsection of the image ?  the
   output of the convolutional layer encodes local information and not the
   information pertaining to the whole cluttered image. the outputs of the
   convolutional layer are 2d feature maps where each location was
   influenced by a small region in the image corresponding to the size
   (receptive field) of the convolutional kernel. to understand this have
   a look at the figure 4(as shown in wikipedia page of convolutional
   network) which shows a convolutional network. just before the output
   layer, there is a fully connected layer which is like one stretched
   vector and stands for the whole input image whereas the convolutional
   layer outputs(all the layers before the fully connected one) are like a
   2d image with many dimensions. the vector extracted from a single
   feature map at a particular location and across all the dimensions
   signify the feature for a local region of the image.


   [id98.png]

   figure: 4


   at each step, we want to determine the location on the feature map that
   is relevant to the current step. the feature vector from this location
   will be fed into the id56. so we model the id203 distribution over
   locations based on previously generated words. let \(l_{t}\) be a
   random variable with n dimensions with each value corresponding to a
   spatial location on feature map. \(l_{t,i} = 1\) means the \(i_{th}\)
   location is selected for generating the word at the t step. let
   \(a_{i}\) be the feature vector at the \(i_{th}\) location taken from
   convolutional maps.

   the value we need is:
   $$p(l_{t,i} = 1 | i,s_{0},s_{1}      ..s_{t-1}) \approx p(l_{t,i}|h_{t}) =
   \beta_{t,i} \propto a_{i}^{t}h_{t} \hspace{1cm}(eqn:4)$$
   here id203 of choosing a location (\(\beta_{t,i}\)) has been
   taken as directly proportional to the dot product i.e. similarity
   between vector at that location and the id56 hidden vector.

   now on the basis of id203 distribution, the feature vector
   corresponding to the location with the maximum id203 can be used
   for making the prediction, but using an aggregated vector from all the
   location vectors weighted according to the probabilities makes the
   training converge simply and fastly. so let the \(z_{t}\) be the
   context or aggregated vector which is to be fed into the id56.

   $$z_{t} = \sum_{i=1}^n\beta_{t,i}a_{i}$$

   so the eqn:2 becomes

   $$p(s_{t}|i,h_{t}) => p(s_{t}|z_{t},h_{t})$$

   so this mechanism simulate the human behavior of focusing their
   attention to various parts of the image while describing it and
   naturally with the focused view, a more sophisticated description can
   be generated for the image which caters to even the finer level details
   in the image. figure 5 shows one of the examples given in the reference
   1.

   figure : 5


   all these small components are like bedrock for most prevalent image
   captioning systems nowadays. there are many papers for image captioning
   with most of them working around these central ideas and between those
   papers, most of the changes are more or less in these areas:
    1. how the word vectors are learned.(ex. glove is another methodology
       to generate word vectors)
    2. id26 to be done on image and id97 layers or not.
    3. lstm(long short term memory) used in place of id56 which takes care
       of vanishing gradient problem in id56.
    4. stochastic vs soft attention: here we have discussed soft attention
       mechanism wherein different location vectors are aggregated on the
       basis of their individual probabilities. in the stochastic
       mechanism, a single location is sampled on the basis of id203
       distribution and vector from just that location is used in the id56
       unit.


   references:
    1. xu, j. ba, r. kiros, a. courville, r. salakhutdinov, r. zemel, and
       y. bengio, show, attend and tell: neural image id134
       with visual attention
    2. vinyals, a. toshev, s. bengio, and d. erhan, show and tell: a
       neural image caption generator

   [14]deep learning, [15]im2txt, [16]id56, [17]show-and-tell,
   [18]show-attend-tell, [19]tensorflow

most popular
     __________________________________________________________________

     * [20]tensorflow tutorial 2: image classifier using convolutional
       neural network
     * [21]tensorflow tutorials [22]a quick complete tutorial to save and
       restore tensorflow models
     * [23]resnet, alexnet, vggnet, inception: understanding various
       architectures of convolutional networks
     * [24]zero to hero: guide to id164 using deep learning:
       ...
     * [25]keras tensorflow tutorial [26]keras tutorial: practical guide
       from getting started to developing complex ...

[27]rss [28]cv-tricks rss feed
     __________________________________________________________________

     * [29]human pose estimation using deep learning in opencv
     * [30]deep learning based image colorization with opencv
     * [31]deep learning based edge detection in opencv
     __________________________________________________________________

share this article

   [32]share on facebook [33]share on twitter [34]share on pinterest

   copyright    2017 cv-tricks.com

references

   visible links
   1. https://cv-tricks.com/computer-vision/case-study-training-better-haar-based-object-detectors/
   2. https://cv-tricks.com/artificial-intelligence/deep-learning/deep-learning-frameworks/install-tensorflow-1-0-gpu-ubuntu-14-04-aws-p2-xlarge/
   3. https://cv-tricks.com/
   4. https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/
   5. https://cv-tricks.com/
   6. https://cv-tricks.com/category/tensorflow-tutorial
   7. https://cv-tricks.com/about-us/
   8. https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/
   9. https://github.com/tensorflow/models/tree/master/im2txt/im2txt
  10. https://github.com/karpathy/neuraltalk2
  11. http://venturebeat.com/2015/02/09/microsoft-researchers-say-their-newest-deep-learning-system-beats-humans-and-google/
  12. https://www.technologyreview.com/s/544651/baidus-deep-learning-system-rivals-people-at-speech-recognition/
  13. http://fortune.com/2015/03/17/google-facenet-artificial-intelligence/
  14. https://cv-tricks.com/tag/deep-learning/
  15. https://cv-tricks.com/tag/im2txt/
  16. https://cv-tricks.com/tag/id56/
  17. https://cv-tricks.com/tag/show-and-tell/
  18. https://cv-tricks.com/tag/show-attend-tell/
  19. https://cv-tricks.com/tag/tensorflow/
  20. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  21. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  22. https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
  23. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  24. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  25. https://cv-tricks.com/tensorflow-tutorial/keras/
  26. https://cv-tricks.com/tensorflow-tutorial/keras/
  27. https://cv-tricks.com/feed/
  28. https://cv-tricks.com/
  29. https://cv-tricks.com/pose-estimation/using-deep-learning-in-opencv/
  30. https://cv-tricks.com/opencv/deep-learning-image-colorization/
  31. https://cv-tricks.com/opencv-dnn/edge-detection-hed/
  32. http://www.facebook.com/share.php?u=https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/
  33. https://twitter.com/share?url=https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/
  34. http://pinterest.com/pin/create/button/?url=https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/

   hidden links:
  36. https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/
  37. https://cv-tricks.com/tensorflow-tutorial/training-convolutional-neural-network-for-image-classification/
  38. https://cv-tricks.com/id98/understand-resnet-alexnet-vgg-inception/
  39. https://cv-tricks.com/object-detection/faster-r-id98-yolo-ssd/
  40. https://cv-tricks.com/artificial-intelligence/show-attend-tell-image-captioning-explained/#top
