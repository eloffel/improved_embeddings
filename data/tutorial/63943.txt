   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

automatic recognition of speed limit signs         deep learning with keras and
tensorflow

   [14]go to the profile of imaad mohamed khan
   [15]imaad mohamed khan (button) blockedunblock (button) followfollowing
   dec 7, 2017

   before i came to germany, i was fascinated by stories about how there
   are no speed limits on german roads. to my disappointment, speed limits
   do exist almost everywhere. there are only certain areas on the highway
   (autobahn) which are designated as no speed limit zones, where cars and
   other vehicles can test the limits of their engineering.

   what if a driver is speeding and he suddenly enters a zone where he has
   to follow a certain speed limit? one way, and the most common way, of
   solving this is that the driver applies brakes manually and reduces the
   accelaration. but hey, we   re in 2017 and computers can now recognize
   more than cats and dogs! so let   s try to make our computers recognize
   speed limit signs automatically!

   i based my implementation based on a tutorial called [16]introduction
   to convolutional neural networks using tensorflow and keras by
   [17]oliver zeigermann. it was a really good talk and i liked that he
   explored the implementation from the perspective of an engineer and not
   that of a researcher. i have been actively trying both approaches,
   where i learn some math and i also learn how to implement it using
   various libraries.

   one of the key things that i discovered in this tutorial was microsoft
   azure notebooks. during his tutorial, zeigermann asked the audience to
   either run notebooks locally or on the cloud. he said he preferred the
   cloud since the cloud machine was more powerful and had more specs than
   his machine. this was true of my machine as well. prior to this, i had
   always worked on jupyter notebooks on my cpu and have had issues with
   my machine getting heated quickly. so i decided to try out azure
   notebooks and i had a really good experience. they are fast and can be
   set up in no time. and they are free! i had to search for the catch,
   but there isn   t one! for people just starting out in python and data
   science, azure notebooks can be help them get off the ground quickly.

   the first task for implementing any data science or ai project is to
   find a suitable dataset. the dataset for the tutorial was provided by
   the presenter. he mentions in the video that the tutorial is based on a
   [18]similar project by [19]waleed abdulla. i am assuming he obtained
   the data from the other project. in that case, it is based on a dataset
   called [20]belgian traffic sign dataset.

   after obtaining the data, we still need to prepare our data in a form
   that can be used by the machine learning models. the images in the
   dataset are in a .ppm format. we convert this into a form usable for
   analysis using the library skimage. here   s the function that performs
   the conversion.
import os
import skimage.data
def load_data(data_dir):
    directories = [d for d in os.listdir(data_dir)
                  if os.path.isdir(os.path.join(data_dir, d))]
    labels = []
    images = []
    for d in directories:
        label_dir = os.path.join(data_dir, d)
        file_names = [os.path.join(label_dir, f)
                      for f in os.listdir(label_dir) if f.endswith(".ppm")]
        for f in file_names:
            images.append(skimage.data.imread(f))
            labels.append(int(d))
    return images, labels

   after we load our data using the load_data function, we try to see how
   our images and labels are organized. we write a function that allows us
   to visualize our data.
import matplotlib
import matplotlib.pyplot as plt
def display_images_and_labels(images, labels):
    """display the first image of each label."""
    unique_labels = set(labels)
    plt.figure(figsize=(15, 15))
    i = 1
    for label in unique_labels:
        # pick the first image for each label.
        image = images[labels.index(label)]
        plt.subplot(8, 8, i)  # a grid of 8 rows x 8 columns
        plt.axis('off')
        plt.title("label {0} ({1})".format(label, labels.count(label)))
        i += 1
        _ = plt.imshow(image)
    plt.show()
display_images_and_labels(images, labels)

   this results in the following.
   [1*ytlgtkiwfdaytxkjzojbtw.png]
   speed limit images with their respective labels and counts

   we see that our dataset has 6 different categories of speed limit
   images. we have images that display 30, 50, 70, 80, 100 and 120 with
   79, 81, 68, 53, 41 and 57 samples of each category respectively. this
   isn   t a very large dataset, but it works well enough for our problem.

   having observed the way our images are organized, let   s now look at the
   shape of each image, with its minimum and maximum value of rgb colors.
for image in images[:5]:
    print("shape: {0}, min: {1}, max: {2}".format(image.shape, image.min(), imag
e.max()))
output:
shape: (21, 22, 3), min: 27, max: 248
shape: (23, 23, 3), min: 10, max: 255
shape: (43, 42, 3), min: 11, max: 254
shape: (61, 58, 3), min: 3, max: 255
shape: (28, 27, 3), min: 8, max: 65

   as we can clearly see from the output that the shape of each image
   varies along with large variation in the distribution of the minimum
   and maximum rgb values. our neural network models expect all of our
   images to have the same shape. while resizing, let us also normalize
   our minimum and maximum rgb values between 0 and 1. we use skimage   s
   transform function to transform the images into 64x64 pixels with 3
   channels for rgb.
import skimage.transform
images64 = [skimage.transform.resize(image, (64,64)) for image in images]

   finally we perform one last step of data preparation where we convert
   our images and labels to numpy arrays. we also use to_categorical
   function to convert our labels to different different categorical
   arrays which contain 1 if that particular category is represented else
   0.
import numpy as np
from keras.utils.np_utils import to_categorical
y = np.array(labels)
x = np.array(images64)
num_categories = 6
y = to_categorical(y, num_categories)

   now after this stage, zeigermann went into explaining how he
   experimented by training a simple keras model which had good
   performance on training data but didn   t do as well on testing data. he
   also discusses why rmsprop seems to be the better optimizer when
   compared to other methods like sgd, adagrad etc.

   finally the experiments lead him to convolutional neural networks.
   convolutional neural networks or id98 are a type of neural networks that
   explicitly assume that the inputs are images. much of the deep learning
   revolution in id161 has been lead by id98s. here   s an example
   of their architecture.
   [1*2swb6cmxzbpzijmevfbe-g.jpeg]
   id98 architecture. source:
   [21]http://cs231n.github.io/convolutional-networks/

   since we are focusing on the application side of id98s here, i will not
   go into the details of what each layer is and how we can tune the
   hyperparameters. we run our id98s by using the keras library. keras is a
   high level api built on tensorflow and theano (theano is no longer
   maintained). let us look into the code now.
from keras.models import model
from keras.layers import dense, flatten, input, dropout
from keras.layers import convolution2d, maxpooling2d
from sklearn.model_selection import train_test_split
inputs = input(shape=(64,64,3))
x = convolution2d(32, 4,4, border_mode='same', activation='relu')(inputs)
x = convolution2d(32, 4,4, border_mode='same', activation='relu')(x)
x = convolution2d(32, 4,4, border_mode='same', activation='relu')(x)
x = maxpooling2d(pool_size=(2,2))(x)
x = dropout(0.25)(x)
# one more block
x = convolution2d(64, 4, 4, border_mode='same', activation='relu')(x)
x = convolution2d(64, 4, 4, border_mode='same', activation='relu')(x)
x = maxpooling2d(pool_size=(2, 2))(x)
x = dropout(0.25)(x)
x = flatten()(x)
# fully connected, 256 nodes
x = dense(256, activation='relu')(x)
x = dropout(0.50)(x)
# softmax activation, 6 categories
predictions = dense(6, activation='softmax')(x)
model = model(input=inputs, output=predictions)
model.compile(optimizer='rmsprop',
              loss='categorical_crossid178',
              metrics=['accuracy'])
model.fit(x_train, y_train, nb_epoch=50, batch_size=100)

   the model now runs for 50 epochs with a batch size of 100. it trains on
   80% of total data which was split earlier using sklearn   s
   train_test_split function. here   s what we get after running for 50
   epochs.
   [1*tfyrm20q4v9dzklrjq3uaq.jpeg]
   id98 output on training data after 50 epochs

   the model converges with a loss of 0.2489 and an accuracy of about 92%.
   not bad for a model without any efforts on hyperparameter tuning! let
   us look at the accuracy of the model on the training dataset.
train_loss, train_accuracy = model.evaluate(x_train,y_train, batch_size=32)
train_loss, train_accuracy

   we get an accuracy of 83.49% on the training data with a loss of 0.41.
   let   s look at the performance on our testing dataset.
test_loss, test_accuracy = model.evaluate(x_test, y_test, batch_size=32)
test_loss, test_accuracy

   we get an accuracy of about 68 % with a loss of 1.29. this is not that
   great. but there is something interesting happening here, which was
   mentioned by zeigermann as well. this was my second run of the model.
   on my previous run, i got a testing accuracy of about 95%. clearly, the
   model outputs are not deterministic. perhaps there is a way to make it
   more stable. i will have to read more on that.

   finally, let us visualize some of the predictions of our model on the
   testing dataset. we randomly sample 10 images from the testing dataset
   and use matplotlib to plot the predictions. we show the predicted class
   along with it   s ground truth value. in case the prediction of our model
   is correct, we color it green otherwise red.
import random
random.seed(3)
sample_indexes = random.sample(range(len(x_test)), 10)
sample_images = [x_test[i] for i in sample_indexes]
sample_labels = [y_test[i] for i in sample_indexes]
#get the indices of the array using argmax
ground_truth = np.argmax(sample_labels, axis=1)
x_sample = np.array(sample_images)
prediction = model.predict(x_sample)
predicted_categories = np.argmax(prediction, axis=1)
# display the predictions and the ground truth visually.
def display_prediction (images, true_labels, predicted_labels):
    fig = plt.figure(figsize=(10, 10))
    for i in range(len(true_labels)):
        truth = true_labels[i]
        prediction = predicted_labels[i]
        plt.subplot(5, 2,1+i)
        plt.axis('off')
        color='green' if truth == prediction else 'red'
        plt.text(80, 10, "truth:  {0}\nprediction:    {1}".format(truth, predict
ion),
                 fontsize=12, color=color)
        plt.imshow(images[i])
display_prediction(sample_images, ground_truth, predicted_categories)

   as you can see below, our model correctly predicts 8/10 randomly
   sampled images from the test dataset. not bad! the previous time when i
   had got an accuracy of 95% on the test dataset, it predicted 10/10
   images correctly!
   [1*mun51lcsndql_sc6w8qvlg.png]

   this was a great project! i definitely did learn a lot. thanks to
   oliver zeigermann for his tutorial (and the code) and waleed abdulla
   for his blog-post. i am looking forward to doing more walk-throughs and
   exploring this amazingly crazy world of artificial intelligence!

     * [22]machine learning
     * [23]deep learning
     * [24]data science
     * [25]driving
     * [26]road safety

   (button)
   (button)
   (button) 175 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of imaad mohamed khan

[28]imaad mohamed khan

   writing at the intersection of data and the world.

     (button) follow
   [29]hacker noon

[30]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 175
     * (button)
     *
     *

   [31]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [32]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/310d90af9826
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/automatic-recognition-of-speed-limit-signs-deep-learning-with-keras-and-tensorflow-310d90af9826&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/automatic-recognition-of-speed-limit-signs-deep-learning-with-keras-and-tensorflow-310d90af9826&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_nt5i1gfuxtq1---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@imaadmkhan1?source=post_header_lockup
  15. https://hackernoon.com/@imaadmkhan1
  16. https://www.youtube.com/watch?v=wihi1w6noz0
  17. http://zeigermann.eu/
  18. https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6
  19. https://medium.com/@waleedka
  20. http://btsd.ethz.ch/shareddata/
  21. http://cs231n.github.io/convolutional-networks/
  22. https://hackernoon.com/tagged/machine-learning?source=post
  23. https://hackernoon.com/tagged/deep-learning?source=post
  24. https://hackernoon.com/tagged/data-science?source=post
  25. https://hackernoon.com/tagged/driving?source=post
  26. https://hackernoon.com/tagged/road-safety?source=post
  27. https://hackernoon.com/@imaadmkhan1?source=footer_card
  28. https://hackernoon.com/@imaadmkhan1
  29. https://hackernoon.com/?source=footer_card
  30. https://hackernoon.com/?source=footer_card
  31. https://hackernoon.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/310d90af9826/share/twitter
  35. https://medium.com/p/310d90af9826/share/facebook
  36. https://medium.com/p/310d90af9826/share/twitter
  37. https://medium.com/p/310d90af9826/share/facebook
