   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*dw0ccmj1hz0ovsxi7kz5mq.jpeg]

machine learning fundamentals (ii): neural networks

   [16]go to the profile of conor mcdonald
   [17]conor mcdonald (button) blockedunblock (button) followfollowing
   dec 21, 2017

   in my [18]previous post i outlined how machine learning works by
   demonstrating the central role that cost functions and id119
   play in the learning process. this post builds on these concepts by
   exploring how neural networks and deep learning work. this post is
   light on explanation and heavy on code. the reason for this is that i
   cannot think of any way to elucidate the internal workings of a neural
   network more clearly that the incredible videos put together by three
   blue one brown         see the full playlist [19]here.

   these videos show how neural networks can be fed raw data         such as
   images of digits         and can output labels for these images with amazing
   accuracy. the videos highlight the underlying mathematics of neural
   networks in a very accessible way, meaning even those without a heavy
   math background can begin to understand what goes on underneath the
   hood in deep learning.
   [1*7hmsjoabtcrzwmvob3fjla.png]

   this post is intended as a    code-along    supplement to these videos
   (full tensorflow and keras scripts are available at the end of the
   post). the purpose is to demonstrate how a neural network can be
   defined and executed in tensorflow such that it can identify digits
   such as those shown above.

   tensorflow (for those who do not know) is google   s deep learning
   library and while it is quite low-level (i typically use the
   higher-level keras library for my deep learning projects), i think it
   is a great way to learn. this is simply because, although it does
   incredible amounts of magical things behind the scenes, it requires you
   (yes you!) to explicitly define the architecture of the nn. in doing so
   you   ll gain a better understanding of how these networks work.

neural networks

   neural networks are mathematical and computational abstractions of
   biological processes that take place in the brain. specifically, they
   loosely mimic the    firing    of interconnected neutrons in response to
   stimuli         such as new incoming information. i don   t find biological
   analogies particularly helpful for understanding neural networks, so i
   won   t continue down this path.

   neural networks work by computing weighted summations of input vectors
   that are then passed through non-linear id180, thereby
   creating a mapping from input to output via a non-linear transformation
   layer. the weights (represented by neutrons) in the transformation, or
   hidden, layer are iteratively adjusted such that they come to represent
   relationships in the data that map inputs to outputs.

defining layers and activations

   in the first step we define the architecture of our network. we will
   create a four layer network comprised of one input layer, two hidden
   layers and one output layer. note how the output from one layer is the
   input to the next. this model is quite simple as far as neural nets go,
   it is comprised of dense or fully connected layers, but is still quite
   powerful.
   [1*dw0ccmj1hz0ovsxi7kz5mq.jpeg]

   the input layer         also sometimes referred to as the visible layer         is
   the layer of the model that represents the data in its raw form. for
   example, for the digit classification task the visible layer is
   represented by numbers corresponding to pixel values.
   [0*phal_ij9fjvhqbnp.]

   in tensorflow (all code is below) we need to create a placeholder
   variable to represent this input data, we will also create a
   placeholder variable for the correct label corresponding to each input.
   this effectively sets up the training data         the x values and y labels
   we will use to train the neural network.

   the hidden layer(s) enable the neural network to create new
   representations of the input data that the model uses to learn complex
   and abstract relationships between the data and the labels. each hidden
   layer is comprised of neurons, each representing a scalar value. this
   scalar value is used to compute a weighted sum of the input plus a bias
   (essentially y1 ~ wx + b)         creating a linear (or more specifically an
   affine) transformation.

   in tensorflow you have to explicitly define the variables for the
   weights and bias that comprise this layer. we do so by wrapping them in
   the tf.variable function         these are wrapped as variables because the
   parameters will update as the model learns the weights and bias that
   best represent relationships in the data. we instantiate the weights
   with random values with very low variance, and fill the bias variable
   with zeros. we then define the id127 that takes place
   in the layer.

   iframe: [20]/media/72c6a161b6a9207e8d1ad0766ad14ac5?postid=f1e7b2cb3eef

   this transformation is then passed through an activation function,
   (here i am using relu or rectified linear units) to make make the
   output of the linear transformation non-linear. this allows the neural
   net to model complex non-linear relationships between input and
   output         check out siraj raval   s excellent video explainer on
   id180 [21]here.

   the output layer is the final layer in the model and, in this case, is
   size ten, one node for each label. we apply a softmax activation to
   this layer so that it outputs values between 0 and 1 across the final
   layer nodes         representing probabilities across the labels.

cost function and optimisation

   now that the neural net architecture is defined, we set the cost
   function and optimiser. for this task i use categorical cross id178.
   i also define an accuracy measure that can be used to evaluate the
   model   s performance. finally, i set the optimiser as stochastic
   id119 and call its minimise method once it is instantiated.

   iframe: [22]/media/98884dd9694794a6e3c6844e57ac3bc3?postid=f1e7b2cb3eef

   finally, the model can be run         here 1000 iterations are run. on each
   iteration a mini-batch of the data is fed to the model, it makes
   predictions, computes the loss and through backpropogation, updates the
   weights and repeats the process.
   [0*61zannpbpmtzllpz.]

   **from three blue one brown   s    but, what is a neural network?    video**

   this simple(ish) model gets to around 95.5% accuracy on the test set,
   which isn   t too bad, but could be a lot better. in the plots below you
   can see the accuracy and cost for each iteration of the model, one
   thing that clearly stands out is the discrepancy between the
   performance on the train set and performance on the test set.
   [0*9xs5aqm6ni8qf5zk.]

   this is indicative of overfitting         that is, the model is learning the
   training data too well, which is limiting its generalisability. we can
   handle overfitting using regularisation methods, which will be the
   subject of my next post.

   thank you for reading     

   p.s the full tensorflow script can be found [23]here and the same model
   defined in keras [24]here.
     __________________________________________________________________

   originally published at [25]dataflume.wordpress.com on december 21,
   2017.

     * [26]machine learning
     * [27]deep learning
     * [28]tensorflow
     * [29]data science
     * [30]towards data science

   (button)
   (button)
   (button) 721 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of conor mcdonald

[32]conor mcdonald

   senior data scientist @ amazon, phd, economics (university of leeds).
   [33]@conormacd

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 721
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f1e7b2cb3eef
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ko7mcj4ts04q---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@conrmcdonald?source=post_header_lockup
  17. https://towardsdatascience.com/@conrmcdonald
  18. https://dataflume.wordpress.com/2017/11/27/understanding-machine-learning-fundamentals-via-linear-regression/
  19. https://www.youtube.com/playlist?list=plzhqobowtqdnu6r1_67000dx_zcjb-3pi
  20. https://towardsdatascience.com/media/72c6a161b6a9207e8d1ad0766ad14ac5?postid=f1e7b2cb3eef
  21. https://www.youtube.com/watch?v=-7scqpjt7uo&t=331s
  22. https://towardsdatascience.com/media/98884dd9694794a6e3c6844e57ac3bc3?postid=f1e7b2cb3eef
  23. https://gist.github.com/conormm/1c82b093c9c6002e7ca6ff6e9fb34f05
  24. https://gist.github.com/conormm/e1dd2ee37733f4817e09a41d625d9e7f
  25. https://dataflume.wordpress.com/2017/12/21/machine-learning-fundamentals-ii-neural-networks/
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/deep-learning?source=post
  28. https://towardsdatascience.com/tagged/tensorflow?source=post
  29. https://towardsdatascience.com/tagged/data-science?source=post
  30. https://towardsdatascience.com/tagged/towards-data-science?source=post
  31. https://towardsdatascience.com/@conrmcdonald?source=footer_card
  32. https://towardsdatascience.com/@conrmcdonald
  33. http://twitter.com/conormacd
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/f1e7b2cb3eef/share/twitter
  40. https://medium.com/p/f1e7b2cb3eef/share/facebook
  41. https://medium.com/p/f1e7b2cb3eef/share/twitter
  42. https://medium.com/p/f1e7b2cb3eef/share/facebook
