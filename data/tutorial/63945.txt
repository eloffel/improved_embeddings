   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

dl05: convolutional neural networks

understanding and visualizing id98s

   [14]go to the profile of sarthak gupta
   [15]sarthak gupta (button) blockedunblock (button) followfollowing
   dec 21, 2017

   previous posts:
   [16]dl01: neural networks theory
   [17]dl02: writing a neural network from scratch (code)
   [18]dl03: id119
   [19]dl04: id26
   [1*_ynzecdat9k-6xsqedxqmg.png]

   now that we understand id26, let   s dive into convolutional
   neural networks (id98s)!

   (there are a lot of images in this one, so please be patient while they
   load)

     code for visualization can be found [20]here
     ([21]https://github.com/thesemicolonguy/convisualize_nb)

intuition

   * applying feedforward networks to images was extremely difficult. the
   number of parameters associated with such a network was huge. a better,
   improved network was needed specifically for images.

   *[22] hubel and wiesel performed experiments that gave some intuition
   about how images are biologically processed by the brain. they found
   that the brain responded in specific ways when it saw edges, patterns,
   etc.
   the visusal cortex was reported to be divided into layers, where the
   layers to the front recognized simple features like edges, but layers
   to the back recognized more complicated features.

concepts

     * filters

   the basic concept behind id98s is filters/kernels. you can think of them
   as smaller sized images which are then slided over the input image. by
   sliding over, i mean that the same filter is multiplied to different
   regions in the image (i.e. convolved over the image).

   images are represented by an array of shape (height, width,
   depth/channels), where there are 3 channels (rgb) for colour images,
   and 1 channel for grayscale images. for a filter to be able to
      convolve    over the image, it should have the same number of channels
   as the input. the output is the sum of the elementwise multiplication
   of the filter elements and the image (can be thought of as dot
   product).

   so, we can use any number of filters. the output depth dimension will
   be equal to the number of filters we use.
   [0*9j3mk1gd2zrfdzdn.gif]
   source: [23]http://cs231n.github.io/convolutional-networks/

   in the above animation, two filters are being used. notice that the
   depth (or channel) dimension of each filter is same as the depth of
   input (in this case, 3). the convolution operation is also clearly
   depicted in the animation. the output depth dimension is equal to the
   number of filters (in this case, 2). the output is the sum of the
   element-wise multiplication of filter and image over all channels (plus
   some optional bias term). so, it can be thought of as the dot product
   over the 3x3x3 (filter_height *filter_width * input_depth) volume.

   to get better intuition of filters, let   s look at an example of a
   filter that detects vertical edges.
   [1*jfr2z5xtadoepvmtjgyyma.jpeg]
   source:
   [24]https://www.coursera.org/learn/convolutional-neural-networks/lectur
   e/4trod/edge-detection-example

   the filter can be thought of as detecting some part of the image that
   is lighter to the left and darker to the right.

   sliding the filters also give an additional advantage of translation
   invariance, because the same features can be detected anywhere,
   irrespective of where they are present in the image.

   also, in practice, it is found that the initial layers tend to learn
   simpler features like edges, corners, etc. whereas the deeper layers
   tend to learn complex features like eyes, lips, face, etc.

   intuitively, it can be thought that we are combining the lower level
   features (eg. edges), and these combinations are resulting in a mixture
   of such features, thus resulting in higher level features. an
   oversimplified example would be, somehow combining two edge detecting
   filters to detect corners:
   [1*iyn-8q5kgdgpljkckoueoq.jpeg]
   this is just for intuition (made using awwapp.com)

   hence, a complete convnet typically looks like this:
   [0*vwxnugfuuvs0705u.jpeg]
   source: [25]http://cs231n.github.io/convolutional-networks/

   filters are randomly initialized, and are then learned via
   id26.
     * layers

   there are mainly 4 types of layers in id98s:
    1. conv layers : these layers perform the convolution operation, as
       described above. the number of learnable parameters in these layers
       is equal to the number of parameters in each filter multiplied by
       the number of filters i.e. filter_height * filter_width *
       number_of_filters
    2. non-linearity : conv layers are typically followed by a
       non-linearity layer. usually relu is used. there are no learnable
       parameters in this layer.
    3. pooling layer : this layer is used to downsample the image. it
       results in lesser parameters in the next layers, and hence the
       convnet can be made a little more deep.

   [0*e273egtyfyo7etso.jpeg]
   source: [26]http://cs231n.github.io/convolutional-networks/

   in theory, average pooling looks like the best option i.e. taking parts
   of the image, and averaging out that part to give one pixel value for
   that part of the image. but in practice, it was found that max-pooling
   works better, i.e. taking the maximum from that region in the image.
   pooling layers maintain the depth dimension, i.e. pooling is done
   independently on all channels of the input. there are no learnable
   parameters in this layer.

   4. fully connected : these are usually at the end of the convnet. they
   are simple feedforward layers, and have as many learnable parameters as
   a normal feedforward net would have.
     * padding

   one might argue that according to the convolution operation described
   above, the edges and corners of the image are getting lesser importance
   than the part in the middle of the image. to overcome this, the input
   is often zero-padded, i.e. a layer of zeros is added to all sides of
   the image. also, zero-padding allows us to alter the size of the output
   image according to the requirement.
     * stride

   the size of the jump while sliding the filter over the image is called
   stride. the more the stride, the smaller the output image will be.

   the formula for calculating size of next layer after a conv layer is
   ((w   f+2p)/s)+1, where where w is the size of the image (width for
   horizontal dimension, and height for vertical dimension), f is the size
   of filter, p is the padding and s is the stride.

   another cool think to note is that as we move deeper into the network,
   the effective receptive field of the nodes increases, i.e. the node can
   be thought of as looking at a larger part of the image as compared to
   the layer before it.

id26

   backprop is done normally like a feedforward neural network.
   derivatives of all weights (or parameters) are calculated w.r.t. loss,
   and are updated by id119 (or some variations of gradient
   descent, which will be discussed in a later post).

   while applying the chain rule to find gradients w.r.t. loss, the
   gradient of one weight will come from all the nodes in the next layer
   to which the weight had contributed in the forward pass (it   ll be the
   sum of all these gradients, according to the multivariate chain rule of
   differentiation).

visualization

   now that we understand how a convnet works, let   s try to visualize some
   of the stuff that happens inside the convnet.

   an awesome explanation of some of the famous network architectures can
   be found [27]here.

   visualizing weights of vgg neworks is not of much use, since the
   filters are only 3x3 in size. however, alexnet has 11x11 sized filters.
   the following image shows some filters learnt by alexnet.
   [1*01ptbzwgwmnr2e895lkfyg.jpeg]
   source: [28]http://cs231n.github.io/convolutional-networks/

   from this point on, all visualizations are done on vgg16 in pytorch.
   the code can be found [29]here.

   so, let   s see what the output of each layer looks like:

   input image:
   [1*ing-0jftsc52zhxhb3op6w.png]

   output at various layers:
   [1*sfk7rq9dxnwj0dtg06dswg.png]

   since the number of channels at every layer are different (not 1 or 3),
   i   ve averaged over all channels to finally give a grayscale image (the
   color scheme is because of the default cmap scheme used by matplotlib).

   notice that maxpooling is simply downsampling the image.

   if we don   t take average over all channels, we can see the output
   ofeach channel (i.e. output of each filter) at a particular layer.
   let   s have a look:
   [1*m3yv81xxhpmpjeadlmn7sa.png]
   output from first conv layer filters (all 64 channels from this layer)
   [1*c6chleubnjbwge5ms6yuvw.png]
   output from last conv layer filters (first 484 channels are shown out
   of the total 512 channels at this layer)

   we   ve seen the outputs from the filters, now let   s see what the filters
   actually look like:
   [1*ewcl0_j-ql-oif7nnvnjdg.png]
   3x3 filters from first conv layer (64 filters)
   [1*ia-6ak1r5m8znj2lbhs_rq.png]
   3x3 filters from last conv layer (first 484 out of total 512 filters at
   this layer)

occlusion

   now let   s try to see actually what part of the image is responsible for
   the classification of an image.

   first, i used a technique called occlusion. in this, we simply remove a
   region from an image, and then find the classification id203 of
   the image being the true class. we do this for a lot of regions from
   the image (basically, we remove regions by slide over the entire
   image).

   if the id203 score comes out to be high, then we know that the
   blacked out region isn   t important for classification; the image is
   getting correctly classified without that region. if the id203
   comes out to be low, it means that the blacked out region was somehow
   important for correctly classifying the image.

   the blue portion indicates low output score id203, and the yellow
   region shows high output class id203. so, the blue part depicts
   the parts that are important for classification.
   [1*e8lo2fztiuysuq8e5ecjyq.png]
   occlusion heatmap

   pretty impressive, right?

   but the results are not so good, and the algorithm takes a lot of time
   to run (because it runs forward pass for classification too many
   times).

   so, we try a different approach.

saliency maps

   the concept behind saliency maps is simple. we find the derivatives of
   input image w.r.t. output score for an (image, class) pair. by
   definition, the derivative tells us the rate of change of
   classification score w.r.t. that pixel.

   that is, how much the output class score will change if we change a
   particular pixel in the input image.

   here are some examples:
   [1*ndf3jfsyxaecsxvufjjiqa.png]
   [1*bwo0vly734tg1qzhbymxuq.png]
   [1*q1w_r_ejpthrt4lofzmeya.png]

   this runs extremely quick, and the results are pretty okay. but as you
   can see, there is a still huge room for improvement.

smoothgrad

   we can improve saliency maps (also called sensitivity maps) using
   smoothgrad technique. i would like to quote the authors of this paper
   here:
"the apparent noise one sees in a sensitivity map may be due to essentially mean
ingless local variations in partial derivatives. after all, given typical traini
ng techniques there is no reason to expect derivatives to vary smoothly."

   the authors added some noise to the image and plotted the derivatives
   of the three channels (rgb) w.r.t. the output class score:
   [1*rvq8meseskytv6ifqosp1g.jpeg]
   source: [30]https://arxiv.org/pdf/1706.03825.pdf (the original
   smoothgrad paper)

   here, t determines the amount of noise to be added to the image. after
   adding a little bit of noise, the image appears not to change at all to
   the naked eye, i.e. it seems to be the same image, which is obvious.
   however, the derivatives of the input image fluctuate a lot.

   so, the authors proposed that we do multiple iterations, and for every
   iteration, add a little bit of random noise to the original image.
   we   ll calculate the derivates of the input image (with added noise) for
   every iteration, and then finally take average of these derivatives
   before plotting it.

   in the average, only those pixels will have high value of corresponding
   derivatives that had high value in every iteration. so, meaningless
   local variations in the derivatives will be removed.
   [1*1hi5r3jm5v6m6dufdztzua.png]
   [1*wzaweq54ovj8wdga6omqxg.png]
   [1*opxrfarmrfombpzzdjjzdq.png]

   as you can see, the results are much better than simple saliency maps.

   these techniques are extremely useful and can be used for semantic
   segmentation using grabcut algorithm (which i haven   t implemented
   myself yet).
   [1*jmhxxocych2e3ej51iikdq.jpeg]
   source:
   [31]http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf

class models

   we can also visualize class models, i.e. what the id98 thinks is thebest
   representation for a particular class. this is done by an algorithm
   called gradient ascent.

   we initialize the input image with zeros (although initializing it to
   the mean of id163 training examples gives slightly better results).
   then we do a forward pass, and calculate the class score for that
   image. the back pass is where things get interesting. we don   t update
   the weights (like we usually do while training). weights are kept
   frozen. instead, we calculate the gradients of input image w.r.t. the
   class score, and update all input pixels in a way that maximizes the
   output class score.

   side note: we want to maximize the class score and not the softmax
   id203 because the softmax id203 can be increased by
   decreasing scores for other classes too, but just want to increase the
   score of the target class.

   various id173 techniques can also be used here (like l2,
   clipping, etc.).

   in the animations below, we started with a blank image, and notice how
   parts resembling actual objects start to appear!

   so let   s see this in action!
   [1*-6ne6kwubwnaobsn4fvfia.gif]
   flamingo
   [1*v06gvmbaiihwoahlarnjbq.gif]
   mosque
   [1*z9x5g7re_z0fwgqyf7otea.gif]
   zebra
   [1*ojhtshyxdgtd2eehotpcwa.gif]
   dumbbell

   that   s all for this one, folks! if you have any questions, leave them
   in comments.

   if you enjoyed this, give me a      on [32]github and      on this post :)

     * [33]machine learning
     * [34]deep learning
     * [35]neural networks
     * [36]python
     * [37]visualization

   (button)
   (button)
   (button) 169 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of sarthak gupta

[39]sarthak gupta

   [40]www.github.com/sar-gupta [41]https://sar-gupta.github.io

     (button) follow
   [42]hacker noon

[43]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 169
     * (button)
     *
     *

   [44]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [45]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1d3bb7fff586
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/dl05-convolutional-neural-networks-1d3bb7fff586&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/dl05-convolutional-neural-networks-1d3bb7fff586&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_in9jwzdhyvgi---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@sargupta?source=post_header_lockup
  15. https://hackernoon.com/@sargupta
  16. https://hackernoon.com/dl01-writing-a-neural-network-from-scratch-theory-c02ccc897864
  17. https://hackernoon.com/dl02-writing-a-neural-network-from-scratch-code-b32f4877c257
  18. https://hackernoon.com/dl03-gradient-descent-719aff91c7d6
  19. https://hackernoon.com/dl04-id26-bbcfbf2528d6
  20. https://github.com/thesemicolonguy/convisualize_nb
  21. https://github.com/thesemicolonguy/convisualize_nb
  22. https://www.ncbi.nlm.nih.gov/pmc/articles/pmc1359523/pdf/jphysiol01247-0121.pdf
  23. http://cs231n.github.io/convolutional-networks/
  24. https://www.coursera.org/learn/convolutional-neural-networks/lecture/4trod/edge-detection-example
  25. http://cs231n.github.io/convolutional-networks/
  26. http://cs231n.github.io/convolutional-networks/
  27. https://www.youtube.com/watch?v=daocjicfr1y&list=plc1qu-lwwrf64f4qkqt-vg5wr4qee1zxk&index=9
  28. http://cs231n.github.io/convolutional-networks/
  29. https://github.com/thesemicolonguy/convisualize_nb
  30. https://arxiv.org/pdf/1706.03825.pdf
  31. http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf
  32. https://github.com/thesemicolonguy/convisualize_nb/
  33. https://hackernoon.com/tagged/machine-learning?source=post
  34. https://hackernoon.com/tagged/deep-learning?source=post
  35. https://hackernoon.com/tagged/neural-networks?source=post
  36. https://hackernoon.com/tagged/python?source=post
  37. https://hackernoon.com/tagged/visualization?source=post
  38. https://hackernoon.com/@sargupta?source=footer_card
  39. https://hackernoon.com/@sargupta
  40. http://www.github.com/sar-gupta
  41. https://sar-gupta.github.io/
  42. https://hackernoon.com/?source=footer_card
  43. https://hackernoon.com/?source=footer_card
  44. https://hackernoon.com/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://medium.com/p/1d3bb7fff586/share/twitter
  48. https://medium.com/p/1d3bb7fff586/share/facebook
  49. https://medium.com/p/1d3bb7fff586/share/twitter
  50. https://medium.com/p/1d3bb7fff586/share/facebook
