   iframe: [1]//www.googletagmanager.com/ns.html?id=gtm-p824sk

   [2]contact us [3][logo_skymind_white.svg]
   ai platform
   [4]skil [5]interactive demo [6]subscription [7]documentation
   [8]community support
   why skymind?
   [9]data scientists [10]solution architects [11]devops and sre
   [12]innovation leaders
   [13]solutions
   case studies
   [14]rpa [15]telecom fraud [16]insurance [17]supply chain & logistics
   [18]cybersecurity and data centers [19]financial services [20]image
   recognition [21]commerce and crm
   [22]about
   resources
   [23]blog [24]ai wiki [25]open-source

a.i. wiki

   do you like this content? we'll send you more.

   iframe: [26]https://go.pardot.com/l/456082/2018-09-28/jsv68v

   [ ] directory
   [27]artificial intelligence wiki
   ____________________
     *
     *
     * [28]ai vs. ml vs. dl
     *
     * [29]apache spark & deep learning
     *
     * [30]attention mechanisms & memory networks
     * [31]automated machine learning & ai
     * [32]ai & autonomous vehicles
     * [33]id26
     * [34]bag of words & tf-idf
     * [35]clojure ai
     * [36]comparison of ai frameworks
     * [37]convolutional neural network (id98)
     * [38]data for deep learning
     * [39]datasets and machine learning
     *
     * [40]decision tree
     * [41]deep autoencoders
     * [42]deep-belief networks
     * [43]deep id23
     * [44]deep learning resources
     * [45]deeplearning4j
     *
     * [46]denoising autoencoders
     * [47]machine learning devops
     * [48]differentiable programming
     * [49]eigenvectors, eigenvalues, pca, covariance and id178
     * [50]evolutionary & id107
     * [51]fraud and anomaly detection
     * [52]generative adversarial network (gan)
     * [53]glossary
     * [54]gluon
     * [55]graph analytics
     * [56]hopfield networks
     * [57]hyperparameter
     * [58]wiki home
     * [59]java ai
     * [60]jumpy
     * [61]id28
     * [62]lstms & id56s
     * [63]machine learning algorithms
     * [64]machine learning demos
     * [65]machine learning software
     * [66]machine learning operations (mlops)
     * [67]machine learning research groups & labs
     * [68]machine learning workflows
     * [69]machine learning
     * [70]id115
     * [71]multilayer id88
     *
     * [72]natural language processing (nlp)
     * [73]nd4j
     * [74]neural network tuning
     * [75]neural networks
     * [76]open datasets
     * [77]python ai
     * [78]questions when applying deep learning
     * [79]id80s
     * [80]id79
     * [81]recurrent network (id56)
     * [82]recursive neural tensor network
     * [83]restricted id82 (rbm)
     * [84]robotic process automation (rpa) & ai
     * [85]scala ai
     * [86]single-layer network
     * [87]skynet, or how to regulate ai
     * [88]spiking neural networks
     * [89]stacked denoising autoencoder (sda)
     * [90]strong ai vs. weak ai
     * [91]supervised learning
     * [92]symbolic reasoning
     * [93]text analysis
     * [94]thought vectors
     * [95]unsupervised learning
     * [96]deep learning use cases
     * [97]variational autoencoder (vae)
     * [98]id97, doc2vec and neural id27s

   [hr_white.png]

a beginner's guide to deep id23

   when it is not in our power to determine what is true, we ought to act
   in accordance with what is most probable. - descartes

   contents
     * [99]id23 definitions
     * [100]domain selection for id23
     * [101]state-action pairs & complex id203 distributions of
       reward
     * [102]machine learning   s relationship with time
     * [103]neural networks and deep id23
     * [104]just show me the code
     * [105]footnotes
     * [106]further reading

   while neural networks are responsible for recent breakthroughs in
   problems like id161, machine translation and time series
   prediction     they can also combine with id23
   algorithms to create something astounding like [107]alphago.

   id23 refers to goal-oriented algorithms, which learn
   how to attain a complex objective (goal) or maximize along a particular
   dimension over many steps; for example, maximize the points won in a
   game over many moves. they can start from a blank slate, and under the
   right conditions they achieve superhuman performance. like a child
   incentivized by spankings and candy, these algorithms are penalized
   when they make the wrong decisions and rewarded when they make the
   right ones     this is reinforcement.

   reinforcement algorithms that incorporate deep learning can beat world
   champions at the [108]game of go as well as human experts playing
   numerous [109]atari video games. while that may sound trivial, it   s a
   vast improvement over their previous accomplishments, and the state of
   the art is progressing rapidly.

   id23 solves the difficult problem of correlating
   immediate actions with the delayed returns they produce. like humans,
   id23 algorithms sometimes have to wait a while to see
   the fruit of their decisions. they operate in a delayed return
   environment, where it can be difficult to understand which action leads
   to which outcome over many time steps.

   id23 algorithms can be expected to perform better and
   better in more ambiguous, real-life environments while choosing from an
   arbitrary number of possible actions, rather than from the limited
   options of a video game. that is, with time we expect them to be
   valuable to achieve goals in the real world.

   two id23 algorithms - deep-id24 and a3c - have
   been implemented in a deeplearning4j library called [110]rl4j. it can
   [111]already play doom.

   [112]learn to build ai apps now   

id23 definitions

   id23 can be understood using the concepts of agents,
   environments, states, actions and rewards, all of which we   ll explain
   below. capital letters tend to denote sets of things, and lower-case
   letters denote a specific instance of that thing; e.g. a is all
   possible actions, while a is a specific action contained in the set.
     * agent: an agent takes actions; for example, a drone making a
       delivery, or super mario navigating a video game. the algorithm is
       the agent. in life, the agent is you.^[113]1
     * action (a): a is the set of all possible moves the agent can make.
       an action is almost self-explanatory, but it should be noted that
       agents choose among a list of possible actions. in video games, the
       list might include running right or left, jumping high or low,
       crouching or standing still. in the stock markets, the list might
       include buying, selling or holding any one of an array of
       securities and their derivatives. when handling aerial drones,
       alternatives would include many different velocities and
       accelerations in 3d space.
     * discount factor: the discount factor is multiplied by future
       rewards as discovered by the agent in order to dampen thse rewards   
       effect on the agent   s choice of action. why? it is designed to make
       future rewards worth less than immediate rewards; i.e. it enforces
       a kind of short-term hedonism in the agent. often expressed with
       the lower-case greek letter gamma:   . if    is .8, and there   s a
       reward of 10 points after 3 time steps, the present value of that
       reward is 0.8   x 10. a discount factor of 1 would make future
       rewards worth just as much as immediate rewards. we   re fighting
       against [114]delayed gratification here.
     * environment: the world through which the agent moves. the
       environment takes the agent   s current state and action as input,
       and returns as output the agent   s reward and its next state. if you
       are the agent, the environment could be the laws of physics and the
       rules of society that process your actions and determine the
       consequences of them.
     * state (s): a state is a concrete and immediate situation in which
       the agent finds itself; i.e. a specific place and moment, an
       instantaneous configuration that puts the agent in relation to
       other significant things such as tools, obstacles, enemies or
       prizes. it can the current situation returned by the environment,
       or any future situation. were you ever in the wrong place at the
       wrong time? that   s a state.
     * reward (r): a reward is the feedback by which we measure the
       success or failure of an agent   s actions. for example, in a video
       game, when mario touches a coin, he wins points. from any given
       state, an agent sends output in the form of actions to the
       environment, and the environment returns the agent   s new state
       (which resulted from acting on the previous state) as well as
       rewards, if there are any. rewards can be immediate or delayed.
       they effectively evaluate the agent   s action.
     * policy (  ): the policy is the strategy that the agent employs to
       determine the next action based on the current state. it maps
       states to actions, the actions that promise the highest reward.
     * value (v): the expected long-term return with discount, as opposed
       to the short-term reward r. v  (s) is defined as the expected
       long-term return of the current state under policy   . we discount
       rewards, or lower their estimated value, the further into the
       future they occur. see discount factor. and remember keynes:    in
       the long run, we are all dead.    that   s why you discount future
       rewards.
     * q-value or action-value (q): q-value is similar to value, except
       that it takes an extra parameter, the current action a. q  (s, a)
       refers to the long-term return of the current state s, taking
       action a under policy   . q maps state-action pairs to rewards. note
       the difference between q and policy.
     * trajectory: a sequence of states and actions that influence those
       states. from the latin    to throw across.    the life of an agent is
       but a ball tossed high and arching through space-time.

   so environments are functions that transform an action taken in the
   current state into the next state and a reward; agents are functions
   that transform the new state and reward into the next action. we can
   know the agent   s function, but we cannot know the function of the
   environment. it is a black box where we only see the inputs and
   outputs. it   s like most people   s relationship with technology: we know
   what it does, but we don   t know how it works. id23
   represents an agent   s attempt to approximate the environment   s
   function, such that we can send actions into the black-box environment
   that maximize the rewards it spits out.

   simple rl schema *credit: [115]sutton & barto

   in the feedback loop above, the subscripts denote the time steps t and
   t+1, each of which refer to different states: the state at moment t,
   and the state at moment t+1. unlike other forms of machine learning    
   such as supervised and unsupervised learning     id23
   can only be thought about sequentially in terms of state-action pairs
   that occur one after the other.

   id23 judges actions by the results they produce. it
   is goal oriented, and its aim is to learn sequences of actions that
   will lead an agent to achieve its goal, or maximize its objective
   function. here are some examples:
     * in video games, the goal is to finish the game with the most
       points, so each additional point obtained throughout the game will
       affect the agent   s subsequent behavior; i.e. the agent may learn
       that it should shoot battleships, touch coins or dodge meteors to
       maximize its score.
     * in the real world, the goal might be for a robot to travel from
       point a to point b, and every inch the robot is able to move closer
       to point b could be counted like points.

   here   s an example of an objective function for id23;
   i.e. the way it defines its goal.

   id23 objective

   we are summing reward function r over t, which stands for time steps.
   so this objective function calculates all the reward we could obtain by
   running through, say, a game. here, x is the state at a given time
   step, and a is the action taken in that state. r is the reward function
   for x and a. (we   ll ignore    for now.)

   id23 differs from both supervised and unsupervised
   learning by how it interprets inputs. we can illustrate their
   difference by describing what they learn about a    thing.   
     * unsupervised learning: that thing is like this other thing. (the
       algorithms learn similarities w/o names, and by extension they can
       spot the inverse and perform anomaly detection by recognizing what
       is unusual or dissimilar)
     * supervised learning: that thing is a    double bacon cheese burger   .
       (labels, putting names to faces   ) these algorithms learn the
       correlations between data instances and their labels; that is, they
       require a labelled dataset. those labels are used to    supervise   
       and correct the algorithm as it makes wrong guesses when predicting
       labels.
     * id23: eat that thing because it tastes good and
       will keep you alive longer. (actions based on short- and long-term
       rewards, such as the amount of calories you ingest, or the length
       of time you survive.) id23 can be thought of as
       supervised learning in an environment of sparse feedback.

   iframe: [116]https://www.youtube.com/embed/-uxvu0l8guo

domain selection for id23

   one way to imagine an autonomous id23 agent would be
   as a blind person attempting to navigate the world with only their ears
   and a white cane. agents have small windows that allow them to perceive
   their environment, and those windows may not even be the most
   appropriate way for them to perceive what   s around them.

   (in fact, deciding which types of input and feedback your agent should
   pay attention to is a hard problem to solve. this is known as domain
   selection. algorithms that are learning how to play video games can
   mostly ignore this problem, since the environment is man-made and
   strictly limited. thus, video games provide the sterile environment of
   the lab, where ideas about id23 can be tested. domain
   selection requires human decisions, usually based on knowledge or
   theories about the problem to be solved; e.g. selecting the domain of
   input for an algorithm in a self-driving car might include choosing to
   include radar sensors in addition to cameras and gps data.)

   rat wired

state-action pairs & complex id203 distributions of reward

   the goal of id23 is to pick the best known action for
   any given state, which means the actions have to be ranked, and
   assigned values relative to one another. since those actions are
   state-dependent, what we are really gauging is the value of
   state-action pairs; i.e. an action taken from a certain state,
   something you did somewhere. here are a few examples to demonstrate
   that the value and meaning of an action is contingent upon the state in
   which it is taken:
     * if the action is marrying someone, then marrying a 35-year-old when
       you   re 18 probably means something different than marrying a
       35-year-old when you   re 90, and those two outcomes probably have
       different motivations and lead to different outcomes.
     * if the action is yelling    fire!   , then performing the action a
       crowded theater should mean something different from performing the
       action next to a squad of men with rifles. we can   t predict an
       action   s outcome without knowing the context.

   we map state-action pairs to the values we expect them to produce with
   the q function, described above. the q function takes as its input an
   agent   s state and action, and maps them to probable rewards.

   id23 is the process of running the agent through
   sequences of state-action pairs, observing the rewards that result, and
   adapting the predictions of the q function to those rewards until it
   accurately predicts the best path for the agent to take. that
   prediction is known as a policy.

   id23 is an attempt to model a complex id203
   distribution of rewards in relation to a very large number of
   state-action pairs. this is one reason id23 is paired
   with, say, a [117]markov decision process, a method to sample from a
   complex distribution to infer its properties. it closely resembles the
   problem that inspired [118]stan ulam to invent the monte carlo method;
   namely, trying to infer the chances that a given hand of solitaire will
   turn out successful.

   any statistical approach is essentially a confession of ignorance. the
   immense complexity of some phenomena (biological, political,
   sociological, or related to board games) make it impossible to reason
   from first principles. the only way to study them is through
   statistics, measuring superficial events and attempting to establish
   correlations between them, even when we do not understand the mechanism
   by which they relate. id23, like deep neural
   networks, is one such strategy, relying on sampling to extract
   information from data.

   after a little time spent employing something like a markov decision
   process to approximate the id203 distribution of reward over
   state-action pairs, a id23 algorithm may tend to
   repeat actions that lead to reward and cease to test alternatives.
   there is a tension between the exploitation of known rewards, and
   continued exploration to discover new actions that also lead to
   victory. just as oil companies have the dual function of pumping crude
   out of known oil fields while drilling for new reserves, so too,
   id23 algorithms can be made to both exploit and
   explore to varying degrees, in order to ensure that they don   t pass
   over rewarding actions at the expense of known winners.

   id23 is iterative. in its most interesting
   applications, it doesn   t begin by knowing which rewards state-action
   pairs will produce. it learns those relations by running through states
   again and again, like athletes or musicians iterate through states in
   an attempt to improve their performance.

the relationship between machine learning with time

   you could say that an algorithm is a method to more quickly aggregate
   the lessons of time. id23 algorithms have a different
   relationship to time than humans do. an algorithm can run through the
   same states over and over again while experimenting with different
   actions, until it can infer which actions are best from which states.
   effectively, algorithms enjoy their very own [119]groundhog day, where
   they start out as dumb jerks and slowly get wise.

   since humans never experience groundhog day outside the movie,
   id23 algorithms have the potential to learn more, and
   better, than humans. indeed, the true advantage of these algorithms
   over humans stems not so much from their inherent nature, but from
   their ability to live in parallel on many chips at once, to train night
   and day without fatigue, and therefore to learn more. an algorithm
   trained on the game of go, such as alphago, will have played many more
   games of go than any human could hope to complete in 100
   lifetimes.^[120]2

neural networks and deep id23

   where do neural networks fit in? neural networks are the agent that
   learns to map state-action pairs to rewards. like all neural networks,
   they use coefficients to approximate the function relating inputs to
   outputs, and their learning consists to finding the right coefficients,
   or weights, by iteratively adjusting those weights along gradients that
   promise less error.

   in id23, convolutional networks can be used to
   recognize an agent   s state; e.g. the screen that mario is on, or the
   terrain before a drone. that is, they perform their typical task of
   image recognition.

   but convolutional networks derive different interpretations from images
   in id23 than in supervised learning. in supervised
   learning, the network applies a label to an image; that is, it matches
   names to pixels.

   convolutional classifier

   in fact, it will rank the labels that best fit the image in terms of
   their probabilities. shown an image of a donkey, it might decide the
   picture is 80% likely to be a donkey, 50% likely to be a horse, and 30%
   likely to be a dog.

   in id23, given an image that represents a state, a
   convolutional net can rank the actions possible to perform in that
   state; for example, it might predict that running right will return 5
   points, jumping 7, and running left none.

   convolutional agent

   the above image illustrates what a policy agent does, mapping a state
   to the best action.

   policy state action

   a policy maps a state to an action.

   if you recall, this is distinct from q, which maps state action pairs
   to rewards.

   to be more specific, q maps state-action pairs to the highest
   combination of immediate reward with all future rewards that might be
   harvested by later actions in the trajectory. here is the equation for
   q, from wikipedia:

   id24 equation

   having assigned values to the expected rewards, the q function simply
   selects the state-action pair with the highest so-called q value.

   at the beginning of id23, the neural network
   coefficients may be initialized stochastically, or randomly. using
   feedback from the environment, the neural net can use the difference
   between its expected reward and the ground-truth reward to adjust its
   weights and improve its interpretation of state-action pairs.

   this feedback loop is analogous to the id26 of error in
   supervised learning. however, supervised learning begins with knowledge
   of the ground-truth labels the neural network is trying to predict. its
   goal is to create a model that maps different images to their
   respective names.

   id23 relies on the environment to send it a scalar
   number in response to each new action. the rewards returned by the
   environment can be varied, delayed or affected by unknown variables,
   introducing noise to the feedback loop.

   this leads us to a more complete expression of the q function, which
   takes into account not only the immediate rewards produced by an
   action, but also the delayed rewards that may be returned several time
   steps deeper in the sequence.

   like human beings, the q function is recursive. just as calling the
   wetware method human() contains within it another method human(), of
   which we are all the fruit, calling the q function on a given
   state-action pair requires us to call a nested q function to predict
   the value of the next state, which in turn depends on the q function of
   the state after that, and so forth.

footnotes

   1) it might be helpful to imagine a id23 algorithm in
   action, to paint it visually. let   s say the algorithm is learning to
   play the video game super mario. it   s trying to get mario through the
   game and acquire the most points. to do that, we can spin up lots of
   different marios in parallel and run them through the space of all
   possible game states. it   s as though you have 1,000 marios all
   tunnelling through a mountain, and as they dig (e.g. as they decide
   again and again which action to take to affect the game environment),
   their experience-tunnels branch like the intricate and fractal twigs of
   a tree. the marios    experience-tunnels are corridors of light cutting
   through the mountain. and as in life itself, one successful action may
   make it more likely that successful action is possible in a larger
   decision flow, propelling the winning marios onward. you might also
   imagine, if each mario is an agent, that in front of him is a heat map
   tracking the rewards he can associate with state-action pairs. (imagine
   each state-action pair as have its own screen overlayed with heat from
   yellow to red. the many screens are assembled in a grid, like you might
   see in front of a wall st. trader with many monitors. one action screen
   might be    jump harder from this state   , another might be    run faster in
   this state    and so on and so forth.) since some state-action pairs lead
   to significantly more reward than others, and different kinds of
   actions such as jumping, squatting or running can be taken, the
   id203 distribution of reward over actions is not a bell curve but
   instead complex, which is why [121]markov and monte carlo techniques
   are used to explore it, much as stan ulam explored winning solitaire
   hands. that is, while it is difficult to describe the reward
   distribution in a formula, it can be sampled. because the algorithm
   starts ignorant and many of the paths through the game-state space are
   unexplored, the heat maps will reflect their lack of experience; i.e.
   there could be blanks in the heatmap of the rewards they imagine, or
   they might just start with some default assumptions about rewards that
   will be adjusted with experience. the marios are essentially
   reward-seeking missiles guided by those heatmaps, and the more times
   they run through the game, the more accurate their heatmap of potential
   future reward becomes. the heatmaps are basically id203
   distributions of reward over the state-action pairs possible from the
   mario   s current state.

   2) the correct analogy may actually be that a learning algorithm is
   like a species. each simulation the algorithm runs as it learns could
   be considered an individual of the species. just as knowledge from the
   algorithm   s runs through the game is collected in the algorithm   s model
   of the world, the individual humans of any group will report back via
   language, allowing the collective   s model of the world, embodied in its
   texts, records and oral traditions, to become more intelligent (at
   least in the ideal case. the subversion and noise introduced into our
   collective models is a topic for another post, and probably for another
   website entirely.). this puts a finer point on why the contest between
   algorithms and individual humans, even when the humans are world
   champions, is unfair. we are pitting a civilization that has
   accumulated the wisdom of 10,000 lives against a single sack of flesh.

further reading

     * richard s. sutton and andrew g. barto   s [122]reinforcement
       learning: an introduction
     * [123]id23 in java
     * andrej karpathy   s [124]convnetjs deep id24 demo
     * [125]brown-umbc id23 and planning (burlap)(apache
       2.0 licensed as of june 2016)
     * [126]glossary of terms in id23
     * [127]id23 and id25, learning to play from pixels
     * video: [128]richard sutton on temporal difference learning
     * [129]a brief survey of deep id23
     * [130]the policy of truth, by ben recht
     * [131]machine learning for humans: id23

rl theory

lectures

     * [ucl] [132]compm050/compgi13 id23 by david silver
     * [uc berkeley] cs188 artificial intelligence by pieter abbeel
          + [133]lecture 8: id100 1
          + [134]lecture 9: id100 2
          + [135]lecture 10: id23 1
          + [136]lecture 11: id23 2
     * [udacity (georgia tech.)] [137]cs7642 id23
     * [stanford] [138]cs229 machine learning - lecture 16: reinforcement
       learning by andrew ng
     * [uc berkeley] [139]deep rl bootcamp
     * [uc berkeley] [140]cs294 deep id23 by john
       schulman and pieter abbeel
     * [cmu] [141]10703: deep id23 and control, spring
       2017
     * [mit] [142]6.s094: deep learning for self-driving cars
          + [143]lecture 2: deep id23 for motion
            planning

id23 books

     * richard sutton and andrew barto, id23: an
       introduction (1st edition, 1998) [144][book] [145][code]
     * richard sutton and andrew barto, id23: an
       introduction (2nd edition, in progress, 2018) [146][book]
       [147][code]
     * csaba szepesvari, algorithms for id23 [148][book]
     * david poole and alan mackworth, artificial intelligence:
       foundations of computational agents [149][book chapter]
     * dimitri p. bertsekas and john n. tsitsiklis, neuro-dynamic
       programming [150][book (amazon)] [151][summary]
     * mykel j. kochenderfer, decision making under uncertainty: theory
       and application [152][book (amazon)]

survey papers

     * leslie pack kaelbling, michael l. littman, andrew w. moore,
       id23: a survey, jair, 1996. [153][paper]
     * s. s. keerthi and b. ravindran, a tutorial survey of reinforcement
       learning, sadhana, 1994. [154][paper]
     * matthew e. taylor, peter stone, id21 for reinforcement
       learning domains: a survey, jmlr, 2009. [155][paper]
     * jens kober, j. andrew bagnell, jan peters, id23
       in robotics, a survey, ijrr, 2013. [156][paper]
     * michael l. littman,    id23 improves behaviour from
       evaluative feedback.    nature 521.7553 (2015): 445-451. [157][paper]
     * marc p. deisenroth, gerhard neumann, jan peter, a survey on policy
       search for robotics, foundations and trends in robotics, 2014.
       [158][book]

id23 papers / thesis

   foundational papers
     * marvin minsky, steps toward artificial intelligence, proceedings of
       the ire, 1961. [159][paper] (discusses issues in rl such as the
          credit assignment problem   )
     * ian h. witten, an adaptive optimal controller for discrete-time
       markov environments, information and control, 1977. [160][paper]
       (earliest publication on temporal-difference (td) learning rule)

   id23 methods
     * id145 (dp):
          + christopher j. c. h. watkins, learning from delayed rewards,
            ph.d. thesis, cambridge university, 1989. [161][thesis]
     * monte carlo:
          + andrew barto, michael duff, monte carlo inversion and
            id23, nips, 1994. [162][paper]
          + satinder p. singh, richard s. sutton, id23
            with replacing eligibility traces, machine learning, 1996.
            [163][paper]
     * temporal-difference:
          + richard s. sutton, learning to predict by the methods of
            temporal differences. machine learning 3: 9-44, 1988.
            [164][paper]
     * id24 (off-policy td algorithm):
          + chris watkins, learning from delayed rewards, cambridge, 1989.
            [165][thesis]
     * sarsa (on-policy td algorithm):
          + g.a. rummery, m. niranjan, on-line id24 using
            connectionist systems, technical report, cambridge univ.,
            1994. [166][report]
          + richard s. sutton, generalization in id23:
            successful examples using sparse coding, nips, 1996.
            [167][paper]
     * r-learning (learning of relative values)
          + andrew schwartz, a id23 method for
            maximizing undiscounted rewards, icml, 1993.
            [168][paper-google scholar]
     * function approximation methods (least-square temporal difference,
       least-square policy iteration)
          + steven j. bradtke, andrew g. barto, linear least-squares
            algorithms for temporal difference learning, machine learning,
            1996. [169][paper]
          + michail g. lagoudakis, ronald parr, model-free least squares
            policy iteration, nips, 2001. [170][paper] [171][code]
     * policy search / policy gradient
          + richard sutton, david mcallester, satinder singh, yishay
            mansour, id189 for id23
            with function approximation, nips, 1999. [172][paper]
          + jan peters, sethu vijayakumar, stefan schaal, natural
            actor-critic, ecml, 2005. [173][paper]
          + jens kober, jan peters, policy search for motor primitives in
            robotics, nips, 2009. [174][paper]
          + jan peters, katharina mulling, yasemin altun, relative id178
            policy search, aaai, 2010. [175][paper]
          + freek stulp, olivier sigaud, path integral policy improvement
            with covariance matrix adaptation, icml, 2012. [176][paper]
          + nate kohl, peter stone, policy gradient id23
            for fast quadrupedal locomotion, icra, 2004. [177][paper]
          + marc deisenroth, carl rasmussen, pilco: a model-based and
            data-efficient approach to policy search, icml, 2011.
            [178][paper]
          + scott kuindersma, roderic grupen, andrew barto, learning
            dynamic arm motions for postural recovery, humanoids, 2011.
            [179][paper]
          + konstantinos chatzilygeroudis, roberto rama, rituraj kaushik,
            dorian goepp, vassilis vassiliades, jean-baptiste mouret,
            black-box data-efficient policy search for robotics, iros,
            2017. [[180]paper]
     * hierarchical rl
          + richard sutton, doina precup, satinder singh, between mdps and
            semi-mdps: a framework for temporal abstraction in
            id23, artificial intelligence, 1999.
            [181][paper]
          + george konidaris, andrew barto, building portable options:
            skill transfer in id23, ijcai, 2007.
            [182][paper]
     * deep learning + id23 (a sample of recent works on
       dl+rl)
          + v. mnih, et. al., human-level control through deep
            id23, nature, 2015. [183][paper]
          + xiaoxiao guo, satinder singh, honglak lee, richard lewis,
            xiaoshi wang, deep learning for real-time atari game play
            using offline monte-carlo tree search planning, nips, 2014.
            [184][paper]
          + sergey levine, chelsea finn, trevor darrel, pieter abbeel,
            end-to-end training of deep visuomotor policies. arxiv, 16 oct
            2015. [185][arxiv]
          + tom schaul, john quan, ioannis antonoglou, david silver,
            prioritized experience replay, arxiv, 18 nov 2015.
            [186][arxiv]
          + hado van hasselt, arthur guez, david silver, deep
            id23 with double id24, arxiv, 22 sep
            2015. [187][arxiv]
          + volodymyr mnih, adri   puigdom  nech badia, mehdi mirza, alex
            graves, timothy p. lillicrap, tim harley, david silver, koray
            kavukcuoglu, asynchronous methods for deep reinforcement
            learning, arxiv, 4 feb 2016. [188][arxiv]
          + simon schmitt, jonathan j. hudson, augustin zidek, simon
            osindero, carl doersch, wojciech m. czarnecki, joel z. leibo,
            heinrich kuttler, andrew zisserman, karen simonyan, s. m. ali
            eslami, kickstarting deep id23, arxiv, 10
            mar 2018, [189]paper

id23 applications

game playing with id23

   traditional games
     * backgammon -    td-gammon    game play using td(  ) (tesauro, acm 1995)
       [190][paper]
     * chess -    knightcap    program using td(  ) (baxter, arxiv 1999)
       [191][arxiv]
     * chess - giraffe: using deep id23 to play chess
       (lai, arxiv 2015) [192][arxiv]

   computer games
     * human-level control through deep id23 (mnih,
       nature 2015) [193][paper] [194][code] [195][video]
     * [196]flappy bird id23 [197][video]
     * mari/o - learning to play mario with evolutionary reinforcement
       learning using id158s (stanley, evolutionary
       computation 2002) [198][paper] [199][video]
     * [200]montezuma   s revenge: id23 with
       prediction-based rewards from openai

robotics with id23

     * policy gradient id23 for fast quadrupedal
       locomotion (kohl, icra 2004) [201][paper]
     * robot motor skill coordination with em-based id23
       (kormushev, iros 2010) [202][paper] [203][video]
     * generalized model learning for id23 on a humanoid
       robot (hester, icra 2010) [204][paper] [205][video]
     * autonomous skill acquisition on a mobile manipulator (konidaris,
       aaai 2011) [206][paper] [207][video]
     * pilco: a model-based and data-efficient approach to policy search
       (deisenroth, icml 2011) [208][paper]
     * incremental semantically grounded learning from demonstration
       (niekum, rss 2013) [209][paper]
     * efficient id23 for robots using informative
       simulated priors (cutler, icra 2015) [210][paper] [211][video]
     * robots that can adapt like animals (cully, nature 2015)
       [[212]paper] [[213]video] [[214]code]
     * black-box data-efficient policy search for robotics
       (chatzilygeroudis, iros 2017) [[215]paper] [[216]video] [[217]code]

control with id23

     * an application of id23 to aerobatic helicopter
       flight (abbeel, nips 2006) [218][paper] [219][video]
     * autonomous helicopter control using id23 policy
       search methods (bagnell, icra 2001) [220][paper]

operations research & id23

     * scaling average-reward id23 for product delivery
       (proper, aaai 2004) [221][paper]
     * cross channel optimized marketing by id23 (abe,
       kdd 2004) [222][paper]

human computer interaction

     * optimizing dialogue management with id23:
       experiments with the njfun system (singh, jair 2002) [223][paper]

id23 tutorials / websites

     * mance harmon and stephanie harmon, [224]id23: a
       tutorial
     * c. igel, m.a. riedmiller, et al., id23 in a
       nutshell, esann, 2007. [225][paper]
     * unsw - [226]id23
     * [227]introduction
     * [228]td-learning
     * [229]id24 and sarsa
     * [230]applet for    cat and mouse    game
     * [231]ros id23 tutorial
     * [232]pomdp for dummies
     * scholarpedia articles on:
     * [233]id23
     * [234]temporal difference learning
     * repository with useful [235]matlab software, presentations, and
       demo videos
     * [236]bibliography on id23
     * uc berkeley - cs 294: deep id23, fall 2015 (john
       schulman, pieter abbeel) [237][class website]
     * [238]blog posts on id23, parts 1-4 by travis
       dewolf
     * [239]the arcade learning environment - atari 2600 games environment
       for developing ai agents
     * [240]deep id23: pong from pixels by andrej
       karpathy
     * [241]demystifying deep id23
     * [242]let   s make a id25
     * [243]simple id23 with tensorflow, parts 0-8 by
       arthur juliani
     * [244]practical_rl - github-based course in id23
       in the wild (lectures, coding labs, projects)

online demos

     * [245]real-world demonstrations of id23
     * [246]deep id24 demo - a deep id24 demonstration using
       convnetjs
     * [247]deep id24 with tensor flow - a deep id24
       demonstration using google tensorflow
     * [248]id23 demo - a id23 demo
       using reinforcejs by andrej karpathy

   [hr_gradient.png]

interactive demo

   learn to build ai applications using our interactive learning portal.
   [249]try it now

   [logo_footer.svg]

company

     * [250]about
     * [251]press kit
     * [252]contact us
     * [253]press
     * [254]privacy

platform

     * [255]skil
     * [256]subscriptions
     * [257]documentation
     * [258]community support

international

     * [259]english
     * [260]japanese

follow us

     * [261]facebook
     * [262]twitter
     * [263]linkedin
     * [264]gitter

subscribe to our mailing list

   iframe: [265]https://go.pardot.com/l/456082/2017-11-01/d9xsrj

references

   visible links
   1. https://www.googletagmanager.com/ns.html?id=gtm-p824sk
   2. https://skymind.ai/contact
   3. https://skymind.ai/
   4. https://skymind.ai/platform
   5. https://skymind.ai/learn
   6. https://skymind.ai/subscription
   7. https://docs.skymind.ai/docs
   8. https://github.com/skymindio/skil-ce/issues
   9. https://skymind.ai/audience/datascientist
  10. https://skymind.ai/audience/architect
  11. https://skymind.ai/audience/devops
  12. https://skymind.ai/audience/executives
  13. https://skymind.ai/solutions
  14. https://solutions.skymind.ai/l/456082/2019-03-08/lxfw7m
  15. https://skymind.ai/case-studies/orange
  16. https://skymind.ai/case-studies/insurance
  17. https://skymind.ai/case-studies/logistics
  18. https://skymind.ai/case-studies/canonical
  19. https://skymind.ai/case-studies/finance
  20. https://skymind.ai/case-studies/image
  21. https://skymind.ai/case-studies/commerce
  22. https://skymind.ai/about
  23. https://blog.skymind.ai/
  24. https://skymind.ai/wiki
  25. https://skymind.ai/open-source
  26. https://go.pardot.com/l/456082/2018-09-28/jsv68v
  27. https://skymind.ai/wiki/
  28. https://skymind.ai/wiki/ai-vs-machine-learning-vs-deep-learning
  29. https://skymind.ai/wiki/apache-spark-deep-learning
  30. https://skymind.ai/wiki/attention-mechanism-memory-network
  31. https://skymind.ai/wiki/automl-automated-machine-learning-ai
  32. https://skymind.ai/wiki/autonomous-vehicle
  33. https://skymind.ai/wiki/id26
  34. https://skymind.ai/wiki/bagofwords-tf-idf
  35. https://skymind.ai/wiki/clojure-ai
  36. https://skymind.ai/wiki/comparison-frameworks-dl4j-tensorflow-pytorch
  37. https://skymind.ai/wiki/convolutional-network
  38. https://skymind.ai/wiki/data-for-deep-learning
  39. https://skymind.ai/wiki/datasets-ml
  40. https://skymind.ai/wiki/decision-tree
  41. https://skymind.ai/wiki/deep-autoencoder
  42. https://skymind.ai/wiki/deep-belief-network
  43. https://skymind.ai/wiki/deep-reinforcement-learning
  44. https://skymind.ai/wiki/deeplearning-research-papers
  45. https://skymind.ai/wiki/deeplearning4j
  46. https://skymind.ai/wiki/denoising-autoencoder
  47. https://skymind.ai/wiki/devops-machine-learning
  48. https://skymind.ai/wiki/differentiableprogramming
  49. https://skymind.ai/wiki/eigenvector
  50. https://skymind.ai/wiki/evolutionary-genetic-algorithm
  51. https://skymind.ai/wiki/fraud-detection
  52. https://skymind.ai/wiki/generative-adversarial-network-gan
  53. https://skymind.ai/wiki/glossary
  54. https://skymind.ai/wiki/gluon
  55. https://skymind.ai/wiki/graph-analysis
  56. https://skymind.ai/wiki/hopfieldnetworks
  57. https://skymind.ai/wiki/hyperparameter
  58. https://skymind.ai/wiki/index
  59. https://skymind.ai/wiki/java-ai
  60. https://skymind.ai/wiki/jumpy
  61. https://skymind.ai/wiki/logistic-regression
  62. https://skymind.ai/wiki/lstm
  63. https://skymind.ai/wiki/machine-learning-algorithms
  64. https://skymind.ai/wiki/machine-learning-demos
  65. https://skymind.ai/wiki/machine-learning-library-software
  66. https://skymind.ai/wiki/machine-learning-operations-mlops
  67. https://skymind.ai/wiki/machine-learning-research-groups-labs
  68. https://skymind.ai/wiki/machine-learning-workflow
  69. https://skymind.ai/wiki/machine-learning
  70. https://skymind.ai/wiki/markov-chain-monte-carlo
  71. https://skymind.ai/wiki/multilayer-id88
  72. https://skymind.ai/wiki/natural-language-processing-nlp
  73. https://skymind.ai/wiki/nd4j
  74. https://skymind.ai/wiki/neural-network-tuning
  75. https://skymind.ai/wiki/neural-network
  76. https://skymind.ai/wiki/open-datasets
  77. https://skymind.ai/wiki/python-ai
  78. https://skymind.ai/wiki/questions-when-applying-deep-learning
  79. https://skymind.ai/wiki/radial-basis-function-network-rbf
  80. https://skymind.ai/wiki/random-forest
  81. https://skymind.ai/wiki/recurrent-network-id56
  82. https://skymind.ai/wiki/recursive-neural-tensor-network
  83. https://skymind.ai/wiki/restricted-boltzmann-machine
  84. https://skymind.ai/wiki/robotic-process-automation-rpa
  85. https://skymind.ai/wiki/scala-ai
  86. https://skymind.ai/wiki/single-layer-network
  87. https://skymind.ai/wiki/skynet
  88. https://skymind.ai/wiki/spiking-neural-network-snn
  89. https://skymind.ai/wiki/stacked-denoising-autoencoder
  90. https://skymind.ai/wiki/strong-ai-general-ai
  91. https://skymind.ai/wiki/supervised-learning
  92. https://skymind.ai/wiki/symbolic-reasoning
  93. https://skymind.ai/wiki/text-analysis
  94. https://skymind.ai/wiki/thought-vectors
  95. https://skymind.ai/wiki/unsupervised-learning
  96. https://skymind.ai/wiki/use-cases
  97. https://skymind.ai/wiki/variational-autoencoder
  98. https://skymind.ai/wiki/id97
  99. https://skymind.ai/wiki/deep-reinforcement-learning#define
 100. https://skymind.ai/wiki/deep-reinforcement-learning#domain
 101. https://skymind.ai/wiki/deep-reinforcement-learning#reward
 102. https://skymind.ai/wiki/deep-reinforcement-learning#time
 103. https://skymind.ai/wiki/deep-reinforcement-learning#neural
 104. https://skymind.ai/wiki/deep-reinforcement-learning#code
 105. https://skymind.ai/wiki/deep-reinforcement-learning#footnote
 106. https://skymind.ai/wiki/deep-reinforcement-learning#reading
 107. https://deepmind.com/blog/alphago-zero-learning-scratch/
 108. https://deeplearning4j.org/deep-learning-and-the-game-of-go
 109. https://www.cs.toronto.edu/~vmnih/docs/id25.pdf
 110. https://github.com/deeplearning4j/rl4j
 111. https://www.youtube.com/watch?v=pgktl6pwa-o
 112. https://skymind.ai/learn
 113. https://skymind.ai/wiki/deep-reinforcement-learning#one
 114. https://en.wikipedia.org/wiki/stanford_marshmallow_experiment
 115. http://incompleteideas.net/book/bookdraft2017nov5.pdf
 116. https://www.youtube.com/embed/-uxvu0l8guo
 117. https://skymind.ai/wiki/markov-chain-monte-carlo
 118. http://permalink.lanl.gov/object/tr?what=info:lanl-repo/lareport/la-ur-88-9068
 119. http://www.imdb.com/title/tt0107048/
 120. https://skymind.ai/wiki/deep-reinforcement-learning#two
 121. https://skymind.ai/wiki/markov-chain-monte-carlo
 122. http://incompleteideas.net/sutton/book/the-book-2nd.html
 123. https://github.com/deeplearning4j/deeplearning4j/tree/master/rl4j
 124. https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html
 125. http://burlap.cs.brown.edu/
 126. http://www-anw.cs.umass.edu/rlr/terms.html
 127. https://rubenfiszel.github.io/posts/rl4j/2016-08-24-reinforcement-learning-and-id25.html
 128. https://www.youtube.com/watch?v=eemceqa85tw
 129. https://arxiv.org/pdf/1708.05866.pdf
 130. http://www.argmin.net/2018/02/20/reinforce/
 131. https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265
 132. http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching.html
 133. https://www.youtube.com/watch?v=i0o-ui1n35u
 134. https://www.youtube.com/watch?v=csiiv6wgzkm
 135. https://www.youtube.com/watch?v=ifma8g7lege
 136. https://www.youtube.com/watch?v=si1_ytw960c
 137. https://classroom.udacity.com/courses/ud600
 138. https://www.youtube.com/watch?v=rtxi449zjsc&feature=relmfu
 139. https://sites.google.com/view/deep-rl-bootcamp/lectures
 140. http://rll.berkeley.edu/deeprlcourse/
 141. https://katefvision.github.io/
 142. http://selfdrivingcars.mit.edu/
 143. https://www.youtube.com/watch?v=qdzm8r3wgbw&list=plraxtmerzgoeikm4sgnokngvnjby9efdf
 144. http://incompleteideas.net/book/ebook/the-book.html
 145. http://incompleteideas.net/book/code/code.html
 146. http://incompleteideas.net/book/bookdraft2018jan1.pdf
 147. https://github.com/shangtongzhang/reinforcement-learning-an-introduction
 148. http://www.ualberta.ca/~szepesva/papers/rlalgsinmdps.pdf
 149. http://artint.info/html/artint_262.html
 150. http://www.amazon.com/neuro-dynamic-programming-optimization-neural-computation/dp/1886529108/ref=sr_1_3?s=books&ie=utf8&qid=1442461075&sr=1-3&refinements=p_27:john+n.+tsitsiklis+dimitri+p.+bertsekas
 151. http://www.mit.edu/~dimitrib/ndp_encycl.pdf
 152. http://www.amazon.com/decision-making-under-uncertainty-application/dp/0262029251/ref=sr_1_1?ie=utf8&qid=1441126550&sr=8-1&keywords=kochenderfer&pebp=1441126551594&perid=1y6rg2egrd26659cjhh9
 153. https://www.jair.org/media/301/live-301-1562-jair.pdf
 154. http://www.cse.iitm.ac.in/~ravi/papers/keerthi.rl-survey.pdf
 155. http://machinelearning.wustl.edu/mlpapers/paper_files/jmlr10_taylor09a.pdf
 156. http://www.ias.tu-darmstadt.de/uploads/publications/kober_ijrr_2013.pdf
 157. http://www.nature.com/nature/journal/v521/n7553/full/nature14540.html
 158. https://spiral.imperial.ac.uk:8443/bitstream/10044/1/12051/7/fnt_corrected_2014-8-22.pdf
 159. http://staffweb.worc.ac.uk/drc/courses 2010-11/comp 3104/tutor inputs/session 9 prep/reading material/minsky60steps.pdf
 160. http://www.cs.waikato.ac.nz/~ihw/papers/77-ihw-adaptivecontroller.pdf
 161. https://www.cs.rhul.ac.uk/home/chrisw/new_thesis.pdf
 162. http://papers.nips.cc/paper/865-monte-carlo-matrix-inversion-and-reinforcement-learning.pdf
 163. http://www-all.cs.umass.edu/pubs/1995_96/singh_s_ml96.pdf
 164. http://webdocs.cs.ualberta.ca/~sutton/papers/sutton-88-with-erratum.pdf
 165. http://www.cs.rhul.ac.uk/home/chrisw/thesis.html
 166. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&ved=0cdiqfjacahukewj2lmm5wzdiahuhkg0kha6kavm&url=ftp://mi.eng.cam.ac.uk/pub/reports/auto-pdf/rummery_tr166.pdf&usg=afqjcnhz6irgcaao5lzc7t8oeiby9epozg&sig2=sa-empme1m5jav7ymaxsnq&cad=rja
 167. http://webdocs.cs.ualberta.ca/~sutton/papers/sutton-96.pdf
 168. https://scholar.google.com/scholar?q=reinforcement+learning+method+for+maximizing+undiscounted+rewards&hl=en&as_sdt=0&as_vis=1&oi=scholart&sa=x&ved=0cbsqgqmwagovchmiho6p_moqyaivwh0ech3xwawm
 169. http://www-anw.cs.umass.edu/pubs/1995_96/bradtke_b_ml96.pdf
 170. http://www.cs.duke.edu/research/ai/lspi/nips01.pdf
 171. http://www.cs.duke.edu/research/ai/lspi/
 172. http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf
 173. https://homes.cs.washington.edu/~todorov/courses/amath579/reading/naturalactorcritic.pdf
 174. http://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf
 175. http://www.kyb.tue.mpg.de/fileadmin/user_upload/files/publications/attachments/aaai-2010-peters_6439[0].pdf
 176. http://arxiv.org/pdf/1206.4621v1.pdf
 177. http://www.cs.utexas.edu/~pstone/papers/bib2html-links/icra04.pdf
 178. http://mlg.eng.cam.ac.uk/pub/pdf/deiras11.pdf
 179. http://www-all.cs.umass.edu/pubs/2011/kuindersma_g_b_11.pdf
 180. https://arxiv.org/abs/1703.07261
 181. https://webdocs.cs.ualberta.ca/~sutton/papers/sps-aij.pdf
 182. http://www-anw.cs.umass.edu/pubs/2007/konidaris_b_ijcai07.pdf
 183. http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=lo_2hfdw4muqecf3cvbzm9rgn0jajwel9jnr3zotv0p5kedccnjz3fj2fhqcgxkapor3zssjaldp-tw3iwgtsernlpac9xqq-vta2z5ji9lg16_wvcy4saogpk5xxa6ecqo8d8j7l4ejsdjwai53gqkt-7juiog0r3iv67mqiro74l6ixvmcvnkbgowimgi8u0izjstlpmqp6vmi_8lw_a==
 184. http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning.pdf
 185. http://arxiv.org/pdf/1504.00702v3.pdf
 186. http://arxiv.org/pdf/1511.05952v2.pdf
 187. http://arxiv.org/abs/1509.06461
 188. https://arxiv.org/abs/1602.01783
 189. https://arxiv.org/abs/1803.03835
 190. http://www.bkgm.com/articles/tesauro/tdl.html
 191. http://arxiv.org/pdf/cs/9901002v1.pdf
 192. http://arxiv.org/pdf/1509.01549v2.pdf
 193. http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=lo_2hfdw4muqecf3cvbzm9rgn0jajwel9jnr3zotv0p5kedccnjz3fj2fhqcgxkapor3zssjaldp-tw3iwgtsernlpac9xqq-vta2z5ji9lg16_wvcy4saogpk5xxa6ecqo8d8j7l4ejsdjwai53gqkt-7juiog0r3iv67mqiro74l6ixvmcvnkbgowimgi8u0izjstlpmqp6vmi_8lw_a==
 194. https://sites.google.com/a/deepmind.com/id25/
 195. https://www.youtube.com/watch?v=iqxkqf2bose
 196. https://github.com/sarvagyavaish/flappybirdrl
 197. https://www.youtube.com/watch?v=xm62spkazhu
 198. http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf
 199. https://www.youtube.com/watch?v=qv6uvoq0f44
 200. https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/
 201. http://www.cs.utexas.edu/~pstone/papers/bib2html-links/icra04.pdf
 202. http://kormushev.com/papers/kormushev-iros2010.pdf
 203. https://www.youtube.com/watch?v=w_gxlksssie
 204. https://ccc.inaoep.mx/~mdprl/documentos/hester_2010.pdf
 205. https://www.youtube.com/watch?v=mrpx9dfcdwi&list=pl5nbayuyjtrm48dviibyi68urttmluv7e&index=12
 206. http://lis.csail.mit.edu/pubs/konidaris-aaai11b.pdf
 207. https://www.youtube.com/watch?v=yuicaksqtzy
 208. http://mlg.eng.cam.ac.uk/pub/pdf/deiras11.pdf
 209. http://people.cs.umass.edu/~sniekum/pubs/niekumrss2013.pdf
 210. http://markjcutler.com/papers/cutler15_icra.pdf
 211. https://www.youtube.com/watch?v=kkclfx6l1hy
 212. https://arxiv.org/abs/1407.3501
 213. https://www.youtube.com/watch?v=t-c17rkh3ue
 214. https://github.com/resibots/cully_2015_nature
 215. https://arxiv.org/abs/1703.07261
 216. https://www.youtube.com/watch?v=kteyyiifgpm
 217. https://github.com/resibots/blackdrops
 218. http://heli.stanford.edu/papers/nips06-aerobatichelicopter.pdf
 219. https://www.youtube.com/watch?v=vcdxqn0fcne
 220. http://repository.cmu.edu/cgi/viewcontent.cgi?article=1082&context=robotics
 221. http://web.engr.oregonstate.edu/~proper/aaai04sproper.pdf
 222. http://www.research.ibm.com/people/n/nabe/kdd04avas.pdf
 223. http://web.eecs.umich.edu/~baveja/papers/rldsjair.pdf
 224. http://old.nbu.bg/cogs/events/2000/readings/petrov/rltutorial.pdf
 225. http://image.diku.dk/igel/paper/rlian.pdf
 226. http://www.cse.unsw.edu.au/~cs9417ml/rl1/index.html
 227. http://www.cse.unsw.edu.au/~cs9417ml/rl1/introduction.html
 228. http://www.cse.unsw.edu.au/~cs9417ml/rl1/tdlearning.html
 229. http://www.cse.unsw.edu.au/~cs9417ml/rl1/algorithms.html
 230. http://www.cse.unsw.edu.au/~cs9417ml/rl1/applet.html
 231. http://wiki.ros.org/reinforcement_learning/tutorials/id23 tutorial
 232. http://cs.brown.edu/research/ai/pomdp/tutorial/index.html
 233. http://www.scholarpedia.org/article/reinforcement_learning
 234. http://www.scholarpedia.org/article/temporal_difference_learning
 235. http://busoniu.net/repository.php
 236. http://liinwww.ira.uka.de/bibliography/neural/reinforcement.learning.html
 237. http://rll.berkeley.edu/deeprlcourse/
 238. https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-id24-and-exploration/
 239. http://www.arcadelearningenvironment.org/
 240. http://karpathy.github.io/2016/05/31/rl/
 241. https://www.nervanasys.com/demystifying-deep-reinforcement-learning/
 242. https://jaromiru.com/2016/09/27/lets-make-a-id25-theory/
 243. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0#.78km20i8r
 244. https://github.com/yandexdataschool/practical_rl
 245. http://www.dcsc.tudelft.nl/~robotics/media.html
 246. http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html
 247. https://github.com/nivwusquorum/tensorflow-deepq
 248. http://cs.stanford.edu/people/karpathy/reinforcejs/
 249. https://skymind.ai/learn
 250. https://skymind.ai/about
 251. https://drive.google.com/drive/folders/1s9n-mdi17euhjmdkp-x0t05e1lgy1ly3
 252. https://skymind.ai/contact
 253. https://skymind.ai/press
 254. https://skymind.ai/privacy
 255. https://skymind.ai/platform
 256. https://skymind.ai/subscription
 257. https://docs.skymind.ai/docs
 258. https://github.com/skymindio/skil-ce/issues
 259. https://skymind.ai/
 260. https://skymind.ai/jp
 261. https://www.facebook.com/deeplearning4j/
 262. https://twitter.com/deeplearning4j
 263. https://www.linkedin.com/company/skymind-io
 264. https://gitter.im/deeplearning4j/deeplearning4j
 265. https://go.pardot.com/l/456082/2017-11-01/d9xsrj

   hidden links:
 267. https://skymind.ai/wiki/accuracy-precision-recall-f1
 268. https://skymind.ai/wiki/ai-infrastructure-machine-learning-operations-mlops
 269. https://skymind.ai/wiki/ai-winter
 270. https://skymind.ai/wiki/arbiter
 271. https://skymind.ai/wiki/datavec
 272. https://skymind.ai/wiki/define-artificial-intelligence-ai
 273. https://skymind.ai/wiki/naive-bayes-theorem
