   #[1]scholarpedia (en) [2]scholarpedia atom feed

id23

   from scholarpedia
   florentin woergoetter and bernd porr (2008), scholarpedia, 3(3):1448.
   [3]doi:10.4249/scholarpedia.1448 revision #127590 [[4]link to/cite this
   article]
   jump to: [5]navigation, [6]search
   post-publication activity
   (button)

   curator: [7]bernd porr
   contributors:


   0.40 -

   [8]eugene m. izhikevich
   0.20 -

   [9]florentin woergoetter
   0.20 -

   [10]jaldert o rombouts

   [11]neil girdhar

   [12]benjamin bronner
     * [13]florentin woergoetter, bccn, university of goettingen, germany
     * [14]dr. bernd porr, university of glasgow

   [15]id23 (rl) is learning by interacting with an
   environment. an rl agent learns from the consequences of its actions,
   rather than from being explicitly taught and it selects its actions on
   basis of its past experiences (exploitation) and also by new choices
   (exploration), which is essentially trial and error learning. the
   [16]reinforcement signal that the rl-agent receives is a numerical
   [17]reward, which encodes the success of an action's outcome, and the
   agent seeks to learn to select actions that maximize the accumulated
   [18]reward over time. (the use of the term reward is used here in a
   neutral fashion and does not imply any pleasure, hedonic impact or
   other psychological interpretations.)

contents

     * [19]1 overview
          + [20]1.1 the algorithmic level (machine-learning perspective)
          + [21]1.2 the mechanistic level (neuronal perspective)
     * [22]2 overview: from the algorithmic level to the neuronal
       implementation
     * [23]3 background and history
     * [24]4 basic algorithms
          + [25]4.1 different rl strategies
          + [26]4.2 the implementation-level (neuroscience)
     * [27]5 challenges and extensions to rl
          + [28]5.1 curse of dimensionality
          + [29]5.2 (temporal) credit assignment problem
          + [30]5.3 partial observability problem
          + [31]5.4 state-action space tiling
          + [32]5.5 non-stationary environments
          + [33]5.6 credit structuring problem
          + [34]5.7 exploration-exploitation dilemma
     * [35]6 references
     * [36]7 acknowledgements
     * [37]8 external links
     * [38]9 see also

overview

   in general we are following marr's approach (marr et al 1982, later
   re-introduced by gurney et al 2004) by introducing different levels:
   the algorithmic, the mechanistic and the implementation level.

the algorithmic level (machine-learning perspective)

   the best studied case is when rl can be formulated as class of
   [39]markov decision problems (mdp). the agent can visit a finite number
   of states and in visiting a state, a numerical [40]reward will be
   collected, where negative numbers may represent punishments. each state
   has a changeable value attached to it. from every state there are
   subsequent states that can be reached by means of actions. the value of
   a given state is defined by the averaged future reward which can be
   accumulated by selecting actions from this particular state. actions
   are selected according to a policy which can also change. the goal of
   an rl [41]algorithm is to select actions that maximize the expected
   cumulative reward (the return) of the agent.

   in general. rl methods are employed to address two related problems:
   the prediction problem and the control problem.
    1. prediction only: rl is used to learn the value function for the
       policy followed. at the end of learning this value function
       describes for every visited state how much future reward we can
       expect when performing actions starting at this state.
    2. control: by interacting with the environment, we wish to find a
       policy which maximizes the reward when traveling through [42]state
       space. at the end we have obtained an optimal policy which allows
       for action planning and [43]optimal control. since this is really a
       predictive type of control, solving the control problem would seem
       to require a solution to the prediction problem as well.

   in general there exist several ways for determining the optimal value
   function and/or the optimal policy.

   if we know the state transition function function t(s,a,s'), which
   describes the transition id203 in going from state s to s' when
   performing action a, and if we know the reward function r(s,a), which
   determines how much reward is obtained at a state, then algorithms
   which are called model based algorithms can be devised. they can be
   used to acquire the optimal value function and/or the optimal policy.
   most notably here value-iteration and policy-iteration are being used,
   both of which have their origins in the field of [44]dynamic
   programming (bellmann 1957) and are, strictly-speaking, therefore not
   rl algorithms (see kaelbling et al 1996 for a discussion).

   if the model (t and r) of the process is not known in advance, then we
   are truly in the domain of rl, where by an adaptive process the optimal
   value function and/or the optimal policy will have to be learned. the
   most influential algorithms, which will be described below, are:
     * [45]temporal difference learning: td; by itself used for value
       function learning,

     * adaptive actor-critics: an adaptive policy iteration algorithm,
       which approximates the model of the value function by td where the
       td error is used for both the actor and critic, and

     * id24: a unifying algorithm which allows for simultaneous
       value function and policy [46]optimization.

the mechanistic level ([47]neuronal perspective)

   early on, we note that the state-action space formalism used in
   id23 (rl) can be also translated into an equivalent
   [48]neuronal network formalism, as will be discussed below. note, the
   neuronal perspective of rl is in general indeed meant to address
   biological questions. its goals are usually not related to those of
   other id158 (ann) approaches (this is addressed by
   the machine-learning approach of rl).

overview: from the algorithmic level to the neuronal implementation

   figure 1: the context of rl

   figure [49]1 shows a summary diagram of the embedding of reinforcement
   learning depicting the links between the different fields. red shows
   the most important theoretical and green the biological aspects related
   to rl, some of which will be described below (w  rg  tter and porr 2005).
   rl plays a role in machine learning (optimal control) but also in
   theories of animal (human) learning relating rl to some aspects of
   psychology ([50]classical conditioning and [51]instrumental
   conditioning). at the same time, rl-concepts developed in machine
   learning seem to find their correspondence in the response of certain
   [52]neurons in the [53]brain (see [54]reward signals). furthermore rl
   is necessarily linked to biophysics and the theory of [55]synaptic
   plasticity.

   rl methods are used in a wide range of applications, mostly in academic
   research but also in fewer cases in industry. typical application
   fields are:
     * systems control, e.g. learning to schedule elevator dispatching,

   (crites and barto 1996);
     * playing games, e.g. [56]td-gammon (thesauro 1994), and
     * simulations of animal learning (simulating [57]classical, sutton
       and barto 1981, or [58]instrumental conditioning tasks, montague et
       al 1995, simulating [59]tropisms, porr and w  rg  tter 2003).

background and history

   a detailed account of the history of rl is found in sutton and barto
   (1998) [60][1]. here, we will only summarize the most important
   contributions.

   the top part of figure [61]1 shows that several academic disciplines
   have contributed to rl. most notably there are two:
    1. optimal control (left side).
    2. animal learning by trial and error (middle).

   [62]optimal control problems have been addressed by methods of
   [63]id145 (bellmann 1957) which is a large scientific
   area in its own right (not to be discussed here). trial-and-error
   learning has roots in psychology, especially [64]classical conditioning
   and [65]instrumental conditioning. as a consequence, the first stream
   (optimal control) was from the beginning governed by highly
   algorithmical/mathematical approaches, whereas for the second stream
   (animal learning) it took much longer for the first, still more
   qualitative, mathematical models to be developed (see, for example, the
   [66]rescorla-wagner model). optimal control and [67]instrumental
   conditioning deal with closed-loop control problems. however,
   [68]classical conditioning deals with a prediction-only problem because
   the response of the animal does not influence the experiment, or - in
   more general terms - does not influence the environment. a good short
   summary relating algorithmic approaches to real [69]classical
   conditioning experiments is given by balkenius and moren (1998).

   arising from the interdisciplinary study of these two fields, there
   appeared a very influential computational method, called the method of
   [70]temporal difference learning (td) (witten 1977, sutton and barto
   1981). [71]td learning was originally mainly associated to animal
   learning ([72]classical conditioning), where an early occurring
   reinforcer (see the stimuli in figure [73]2), the conditioned stimulus
   (cs), needs to be associated with a later occurring unconditioned
   stimulus (us) creating a situation where temporal differences of a
   (value-) function need to be evaluated. goal of this computation is to
   assure that after learning the cs becomes a predictor of the us
   (prediction problem). while td was originally designed to deal with
   such prediction problems (sutton and barto 1981, sutton 1988), is was
   also used to solve optimal control problems. of particular note is the
   work of watkins (1989) on [74]id24, a temporal difference-based
   control algorithm.

   it was essentially the work of klopf (1972, 1975, 1982, 1988), that
   began to bring td-methods together with animal learning theories. he
   also introduced the difference between evaluative and non-evaluative
   feedback, where he associated evaluative feedback to [[supervised
   learning]] (feedback from a teacher) and rightfully stated that the
   environment does not produce any evaluation. feedback that arrives from
   the environment at the sensors of a creature can only be
   non-evaluative. any evaluation, in this case, must be performed only
   internally by the animal itself. because animals don't receive
   evaluative feedback, rl would appear to be an example of unsupervised
   learning. however, this formulation hides the subtle, sometimes very
   troubling, problem of how the environment is actually defined. the
   reason this issue is a problem will be discussed later (see [75]section
   on problems below).

   [76]td methods need to predict future [77]rewards. in order to achieve
   this, td learning use value functions v(t) which assign values to
   states and then calculates the change of those values by ways of a
   temporal derivative. as a consequence, these methods are related to
   methods of correlation based, differential [78]hebbian learning (right
   side of figure [79]1), where a synaptic weight changes by the
   correlation between its input signals with the derivative of its
   output. such rules were first discussed by kosco (1986) as well as
   klopf (1986, 1988). sutton and barto's 1981 paper, however, really also
   described a differential hebbian learning rule. differential hebbian
   rules moved back into the focus of interest only after 1997, when they
   had been related to [80]spike-timing dependent plasticity (markram et
   al 1997). in this new context, gerstner et al rediscovered these rules
   in 1996 (gerstner et al 1996) and they had been applied to rl control
   problems some years later by porr and coworkers (porr and w  rg  tter
   2003, 2006).

basic algorithms

   a wide variety of algorithms exist to date with which rl problems can
   be addressed. as most of them will be covered by separate articles of
   this encyclopedia, we will only give a brief summary over the most
   important ones here.

   specifically here we need to distinguish between the machine learning-
   (sutton and barto 1998, kaelbling et al 1996) and the neuronal
   perspective (w  rg  tter and porr 2005). the machine learning perspective
   deals with states, values and actions, etc., whereas the neuronal
   perspective tries to obtain [81]neuronal signals related to
   reward-expectation or prediction-error (see below).

   again we divide the discussion into the prediction-problem (open loop)
   as well as the control problem (closed loop).

   the following side-by-side presentation compares the most important
   basic approaches and should serve as a direction for further reading.
   prediction
   algorithms: machine learning mechanisms: neuronal

   [82]td-learning: at the core of most rl algorithm lies the method of
   temporal differences (td, figure [83]2a). we consider a sequence of
   states followed by rewards\[s_t, r_{t+1}, s_{t+1}, r_{t+2},\ldots,r_t,
   s_t\ .\] the complete return \(r_t\) to be expected in the future from
   state \(s_t\) is, thus\[r_t = r_{t+1}+\gamma^1 r_{t+2}+ \ldots +
   \gamma^{t-t-1} r_t\ ,\] where \(\gamma<1\) is a discount factor
   (distant rewards are less important). id23 assumes
   that the value of a state \(v(s)\) is directly equivalent to the
   expected return\[v(s) = e_{\pi} (r_t | s_t=s)\ ,\] where \(\pi\) is
   here an unspecified action policy. thus, the value of state \(s_t\) can
   be iteratively updated with\[v(s_t) \to v(s_t) + \alpha[r_t - v(s_t)]\
   ,\] where \(\alpha\) is a step-size (often =1). note, if \(v(s_t)\)
   correctly predicts the expected complete return \(r_t\ ,\) the update
   will be zero in average and we have found the final value for \(v\ .\)
   this method requires to wait until a sequence has reached its terminal
   state before the value-update can commence. for long sequences this may
   be problematic. however, given that \(e(r_t) = e(r_{t+1}) + \gamma
   v(s_{t+1})\) we can also update iteratively by\[v(s_t) \to v(s_t) +\
   :\]

          \[\alpha[r_{t+1}+ \gamma v(s_{t+1}) - v(s_t)]\ ,\]

   which is the td(0) procedure. the elegant trick is to assume that, if
   the process converges, the value of the next state \(v(s_t+1)\) should
   be an accurate estimate of the expected return downstream to \(s_{t+1}\
   .\) we define the \(\delta\)-error as\[\delta_t= [r_{t+1}+ \gamma
   v(s_{t+1}) - v(s_t)]\ .\] normally we would only assign a new value to
   one state by performing \(v(s_t) \to v(s_t) + \alpha\delta\ ,\) not
   considering any other previously visited states. this, however, can be
   desirable and can be achieved by so called [84]eligibility traces \(e\
   ,\) which are used to update earlier visited states "a little bit". we
   define \(e_t=1\) at the currently visited state and let e decay
   gradually along states visited in the past with a decay factor
   \(\lambda=1\ ,\) so we can define\[v(s_{t+j}) \to v(s_{t+j}) +
   \alpha\delta_j e_{t+j}\] for \(j\ge 0\ .\) this procedure is known as
   \(backward-td(\lambda)\ .\) if \(\lambda=1\) then we are equally
   considering all previously visited states.

   properties of td-learning: td-learning will converge to the final value
   function assigning to each state its final value, if all states have
   been visited "often enough". this can, however, lead to very slow
   convergence if the state space is large. the expectation value of the
   \(\delta\)-error denoted by \(\xi(\delta)\) will converge to zero,
   while \(\delta\) itself can - for example - also alternate between
   positive and negative values. for large state spaces and/or sparse
   rewards convergence may require many steps and can be very slow. it is
   not possible to a priori assess if td(\(\lambda\)) will
   perform better than td(0). see sutton and barto (1998).

   neuronal-td: a similar algorithm can be designed for the neuronal
   perspective (as suggested by dayan, 2002). we assume that a [85]neuron
   \(v\) can approximately predict a [86]reward \(r\ ,\) then we should at
   every time step \(t, t+1\) find that \(v(t)\cong r(t)\) and
   \(v(t+1)\cong r(t+1)\ .\) since this is only approximately true (until
   convergence) we can in the same way define the error by\[\delta =
   r(t+1)+v(t+1)-v(t) = r(t+1)-v'\] (neglecting discount factors here for
   brevity). thus we can update weight \(w_1\) with\[\delta \omega_1=
   [x_1*e]\delta\ ,\] where \(x_1*e\) is a [87]convolution with the filter
   kernel e and denotes the fact that input \(x_1\) needs to be remembered
   for some time in the system. the function e is usually a first order
   low pass response and is also known as [88]eligibility trace. because
   the error \(\delta\) occurs later than \(x_1\ ,\) the correlation of
   \(\delta x_1\) would be zero without this type of [89]memory. figure
   [90]2b shows the basic td-rule for a neuron with one predictive
   cs-input \(x_1\) and a reward line \(r\ ,\) the us. when combining this
   with a delay line which splits \(x_1\) into many inputs, each delayed
   with respect to each other by a unit delay (serial compound
   representation) this algorithm emulates the backward-td(1) procedure.

   iso/ico-learning: an alternative neuronal approach (figure [91]2b) uses
   a correlation based differential hebbian learning rule given by:

   \[\delta \omega_i= \mu [x_i*e] \frac{d}{dt}v\] (iso-rule), or
   alternatively using pure input correlations:

   \[\delta \omega_i= \mu [x_i*e] \frac{d}{dt}[x_0*e]\] (ico-rule), where
   \(\mu<1\) is the learning rate.

   properties of iso/ico: in general \(x_0\) is the unconditioned input
   which drives the post-synaptic potential whereas the conditioned input
   \(x_1\) converges through a plastic synapse on the neuron. after
   learning the neuron's output will co-occur with the cs, but since the
   us-input \(x_0\) converges onto the neuron, this circuit (figure
   [92]3b) can also before learning be used for generating (motor-)output.
   in iso all weights are allowed to change, in ico weight \(\omega_0\)
   remains fixed. the weight change curve is similar to those observed
   with [93]spike-timing dependent plasticity. proof exists that
   \(\omega_1\) will converge if \(x_0=0\ ,\) which is a condition that
   needs to be fulfilled by an input taking a certain value and is called
   input control.

   see w  rg  tter and porr (2005) for a review and porr and w  rg  tter
   (2006).

   figure 2: diagrammatic representation of the different learning methods
   for the prediction problem. a) an agent travels through states s[i] by
   ways of actions a[i]. when reaching a new state a reward r[i] will be
   collected. td-learning uses these rewards to update the values v[i] of
   previously visited states. td(0) only updates the value of the most
   recently visited state (dashed arrows). b) neuronal td learning
   correlates a cs (r=reward) with a us (x[1]), where the cs is prolonged
   by trace e. pe=prediction error neuron, re=reward expectation neuron.
   the blue amplifier symbol denotes a changeable synapse, the 'x' a
   multiplication and the prime symbol a temporal derivative. in td only
   \(\omega_1\) can change. c) in iso-learning both synapses \(\omega_0\)
   and \(\omega_1\) can change.
                       control
   algorithms: machine learning mechanisms: neuronal

   sarsa (initially known as modified id24 rummery and niranjan,
   1994): probably the nicest aspect of the td-formalism is that it can be
   used almost unaltered to address the control problem. we note first
   that the value of state-action pairs is given by the same formal
   expectation value of an expected total return \(r_t\) as before:
   \[q(s,a) = e_\pi (r_t | s_t=s, a_t=a)\ .\] the difference is that we
   have to calculate this now assuming that at moment \(t\) we are
   visiting state \(s\) from where we take the specific action \(a\ ,\)
   whereas above the action was left unspecified. the same td(0) rule can
   be used to approximate \(q\) with\[q(s_t,a_t) \to q(s_t,a_t) +\ :\]

          \[\alpha[r_{t+1}+ \gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t)]\ .\]

   to calculate this we must for t and t+1 go through the transition:
   state, action, reward, state, action; which gives this update rule its
   name sarsa (sutton 1996). this method starts with a policy \(\pi\)
   which is continuously updated during learning (on-policy update).

   [94]id24: uses the rule\[q(s_t,a_t) \to q(s_t,a_t) +\ :\]

          \[\alpha[r_{t+1}+ \gamma \mbox{max}_a[q(s_{t+1},a)] -
          q(s_t,a_t)]\ .\]

   taking the maximum across all actions a which are possible at state
   \(s_t\) seems to be only a minor modification as compared to sarsa. in
   effect, however, it makes learning independent of the starting policy
   \(\pi\) and it allows keeping this policy throughout the whole learning
   process (off-policy update). when id24 has finished, the optimal
   policy and the optimal value function have been found, without having
   to continuously update the policy during learning. formulations for
   sarsa(\(\lambda\)) and q(\(\lambda\)) can be derived in a similar way
   as above (see sutton and barto 1998 for a discussion).

   properties of id24 and sarsa: id24 is the reinforcement
   learning algorithm most widely used for addressing the control problem
   because of its off-policy update, which makes convergence control
   easier. sarsa and actor-critics (see below) are less easy to handle. it
   can be shown that under certain boundary conditions sarsa and
   id24 will converge to the optimal policy if all state-action
   pairs are visited infinitely often.

   [95]actor-critic architectures: these play a specific role because
   originally they had been designed in the context of machine learning as
   an adaptive policy iteration algorithm. more recently actor-critics,
   however, have been much more discussed in conjunction with the
   architecture of the [96]basal ganglia (joel et al 2002). hence, their
   properties are being described on the right side of this page under
   "neuronal control".

   note: for id24 and sarsa no neuronal architectures exist so far.
   recent results suggest that animals might rather follow a sarsa-like,
   on-policy update as opposed to a id24 like, off-policy update
   (morris et al 2006, see also commentary by niv et al 2006).

   [97]actor-critic architectures: neuronal approaches, which address the
   control problem and can generate behavior, in general follow a
   control-loop architecture. figure [98]3a shows a conventional
   [99]feedback control system. in neuronal terms this is a
   [100]reflex-loop. a controller provides control signals to a system,
   which is influenced by disturbances. feedback allows the controller to
   adjusts it signals. in addition, a set-point is defined which the
   control loop tries to approximate. part b shows how to extend this into
   an actor-critic architecture (witten 1977, barto et al. 1983, sutton
   1984, barto 1995). the critic produces evaluative, reinforcement
   feedback for the actor by observing the consequences of its actions.
   the critic takes the form of a td-error, which gives an indication if
   things have gone better or worse than expected with the preceding
   action. if the td-error is positive the tendency to select this action
   should be strengthened or else, lessened. thus, actor and critic are
   adaptive through id23. on the side of machine
   learning, actor-critics are related to interleaved
   value/policy-iteration methods (kaelbling et al 1996). on the side of
   control, they are related to advanced feed-forward control and
   feed-forward compensation techniques.

   properties of actor-critics: they rely on the return maximization
   principle trying to maximize the expected return by choosing the best
   actions. they allow for the learning of goal-directed actions. the
   actor uses in general a set of predefined actions. actions are not
   easily generated de novo. the critic cannot generate actions on its own
   but must work together with the actor. convergence is slow if these
   methods are not augmented by additional mechanisms (touzet and santos
   2001). actor-critics use evaluative feedback from the environment
   labelled reward=positive or punishment=negative. as \(\xi(\delta)=0\)
   is the convergence condition, these systems are governed by
   output-control. actor-critic architectures are specifically being
   discussed in conjunction with the [101]basal ganglia where different
   models have been proposed (gurney et.al. 2004).

   closed-loop iso/ico: figure [102]3c shows a different approach, where
   it is assumed that the environment will provide temporally correlated
   signals \(x_1,x_0\) about upcoming events like a cs-us pair. learning
   goal is to minimize the later signal \(x_0\ ,\) which represents an
   error signal. after learning the primary [103]reflex input \(x_0\)
   converges to zero. actor and critic are not separate in this
   architecture which does not allow so far the sequencing of actions.
   because the system uses only correlations between signals for the
   learning, it receives strictly non-evaluative feedback from the
   environment. convergence is very fast (id62 of ico, porr
   and w  rg  tter 2006). as \(x_0=0\) is the convergence condition which is
   defined at the input, this system performs input control. see w  rg  tter
   and porr (2005).

   figure 3: diagrammatic representation of closed-loop reinforcement
   methods mainly applied using the neuronal perspective.

different rl strategies

   rl-methods can be used for learning to reach a goal step by step (goal
   directedness). they can however also be used to learn avoiding a
   disturbance (homeostasis). td methods can be used to learn goals,
   iso/ico methods are better suited for homeostasis learning. iso/ico
   methods have also been employed to learn attractive (food retrieval) or
   repulsive (obstacle avoidance) [104]tropisms, but so far not for
   learning step-wise goal-directed actions.

the implementation-level ([105]neuroscience)

   in the section we are establishing the link between the mechanistic
   level and the neuroscience, hence establishing a link between the
   abstracts anns presented in the previous sections with
   neurophysiological findings such as [106]spike timing dependent
   plasticity and [107]dopaminergic modulation.
   figure 4: examples of a prediction error (pe, a-c) and some reward
   expectation (re, d,e) neurons (schultz, 2002). the bottom panels show
   the similarity of real stdp curves (f, markram et al 1997) with the
   ones obtained from iso-learning (g, porr et al 2003).

   id23 is also reflected at the level of neuronal
   sub-systems or even at the level of single neurons. in general the
   [108]dopaminergic system of the brain is held responsible for rl.
   responses from dopaminergic neurons have been recorded in the
   [109]substantia nigra pars compacta (snc) and the [110]ventral
   tegmental area (vta) where some reflect the prediction error \(\delta\)
   of td-learning (see figure [111]3b pe). neurons in the [112]striatum,
   [113]orbitofrontal cortex and [114]amygdala seem to encode reward
   expectation (for a review see [115]reward signals, schultz 2002, see
   figure [116]3b re). these neurons have been discovered mostly in
   conjunction with appetitive (food-related) rewards. figure [117]4 shows
   some examples of prediction error- as well as reward expectation
   neurons.

   however, only few dopaminergic neurons produce error signals that
   comply with the demands of id23. most dopaminergic
   cells seem to be tuned to arousal, novelty, [118]attention or even
   intention and possibly other driving forces for animal behavior.
   furthermore the td-rule reflects a well-defined mathematical formalism
   that demands precise timing and duration of the \(\delta\) error, which
   cannot be guaranteed in the basal ganglia or the [119]limbic system
   (redgrave et al. 1999). consequently, it might be difficult to
   calculate predictions of future rewards. for that reason alternative
   mechanisms have been proposed which either do not rely on explicit
   predictions (derivatives) but rather on a hebbian association between
   reward and cs (o'reilly et al. 2007), or which use the da signal just
   as a switch which times learning after salient stimuli (redgrave and
   gurney 2007, porr and w  rg  tter 2007). hence the concept of derivatives
   and therefore predictions has been questioned in the basal ganglia and
   the limbic system and alternative more simpler mechanisms have been
   proposed which reflect the actual neuronal structure and measured
   signals.

   differential hebbian learning (e.g. iso-rule) seem to be to some degree
   compatible with novel findings on [120]spike-timing dependent synaptic
   plasticity (stdp, markram et al 1997). in this type of plasticity,
   synapses potentiate (become stronger) when the presynaptic input is
   followed by post-synaptic spiking activity, while else they are
   depressed (become weaker). the multiplicative (correlative) properties
   necessary to emulate a hebb rule can be traced back to second messenger
   chains, which phosphorylate ampa receptors and the required
   differential aspect appears to arise from the sensitivity of real
   synaptic plasticity to calcium gradients (lindskog et.al. 2006). figure
   [121]4 shows two examples of weight change curves (often called
   learning window) from a real neuron and from a differential hebbian
   learning rule emulated to be compatible with some basic biophysical
   characteristics (saudargiene et al 2004).

challenges and extensions to rl

   in spite of its influence across different fields rl is confronted with
   a variety of problems, which we will list here. note that in any given
   rl scenario the different problems listed here occur with different
   relevance. unfortunately this has in general led to a situation in the
   literature where solutions are tailor-made for the one given
   problem(-domain). the elegance of the basic rl-methods is this way
   often lost in a wide variety of add-on mechanisms and add-on
   parameters.

curse of dimensionality

   in general it is difficult to define appropriate state- and action
   spaces in all real-world rl problems. most often the tiling of the
   state space has to be rather fine to cover all possibly relevant
   situations and there can also be a wide variety of actions to choose
   from. as a consequence there exists a combinatorial explosion problem
   when trying to explore all possible actions from all possible states.
   solutions to this problem apply scale-spacing methods and/or function
   approximation methods to reduce and/or interpolate the searchable
   value-space. both methods try to generalize the value function.

(temporal) credit assignment problem

   this is a related problem. it refers to the fact that rewards,
   especially in fine grained state-action spaces, can occur terribly
   temporally delayed. for example, a robot will normally perform many
   moves through its state-action space where immediate rewards are
   (almost) zero and where more relevant events are rather distant in the
   future. as a consequence such reward signals will only very weakly
   affect all temporally distant states that have preceded it. it is
   almost as if the influence of a reward gets more and more diluted over
   time and this can lead to bad convergence properties of the rl
   mechanism. many steps must be performed by any iterative
   reinforcement-learning algorithm to propagate the influence of delayed
   reinforcement to all states and actions that have an effect on that
   reinforcement. similar strategies as above are applied to solve this
   problem.

partial observability problem

   in a real-world scenario an rl-agent will often not know exactly in
   what state it will end up after performing an action. basic
   rl-algorithms cannot be used in this case, because they require full
   observability. furthermore states must be history independent. hence it
   must be irrelevant how one has reached a certain state (systems must be
   markovian). often pomdp methods (pomdp=partial observable [122]markov
   decision problems) are used to address this problem. many solutions to
   pomdps have been designed and cannot be reviewed here. a simple
   introduction is given by (anthony r. cassandra: [123][2]) for more
   advanced literature one should mainly consult the work of littman and
   kaelbling et al [124][3].

state-action space tiling

   in view of the above problems it turns out that deciding about the
   actual state- and action-space tiling is difficult as it is often
   critical for the convergence of rl-methods. alternatively one could
   employ a continuous version of rl, but these methods are equally
   difficult to handle.

non-stationary environments

   as for other learning methods, rl will only work quasi stationary
   environments if the [125]dynamics change slowly. this is a fundamental
   problem and cannot be mitigated. if the world changes too fast, you
   cannot learn. as indicated above, many times rl-algorithms do not
   converge very fast. hence, slowly converging rl-methods may even fail
   in slowly changing environments.

credit structuring problem

   after deciding about the basic structure on which the rl-agent should
   operate we are still not done, because one also need to decide about
   the reward-structure, which will affect the learning. several possible
   strategies exist:
     * external evaluative feedback: the designer of the rl-system places
       rewards and punishments by hand. this strategy generally works only
       in very limited scenarios because it essentially requires detailed
       knowledge about the rl-agent's world.
     * internal evaluative feedback: here the rl-agent will be equipped
       with sensors that can measure physical aspects of the world (as
       opposed to 'measuring' numerical rewards). the designer then only
       decides, which of these physical influences are rewarding and which
       not.

   for a driving robot with battery, finding the charging station ought to
   be very rewarding, while hitting a wall should create a punishment.
   this strategy can be more generally applied, but might create a
   partially observable situation. in addition, evaluative feedback will
   always require influence of the designer. this can even lead to
   substantial problems if the world-model of the designer does not match
   to that of the rl-agent. pure correlation based methods (e.g.
   differential hebbian methods like iso/ico) do not use evaluative
   feedback, because their feedback from the environment is not
   pre-labeled "good" or "bad" by the designer. rather the task defines
   the learning goal which has the advantage that feedback is not limited
   to a one-dimensional signal such as the reward but can use
   multidimensional feedback from the environment.

   alternatively one could employ evolutionary methods that evolve their
   own reward function over a series of generations and avoid the
   assignment of rewards to environmental stimuli by the experimenter.

exploration-exploitation dilemma

   rl-agents need to explore their environment in order to assess its
   reward structure. after some exploration the agent might have found a
   set of apparently rewarding actions. however, how can the agent be sure
   that the found actions where actually the best? hence, when should an
   agent continue to explore or else, when should it just exploit its
   existing knowledge? mostly heuristic strategies are employed for
   example [126]annealing-like procedures, where the naive agent starts
   with exploration and its exploration-drive gradually diminishes over
   time, turning it more towards exploitation. the annealing rate, however
   depends also on the structure of the world and especially also on the
   graining of the state space and cannot be decided without guided
   guessing. recently singh et al, 2002 have developed more efficient
   solutions for the exploration/exploitation dilemma.

references

     * balkenius, c. and moren, j. (1998). [127]computational models of
       classical conditioning: a comparative study. lucs 62 issn
       1101-8453, lund university cognitive studies.

     * barto, a. (1995). adaptive critics and the basal ganglia. in houk,
       j. c., davis, j. l., and beiser, d. g., editors, models of
       information processing in the basal ganglia, 215-232. mit press,
       cambridge, ma.

     * barto, a. g., sutton, r. s., and anderson, c. w. (1983). neuronlike
       elements that can solve difficult learning control problems. in
       ieee transactions on systems, man, and cybernetics, volume 13,
       835-846

     * bellman, r. e. (1957). id145. princeton university
       press, princeton, nj.

     * crites, r.h. and barto, a.g. (1998). elevator group control using
       multiple id23 agents. machine learning,
       33:235-262.

     * dayan, p (2002). matters temporal. trends in cognitive sciences, 6,
       105-106.

     * gerstner, w., kempter, r., van hemmen, j. l., and wagner, h.
       (1996). a neuronal learning rule for sub-millisecond temporal
       coding. nature, 383:76-78.

     * gomi, h. and kawato, m. (1993). neural network control for a
       closed-loop system using feedback-error-learning. neural netw.,
       6(7):933-946

     * gurney, k. n., prescott, t. j., wickens, j. r. and redgrave, p.
       (2004). computational models of the basal ganglia: from robots to
       membranes. trends neurosci 27(8): 453-459.

     * joel, d., niv, y., and ruppin, e. (2002). actor-critic models of
       the basal ganglia: new anatomical and computational perspectives.
       neural networks, 15:535\u2013547.

     * kaelbling, l. p., littman, m. l., and moore, a. w. (1996).
       id23: a survey. journal of artificial
       intelligence research, 4:237-285.

     * klopf, a. h. (1972). brain function and adaptive systems - a
       heterostatic theory. technical report, air force cambridge research
       laboratories special report no. 133, defense technical information
       center, cameron station, alexandria, va 22304.

     * klopf, a. h. (1982). the hedonistic neuron: a theory of memory,
       learning, and intelligence. hemisphere, washington, dc.

     * klopf, a. h. (1986). a drive-reinforcement model of single neuron
       function. in denker, j. s., editor, neural networks for computing:
       aip conf. proc. , volume 151. new york: american institute of
       physics.

     * klopf, a. h. (1988). a neuronal model of classical conditioning.
       psychobiol., 16(2):85-123.

     * kosco, b. (1986). differential hebbian learning. in denker, j. s.,
       editor, neural networks for computing: aip conference proc.
       proceedings, volume 151. new york: american institute of physics.

     * lindskog, m., kim, m., wikstr  m, m.a., blackwell, k.t. and
       hellgren-kotaleski j. (2006). transient calcium and dopamine
       increase pka activity and darpp-32 phosphorylation. plos, 2:9.

     * markram, h., l  bke, j., frotscher, m., and sakmann, b. (1997).
       regulation of synaptic efficacy by coincidence of postsynaptic aps
       and epsps. science, 275:213-215.

     * marr, d., & poggio, t. (1976). from understanding computation to
       understanding neural circuitry. mit ai laboratory.

     * montague, p. r., dayan, p., person, c., and sejnowski, t. j.
       (1995). bee foraging in uncertain environments using predictive
       hebbian learning. nature, 377:725-728.

     * morris, g., nevet, a., arkadir, d., vaadia, e. and bergman, h.
       (2006). midbrain dopamine neurons encode decisions for future
       action, nature neurosci., 9(8), 1057-1063.

     * niv, y., daw, n. d. and dayan, p. (2006). choice values (commentary
       to morris et al 2006), nature neurosci., 9(8), 987-988.

     * o'reilly, r.c., frank, m.j., hazy, t.e. & watz, b. (2007). pvlv:
       the primary value and learned value pavlovian learning algorithm.
       behavioral neuroscience. in press.

     * porr, b. and w  rg  tter, f. (2003). isotropic sequence order
       learning. neural comp., 15:831-864.

     * porr, b. and w  rg  tter, f. (2007). learning with relevance: using a
       third factor to stabilise hebbian learning. neural comp. in press.

     * redgrave, p, prescott, t.j. and gurney, k. (1999). is the
       short-latency dopamine response too short to signal reward error?
       trends neurosci. 22:4 146-151.

     * redgrave, p and gurney, k.n. (2007). what does the short-latency
       dopamine signal reinforce? nature reviews neuroscience. in press.

     * rescorla, r. a. and wagner, a. r. (1972). a theory of pavlovian
       conditioning: variations in the effectiveness of reinforcement and
       nonreinforcement. in black, a. h. and prokasy, w. f., editors,
       classical conditioning ii: current research and theory. appleton
       century crofts, new york.

     * saudargiene, a., porr, b., and w  org  otter, f. (2004). how the
       shape of pre- and postsynaptic signals can influence stdp: a
       biophysical model. neural comp., 16:595-626.

     * singh, s., kearns, m. (2002). near-optimal id23
       in polynomial time. machine learning journal, volume 49, issue 2,
       pages 209-232, 2002.

     * schultz, w. (2002). getting formal with dopamine and reward.
       neuron, 36:241-263.

     * sutton, r. s. (1984). temporal credit assignment in reinforcement
       learning. phd thesis, university of massachusetts, amherst

     * sutton, r. s. (1988). learning to predict by the methods of
       temporal differences. mach. learn., 3:9-44.

     * sutton, r. s. (1996). generalization in id23:
       successful examples using [128]sparse coding. in d. s. touretzky,
       m. c. mozer and m. e. hasselmo (eds.) advances in neural
       information processing, pp. 1038-1044, mit press, cambridge, ca.

     * sutton, r. s. and barto, a. g. (1998). id23: an
       introduction. bradford books, mit press, cambridge, ma, 2002
       edition.

     * tesauro, g. (1994). td-gammon, a self-teaching backgammon program,
       achieves master-level play. neural comp. 6/2:215-219.

     * touzet, c. and santos, j. f. (2001). id24 and robotics.
       ijid98'99, european simulation symposium, marseille.

     * watkins, c. j. c. h. (1989). learning from delayed rewards. phd
       thesis, university of cambridge, cambridge, england.

     * witten, i. h. (1977). an adaptive optimal controller for
       discrete-time markov environments. information and control,
       34:286-295

     * w  rg  tter, f. and porr, b. (2005) temporal [129]sequence learning,
       prediction and control - a review of different models and their
       relation to biological mechanisms. neural comp. 17:245-319

   internal references
     * joseph e. ledoux (2008) [130]amygdala. [131]scholarpedia,
       3(4):2698.

     * peter redgrave (2007) [132]basal ganglia. scholarpedia, 2(6):1825.

     * valentino braitenberg (2007) [133]brain. scholarpedia, 2(11):2918.

     * nestor a. schmajuk (2008) [134]classical conditioning.
       scholarpedia, 3(3):2316.

     * jeremy seamans and daniel durstewitz (2008) [135]dopamine
       modulation. scholarpedia, 3(4):2711.

     * james meiss (2007) [136]dynamical systems. scholarpedia, 2(2):1629.

     * howard eichenbaum (2008) [137]memory. scholarpedia, 3(3):1747.

     * victor m. becerra (2008) [138]optimal control. scholarpedia,
       3(1):5354.

     * robert rescorla (2008) [139]rescorla-wagner model. scholarpedia,
       3(3):2237.

     * wolfram schultz (2007) [140]reward. scholarpedia, 2(3):1652.

     * wolfram schultz (2007) [141]reward signals. scholarpedia,
       2(6):2184.

     * david h. terman and eugene m. izhikevich (2008) [142]state space.
       scholarpedia, 3(3):1924.

     * andrew g. barto (2007) [143]temporal difference learning.
       scholarpedia, 2(11):1604.

acknowledgements

   f. woergoetter acknowledges the support by bmbf (federal ministry of
   education and research), bccn (bernstein center for [144]computational
   neuroscience) - goettingen project w3.

external links

see also

   [145]actor-critic method, [146]basal ganglia, [147]conditioning,
   [148]id145, [149]eligibility trace, [150]exploration vs.
   exploitation, [151]metalearning, [152]neural networks,
   [153]neuroeconomics, [154]id24, [155]rescorla-wagner model,
   [156]reward, [157]reward signals, [158]supervised learning,
   [159]td-gammon, [160]temporal difference learning, [161]unsupervised
   learning
   sponsored by: [162]eugene m. izhikevich, editor-in-chief of
   scholarpedia, the peer-reviewed open-access encyclopedia
   [163]reviewed by: [164]anonymous
   accepted on: [165]2007-09-18 05:37:33 gmt
   retrieved from
   "[166]http://www.scholarpedia.org/w/index.php?title=reinforcement_learn
   ing&oldid=127590"
   [167]categories:
     * [168]conditioning
     * [169]id23
     * [170]computational neuroscience
     * [171]computational intelligence
     * [172]multiple curators

personal tools

     * [173]log in / create account

namespaces

     * [174]page
     * [175]discussion

variants

views

     * [176]read
     * [177]view source
     * [178]view history

actions

search

   ____________________ (button) search

navigation

     * [179]main page
     * [180]about
     * [181]propose a new article
     * [182]instructions for authors
     * [183]random article
     * [184]faqs
     * [185]help
     * [186]blog

focal areas

     * [187]astrophysics
     * [188]celestial mechanics
     * [189]computational neuroscience
     * [190]computational intelligence
     * [191]dynamical systems
     * [192]physics
     * [193]touch
     * [194]more topics

activity

     * [195]recently published articles
     * [196]recently sponsored articles
     * [197]recent changes
     * [198]all articles
     * [199]list all curators
     * [200]list all users
     * [201]scholarpedia journal

tools

     * [202]what links here
     * [203]related changes
     * [204]special pages
     * [205]printable version
     * [206]permanent link

     * [207][twitter.png?303]
     * [208][gplus-16.png]
     * [209][facebook.png?303]
     * [210][linkedin.png?303]

     * [211]powered by mediawiki [212]powered by mathjax [213]creative
       commons license

     * this page was last modified on 10 september 2012, at 09:11.
     * this page has been accessed 231,856 times.
     * "id23" by [214]florentin woergoetter and bernd
       porr is licensed under a [215]creative commons
       attribution-noncommercial-sharealike 3.0 unported license.
       permissions beyond the scope of this license are described in the
       [216]terms of use

     * [217]privacy policy
     * [218]about scholarpedia
     * [219]disclaimers

references

   visible links
   1. http://www.scholarpedia.org/w/opensearch_desc.php
   2. http://www.scholarpedia.org/w/index.php?title=special:recentchanges&feed=atom
   3. http://dx.doi.org/10.4249/scholarpedia.1448
   4. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&action=cite&rev=127590
   5. http://www.scholarpedia.org/article/reinforcement_learning#mw-head
   6. http://www.scholarpedia.org/article/reinforcement_learning#p-search
   7. http://www.scholarpedia.org/article/user:bernd_porr
   8. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
   9. http://www.scholarpedia.org/article/user:florentin_woergoetter
  10. http://www.scholarpedia.org/article/user:jaldert_o_rombouts
  11. http://www.scholarpedia.org/article/user:neil_girdhar
  12. http://www.scholarpedia.org/article/user:benjamin_bronner
  13. http://www.scholarpedia.org/article/user:florentin_woergoetter
  14. http://www.scholarpedia.org/article/user:bernd_porr
  15. http://www.scholarpedia.org/article/reinforcement
  16. http://www.scholarpedia.org/article/reward_signals
  17. http://www.scholarpedia.org/article/reward
  18. http://www.scholarpedia.org/article/reward
  19. http://www.scholarpedia.org/article/reinforcement_learning#overview
  20. http://www.scholarpedia.org/article/reinforcement_learning#the_algorithmic_level_.28machine-learning_perspective.29
  21. http://www.scholarpedia.org/article/reinforcement_learning#the_mechanistic_level_.28neuronal_perspective.29
  22. http://www.scholarpedia.org/article/reinforcement_learning#overview:_from_the_algorithmic_level_to_the_neuronal_implementation
  23. http://www.scholarpedia.org/article/reinforcement_learning#background_and_history
  24. http://www.scholarpedia.org/article/reinforcement_learning#basic_algorithms
  25. http://www.scholarpedia.org/article/reinforcement_learning#different_rl_strategies
  26. http://www.scholarpedia.org/article/reinforcement_learning#the_implementation-level_.28neuroscience.29
  27. http://www.scholarpedia.org/article/reinforcement_learning#challenges_and_extensions_to_rl
  28. http://www.scholarpedia.org/article/reinforcement_learning#curse_of_dimensionality
  29. http://www.scholarpedia.org/article/reinforcement_learning#.28temporal.29_credit_assignment_problem
  30. http://www.scholarpedia.org/article/reinforcement_learning#partial_observability_problem
  31. http://www.scholarpedia.org/article/reinforcement_learning#state-action_space_tiling
  32. http://www.scholarpedia.org/article/reinforcement_learning#non-stationary_environments
  33. http://www.scholarpedia.org/article/reinforcement_learning#credit_structuring_problem
  34. http://www.scholarpedia.org/article/reinforcement_learning#exploration-exploitation_dilemma
  35. http://www.scholarpedia.org/article/reinforcement_learning#references
  36. http://www.scholarpedia.org/article/reinforcement_learning#acknowledgements
  37. http://www.scholarpedia.org/article/reinforcement_learning#external_links
  38. http://www.scholarpedia.org/article/reinforcement_learning#see_also
  39. http://www.scholarpedia.org/w/index.php?title=markov_decision_problems&action=edit&redlink=1
  40. http://www.scholarpedia.org/article/reward
  41. http://www.scholarpedia.org/article/algorithm
  42. http://www.scholarpedia.org/article/state_space
  43. http://www.scholarpedia.org/article/optimal_control
  44. http://www.scholarpedia.org/w/index.php?title=dynamic_programming&action=edit&redlink=1
  45. http://www.scholarpedia.org/article/temporal_difference_learning
  46. http://www.scholarpedia.org/article/optimization
  47. http://www.scholarpedia.org/article/neuron
  48. http://www.scholarpedia.org/w/index.php?title=neuronal_network&action=edit&redlink=1
  49. http://www.scholarpedia.org/article/reinforcement_learning#fig:overview05.gif
  50. http://www.scholarpedia.org/article/classical_conditioning
  51. http://www.scholarpedia.org/article/instrumental_conditioning
  52. http://www.scholarpedia.org/article/neuron
  53. http://www.scholarpedia.org/article/brain
  54. http://www.scholarpedia.org/article/reward_signals
  55. http://www.scholarpedia.org/w/index.php?title=synaptic_plasticity&action=edit&redlink=1
  56. http://www.scholarpedia.org/w/index.php?title=td-gammon&action=edit&redlink=1
  57. http://www.scholarpedia.org/article/classical_conditioning
  58. http://www.scholarpedia.org/article/instrumental_conditioning
  59. http://www.scholarpedia.org/w/index.php?title=tropisms&action=edit&redlink=1
  60. http://www.cs.ualberta.ca/~sutton/book/the-book.html
  61. http://www.scholarpedia.org/article/reinforcement_learning#fig:overview05.gif
  62. http://www.scholarpedia.org/article/optimal_control
  63. http://www.scholarpedia.org/w/index.php?title=dynamic_programming&action=edit&redlink=1
  64. http://www.scholarpedia.org/article/classical_conditioning
  65. http://www.scholarpedia.org/article/instrumental_conditioning
  66. http://www.scholarpedia.org/article/rescorla-wagner_model
  67. http://www.scholarpedia.org/article/operant_conditioning
  68. http://www.scholarpedia.org/article/classical_conditioning
  69. http://www.scholarpedia.org/article/classical_conditioning
  70. http://www.scholarpedia.org/article/temporal_difference_learning
  71. http://www.scholarpedia.org/article/temporal_difference_learning
  72. http://www.scholarpedia.org/article/classical_conditioning
  73. http://www.scholarpedia.org/article/reinforcement_learning#fig:sandb.gif
  74. http://www.scholarpedia.org/w/index.php?title=id24&action=edit&redlink=1
  75. http://www.scholarpedia.org/article/reinforcement_learning#problems_with_rl
  76. http://www.scholarpedia.org/article/temporal_difference_learning
  77. http://www.scholarpedia.org/article/reward
  78. http://www.scholarpedia.org/article/donald_olding_hebb
  79. http://www.scholarpedia.org/article/reinforcement_learning#fig:overview05.gif
  80. http://www.scholarpedia.org/article/spike-timing_dependent_plasticity
  81. http://www.scholarpedia.org/article/reward_signals
  82. http://www.scholarpedia.org/article/td-learning
  83. http://www.scholarpedia.org/article/reinforcement_learning#fig:sandb.gif
  84. http://www.scholarpedia.org/w/index.php?title=eligibility_traces&action=edit&redlink=1
  85. http://www.scholarpedia.org/article/neuron
  86. http://www.scholarpedia.org/article/reward
  87. http://www.scholarpedia.org/w/index.php?title=convolution&action=edit&redlink=1
  88. http://www.scholarpedia.org/w/index.php?title=eligibility_trace&action=edit&redlink=1
  89. http://www.scholarpedia.org/article/memory
  90. http://www.scholarpedia.org/article/reinforcement_learning#fig:sandb.gif
  91. http://www.scholarpedia.org/article/reinforcement_learning#fig:sandb.gif
  92. http://www.scholarpedia.org/article/reinforcement_learning#fig:closedloop01.gif
  93. http://www.scholarpedia.org/article/spike-timing_dependent_plasticity
  94. http://www.scholarpedia.org/w/index.php?title=id24&action=edit&redlink=1
  95. http://www.scholarpedia.org/w/index.php?title=actor-critic_method&action=edit&redlink=1
  96. http://www.scholarpedia.org/article/basal_ganglia
  97. http://www.scholarpedia.org/w/index.php?title=actor-critic_method&action=edit&redlink=1
  98. http://www.scholarpedia.org/article/reinforcement_learning#fig:closedloop01.gif
  99. http://www.scholarpedia.org/w/index.php?title=feedback_control&action=edit&redlink=1
 100. http://www.scholarpedia.org/w/index.php?title=reflex&action=edit&redlink=1
 101. http://www.scholarpedia.org/article/basal_ganglia
 102. http://www.scholarpedia.org/article/reinforcement_learning#fig:closedloop01.gif
 103. http://www.scholarpedia.org/w/index.php?title=reflex&action=edit&redlink=1
 104. http://www.scholarpedia.org/w/index.php?title=tropisms&action=edit&redlink=1
 105. http://www.scholarpedia.org/article/neuroscience
 106. http://www.scholarpedia.org/article/spike-timing_dependent_plasticity
 107. http://www.scholarpedia.org/article/dopamine_modulation
 108. http://www.scholarpedia.org/w/index.php?title=dopamine&action=edit&redlink=1
 109. http://www.scholarpedia.org/w/index.php?title=substantia_nigra&action=edit&redlink=1
 110. http://www.scholarpedia.org/article/ventral_tegmental_area
 111. http://www.scholarpedia.org/article/reinforcement_learning#fig:closedloop01.gif
 112. http://www.scholarpedia.org/article/striatum
 113. http://www.scholarpedia.org/w/index.php?title=orbitofrontal_cortex&action=edit&redlink=1
 114. http://www.scholarpedia.org/article/amygdala
 115. http://www.scholarpedia.org/article/reward_signals
 116. http://www.scholarpedia.org/article/reinforcement_learning#fig:closedloop01.gif
 117. http://www.scholarpedia.org/article/reinforcement_learning#fig:neuronallinks.gif
 118. http://www.scholarpedia.org/article/attention
 119. http://www.scholarpedia.org/w/index.php?title=limbic_system&action=edit&redlink=1
 120. http://www.scholarpedia.org/article/spike-timing_dependent_synaptic_plasticity
 121. http://www.scholarpedia.org/article/reinforcement_learning#fig:neuronallinks.gif
 122. http://www.scholarpedia.org/w/index.php?title=markov_decision_problems&action=edit&redlink=1
 123. http://www.cs.brown.edu/research/ai/pomdp/tutorial/index.html
 124. http://www.cs.duke.edu/~mlittman/topics/pomdp-page.html
 125. http://www.scholarpedia.org/article/dynamical_systems
 126. http://www.scholarpedia.org/w/index.php?title=simulated_annealing&action=edit&redlink=1
 127. http://www.scholarpedia.org/article/computational_models_of_classical_conditioning
 128. http://www.scholarpedia.org/article/sparse_coding
 129. http://www.scholarpedia.org/article/sequence_learning
 130. http://www.scholarpedia.org/article/amygdala
 131. http://www.scholarpedia.org/article/scholarpedia
 132. http://www.scholarpedia.org/article/basal_ganglia
 133. http://www.scholarpedia.org/article/brain
 134. http://www.scholarpedia.org/article/classical_conditioning
 135. http://www.scholarpedia.org/article/dopamine_modulation
 136. http://www.scholarpedia.org/article/dynamical_systems
 137. http://www.scholarpedia.org/article/memory
 138. http://www.scholarpedia.org/article/optimal_control
 139. http://www.scholarpedia.org/article/rescorla-wagner_model
 140. http://www.scholarpedia.org/article/reward
 141. http://www.scholarpedia.org/article/reward_signals
 142. http://www.scholarpedia.org/article/state_space
 143. http://www.scholarpedia.org/article/temporal_difference_learning
 144. http://www.scholarpedia.org/article/encyclopedia_of_computational_neuroscience
 145. http://www.scholarpedia.org/w/index.php?title=actor-critic_method&action=edit&redlink=1
 146. http://www.scholarpedia.org/article/basal_ganglia
 147. http://www.scholarpedia.org/article/conditioning
 148. http://www.scholarpedia.org/w/index.php?title=dynamic_programming&action=edit&redlink=1
 149. http://www.scholarpedia.org/w/index.php?title=eligibility_trace&action=edit&redlink=1
 150. http://www.scholarpedia.org/w/index.php?title=exploration_vs._exploitation&action=edit&redlink=1
 151. http://www.scholarpedia.org/article/metalearning
 152. http://www.scholarpedia.org/w/index.php?title=neural_networks&action=edit&redlink=1
 153. http://www.scholarpedia.org/article/neuroeconomics
 154. http://www.scholarpedia.org/w/index.php?title=id24&action=edit&redlink=1
 155. http://www.scholarpedia.org/article/rescorla-wagner_model
 156. http://www.scholarpedia.org/article/reward
 157. http://www.scholarpedia.org/article/reward_signals
 158. http://www.scholarpedia.org/w/index.php?title=supervised_learning&action=edit&redlink=1
 159. http://www.scholarpedia.org/w/index.php?title=td-gammon&action=edit&redlink=1
 160. http://www.scholarpedia.org/article/temporal_difference_learning
 161. http://www.scholarpedia.org/w/index.php?title=unsupervised_learning&action=edit&redlink=1
 162. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
 163. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&oldid=16462
 164. http://www.scholarpedia.org/article/user:anonymous
 165. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&oldid=20972
 166. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&oldid=127590
 167. http://www.scholarpedia.org/article/special:categories
 168. http://www.scholarpedia.org/article/category:conditioning
 169. http://www.scholarpedia.org/article/category:reinforcement_learning
 170. http://www.scholarpedia.org/article/category:computational_neuroscience
 171. http://www.scholarpedia.org/article/category:computational_intelligence
 172. http://www.scholarpedia.org/article/category:multiple_curators
 173. http://www.scholarpedia.org/w/index.php?title=special:userlogin&returnto=reinforcement+learning
 174. http://www.scholarpedia.org/article/reinforcement_learning
 175. http://www.scholarpedia.org/article/talk:reinforcement_learning
 176. http://www.scholarpedia.org/article/reinforcement_learning
 177. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&action=edit
 178. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&action=history
 179. http://www.scholarpedia.org/article/main_page
 180. http://www.scholarpedia.org/article/scholarpedia:about
 181. http://www.scholarpedia.org/article/special:proposearticle
 182. http://www.scholarpedia.org/article/scholarpedia:instructions_for_authors
 183. http://www.scholarpedia.org/article/special:random
 184. http://www.scholarpedia.org/article/help:frequently_asked_questions
 185. http://www.scholarpedia.org/article/scholarpedia:help
 186. http://blog.scholarpedia.org/
 187. http://www.scholarpedia.org/article/encyclopedia:astrophysics
 188. http://www.scholarpedia.org/article/encyclopedia:celestial_mechanics
 189. http://www.scholarpedia.org/article/encyclopedia:computational_neuroscience
 190. http://www.scholarpedia.org/article/encyclopedia:computational_intelligence
 191. http://www.scholarpedia.org/article/encyclopedia:dynamical_systems
 192. http://www.scholarpedia.org/article/encyclopedia:physics
 193. http://www.scholarpedia.org/article/encyclopedia:touch
 194. http://www.scholarpedia.org/article/scholarpedia:topics
 195. http://www.scholarpedia.org/article/special:recentlypublished
 196. http://www.scholarpedia.org/article/special:recentlysponsored
 197. http://www.scholarpedia.org/article/special:recentchanges
 198. http://www.scholarpedia.org/article/special:allpages
 199. http://www.scholarpedia.org/article/special:listcurators
 200. http://www.scholarpedia.org/article/special:listusers
 201. http://www.scholarpedia.org/article/special:journal
 202. http://www.scholarpedia.org/article/special:whatlinkshere/reinforcement_learning
 203. http://www.scholarpedia.org/article/special:recentchangeslinked/reinforcement_learning
 204. http://www.scholarpedia.org/article/special:specialpages
 205. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&printable=yes
 206. http://www.scholarpedia.org/w/index.php?title=reinforcement_learning&oldid=127590
 207. https://twitter.com/scholarpedia
 208. https://plus.google.com/112873162496270574424
 209. http://www.facebook.com/scholarpedia
 210. http://www.linkedin.com/groups/scholarpedia-4647975/about
 211. http://www.mediawiki.org/
 212. http://www.mathjax.org/
 213. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 214. http://www.scholarpedia.org/article/reinforcement_learning
 215. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 216. http://www.scholarpedia.org/article/scholarpedia:terms_of_use
 217. http://www.scholarpedia.org/article/scholarpedia:privacy_policy
 218. http://www.scholarpedia.org/article/scholarpedia:about
 219. http://www.scholarpedia.org/article/scholarpedia:general_disclaimer

   hidden links:
 221. http://www.scholarpedia.org/article/file:overview05.gif
 222. http://www.scholarpedia.org/article/file:overview05.gif
 223. http://www.scholarpedia.org/article/file:sandb.gif
 224. http://www.scholarpedia.org/article/file:sandb.gif
 225. http://www.scholarpedia.org/article/file:closedloop01.gif
 226. http://www.scholarpedia.org/article/file:closedloop01.gif
 227. http://www.scholarpedia.org/article/file:neuronallinks.gif
 228. http://www.scholarpedia.org/article/file:neuronallinks.gif
 229. http://www.scholarpedia.org/article/reinforcement_learning
 230. http://www.scholarpedia.org/article/reinforcement_learning
 231. http://www.scholarpedia.org/article/main_page
