   #[1]scholarpedia (en) [2]scholarpedia atom feed

recurrent neural networks

   from scholarpedia
   stephen grossberg (2013), scholarpedia, 8(2):1888.
   [3]doi:10.4249/scholarpedia.1888 revision #138057 [[4]link to/cite this
   article]
   jump to: [5]navigation, [6]search
   post-publication activity
   (button)

   curator: [7]stephen grossberg
   contributors:


   1.00 -

   [8]trevor bekolay
   0.50 -

   [9]nick orbeck

   [10]birgitta dresp-langley

   [11]baingio pinna

   [12]eugene m. izhikevich
     * [13]dr. stephen grossberg, boston university, ma

   a recurrent neural network (id56) is any network whose [14]neurons send
   feedback signals to each other. this concept includes a huge number of
   possibilities. a number of reviews already exist of some types of id56s.
   these include [15][1], [16][2], [17][3], [18][4].

   typically, these reviews consider id56s that are artificial neural
   networks (aid56) useful in technological applications. to complement
   these contributions, the present summary focuses on biological
   recurrent neural networks (bid56) that are found in the [19]brain. since
   feedback is ubiquitous in the brain, this task, in full generality,
   could include most of the brain's [20]dynamics. the current review
   divides bid56s into those in which feedback signals occur in neurons
   within a single processing layer,  which occurs in networks for such
   diverse functional roles as storing spatial patterns in short-term
   [21]memory, winner-take-all decision making, contrast enhancement and
   id172, hill climbing, [22]oscillations of multiple types
   ([23]synchronous, [24]traveling waves, chaotic), storing temporal
   sequences of events in [25]working memory, and serial learning of
   lists; and those in which feedback signals occur between multiple
   processing layers, such as occurs when bottom-up adaptive filters
   activate learned recognition categories and top-down learned
   expectations focus [26]attention on expected patterns of critical
   features and thereby modulate both types of learning.

contents

     * [27]1 types of recurrent neural networks
          + [28]1.1 binary
          + [29]1.2 linear
          + [30]1.3 continuous-nonlinear
          + [31]1.4 additive stm equation
          + [32]1.5 shunting stm equation
          + [33]1.6 generalized stm equation
          + [34]1.7 mtm: habituative transmitter gates and depressing
            synapses
          + [35]1.8 ltm: gated steepest descent learning: not hebbian
            learning
     * [36]2 processing and stm of spatial patterns
          + [37]2.1 transformation and short-term storage of distributed
            input patterns by neural networks
          + [38]2.2 the noise-saturation dilemma
          + [39]2.3 a thought experiment to solve the noise-saturation
            dilemma
          + [40]2.4 automatic gain control by the off surround prevents
            saturation
          + [41]2.5 contrast id172 and pattern processing by
            real-time probabilities.
          + [42]2.6 weber law and shift property
          + [43]2.7 physiological interpretation of shunting dynamics: the
            membrane equation of neurophysiology
          + [44]2.8 recurrent competitive fields
          + [45]2.9 winner-take-all, contrast enhancement, id172,
            and quenching threshold
          + [46]2.10 shunting dynamics in cortical models
          + [47]2.11 decision-making in competitive systems: liapunov
            methods
          + [48]2.12 competition, decision, and consensus
          + [49]2.13 adaptation level systems: globally-consistent
            decision-making
          + [50]2.14 cohen-grossberg model, liapunov function, and theorem
          + [51]2.15 symmetry does not imply convergence: synchronized
            oscillations
          + [52]2.16 unifying horizontal, bottom-up, and top-down stm and
            ltm interactions
     * [53]3 interactions of stm and ltm during neuronal learning
          + [54]3.1 unbiased spatial pattern learning by generalized
            additive id56s
          + [55]3.2 outstar learning theorem
          + [56]3.3 sparse stable category learning theorem
          + [57]3.4 adaptive bidirectional associative memory
          + [58]3.5 adaptive resonance theory
     * [59]4 working memory: processing and stm of temporal sequences
          + [60]4.1 relative activity codes temporal order in working
            memory
          + [61]4.2 working memory design enables stable learning of list
            chunks
          + [62]4.3 ltm invariance and id172 rule are realized by
            specialized rcfs
          + [63]4.4 primacy, recency, and bowed activation gradients
          + [64]4.5 experimental support
          + [65]4.6 stable chunk learning implies the magical numbers four
            and seven
          + [66]4.7 equations for some item-and-order id56s
     * [67]5 serial learning: from command cells to values, decisions, and
       plans
          + [68]5.1 avalanches
          + [69]5.2 command cells and nonspecific arousal
          + [70]5.3 self-organizing avalanches: instar-outstar maps and
            serial learning of temporal order
          + [71]5.4 context-sensitive self-organizing avalanches: what
            categories control temporal order?
          + [72]5.5 serial learning
     * [73]6 references

types of recurrent neural networks

   there are at least three streams of bid56 research: binary, linear, and
   continuous-nonlinear (grossberg, 1988):

binary

   binary systems were inspired in part by neurophysiological observations
   showing that signals between many neurons are carried by all-or-none
   spikes. the binary stream was initiated by the classical mcculloch and
   pitts (1943) model of threshold [74]logic systems that describes how
   the activities, or short-term memory (stm) traces, \(x_i\) of the
   \(i^{th}\) node in a network interact in discrete time according to the
   equation:

   \[ \tag{1} x_i(t+1) = \text{sgn} \left[ \sum_j a_{ij} x_j(t) - b_j
   \right], \]

   where \(\text{sgn}(w) = +1\) if \(w > 0\), \(0\) if \(w = 0\), and
   \(-1\) if \(w < 0\). the mcculloch-pitts model had an influence far
   beyond the field of neural networks through its influence on von
   neumann's development of the digital computer.

   caianiello (1961) used a binary stm equation that is influenced by
   activities at multiple times in the past:

   \[ \tag{2} x_i(t + \tau) = 1 \left[ \sum_{j=1}^n \sum_{k=0}^{l(m)}
   a_{ij}^{(k)} x_j(t - k \tau) - b_i \right] ,\]

   where \(l(w) = 1\) if \(w \ge 0\) and \(0\) if \(w < 0\).

   rosenblatt (1962) used an stm equation that evolves in continuous time,
   whose activities can spontaneously decay, and which can generate binary
   signals above a non-zero threshold:

   \[ \tag{3} \frac{d}{dt} x_i = -ax_i + \sum_{j=1}^n \phi (b_j + x_j)
   c_{ij}, \]

   where \(\phi(w) = 1\) if \(w \ge \theta\) and \(0\) if \(w < \theta\).
   this equation was used in the classical id88 model.

   both caianiello (1961) and rosenblatt (1962) introduced equations to
   change the weights \(a_{ij}^{(k)}\) in ([75]2) and \(c_{ij}\) in
   ([76]3) through learning. such adaptive weights are often called
   long-term memory (ltm) traces. in both these models, interactions
   between stm and ltm were uncoupled in order to simplify the analysis.
   these ltm equations also had a digital aspect. the caianiello (1961)
   ltm equations increased or decreased at constant rates until they hit
   finite upper or lower bounds. the rosenblatt (1962) ltm equations were
   used to classify patterns into two distinct classes, as in the
   id88 learning theorem.

linear

   widrow (1962) drew inspiration from the brain to introduce the gradient
   descent adeline adaptive pattern recognition machine. anderson (1968)
   initially described his intuitions about neural pattern recognition
   using a spatial cross-correlation function.  concepts from linear
   system theory were adapted to represent some aspects of neural
   dynamics, including solutions of simultaneous linear equations \(y =
   ax\) using matrix theory, and concepts about cross-correlation. kohonen
   (1971) made a transition from id202 concepts such as the
   moore-penrose pseudoinverse to more biologically motivated studies that
   are summarized in his books (kohonen, 1977, 1984). these ideas began
   with a mathematically familiar engineering framework before moving
   towards more biologically motivated nonlinear interactions.

continuous-nonlinear

   continuous-nonlinear network laws typically arose from an analysis of
   behavioral or neural data. neurophysiological experiments on the
   lateral eye of the limulus, or horseshoe crab, led to the award of a
   nobel prize to h.k. hartline. these data inspired the steady state
   [77]hartline-ratliff model (hartline and ratliff, 1957):

   \[ \tag{4} r_i = e_i - \sum_{j=1}^n k_{ij} \left[ r_j - r_{ij}
   \right]^+ , \]

   where \([w]^+ = \text{max}(w, 0)\). equation ([78]4) describes how cell
   activations \(e_i\) are transformed into smaller net responses \(r_i\)
   due to recurrent inhibitory threshold-linear signals \(-k_{ij}
   \left[r_i - r_{ij}\right]^+\). the hartline-ratliff model is thus a
   kind of continuous threshold-logic system. ratliff et al. (1963)
   extended this steady-state model to a dynamical model:

   \[ \tag{5} r_i(t) = e_i(t) - \sum_{j=1}^n k_{ij} \left[ \frac{1}{\tau}
   \int_0^t e^{- \frac{t-s}{\tau}} r_j(s) ds - r_{ij} \right]^+, \]

   which also behaves linearly above threshold. this model is a precursor
   of the additive model that is described below.

   another classical tradition arose from the analysis of how the
   excitable membrane of a single neuron can generate electrical spikes
   capable of rapidly and non-decrementally traversing the axon, or
   pathway, from one neuron's cell body to a neuron to which it is sending
   signals. this experimental and modeling work on the squid giant axon by
   hodgkin and huxley (1952) also led to the award of a nobel prize. since
   this work focused on individual neurons rather than neural networks, it
   will not be further discussed herein except to note that it provides a
   foundation for the shunting model described below.

   another source of continuous-nonlinear id56s arose through a study of
   adaptive behavior in real time, which led to the derivation of neural
   networks that form the foundation of most current biological neural
   network research (grossberg, 1967, 1968b, 1968c). these laws were
   discovered in 1957-58 when grossberg, then a college freshman,
   introduced the paradigm of using nonlinear systems of differential
   equations to model how brain mechanisms can control behavioral
   functions. the laws were derived from an analysis of how psychological
   data about human and animal learning can arise in an individual learner
   adapting autonomously in real time. apart from the rockefeller
   institute student monograph grossberg (1964), it took a decade to get
   them published.

additive stm equation

   the following equation is called the additive model because it adds the
   terms, possibly nonlinear, that determine the rate of change of
   neuronal activities, or potentials, \(x_i\):

   \[ \tag{6} \frac{d}{dt} x_i = - a_i x_i + \sum_{j=1}^n f_j(x_j) b_{ji}
   z_{ji}^{(+)} - \sum_{j=1}^n g_j(x_j) c_{ji} z_{ji}^{(-)} + i_i . \]

   equation ([79]6) includes a term for passive decay (\(-a_i x_i\)),
   positive feedback (\(\sum_{j=1}^n f_j(x_j) b_{ji} z_{ji}^{(+)}\)),
   negative feedback (\(-\sum_{j=1}^n g_j(x_j) c_{ji} z_{ji}^{(-)}\)) and
   input (\(i_i\)). each feedback term includes an activity-dependent
   (possibly) nonlinear signal (\(f_j(x_j)\), \(g_j(x_j)\)); a connection,
   or path, strength (\(b_{ji}, c_{ji}\)), and an adaptive weight, or ltm
   trace (\(z_{ij}^{(+)}, z_{ij}^{(-)}\)). if the positive and negative
   feedback terms are lumped together and the connection strengths are
   lumped with the ltm traces, then the additive model may be written in
   the simpler form:

   \[ \tag{7} \frac{d}{dt} x_i = -a_i x_i + \sum_{j=1}^n f_j(x_j)z_{ji} +
   i_i. \]

   early applications of the additive model included computational
   analyses of [80]vision, learning, recognition, [81]reinforcement
   learning, and learning of temporal order in speech, [82]language, and
   sensory-motor control (grossberg, 1969b, 1969c, 1969d, 1970a, 1970b,
   1971a, 1971b, 1972a, 1972b, 1974, 1975; grossberg and pepe, 1970,
   1971). the additive model has continued to be a cornerstone of neural
   network research to the present time; e.g., in decision-making (usher
   and mcclelland, 2001). physicists and engineers unfamiliar with the
   classical status of the additive model in neural networks called it the
   [83]hopfield model after the first application of this equation in
   hopfield (1984). grossberg (1988) summarizes historical factors that
   contributed to their unfamiliarity with the neural network literature.
   the additive model in ([84]7) may be generalized in many ways,
   including the effects of delays and other factors. in the limit of
   infinitely many cells, an abstraction which does not exist in the
   brain, the discrete sum in ([85]7) may be replaced by an integral (see
   [86]neural fields).

shunting stm equation

   grossberg (1964, 1968b, 1969b) also derived an stm equation for neural
   networks that more closely model the shunting dynamics of individual
   neurons (hodgkin, 1964). in such a shunting equation, each stm trace is
   bounded within an interval \([-d,b]\). automatic gain control,
   instantiated by multiplicative shunting, or mass action, terms,
   interacts with balanced positive and negative signals and inputs to
   maintain the sensitivity of each stm trace within its interval (see
   [87]the noise-saturation dilemma):

   \[ \tag{8} \frac{d}{dt} x_i = -a_i x_i + (b - x_i) \left[ \sum_{j=1}^n
   f_j(x_j) c_{ji} z_{ji}^{(+)} + i_i \right] - (d + x_i) \left[
   \sum_{j=1}^n g_j(x_j) e_{ji} z_{ji}^{(-)} + j_i \right]. \]

   the shunting model is approximated by the additive model in cases where
   the inputs are sufficiently small that the resulting activities \(x_i\)
   do not come close to their saturation values \(-d\) and \(b\).

   the wilson-cowan model (wilson and cowan, 1972) also uses a combination
   of shunting and additive terms, as in ([88]8). however, instead of
   using sums of sigmoid signals that are multiplied by shunting terms, as
   in the right hand side of ([89]8), the wilson-cowan model uses a
   sigmoid of sums that is multiplied by a shunting term, as in the
   expression \((b - x_i) f_j \left( \sum_{j=1}^n c_{ji} x_j z_{ji}^{(+)}
   - x_j e_{ji} z_{ji}^{(-)} + i_i \right)\). this form can saturate
   activities when inputs or recurrent signals get large, unlike ([90]8),
   as noted in grossberg (1973).

generalized stm equation

   equations ([91]6) and ([92]8) are special cases of an stm equation,
   introduced in grossberg (1968c), which includes ltm and medium-term
   memory (mtm) terms that changes at a rate intermediate between the
   faster stm and the slower ltm. the laws for stm, mtm, and ltm are
   specialized to deal with different evolutionary pressures in neural
   models of different brain systems, including additional factors such as
   transmitter mobilization (grossberg, 1969c, 1969b). this generalized
   stm equation is:

   \[ \tag{9} \frac{dx_i}{dt} = -a x_i + (b - cx_i) \left[ \sum_{k=1}^n
   f_k(x_k) d_{ki} y_{ki} z_{ki} + i_i \right] - (e + fx_i) \left[
   \sum_{k=1}^n g_k (x_k) g_{ki} y_{ki} z_{ki} + j_i \right]. \]

   in the shunting model, the parameters \(c \ne 0\) and \(f \ne 0\). the
   parameter \(e = 0\) when there is "silent" [93]shunting inhibition,
   whereas \(e \ne 0\) describes the case of hyperpolarizing shunting
   inhibition. in the additive model, parameters \(c = f = 0\). the
   excitatory interaction term \(\left[ \sum_{k=1}^n f_k (x_k) d_{ki}
   y_{ki} z_{ki} + i_i \right]\)describes an external input \(i_i\) plus
   the total excitatory feedback signal \(\left[ \sum_{k=1}^n f_k (x_k)
   d_{ki} y_{ki} z_{ki} \right]\) that is a sum of signals from other
   populations via their output signals \(f_k (x_k)\). the term \(d_{ki}\)
   is a constant connection strength between cell populations \(v_k\) and
   \(v_i\), whereas terms \(y_{ki}\) and \(z_{ki}\) describe mtm and ltm
   variables, respectively. the inhibitory interaction term \(\left[
   \sum_{k=1}^n g_k (x_k) g_{ki} y_{ki} z_{ki} + j_i \right]\) has a
   similar interpretation. equation ([94]9) assumes "fast inhibition";
   that is, inhibitory interneurons respond instantaneously to their
   inputs. slower inhibition with inhibitory interneuronal activities
   \(x_i\) uses an equation like ([95]9) to describe the temporal
   evolution of the inhibitory activities. the output signals from these
   inhibitory interneurons provide inhibitory feedback signals to the
   excitatory activities. with slow inhibition, the inhibitory feedback
   signals in ([96]9) would be \(g_k (x_k)\) instead of \(g_k (x_k)\).

   cohen and grossberg (1983) derived a liapunov function for a
   generalization of the additive and shunting models in ([97]9), with
   constant mtm and ltm variables and symmetric connections. this liapunov
   function includes as a special case the liapunov function that hopfield
   (1984) stated for the additive model (see [98]cohen-grossberg model,
   liapunov function, and theorem).

mtm: habituative transmitter gates and depressing synapses

   medium-term memory (mtm), or activity-dependent habituation, often
   called habituative transmitter gates, has multiple roles. one role is
   to carry out intracellular adaptation that divides the response to a
   current input with a time-average of recent input intensity. a related
   role is to prevent recurrent activation from persistently choosing the
   same neuron, by reducing the net input to this neuron. mtm traces also
   enable reset events to occur. for example, in a gated dipole opponent
   processing network, they enable an antagonistic rebound in activation
   to occur in the network's off channel in response to either a rapidly
   decreasing input to the on channel, or to an arousal [99]burst to both
   channels that is triggered by an unexpected event (grossberg, 1972b,
   1980a). this property enables a [100]resonance that reads out a
   predictive error to be quickly reset, thereby triggering a memory
   search, or hypothesis testing, to discover a recognition category
   capable of better representing an attended object or event (see
   [101]adaptive resonance theory; grossberg, 2012; [102][5]). mtm reset
   dynamics also help to explain data about the dynamics of visual
   perception, cognitive-emotional interactions, decision-making under
   risk, and sensory-motor control (francis and grossberg, 1996; francis
   et al., 1994; gaudiano and grossberg, 1991, 1992; grossberg, 1972b,
   1980a, 1984a, 1984b; grossberg and gutowski, 1987; ogmen and gagn  ,
   1990).

   in ([103]9), the \(i^{th}\) mtm trace, or habituative transmitter gate,
   \(y_i\), typically obeys the equation:

   \[ \tag{10} \frac{dy_i}{dt} = h(k - y_i) - lf_k(x_k)y_k. \]

   by ([104]10), \(y_i\) accumulates at a fixed rate \(h\) to its maximum
   value \(k\) via term \(h(k - y_i)\) and is inactivated, habituated, or
   depressed via a mass action interaction between the feedback signal
   \(f_k(x_k)\) and the gate concentration \(y_k\) via term
   \(lf_k(x_k)y_k\). abbott et al. (1997) reported neurophysiological data
   from the visual cortex and rederived this mtm equation from it, calling
   it a depressing synapse. tsodyks and markram (1997) derived a related
   equation using their data from the somatosensory cortex, calling it a
   dynamic synapse. the mass action term may be more complex than it is in
   ([105]10) in some situations; e.g., gaudiano and grossberg (1991, 1992)
   and grossberg and seitz (2003). the habituative transmitter gate
   \(y_k\) in the inhibitory feedback term of ([106]1) obeys a similar
   equation. by multiplying intercellular signals, transmitter gates can
   modulate their efficacy in an activity-dependent way. not all signals
   need to be habituative.
   figure 1: outstar network for spatial pattern learning. red bars are a
   schematic for cell activities, and green hemidisks for synaptic
   weights. two typical learning laws for outstar learning are also
   indicated.

ltm: gated steepest descent learning: not [107]hebbian learning

   an oft-used equation for the learning of adaptive weights, or long-term
   memory (ltm) traces, is called gated steepest descent learning. gated
   steepest descent learning permits adaptive weights to increase or
   decrease (grossberg, 1967, 1968b, 1968c). this is because the unit of
   ltm in the additive and shunting models was proved to be a distributed
   pattern of ltm traces across a network, and the ltm traces learn to
   match the pattern of activities, or stm traces, of cells across the
   network (see [108]processing and stm of spatial patterns). if an stm
   activity is large (small), then the ltm trace can increase (decrease).
   these learning laws are thus not hebbian, because the hebb (1949)
   learning postulate says that: "when an axon of cell a is near enough to
   excite a cell b and repeatedly or persistently takes part in firing it,
   some grown process or metabolic change takes place in one or both cells
   such that a's efficiency, as one of the cells firing b, is increased".
   this postulate only allows ltm traces to increase. thus, after
   sufficient learning took place, hebbian traces would saturate at their
   maximum values. the hebb postulate assumed the wrong processing unit:
   it is not the strength of an individual connection; rather it is a
   distributed pattern of ltm traces.
   figure 2: outstar drawn to clarify where the name outstar comes from.

   one variant of gated steepest descent learning, called outstar
   learning, was introduced in grossberg (1968b) for spatial pattern
   learning (figure [109]1 and figure [110]2).
   figure 3: instar network for adaptive filtering and spatial pattern
   classification, or categorization. same conventions as in figure
   [111]1.

   another variant is called instar learning, which was used in grossberg
   (1976a) for the learning of bottom-up adaptive filters (figure [112]3)
   in [113]self-organizing map (som) models [114][6]. a som uses a
   recurrent on-center off-surround network (figure [115]4) to choose one,
   or a small number, of cells for storage in stm (see [116]processing and
   stm of spatial patterns), before the stored activities trigger learning
   of ltm traces in abutting synapses (see [117]sparse stable category
   learning theorem). kohonen (1984) also used instar learning in his
   applications of the [118]som model.
   figure 4: competitive learning and self-organizing map models use
   instar inputs to drive a contrast-enhancing competition by a recurrent
   on-center off-surround network.

   outstar and instar learning are dual networks in the sense that they
   are the same, except for reversing which cells are sampling and which
   are sampled (figure [119]5).
   figure 5: the outstar and instar networks are related to each other by
   duality; that is, by the exchange of the sampling and sampled cells.

   outstars and instars were combined in grossberg (1976a) to form a
   three-layer instar-outstar network for learning multi-dimensional maps
   from any m-dimensional input space to any n-dimensional output space
   (figure [120]6). the instars learn recognition categories that
   selectively respond to an m-dimensional input pattern (see [121]sparse
   stable category learning theorem), and an active category samples a
   simultaneously active n-dimensional input pattern (see [122]outstar
   learning theorem). hecht-nielsen (1987) called such a network a
   counterpropagation network.
   figure 6: an instar-outstar network can learn an arbitrary mapping
   between m-dimensional input vectors and n-dimensional output vectors.

   in art models, these concepts were used to define a bid56. in the
   article grossberg (1976b) that introduced art, instars define the
   learning in bottom-up adaptive filters, and outstars define the
   learning in top-down expectations (figure [123]7). the learning
   instabilities of competitive learning and som models that were
   described in grossberg (1976a) led grossberg (1976b) to show how
   matching of bottom-up feature patterns by top-down learned
   expectations, and the ensuing focusing of attention upon critical
   feature patterns, can dynamically stabilize the memories learned in som
   models, as well as the multi-dimensional maps learned by an
   instar-outstar network (see [124]adaptive resonance theory).
   figure 7: in an art model, bottom-up category learning often uses
   instars, whereas top-down learning of expectations often uses outstars.

   outstar learning equation:

   \[ \tag{11} \frac{dz_{ij}}{dt} = mf_i(x_i) \left[ h_j(x_j) - z_{ij}
   \right] \]

   instar learning equation:

   \[ \tag{12} \frac{dz_{ij}}{dt} = mf_j(x_j) \left[ h_i(x_i) - z_{ij}
   \right]. \]

   equation ([125]11) describes the outstar learning equation, by which
   the \(i^{th}\) source, or sampling, cell can sample and learn a
   distributed spatial pattern of activation across a network of sampled
   cells (\(j \in j\)). when the gating signal \(f_i(x_i)\) is positive,
   the adaptive weights \(z_{ij}\) can sample the activity-dependent
   signals \(h_j(x_j)\) across the sampled network of cells. equation
   ([126]12) describes the instar learning equation, by which the
   \(j^{th}\) target cell can sample and learn the distributed pattern of
   signals (\(i \in i\)) that activated it. there are many variations of
   these gated steepest descent equations, including doubly-gated
   learning, spike-timing dependent learning, and self-normalizing
   learning (e.g., gorchetchnikov et al., 2005; grossberg and seitz,
   2003). not all connections need to be adaptive.

   as illustrated below, various combinations of these stm, mtm, and ltm
   equations have been used in scores of modeling studies since the 1960s.
   in particular, they were used by o'reilly and munakata (2000) in what
   they call the leabra model.

processing and stm of spatial patterns

transformation and short-term storage of distributed input patterns by neural
networks

   the brain is designed to process patterned information that is
   distributed across networks of neurons. for example, a picture is
   meaningless as a collection of independent pixels. in order to
   understand 2d pictures and 3d scenes, the brain processes the spatial
   pattern of inputs that is received from them by the photosensitive
   [127]retinas. within the context of a spatial pattern, the information
   from each pixel can acquire meaning. the same is true during temporal
   processing. for example, individual speech sounds heard out of context
   may sound like meaningless chirps. they sound like speech and language
   when they are part of a characteristic temporal pattern of signals. the
   stm, mtm, and ltm equations enable the brain to effectively process and
   learn from both spatial and temporal patterns of information.

   both spatial and temporal patterns may be received at multiple
   intensities. scenes can be seen in dim or bright light, and speech can
   be heard if it is whispered or shouted. in order to process either
   spatial or temporal patterns using neurons, brains have evolved network
   designs that can compensate for variable input intensities without a
   loss of pattern information.

the noise-saturation dilemma

   without suitable interactions between neurons, their input patterns can
   be lost in cellular noise if they are too small, or can saturate cell
   activities at their maximum values if they are too large. input
   amplitudes can also vary from small to large through time, just as the
   intensity of light can vary from dim to bright.  during the processing
   of a visual input from a fixed object, the total intensity of an input
   can change while the relative intensity remains constant. the relative
   intensity is called the reflectance of the surface that reflects
   variable intensities of light to the eye. many other examples exist
   wherein total intensity changes while relative intensity remains
   constant.

   what sort of network interactions enable neurons to retain their
   sensitivities to the relative sizes of their inputs across the network,
   even while these inputs may vary in size through time over several
   orders of magnitude?  the answer is: an on-center off-surround network
   whose cells obey the membrane, or shunting, equations of
   neurophysiology (grossberg, 1973, 1980a). this fact helps to explain
   why such networks are ubiquitous in the brain.

a thought experiment to solve the noise-saturation dilemma

   suppose that a spatial pattern \(i_i = \theta_i i\) of inputs is
   processed by a network of cells \(v_i, i=1,2,...,n\). each \(\theta_i\)
   is the constant relative size, or reflectance, of its input \(i_i\) and
   \(i = \sum_{k=1}^n i_k\) is the variable total input size. thus
   \(\sum_{k=1}^n \theta_k = 1\). how can each cell \(v_i\) maintain its
   sensitivity to \(\theta_i\) when \(i\) is parametrically increased? how
   is saturation avoided?

   to compute \(\theta_i = i_i \left( \sum_{k=1}^n i_k \right)^{-1}\),
   each cell \(v_i\) must have information about all the inputs \(i_k,
   k=1,2,...,n\). rewriting the ratio \(\theta_i\) as \(\theta_i = i_i
   \left( i_i + \sum_{k \ne i} i_k \right)^{-1}\) calls attention to the
   fact that increasing \(i_i\) increases \(\theta_i\), whereas increasing
   any \(i_k, k \ne i\), decreases \(\theta_i\). when this property is
   translated into an anatomy for delivering feedforward inputs to the
   cells \(v_i\), it suggests that the input \(i_i\) excited \(v_i\) and
   that all the inputs \(i_k, k \ne i\), inhibit \(v_i\). in other words,
   all the inputs compete among themselves while trying to activate their
   own cell. this rule represents a feedforward on-center off-surround
   anatomy. it has been known that on-center off-surround anatomies are
   ubiquitous in the brain at least since they were reported in the cat
   retina by kuffler (1953).

   how does the on-center off-surround anatomy activate and inhibit the
   cells \(v_i\) through time? suppose that each cell possesses \(b\)
   excitable sites of which \(x_i(t)\) are excited and \(b - x_i(t)\) are
   not excited at time \(t\). thus, at cell \(v_i\), input \(i_i\) excites
   the \(b - x_i(t)\) unexcited sites, and the total inhibitory input
   \(\sum_{k \ne i} i_k\) from the off-surround inhibits the \(x_i(t)\)
   excited sites. suppose, in addition, that excitation \(x_i(t)\) can
   spontaneously decay at a fixed rate \(a\), so that the cell can return
   to an [128]equilibrium point, set to equal \(0\) for simplicity, after
   all inputs cease. putting these properties together in one equation
   yields:

   \[ \tag{13} \frac{d}{dt} x_i = -a x_i + (b - x_i) i_i - x_i \sum_{k \ne
   i} i_k . \]

   equation ([129]13) defines a feedforward on-center (\(i_i\))
   off-surround (\(\sum_{k \ne i} i_k\)) network whose cells obey a simple
   version of the shunting model in equation ([130]8).

   if a fixed spatial pattern \(i_i = \theta_i i\) is presented and the
   total input \(i\) is held constant for awhile, then each \(x_i(t)\)
   approaches an equilibrium value that is found by setting \(\frac{d}{dt}
   x_i = 0\) in equation ([131]13). then

   \[ \tag{14} x_i = \theta_i \frac{bi}{a + i}. \]

   note that the relative activity \(x_i = x_i \left( \sum_{k=1}^n x_k
   \right)^{-1}\) equals \(\theta_i\) no matter how large \(i\) is chosen;
   there is no saturation. however, if the off-surround input is removed,
   then all the \(x_i\) saturate at \(b\) as the total input \(i\) becomes
   large.

automatic gain control by the off surround prevents saturation

   saturation is prevented in ([132]13) due to automatic gain control by
   the inhibitory inputs from the off-surround. in other words, the
   off-surround \(\sum_{k \ne i} i_k\) multiplies \(x_i\). the total gain
   is found by rewriting ([133]13) as:

   \[ \tag{15} \frac{d}{dt} x_i = -(a + i) x_i + bi_i. \]

   the gain is the coefficient of \(x_i\), namely \(-(a + i)\). indeed, if
   \(x_i(0) = 0\), then ([134]15) can be integrated to yield:

   \[ \tag{16} x_i(t) = \theta_i \frac{bi}{a + i} \left( 1 - e^{-(a + i)t}
   \right) .\]

   by ([135]16), both the steady state and the rate of change of \(x_i\)
   depend upon input strength \(i\). this is characteristic of mass
   action, or shunting, networks but not of additive networks, in which
   the inputs do not multiply the activities \(x_i\).

contrast id172 and pattern processing by real-time probabilities.

   another property of ([136]14) is that the total activity:

   \[ \tag{17} x = \sum_{k=1}^n x_k = \frac{bi}{a + i} \]

   is independent of the number of active cells and approaches \(b\) as
   \(i\) increases. this id172 rule is a conservation law which
   says, for example, that increasing one activity forces a decrease in
   other activities. this property helps to explain such properties of
   visual perception as brightness constancy and brightness contrast
   (cornsweet, 1970; grossberg and todorovic, 1988). during brightness
   contrast, increasing the luminance of inputs to the off-surround makes
   the on-center look darker. the id172 property is called
   contrast id172 in applications to visual perception. more
   generally, id172 underlies many properties of limited capacity
   processing in the brain, notably in perception and cognition, with
   working memory capacity limits being a classical example.

weber law and shift property

   writing equation ([137]14) in logarithmic coordinates shows that
   increasing the off-surround input does not reduce the sensitivity of
   the network to inputs to the on-center; rather, it shifts network
   responses to larger input sizes without a loss of sensitivity. in
   particular, let \(k_i = \ln(i_i)\) and \(i_i = e^{k_i}\). also write
   the total off-surround input as \(j_i = \sum_{k \ne i} i_k\). then
   ([138]14) can be written in logarithmic coordinates as:

   \[ \tag{18} x_i(k_i, j_i) = \frac{b e^{k_i}}{a + e^{k_i} + j_i}. \]

   how does the activity \(x_i\) change if the off-surround input \(j_i\)
   is parametrically set at increasingly high values? equation ([139]18)
   shows that the entire response curve of \(x_i\) to its on-center input
   \(k_i\) also shifts, and thus its dynamic range is not compressed. for
   example, suppose that the off-surround input is increased from
   \(j_i^{(1)}\) to \(j_i^{(2)} = j_i^{(1)} + s_i\) by an amount \(s_i\) .
   then the amount of shift in the response curve is:

   \[ \tag{19} s_i = \ln \frac{a + j_i^{(2)}}{a + j_i^{(1)}}. \]

   such a shift property is found, for example, in the retina of the
   mudpuppy necturus (werblin, 1971). generalizations of the feedforward
   on-center off-surround shunting network equations generate many other
   useful properties, including weber law processing, adaptation level
   processing, and edge and spatial frequency processing (grossberg,
   1983).

physiological interpretation of shunting dynamics: the membrane equation of
neurophysiology

   the shunting equation ([140]13) has the form of the membrane equation
   on which cellular neurophysiology is based. this membrane equation is
   the voltage equation that appears in the equations of hodgkin and
   huxley (1952). in other words, the gedanken experiment shows how the
   noise-saturation dilemma is solved by using the membrane, or shunting,
   equation of neurophysiology to describe cells interacting in on-center
   off-surround anatomies. because on-center off-surround anatomy and
   shunting dynamics work together to solve the noise-saturation dilemma,
   it is reasonable to predict that they coevolved during evolution.

   the membrane equation describes the voltage \(v(t)\) of a cell by the
   law:

   \[ \tag{20} c \frac{\partial v}{\partial t} = (v^+ - v)g^+ + (v^- -
   v)g^- + (v^p - v) g^- . \]

   in ([141]20), \(c\) is a capacitance; \(v^+\), \(v^-\), and \(v^p\) are
   constant excitatory, inhibitory, and passive saturation voltages,
   respectively; and \(g^+\), \(g^-\), and \(g^p\) are excitatory,
   inhibitory, and passive conductances, respectively. when the saturation
   voltages are chosen to satisfy \(v^- \le v^p < v^+\), then the cell
   voltage satisfies \(v^- \le v(t) \le v^+\). often \(v^+\) represents
   the saturation point of a \(na^+\) channel, and \(v^-\) represents the
   saturation point of a \(k^+\) channel.

   there is [142]symmetry-breaking in ([143]20) because \(v^+ - v^p\) is
   usually much larger than \(v^p - v^-\). symmetry-breaking implies a
   noise suppression property when it is coupled to an on-center
   off-surround anatomy (grossberg, 1988). then the network suppresses
   uniformly active inputs and generates suprathreshold responses only to
   inputs that are larger than a baseline value, or adaptation level. this
   property illustrates that excitation and inhibition need to be properly
   balanced to achieve efficient neuronal dynamics: when excitation is too
   large, seizure activity can occur in a bid56. when inhibition is too
   large, processing can never get started. symmetry-breaking can be
   achieved during development by an opposites attract rule whereby the
   relative sizes of the intracellular excitatory and inhibitory
   saturation voltages \(v^+\) and \(v^-\) control the relative total
   strengths of the intercellular off-surround and on-center connections,
   respectively (grossberg, 1978a, section 45).

recurrent competitive fields

   the activities \(x_i\) in ([144]13) rapidly decay if their inputs
   \(i_i\) are shut off. persistent storage in stm is achieved when
   feedback signals exist among the various populations, thereby creating
   a bid56. the noise-saturation dilemma confronts all cellular tissues
   which process input patterns, whether the cells exist in a feedforward
   or feedback anatomy. to solve the noise-saturation dilemma in a id56,
   excitatory feedback signals need to be balanced by inhibitory feedback
   signals. the simplest recurrent on-center off-surround shunting id56,
   also called a recurrent competitive field (rcf), is defined by
   (grossberg, 1973):

   \[ \tag{21} \frac{d}{dt} x_i = -a x_i + (b - x_i) \left[ i_i + f(x_i)
   \right] - x_i \left[ j_i + \sum_{k \ne i} f(x_k) \right] . \]

winner-take-all, contrast enhancement, id172, and quenching threshold

   grossberg (1973) proved theorems showing how the choice of feedback
   signal function \(f(w)\) transforms an input pattern before it is
   stored persistently in stm. given the fundamental nature of these
   results for all bid56s, they will be reviewed below.
   figure 8: how different choices of the feedback signal function \(f\)
   in a recurrent on-center off-surround shunting network can transform
   input patterns before they are stored in short-term memory.

   figure [145]8 summarizes the results. these theorems provided the first
   rigorous proofs of winner-take-all (wta) properties, and of the use of
   sigmoid signal functions to generate a self-normalizing "bubble", or
   partial contrast-enhancement, above a quenching threshold. the theorems
   began the mathematical classification of cooperative-competitive
   recurrent nonlinear dynamical systems, whose properties are applicable
   to many fields, ranging from [146]morphogenesis to economics
   (grossberg, 1988).

   to prove the theorems, ([147]21) is transformed into total activity
   variables \(\sum_{k=1}^n x_k\) and pattern variables \(x_i = x_i
   x^{-1}\) under the assumption that the inputs \(i_i\) and \(j_i\) are
   set to zero during the stm storage process. then ([148]21) may be
   rewritten as:

   \[ \tag{22} \frac{d}{dt} x_i = bx_i \sum_{k=1}^n x_k \left[ h(x_i x) -
   h(x_k x) \right] \]

   and

   \[ \tag{23} \frac{d}{dt} x = -ax + (b - x) \sum_{k=1}^n f(x_k x), \]

   where the function \(h(w) = f(w)w^{-1}\) is called the hill function
   because it exhibits a "hill" of activity for every transition between a
   faster-than-linear, linear, and slower-than-linear shape in the signal
   function, as shown for the sigmoid function in figure [149]8 and figure
   [150]9.
   figure 9: a sigmoid signal function exploits the properties of its
   faster-than-linear, approximately linear, and slower-than-linear
   processing ranges to partially contrast-enhance an input pattern before
   storing it in short-term memory, while suppressing signals that fall
   below a quenching threshold.

   if \(f(w)\) is linear   that is, \(f(w) = cw\), then \(h(w) = c\) and all
   \(\frac{d}{dt} x_i = 0\) in ([151]22). then ([152]21) can preserve any
   pattern in stm! however, by ([153]23), if \(a \ge b\), then \(x(t)\)
   approaches \(0\) as \(t \rightarrow \infty\), so that no pattern is
   stored in stm. a pattern is stored in stm only if \(b > a\). then
   \(x(t) \rightarrow b - a\) as \(t \rightarrow \infty\), so that the
   total activity is normalized. this result implies that, if stm storage
   is possible and \(x(0) > 0\), then \(x(t) \rightarrow b - a\) even if
   no input occurs. in other words, noise will be amplified as vigorously
   as inputs. a linear signal function amplifies noise, and is therefore
   inadequate despite its perfect memory of any input pattern. that is why
   nonlinear signal functions are needed.

   a slower-than-linear signal function   for example, \(f(x) = cw(d +
   w)^{-1}\) or, more generally, any \(f(w)\) whose hill function \(h(w)\)
   is monotone decreasing   is even worse, because it amplifies noise and
   eliminates all differences in inputs within the stored pattern. this
   happens because, by ([154]22), if \(x_i > x_k, k \ne i\), then
   \(\frac{d}{dt} x_i < 0\) and if \(x_i < x_k, k \ne i\) then
   \(\frac{d}{dt} x_i > 0\). thus the maximum activity decreases and the
   minimum activity increases until all the activities become equal.

   if both linear and slower-than-linear signal functions amplify noise,
   then one must turn to faster-than-linear functions in the hope that
   they suppress noise. if \(f(w)\) is faster-than-linear   --for example,
   \(f(x) = cw^n, n > 1\), or, more generally, any \(f(w)\) whose hill
   function \(h(w)\) is monotone increasing   then noise is, indeed,
   suppressed. in this case, if \(x_i > x_k, k \ne i\), then
   \(\frac{d}{dt} x_i > 0\) and if \(x_i < x_k, k \ne i\), then
   \(\frac{d}{dt} x_i < 0\). as a result, the network chooses the
   population with the initial maximum of activity and totally inhibits
   activity in all other populations. this network behaves like a
   winner-take-all binary choice machine. the same is true for total
   activity, since as \(t \rightarrow \infty\), ([155]23) becomes
   approximately:

   \[ \tag{24} \frac{d}{dt} x \cong x [ -a + (b - x) h(x) ]. \]

   thus, the equilibrium points of \(x_i\) as \(t \rightarrow \infty\) are
   \(e_0 = 0\) and all the solutions of the equation

   \[ \tag{25} h(x) = a(b - x)^{-1}; \]

   see figure [156]10. if \(h(0) < a / b\), then the smallest solution,
   \(e_1\), of ([157]25) is unstable, so that activities \(x(t) < e_1\)
   are suppressed as \(t \rightarrow \infty\). this is noise suppression
   due to recurrent competition. every other solution \(e_2, e_4, ...\) of
   ([158]25) is a [159]stable equilibrium point of \(x(t)\) as \(t
   \rightarrow \infty\) (total activity quantization) and all equilibria
   are smaller than \(b\) (id172).
   figure 10: equilibrium points of a winner-take all recurrent
   competitive field with a faster-than-line signal function are
   intersections of the hill function \(h(w)\) with the function \(a(b -
   w)^{-1}\). see text for details.

   the faster-than-linear signal contrast-enhances the pattern so
   vigorously that the good property of noise suppression is joined to the
   extreme property of winner-take-all (wta) choice. although wta is often
   a useful property in applications to choice behavior (e.g., dev, 1975;
   grossberg, 1976a; grossberg and pilly, 2008; koch and ullman, 1985;
   wang, 2008), there are many cases where noise suppression is desired
   but more than one feature or category needs to be stored in stm. how
   can this be accomplished?

   the results above show that any signal function that suppresses noise
   must be faster-than-linear at small activities. in addition, all signal
   functions in biology must be bounded. such a combination is achieved
   most simply by using a sigmoid signal function, which is a hybrid of
   faster-than-linear at small activities, approximately linear at
   intermediate activities, and slower-than-linear at high activities
   (figure [160]9). then there exists a quenching threshold (qt) such that
   if initial activity falls below the qt, then its activity is quenched.
   all initial activities that exceed the qt are contrast-enhanced and
   stored in stm (figure [161]8). the faster-than-linear part of the
   sigmoid suppresses noise and starts to contrast-enhance the activity
   pattern. as total activity normalizes, the approximately linear range
   is reached and tends to store the partially contrast-enhanced pattern.
   the qt converts the network into a tunable filter. for example, a burst
   of nonspecific arousal in response to an unexpected event that
   multiplicatively inhibits all the recurrent inhibitory
   [162]interneurons will lower the qt and facilitate storage of inputs in
   stm until the cause of the unexpected event can be determined.

shunting dynamics in cortical models

   multiple generalizations of rcfs have been studied and used to explain
   data ranging from visual and speech perception and attentive category
   learning (see [163]unifying horizontal, bottom-up, and top-down stm and
   ltm interactions) to the selection of commands for arm movement control
   (e.g., cisek, 2006) and for [164]eye movement control in response to
   probabilistically defined visual motion signals (e.g., grossberg and
   pilly, 2008). as noted above, usher and mcclelland (2001) modeled
   probabilistic decision making using an additive model. this model does
   not exhibit the self-id172 properties that arise from rcf
   shunting dynamics.

   a number of authors have applied shunting properties to simulate data
   about the properties of the cortical circuits that subserve visual
   perception; e.g., douglas et al. (1995), grossberg and mingolla (1985),
   grossberg and todorovic (1988), heeger (1992), and mclaughlin et al.
   (2000). shunting dynamics also played a key role in the development of
   the competitive learning (cl), self-organizing map (som), and
   [165]adaptive resonance theory (art) models ([166]scholarpedia:
   adaptive resonance theory; grossberg, 1976a, 1976b, 1980a), but not in
   the cl and som versions of von der malsburg (1973) and kohonen (1984).
   an rcf with spiking neurons has also been shown to replicate key
   properties of the grossberg (1973) theorems for rate-based neurons
   (palma et al., 2012).

decision-making in competitive systems: liapunov methods

   the ubiquity of rcfs led to a search for the most general networks that
   could guarantee stable stm storage. the rcf in ([167]21) is a special
   case of a competitive dynamical system. in general, a competitive
   dynamical system is defined by a system of differential equations such
   that:

   \[ \tag{26} \frac{d}{dt} x_i = f_i(x_1, x_2, ..., x_n) \]

   where

   \[ \tag{27} \frac{\partial f_i}{\partial x_j} \le 0, i \ne j \]

   and the \(f_i\) are chosen to generate bounded trajectories. by
   ([168]27), increasing the activity \(x_j\) of a given population can
   only decrease the growth rates \(\frac{d}{dt}x_i\) of other
   populations, \(i \ne j\), or may not influence them at all. in such
   systems, cooperative interactions typically occur within a population
   while competitive interactions can occur between populations, as in the
   recurrent on-center off-surround network ([169]21). grossberg  (1978d,
   1980b) developed a mathematical method to classify the dynamics of
   competitive dynamical systems by proving that any competitive system
   can be analyzed by keeping track of the population that is winning the
   competition through time. this method defines jump sets at the times
   when the winning population is replaced by   that is, jumps to   another
   population. tracking trajectories through jump sets formalizes keeping
   track of the population that is winning the competition through time.
   jump sets define a kind of decision hypersurface. if the jumps form a
   cycle, so that no globally consistent winner exists, then oscillations
   can occur. in particular, such a jump cycle occurs in the may and
   leonard (1975) model of the voting paradox. if the jumps only form
   [170]id90, without cycles, then all trajectories converge to
   limits. a global liapunov functional was defined and provides the
   "energy" that moves system trajectories through these oscillatory or
   convergent decision hypersurfaces through time. see grossberg (1988,
   section 11) for a review.

competition, decision, and consensus

   this method was applied to study a general problem that has intrigued
   philosophers and scientists for hundreds of years, and which includes
   many rcfs as special cases: how do arbitrarily many individuals,
   populations, or states, each obeying unique and personal laws, ever
   succeed in harmoniously interacting with each other to form some sort
   of stable society, or collective mode of behavior? if each individual
   obeys complex laws, and is ignorant of other individuals except via
   locally received signals, how is social chaos averted? how can local
   ignorance and global order, or consensus, be reconciled? considerable
   interest has focused on the question: how simple can a system be and
   still generate "chaotic" behavior (e.g., alligood et al., 1996)? the
   above issue considers the converse question: how complicated can a
   system be and still generate order?

   grossberg (1978c) posed these questions and introduced a class of bid56s
   in which this type of global consensus arises, along with mathematical
   methods to prove it. consensus arises in these systems because, despite
   essentially arbitrary irregularities and nonlinearities in local system
   design, there exists a powerful symmetry in the global rules that bind
   together the interacting populations. this symmetry is expressed by the
   existence of a shared, but state-dependent, inter-population
   competition function, also called an adaptation level. these results
   suggest that a breakdown of symmetry in competitive id56s, say due to
   the existence of asymmetric biases in short-range inter-population
   interactions, is a basic cause of oscillations and chaos in these
   systems, as is illustrated by the voting paradox. there appears to
   exist a trade-off between how global the adaptation level ("communal
   understanding") is and how freely local signals ("individual
   differences") can be chosen without destroying global consensus.

adaptation level systems: globally-consistent decision-making

   system ([171]21) is a special case of a competitive network with a
   broad inhibitory surround. a much more general class of systems, the
   adaptation level systems, also has this property:

   \[ \tag{28} \frac{d}{dt} x_i = a_i(x) \left[ b_i(x_i) - c(x) \right] ,
   \]

   where \(x = (x_1, x_2, ..., x_n)\), \(a_i(x)\) is a state-dependent
   amplification function, \(b_i(x_i)\) is a self-signal function, and
   \(c(x)\) is the state-dependent adaptation level against which each
   \(b_i(x_i)\) is compared. for the special case of ([172]21),

   \[ \tag{29} a_i(x) = x_i, \]

   \[ \tag{30} b_i(x_i) = x_i^{-1} [bf(x_i) + i_i] - a - i_i - j_i, \]

   and

   \[ \tag{31} c(x) = \sum_{k=1}^n f(x_k). \]

   the same equations hold with \(a\), \(b\), and \(f(x_i)\) in ([173]21)
   replaced by \(a_i\), \(b_i\), and \(f_i(x_i)\); that is, different
   parameters and signal functions for each cell, for arbitrarily many
   cells.

   grossberg (1978c) proved that all trajectories in such systems are
   "stored in stm"; that is, converge to equilibrium values as \(t
   \rightarrow \infty\), even in systems which possess infinitely many
   equilibrium points. the proof shows how each \(x_i(t)\) gets trapped
   within a sequence of decision boundaries that get laid down through
   time at the abscissa values of the peaks in the graphs of the signal
   functions \(b_i(x_i)\), starting with the highest peaks and working
   down. these signal functions generalize the hill function in ([174]22);
   see ([175]30). multiple peaks correspond to multiple cooperating
   subpopulations. these graphs may thus be very complex if each
   population contains multiple cooperating subpopulations. after all the
   decision boundaries get laid down, each \(x_i(t)\) is trapped within a
   single valley of its \(b_i\) graph. after this occurs for all the
   \(x_i\) variables, the function \(b(x(t)) = \max [ b_i (x(t)) : i = 1,
   2, ..., n]\) is a liapunov function, whose liapunov property is then
   used to complete the proof of the theorem.

   a special case of the theorem concerns a competitive market with an
   arbitrary number of competing firms (grossberg, 1988, section 12). each
   firm can choose one of infinitely many production and savings
   strategies that is unknown to the other firms. the firms know each
   other's behaviors only through their effect on a competitive market
   price, and they produce more goods at any time only if application of
   their own firm's production and savings strategy will lead to a net
   profit with respect to that market price. the theorem proves that the
   price in such a market is stable and that each firm balances its books.
   the theorem does not, however, determine which firms become rich and
   which go broke.

cohen-grossberg model, liapunov function, and theorem

   due to the importance of symmetry in proving global approach to
   equilbria, as in the adaptation level systems ([176]28), cohen and
   grossberg attempted to prove that all trajectories of systems of the
   cohen-grossberg form:

   \[ \tag{32} \frac{d}{dt} x_i = a_i(x_i) [ b_i(x_i) - \sum_{k=1}^n
   c_{ij} d_j(x_j)], \]

   with symmetric interaction coefficients \(c_{ij} = c_{ji}\) and weak
   assumptions on their defining functions, approach equilibria as \(t
   \rightarrow \infty\). systems ([177]32) include both additive model and
   shunting model networks ([178]6) and ([179]8) with distance-dependent,
   and thus symmetric, interaction coefficients, the brain-state-in-a-box
   model (anderson et al., 1977), the continuous-time version of the
   mcculloch and pitts (1943) model, the [180]id82 equation
   (ackley et al., 1985), the ratliff et al. (1963) model, the
   volterra-lotka model (lotka, 1956), the gilpin and ayala (1973) model,
   the eigen and schuster (1978) model, the cohen and grossberg (1986,
   1997) masking field model, and so on.

   cohen and grossberg first attempted to prove global equilibrium by
   showing that all cohen-grossberg systems generate jump trees, and thus
   no jump cycles, which would immediately prove the desired result. this
   hypothesis still stands as an unproved conjecture. while doing this,
   inspired by the use of liapunov methods for more general competitive
   systems, cohen and grossberg (1983; see also grossberg (1982))
   discovered the cohen-grossberg liapunov function that they used to
   prove that global equilibria exist:

   \[ \tag{33} v = - \sum_{i=1}^n \int^{x_i} b_i (\xi_i) d_i^{\prime}
   (\xi_i) d \xi_i + \frac{1}{2} \sum_{j, k=1}^n c_{jk} d_j(x_j) d_k
   (x_k). \]

   equation ([181]33) defines a liapunov function because integrating
   \(v\) along trajectories implies that:

   \[ \tag{34} \frac{d}{dt} v = - \sum_{i=1}^n a_i d_i^{\prime} \left[ b_i
   - \sum_{j=1}^n c_{ij} d_j \right]^2 . \]

   if \(a_i d_i^{\prime} \ge 0\), then ([182]34) implies that
   \(\frac{d}{dt} v \le 0\) along trajectories. once this basic property
   of a liapunov function is in place, it is a technical matter to
   rigorously prove that every trajectory approaches one of a possibly
   large, or infinite, number of equilibrium points.

   as noted above, the liapunov function ([183]33) proposed in cohen and
   grossberg (1983) includes both the additive model and shunting model,
   among others. a year later, hopfield (1984) published the special case
   of the additive model and liapunov function and asserted, without
   proof, that trajectories approach equilibria. based on this 1984
   article, the additive model has been erroneously called the
   [184]hopfield network by a number of investigators, despite the fact
   that it was published in multiple articles since the 1960s and its
   liapunov function was also published in 1982-83. a historically more
   correct name, if indeed names must be given, is the
   cohen-grossberg-hopfield model, which is the name already used in
   articles such as burton (1993), burwick (2006), guo et al. (2004),
   hoppensteadt and izhikevich (2001), menon et al. (1996), and wu and zou
   (1995).

symmetry does not imply convergence: synchronized oscillations

   cohen (1988) showed that symmetric coefficients are not sufficient to
   ensure global convergence by constructing distance-dependent (hence
   symmetric) on-center off-surround networks that support persistent
   oscillations. these networks can send excitatory feedback signals to
   other populations than themselves. they are a special case of ([185]8)
   with fast-acting inhibitory interneurons. it has long been known that
   shunting networks with slow inhibitory interneurons can persistently
   oscillate (e.g., ellias and grossberg, 1975). this observation led to
   the prediction that neural networks can undergo synchronized
   oscillations, first called order-preserving limit cycles (grossberg,
   1976b), during attentive resonant states. the early articles concerning
   synchronized oscillations during attentive brain dynamics (e.g., gray
   et al., 1989; grossberg and somers, 1991; grossberg and grunewald,
   1997; eckhorn et al., 1990; somers and kopell, 1993) have been followed
   by hundreds more. persistent oscillations can also occur in rcfs in
   which, instead of slow inhibitory interneurons, habituative gates
   ([186]10) multiply the recurrent signals in ([187]9) (e.g., carpenter
   and grossberg, 1983)

unifying horizontal, bottom-up, and top-down stm and ltm interactions

   most of the id56s considered above characterize their interaction terms
   in abstract mathematical terms; e.g., symmetry of connection strengths.
   in contrast, the bid56s in the brain are embodied in architectures with
   highly differentiated anatomical circuits. for example, models of how
   the cerebral cortex works are defined by bid56s that integrate
   bottom-up, horizontal, and top-down interactions in laminar circuits
   with identified cells. these models illustrate the computational
   paradigm of laminar computing (grossberg, 1999, 2003) which has begun
   to classify how different behavioral functions, such as vision,
   cognition, speech, and behavioral choice, may be realized by
   architectures that are all variations on a shared laminar design. these
   architectures include the:

   laminart family of models of how the visual cortex, notably cortical
   areas v1, v2, and v4, interact together to see (figure [188]11; e.g.,
   cao and grossberg, 2005, 2012; grossberg and raizada, 2000; grossberg
   and versace, 2008; raizada and grossberg, 2001),
   figure 11: schematic of the laminar circuitry within the laminart model
   for bottom-up adaptive filtering, horizontal perceptual grouping, and
   top-down attention in lateral geniculate nucleus (lgn) and cortical
   areas v1 and v2. see text and raizada and grossberg (2001) for details.

   the list parse model of how prefrontal cortical working memory and list
   chunk learning in the ventrolateral and dorsolateral prefrontal
   cortices interact with adaptively-timed volitional processes in the
   [189]basal ganglia to generate variable-speed motor trajectory commands
   in the [190]motor cortex and [191]cerebellum (figure [192]12; grossberg
   and pearson, 2008),
   figure 12: schematic of the laminar circuits for cognitive working
   memory and learned list chunking, proposed to be in the ventrolateral
   prefrontal cortex, that is modeled by the list parse model. these
   circuits interact with circuits for motor working memory, variable-rate
   volitional control and timing, and trajectory generation. see text and
   grossberg and pearson (2008) for details.

   the cartword model of contextual interactions during speech perception
   by the auditory cortex, including backwards effects in time (figure
   [193]13; grossberg and kazerounian, 2011),
   figure 13: (a) the artword model processing stages. (b) the
   [194]conscious artword, or cartword, laminar circuits for processing
   sequences of speech sounds and generating conscious representations of
   them.  see text and grossberg and kazerounian (2011) for details.

   the telos model of learning and choice of saccadic eye movement
   commands by interactions between prefrontal cortex (pfc), [195]frontal
   eye fields (fef), posterior [196]parietal cortex (ppc) , anterior and
   posterior inferotemporal cortex (ita, itp), nigro-thalamic and
   nigro-collicular circuits of the basal ganglia (bg), superior
   colliculus (sc), and related brain regions (figure [197]14; brown et
   al., 1999, 2004).
   figure 14: schematic of the laminar circuits within the telos model of
   how the frontal eye fields (fef) interact with the basal ganglia and
   multiple other brain regions to learn and perform different saccadic
   eye movement tasks. see text and brown et al. (2004) for details.

   and the listelos model of learning and choice of sequences of saccadic
   eye movements, wherein an item-order-rank spatial working memory in the
   prefrontal cortex (pfc) stores sequences of saccadic eye movement
   commands that can include repeats, and which are selected in the
   supplementary eye fields (sef) as these regions interact with posterior
   parietal cortex (ppc), frontal eye fields (fef), basal ganglia (bg),
   and superior colliculus (sc) to carry out operations such as loading
   the sequences in working memory, opening gates to enable the various
   processing stages to selectively generate their outputs, and releasing
   saccadic commands (figure [198]15; silver et al., 2011).
   figure 15: schematic of the listelos model of how an item-order-rank
   spatial working memory in the prefrontal cortex (pfc) stores sequences
   of saccadic eye movement commands that are loaded into working memory
   and performed by interactions of multiple brain regions. see text and
   silver et al. (2011) for details.

   there are also bid56s of cognitive-emotional interactions during
   id23 and motivated attention, such as the motivator
   model, which can focus on valued goals and block learning of irrelevant
   events (grossberg and levine, 1987; [199]kamin blocking) by
   interactions of the object categories in the inferotemporal (it) and
   rhinal (rhin) cortices, the object-value categories in the lateral and
   medial orbitofrontal cortices (orbl, orbm), the value categories in the
   [200]amygdala (amygd) and lateral [201]hypothalamus (lh), and the
   [202]reward expectation filter in several parts of the basal ganglia
   (figure [203]16; dranias et al., 2008),
   figure 16: schematic of the motivator model for how reinforcement
   learning and motivated attention enable animals to choose environmental
   options that can be satisfy current needs. see text and dranias et al.
   (2008) for details.

   the artscan model of view-invariant object learning and [204]visual
   search during unconstrained saccadic eye movements by interactions
   between visual cortices v1, v2, v3a, and v4, prefrontal cortex (pfc),
   posterior parietal cortex (ppc) and lateral intraparietal area (lip),
   posterior and anterior inferotemporal cortex (pit, ait), and superior
   colliculus (sc) (figure [205]17; fazl et al., 2009; grossberg, 2009),
   figure 17: schematic of the artscan model for how view-invariant object
   categories can be learned and recognized while the eyes freely scan a
   scene. see text and fazl et al. (2009) for details.

   the artscene search model of object and spatial contextual cueing of
   visual search for desired objects in a scene by interactions between
   visual cortices v1, v2, and v4, ventral and dorsolateral prefrontal
   cortex (vpfc, dlpfc), perirhinal cortex (prc), parahippocampal cortex
   (phc), anterior and posterior inferotemporal cortex (ita, itp),
   posterior parietal cortex (ppc), and superior colliculus (sc) (figure
   [206]18; huang and grossberg, 2010),
   figure 18: schematic of the artscene model for how object and spatial
   contextual cues can be learned and used to drive an efficient search
   for a desired target in a scene. see text and huang and grossberg
   (2010) for details.

   and the gridplacemap model of entorhinal [207]grid cell learning of
   hexagonal [208]receptive fields, and [209]hippocampal place cell
   learning of (primarily) unimodal receptive fields, in a hierarchy of
   self-organizing maps that obey the same laws, driven by path
   integration signals that are generated as the model navigates realistic
   trajectories (figure [210]19; pilly and grossberg, 2012).
   figure 19: schematic of the gridplacemap model for how a hierarchy of
   self-organizing maps can self-organize entorhinal grid cells and
   hippocampal place cells during spatial navigation. see text and pilly
   and grossberg (2012) for details.

interactions of stm and ltm during neuronal learning

unbiased spatial pattern learning by generalized additive id56s

   various of the architectures above include interactions between stm and
   ltm that allow them to learn from their environments. the fact that
   these architectures "work" is based on a foundation of mathematical
   theorems which demonstrate how stm and ltm laws can be joined to design
   the most general networks capable of learning spatial patterns in an
   unbiased way, even when the cells in the network sample each other's
   activities through recurrent interactions.  these theorems demonstrate
   how unbiased learning can be achieved in networks with an arbitrarily
   large number of neurons, or neuron populations, that interact in
   suitable anatomies under general neurophysiological constraints. once
   such spatial pattern learning is assured, then the results can be
   extended to demonstrate learning of any number of arbitrarily
   complicated space-time patterns, and to build from there in a series of
   steps towards the type of [211]complexity that is found in primate
   brains. some of these steps are reviewed below.

   the theorem in this section shows that two types of anatomy and
   variants thereof are particularly well suited to spatial pattern
   learning: let any finite number of cells \(v_i, i \in i\), send axons
   to any finite number of cells \(v_j, j \in j\). the cases \(i = j\)
   (complete recurrence) and \(i \cap j = \varnothing\) (complete
   non-recurrence) permit perfect pattern learning even if the strengths
   of axon connections from \(i\) to \(j\) are arbitrary positive numbers.
   in these anatomies, axon diameters can be chosen with complete freedom,
   and one can grow axons between cells separated by arbitrary distances
   without concern about their diameters. grossberg (1969a, 1971b)
   summarizes how to extend these results to more general anatomies.

   only certain types of signal transmission between cells can compensate
   for differences in connection strengths, and thereby yield unbiased
   pattern learning (grossberg, 1974, section vi). the simplest
   possibility is to let action potentials propagate along the
   circumference of a cylindrical axon to the axon's synaptic knob
   (hodgkin and huxley, 1952). let the signal disperse throughout the
   cross-sectional area of the synaptic knob as ionic fluxes, and let
   local chemical transmitter production/release in the knob be
   proportional to the local signal density.  finally, let the effect of
   the signal on the postsynaptic cell be proportional to the product of
   local signal density, available transmitter density, and the
   cross-sectional area of the knob (katz, 1969). by contrast, signals
   that propagate throughout the cross-sectional area of the axon, such as
   electrotonic signals, do not produce unbiased learning given arbitrary
   axon connection strengths.

   another constraint is that the time lag for signals to propagate from
   any cell to all the cells that it activates should be (almost) the
   same. how can different axons from a given source cell have the same
   time lag if they have different lengths? this property is achieved if
   signal velocity is proportional to axon length. but signal velocity is
   a local property of signal transmission, whereas axon length is a
   global property of the anatomy. how can this global property be
   controlled by a local one? this is possible if axon length is
   proportional to axon diameter, and signal velocity is proportional to
   axon diameter. the latter is often the case during spike transmission
   down an axon (ruch et al., 1961, p. 73) and the former is qualitatively
   true: longer axons are usually thicker; see cohen and grossberg (1986)
   for developmental laws whereby this can happen. systems with
   self-similar connections of this kind can be converted, through a
   coordinate change, into systems whose connections depend only on the
   source, or sampling, cells. the following generalized additive system
   is of this type. its activities, or stm traces, \(x_i\) and adaptive
   weights, or ltm traces, \(z_{ij}\), obey:

   \[ \tag{35} \frac{d}{dt} x_i = ax_i + \sum_{k \in j} b_k z_{ki} +
   \theta_i c(t) \]

   and

   \[ \tag{36} \frac{d}{dt} z_{ji} = d_j z_{ji} + e_j x_i, \]

   where the number of sampled cells (\(i \in i\)) and sampling cells (\(j
   \in j\)) can be arbitrarily large, and \(a\), \(b_j\), \(d_j\), and
   \(e_j\) can be continuous functionals, possibly highly nonlinear, of
   the entire past of the system. the signal functional \(b_j\) and the
   sampling functional \(e_j\) are non-negative, since they represent
   spike-based signaling terms. the decay functional \(d_j\) also includes
   a wide range of possibilities, including passive decay of associative
   learning, and gated steepest descent learning (figure [212]1 and figure
   [213]3). the terms \(\theta_i\) represent an arbitrary spatial pattern
   (\(\sum_{i \in i} \theta_i = 1\)), and different spatial patterns can
   be presented (\(c(t) > 0\)) as different combinations of sampling cells
   are active. of particular note is the stimulus sampling operation,
   which means that learning only occurs if the sampling functional \(e_j
   > 0\). if both the decay and learning functionals equal zero (\(d_j =
   e_j = 0\)), then neither learning nor forgetting occurs. the stimulus
   sampling property enables arbitrary subsets of sampling cells to learn
   different spatial patterns through time; see [214]serial learning.

   the unbiased spatial pattern learning theorem proves how unbiased
   learning may occur in response to sampling signals, or conditioned
   stimuli (cs), that are correlated with particular spatial patterns, or
   unconditioned stimuli (us). this simple form of associative learning is
   also called classical, or pavlovian, [215]conditioning. the theorems
   prove that, if the system is bounded, and each cs and us are practiced
   together sufficiently often, then perfect pattern learning occurs
   (grossberg, 1969a, 1971b). that is, the relative activities \(x_i = x_i
   \left( \sum_k x_k \right)^{-1}\) and \(z_{ji} = z_{ji} \left( \sum_k
   z_{jk} \right)^{-1}\) approach the training pattern \(\theta_i\)
   without bias as time goes on, no matter how many sampling cells \(j \in
   j\) are simultaneously active, each with its own signaling, sampling,
   and decay functionals, even in a fully recurrent anatomy.

   if the delays from a given cell to all of its target cells are not
   identical, properly designed networks can rapidly resynchronize the
   activities of the target cells using recurrent interactions (grossberg
   and somers, 1991; grossberg and grunewald, 1997; somers and kopell,
   1993, 1995), even in laminar cortical circuits with realistic synaptic
   and axonal delays (yazdanbakhsh and grossberg, 2004).

outstar learning theorem

   the simplest case of the generalized additive model in ([216]35) and
   ([217]36) occurs for the outstar learning theorem (grossberg, 1968b),
   in which the network has a single sampling cell (population) in
   \(j\) and a non-recurrent anatomy (figure [218]1 and figure [219]2).
   given this theorem, the stimulus sampling operation suggests how a
   series of outstars can learn an arbitrary spatiotemporal pattern as a
   series of spatial patterns, ordered in time; see [220]avalanches.

sparse stable category learning theorem

   another version of spatial pattern learning occurs using the dual
   network to the outstar, namely the instar (figure [221]3 and figure
   [222]5). when multiple instars compete with each other via a rcf, they
   form a competitive learning or self-organizing map network (figure
   [223]4; grossberg, 1976a; kohonen, 1984; von der malsburg, 1973).
   grossberg (1976a) proved that, if there are not too many input spatial
   patterns presented sequentially to the network, relative to the number
   of available category learning cells, then category learning occurs
   with adaptive weights that track the input statistics, self-normalize,
   and lead to stable ltm, and the network has [224]bayesian decision
   properties. however, in response to a sequence of sufficiently dense
   non-stationary input patterns, the system can experience catastrophic
   forgetting in which previously learned categories are recoded by
   intervening input patterns (carpenter and grossberg, 1987, grossberg,
   1976a). adaptive resonance theory, or art, was introduced in grossberg
   (1976b) to propose how top-down learned expectations and attentional
   focusing could dynamically stabilize learning in a competitive learning
   or self-organizing map model in response to an arbitrary series of
   input patterns.

adaptive bidirectional associative memory

   kosko (1987, 1988) adapted the cohen-grossberg model and liapunov
   function (cohen and grossberg, 1983), which proved global convergence
   of stm, to define a system that combines stm and ltm and which also
   globally converges to a limit. the main trick was to observe how the
   symmetric connections in the cohen-grossberg equation ([225]32) could
   be used to define symmetric ltm traces interacting reciprocally between
   two processing levels. an additive model bam system is, accordingly,
   defined by:

   \[ \tag{37} \frac{d}{dt} x_i = -x_i + \sum_k f(y_k) z_{ki} + i_i \]

   and

   \[ \tag{38} \frac{d}{dt} y_j = -y_j + \sum_m f(x_m) z_{mj} + j_i. \]

   a shunting model bam can also be analogously defined. one type of
   learning law to which bam methods apply is the passive decay
   associative law that was introduced in grossberg (1967, 1968b, 1968c);
   see figure [226]1 and figure [227]3:

   \[ \tag{39} \frac{d}{dt} z_{ij} = -z_{ij} + f(x_i) f(x_j). \]

   kosko calls the equation in ([228]39) the signal hebb law, although it
   does not obey the property of monotonely increasing learned weights
   that hebb (1949) ascribed to his law. kosko (1988) wrote that: "when
   the bam neurons are activated, the network quickly evolves to a stable
   state of two-pattern reverberation, or resonance". indeed, another
   inspiration for bam was adaptive resonance theory, or art.

adaptive resonance theory

   adaptive resonance theory, or art, is currently the most advanced
   cognitive and bid56 theory of how the brain autonomously learns to
   categorize, recognize, and predict objects and events in a changing
   world. to a remarkable degree, humans can rapidly learn new facts
   without being forced to just as rapidly forget what they already know.
   grossberg (1980) called this problem the stability-plasticity dilemma.
   it is also sometimes called the problem of catastrophic forgetting
   (carpenter, 2001; french, 1999; page, 2000). art proposes a solution of
   this problem by demonstrating how top-down expectations (figure [229]7)
   can learn to focus attention on salient combinations of cues ("critical
   feature patterns"), and characterizing how attention may operate via a
   form of self-normalizing "biased competition" (desimone, 1998). in
   particular, when a good enough match between a bottom-up input pattern
   and a top-down expectation occurs, a synchronous resonant state emerges
   that embodies an attentional focus and is capable of driving fast
   learning of bottom-up recognition categories and top-down expectations;
   hence the name adaptive resonance. for a review of art, see
   (scholarpedia: adaptive resonance theory). for a more comprehensive
   review, see grossberg (2012; [230][7]).

working memory: processing and stm of temporal sequences

   intelligent behavior depends upon the capacity to think about, plan,
   execute, and evaluate sequences of events. whether we learn to
   understand and speak a language, solve a mathematics problem, cook an
   elaborate meal, or merely dial a phone number, multiple events in a
   specific temporal order must somehow be stored temporarily in working
   memory. a working memory (wm) is thus a network that is capable of
   temporarily storing a sequence of events in stm (e.g., baddeley, 1986;
   baddeley and hitch, 1974; bradski et al., 1994; cooper and shallice,
   2000); see [231]working memory). as event sequences are temporarily
   stored, they are grouped, or chunked, through learning into unitized
   plans, or list chunks, and can later be performed at variable rates
   under volitional control, either via imitation or from a previously
   learned plan. how these processes work remains one of the most
   important problems confronting cognitive scientists and
   neuroscientists.
   figure 20: an item-and-order working memory circuit whose outputs are
   triggered by a nonspecific rehearsal wave, and wherein perseverative
   performance of the last item to be performed is prevented by
   self-inhibitory feedback ([232]inhibition of return). see text and
   grossberg and pearson (2008) for details.

relative activity codes temporal order in working memory

   grossberg (1978a, 1978b) introduced an item-and-order wm to explain
   how, as successive items in a list are presented through time, they may
   be stored in wm as a temporally evolving spatial pattern of activity
   across working memory cells (figure [233]20). the "relative activity"
   of different cell populations codes the temporal order in which the
   items will be rehearsed. items with the largest activities are
   rehearsed earliest. hence, the name item-and-order working memory for
   this class of models. this representation represented a radical break
   from the popular model of atkinson and shiffrin (1971), which proposed
   binary activations of a series of linearly ordered "slots" wherein each
   item moves to the next slot as additional items are stored.

working memory design enables stable learning of list chunks

   how is an item-and-order wm in the brain designed? in particular, is a
   wm a bid56 and, if it is, how could evolution discover a bid56 network to
   embody a function as seemingly sophisticated as a wm? grossberg (1978a,
   1978b) noted that wms would be useless unless the item sequences that
   they temporarily stored could be unitized through learning into list
   categories, or chunks, for recognition and recall of familiar lists,
   much as words and motor skills are recognized and recalled.  he
   predicted that all wms are designed to solve the temporal chunking
   problem; namely, wms are designed to be able to learn a novel list
   chunk, under unsupervised learning conditions, from a sequence of
   stored items some of whose subsequences may have already learned to
   code their own list chunks, without forcing the previously learned list
   chunks to be forgotten. for example, a list chunk for the novel word
   myself can be learned even when there may already be strong learned
   representations for the familiar words my, self, and/or elf. why does
   not storage of myself in wm distort the storage of its subwords my,
   self, and elf in a way that leads to catastrophic forgetting of their
   already learned list chunks?

ltm invariance and id172 rule are realized by specialized rcfs

   grossberg (1978a, 1978b) predicted that item-and-order models embody
   two constraints to ensure that stable learning and memory of list
   chunks can occur: the ltm invariance principle and the id172
   rule. the ltm invariance principle is the main postulate to ensure that
   a new superset list chunk, such as myself, can be learned without
   forcing catastrophic forgetting of familiar subset list chunks, such as
   my, self, and elf. as a result, subset list chunks can continue to
   activate their familiar list chunks until they are inhibited by
   contextually more predictive superset list chunks; e.g., until my is
   supplanted by myself through time. mathematically, this postulate
   implies the following property: activities of items in working memory
   preserve their relative activations, or ratios, throughout the time
   that they are stored in working memory.

   the id172 rule assumes that the total activity of the working
   memory network has a maximum that is (approximately) independent of the
   total number of actively stored items. in other words, working memory
   has a limited capacity and activity is redistributed, not just added,
   when new items are stored.

   it was proved in grossberg (1978a, 1978b) that these simple rules
   generate working memories that can support stable learning and
   long-term memory of list chunks. this analysis also showed that
   item-and-order wms could be embodied by specialized recurrent on-center
   off-surround shunting networks, or rcfs, which are ubiquitous in the
   brain, thereby clarifying how wms could arise through evolution. the
   recurrent connections in an rcf help to store inputs in short-term
   memory after the inputs shut off. an rcf obeys the ltm invariance
   principle due to the way that shunting, or multiplicative, interactions
   compute ratios of cell activities across the network; e.g., equation
   ([234]14). the id172 rule follows from the tendency of rcfs to
   normalize total network activity; e.g., equation ([235]24). as
   explained below, an rcf behaves like an item-and-order working memory
   model when it is equipped with a volitionally-activated nonspecific
   rehearsal wave to initiate read-out of stored activity patterns, and
   output-contingent self-inhibitory feedback interneurons to prevent
   perseverative performance of the most activity stored item (figure
   [236]20).

   the prediction that all wms are specialized rcfs that obey the ltm
   invariance principle and id172 rule implies the additional
   prediction that all verbal, spatial, and motor wms have a similar
   network design. for example, the list parse model predicts how such a
   wm can be realized in the deeper layers of ventrolateral prefrontal
   cortex and how list chunks of the stored sequences can be learned in
   the superficial layers of the same cortex (see figure [237]12,
   cognitive working memory). item-and-order wms have also been
   generalized to item-order-rank working memories in which rank, or
   positional, information is also included, thereby permitting the
   temporary storage of repeated items in a list, as in the list abacbde
   (figure [238]15; grossberg and pearson, 2008; silver et al., 2011).

primacy, recency, and bowed activation gradients

   free recall data were one source of inspiration for the discovery of
   item-and-order wms. during free recall, a human tries to recall a
   once-heard list in any order. typically, the beginning and end of the
   list are recalled earlier and with higher id203. if the brain is
   adaptively designed, then why are listed not recalled always in the
   correct temporal order?

   it was mathematically proved that, under constant attentional
   conditions, the pattern of activation that evolves in an item-and-order
   working memory is one of following types (bradski et al., 1992, 1994;
   grossberg, 1978a, 1978b):
    1. primacy gradient. here, the first item to be stored has the largest
       activity and the last item to be stored has the smallest activity.
       a primacy gradient allows the stored items to be rehearsed in their
       presented order.
    2. recency gradient. here the first item is stored with the smallest
       activity and the last item with the largest activity. rehearsal of
       a recency gradient recalls the most recent item first and the first
       item last.
    3. bowed gradient. here, the first and last items to be stored have
       larger activities, and thus are earlier rehearsed, than items in
       the middle of the list.
    4. from primacy to bowed gradient. it was also proved that, as more
       and more items are stored, a primacy gradient becomes a bowed
       pattern whose recency part becomes increasingly dominant. this last
       result predicted that there is a fundamental reason for the
       ubiquitous occurrence of bowed gradients in many types of serially
       ordered behavior: stable learning and memory of list chunks imposes
       an upper bound on the number of items that can be recalled from wm
       in the correct temporal order.
    5. rehearsal. item-and-order models predict that a
       volitionally-controlled, nonspecific rehearsal wave activates all
       the working memory cells to initiate recall, with the most active
       cell generating its output first (figure [239]20). this happens
       whether the wm is activated bottom-up by sequences of individual
       items, or top-down by a list chunk. rehearsing the most active item
       first immediately raises the problem of perseveration; namely, what
       keeps the most active item from being rehearsed over and over
       again, thereby preventing less active items from being rehearsed?
       perseveration can, in fact, occur in some clinical syndromes.
    6. inhibition of return. item-and-order models predict that the
       perseveration problem is solved by output-contingent
       self-inhibition. in other words, each rehearsed cell   s output
       activates a recurrent inhibitory interneuron that shuts off the
       cell, and thereby enables less active items to be rehearsed in the
       order of their relative activation (figure [240]20).
       self-inhibition of each rehearsed item causes [241]inhibition of
       return to that item.
    7. recurrent id172. the activity level of a stored item
       influences the reaction time with which it is rehearsed, with the
       largest activity rehearsed first. as items with the largest
       activities are rehearsed, the remaining items have progressively
       smaller activities. however, as already rehearsed items
       self-inhibit, the remaining stored items are no longer inhibited by
       them via the recurrent off-surround. as a result, the stored
       pattern of cell activations can begin to renormalize itself through
       time as items are rehearsed, and can thereby, at least partially,
       compensate for the slow-down in reaction time that would otherwise
       have occurred due to small activities. this id172 property
       follows from the fact that item-and-order networks are rcfs whose
       cells obey the shunting model.

   these hypotheses have found their way into many variants of the
   item-and-order wm design (e.g., boardman and bullock, 1991; houghton
   and hartley, 1996; page and norris, 1998; rhodes et al., 2004).
   houghton (1990) called item-and-order models competitive queuing
   models.

experimental support

   item-and-order wm properties have received support from many subsequent
   psychological and [242]neurobiological experiments. farrell and
   lewandowsky (2004) wrote:    several competing theories of short-term
   memory can explain serial recall performance at a quantitative level.
   however, most theories to date have not been applied to the
   accompanying pattern of response latencies   these data rule out three of
   the four representational mechanisms. the data support the notion that
   serial order is represented by a primacy gradient that is accompanied
   by suppression of recalled items   .

   averbeck et al. (2002, 2003a, 2003b) reported the first
   neurophysiological evidence in monkeys that a primacy gradient,
   together with inhibition of the most active cell after its command is
   read out, governs the sequential performance of sequential copying
   movements.

   jones et al. (1995) reported similar performance characteristics to
   those of verbal wm for a spatial serial recall task, in which visual
   locations were remembered. agam et al. (2005) reported psychophysical
   evidence of item-and-order wm properties in humans as they perform
   sequential copying movements. silver et al. (2011) used item-and-order
   wms to simulate neurophysiological data about spatial wms. the fact
   that verbal, spatial, and motor sequences, in both humans and monkeys,
   seem to obey the same wm laws provides accumulating evidence for the
   grossberg (1978a, 1978b) prediction that all working memories have a
   similar design to enable stable list chunks to be learned

   agam et al. (2007) reported data consistent with the formation of list
   chunks as movement sequences are practiced, thereby supporting the
   grossberg (1978a) prediction that wm networks are designed to interact
   closely with list chunking networks.

stable chunk learning implies the magical numbers four and seven

   the grossberg (1978a, 1978b) prediction that primacy gradients become
   bows for longer lists provides a conceptually satisfying explanation of
   the well-known immediate memory span of 7 +/- 2 items (miller, 1956).
   because relative activity translates into both relative order and
   id203 of recall (bigger activities can provide more reliable
   recall in a noisy brain), such a model helps to explain why items from
   the beginning and end of a list in free recall may be recalled earlier
   and with larger id203 (murdock, 1962). transposition errors also
   have a natural explanation in such a working memory, since stored items
   with similar activity levels will transpose their relative activities,
   and thus their rehearsal order, more easily than items with very
   different activity levels if noise perturbs these levels through time.
   grossberg (1978a, 1978b) also proved that, if attention varies across
   items, then multi-modal bows, or von restorff (1933) effects, also
   called isolation effects (hunt and lamb, 2001), can be obtained by
   altering the relative sizes of stored activities. von restorff effects
   can also be caused by rate and feature similarity differences across
   items, factors that also influence bowing in the present modeling
   framework. associative and competitive mechanisms can also cause von
   restorff effects during serial verbal learning (see [243]serial
   learning and grossberg, 1969, 1974).

   grossberg (1978a) distinguished between the classical immediate memory
   span (ims) of miller (1956) and the then new concept of the transient
   memory span([244]tms). the tms was predicted to be the result of purely
   stm wm storage and recall, without a significant top-down long-term
   memory (ltm) component.  the tms is, accordingly, the longest list
   length for which a wm can store a primacy gradient. the ims was
   predicted to be the result of combining bottom-up inputs and top-down
   read-out of list chunk learned expectations on the relative activities
   stored in wm, and thus the temporal order that is recalled. grossberg
   (1978a) proved that the tms is smaller than the ims. estimating the ims
   at the famous magical number seven, it was predicted that the tms would
   be around four. cowan (2001) has reviewed experimental data that
   support the existence of a four plus-or-minus one wm capacity limit
   when ltm and grouping influences are minimized, consistent with this
   prediction. indeed, long-term memory (ltm) does bias working memory
   toward more primacy dominance (e.g. knoedler, 1999), and its influence
   can be difficult to limit. also see [245]visual short term memory.

equations for some item-and-order id56s

   an item-and-order rcf with mathematically provable primacy, recency,
   and bowed gradient properties is defined by the family of store
   (sustained temporal order recurrent) models (bradski et al., 1992,
   1994). the store 1 model is defined by the following id56:

   input: let the input \(i_i(t)\) to the \(i^{th}\) neuronal population
   satisfy:

   \[ \tag{40} i_i(t) = 1 \text{ if } \alpha_i - t_i < t < t_i \text{ and
   } 0 \text{ otherwise.} \]

   let the inter-input delays \(\beta_i\) be long enough for the system
   variables \(x_i\) and \(y_i\) to approach equilibrium.

   layer \(f_1\): let the activity \(x_i\) of the \(i^{th}\) population in
   layer \(f_1\) satisfy:

   \[ \tag{41} \frac{d}{dt} x_i = (ai_i + y_i - x_i x)i, \]

   where \(x = \sum_{k=1}^n x_k\) and \(i = \sum_{k=1}^n i_k\).

   layer \(f_2\): let the activity \(y_i\) of the \(i^{th}\) population in
   layer \(f_2\) satisfy:

   \[ \tag{42} \frac{d}{dt} y_i = (x_i - y_i) i^c , \]

   where \(i^c = 1 - i\). initially, \(x_i(0) = y_i(0) = 0\). equation
   ([246]41) is a rcf with a broad off-surround \(x\) that can update its
   stm pattern only when some input is on; that is, when \(i > 0\).
   equations ([247]41) and ([248]42) together define recurrent feedback
   loops whereby \(y_i\) can store a new value of \(x_i\) when all inputs
   shut off; that is, when \(i = 0\). while some inputs are on, previously
   stored values of \(y_i\) influence the stm pattern of \(x_i\)'s that is
   stored, without themselves being changed by them.

serial learning: from command cells to values, decisions, and plans

   when sequences of items from a list are stored in stm, they can trigger
   learning and ltm that enable them to be fluently recalled at a future
   time. such serial learning has been studied experimentally and
   theoretically for a long time in experimental psychology; e.g., dixon
   and horton (1968); hovland (1938a, 1938); hull et al. (1940); jung
   (1968); mcgeogh and irion (1952); osgood (1953), and underwood (1952).
   in fact, the additive model and shunting model were first derived in
   order to explain associative learning of temporal order information in
   serial learning and related paradigms (grossberg, 1969c; grossberg and
   pepe, 1970, 1971).  the step-wise historical development of models for
   learning of temporal order, leading to sophisticated bid56s that can
   respond with increasing flexibility to different types of environmental
   feedback, can be summarized as follows:

avalanches

   the properties of stimulus sampling and of encoding ltm in spatial
   pattern units show how to learn an arbitrary act, such as a piano
   recital, a dance, or a sequence of sensory images, in a minimal way.
   the simplest example, called an avalanche (grossberg, 1969d, 1970b,
   1974), describes a ritualistic encoding wherein performance is
   insensitive to environmental feedback. in this case, only one cell is
   needed to encode the memory of an arbitrary space-time pattern. this
   fact shows that encoding complexity per se is relatively easy. indeed,
   [249]nervous systems with few cells can activate complicated behaviors,
   as is well known in invertebrates. the ritualistic construction is also
   universal, because such a cell can encode any act.
   figure 21: an avalanche circuit learns to perform arbitrary
   spatiotemporal patterns by sampling and learning them using
   sequentially-activated outstars.

   intuitively, an avalanche samples and learns a spatiotemporal pattern
   as a sequence of spatial patterns, much as we experience the continuous
   flow of scenes in a movie from a rapidly presented sequence of still
   pictures (figure [250]21). in the avalanche, a series of outstars
   (black circuits in figure [251]21) are sequentially activated (red
   series of connections in figure [252]20). the outstars sample a
   spatiotemporal pattern (green region in figure [253]21) as they are
   sequentially activated by the sampling pulse (blue pulse in figure
   [254]21). a related concept is the [255]synfire chain (abeles, 1991).

   despite their simplicity, avalanche-type circuits occur in vivo. figure
   [256]22 illustrates that an avalanche-type circuit occurs in the hvc-ra
   network that controls songbird singing (hahnloser et al., 2002). as
   illustrated by the bid56s in figure [257]12, figure [258]13, and figure
   [259]15, in addition to a primary circuit for temporally ordered
   recall, many other circuits, such as those in frontal cortex and the
   basal ganglia, are also needed to ensure flexible performance, at least
   in higher species. even in the songbird, frontal and basal ganglia
   circuits modulate song performance (andalman and fee, 2009).
   figure 22: the songbird hvc-ra circuit
   ([260]http://web.mit.edu/feelab/research.html) includes an avalanche
   anatomy to execute song sounds in the correct order.

command cells and nonspecific arousal

   once a pulse activates the avalanche in figure [261]21, there is no way
   to stop it. if, for example, the avalanche controlled the performance
   of a long dance, and the stage on which the dance was being performed
   began to burn, there would be no way to stop the dance in mid-course to
   escape the flames. sensitivity to environmental feedback is possible
   only if the pulse can be abruptly terminated as it travels along the
   avalanche axon, and replaced by a more appropriate escape behavior.
   grossberg (1969d, 1970b) proposed that the minimal circuit for
   including such sensitivity to environmental feedback would include
   command cells (figure [262]23). command cells are, in fact, found even
   in invertebrates, where they control such stereotyped behaviors as the
   rhythmic beating of crayfish swimmerets (stein, 1971).
   figure 23: command cell modulation of avalanche sequential performance.

   suppose that activation of a command cell is necessary to fire the
   chain of avalanche cells (figure [263]23). the command cell
   simultaneously sends signals to all of the outstars within the
   avalanche, which can now fire only if they receive a signal from the
   previous outstar source cell and from the command cell ("polyvalence").
   thus, the command cell provides nonspecific arousal to the avalanche.
   withdrawal of the command cell arousal can abruptly terminate output
   from the next link in the avalanche. in addition, changing the size of
   the command cell signal can vary the speed of performance, with larger
   command signals causing faster performance speeds. command cells are
   also familiar in the control of other behavioral acts in invertebrates
   (carlson, 1968; dethier, 1968). competition between command cells can
   then determine which ritualistic behavior the system will activate.

   grossberg (1978a) describes a series of increasingly sophisticated
   mechanisms that modulate avalanche performance, leading to ever-greater
   sensitivity to environmental feedback, including recurrent
   interactions. these include issues such as sensitivity to the value of
   actions for achieving desired goals, and the ability to volitionally
   decide what actions to perform and at what speed.

   concerning the former issue: there is a difference between aborting
   your dance on stage if the theater is being consumed by flames, and
   risking your career because a mosquito is hovering above. only more
   important events should be able to shut off the arousal that supports a
   given act. knowing what is important to an organism requires that the
   network can evaluate what events are rewarding and punishing. this
   issue historically led to the cognitive-emotional-motor (cogem) theory
   of id23 in which incentive [264]motivation plays the
   role of a conditionable form of nonspecific arousal, and competition
   between different drive representations that control the incentive
   motivation can determine switching between different valued actions
   (grossberg, 1971a, 1972a, 1972b, 1975; see also armony et al. (1995)
   and [265]computational models of classical conditioning). these ideas
   and their generalizations and extensions led eventually to the
   motivator model (figure [266]16).

   concerning the latter issue: how does an organism decide what act to
   perform? this question involves issues about volitional control of
   behavioral choice by prefrontal-basal ganglia circuits that led
   eventually to circuits such as telos and listelos (figure [267]14 and
   figure [268]15) and related models (e.g., frank and claus, 2006;
   schultz et al., 1997). the ritualistic performance in avalanches hereby
   focused attention in grossberg (1978a) and thereafter on multiple
   issues concerning the global organization of brain mechanisms that are
   sensitive to different kinds of environmental feedback. articulating
   these mechanisms led to the types of high-dimensional bid56s that are
   illustrated in figure [269]11-figure [270]19 and that are familiar in
   advanced brains.

[271]self-organizing avalanches: instar-outstar maps and serial learning of
temporal order

   the outstar source cells and the links between them are pre-wired in an
   avalanche (figure [272]21). these limitations led grossberg (1972,
   1976a, 1976b) to interact through the published literature with von der
   malsburg (1973; see also willshaw and malsburg, 1976)) to introduce
   competitive learning and self-organizing maps (figure [273]4) so that
   the source, or sampling, cells could self-organize as learned category
   cells.
   figure 24: in a self-organizing avalanche, instars (often embedded in
   self-organizing map or adaptive resonance theory circuits) learn to
   activate categories that act as source, or sampling, cells of outstars.
   each outstar is capable of learning an arbitrary spatial pattern. the
   temporal order in which the outstar sampling cells are activated is
   determined by serial associative learning among the sampling cells,
   starting out with adaptive completely recurrent connections.

   after the outstar source cells self-organize, they need to learn the
   spatial patterns that they will perform (figure [274]24), as occurs in
   pre-wired outstars as well. taken together, these learned instars and
   outstars define the instar-outstar associative map learning circuits
   (figure [275]6) that were introduced in grossberg (1976a).

   if the source cells self-organize, then the links between them must
   also be learned. this is the problem of serial learning.  the simplest
   network capable of learning an arbitrary temporal order among its
   constituent cells is a fully-recurrent id56 (figure [276]25) whose
   sampling cells can sequentially learn to embed a temporal order of
   performance in the network, by building on the guarantee of the
   unbiased spatial pattern learning theorem; see equations ([277]35) and
   ([278]36). grossberg (1969c) and grossberg and pepe (1970, 1971)
   provide mathematical analyses of how serial learning can proceed
   through time, and thereby explain classical data properties such as the
   bowed serial position curve. the net result of all these learning
   processes is a self-organizing avalanche that can learn its sampling
   cells, its temporal order links, and its output spatial patterns
   (figure [279]24).
   figure 25: the simplest circuit for serial learning is a completely
   recurrent neural network with adaptive synapses that obey the outstar
   learning theorem.

context-sensitive self-organizing avalanches: what categories control
temporal order?

   once a mechanism is in place for learning categories that act as
   sampling cells, the question arises: what do these categories code? in
   avalanches, each link in the associative chain is sensitive only to the
   previous link. however, in many types of tasks, information about more
   than one previous event or action is needed to choose the correct
   subsequent action. this issue led to the introduction of item-and-order
   working memories (figure [280]20) so that list chunks could be learned
   which are sensitive to whole sequences of previous events. in such a
   network, list chunks are the sampling cells that are linked through
   serial learning into a temporally ordered circuit. list chunks also
   play the role of planning nodes through read-out of their learned
   top-down spatial patterns and serial links. such a network is called a
   context-sensitive self-organizing avalanche, or a context-sensitive
   avalanche, for short.

serial learning

   issues about what previous events control subsequent responses were
   articulated in the classical literature about serial verbal learning.
   new verbal units are continually being synthesized as a result of
   practice, and need not be the obvious units that the experimentalist is
   directly manipulating. indeed, entire sequences of previous events can
   create the context that determines the next response. the same problem
   arises in verbal, spatial, and motor learning. the concept of list
   chunks was introduced to explain such learned sequence-sensitive
   contextual control.

   the severity of such difficulties led the serial learning expert young
   (1968, p. 146) to write: "if an investigator is interested in studying
   verbal learning processes ... he would do well to choose some method
   other than serial learning". underwood (1966, p. 491) went even further
   by writing: "the person who originates a theory that works out to
   almost everyone's satisfaction will be in line for an award in
   psychology equivalent to the nobel prize". the mechanisms summarized in
   this review enable many of the classical serial learning data that
   inspired these statements to be explained and simulated. however, a
   full discussion of these data and their explanations goes beyond the
   scope of the current review. see grossberg (1969c) and grossberg and
   pepe (1970, 1971) for explanations and simulations of classical serial
   learning data, and grossberg (1978a, 1993) for reviews.

references

     * abbott, l.f., varela, k. sen, k., and nelson, s.b. (1997). synaptic
       depression and cortical gain control. science, 275, 220-223.

     * abeles, m. (1991). corticonics: neural circuits of the cerebral
       cortex. cambridge, uk: cambridge university press.

     * ackley, d.h., hinton, g.e., and sejnowski, t.j. (1985). a learning
       [281]algorithm for id82s. cognitive science, 9,
       147-169.

     * agam, y., bullock, d., & sekuler, r. (2005). imitating unfamiliar
       sequences of connected linear motions. journal of neurophysiology,
       94, 2832-2843.

     * agam, y., galperin, h., gold, b. j., and sekuler, r. (2007).
       learning to imitate novel motion sequences. journal of vision,
       doi:10.1167/7.5.1

     * alligood, k.t., saurer, t.d., and yorke, j.a. (1996). chaos: an
       introduction to dynamical systems. new york: springer.

     * andalman, a.s., and fee, m.s. (2009). a basal ganglia-forebrain
       circuit in the songbird biases motor output to avoid vocal errors.
       proceedings of the national academy of sciences, 106, 12518-12523.

     * anderson, j.a. (1968). a memory model utilizing spatial correlation
       functions. kybernetik, 5, 113-119.

     * anderson, j.a., silverstein, j.w., ritz, s.r., and jones, r.s.
       (1977). distinctive features, categorical perception, and
       id203 learning: some applications of a neural model.
       psychological review, 84, 413-451.

     * armony, j.l. servan-schreiber, d., cohen, j.d., and ledoux, j.e.
       (1995). an anatomically constrained neural network model of fear
       conditioning. behavioral neuroscience, 109, 246-257.

     * atkinson, r.c., & shiffrin, r. m. (1971). the control of short term
       memory. scientific american, 225(2), 82-90.

     * averbeck, b. b., chafee, m. v., crowe, d. a., and georgopoulos, a.
       p. (2002). parallel processing of serial movements in prefrontal
       cortex. proceedings of the national academy of sciences, usa, 99,
       20, 13172   13177.

     * averbeck, b. b., crowe, d. a., chafee, m. v., and georgopoulos, a.
       p. (2003a). neural activity in prefrontal cortex during copying
       geometrical shapes. i. single cells encode shape, sequence, and
       metric parameters. experimental brain research, 150, 127-141.

     * averbeck, b. b., crowe, d. a., chafee, m. v., and georgopoulos, a.
       p. (2003b). neural activity in prefrontal cortex during copying
       geometrical shapes. ii. decoding shape segments from neural
       ensembles. experimental brain research,150, 142   153.

     * baddeley, a. (1986). working memory. london: oxford university
       press.

     * baddeley, a. d., and hitch, g. j. (1974). working memory. in g. h.
       bower (ed.), recent advances in learning and motivation (vol. 8),
       new york: academic press.

     * boardman, i., and bullock, d. (1991). a neural network model of
       serial order recall from short-term memory. proceedings of the
       international joint conference on neural networks,2, 879-884.
       piscataway, nj: ieee service center.

     * bradski, g., carpenter, g. a., and grossberg, s. (1994).  store
       working memory networks for storage and recall of arbitrary
       temporal sequences.  biological cybernetics,71, 469-480.

     * brown, j., bullock, d., and grossberg, s. (1999). how the basal
       ganglia use parallel excitatory and inhibitory learning pathways to
       selectively respond to unexpected rewarding cues. journal of
       neuroscience, 19, 10502-10511.

     * brown, j.w., bullock, d., and grossberg, s. (2004). how laminar
       frontal cortex and basal ganglia circuits interact to control
       planned and reactive [282]saccades. neural networks, 17, 471-510.

     * burton, t.a. (1993). averaged neural networks. neural networks, 6,
       677-680.

     * burwick, t. (2006). oscillatory networks: pattern recognition
       without a superposition catastrophe. neural computation, 18,
       356-380.

     * caianiello, e.r. (1961). outline of a theory of thought and
       thinking machines. journal of theoretical biology, 1, 204-235.

     * cao, y. and grossberg, s. (2005). a laminar cortical model of
       stereopsis and 3d surface perception: closure and da vinci
       stereopsis. spatial vision, 18, 515-578.

     * cao, y., and grossberg, s. (2012). stereopsis and 3d surface
       perception by spiking neurons in laminar cortical circuits: a
       method of converting neural rate models into spiking models. neural
       networks, 26, 75-98.

     * carlson, f.o. (ed.). physiological and biochemical aspects of
       nervous integration. englewood cliffs, new jersey: prentice-hall.

     * carpenter, g.a. (2001). neural network models of learning and
       memory: leading questions and an emerging framework. trends in
       cognitive sciences, 5, 114-118.

     * carpenter, g.a. and grossberg, s. (1983). a neural theory of
       circadian rhythms: the gated pacemaker. biological cybernetics, 48,
       35-59.

     * carpenter, g.a., and grossberg, s. (1987). a massively parallel
       architecture for a self-organizing neural pattern recognition
       machine. id161, graphics, and image processing, 37,
       54-115.

     * cisek, p. (2006). integrated neural processes for defining
       potential actions and deciding between them: a computational model.
       the journal of neuroscience, 26, 9761-9770.

     * cohen, m.a. (1988). sustained oscillations in a symmetric
       cooperative-competitive neural network: disproof of a conjecture
       about content addressable memory. neural networks, 1, 217-221.

     * cohen, m.a., and grossberg, s. (1983). absolute stability of global
       pattern formation and parallel memory storage by competitive neural
       networks. ieee transactions on systems, man, and cybernetics,
       13,815-826.

     * cohen, m.a. and grossberg, s. (1986). neural dynamics of speech and
       language coding: developmental programs, perceptual grouping, and
       competition for short-term memory. human neurobiology, 5, 1-22.

     * cohen, m.a. and grossberg, s. (1987). masking fields: a massively
       parallel neural architecture for learning, recognizing, and
       predicting multiple groupings of patterned data. applied optics,
       26, 1866-1891.

     * cooper, r. p., & shallice, t. (2000). contention scheduling and the
       control of routine activities. [283]cognitive neuropsychology, 17,
       297   338.

     * cornsweet, t.n. (1970). visual perception. new york: academic
       press.

     * cowan, n. (2001). the magical number 4 in short-term memory: a
       reconsideration of mental storage capacity. behavioral and brain
       sciences, 24, 87-185.

     * desimone, r. (1998). visual attention mediated by biased
       competition in extrastriate visual cortex. philosophical
       transactions of the royal society of london, 353, 1245   1255.

     * dethier, v.g. (1968). physiology of insect senses. london: methuen.

     * dev, p. (1975). perception of depth surfaces in random-dot
       stereograms: a neural model. international journal of man-machine
       studies, 7, 511-528.

     * douglas, r.j., koch, c., mahowald, m., and martin, k.a., suarez,
       h.h. (1995). recurrent excitation in neocortical circuits. science,
       269, 981-985.

     * dranias, m., grossberg, s., and bullock, d. (2008). dopaminergic
       and non-dopaminergic value systems in conditioning and
       outcome-specific revaluation. brain research, 1238, 239-287.

     * eckhorn, r., reitboeck, h.j., arndt. m., and dicke, p. (1990).
       feature linking via synchronization among distributed assemblies:
       simulations of results from cat visual cortex. neural computation,
       2, 293-307.

     * eigen, m., and schuster, p. (1978). the hypercycle: a principle of
       natural self-organization, b: the abstract hypercycle.
       naturwissensha   ten, 65, 7-41.

     * ellias, s.a. and grossberg, s. (1975). pattern formation, contrast
       control, and oscillations in the short-term memory of shunting
       on-center off-surround networks. biological cybernetics, 20, 69-98.

     * farrell, s., and lewandowsky, s. (2002). an endogenous distributed
       model of ordering in serial recall. psychonomic bulletin & review,
       9, 59   79.

     * fazl, a., grossberg, s., and mingolla, e. (2009). view-invariant
       object category learning, recognition, and search: how spatial and
       object attention are coordinated using surface-based attentional
       shrouds. [284]cognitive psychology, 58, 1-48.

     * francis, g. and grossberg, s. (1996). cortical dynamics of boundary
       segmentation and reset: persistence, afterimages, and residual
       traces . perception, 35, 543-567.

     * francis, g., grossberg, s., mingolla, e. (1994). cortical dynamics
       of feature binding and reset: control of visual persistence. vision
       research, 34, 1089-1104.

     * frank, m.j., and claus, e.d. (2006). anatomy of a decision:
       striato-orbitofrontal interactions in id23,
       decision making, and reversal. psychological review, 113, 300-326.

     * french, r.m. (1999). catastrophic forgetting in connectionist
       networks. trends in cognitive sciences, 3, 128-135.

     * gorchetchnikov, a., versace, m., and hasselmo, m.e. (2005). a model
       of [285]stdp based on spatially and temporally local information:
       derivation and combination with gated decay. neural networks, 16,
       458-466.

     * gaudiano p., and grossberg s. (1991). vector associative maps:
       unsupervised real-time error-based learning and control of movement
       trajectories. neural networks, 4, 147-183.

     * gaudiano, p., and grossberg, s. (1992). adaptive vector integration
       to endpoint: self-organizing neural circuits for control of planned
       movement trajectories. human movement science, 11, 141-155.

     * gilpin, m.e., and ayala, f.j. (1973). global models of growth and
       competition. proceedings of the national academy of sciences, 70,
       3590-3593.

     * gray, c.m., konig, p., engel, a.k., and singer, w. (1989).
       oscillatory responses in cat visual cortex exhibit inter-columnar
       synchronization which reflects global stimulus properties. nature,
       338, 334-337.

     * grossberg, s. (1964). the theory of embedding fields with
       applications to psychology and neurophysiology. rockefeller
       institute for medical research.

     * grossberg, s. (1967). nonlinear difference-differential equations
       in prediction and learning theory. proceedings of the national
       academy of sciences, 58, 1329-1334.

     * grossberg, s. (1968a). a prediction theory for some nonlinear
       functional-differential equations, ii: learning of patterns.
       journal of mathematical analysis and applications, 22, 490-522.

     * grossberg, s. (1968b). some nonlinear networks capable of learning
       a spatial pattern of arbitrary complexity. proceedings of the
       national academy of sciences, 59, 368-372.

     * grossberg, s. (1968c). some physiological and biochemical
       consequences of psychological postulates. proceedings of the
       national academy of sciences, 60, 758-765.

     * grossberg, s. (1969a). on learning and energy-id178 dependence in
       recurrent and nonrecurrent signed networks. journal of statistical
       physics, 1, 319-350.

     * grossberg, s. (1969b). on the production and release of chemical
       transmitters and related topics in cellular control. journal of
       theoretical biology, 22, 325-364.

     * grossberg, s. (1969c). on the serial learning of lists.
       mathematical biosciences, 4, 201-253.

     * grossberg, s. (1969d). some networks that can learn, remember, and
       reproduce any number of complicated space-time patterns, i. journal
       of mathematics and mechanics, 19, 53-91.

     * grossberg, s. (1970a). neural pattern discrimination. journal of
       theoretical biology, 27, 291-337.

     * grossberg, s. (1970b). some networks that can learn, remember, and
       reproduce any number of complicated space-time patterns, ii.
       studies in applied mathematics, 49, 135-166.

     * grossberg, s. (1971a). on the dynamics of [286]operant
       conditioning. journal of theoretical biology, 33, 225-255.

     * grossberg, s. (1971b). parlovian pattern learning by nonlinear
       neural networks. proceedings of the national academy of sciences,
       68, 828-831.

     * grossberg, s. (1972a). a neural theory of punishment and avoidance,
       i: qualitative theory. mathematical biosciences, 15, 39-67.

     * grossberg, s. (1972b). a neural theory of punishment and avoidance,
       ii: quantitative theory. mathematical biosciences, 15, 253-285.

     * grossberg, s. (1973). contour enhancement, short-term memory, and
       constancies in reverberating neural networks. studies in applied
       mathematics, 52, 213-257.

     * grossberg, s. (1974). classical and instrumental learning by neural
       networks. in rosen, r., and snell, f. (eds.), progress in
       theoretical biology. new york: academic press, pp. 51-141.

     * grossberg, s. (1975). a neural model of attention,
       [287]reinforcement, and discrimination learning. international
       review of neurobiology, 18, 263-327.

     * grossberg, s. (1976a). adaptive pattern classification and
       universal recoding, i: parallel development and coding of neural
       feature detectors. biological cybernetics, 23, 121-134.

     * grossberg, s. (1976b). adaptive pattern classification and
       universal recoding, ii: feedback, expectation, olfaction, and
       illusions. biological cybernetics, 23, 187-202.

     * grossberg, s. (1978a). a theory of human memory: self-organization
       and performance of sensory-motor codes, maps, and plans. in r.
       rosen and f. snell (eds.), progress in theoretical biology, volume
       5. new york: academic press, pp. 233-374.

     * grossberg, s. (1978b). behavioral contrast in short-term memory:
       serial binary memory models or parallel continuous memory models?
       journal of mathematical psychology, 3, 199-219.

     * grossberg, s. (1978c). competition, decision, and consensus.
       journal of mathematical analysis and applications, 66, 470-493.

     * grossberg, s. (1978d). decisions, patterns, and oscillations in
       nonlinear competitive systems with applications to volterra-lotka
       systems. journal of theoretical biology, 73, 101-130.

     * grossberg, s. (1980a). how does a brain build a cognitive code?
       psychological review, 87, 1-51.

     * grossberg, s. (1980b). biological competition: decision rules,
       pattern formation, and oscillations. proceedings of the national
       academy of sciences, 77, 2338-2342.

     * grossberg, s. (1982). associative and competitive principles of
       learning and development: the temporal [288]unfolding and stability
       of stm and ltm patterns. in amari, s.i.,  and arbib, m. (eds.),
       competition and cooperation in neural networks. new york:
       springer-verlag, 1982.

     * grossberg, s. (1983). the quantized geometry of visual space: the
       coherent computation of depth, form, and lightness. the behavioral
       and brain sciences, 6, 625-692.

     * grossberg, s. (1984a). some normal and abnormal behavioral
       syndromes due to transmitter gating of opponent processes.
       biological psychiatry, 19, 1075-1118.

     * grossberg, s. (1984b). some psychophysiological and pharmacological
       correlates of a developmental, cognitive, and motivational theory.
       in karrer r, cohen j, tueting p, editors. brain and information:
       event related potentials, new york: new york academy of sciences,
       pp.  58-142.

     * grossberg, s. (1988) nonlinear neural networks: principles,
       mechanisms, and architectures. neural networks, 1, 17-61.

     * grossberg, s. (1993). self-organizing neural networks for stable
       control of autonomous behavior in a changing world. in j.g. taylor
       (ed.), mathematical approaches to neural networks. amsterdam :
       elsevier science publishers,  pp.139-197.

     * grossberg, s. (1999). how does the cerebral cortex work? learning,
       attention and grouping by the laminar circuits of visual cortex.
       spatial vision, 12, 163-186.

     * grossberg, s. (2003). how does the cerebral cortex work?
       development, learning, attention, and 3d vision by laminar circuits
       of visual cortex. behavioral and cognitive neuroscience reviews, 2,
       47-76.

     * grossberg, s. (2009). cortical and subcortical predictive dynamics
       and learning during perception, cognition, [289]emotion and action.
       philosophical transactions of the royal society of london b
       biological sciences, 364, 1223-1234.

     * grossberg, s. (2012). adaptive resonance theory: how a brain learns
       to consciously attend, learn, and recognize a changing world.
       neural networks, 37, 1-47.

     * grossberg, s. and grunewald, a. (1997). cortical synchronization
       and perceptual framing. journal of cognitive neuroscience, 9,
       117-132.

     * grossberg, s. and gutowski, w.e. (1987). neural dynamics of
       decision making under risk: affective balance and
       cognitive-emotional interactions. psychological review, 94,
       300-318.

     * grossberg, s., and kazerounian, s. (2011). laminar cortical
       dynamics of conscious speech perception: a neural model of phonemic
       restoration using subsequent context in noise. journal of the
       acoustical society of america, 130, 440-460.

     * grossberg, s. and levine, d.s. (1987). neural dynamics of
       attentionally modulated pavlovian conditioning: blocking,
       inter-stimulus interval, and secondary reinforcement. applied
       optics, 26, 5015-5030.

     * grossberg, s. and mingolla, e. (1985). neural dynamics of
       perceptual grouping: textures, boundaries, and emergent
       segmentations. perception and psychophysics, 38, 141-171.

     * grossberg, s., and pearson, l. (2008). laminar cortical dynamics of
       cognitive and motor working memory, [290]sequence learning and
       performance: toward a unified theory of how the cerebral cortex
       works. psychological review, 115, 677-732.

     * grossberg, s. and pepe, j. (1970). [291]schizophrenia: possible
       dependence of associational span, bowing, and primacy vs. recency
       on spiking threshold. behavioral science, 15, 359-362.

     * grossberg, s. and pepe, j. (1971). spiking threshold and
       overarousal effects in serial learning. journal of statistical
       physics, 3, 95-125.

     * grossberg, s. and pilly, p. (2008). temporal dynamics of
       decision-making during motion perception in the visual cortex.
       vision research, 48, 1345-1373.

     * grossberg, s. and raizada, r. (2000). contrast-sensitive perceptual
       grouping and object-based attention in the laminar circuits of
       [292]primary visual cortex. vision research, 40', 1413-1432.

     * grossberg, s., and seitz, a. (2003). laminar development of
       receptive fields, maps, and columns in visual cortex: the
       coordinating role of the subplate. cerebral cortex,13, 852-863.

     * grossberg, s. and somers, d. (1991). synchronized oscillations
       during cooperative feature linking in a cortical model of visual
       perception. neural networks, 4, 453-466.

     * grossberg, s. and todorovic, d. (1988). neural dynamics of 1-d and
       2-d brightness perception: a unified model of classical and recent
       phenomena. perception and psychophysics, 43, 241-277.

     * grossberg, s., and versace, m. (2008). spikes, synchrony, and
       attentive learning by laminar thalamocortical circuits. brain
       research, 1218, 278-312.

     * guo, s., huang, l., and wang, l. (2004). linear stability and
       [293]hopf bifurcation in a two-neuron network with three delays.
       international journal of [294]bifurcation and chaos, 14, 2799-2810.

     * hahnloser, r.h.r., kozhevnikov, a.a., and fee, m.s. (2002). an
       ultra-sparse code underlies the generation of neural sequences in a
       songbird. nature, 419, 65-70.

     * hartline, h.k., and ratliff, f. (1957). inhibitory interaction of
       receptor units in the eye of limulus. journal of general
       physiology, 40, 357-376.

     * hebb, d.o. (1949). the organization of behavior. new york: wiley.

     * hecht-nielsen, r. (1987). counterpropagation networks. applied
       optics, 26, 4979-4983.

     * heeger, d.j. (1992). id172 of cell responses in cat visual
       cortex. visual neuroscience, 9, 181-197.

     * hodgkin, a.l. (1964). the conduction of the nervous impulse.
       springfield: c.c. thomas.

     * hodgkin, a.l., and huxley, a.f. (1952). a quantitative description
       of membrane current and its application to conduction and
       excitation in nerve. journal of physiology, 117, 500-544.

     * hopfield, j.j. (1984). neurons with graded response have collective
       computational properties like those of two-state neurons.
       proceedings of the national academy of sciences, 81, 3088-3092.

     * hoppensteadt, f., and izhikevich, e. (2001). canonical neural
       models. (2001). in arbib, m.a. (ed.). brain theory and neural
       networks. second edition. cambridge, ma: mit press, 1-7.
       [295]http://www.izhikevich.org/publications/arbib.pdf.

     * houghton, g. (1990). the problem of serial order: a neural network
       model of sequence learning and recall. in r. dale, c. mellish, and
       m. zock (eds.). current research in national language generation,
       pp. 287-319. london: academic press.

     * houghton, g., and hartley, t. (1996).  parallel models of serial
       behavior: lashley revisited.  psyche,2, 25.

     * hovland, c.i. (1938a). experimental studies in rote-learning
       theory. i. reminiscence following learning by massed and by
       distributed practice. journal of experimental psychology, 22,
       201-224.

     * hovland, c.i. (1938b). experimental studies in rote-learning
       theory. iii. distribution of practice with varying speeds of
       syllable presentation. journal of experimental psychology, 23,
       172-190.

     * huang, t.-r., and grossberg, s. (2010). cortical dynamics of
       contextually cued attentive visual learning and search: spatial and
       object evidence accumulation. psychological review, 117, 1080-1112.

     * hull, c.l., hovland, c.i., ross, r.t., hall, m., perkins, d.t., and
       fitch, f.b. (1940). mathematico-deductive theory of rote learning.
       new haven: yale university press.

     * hunt, r. r., and lamb, c. a. (2001). what causes the isolation
       effect? journal of experimental psychology: learning, memory and
       cognition, 27, 6, 1359-66.

     * jones, d., farrand, p., stuart, g., and morris, n. (1995). the
       functional equivalence of verbal and spatial memory in serial
       short-term memory, journal of experimental psychology: learning
       memory and cognition, 21, 1008-1018.

     * katz, b. (1969). the release of neural transmitter substances.
       liverpool: liverpool university press.

     * knoedler, a. j., hellwig, k. a., and  neath, i. (1999). the shift
       from recency to primacy with increasing delay. journal of
       experimental psychology: learning, memory and cognition, 25,
       474-487.

     * koch, c., and ullman, s. (1985). shifts in selective visual
       attention: towards the underlying neural circuitry. human
       neurobiology, 4, 219-227.

     * kohonen, t. (1971). a class of randomly organized associative
       memories. acta polytechnica scandinavica, e1. 25.

     * kohonen, t. (1977). associative memory: a system-theoretical
       approach. new york: springer-verlag.

     * kohonen, t. (1984). self-organization and associative memory. new
       york: springer-verlag.

     * kosko, b. (1987). adaptive bidirectional associative memories.
       applied optics, 26, 4947-4960.

     * kosko, b. (1988). bidirectional associative memories. ieee
       transactions on systems, man, and cybernetics, smc-18, 49-60.

     * kuffler, s. w. (1953). discharge patterns and functional
       organization of mammalian retina. journal of neurophysiology,
       16,37-68.

     * lotka, a.j. (1956). elements of [296]mathematical biology. new
       york: dover.

     * may, r.m., and leonard, w.j. (1975). nonlinear aspects of
       competition between three species. siam journal on applied
       mathematics, 29, 243-253.

     * mcculloch, w.s., and pitts, w. (1943). a logical calculus of the
       ideas immanent in nervous activity. bulletin of mathematical
       biophysics, 5, 115-133.

     * mclaughlin, d., shapley, r., shelley, m., and wielaard, d.j.
       (2000). a neuronal network model of macaque primary visual cortex
       (v1): orientation selectivity and dynamics in the input layer 4ca.
       proceedings of the national academy of sciences, 97, 8087-8092.

     * menon, a., mehrotra, k., mohan, c.k., and ranka, s. (1996).
       characterization of a class of sigmoid functions with applications
       to neural networks. neural networks, 9, 819-835.

     * miller, g. a. (1956). the magical number seven, plus or minus two:
       some limits on our capacity for processing information.
       psychological review, 63, 2, 81-97.

     * murdock, b. b. (1962). the serial position effect of free recall.
       journal of experimental psychology, 64, 482-488.

     * ogmen, h., and gagn  , s. (1990). neural network architectures for
       motion perception and elementary motion detection in the fly visual
       system. neural networks, 3, 487-505.

     * o'reilly, r.c., and munakata, y. (2000). computational explorations
       in cognitive neuroscience: understanding the mind by simulating the
       brain. cambridge, ma: mit press.

     * osgood, c.e. (1953). method and theory in experimental psychology.
       oxford university press: london.

     * page, m. (2000). connectionist modeling in psychology: a localist
       manifesto. behavioral and brain sciences, 23, 443-512.

     * page, m. p. a., and norris, d. (1998). the primacy model: a new
       model of immediate serial recall. psychological review, 105, 4,
       761-781.

     * palma, j., grossberg, s., and versace, m. (2012). persistence and
       storage of activity patterns in spiking recurrent cortical
       networks: modulation of sigmoid signals by after-hyperpolarization
       currents and acetylcholine. frontiers in [297]computational
       neuroscience, 6:42. doi:  [298]10.3389/fncom.2012.00042

     * pilly, p.k. and grossberg, s. (2012). how do spatial learning and
       memory occur in the brain? coordinated learning of entorhinal grid
       cells and hippocampal place cells. journal of cognitive
       neuroscience, 5, 1031-1054.

     * raizada, r. and grossberg, s. (2001). context-sensitive bindings by
       the laminar circuits of v1 and v2: a unified model of perceptual
       grouping, attention, and orientation contrast. visual cognition, 8,
       431-466.

     * ratliff, f., hartline, h.k., and miller, w.h. (1963). spatial and
       temporal aspects of retinal inhibitory interactions. journal of the
       optical society of america, 53, 110-120.

     * rhodes, b. j., bullock, d., verwey, w. b., averbeck, b. b., and
       page, m. p. a. (2004). learning and production of movement
       sequences: behavioral, neurophysiological, and modeling
       perspectives. human movement science, 23, 683-730.

     * rosenblatt, f. (1962). principles of neurodynamics. washington,
       d.c.: spartan books.

     * ruch, t.c., patton, h.d., woodbury, j.w., and towe, a.l.
       neurophysiology. philadelphia: w.b. saunders.

     * schultz, w., dayan, p., and montague, p.r. (1997). a neural
       substrate of prediction and reward. science, 275, 1593-1599.

     * silver, m.r., grossberg, s., bullock, d., histed, m.h., and miller,
       e.k. (2011). a neural model of sequential movement planning and
       control of eye movements: item-order-rank working memory and
       saccade selection by the supplementary eye fields. neural networks,
       26, 29-58.

     * somers, d., and kopell, n. (1993). rapid synchronization through
       fast threshold modulation. biological cybernetics, 68, 393-407.

     * somers, d., and kopell, n. (1995). waves and synchrony in networks
       of oscillators of relaxation and non-relaxation type. physica d:
       nonlinear phenomena, 89, 169-183.

     * stein, p.s.g. (1971). intersegmental coordination of swimmeret
       motoneuron activity in crayfish. journal of neurophysiology, 34,
       310-318.

     * tsodyks, m.v., and markram, h. (1997). the neural code between
       neocortical [299]pyramidal neurons depends on neurotransmitter
       release id203. proceedings of the national academy of
       sciences, 94, 719-723.

     * underwood, b.j. (1952). studies of distributed practice: vii.
       learning and retention of serial nonsense lists as a function of
       intralist similarity. journal of experimental psychology, 44,
       80-87.

     * usher, m., and mcclelland, j.l. (2001). the time course of
       perceptual choice: the leaky, competing accumulator model.
       psychological review, 108, 550-592.

     * von der malsburg, c. (1973). self-organization of orientation
       sensitive cells in the striate cortex. biological cybernetics, 14,
       85-100.

     * von restorff, h. (1933).   ber die wirkung von bereichsbildungen im
       spurenfeld (the effects of field formation in the trace field).
       psychologie forschung, 18, 299-34.

     * wang, x.-j. (2008). decision makes in recurrent neuronal circuits.
       neuron review, 60, 215-234.

     * werblin, f.s. (1971). adaptation in a vertebrate retina:
       [300]intracellular recordings in necturus. journal of
       neurophysiology, 34, 228-241.

     * widrow, b. (1962). generalization and information storage in
       networks of adaline neurons. in yovits, m.c., jacobi, g.t., and
       goldstein, g.d. (eds.). self-organizing systems. washington, d.c.:
       spartan books.

     * willshaw, d.j., and malsburg, c. von der (1976). how patterned
       neural connections can be set up by self-organization. proceedings
       of the royal society of london, 194, 431-445.

     * wilson, h.r., and cowan, j.d. (1972). excitatory and inhibitory
       interactions in localized populations of model neurons. biophysical
       journal, 12, 1-24.

     * wu, j., and zou, x. (1995). patterns of sustained oscillations in
       neural networks with delayed interactions. applied mathematics and
       computation, 73, 55-75.

     * yazdanbakhsh, a., and grossberg, s. (2004). fast synchronization of
       perceptual grouping in laminar visual cortical circuits. neural
       networks, 17, 707-718.

   sponsored by: [301]eugene m. izhikevich, editor-in-chief of
   scholarpedia, the peer-reviewed open-access encyclopedia
   [302]reviewed by: [303]dr. baingio pinna, dipartimento di scienze dei
   linguaggi, universit   di sassari, italy
   [304]reviewed by: [305]dr. birgitta dresp-langley, universit  
   strasbourg umr 7357 icube cnrs
   accepted on: [306]2013-02-21 13:38:42 gmt
   retrieved from
   "[307]http://www.scholarpedia.org/w/index.php?title=recurrent_neural_ne
   tworks&oldid=138057"
   [308]categories:
     * [309]computational intelligence
     * [310]recurrent neural networks
     * [311]neural networks

personal tools

     * [312]log in / create account

namespaces

     * [313]page
     * [314]discussion

variants

views

     * [315]read
     * [316]view source
     * [317]view history

actions

search

   ____________________ (button) search

navigation

     * [318]main page
     * [319]about
     * [320]propose a new article
     * [321]instructions for authors
     * [322]random article
     * [323]faqs
     * [324]help
     * [325]blog

focal areas

     * [326]astrophysics
     * [327]celestial mechanics
     * [328]computational neuroscience
     * [329]computational intelligence
     * [330]dynamical systems
     * [331]physics
     * [332]touch
     * [333]more topics

activity

     * [334]recently published articles
     * [335]recently sponsored articles
     * [336]recent changes
     * [337]all articles
     * [338]list all curators
     * [339]list all users
     * [340]scholarpedia journal

tools

     * [341]what links here
     * [342]related changes
     * [343]special pages
     * [344]printable version
     * [345]permanent link

     * [346][twitter.png?303]
     * [347][gplus-16.png]
     * [348][facebook.png?303]
     * [349][linkedin.png?303]

     * [350]powered by mediawiki [351]powered by mathjax [352]creative
       commons license

     * this page was last modified on 13 december 2013, at 21:27.
     * this page has been accessed 100,370 times.
     * "recurrent neural networks" by [353]stephen grossberg is licensed
       under a [354]creative commons attribution-noncommercial-sharealike
       3.0 unported license. permissions beyond the scope of this license
       are described in the [355]terms of use

     * [356]privacy policy
     * [357]about scholarpedia
     * [358]disclaimers

references

   visible links
   1. http://www.scholarpedia.org/w/opensearch_desc.php
   2. http://www.scholarpedia.org/w/index.php?title=special:recentchanges&feed=atom
   3. http://dx.doi.org/10.4249/scholarpedia.1888
   4. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&action=cite&rev=138057
   5. http://www.scholarpedia.org/article/recurrent_neural_networks#mw-head
   6. http://www.scholarpedia.org/article/recurrent_neural_networks#p-search
   7. http://www.scholarpedia.org/article/user:stephen_grossberg
   8. http://www.scholarpedia.org/article/user:trevor_bekolay
   9. http://www.scholarpedia.org/article/user:nick_orbeck
  10. http://www.scholarpedia.org/article/user:birgitta_dresp-langley
  11. http://www.scholarpedia.org/article/user:baingio_pinna
  12. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
  13. http://www.scholarpedia.org/article/user:stephen_grossberg
  14. http://www.scholarpedia.org/article/neuron
  15. http://en.wikipedia.org/wiki/recurrent_neural_network
  16. http://www.intechopen.com/books/recurrent-neural-networks-for-temporal-data-processing
  17. http://minds.jacobs-university.de/sites/default/files/uploads/papers/esntutorialrev.pdf
  18. http://www.idsia.ch/~juergen/id56.html
  19. http://www.scholarpedia.org/article/brain
  20. http://www.scholarpedia.org/article/dynamical_systems
  21. http://www.scholarpedia.org/article/memory
  22. http://www.scholarpedia.org/article/periodic_orbit
  23. http://www.scholarpedia.org/article/synchronization
  24. http://www.scholarpedia.org/article/traveling_waves
  25. http://www.scholarpedia.org/article/working_memory
  26. http://www.scholarpedia.org/article/attention
  27. http://www.scholarpedia.org/article/recurrent_neural_networks#types_of_recurrent_neural_networks
  28. http://www.scholarpedia.org/article/recurrent_neural_networks#binary
  29. http://www.scholarpedia.org/article/recurrent_neural_networks#linear
  30. http://www.scholarpedia.org/article/recurrent_neural_networks#continuous-nonlinear
  31. http://www.scholarpedia.org/article/recurrent_neural_networks#additive_stm_equation
  32. http://www.scholarpedia.org/article/recurrent_neural_networks#shunting_stm_equation
  33. http://www.scholarpedia.org/article/recurrent_neural_networks#generalized_stm_equation
  34. http://www.scholarpedia.org/article/recurrent_neural_networks#mtm:_habituative_transmitter_gates_and_depressing_synapses
  35. http://www.scholarpedia.org/article/recurrent_neural_networks#ltm:_gated_steepest_descent_learning:_not_hebbian_learning
  36. http://www.scholarpedia.org/article/recurrent_neural_networks#processing_and_stm_of_spatial_patterns
  37. http://www.scholarpedia.org/article/recurrent_neural_networks#transformation_and_short-term_storage_of_distributed_input_patterns_by_neural_networks
  38. http://www.scholarpedia.org/article/recurrent_neural_networks#the_noise-saturation_dilemma
  39. http://www.scholarpedia.org/article/recurrent_neural_networks#a_thought_experiment_to_solve_the_noise-saturation_dilemma
  40. http://www.scholarpedia.org/article/recurrent_neural_networks#automatic_gain_control_by_the_off_surround_prevents_saturation
  41. http://www.scholarpedia.org/article/recurrent_neural_networks#contrast_id172_and_pattern_processing_by_real-time_probabilities.
  42. http://www.scholarpedia.org/article/recurrent_neural_networks#weber_law_and_shift_property
  43. http://www.scholarpedia.org/article/recurrent_neural_networks#physiological_interpretation_of_shunting_dynamics:_the_membrane_equation_of_neurophysiology
  44. http://www.scholarpedia.org/article/recurrent_neural_networks#recurrent_competitive_fields
  45. http://www.scholarpedia.org/article/recurrent_neural_networks#winner-take-all.2c_contrast_enhancement.2c_id172.2c_and_quenching_threshold
  46. http://www.scholarpedia.org/article/recurrent_neural_networks#shunting_dynamics_in_cortical_models
  47. http://www.scholarpedia.org/article/recurrent_neural_networks#decision-making_in_competitive_systems:_liapunov_methods
  48. http://www.scholarpedia.org/article/recurrent_neural_networks#competition.2c_decision.2c_and_consensus
  49. http://www.scholarpedia.org/article/recurrent_neural_networks#adaptation_level_systems:_globally-consistent_decision-making
  50. http://www.scholarpedia.org/article/recurrent_neural_networks#cohen-grossberg_model.2c_liapunov_function.2c_and_theorem
  51. http://www.scholarpedia.org/article/recurrent_neural_networks#symmetry_does_not_imply_convergence:_synchronized_oscillations
  52. http://www.scholarpedia.org/article/recurrent_neural_networks#unifying_horizontal.2c_bottom-up.2c_and_top-down_stm_and_ltm_interactions
  53. http://www.scholarpedia.org/article/recurrent_neural_networks#interactions_of_stm_and_ltm_during_neuronal_learning
  54. http://www.scholarpedia.org/article/recurrent_neural_networks#unbiased_spatial_pattern_learning_by_generalized_additive_id56s
  55. http://www.scholarpedia.org/article/recurrent_neural_networks#outstar_learning_theorem
  56. http://www.scholarpedia.org/article/recurrent_neural_networks#sparse_stable_category_learning_theorem
  57. http://www.scholarpedia.org/article/recurrent_neural_networks#adaptive_bidirectional_associative_memory
  58. http://www.scholarpedia.org/article/recurrent_neural_networks#adaptive_resonance_theory
  59. http://www.scholarpedia.org/article/recurrent_neural_networks#working_memory:_processing_and_stm_of_temporal_sequences
  60. http://www.scholarpedia.org/article/recurrent_neural_networks#relative_activity_codes_temporal_order_in_working_memory
  61. http://www.scholarpedia.org/article/recurrent_neural_networks#working_memory_design_enables_stable_learning_of_list_chunks
  62. http://www.scholarpedia.org/article/recurrent_neural_networks#ltm_invariance_and_id172_rule_are_realized_by_specialized_rcfs
  63. http://www.scholarpedia.org/article/recurrent_neural_networks#primacy.2c_recency.2c_and_bowed_activation_gradients
  64. http://www.scholarpedia.org/article/recurrent_neural_networks#experimental_support
  65. http://www.scholarpedia.org/article/recurrent_neural_networks#stable_chunk_learning_implies_the_magical_numbers_four_and_seven
  66. http://www.scholarpedia.org/article/recurrent_neural_networks#equations_for_some_item-and-order_id56s
  67. http://www.scholarpedia.org/article/recurrent_neural_networks#serial_learning:_from_command_cells_to_values.2c_decisions.2c_and_plans
  68. http://www.scholarpedia.org/article/recurrent_neural_networks#avalanches
  69. http://www.scholarpedia.org/article/recurrent_neural_networks#command_cells_and_nonspecific_arousal
  70. http://www.scholarpedia.org/article/recurrent_neural_networks#self-organizing_avalanches:_instar-outstar_maps_and_serial_learning_of_temporal_order
  71. http://www.scholarpedia.org/article/recurrent_neural_networks#context-sensitive_self-organizing_avalanches:_what_categories_control_temporal_order.3f
  72. http://www.scholarpedia.org/article/recurrent_neural_networks#serial_learning
  73. http://www.scholarpedia.org/article/recurrent_neural_networks#references
  74. http://www.scholarpedia.org/article/logic
  75. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-2
  76. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-3
  77. http://www.scholarpedia.org/article/hartline-ratliff_model
  78. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-4
  79. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-6
  80. http://www.scholarpedia.org/article/vision
  81. http://www.scholarpedia.org/article/reinforcement_learning
  82. http://www.scholarpedia.org/article/language
  83. http://www.scholarpedia.org/article/hopfield_model
  84. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-7
  85. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-7
  86. http://www.scholarpedia.org/article/neural_fields
  87. http://www.scholarpedia.org/article/recurrent_neural_networks#the_noise-saturation_dilemma
  88. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
  89. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
  90. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
  91. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-6
  92. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
  93. http://www.scholarpedia.org/article/neural_inhibition
  94. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
  95. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
  96. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
  97. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
  98. http://www.scholarpedia.org/article/recurrent_neural_networks#cohen-grossberg_model.2c_liapunov_function.2c_and_theorem
  99. http://www.scholarpedia.org/article/bursting
 100. http://www.scholarpedia.org/article/resonance
 101. http://www.scholarpedia.org/article/adaptive_resonance_theory
 102. http://cns.bu.edu/~steve/art.pdf
 103. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
 104. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-10
 105. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-10
 106. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-1
 107. http://www.scholarpedia.org/article/donald_olding_hebb
 108. http://www.scholarpedia.org/article/recurrent_neural_networks#processing_and_stm_of_spatial_patterns
 109. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar.png
 110. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar2.png
 111. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar.png
 112. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-instar.png
 113. http://www.scholarpedia.org/article/kohonen_network
 114. http://cns-web.bu.edu/profiles/grossberg/learning.html
 115. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-competitive-som.png
 116. http://www.scholarpedia.org/article/recurrent_neural_networks#processing_and_stm_of_spatial_patterns
 117. http://www.scholarpedia.org/article/recurrent_neural_networks#sparse_stable_category_learning_theorem
 118. http://www.scholarpedia.org/article/kohonen_network
 119. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-inout-duality.png
 120. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-inout-assoc-map.png
 121. http://www.scholarpedia.org/article/recurrent_neural_networks#sparse_stable_category_learning_theorem
 122. http://www.scholarpedia.org/article/recurrent_neural_networks#outstar_learning_theorem
 123. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-art-bottomup-topdown.png
 124. http://www.scholarpedia.org/article/recurrent_neural_networks#adaptive_resonance_theory
 125. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-11
 126. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-12
 127. http://www.scholarpedia.org/article/retina
 128. http://www.scholarpedia.org/article/equilibrium
 129. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 130. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
 131. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 132. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 133. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 134. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-15
 135. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-16
 136. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-14
 137. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-14
 138. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-14
 139. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-18
 140. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 141. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-20
 142. http://www.scholarpedia.org/article/symmetry_breaking
 143. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-20
 144. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-13
 145. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-input-transforms.png
 146. http://www.scholarpedia.org/article/morphogenesis
 147. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 148. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 149. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-input-transforms.png
 150. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-sigmoid.png
 151. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-22
 152. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 153. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-23
 154. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-22
 155. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-23
 156. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-equilibruim.png
 157. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-25
 158. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-25
 159. http://www.scholarpedia.org/article/stability
 160. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-sigmoid.png
 161. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-input-transforms.png
 162. http://www.scholarpedia.org/article/interneurons
 163. http://www.scholarpedia.org/article/recurrent_neural_networks#unifying_horizontal.2c_bottom-up.2c_and_top-down_stm_and_ltm_interactions
 164. http://www.scholarpedia.org/article/eye_movements
 165. http://www.scholarpedia.org/article/adaptive_resonance_theory
 166. http://www.scholarpedia.org/article/scholarpedia
 167. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 168. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-27
 169. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 170. http://www.scholarpedia.org/article/decision_trees
 171. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 172. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 173. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-21
 174. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-22
 175. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-30
 176. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-28
 177. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-32
 178. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-6
 179. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
 180. http://www.scholarpedia.org/article/boltzmann_machine
 181. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-33
 182. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-34
 183. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-33
 184. http://www.scholarpedia.org/article/hopfield_network
 185. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-8
 186. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-10
 187. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-9
 188. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-laminar-circuits.png
 189. http://www.scholarpedia.org/article/basal_ganglia
 190. http://www.scholarpedia.org/article/motor_cortex:_representations
 191. http://www.scholarpedia.org/article/cerebellum
 192. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-laminar-circuits2.png
 193. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-artword.png
 194. http://www.scholarpedia.org/article/consciousness
 195. http://www.scholarpedia.org/article/frontal_eye_field
 196. http://www.scholarpedia.org/article/parietal_cortex
 197. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-telos.png
 198. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-listelos.png
 199. http://www.scholarpedia.org/article/kamin_blocking
 200. http://www.scholarpedia.org/article/amygdala
 201. http://www.scholarpedia.org/article/hypothalamus
 202. http://www.scholarpedia.org/article/reward
 203. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-motivator.png
 204. http://www.scholarpedia.org/article/visual_search
 205. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-artscan.png
 206. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-artscene.png
 207. http://www.scholarpedia.org/article/grid_cells
 208. http://www.scholarpedia.org/article/receptive_field
 209. http://www.scholarpedia.org/article/hippocampus
 210. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-gridplacemap.png
 211. http://www.scholarpedia.org/article/complexity
 212. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar.png
 213. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-instar.png
 214. http://www.scholarpedia.org/article/recurrent_neural_networks#serial_learning
 215. http://www.scholarpedia.org/article/classical_conditioning
 216. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-35
 217. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-36
 218. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar.png
 219. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar2.png
 220. http://www.scholarpedia.org/article/recurrent_neural_networks#avalanches
 221. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-instar.png
 222. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-inout-duality.png
 223. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-competitive-som.png
 224. http://www.scholarpedia.org/article/bayesian_statistics
 225. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-32
 226. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-outstar.png
 227. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-instar.png
 228. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-39
 229. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-art-bottomup-topdown.png
 230. http://cns.bu.edu/~steve/art.pdf
 231. http://www.scholarpedia.org/article/working_memory
 232. http://www.scholarpedia.org/article/inhibition_of_return
 233. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 234. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-14
 235. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-24
 236. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 237. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-laminar-circuits2.png
 238. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-listelos.png
 239. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 240. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 241. http://www.scholarpedia.org/article/inhibition_of_return
 242. http://www.scholarpedia.org/article/neuroscience
 243. http://www.scholarpedia.org/article/recurrent_neural_networks#serial_learning
 244. http://www.scholarpedia.org/article/transcranial_magnetic_stimulation
 245. http://www.scholarpedia.org/article/visual_short_term_memory
 246. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-41
 247. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-41
 248. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-42
 249. http://www.scholarpedia.org/article/nervous_system
 250. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 251. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 252. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 253. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 254. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 255. http://www.scholarpedia.org/article/synfire_chains
 256. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-songbird-pattern-generator.png
 257. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-laminar-circuits2.png
 258. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-artword.png
 259. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-listelos.png
 260. http://web.mit.edu/feelab/research.html
 261. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 262. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-command-cells.png
 263. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-command-cells.png
 264. http://www.scholarpedia.org/article/motivation
 265. http://www.scholarpedia.org/article/computational_models_of_classical_conditioning
 266. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-motivator.png
 267. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-telos.png
 268. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-listelos.png
 269. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-laminar-circuits.png
 270. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-gridplacemap.png
 271. http://www.scholarpedia.org/article/self-organization
 272. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-avalanche.png
 273. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-competitive-som.png
 274. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-self-organizing-avalanche.png
 275. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-inout-assoc-map.png
 276. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-serial-learning.png
 277. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-35
 278. http://www.scholarpedia.org/article/recurrent_neural_networks#eq-36
 279. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-self-organizing-avalanche.png
 280. http://www.scholarpedia.org/article/recurrent_neural_networks#fig:id56-item-and-order.png
 281. http://www.scholarpedia.org/article/algorithm
 282. http://www.scholarpedia.org/article/human_saccadic_eye_movements
 283. http://www.scholarpedia.org/article/cognitive_neuropsychology
 284. http://www.scholarpedia.org/article/cognitive_psychology
 285. http://www.scholarpedia.org/article/spike-timing_dependent_plasticity
 286. http://www.scholarpedia.org/article/operant_conditioning
 287. http://www.scholarpedia.org/article/reinforcement
 288. http://www.scholarpedia.org/article/unfoldings
 289. http://www.scholarpedia.org/article/neural_basis_of_emotions
 290. http://www.scholarpedia.org/article/sequence_learning
 291. http://www.scholarpedia.org/article/schizophrenia
 292. http://www.scholarpedia.org/article/area_v1
 293. http://www.scholarpedia.org/article/andronov-hopf_bifurcation
 294. http://www.scholarpedia.org/article/bifurcation
 295. http://www.izhikevich.org/publications/arbib.pdf
 296. http://www.scholarpedia.org/article/mathematical_biology
 297. http://www.scholarpedia.org/article/encyclopedia_of_computational_neuroscience
 298. http://dx.doi.org/10.3389/fncom.2012.00042
 299. http://www.scholarpedia.org/article/pyramidal_neuron
 300. http://www.scholarpedia.org/article/intracellular_recording
 301. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
 302. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&oldid=130481
 303. http://www.scholarpedia.org/article/user:baingio_pinna
 304. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&oldid=130481
 305. http://www.scholarpedia.org/article/user:birgitta_dresp-langley
 306. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&oldid=130683
 307. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&oldid=138057
 308. http://www.scholarpedia.org/article/special:categories
 309. http://www.scholarpedia.org/article/category:computational_intelligence
 310. http://www.scholarpedia.org/article/category:recurrent_neural_networks
 311. http://www.scholarpedia.org/article/category:neural_networks
 312. http://www.scholarpedia.org/w/index.php?title=special:userlogin&returnto=recurrent+neural+networks
 313. http://www.scholarpedia.org/article/recurrent_neural_networks
 314. http://www.scholarpedia.org/w/index.php?title=talk:recurrent_neural_networks&action=edit&redlink=1
 315. http://www.scholarpedia.org/article/recurrent_neural_networks
 316. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&action=edit
 317. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&action=history
 318. http://www.scholarpedia.org/article/main_page
 319. http://www.scholarpedia.org/article/scholarpedia:about
 320. http://www.scholarpedia.org/article/special:proposearticle
 321. http://www.scholarpedia.org/article/scholarpedia:instructions_for_authors
 322. http://www.scholarpedia.org/article/special:random
 323. http://www.scholarpedia.org/article/help:frequently_asked_questions
 324. http://www.scholarpedia.org/article/scholarpedia:help
 325. http://blog.scholarpedia.org/
 326. http://www.scholarpedia.org/article/encyclopedia:astrophysics
 327. http://www.scholarpedia.org/article/encyclopedia:celestial_mechanics
 328. http://www.scholarpedia.org/article/encyclopedia:computational_neuroscience
 329. http://www.scholarpedia.org/article/encyclopedia:computational_intelligence
 330. http://www.scholarpedia.org/article/encyclopedia:dynamical_systems
 331. http://www.scholarpedia.org/article/encyclopedia:physics
 332. http://www.scholarpedia.org/article/encyclopedia:touch
 333. http://www.scholarpedia.org/article/scholarpedia:topics
 334. http://www.scholarpedia.org/article/special:recentlypublished
 335. http://www.scholarpedia.org/article/special:recentlysponsored
 336. http://www.scholarpedia.org/article/special:recentchanges
 337. http://www.scholarpedia.org/article/special:allpages
 338. http://www.scholarpedia.org/article/special:listcurators
 339. http://www.scholarpedia.org/article/special:listusers
 340. http://www.scholarpedia.org/article/special:journal
 341. http://www.scholarpedia.org/article/special:whatlinkshere/recurrent_neural_networks
 342. http://www.scholarpedia.org/article/special:recentchangeslinked/recurrent_neural_networks
 343. http://www.scholarpedia.org/article/special:specialpages
 344. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&printable=yes
 345. http://www.scholarpedia.org/w/index.php?title=recurrent_neural_networks&oldid=138057
 346. https://twitter.com/scholarpedia
 347. https://plus.google.com/112873162496270574424
 348. http://www.facebook.com/scholarpedia
 349. http://www.linkedin.com/groups/scholarpedia-4647975/about
 350. http://www.mediawiki.org/
 351. http://www.mathjax.org/
 352. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 353. http://www.scholarpedia.org/article/recurrent_neural_networks
 354. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 355. http://www.scholarpedia.org/article/scholarpedia:terms_of_use
 356. http://www.scholarpedia.org/article/scholarpedia:privacy_policy
 357. http://www.scholarpedia.org/article/scholarpedia:about
 358. http://www.scholarpedia.org/article/scholarpedia:general_disclaimer

   hidden links:
 360. http://www.scholarpedia.org/article/file:id56-outstar.png
 361. http://www.scholarpedia.org/article/file:id56-outstar.png
 362. http://www.scholarpedia.org/article/file:id56-outstar2.png
 363. http://www.scholarpedia.org/article/file:id56-outstar2.png
 364. http://www.scholarpedia.org/article/file:id56-instar.png
 365. http://www.scholarpedia.org/article/file:id56-instar.png
 366. http://www.scholarpedia.org/article/file:id56-competitive-som.png
 367. http://www.scholarpedia.org/article/file:id56-competitive-som.png
 368. http://www.scholarpedia.org/article/file:id56-inout-duality.png
 369. http://www.scholarpedia.org/article/file:id56-inout-duality.png
 370. http://www.scholarpedia.org/article/file:id56-inout-assoc-map.png
 371. http://www.scholarpedia.org/article/file:id56-inout-assoc-map.png
 372. http://www.scholarpedia.org/article/file:id56-art-bottomup-topdown.png
 373. http://www.scholarpedia.org/article/file:id56-art-bottomup-topdown.png
 374. http://www.scholarpedia.org/article/file:id56-input-transforms.png
 375. http://www.scholarpedia.org/article/file:id56-input-transforms.png
 376. http://www.scholarpedia.org/article/file:id56-sigmoid.png
 377. http://www.scholarpedia.org/article/file:id56-sigmoid.png
 378. http://www.scholarpedia.org/article/file:id56-equilibruim.png
 379. http://www.scholarpedia.org/article/file:id56-equilibruim.png
 380. http://www.scholarpedia.org/article/file:id56-laminar-circuits.png
 381. http://www.scholarpedia.org/article/file:id56-laminar-circuits.png
 382. http://www.scholarpedia.org/article/file:id56-laminar-circuits2.png
 383. http://www.scholarpedia.org/article/file:id56-laminar-circuits2.png
 384. http://www.scholarpedia.org/article/file:id56-artword.png
 385. http://www.scholarpedia.org/article/file:id56-artword.png
 386. http://www.scholarpedia.org/article/file:id56-telos.png
 387. http://www.scholarpedia.org/article/file:id56-telos.png
 388. http://www.scholarpedia.org/article/file:id56-listelos.png
 389. http://www.scholarpedia.org/article/file:id56-listelos.png
 390. http://www.scholarpedia.org/article/file:id56-motivator.png
 391. http://www.scholarpedia.org/article/file:id56-motivator.png
 392. http://www.scholarpedia.org/article/file:id56-artscan.png
 393. http://www.scholarpedia.org/article/file:id56-artscan.png
 394. http://www.scholarpedia.org/article/file:id56-artscene.png
 395. http://www.scholarpedia.org/article/file:id56-artscene.png
 396. http://www.scholarpedia.org/article/file:id56-gridplacemap.png
 397. http://www.scholarpedia.org/article/file:id56-gridplacemap.png
 398. http://www.scholarpedia.org/article/file:id56-item-and-order.png
 399. http://www.scholarpedia.org/article/file:id56-item-and-order.png
 400. http://www.scholarpedia.org/article/file:id56-avalanche.png
 401. http://www.scholarpedia.org/article/file:id56-avalanche.png
 402. http://www.scholarpedia.org/article/file:id56-songbird-pattern-generator.png
 403. http://www.scholarpedia.org/article/file:id56-songbird-pattern-generator.png
 404. http://www.scholarpedia.org/article/file:id56-command-cells.png
 405. http://www.scholarpedia.org/article/file:id56-command-cells.png
 406. http://www.scholarpedia.org/article/file:id56-self-organizing-avalanche.png
 407. http://www.scholarpedia.org/article/file:id56-self-organizing-avalanche.png
 408. http://www.scholarpedia.org/article/file:id56-serial-learning.png
 409. http://www.scholarpedia.org/article/file:id56-serial-learning.png
 410. http://www.scholarpedia.org/article/recurrent_neural_networks
 411. http://www.scholarpedia.org/article/recurrent_neural_networks
 412. http://www.scholarpedia.org/article/main_page
