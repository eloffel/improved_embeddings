   #[1]scholarpedia (en) [2]scholarpedia atom feed

latent semantic analysis

   from scholarpedia
   thomas k landauer and susan dumais (2008), scholarpedia, 3(11):4356.
   [3]doi:10.4249/scholarpedia.4356 revision #142371 [[4]link to/cite this
   article]
   jump to: [5]navigation, [6]search
   post-publication activity
   (button)

   curator: [7]susan dumais
   contributors:


   0.57 -

   [8]thomas k landauer
   0.29 -

   [9]ke chen
   0.14 -

   [10]ivan savov
   0.14 -

   [11]eugene m. izhikevich

   [12]srivas chennu
     * [13]dr. thomas k landauer, university of colorado at boulder and
       pearson knowledge technologies
     * [14]dr. susan dumais, microsoft inc., one microsoft way, redmond wa

   latent semantic analysis (lsa) is a mathematical method for computer
   modeling and simulation of the meaning of words and passages by
   analysis of representative corpora of natural text. lsa closely
   approximates many aspects of human [15]language learning and
   understanding. it supports a variety of applications in information
   retrieval, educational technology and other pattern recognition
   problems where complex wholes can be treated as additive functions of
   component parts.

contents

     * [16]1 overview of purpose and method
     * [17]2 lsa as a theory and model of language
     * [18]3 technical description of lsa
          + [19]3.1 term-passage matrix
          + [20]3.2 transformed term-passage matrix
          + [21]3.3 stop-listing and id30
          + [22]3.4 dimension reduction
          + [23]3.5 the mathematics
          + [24]3.6 similarity in the reduced space
     * [25]4 applying lsa
          + [26]4.1 application guidelines
               o [27]4.1.1 id30
               o [28]4.1.2 recomputing, folding in, computational
                 limitations
               o [29]4.1.3 svd and lsa packages
     * [30]5 typical language simulation applications
     * [31]6 non-english and cross-language applications
     * [32]7 linguistic and philosophical implications
     * [33]8 shortfalls, objections, evidence and arguments
     * [34]9 references
     * [35]10 recommended reading
     * [36]11 external links
     * [37]12 see also

overview of purpose and method

   latent semantic analysis (also called lsi, for latent semantic
   indexing) models the contribution to natural language attributable to
   combination of words into coherent passages. it uses a long-known
   matrix-algebra method, singular value decomposition (svd), which became
   practical for application to such complex phenomena only after the
   advent of powerful digital computing machines and [38]algorithms to
   exploit them in the late 1980s. to construct a semantic space for a
   language, lsa first casts a large representative text corpus into a
   rectangular matrix of words by coherent passages, each cell containing
   a transform of the number of times that a given word appears in a given
   passage. the matrix is then decomposed in such a way that every passage
   is represented as a vector whose value is the sum of vectors standing
   for its component words. similarities between words and words, passages
   and words, and of passages to passages are then computed as dot
   products, cosines or other vector-algebraic metrics. (for word and
   passage in the above, any objects that can be considered parts that add
   to form a larger object may be substituted.)

lsa as a theory and model of language

   the language-theoretical interpretation of the result of the analysis
   is that lsa vectors approximate the meaning of a word as its average
   effect on the meaning of passages in which it occurs, and reciprocally
   approximates the meaning of passages as the average of the meaning of
   their words. the derived relation between individual words should not
   be confused with surface co-occurrence, the frequency or likelihood
   that words appear in the same passages: it is correctly interpreted as
   the similarity of the effects that the words have on passages in which
   they occur. that this kind of mutual constraint can be realized in
   other ways than svd, for example with [39]neural network models,
   recommends it as a potential theory of corresponding biological
   mechanisms in language and thought. the use of a large and
   representative language corpus supports representation of the meanings
   of new passages by statistical induction, thus comparison of the
   meaning of new passages to each other whether containing literal words
   in common or not, and thence to a wide range of practical applications.
   for example, the sentences cardiac surgeries are quite safe these days
   and nowadays, it is not at all risky to operate on the heart have very
   high similarity (cos =.76, p<< 0001). in lsa, there is no notion of
   multiple discrete senses or disambiguation prior to passage meaning
   formation. a word-form type has the same effect on every passage in
   which it occurs, and that in turn is the average of the vectors for all
   of the passages in which it occurs. thus, a word vector represents a
   mixture of all its senses, in proportion to the sum of their contextual
   usages.

technical description of lsa

term-passage matrix

   a large collection of text statistically representative of human
   language experience is first divided into passages with coherent
   meanings, typically paragraphs or documents. the collection is then
   represented as a term-passage matrix. rows stand for individual terms
   and columns stand for passages or documents (or other units of analysis
   of interest.) individual cell entries contain the frequency with which
   each term occurs in a document.

transformed term-passage matrix

   the entries in the term-document matrix are often transformed to weight
   them by their estimated importance in order to better mimic the human
   comprehension process. for language simulation, the best performance is
   observed when frequencies are cumulated in a sublinear fashion within
   cells (typically \(log(freq_{ij}+1)\ ,\) where \(freq_{ij}\) is the
   frequency of term \(i\) in document \(j\)), and inversely with the
   overall occurrence of the term in the collection (typically using
   inverse document frequency or [40]id178 measures).

stop-listing and id30

   these are very rarely used. in keeping with the underlying theory and
   model, neither id30 nor stop-listing is appropriate or usually
   effective. as in natural language, the meaning of passages cannot be
   accurately reconstructed or understood without all of its words.
   however, when lsa is used to compare word strings shorter than normal
   text paragraphs, e.g. short sentences, zero weighting of function words
   is often pragmatically useful.

dimension reduction

   a reduced-rank singular value decomposition (svd) is performed on the
   matrix, in which the \(k\) largest singular values are retained, and
   the remainder set to 0. the resulting representation is the best
   \(k\)-dimensional approximation to the original matrix in the
   least-squares sense. each passage and term is now represented as a
   \(k\)-dimensional vector in the space derived by the svd. in most
   applications \(k\) the dimensionality is much smaller than the number
   of terms in the term-passage matrix. for most language simulation \(k\
   ,\) >50 and <1,000 dimensions are optimal, with 300 +/- 50 most often
   best, although there is neither theory nor method to predict the
   optimum. it has been conjectured that in many cases, such as language
   simulation, that the optimal dimensionality is intrinsic to the domain
   being simulated and thus must be empirically determined. the dimension
   reduction step performs a powerful induction: a different value for the
   similarity of every word to every other whether or not they have ever
   occurred in a common context.

the mathematics

   consider a rectangular \(t\) x \(p\) matrix of terms and passages, \(x\
   .\) any rectangular matrix can be decomposed into the product of three
   other matrices using the singular value decomposition. thus,

   \[\tag{1} x = t*s*p^t \]

   is the svd of a matrix \(x\) where \(t\) is a \(t\) x \(r\) matrix with
   orthonormal columns, \(p\) is a \(p\) x \(r\) matrix with orthonormal
   columns, and \(s\) is an \(r\) x \(r\) diagonal matrix with the entries
   sorted in decreasing order. the entries of the \(s\) matrix are the
   singular values (eigenvalue\(^{.5}\)), and the \(t\) and \(p\) matrices
   are the left and right singular vectors, corresponding to term and
   passage vectors. this is simply a re-representation of the \(x\) matrix
   using orthogonal indexing dimensions. lsa uses a truncated svd, keeping
   only the \(k\) largest singular values and their associated vectors, so

   \[\tag{2} x = t_k*s_k*p_k^t \]

   is the reduced-dimension svd, as used in lsa. this is the best (in a
   least squares sense) approximation to \(x\) with \(k\) parameters, and
   is what lsa uses for its semantic space. the rows in \(t_k\) are the
   term vectors in lsa space and the rows in \(p_k\) are the passage
   vectors.

similarity in the reduced space

   since both passages and terms are represented as vectors, it is
   straightforward to compute the similarity between passage-passage,
   term-term, and term-passage. in addition, terms and/or passages can be
   combined to create new vectors in the space. the process by which new
   vectors can be added to an existing lsa space is called folding-in. the
   cosine distance between vectors is used as the measure of their
   similarity for many applications because of its relation to the
   dot-product criterion and has been found effective in practice,
   although other metrics, such as euclidean or city-block distances are
   sometimes used. to accurately update the svd and thereby take into
   account new term frequencies and/or new terms requires considerable
   computation; minor perturbations to the original term-by-document
   matrix \(x\) can produce different term and passage vectors (and
   therefore affect precision and recall for query matching).

applying lsa

   in applying lsa successfully to language simulations, the most
   important factor is to have an appropriate and large enough text (or
   other) corpus. as a very rough rule of thumb, corpora supplying less
   than ~ 20k word types in less than ~ 20k passages are likely to yield
   faulty results. vector precision in the results of two bytes is usually
   sufficient; 300 dimensions is almost always near optimal, ~ 200-2,000
   usually within a useful range.

application guidelines

id30

   every derivative form of a word-form type will have a different vector
   that is appropriately similar or dissimilar to others, modulo training
   success. id30 often confabulates meanings, e.g. flower becomes
   flow'. by contrast, lsa assigns a different meaning to each variant and
   a continuous similarity value between each pair: for example,
   flower-flow have cos = -.01, dish-dishes cos = .68. the meanings
   associated with words by lsa can be intuitively appreciated by
   examining cosine similarities with other words in a typical lsa
   semantic space, for example: flower: petals .93, gymnosperms 0.47 flow:
   flows .84, opens 0.46 dish: sauce 0.70, bowl 0.63 dishes: kitchen 0.75,
   cup 0.57

recomputing, folding in, computational limitations

   once a semantic space has been created from a sufficiently large and
   representative sample, it is usually unnecessary to re-compute it in
   order to add new terms or passages. for new terms, it is usually
   sufficient to compute the vector value of a new term as the average of
   the vectors of all (~ >10) passages in which it has occurred. again
   very roughly, unless a corpus of over ~ 20k passages has changed by
   about 20% or more, recomputing the svd will have insignificant effects
   on old vectors. because new passage vectors are computed as the sum of
   their word vectors, the same rule of thumb applies to their addition to
   a semantic space. these observations along with the vast increases in
   computing power and the advent of highly parallel svd algorithms have
   answered early concerns about the practicality of lsa based on the
   assumption that frequent retraining would be necessary. the same
   advances have made training on far larger corpora possible; for example
   a 500 million word corpus in 2007 took less than a day on a
   modest-sized cluster.

svd and lsa packages

   free svd and lsa packages include svdpack/svdpackc which use iterative
   methods to compute the svd of large sparse matrices. some have special
   features for particular applications and thus might be worth
   considering, especially for non-language applications. as of 2008,
   there were many useful tools and semantic spaces available at
   [41]http://lsa.colorado.edu/, but continuance is not guaranteed. lsi++
   is a commonly used toolkit for information retrieval applications.
   computational [42]complexity of the sparse iterative methods is \(o(z +
   k)\ ,\) where \(z\) is the number of non-zero entries per passage and
   \(k\) is the number of dimensions in the reduced space.

   theory, experience and experiments all indicate that semantically and
   topically coherent passages of ~ 50-100 words, for example paragraphs
   from newspapers or text books, are optimal units for training, but that
   substantial variation is well tolerated.

typical language simulation applications

   lsa has been used most widely for small database ir and educational
   technology applications. in ir test collections when all other features
   (e.g. id30, stop-listing, and term-weighting) of comparison methods
   are held constant, lsa gives combined precision and recall results
   around 30% better than others. its strength is in recall because of its
   independence of literal word overlap. its lack of wider use in ir
   appears to be due to widely over-estimated training and retraining
   requirements. lsa   s best-known educational applications are as the
   primary component in automatic essay grading systems that equal human
   readers in accuracy and in summary writing and other computer tutors.
   it has been the basis of technologies to improve indexing, to assess
   the coherence and content sequencing of books, diagnose psychological
   disorders, match jobs and applicants, monitor and enhance team
   communications and other applications. it has been used as the basis of
   a metric for the developmental status of words as a function of the
   amount of language encountered. it has been used as a tool for
   experiments and as a component of theories and applications in
   psychology, anthropology, sociology, psycholinguistics, data mining and
   machine learning.

non-english and cross-language applications

   lsa has been used successfully in a wide variety of languages. these
   include all the u.n. and european union languages, chinese and japanese
   (in chinese character representations where the sum of components
   assumption holds over different complexity of components), swahili,
   hindi, arabic and latvian. highly inflected and word-compounding
   languages have been surprisingly amenable so long as sufficiently large
   and topic-covering training corpora are used. one demonstration of
   linguistic and anthropological/philosophical interest, as well as
   practical value, of lsa   s multiple language capability comes from
   cross-language information retrieval. in this method, independent lsa
   spaces in two or more languages are first created from single language
   corpora in which several hundred passages are direct translations or
   topically close corresponding texts in the other languages. then the
   different language spaces are rotated by the least squares procrustes
   method so that the common passages are best aligned. tested by
   similarity of one random passage to the other of translated pairs not
   used in the alignment, recall and precision are within normal ranges
   for single-language ir.

linguistic and philosophical implications

   plato, chomsky, pinker and others have claimed that neither grammar nor
   semantics can be learned from exposure to language because there is too
   little information in experience, so must be primarily innate. lsa has
   shown that computational induction can extract much more information
   than previously supposed. the finding that words and passages of
   similar meaning as expressed in a wide variety of different languages
   can be mapped onto each other by a simple linear transform implies that
   the semantic structure of language may, in a sense, be
   universal   presumably because everywhere people must learn to talk
   mostly about the same things.

shortfalls, objections, evidence and arguments

   potential limitations should be noted:
    1. training by text alone does not include oral language exposure,
       direct instruction by parents and teachers, and the association of
       language with perception and action. the amount of such loss for an
       exemplary application was as estimated as follows. in expert
       ratings of the quality of student essays, agreement between humans
       and a method using only lsa, the [43]mutual information between lsa
       and humans was 90% as high as that between the humans.
    2. lsa is blind to word order. a kind of upper bound on loss of
       information in this respect was estimated as follows the amount of
       information possible in sentences   where syntax and grammar have
       most of their influence   was divided into two parts. in a
       twenty-word sentence, the amount of information in the possible
       combinations of the 100,000 word types known by a literate adult is
       about nine times that in the possible permutations of the twenty
       words. both these approaches suggest that lsa might be only about
       10% inferior to humans, but they clearly do not go far enough.
       humans also generate meaningful language, make and understand
       linguistically posed propositions, appreciate metaphors and
       analogies--among many other things.

   some commentators have also argued that lsa must be fundamentally wrong
   as theory because is not grounded in perception and intention. the
   strength of this objection is considerably reduced by the observation
   that language must veridically reflect these sources or it would be
   nearly useless, and by the human ability to generate, use and
   understand words as abstract and unrelated to perception as the word
   abstract itself, and by lsa   s varied successes.

references

     * berry, m. w., dumais, s. t. and o'brien, g. w. (1995). using linear
       algebra for intelligent information retrieval. siam: review, 37(4):
       573-595.
     * deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., &
       harshman, r. (1990). indexing by latent semantic analysis. journal
       of the american society for information science, 41, 391-407.
     * foltz, p. w., laham, d., and landauer, t. k. (1999). the
       intelligent essay assessor: applications to educational technology.
       interactive multimedia electronic journal of computer-enhanced
       learning, 1(2). online journal.
     * foltz, p. w. kintsch, w. and landauer t. k. (1998). the measurement
       of textual coherence with latent semantic analysis. discourse
       processes, 25(2&3), 285-307.
     * graesser, a., wiemer-hastings, p., wiemer-hastings, k., harter,
       d.,person, n., & the tutoring research group. (2000). using latent
       semantic analysis to evaluate the contributions of students in
       autotutor. interactive learning environments, 129-148.
     * hofmann, t. (1999). probabilistic latent semantic analysis. in
       proceedings of 22nd international acm sigir conference on research
       and development in information retrieval, 50-57.
     * kahana, m. j. (1996). associative retrieval processes in free
       recall. [44]memory & cognition,24, 103-109.
     * landauer, t. k., and dumais, s. t. (1997). a solution to plato's
       problem: the latent semantic analysis theory of the acquisition,
       induction, and representation of knowledge. psychological review,
       104, 211-240.
     * berry, m.w., and browne, m. (2005). understanding search engines:
       mathematical modeling and text retrieval, second edition, siam,
       philadelphia.

   internal references
     * olaf sporns (2007) [45]complexity. [46]scholarpedia, 2(10):1623.

     * tomasz downarowicz (2007) [47]id178. scholarpedia, 2(11):3901.

recommended reading

     * landauer, t. k., and dumais, s. t. (1997). a solution to plato's
       problem: the latent semantic analysis theory of the acquisition,
       induction, and representation of knowledge. psychological review,
       104, 211-240
     * berry, m. w., dumais, s. t. and o'brien, g. w. (1995). using linear
       algebra for intelligent information retrieval. siam: review, 37(4):
       573-595.
     * landauer,t. k., mcnamara, d.s., dennis, s. and kintsch w. (eds).
       (2007) handbook of latent semantic analysis, mahwah nj: lawrence
       erlbaum associates.

external links

     * [48]thomas landauer's website

see also

   sponsored by: [49]eugene m. izhikevich, editor-in-chief of
   scholarpedia, the peer-reviewed open-access encyclopedia
   sponsored by: [50]prof. ke chen, school of computer science, the
   university of manchester, u.k.
   [51]reviewed by: [52]anonymous
   accepted on: [53]2008-08-30 16:38:37 gmt
   retrieved from
   "[54]http://www.scholarpedia.org/w/index.php?title=latent_semantic_anal
   ysis&oldid=142371"
   [55]categories:
     * [56]pattern recognition
     * [57]artificial intelligence
     * [58]multiple curators

personal tools

     * [59]log in / create account

namespaces

     * [60]page
     * [61]discussion

variants

views

     * [62]read
     * [63]view source
     * [64]view history

actions

search

   ____________________ (button) search

navigation

     * [65]main page
     * [66]about
     * [67]propose a new article
     * [68]instructions for authors
     * [69]random article
     * [70]faqs
     * [71]help
     * [72]blog

focal areas

     * [73]astrophysics
     * [74]celestial mechanics
     * [75]computational neuroscience
     * [76]computational intelligence
     * [77]dynamical systems
     * [78]physics
     * [79]touch
     * [80]more topics

activity

     * [81]recently published articles
     * [82]recently sponsored articles
     * [83]recent changes
     * [84]all articles
     * [85]list all curators
     * [86]list all users
     * [87]scholarpedia journal

tools

     * [88]what links here
     * [89]related changes
     * [90]special pages
     * [91]printable version
     * [92]permanent link

     * [93][twitter.png?303]
     * [94][gplus-16.png]
     * [95][facebook.png?303]
     * [96][linkedin.png?303]

     * [97]powered by mediawiki [98]powered by mathjax [99]creative
       commons license

     * this page was last modified on 10 july 2014, at 14:32.
     * this page has been accessed 82,477 times.
     * "latent semantic analysis" by [100]thomas k landauer and susan
       dumais is licensed under a [101]creative commons
       attribution-noncommercial-sharealike 3.0 unported license.
       permissions beyond the scope of this license are described in the
       [102]terms of use

     * [103]privacy policy
     * [104]about scholarpedia
     * [105]disclaimers

references

   visible links
   1. http://www.scholarpedia.org/w/opensearch_desc.php
   2. http://www.scholarpedia.org/w/index.php?title=special:recentchanges&feed=atom
   3. http://dx.doi.org/10.4249/scholarpedia.4356
   4. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&action=cite&rev=142371
   5. http://www.scholarpedia.org/article/latent_semantic_analysis#mw-head
   6. http://www.scholarpedia.org/article/latent_semantic_analysis#p-search
   7. http://www.scholarpedia.org/article/user:susan_dumais
   8. http://www.scholarpedia.org/article/user:thomas_k_landauer
   9. http://www.scholarpedia.org/article/user:ke_chen
  10. http://www.scholarpedia.org/article/user:ivan_savov
  11. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
  12. http://www.scholarpedia.org/article/user:srivas_chennu
  13. http://www.scholarpedia.org/article/user:thomas_k_landauer
  14. http://www.scholarpedia.org/article/user:susan_dumais
  15. http://www.scholarpedia.org/article/language
  16. http://www.scholarpedia.org/article/latent_semantic_analysis#overview_of_purpose_and_method
  17. http://www.scholarpedia.org/article/latent_semantic_analysis#lsa_as_a_theory_and_model_of_language
  18. http://www.scholarpedia.org/article/latent_semantic_analysis#technical_description_of_lsa
  19. http://www.scholarpedia.org/article/latent_semantic_analysis#term-passage_matrix
  20. http://www.scholarpedia.org/article/latent_semantic_analysis#transformed_term-passage_matrix
  21. http://www.scholarpedia.org/article/latent_semantic_analysis#stop-listing_and_id30
  22. http://www.scholarpedia.org/article/latent_semantic_analysis#dimension_reduction
  23. http://www.scholarpedia.org/article/latent_semantic_analysis#the_mathematics
  24. http://www.scholarpedia.org/article/latent_semantic_analysis#similarity_in_the_reduced_space
  25. http://www.scholarpedia.org/article/latent_semantic_analysis#applying_lsa
  26. http://www.scholarpedia.org/article/latent_semantic_analysis#application_guidelines
  27. http://www.scholarpedia.org/article/latent_semantic_analysis#id30
  28. http://www.scholarpedia.org/article/latent_semantic_analysis#recomputing.2c_folding_in.2c_computational_limitations
  29. http://www.scholarpedia.org/article/latent_semantic_analysis#svd_and_lsa_packages
  30. http://www.scholarpedia.org/article/latent_semantic_analysis#typical_language_simulation_applications
  31. http://www.scholarpedia.org/article/latent_semantic_analysis#non-english_and_cross-language_applications
  32. http://www.scholarpedia.org/article/latent_semantic_analysis#linguistic_and_philosophical_implications
  33. http://www.scholarpedia.org/article/latent_semantic_analysis#shortfalls.2c_objections.2c_evidence_and_arguments
  34. http://www.scholarpedia.org/article/latent_semantic_analysis#references
  35. http://www.scholarpedia.org/article/latent_semantic_analysis#recommended_reading
  36. http://www.scholarpedia.org/article/latent_semantic_analysis#external_links
  37. http://www.scholarpedia.org/article/latent_semantic_analysis#see_also
  38. http://www.scholarpedia.org/article/algorithm
  39. http://www.scholarpedia.org/article/neuron
  40. http://www.scholarpedia.org/article/id178
  41. http://lsa.colorado.edu/
  42. http://www.scholarpedia.org/article/complexity
  43. http://www.scholarpedia.org/article/mutual_information
  44. http://www.scholarpedia.org/article/memory
  45. http://www.scholarpedia.org/article/complexity
  46. http://www.scholarpedia.org/article/scholarpedia
  47. http://www.scholarpedia.org/article/id178
  48. http://www.pearsonkt.com/biolandauer.shtml
  49. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
  50. http://www.scholarpedia.org/article/user:ke_chen
  51. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&oldid=46314
  52. http://www.scholarpedia.org/article/user:anonymous
  53. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&oldid=46314
  54. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&oldid=142371
  55. http://www.scholarpedia.org/article/special:categories
  56. http://www.scholarpedia.org/article/category:pattern_recognition
  57. http://www.scholarpedia.org/article/category:artificial_intelligence
  58. http://www.scholarpedia.org/article/category:multiple_curators
  59. http://www.scholarpedia.org/w/index.php?title=special:userlogin&returnto=latent+semantic+analysis
  60. http://www.scholarpedia.org/article/latent_semantic_analysis
  61. http://www.scholarpedia.org/w/index.php?title=talk:latent_semantic_analysis&action=edit&redlink=1
  62. http://www.scholarpedia.org/article/latent_semantic_analysis
  63. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&action=edit
  64. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&action=history
  65. http://www.scholarpedia.org/article/main_page
  66. http://www.scholarpedia.org/article/scholarpedia:about
  67. http://www.scholarpedia.org/article/special:proposearticle
  68. http://www.scholarpedia.org/article/scholarpedia:instructions_for_authors
  69. http://www.scholarpedia.org/article/special:random
  70. http://www.scholarpedia.org/article/help:frequently_asked_questions
  71. http://www.scholarpedia.org/article/scholarpedia:help
  72. http://blog.scholarpedia.org/
  73. http://www.scholarpedia.org/article/encyclopedia:astrophysics
  74. http://www.scholarpedia.org/article/encyclopedia:celestial_mechanics
  75. http://www.scholarpedia.org/article/encyclopedia:computational_neuroscience
  76. http://www.scholarpedia.org/article/encyclopedia:computational_intelligence
  77. http://www.scholarpedia.org/article/encyclopedia:dynamical_systems
  78. http://www.scholarpedia.org/article/encyclopedia:physics
  79. http://www.scholarpedia.org/article/encyclopedia:touch
  80. http://www.scholarpedia.org/article/scholarpedia:topics
  81. http://www.scholarpedia.org/article/special:recentlypublished
  82. http://www.scholarpedia.org/article/special:recentlysponsored
  83. http://www.scholarpedia.org/article/special:recentchanges
  84. http://www.scholarpedia.org/article/special:allpages
  85. http://www.scholarpedia.org/article/special:listcurators
  86. http://www.scholarpedia.org/article/special:listusers
  87. http://www.scholarpedia.org/article/special:journal
  88. http://www.scholarpedia.org/article/special:whatlinkshere/latent_semantic_analysis
  89. http://www.scholarpedia.org/article/special:recentchangeslinked/latent_semantic_analysis
  90. http://www.scholarpedia.org/article/special:specialpages
  91. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&printable=yes
  92. http://www.scholarpedia.org/w/index.php?title=latent_semantic_analysis&oldid=142371
  93. https://twitter.com/scholarpedia
  94. https://plus.google.com/112873162496270574424
  95. http://www.facebook.com/scholarpedia
  96. http://www.linkedin.com/groups/scholarpedia-4647975/about
  97. http://www.mediawiki.org/
  98. http://www.mathjax.org/
  99. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 100. http://www.scholarpedia.org/article/latent_semantic_analysis
 101. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 102. http://www.scholarpedia.org/article/scholarpedia:terms_of_use
 103. http://www.scholarpedia.org/article/scholarpedia:privacy_policy
 104. http://www.scholarpedia.org/article/scholarpedia:about
 105. http://www.scholarpedia.org/article/scholarpedia:general_disclaimer

   hidden links:
 107. http://www.scholarpedia.org/article/latent_semantic_analysis
 108. http://www.scholarpedia.org/article/latent_semantic_analysis
 109. http://www.scholarpedia.org/article/main_page
