   #[1]scholarpedia (en) [2]scholarpedia atom feed

temporal difference learning

   from scholarpedia
   andrew g. barto (2007), scholarpedia, 2(11):1604.
   [3]doi:10.4249/scholarpedia.1604 revision #186927 [[4]link to/cite this
   article]
   jump to: [5]navigation, [6]search
   post-publication activity
   (button)

   curator: [7]andrew g. barto
   contributors:


   0.17 -

   [8]eugene m. izhikevich
   0.17 -

   [9]il park
   0.17 -

   [10]alireza golmohammadi

   [11]jie bao

   [12]benjamin bronner
     * [13]dr. andrew g. barto, dept. of computer science, university of
       massachuestts - amherst

   temporal difference (td) learning is an approach to learning how to
   predict a quantity that depends on future values of a given signal. the
   name td derives from its use of changes, or differences, in predictions
   over successive time steps to drive the learning process. the
   prediction at any given time step is updated to bring it closer to the
   prediction of the same quantity at the next time step. it is a
   [14]supervised learning process in which the training signal for a
   prediction is a future prediction. td [15]algorithms are often used in
   [16]id23 to predict a measure of the total amount of
   [17]reward expected over the future, but they can be used to predict
   other quantities as well. continuous-time td algorithms have also been
   developed.

contents

     * [18]1 the problem
     * [19]2 the simplest td algorithm
     * [20]3 td with function approximation
     * [21]4 eligibility traces
     * [22]5 action-conditional prediction
     * [23]6 other prediction problems and update rules
     * [24]7 psychology and neuroscience
     * [25]8 history
     * [26]9 references
     * [27]10 recommended readings
     * [28]11 external links
     * [29]12 see also

the problem

   suppose a system receives as input a time sequence of vectors \((x_t,
   y_t)\ ,\) \(t=0, 1, 2, \dots\ ,\) where each \(x_t\) is an arbitrary
   signal and \(y_t\) is a real number. [30]td learning applies to the
   problem of producing at each discrete time step \(t\ ,\) an estimate,
   or prediction, \(p_t\ ,\) of the following quantity:

          \[ y_t = y_{t+1} + \gamma y_{t+2} + \gamma^2 y_{t+3} + \cdots =
          \sum_{i=1}^\infty \gamma^{i-1} y_{t+i}, \]

   where \(\gamma\) is a discount factor, with \(0 \le \gamma < 1\ .\)
   each estimate is a prediction because it involves future values of \(y\
   .\) the signal \(x_t\) is the information available to the system at
   time \(t\) to enable it to make the prediction \(p_t\ .\) in other
   words, \(p_t\) is a function of \(x_t\ ,\) and we can write \(p_t =
   p(x_t)\ ,\) where \(p\) is a prediction function. the discount factor
   determines how strongly future values of \(y\) influence current
   predictions. when \(\gamma=0\ ,\) \(p_t\) predicts just the next value
   of \(y\ ,\) that is, \(y_{t+1}\ .\) as \(\gamma\) increases toward one,
   values of \(y\) in the more distant future become more significant. the
   problem, then, is to select a function \(p\) so that \(p_t = p(x_t)
   \approx y_t\) as closely as possible for \(t = 0, 1, 2, \dots\ .\) this
   is the infinite-horizon discounted prediction problem. td algorithms
   apply to other prediction problems as described below.

   of course, it is not always possible to find a prediction function that
   does a good job of solving this problem. if there are no regularities
   in the relationship between signals \(x_t\) and future values of \(y\
   ,\) then the best predictions will be no better than random guessing.
   however, if there are regularities in this relationship, then
   predictions may be possible that are more accurate than chance. the
   usual way to model possible regularities is to assume that the signals
   \(x_t\) are derived from the states of a [31]markov chain, on which the
   numbers \(y\) also functionally depend. depending on how the \(x_t\)
   represent the markov chain's states (e.g., do these signals
   unambiguously identify states, or are some states unobservable), a
   prediction function may exist that accurately gives the expected value
   of the quantity \(y_t\) for each \(t\ .\)

the simplest td algorithm

   a good way to explain td learning is to start with a simple case and
   then extend it to the full algorithm. consider first the problem of
   making a one-step-ahead prediction, i.e., the above problem with
   \(\gamma=0\ ,\) meaning that one wants \(p_t=y_{t+1}\) for each \(t\
   .\) incremental error-correction supervised learning can be used to
   update the prediction function as inputs arrive. letting \(p_t\) denote
   the prediction function at step \(t\ ,\) the algorithm updates \(p_t\)
   to a new prediction function \(p_{t+1}\) at each step. to do this, it
   uses the error between the current prediction, \(p_t\ ,\) and the
   prediction target (the quantity being predicted), \(y_{t+1}\ .\) this
   error can be obtained by computing \(p_t\) by applying the current
   prediction function, \(p_t\ ,\) to \(x_t\ ,\) waiting one time step
   while remembering \(p_t\ ,\) then observing \(y_{t+1}\) to obtain the
   information required to compute the error
   \(y_{t+1}-p_t=y_{t+1}-p_t(x_t)\ .\)

   the simplest example of a prediction function is one implemented as a
   lookup table. suppose \(x_t\) can take only a finite number of values
   and that there is an entry in a lookup table to store a prediction for
   each of these values. at step \(t\ ,\) the table entry for input
   \(x_t\) is \(p_t = p_t(x_t)\ .\) when \(y_{t+1}\) is observed, the
   table entry for \(x_t\) is changed from its current value of \(p_t\) to
   \(p_t + \alpha(y_{t+1} - p_t) =(1-\alpha)p_t+ \alpha y_{t+1}\ ,\) where
   \(\alpha\) is a positive fraction that controls how quickly new data is
   incorporated into the table and old data forgotten. only the table
   entry corresponding to \(x_t\) is changed from step \(t\) to step
   \(t+1\ ,\) to produce the a table, \(p_{t+1}\ .\) the fraction
   \(\alpha\) is called the learning rate or step-size parameter. note
   that if \(\alpha=1\ ,\) the table entry is simply set to \(y_{t+1}\ ,\)
   which is appropriate if the predictive relationship is deterministic.
   using \(\alpha <1\ ,\) on the other hand, causes the table entries to
   approach the expected prediction targets when the predictive
   relationship is not deterministic.

   to extend this approach to the full prediction problem with \(\gamma
   \ne 0\ ,\) the prediction target would be \(y_t=y_{t+1} + \gamma
   y_{t+2} + \gamma^2 y_{t+3} + \cdots\) instead of just \(y_{t+1}\ .\)
   the algorithm above would have to update the table entry for \(x_t\) by
   changing its value from \(p_t\) to \(p_t + \alpha(y_t- p_t) = (1 -
   \alpha)p_t + \alpha y_t\ .\) but to do this would require waiting for
   the arrival of all the future values of \(y\) instead of waiting for
   just the next value. this would prevent the approach from forming a
   useful learning rule. td algorithms use the following observation to
   get around this problem. notice that

          \[ y_t = y_{t+1} + \gamma y_{t+2} + \gamma^2 y_{t+3} + \cdots\
          :\]

          \[ = y_{t+1} + \gamma [y_{t+2} + \gamma y_{t+3} + \gamma^2
          y_{t+4} + \cdots] \ :\]

          \[\tag{1} = y_{t+1} + \gamma y_{t+1},\]

   for \(t = 0, 1, 2, \dots\ .\) therefore, since \(p_t(x_{t+1})\) is the
   estimate of \(y_{t+1}\) available at step \(t\) (i.e., using the
   step-\(t\) table), one can estimate \(y_t\) by the quantity
   \(y_{t+1}+\gamma p_t(x_{t+1})\ .\) that is, the current prediction
   function, \(p_t\ ,\) applied to the next input, \(x_{t+1}\ ,\)
   multiplied by \(\gamma\ ,\) gives an estimate of the part of the
   prediction target that would otherwise require waiting forever (in this
   case) to obtain. the result in this lookup-table case is an algorithm
   that, upon receiving input \((x_{t+1}, y_{t+1})\ ,\) updates the table
   entry for \(x_t\) by changing its value from \(p_t\) to \(p_t +
   \alpha[y_{t+1} + \gamma p_t(x_{t+1})- p_t] = (1 - \alpha)p_t + \alpha
   [y_{t+1}+\gamma p_t(x_{t+1})]\ .\)

   more concisely, define the following temporal difference error (or td
   error):

          \[ \delta_{t+1} = y_{t+1} + \gamma p_t(x_{t+1}) - p_t (x_t).\]

   then the simplest td algorithm updates a lookup-table as follows: for
   each \(t = 0, 1, 2, \dots\ ,\) upon observing \((x_{t+1}, y_{t+1})\ :\)

          \[\tag{2} p_{t+1}(x)= \begin{cases} p_t(x) + \alpha
          \,\delta_{t+1} & \mbox{if} \;x=x_t\\ p_t(x) & \mbox{otherwise,}
          \end{cases} \]

   where \(x\) denotes any possible input signal. [note that some authors
   would label the td error as defined here \(\delta_t\) instead of
   \(\delta_{t+1}\ ,\) giving the impression that td learning is somehow
   not causal in that its updates depend on unavailable future values of
   certain variables. if it is understood that the update defined here
   occurs on step \(t+1\ ,\) it is completely causal. the time subscript
   for \(\delta\) is unimportant.]

   although the detailed behavior of this algorithm is complicated, an
   intuitive understanding is possible. the algorithm uses a prediction of
   a later quantity, \(p_t (x_{t+1})\ ,\) to update a prediction of an
   earlier quantity, \(p_t(x_t)\ ,\) where each prediction is computed via
   the same prediction function, \(p_t\ .\) this may seem like a futile
   process since neither prediction need be accurate, but in the course of
   the algorithm's operation, later predictions tend to become accurate
   sooner than earlier ones, so there tends to be an overall error
   reduction as learning proceeds. this depends on the learning system
   receiving an input sequence with sufficient regularity to make
   predicting possible. in formal treatments, the inputs \(x_t\) represent
   states of a markov chain and the \(y\) values are given by a function
   of these states. this makes it possible to form accurate predictions of
   the expected values of the discounted sums \(y_t\ .\)

   another view of the td algorithm is that it operates to maintain a
   consistency condition that must be satisfied by correct predictions.
   since \(y_t=y_{t+1}+\gamma y_{t+1}\ ,\) for \(t=0, 1, \dots\) (equation
   ([32]1), the following relationship should hold between successive
   predictions:

          \[ p(x_t)=y_{t+1}+\gamma p(x_{t+1}) \]

   for \(t = 0, 1, 2, \dots\ .\) one can show, in fact, that within an
   appropriate mathematical framework any function that satisfies this
   condition for all \(t\) must actually give the correct predictions. td
   algorithms adjust the prediction function with the goal of making its
   values always satisfy this condition. the td error indicates how far
   the current prediction function deviates from this condition for the
   current input, and the algorithm acts to reduce this error. this view
   of td learning is connected to the theory of [33]markov decision
   processes, where the prediction function corresponds to a value
   function and the update process is closely related to [34]dynamic
   programming methods. most of the theoretical results about td learning
   derive from these connections.

td with function approximation

   beyond lookup tables, td learning can update prediction functions
   represented in a variety of ways. consider, for example, the case in
   which each prediction is a linear function of the input signals, where
   each input signal \(x_t\) is now a vector of real numbers\[x_t=(x^1_t,
   x^2_t, \dots, x^n_t)\ .\] then the prediction function of step \(t\)
   applied to the signal at step \(t'\ ,\) where \(t'\) can be different
   from \(t\ ,\) is defined by

          \[ p_{t}(x_{t'})=\sum_{i=1}^n v^i_{t} x^i_{t'}, \]

   where the \(v^i_{t}\ ,\) \(i=1, \dots, n\ ,\) are the coefficients, or
   weights, of the linear prediction function of step \(t\ ,\) and the
   \(x^i_{t'}\) are components of the vector \(x_{t'}\ .\) then td
   learning adjusts the weights according to the following rule: for each
   \(t = 0, 1, 2, \dots\ ,\) upon observing \((x_{t+1}, y_{t+1})\ :\)

          \[ v^i_{t+1} = v^i_{t} + \alpha [y_{t+1} + \gamma p_t(x_{t+1}) -
          p_t (x_t)] x^i_{t}\ :\]

                \[\tag{3} = v^i_t + \alpha \delta_{t+1} x^i_t.\]

   for \(i=1, \dots, n\ ,\) where \(\alpha > 0\) is a step-size parameter.

   this learning rule adjusts each weight in a direction that tends to
   reduce the td error. it is similar to the conventional [35]least mean
   square (lms) or [36]delta rule for supervised learning with the
   difference being the presence of \(\gamma p_t(x_{t+1})\) in the error
   term. td learning with linear function approximation is the best
   understood extension of the lookup-table version. it is widely used
   with input vectors consisting of the outputs of a possibly large number
   of sophisticated feature detectors, which is equivalent to employing
   representations involving sophisticated basis vectors. note that
   lookup-tables correspond to linear function approximation using
   standard unit basis vectors (in which case equation ([37]3) reduces to
   equation ([38]2). td learning with nonlinear function approximation is
   also possible. in general, any incremental function approximation, or
   [39]regression, method can be adapted for use with td learning. in all
   of these cases, it is important to recognize that td learning does not
   entail specific assumptions about how stimuli are represented over
   time. there is a wide latitude in the alternatives that can be
   employed, and each has implications for the behavior of the algorithm.

eligibility traces

   td learning can often be accelerated by the addition of eligibility
   traces. when the lookup-table td algorithm described above receives
   input \((y_{t+1}, x_{t+1})\ ,\) it updates the table entry only for the
   immediately preceding signal \(x_t\ .\) that is, it modifies only the
   immediately preceding prediction. but since \(y_{t+1}\) provides useful
   information for learning earlier predictions as well, one can extend td
   learning so it updates a collection of many earlier predictions at each
   step. eligibility traces do this by providing a short-term [40]memory
   of many previous input signals so that each new observation can update
   the parameters related to these signals. eligibility traces are usually
   implemented by an exponentially-decaying memory trace, with decay
   parameter \(\lambda\ .\) this generates a family of td algorithms
   td(\(\lambda\)), \(0 \le \lambda \le 1\ ,\) with td(0) corresponding to
   updating only the immediately preceding prediction as described above,
   and td(1) corresponding to equally updating all the preceding
   predictions. this also applies to non lookup-table versions of td
   learning, where traces of the components of the input vectors are
   maintained. eligibility traces do not have to be exponentially-decaying
   traces, but these are usually used since they are relatively easy to
   implement and to understand theoretically.

action-conditional prediction

   consider a setting in which the future values of inputs \((x_t, y_t)\)
   are influenced by an agent's actions. the prediction at step \(t\) is
   often denoted \(q(x_t, a_t)\ ,\) where \(x_t\) is the input signal and
   \(a_t\) is the agent action at step \(t\ .\) the objective is to find a
   function that accurately predicts \(y_t\ ,\) the discounted sum of
   future values of \(y\ ,\) on the basis of \(x_t\) and \(a_t\ .\) in the
   usual [41]id23 setting of a [42]markov decision
   process, \(y_t\) also depends on all the agent's actions taken after
   step \(t\ .\) two different cases are considered for the actions taken
   after step \(t\ :\) (1) assume that after step \(t\ ,\) the agent
   selects actions that generate the largest possible value of \(y_t\ ,\)
   or (2) assume that the agent follows a fixed rule, or policy, for
   selecting actions as a function of future inputs. in either case, the
   desired predictions are well-defined.

   a td learning process for case (1), known as [43]id24, works as
   follows for a lookup-table representation. for each \(t = 0, 1, 2,
   \dots\ ,\) upon generating action \(a_t\) and observing \((x_{t+1},
   y_{t+1})\ ,\) the prediction function \(q_t\) is updated to \(q_{t+1}\)
   defined as follows:

          \[ q_{t+1}(x, a )= \begin{cases} q_t(x, a) + \alpha[y_{t+1} +
          \gamma \max_a q_t(x_{t+1}, a) - q_t (x_t, a_t)] & \mbox{if}
          \;x=x_t \;\mbox{and} \;a=a_t\\ q_t(x, a) & \mbox{otherwise,}
          \end{cases} \]

   where \(x\) denotes any possible input signal and \(a\) denotes any of
   a finite number of possible actions. for case (2), the update is the
   same except that \(\max_a q_t(x_{t+1}, a)\) is replaced by
   \(q_t(x_{t+1}, a_{t+1})\ ,\) producing a td learning rule called
   [44]sarsa (for state-action-reward-state-action). these learning rules
   can also be extended to allow function approximation.

   q-leaid56g and sarsa are useful in id23 where \(y_t\)
   is a [45]reward signal. an agent selecting actions that maximize \(q(x,
   a)\) for each current \(x\) is fully exploiting the knowledge contained
   in \(q\) in its attempt to maximize the measure of long-term reward,
   \(y_t\ .\) under appropriate conditions, both id24 and sarsa
   converge to prediction functions that allow an agent to make optimal
   action choices.

other prediction problems and update rules

   td learning is not restricted to the infinite-horizon discounted
   problem. if the input sequence is divided up into finite-length
   episodes, and the predictions are not expected to extend beyond single
   episodes, then the discount factor \(\gamma\) can be set to one,
   resulting in the undiscounted case. td algorithms also exist for
   problems requiring the prediction of the average value per-unit-time of
   the target variable. in id23 where the target
   variable is reward, this is called the average reward case (mahadevan,
   1996). td learning has also been generalized to [46]td networks (sutton
   and tanner, 2005). conventional td learning is based on a consistency
   condition relating the prediction of a quantity to the prediction of
   the same quantity at a later time. td networks generalize this to
   conditions relating predictions of one quantity to a set of predictions
   of other quantities at a later time. more sophisticated update methods
   have been studied that use the basic td idea but that often have better
   performance. examples include least squares td, an adaptation of a
   conventional recursive least squares regression method (bradtke and
   barto, 1996), and gaussian process td (engle, et al., 1993), an
   adaptation of [47]gaussian process methods.

psychology and [48]neuroscience

   theories of animal learning as studied by psychologists were a major
   influence on the development of td learning. in particular, td learning
   can be viewed as an extension of the [49]rescorla-wagner model of
   [50]pavlovian conditioning in which the timing of stimuli within
   learning trials plays a significant role in how associative strengths
   change. a slightly modified version of the td algorithm given above
   accounts for a range of phenomena observed in [51]pavlovian
   conditioning experiments. of particular significance is the ability of
   the algorithm to produce an analog of secondary [52]reinforcement when
   used as a component of a id23 system. predictions of
   future reward formed by td learning can be treated as rewarding
   themselves, thereby acting as secondary reinforcers. this is most
   clearly demonstrated in id23 systems that use the td
   error \(\delta\) as a reward signal. td learning is also related to the
   neuroscience of reward learning through the similarity of the behavior
   of \(\delta\) in analogs of conditioning experiments and the behavior
   of midbrain [53]dopamine neurons in the [54]brain. this observation has
   stimulated both neuroscientific and computational research based on the
   computational theory of td learning in efforts to improve understanding
   of the brain's dopamine system and its role in reward-related behavior
   and drug [55]addiction.

history

   td learning is most closely associated with r. s. sutton, whose 1984
   ph.d. dissertation addressed td learning and whose 1988 paper, in which
   the term temporal difference was first used, has become the definitive
   reference. td learning is the basis of the adaptive critic component of
   the [56]actor-critic architecture for id23 (barto et
   al., 1983), as well as the basis of models of [57]pavlovian
   conditioning (sutton and barto, 1990). this line of research work began
   with the exploration of klopf's 1972 idea of generalized reinforcement
   which emphasized the importance of sequentiality in a [58]neuronal
   model of learning (see klopf, 1982). mention should also be made of the
   work of s. e. hampson, whose 1983 ph.d. dissertation (see hampson,
   1990) presented a similar algorithm. earlier precursors are the method
   used by a. l. samuel (1959) in his learning program for the game of
   checkers, i. h. witten's (1977) algorithm presented in the context of
   id100, p. j. werbos' 1974 mention of similar ideas
   in the context of robust prediction and his later introduction of
   heuristic id145 (see werbos, 1994). the similarity
   between the behavior of the td error and the activity of dopamine
   neurons, based on the findings of w. schultz (reviewed in schultz,
   1998), was first noted by houk et al. (1995) and schultz et al. (1997).
   id24 was introduced in c. j. c. h. watkins' 1989 ph.d.
   dissertation, though it also has precursors in l. tesfatsion's
   introduction in 1976 of criterion filtering (see also tesfatsion,
   1982), and s. bozinovski's 1982 crossbar adaptive array (see
   bozinovski, 1995). sarsa, proposed by rummery and niranjan (1994) and
   named by sutton (1996), is similar to j. h. holland's bucket brigade
   algorithm (holland, 1986).

references

     * barto, a.g., sutton, r.s., anderson, c.w., 1983, neuronlike
       elements that can solve difficult learning control problems, ieee
       transactions on systems, man and cybernetics, 13, pp. 835-846,
       reprinted in j. a. anderson and e. rosenfeld, eds., 1988,
       neurocomputing: foundations of research, cambridge, ma: mit press.

     * bozinovski, s., 1995, consequence driven systems, bitola macedonia:
       gocmar publishers.

     * bradtke, s.j, barto, a.g., 1996, linear least-squares algorithms
       for temporal difference learning", machine learning, 22, pp. 33-57.

     * engel, y., mannor, s., meir, r., 2003, bayes meets bellman: the
       gaussian process approach to temporal difference learning, in
       proceedings of the international conference on machine learning.

     * hampson, s.e., 1990, connectionist problem solving, boston, basal,
       berlin: birkhauser.

     * holland, j.h., 1986, escaping brittleness: the possibility of
       general-purpose learning algorithms applied to rule-based systems,
       in machine learning: an artificial intelligence approach, volume
       ii, michalski, r.s., carbonell, j.g., mitchell, t.m., eds., pp.
       593-623, san mateo, ca: morgan kaufmann.

     * houk, j.c., adams, j.l., barto, a.g., 1995, a model of how the
       [59]basal ganglia generates and uses neural signals that predict
       reinforcement, in models of information processing in the basal
       ganglia, j.c. houk, j.l. davis, d.g. beiser, eds., pp. 249-270,
       cambridge, ma: mit press.

     * klopf, a.h., 1982, the hedonistic neuron: a theory of memory,
       learning, and intelligence, washington, d.c.: hemisphere.

     * mahadevan, s., 1996, average reward id23:
       foundations, algorithms, and empirical results, machine learning,
       22, pp. 159-196.

     * rummery,g.a., niranjan, m., 1994, on-line id24 using
       connectionist systems, cambridge university engineering department
       technical report cued/f-infeng/tr 166.

     * samuel, a.l., 1959, some studies in machine learning using the game
       of checkers, ibm journal on research and development, pp. 210-229.
       reprinted in e.a. feigenbaum and j. feldman, eds., 1963, computers
       and thought, new york: mcgraw-hill.

     * schultz, w., 1998, predictive reward signal of dopamine neurons,
       journal of neurophysiology, 80, pp. 1-27.

     * schultz, w., dayan, p., montague, p.r., 1997, a neural substrate of
       prediction and reward, science, 275, pp. 1593-1598.

     * sutton, r.s., 1996, generalization in id23:
       successful examples using sparse coarse coding, in advances in
       neural information processing systems: proceedings of the 1995
       conference (nips 8), pp. 1038-1044, d. s. touretzky, m. c. mozer,
       m. e. hasselmo, eds., cambridge, ma: mit press.

     * sutton, r.s., 1988, learning to predict by the method of temporal
       differences, machine learning, 3, pp. 9-44.

     * sutton, r.s., 1984, temporal credit assignment in reinforcement
       learning, ph.d. dissertation, university of massachusetts, amherst,
       ma.

     * sutton, r.s., barto, a.g., 1990, time-derivative models of
       pavlovian reinforcement, in learning and [60]computational
       neuroscience: foundations of adaptive networks, m. gabriel, j.
       moore, eds., pp. 497 -537, cambridge ma: the mit press.

     * sutton, r. s., tanner, b., 2005, temporal-difference networks.
       advances in neural information processing systems 17, pp.
       1377-1384.

     * tesfatsion, l., 1976, bayes' theorem for utility, discussion paper
       76-65, center for economic research, university of minnesota.

     * tesfatsion l., 1982, a dual approach to [61]bayesian id136 and
       adaptive control, theory and decision, 14, pp. 177-194.

     * watkins, c.j.c.h., 1989, learning from delayed rewards, ph.d.
       dissertation, cambridge university, cambridge, england.

     * werbos, p.j., 1994, the roots of id26: from ordered
       derivatives to neural networks and political forecasting, new york:
       john wiley & sons, inc.

     * witten, i. h., 1977, an adaptive optimal controller for
       discrete-time markov environments, information and control, 34, pp.
       286-295.

   internal references
     * valentino braitenberg (2007) [62]brain. [63]scholarpedia,
       2(11):2918.
     * wolfram schultz (2007) [64]reward. scholarpedia, 2(3):1652.
     * wolfram schultz (2007) [65]reward signals. scholarpedia, 2(6):2184.

recommended readings

     * sutton, r.s., barto, a.g., 1998, id23: an
       introduction, cambridge, ma: mit press.
     * bertsekas, d.p., tsitsiklis, j.n., 1996, neuro-id145,
       belmont, ma: athena scientific.
     * powell, w. b., 2007, approximate id145: solving the
       curses of dimensionality, hoboken, nj: john wiley & sons, inc.

external links

     * [66]author's web page
     * [67]r. s. sutton's web page

see also

   [68]adaptive critic, [69]conditioning, [70]dopamine, [71]dynamic
   programming, [72]id100, [73]neuroeconomics,
   [74]optimal control, [75]id24, [76]id23,
   [77]rescorla-wagner model, [78]reward, [79]reward signals
   sponsored by: [80]eugene m. izhikevich, editor-in-chief of
   scholarpedia, the peer-reviewed open-access encyclopedia
   [81]reviewed by: [82]anonymous
   accepted on: [83]2007-11-19 01:09:50 gmt
   retrieved from
   "[84]http://www.scholarpedia.org/w/index.php?title=temporal_difference_
   learning&oldid=186927"
   [85]categories:
     * [86]machine learning
     * [87]id23

personal tools

     * [88]log in / create account

namespaces

     * [89]page
     * [90]discussion

variants

views

     * [91]read
     * [92]view source
     * [93]view history

actions

search

   ____________________ (button) search

navigation

     * [94]main page
     * [95]about
     * [96]propose a new article
     * [97]instructions for authors
     * [98]random article
     * [99]faqs
     * [100]help
     * [101]blog

focal areas

     * [102]astrophysics
     * [103]celestial mechanics
     * [104]computational neuroscience
     * [105]computational intelligence
     * [106]dynamical systems
     * [107]physics
     * [108]touch
     * [109]more topics

activity

     * [110]recently published articles
     * [111]recently sponsored articles
     * [112]recent changes
     * [113]all articles
     * [114]list all curators
     * [115]list all users
     * [116]scholarpedia journal

tools

     * [117]what links here
     * [118]related changes
     * [119]special pages
     * [120]printable version
     * [121]permanent link

     * [122][twitter.png?303]
     * [123][gplus-16.png]
     * [124][facebook.png?303]
     * [125][linkedin.png?303]

     * [126]powered by mediawiki [127]powered by mathjax [128]creative
       commons license

     * this page was last modified on 18 october 2018, at 15:35.
     * this page has been accessed 135,125 times.
     * "temporal difference learning" by [129]andrew g. barto is licensed
       under a [130]creative commons attribution-noncommercial-sharealike
       3.0 unported license. permissions beyond the scope of this license
       are described in the [131]terms of use

     * [132]privacy policy
     * [133]about scholarpedia
     * [134]disclaimers

references

   visible links
   1. http://www.scholarpedia.org/w/opensearch_desc.php
   2. http://www.scholarpedia.org/w/index.php?title=special:recentchanges&feed=atom
   3. http://dx.doi.org/10.4249/scholarpedia.1604
   4. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&action=cite&rev=186927
   5. http://www.scholarpedia.org/article/temporal_difference_learning#mw-head
   6. http://www.scholarpedia.org/article/temporal_difference_learning#p-search
   7. http://www.scholarpedia.org/article/user:andrew_g._barto
   8. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
   9. http://www.scholarpedia.org/article/user:il_park
  10. http://www.scholarpedia.org/article/user:alireza_golmohammadi
  11. http://www.scholarpedia.org/article/user:jie_bao
  12. http://www.scholarpedia.org/article/user:benjamin_bronner
  13. http://www.scholarpedia.org/article/user:andrew_g._barto
  14. http://www.scholarpedia.org/w/index.php?title=supervised_learning&action=edit&redlink=1
  15. http://www.scholarpedia.org/article/algorithm
  16. http://www.scholarpedia.org/article/reinforcement_learning
  17. http://www.scholarpedia.org/article/reward
  18. http://www.scholarpedia.org/article/temporal_difference_learning#the_problem
  19. http://www.scholarpedia.org/article/temporal_difference_learning#the_simplest_td_algorithm
  20. http://www.scholarpedia.org/article/temporal_difference_learning#td_with_function_approximation
  21. http://www.scholarpedia.org/article/temporal_difference_learning#eligibility_traces
  22. http://www.scholarpedia.org/article/temporal_difference_learning#action-conditional_prediction
  23. http://www.scholarpedia.org/article/temporal_difference_learning#other_prediction_problems_and_update_rules
  24. http://www.scholarpedia.org/article/temporal_difference_learning#psychology_and_neuroscience
  25. http://www.scholarpedia.org/article/temporal_difference_learning#history
  26. http://www.scholarpedia.org/article/temporal_difference_learning#references
  27. http://www.scholarpedia.org/article/temporal_difference_learning#recommended_readings
  28. http://www.scholarpedia.org/article/temporal_difference_learning#external_links
  29. http://www.scholarpedia.org/article/temporal_difference_learning#see_also
  30. http://www.scholarpedia.org/article/temporal_difference_learning
  31. http://www.scholarpedia.org/article/markov_chain
  32. http://www.scholarpedia.org/article/temporal_difference_learning#eq-1
  33. http://www.scholarpedia.org/w/index.php?title=markov_decision_processes&action=edit&redlink=1
  34. http://www.scholarpedia.org/w/index.php?title=dynamic_programming&action=edit&redlink=1
  35. http://www.scholarpedia.org/w/index.php?title=least_mean_square&action=edit&redlink=1
  36. http://www.scholarpedia.org/w/index.php?title=delta_rule&action=edit&redlink=1
  37. http://www.scholarpedia.org/article/temporal_difference_learning#eq-3
  38. http://www.scholarpedia.org/article/temporal_difference_learning#eq-2
  39. http://www.scholarpedia.org/w/index.php?title=regression&action=edit&redlink=1
  40. http://www.scholarpedia.org/article/memory
  41. http://www.scholarpedia.org/article/reinforcement_learning
  42. http://www.scholarpedia.org/w/index.php?title=markov_decision_processes&action=edit&redlink=1
  43. http://www.scholarpedia.org/w/index.php?title=id24&action=edit&redlink=1
  44. http://www.scholarpedia.org/article/sarsa
  45. http://www.scholarpedia.org/article/reward_signals
  46. http://www.scholarpedia.org/w/index.php?title=td_networks&action=edit&redlink=1
  47. http://www.scholarpedia.org/w/index.php?title=gaussian_processes&action=edit&redlink=1
  48. http://www.scholarpedia.org/article/neuroscience
  49. http://www.scholarpedia.org/article/rescorla-wagner_model
  50. http://www.scholarpedia.org/article/pavlovian_conditioning
  51. http://www.scholarpedia.org/article/classical_conditioning
  52. http://www.scholarpedia.org/article/reinforcement
  53. http://www.scholarpedia.org/article/dopamine_modulation
  54. http://www.scholarpedia.org/article/brain
  55. http://www.scholarpedia.org/article/addiction
  56. http://www.scholarpedia.org/w/index.php?title=actor-critic_architecture&action=edit&redlink=1
  57. http://www.scholarpedia.org/article/pavlovian_conditioning
  58. http://www.scholarpedia.org/article/neuron
  59. http://www.scholarpedia.org/article/basal_ganglia
  60. http://www.scholarpedia.org/article/encyclopedia_of_computational_neuroscience
  61. http://www.scholarpedia.org/article/bayesian_statistics
  62. http://www.scholarpedia.org/article/brain
  63. http://www.scholarpedia.org/article/scholarpedia
  64. http://www.scholarpedia.org/article/reward
  65. http://www.scholarpedia.org/article/reward_signals
  66. http://www-anw.cs.umass.edu/~barto/
  67. http://www.cs.ualberta.ca/~sutton/index.html
  68. http://www.scholarpedia.org/w/index.php?title=adaptive_critic&action=edit&redlink=1
  69. http://www.scholarpedia.org/article/conditioning
  70. http://www.scholarpedia.org/w/index.php?title=dopamine&action=edit&redlink=1
  71. http://www.scholarpedia.org/w/index.php?title=dynamic_programming&action=edit&redlink=1
  72. http://www.scholarpedia.org/w/index.php?title=markov_decision_processes&action=edit&redlink=1
  73. http://www.scholarpedia.org/article/neuroeconomics
  74. http://www.scholarpedia.org/article/optimal_control
  75. http://www.scholarpedia.org/w/index.php?title=id24&action=edit&redlink=1
  76. http://www.scholarpedia.org/article/reinforcement_learning
  77. http://www.scholarpedia.org/article/rescorla-wagner_model
  78. http://www.scholarpedia.org/article/reward
  79. http://www.scholarpedia.org/article/reward_signals
  80. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
  81. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&oldid=25755
  82. http://www.scholarpedia.org/article/user:anonymous
  83. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&oldid=25755
  84. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&oldid=186927
  85. http://www.scholarpedia.org/article/special:categories
  86. http://www.scholarpedia.org/article/category:machine_learning
  87. http://www.scholarpedia.org/article/category:reinforcement_learning
  88. http://www.scholarpedia.org/w/index.php?title=special:userlogin&returnto=temporal+difference+learning
  89. http://www.scholarpedia.org/article/temporal_difference_learning
  90. http://www.scholarpedia.org/article/talk:temporal_difference_learning
  91. http://www.scholarpedia.org/article/temporal_difference_learning
  92. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&action=edit
  93. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&action=history
  94. http://www.scholarpedia.org/article/main_page
  95. http://www.scholarpedia.org/article/scholarpedia:about
  96. http://www.scholarpedia.org/article/special:proposearticle
  97. http://www.scholarpedia.org/article/scholarpedia:instructions_for_authors
  98. http://www.scholarpedia.org/article/special:random
  99. http://www.scholarpedia.org/article/help:frequently_asked_questions
 100. http://www.scholarpedia.org/article/scholarpedia:help
 101. http://blog.scholarpedia.org/
 102. http://www.scholarpedia.org/article/encyclopedia:astrophysics
 103. http://www.scholarpedia.org/article/encyclopedia:celestial_mechanics
 104. http://www.scholarpedia.org/article/encyclopedia:computational_neuroscience
 105. http://www.scholarpedia.org/article/encyclopedia:computational_intelligence
 106. http://www.scholarpedia.org/article/encyclopedia:dynamical_systems
 107. http://www.scholarpedia.org/article/encyclopedia:physics
 108. http://www.scholarpedia.org/article/encyclopedia:touch
 109. http://www.scholarpedia.org/article/scholarpedia:topics
 110. http://www.scholarpedia.org/article/special:recentlypublished
 111. http://www.scholarpedia.org/article/special:recentlysponsored
 112. http://www.scholarpedia.org/article/special:recentchanges
 113. http://www.scholarpedia.org/article/special:allpages
 114. http://www.scholarpedia.org/article/special:listcurators
 115. http://www.scholarpedia.org/article/special:listusers
 116. http://www.scholarpedia.org/article/special:journal
 117. http://www.scholarpedia.org/article/special:whatlinkshere/temporal_difference_learning
 118. http://www.scholarpedia.org/article/special:recentchangeslinked/temporal_difference_learning
 119. http://www.scholarpedia.org/article/special:specialpages
 120. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&printable=yes
 121. http://www.scholarpedia.org/w/index.php?title=temporal_difference_learning&oldid=186927
 122. https://twitter.com/scholarpedia
 123. https://plus.google.com/112873162496270574424
 124. http://www.facebook.com/scholarpedia
 125. http://www.linkedin.com/groups/scholarpedia-4647975/about
 126. http://www.mediawiki.org/
 127. http://www.mathjax.org/
 128. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 129. http://www.scholarpedia.org/article/temporal_difference_learning
 130. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
 131. http://www.scholarpedia.org/article/scholarpedia:terms_of_use
 132. http://www.scholarpedia.org/article/scholarpedia:privacy_policy
 133. http://www.scholarpedia.org/article/scholarpedia:about
 134. http://www.scholarpedia.org/article/scholarpedia:general_disclaimer

   hidden links:
 136. http://www.scholarpedia.org/article/temporal_difference_learning
 137. http://www.scholarpedia.org/article/temporal_difference_learning
 138. http://www.scholarpedia.org/article/main_page
