   #[1]scholarpedia (en) [2]scholarpedia atom feed

id189

   from scholarpedia
                                    jan peters (2010), scholarpedia, 5(11):3698.
   [3]doi:10.4249/scholarpedia.3698 revision #137199 [[4]link to/cite this
   article]
   jump to: [5]navigation, [6]search
   post-publication activity
   (button)

   curator: [7]jan peters
   contributors:


   0.33 -

   [8]eugene m. izhikevich
   0.33 -

   [9]nick orbeck

   [10]christoph kolodziejski

   [11]benjamin bronner
     * [12]prof. jan peters, max-planck institute, germany & university of
       southern california, usc

   id189 are a type of [13]id23
   techniques that rely upon optimizing parametrized policies with respect
   to the expected return (long-term cumulative [14]reward) by gradient
   descent. they do not suffer from many of the problems that have been
   marring traditional [15]id23 approaches such as the
   lack of guarantees of a value function, the intractability problem
   resulting from uncertain state information and the [16]complexity
   arising from continuous states & actions.

contents

     * [17]1 introduction
     * [18]2 assumptions and notation
     * [19]3 approaches to policy gradient estimation
          + [20]3.1 finite-difference methods
          + [21]3.2 likelihood ratio methods and reinforce
          + [22]3.3 natural policy gradients
     * [23]4 conclusion
     * [24]5 references
     * [25]6 see also

introduction

   [26]id23 is probably the most general framework in
   which reward-related learning problems of animals, humans or machine
   can be phrased. however, most of the methods proposed in the
   id23 community are not yet applicable to many
   problems such as robotics, motor control, etc. this inapplicability may
   result from problems with uncertain state information. thus, those
   systems need to be modeled as partially observable [27]markov decision
   problems which often results in excessive computational demands. most
   traditional [28]id23 methods have no convergence
   guarantees and there exist even divergence examples. continuous states
   and actions in high dimensional spaces cannot be treated by most
   off-the-shelf [29]id23 approaches.

   id189 differ significantly as they do not suffer from
   these problems in the same way. for example, uncertainty in the state
   might degrade the performance of the policy (if no additional state
   estimator is being used) but the [30]optimization techniques for the
   policy do not need to be changed. continuous states and actions can be
   dealt with in exactly the same way as discrete ones while, in addition,
   the learning performance is often increased. convergence at least to a
   local optimum is guaranteed.

   the advantages of id189 for real world applications
   are numerous. among the most important ones are that the policy
   representations can be chosen so that it is meaningful for the task and
   can incorporate domain knowledge, that often fewer parameters are
   needed in the learning process than in value-function based approaches
   and that there is a variety of different [31]algorithms for policy
   gradient estimation in the literature which have a rather strong
   theoretical underpinning. additionally, id189 can be
   used either model-free or model-based as they are a generic
   formulation.

   of course, policy gradients are not the salvation to all problems but
   also have significant problems. they are by definition on-policy (note
   that tricks like importance sampling can slightly alleviate this
   problem) and need to forget data very fast in order to avoid the
   introduction of a bias to the gradient estimator. hence, the use of
   sampled data is not very efficient. in tabular representations, value
   function methods are guaranteed to converge to a global maximum while
   policy gradients only converge to a local maximum and there may be many
   maxima in discrete problems. id189 are often quite
   demanding to apply, mainly because one has to have considerable
   knowledge about the system one wants to control to make reasonable
   policy definitions. finally, id189 always have an
   open parameter, the learning rate, which may decide over the order of
   magnitude of the speed of convergence, these have led to new approaches
   inspired by expectation-maximization (see, e.g., vlassis et al., 2009;
   kober & peters, 2008).

   nevertheless, due to their advantages stated above, policy gradient
   methods have become particularly interesting for robotics applications
   as these have both continuous actions and states. for example, there
   has been a series of successful applications in robot locomotion, where
   good policy parametrizations such as cpgs are known. benbrahim &
   franklin (1997) already explored 2d dynamic biped walking, tedrake et
   al. (2004) extended these results to 3d passive dynamics-based walkers
   and endo (2005) showed that a full-body gait with sensory feedback can
   be learned with policy gradients. kohl & stone (2004) were able to
   apply policy gradients to optimize quadruped gaits. there have also
   been various applications in skill learning starting with the
   peg-in-a-hole tasks learned by gullapalli et al. (1994) and ranging to
   peters & schaals' optimizations of discrete movements primitives such
   as t-ball swings.

   note that in most applications, there exist many local maxima; for
   example, if we were told build a high jumping robot, there is a
   multitude of styles. current id189 would be helpful
   for improving a jumping style of a teacher, let's say the classical
   straddle jump. however, discovering a fosbery flop when starting with a
   basic straddle jump policy is probably not possible with policy
   gradient methods.

assumptions and notation

   we assume that we can model the control system in a discrete-time
   manner and we will denote the current time step by \(k\ .\) in order to
   take possible stochasticity of the plant into account, we denote it
   using a id203 distribution \(\mathbf{x}_{k+1}\sim p\left(
   \mathbf{x} _{k+1}\left\vert \mathbf{x}_{k},\mathbf{u}_{k}\right.
   \right) \) as model where \(\mathbf{u}_{k}\in\mathbb{r}^{m}\) denotes
   the current action, and \(\mathbf{x}_{k}\ ,\)
   \(\mathbf{x}_{k+1}\in\mathbb{r}^{n}\) denote the current and next
   state, respectively. we furthermore assume that actions are generated
   by a policy \(\mathbf{u}_{k} \sim\pi_{\mathbf{\theta}}\left(
   \mathbf{u}_{k}\left\vert \mathbf{x} _{k}\right. \right) \) which is
   modeled as a id203 distribution in order to incorporate
   exploratory actions; for some special problems, the optimal solution to
   a control problem is actually a stochastic controller (sutton,
   mcallester, singh, and mansour, 2000). the policy is assumed to be
   parameterized by \( k \) policy parameters \(\mathbf{\theta}
   \in\mathbb{r}^{k}\ .\)the sequence of states and actions forms a
   [32]trajectory denoted by
   \(\mathbf{\tau}=[\mathbf{x}_{0:h},\mathbf{u}_{0:h}]\) where \(h\)
   denotes the horizon which can be infinite. in this article, we will use
   the words trajectory, history, trial, or roll-out interchangeably. at
   each instant of time, the learning system receives a reward denoted by
   \(r_{k} = r\left( \mathbf{x} _{k},\mathbf{u}_{k}\right) \in\mathbb{r}\
   .\)

   the general goal of policy optimization in id23 is to
   optimize the policy parameters \(\mathbf{\theta}\in\mathbb{r}^{k}\) so
   that the expected return

                 \( j\left( \mathbf{\theta}\right) =e\left\{
                \sum\nolimits_{k=0}^{h}a_{k} r_{k}\right\} \)

   is optimized where \(a_{k}\) denote time-step dependent weighting
   factors, often set to \(a_{k}=\gamma^{k}\) for discounted reinforcement
   learning (where \(\gamma\) is in \([0,1]\)) or \(a_{k}=1/h\) for the
   average reward case.

   for real-world applications, we require that any change to the policy
   parameterization has to be smooth as drastic changes can be hazardous
   for the actor as well as useful initializations of the policy based on
   domain knowledge would otherwise vanish after a single update step. for
   these reasons, id189 which follow the steepest
   descent on the expected return are the method of choice. these methods
   update the policy parameterization according to the gradient update
   rule

        \( \mathbf{\theta}_{h+1}=\mathbf{\theta}_{h}+\alpha_{h}\left.
               \mathbf{\nabla }_{\mathbf{\theta}}j\right\vert
                 _{\mathbf{\theta}=\mathbf{\theta}_{h}}, \)

   where \(\alpha_{h}\in\mathbb{r}^{+}\) denotes a learning rate and
   \(h\in\{0,1,2,\ldots\}\) the current update number.

   the time step \(k\) and update number \(h\) are two different
   variables. in actor-critic-based id189, the frequency
   of updates of \(h\) can be nearly as high as of \(k\ .\) however, in
   most episodic methods, the policy update \(h\) will be significantly
   less frequent. here, cut-off allows updates before the end of the
   episode (for \(a_{k}=\gamma^{k}\ ,\) it is obvious that there comes a
   point where any future reward becomes irrelevant; a generically good
   cut-off point). if the gradient estimate is unbiased and learning rates
   fulfill \(\sum\textstyle_{h=0}^{\infty}\alpha_{h}>0\) and
   \(\sum\textstyle_{h=0}^{\infty}\alpha_{h}^{2}=\textrm{const}\ ,\) the
   learning process is guaranteed to converge at least to a local minimum.

   the main problem in id189 is to obtain a good
   estimator of the policy gradient \(\left.
   \mathbf{\nabla}_{\mathbf{\theta}}j\right\vert
   _{\mathbf{\theta}=\mathbf{\theta}_{h}}\ .\) in robotics and control,
   people have traditionally used deterministic model-based methods for
   obtaining the gradient (jacobson & mayne, 1970; dyer & mcreynolds,
   1970; hasdorff, 1976). however, in order to become autonomous we cannot
   expect to be able to model every detail of the system. therefore, we
   need to estimate the policy gradient simply from data generated during
   the execution of a task, i.e., without the need for a model. in this
   section, we will study different approaches and discuss their
   properties.

approaches to policy gradient estimation

   the literature on id189 has yielded a variety of
   estimation methods over the last years. the most prominent approaches,
   which have been applied to robotics are finite-difference and
   likelihood ratio methods, better known as reinforce in reinforcement
   learning.

finite-difference methods

   finite-difference methods are among the oldest policy gradient
   approaches; they originated from the stochastic simulation community
   and are quite straightforward to understand. the policy
   parameterization is varied \(i\) times by small increments
   \(\mathbf{\delta\theta}_{i}, \, i=1\ldots i\) and for each policy
   parameter variation \(\mathbf{\theta}_{h}+\mathbf{\delta\theta}_{i}\)
   roll-outs (or trajectories) are performed which generate estimates
   \(\delta\hat{j}_{i}\approx j(\mathbf{\theta
   }_{h}+\mathbf{\delta\theta}_{i})-j_{\text{ref}}\) of the expected
   return. there are different ways of choosing the reference value
   \(j_{\text{ref}}\ ,\) e.g. forward-difference estimators with
   \(j_{\text{ref}}=j(\mathbf{\theta}_{h})\) and central-difference
   estimators with
   \(j_{\text{ref}}=j(\mathbf{\theta}_{h}-\mathbf{\delta\theta}_{i})\ .\)
   the policy gradient estimate \(\mathbf{g}_{\text{fd}}\approx\left.
   \mathbf{\nabla}_{\mathbf{\theta}}j\right\vert_{\mathbf{\theta}
   =\mathbf{\theta}_{h}}\) can be estimated by regression yielding

                      \( \mathbf{g}_{\text{fd}}=\left(
           \mathbf{\delta\theta}^{t}\mathbf{\delta\theta }\right)
          ^{-1}\mathbf{\delta\theta}^{t}\mathbf{\delta\hat{j}}, \)

   where \(\mathbf{\delta\theta}
   =[\mathbf{\delta\theta}_{1},\ldots,\mathbf{\delta\theta}_{i}]^{t}\) and
   \(\mathbf{\delta\hat{j}}=[\delta\hat{j}_{1},\ldots
   ,\delta\hat{j}_{i}]^{t}\) denote the \(i\) samples. this approach can
   be highly efficient in simulation optimization of deterministic systems
   (spall, 2003) or when a common history of random numbers (glynn, 1987)
   is being used (the latter is known as pegasus in id23
   (ng & jordan, 2000)), and the error of the gradient estimate can get
   close to \(o\left( i^{-1/2}\right) \) (glynn, 1987). however, the
   uncertainty of real systems will result in stochasticity and an
   artificial common history of random numbers can no longer be applied.
   hence, when used on a real system, the performance degrades in a
   gradient estimate error ranging between \(o\left( i^{-1/4}\right) \) to
   \(o\left( i^{-2/5} \right) \) depending on the chosen reference value
   (glynn, 1987). an implementation of this algorithm is shown below.
input: policy parameterization \(\mathbf{\theta}_{h}\)
for \(i=1\) to  \(i\) do
    generate policy variation \(\mathbf{\delta\theta}_{i}\)
    estimate \(\hat{j}_{i}\approx j(\mathbf{\theta}_{h}+\mathbf{\delta\theta}_{i
})=\left\langle \sum\nolimits_{k=0}^{h} a_{k}r_{k}\right\rangle \) from roll-out
    estimate \(\hat{j}_{\text{ref}}\ ,\) e.g., \(\hat{j}_{\text{ref}}=j(\mathbf{
\theta}_{h}-\mathbf{\delta\theta}_{i})\) from roll-out
    compute \(\delta\hat{j}_{i}\approx j(\mathbf{\theta}_{h}+\mathbf{\delta\thet
a}_{i})-j_{\text{ref}}\)
end for
return gradient estimate \(\mathbf{g}_{\text{fd}} = \left(
\mathbf{\delta\theta}^{t}\mathbf{\delta\theta}\right)  ^{-1}\mathbf{\delta
\theta}^{t}\mathbf{\delta\hat{j}}\)

   note that an alternative implementation would keep increasing \(i\)
   until the gradient estimate converges. the choice of \(i\) can be
   essential; empirically it can be observed that taking \(i\) as twice
   the number of parameters yields very accurate gradient estimates.

   due to the simplicity of this approach, such methods have been
   successfully applied to numerous applications. however, the
   straightforward application is not without peril as the generation of
   the \(\mathbf{\delta\theta}_{i}\) requires proper knowledge of the
   system, as badly chosen \(\mathbf{\delta\theta}_{i}\) can destabilize
   the policy so that the system becomes instable and the gradient
   estimation process is prone to fail. if the parameters differ highly in
   scale, significant difficulties could be the consequences.

   advantages of this approach: finite-difference methods require very
   little skill and can usually be implemented out of the box. they work
   both with stochastic and deterministic policies without any change. it
   is highly efficient in simulation with a set of common histories of
   random numbers and on totally deterministic systems.

   disadvantages of this approach: the perturbation of the parameters is a
   very difficult problem often with disastrous impact on the learning
   process when the system goes [33]unstable. in the presence of noise on
   a real system, the gradient estimate error decreases much slower than
   for the following methods. performance depends highly on the chosen
   policy parametrization.

   sehnke et al. (2010) show several interesting newer methods developed
   in this domain.

likelihood ratio methods and reinforce

   likelihood ratio methods are driven by a different important insight.
   assume that trajectories \(\mathbf{\tau}\) are generated from a system
   by roll-outs, i.e., \(\mathbf{\tau}\sim p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right) =p\left( \left. \mathbf{\tau}\right\vert
   \mathbf{\theta}\right) \) with return
   \(r(\mathbf{\tau})=\sum\textstyle_{k=0}^{h}a_{k}r_{k}\) which leads to
   \(j(\theta) = e\left\{ r(\tau) \right\} = \int_{\mathbb{t}}
   p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
   r(\mathbf{\tau})d\mathbf{\tau} \ .\) in this case, the policy gradient
   can be estimated using the likelihood ratio (see e.g. glynn, 1987;
   aleksandrov, sysoyev, and shemeneva, 1968) better known as reinforce
   (williams, 1992) trick, i.e., by using

        \( \mathbf{\nabla}_{\mathbf{\theta}}p_{\mathbf{\theta}}\left(
    \mathbf{\tau}\right) = p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
      \mathbf{\nabla }_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
                           \mathbf{\tau}\right) \)

   from standard id128 (\( \mathbf{\nabla
   }_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
   =\mathbf{\nabla}_{\mathbf{\theta}}p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right)/p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
   \)), we obtain

    \( \mathbf{\nabla}_{\mathbf{\theta}}j\left( \mathbf{\theta}\right) =
   \int_{\mathbb{t}}\mathbf{\nabla}_{\mathbf{\theta}}p_{\mathbf{\theta}}\l
         eft( \mathbf{\tau}\right) r(\mathbf{\tau})d\mathbf{\tau} =
      \int_{\mathbb{t}} p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
      \mathbf{\nabla }_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
       \mathbf{\tau}\right) r(\mathbf{\tau})d\mathbf{\tau} = e\left\{
      \mathbf{\nabla }_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
              \mathbf{\tau}\right) r(\mathbf{\tau})\right\}. \)

   as the expectation \(e\{\cdot\}\) can be replaced by sample averages,
   denoted by \(\langle\cdot\rangle\ ,\) only the derivative
   \(\mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right) \) is needed for the estimator. importantly, this
   derivative can be computed without knowledge of the generating
   distribution \(p_{\mathbf{\theta}}\left( \mathbf{\tau}\right) \) as

                        \( p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right)=p(\mathbf{x}_{0})\prod\nolimits_{k=0} ^{h}p\left(
   \mathbf{x}_{k+1}\left\vert \mathbf{x}_{k},\mathbf{u}_{k}\right. \right)
       \pi_{\mathbf{\theta}}\left( \mathbf{u}_{k}\left\vert \mathbf{x}
                           _{k}\right. \right) \)

   implies that

     \( \mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
                            \mathbf{\tau }\right)
        =\sum\nolimits_{k=0}^{h}\mathbf{\nabla}_{\mathbf{\theta}}\log
            \pi_{\mathbf{\theta}}\left( \mathbf{u}_{k}\left\vert
                      \mathbf{x}_{k}\right. \right) \)

   as only the policy depends on \(\mathbf{\theta}\ .\)

   thus, the derivatives of \(p\left( \mathbf{x}_{k+1}\left\vert
   \mathbf{x}_{k},\mathbf{u}_{k}\right. \right)\) do not have to be
   computed and no model needs to be maintained. however, if we had a
   deterministic policy \(\mathbf{u}=\pi(\mathbf{x})\) instead of a
   stochastic policy \(\mathbf{u}\sim\pi(\mathbf{u}|\mathbf{x})\ ,\)
   computing such a derivative would require the derivative
   \(\mathbf{\nabla}_{\mathbf{\theta}}\log p\left(
   \mathbf{x}_{k+1}\left\vert \mathbf{x}_{k},\mathbf{u}_{k}\right. \right)
   = \mathbf{\nabla}_{\mathbf{u}_k}\log p\left( \mathbf{x}_{k+1}\left\vert
   \mathbf{x}_{k},\mathbf{u}_{k}\right.\right)
   \mathbf{\nabla}_{\mathbf{\theta}}
   \pi_{\mathbf{\theta}}\left(\mathbf{x}_{k}\right) \) to compute \(
   \mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right) \) and, hence, it would require a system model.

   in order to reduce the variance of the gradient estimator, a constant
   baseline can be subtracted from the gradient, i.e.,

     \( \mathbf{\nabla}_{\mathbf{\theta}}j\left( \mathbf{\theta}\right)
               =e\left\{ \mathbf{\nabla}_{\mathbf{\theta}}\log
           p_{\mathbf{\theta}}\left( \mathbf{\tau }\right) \left(
                   r(\mathbf{\tau})-b\right) \right\} , \)

   where the baseline \(b\in\mathbb{r}\) can be chosen arbitrarily
   (williams, 1992). it is straightforward to show that this baseline does
   not introduce bias in the gradient as differentiating \(
   \int_{\mathbb{t}} p_{\mathbf{\theta}}\left( \mathbf{\tau}\right)
   d\mathbf{\tau}=1 \) implies that

            \( \int_{\mathbb{t}}\mathbf{\nabla}_{\mathbf{\theta}
   }p_{\mathbf{\theta}}\left( \mathbf{\tau}\right) d\mathbf{\tau}=0 \ ,\)

   and, hence, the constant baseline will vanish for infinite data while
   reducing the variance of the gradient estimator for finite data. see
   peters & schaal, 2008 for an overview of how to choose the baseline
   optimally. therefore, the general path likelihood ratio estimator or
   episodic reinforce gradient estimator is given by

    \( \mathbf{g}_{\text{rf}}=\left\langle \left( \sum\nolimits_{k=0}^{h}
      \mathbf{\nabla}_{\mathbf{\theta}}\log\pi_{\mathbf{\theta}}\left(
    \mathbf{u}_{k}\left\vert \mathbf{x}_{k}\right. \right) \right) \left(
        \sum\nolimits_{l=0}^{h}a_{l}r_{l}-b\right) \right\rangle, \)

   where \(\left\langle \cdot\right\rangle \) denotes the average over
   trajectories. this type of method is guaranteed to converge to the true
   gradient at the fastest theoretically possible error decrease of
   \(o\left( i^{-1/2}\right) \) where \(i\) denotes the number of
   roll-outs (glynn, 1987) even if the data is generated from a highly
   stochastic system. an implementation of this algorithm will be shown
   below together with the estimator for the optimal baseline.
input: policy parameterization \(\mathbf{\theta}_{h}\)
repeat
    perform a trial and obtain \(\mathbf{x}_{0:h},\mathbf{u}_{0:h},r_{0:h}\)
    for each  gradient element \(g_{k}\)
         estimate optimal baseline
              \(b=\frac{\left\langle \left(  \sum\nolimits_{h=0}^{h} \mathbf{\na
bla}_{\theta_{k}}\log\pi_{\mathbf{\theta}}\left(  \mathbf{u}_{h}\left\vert \math
bf{x}_{h}\right.  \right)  \right)  ^{2}\sum\nolimits_{l=0}^{h}a_{l}r_{l}\right\
rangle }{\left\langle \left(
\sum\nolimits_{h=0}^{h}\mathbf{\nabla}_{\theta_{k}}\log\pi_{\mathbf{\theta}
}\left(  \mathbf{u}_{h}\left\vert \mathbf{x}_{h}\right.  \right)  \right)
^{2}\right\rangle }\)
         estimate the gradient element
              \(g_{k}=\left\langle \left(  \sum\nolimits_{h=0}^{h}\mathbf{\nabla
}_{\theta_{k}}\log\pi_{\mathbf{\theta}}\left(  \mathbf{u}_{h}\left\vert
\mathbf{x}_{h}\right.  \right)  \right)  \left(  \sum\nolimits_{l=0}^{h}
a_{l}r_{l}-b\right)  \right\rangle \)
    end for
until  gradient estimate \(\mathbf{g}_{\text{rf}}=[g_{1},\ldots,g_{k}]\) converg
ed
return gradient estimate \(\mathbf{g}_{\text{rf}}=[g_{1},\ldots,g_{k}]\)

   advantages of this approach: besides the theoretically faster
   convergence rate, likelihood ratio gradient methods have a variety of
   advantages in comparison to [34]finite difference methods when applied
   to robotics. as the generation of policy parameter variations is no
   longer needed, the complicated control of these variables can no longer
   endanger the gradient estimation process. furthermore, in practice,
   already a single roll-out can suffice for an unbiased gradient estimate
   viable for a good policy update step, thus reducing the amount of
   roll-outs needed. finally, this approach has yielded the most
   real-world robotics results (peters & schaal, 2008) and the likelihood
   ratio gradient is guaranteed to achieve the fastest convergence of the
   error for a stochastic system.

   disadvantages of this approach: when used with a deterministic policy,
   likelihood ratio gradients have to maintain a system model. such a
   model can be very hard to obtain for continuous states and actions,
   hence, the simpler finite difference gradients are often superior in
   this scenario. similarly, finite difference gradients can still be more
   useful than likelihood ratio gradients if the system is deterministic
   and very repetitive. also, the practical implementation of a likelihood
   ratio gradient method is much more demanding than the one of a finite
   difference method.

natural policy gradients

   one of the main reasons for using id189 is that we
   intend to do just a small change \(\mathbf{\delta\theta}\) to the
   policy \(\pi_{\mathbf{\theta}}\) while improving the policy. however,
   the meaning of small is ambiguous. when using the euclidian metric of
   \(\sqrt{\mathbf{\delta \theta}^{t}\mathbf{\delta\theta}}\ ,\) then the
   gradient is different for every parameterization \(\mathbf{\theta}\) of
   the policy \(\pi_{\mathbf{\theta}}\) even if these parameterization are
   related to each other by a linear transformation (kakade, 2002). this
   problem poses the question of how we can measure the closeness between
   the current policy and the updated policy based upon the distribution
   of the paths generated by each of these. in statistics, a variety of
   distance measures for the closeness of two distributions (e.g.,
   \(p_{\mathbf{\theta}}\left( \mathbf{\tau}\right) \) and
   \(p_{\mathbf{\theta}+\mathbf{\delta\theta}}\left( \mathbf{\tau}\right)
   \)) have been suggested, e.g., the id181
   \(d_{\text{kl}}\left(
   p_{\mathbf{\theta}},p_{\mathbf{\theta}+\mathbf{\delta\theta}}\right)\
   ,\) the hellinger distance \(d_{\text{hd}}\) and others (su & gibbs,
   2002). many of these distances (e.g., the previously mentioned ones)
   can be approximated by its second order taylor expansion, i.e., by

                           \( d_{\text{kl}}\left(
    p_{\mathbf{\theta}},p_{\mathbf{\theta}+\mathbf{\delta \theta}}\right)
     \approx\mathbf{\delta\theta}^{t}\mathbf{f}_{\mathbf{\theta} }\text{
                         }\mathbf{\delta\theta}, \)

   where

     \( \mathbf{f}_{\mathbf{\theta}}=\int_{\mathbb{t}}p_{\mathbf{\theta}
      }\left( \mathbf{\tau}\right) \nabla\log p_{\mathbf{\theta}}\left(
   \mathbf{\tau}\right) \nabla\log p_{\mathbf{\theta}}\left( \mathbf{\tau
   }\right) ^{t}d\mathbf{\tau}=\left\langle \nabla\log p_{\mathbf{\theta}
      }\left( \mathbf{\tau}\right) \nabla\log p_{\mathbf{\theta}}\left(
                  \mathbf{\tau}\right) ^{t}\right\rangle \)

   is known as the fisher-information matrix. let us now assume that we
   restrict the change of our policy to \( d_{\text{kl}}\left(
   p_{\mathbf{\theta}},p_{\mathbf{\theta}+\mathbf{\delta \theta}}\right)
   \approx\mathbf{\delta\theta}^{t}\mathbf{f}_{\mathbf{\theta} }\text{
   }\mathbf{\delta\theta} = \varepsilon, \) where \(\varepsilon\) needs to
   be a very small number (i.e., close to zero).

   in that case, the natural gradient is defined by amari (1998) as the
   update \(\mathbf{\delta\tilde{\theta}}\) that is most similar to the
   true gradient \(\mathbf{\nabla}_{\mathbf{\theta}}j\) while the change
   in our path distribution is limited to \(\varepsilon\ .\) hence, it is
   given by the program

                                     \(
   \operatorname{argmax}_{\mathbf{\delta{\theta}}}\mathbf{\delta{\theta}}^
      {t}\mathbf{\nabla }_{\mathbf{\theta}}j \quad \mathbf{s.t.} \quad
       \mathbf{\delta{\theta}}^{t}\mathbf{f} _{\mathbf{\theta}}\text{
                 }\mathbf{\delta{\theta}} = \varepsilon. \)

   the solution to this program is given by

                      \( \mathbf{\delta\theta} \propto
   \mathbf{f}_{\mathbf{\theta}}^{-1}\mathbf{\nabla}_{\mathbf{\theta}}j, \)

   where \(\mathbf{\nabla}_{\mathbf{\theta}}j\) denotes the regular
   likelihood ratio policy gradient from the previous section. the update
   step is unique up to a scaling factor, which is often subsumed into the
   learning rate. it can be interpreted as follows: determine the maximal
   improvement \(\mathbf{\delta{\theta}}\) of the policy for a constant
   fixed change of the policy
   \(\mathbf{\delta{\theta}}^{t}\mathbf{f}_{\mathbf{\theta}}\mathbf{\delta
   {\theta}}\ .\)

   as illustrated in figure 1, the natural gradient update in figure 1 (b)
   corresponds to a slightly rotated regular policy gradient update in
   figure 1 (a). it can be guaranteed that it is always turned by less
   than 90 degrees (amari, 1998), hence all convergence properties of the
   regular policy gradient transfer.
   figure 1: when plotting the expected return landscape for a simple
   problem such as an 1d linear quadratic regulation, the differences
   between regular (   vanilla   ) and natural policy gradients becomes
   apparent.

   this type of approach has its origin in supervised learning (amari,
   1998). it was first suggested in the context of id23
   by kakade (2002) and has been explored in greater depth in (bagnell &
   schneider, 2003; peters, vijayakumar & schaal, 2003, 2005; peters &
   schaal, 2008). the strongest theoretical advantage of this approach is
   that its performance no longer depends on the parameterization of the
   policy and is therefore safe to be used for arbitrary policies. hence,
   the regular policy gradient is sometimes referred to as a flavored or
   vanilla gradient, as it keeps the `vanilla flavor' of the policy.
   however, a well-chosen policy parametrization can sometimes results in
   better convergence of the policy gradient than the natural policy
   gradient. nevertheless, in practice, a learning process based on
   natural policy gradients often converges significantly faster for most
   practical cases. figure 1 gives an impression of the differences in the
   learning process: while the regular policy gradient often points to
   plateaus with little exploration, the natural gradient points to the
   optimal solution.

   one of the fastest general algorithms for estimating natural policy
   gradients which does not need complex parameterized baselines is the
   episodic natural actor critic. this algorithm, originally derived in
   (peters, vijayakumar & schaal, 2003), can be considered the `natural'
   version of reinforce with a baseline optimal for this gradient
   estimator. however, for steepest descent with respect to a metric, the
   baseline also needs to minimize the variance with respect to the same
   metric. in this case, we can minimize the whole [35]covariance matrix
   of the natural gradient estimate \(\mathbf{\delta\hat{\theta}}\) given
   by

                \( \mathbf{\sigma} =\operatorname{cov}\left\{
                    \mathbf{\delta\hat{\theta} }\right\}
               _{\mathbf{f}_{\mathbf{\theta}}}=e\left\{ \left(
               \mathbf{\delta\hat{\theta}-f}_{\mathbf{\theta}}
      ^{-1}\mathbf{g}_{\text{rf}}\left( b\right) \right) ^{t}\mathbf{f}
                          _{\mathbf{\theta}}\left(
               \mathbf{\delta\hat{\theta}-f}_{\mathbf{\theta}}
      ^{-1}\mathbf{g}_{\text{rf}}\left( b\right) \right) \right\} , \)

   with

      \(\mathbf{g}_{\text{rf}}\left( b\right) =\left\langle \nabla\log
        p_{\mathbf{\theta}}\left( \mathbf{\tau}\right) \left( r\left(
               \mathbf{\tau}\right) -b\right) \right\rangle \)

   being the reinforce gradient with baseline \(b\ .\) the re-weighting
   with the fisher information ensures that the best identifiable gradient
   components get the highest weight. as outlined in (peters & schaal,
   2008), it can be shown that the minimum-variance unbiased natural
   gradient estimator can be determined as shown below.
input: policy parameterization \(\mathbf{\theta}_{h}\)
repeat
    perform \(m\) trials and obtain \(\mathbf{x}_{0:h},\mathbf{u}_{0:h},r_{0:h}\
) for each trial
    obtain the sufficient statistics
         policy derivatives \(\mathbf{\psi}_{k}=\mathbf{\nabla}_{\theta}\log\pi_
{\mathbf{\theta}}\left(  \mathbf{u}_{k}\left\vert \mathbf{x}_{k}\right.  \right)
  \)
         fisher matrix \(\mathbf{f}_{\mathbf{\theta}}=\left\langle \left( \sum\n
olimits_{k=0}^{h}\mathbf{\psi}_{k}\right)\left(  \sum\nolimits_{l=0}^{h}\mathbf{
\psi}_{l}\right)  ^{t}\right\rangle\)
         vanilla gradient \(\mathbf{g}=\left\langle\left(\sum\nolimits_{k=0}^{h}
\mathbf{\psi}_{k}\right)  \left(\sum\nolimits_{l=0}^{h}a_{l}r_{l}\right)  \right
\rangle \)
         eligibility \(\mathbf{\phi}=\left\langle \left(\sum\nolimits_{k=0}^{h}\
mathbf{\psi}_{k}\right)  \right\rangle \)
         average reward \(\bar{r}=\left\langle\sum\nolimits_{l=0}^{h}a_{l}r_{l}\
right\rangle \)
    obtain natural gradient by computing
         baseline \(b=\mathbf{q}\left(  \bar {r}-\mathbf{\phi}^{t}\mathbf{f}_{\m
athbf{\theta}}^{-1}\mathbf{g}\right)  \)
              with \(\mathbf{q}=m^{-1}\left(  1+\mathbf{\phi}^{t}\left( m\mathbf
{f}_{\mathbf{\theta}}-\mathbf{\phi\phi}^{t}\right)^{-1}\mathbf{\phi}\right)  \)
         natural gradient \(\mathbf{g}_{\text{ng}}=\mathbf{f}_{\mathbf{\theta}}^
{-1}\left(  \mathbf{g}-\mathbf{\phi}b\right) \)
until  gradient estimate \(\mathbf{g}_{\text{ng}}=[g_{1},\ldots,g_{h}]\) converg
ed
return gradient estimate \(\mathbf{g}_{\text{ng}}=[g_{1},\ldots,g_{h}]\)

   for the derivation, see (peters & schaal, 2008).

   advantages of this approach: natural policy gradients differ in a
   deciding aspect from both finite difference gradients and regular
   likelihood ratio gradients, i.e., they are independent from the choice
   of policy parametrization if the choices have the same representational
   power. as a result, they can be an order of magnitude faster than the
   regular gradient. they also profit from most other advantages of the
   regular policy gradients.

   disadvantages of this approach: in comparison to the regular policy
   gradient, there are three disadvantages: first, the matrix inversion in
   the gradient estimators may be numerically brittle and may scale worse
   (note that there are tricks to alleviate this problem). second, if we
   can find a special policy parametrization that trivializes a problem,
   the natural policy gradient may not make use of it. third, the natural
   policy gradient estimators are often much harder to implement.

conclusion

   we have presented an quick overview on id189. while
   many details needed to be omitted and may be found in (peters & schaal,
   2008), this entry roughly represents the state of the art in policy
   gradient methods. all three major ways of estimating first order
   gradients, i.e., finite-difference gradients, vanilla policy gradients
   and natural policy gradients are discussed in this article and
   practical algorithms are given.

references

     * v. aleksandrov, v. sysoyev, and v. shemeneva, stochastic
       optimization, engineering cybernetics, vol. 5, pp. 11-16, 1968.

     * s. amari, natural gradient works efficiently in learning,
       [36]neural computation, vol. 10, 1998.

     * j. bagnell and j. schneider, covariant policy search, international
       joint article on artificial intelligence, 2003.

     * j. baxter and p. bartlett, direct gradient-based reinforcement
       learning, journal of artificial intelligence research, 1999.

     * h. benbrahim and j. franklin, biped dynamic walking using
       id23, robotics and autonomous systems, vol. 22,
       pp. 283   302, 1997.

     * a. berny, statistical machine learning and combinatorial
       optimization, in l. kallel, b. naudts, and a. rogers, editors,
       theoretical aspects of evolutionary computing, lecture notes in
       natural computing, 2000.

     * p. dyer and s. r. mcreynolds, the computation and theory of
       [37]optimal control. new york: academic press, 1970.

     * g. endo, j. morimoto, t. matsubara, j. nakanishi, and g. cheng,
       learning cpg sensory feedback with policy gradient for biped
       locomotion for a full-body humanoid, in aaai 2005, 2005.

     * r. fletcher and r. fletcher, practical methods of optimization.
       john wiley & sons, 2000.

     * p. glynn, likelihood ratio gradient estimation for stochastic
       systems, communications of the acm, vol. 33, no. 10, pp. 75-84,
       october 1990.

     * p. glynn, likelihood ratio gradient estimation: an overview, in
       proceedings of the 1987 winter simulation conference, atlanta, ga,
       1987, pp. 366--375.

     * e. greensmith, p. bartlett, and j. baxter, variance reduction
       techniques for gradient estimates in id23,
       advances in neural information processing systems, 2001.

     * v. gullapalli, associative id23 of real-value
       functions, smc, 1991.

     * v. gullapalli, j. franklin, and h. benbrahim, acquiring robot
       skills via id23, ieee control systems, vol. 14,
       no. 39, 1994.

     * l. hasdorff, gradient optimization and nonlinear control. john
       wiley & sons, 1976.

     * d. h. jacobson and d. q. mayne, differential id145.
       new york: american elsevier publishing company, inc., 1970.

     * j. kober and j. peters, policy search for motor primitives in
       robotics, advances in neural information processing systems 22
       (nips), cambridge, ma: mit press, 2008.

     * n. kohl and p. stone, policy gradient id23 for
       fast quadrupedal locomotion, in proceedings of the ieee
       international conference on robotics and automation, new orleans,
       la, may 2004.

     * g. lawrence, n. cowan, and s. russell, efficient gradient
       estimation for motor control learning, in proc. uai-03, acapulco,
       mexico, 2003.

     * a. y. ng and m. jordan, pegasus: a policy search method for large
       mpds and pomdps, in proceedings of the sixteenth conference on
       uncertainty in artificial intelligence, 2000.

     * j. peters and s. schaal, (2008). "natural actor critic,"
       neurocomputing, 71, 7-9, pp.1180-1190.

     * j. peters and s. schaal, (2008). "id23 of motor
       skills with policy gradients," neural networks, 21, 4, pp.682-97.

     * j. peters, s. vijayakumar, and s. schaal, id23
       for humanoid robotics, in ieee/rsj international conference on
       humanoid robotics, 2003.

     * j. peters, s. vijayakumar, and s. schaal, natural actor-critic, in
       proceedings of the european machine learning conference (ecml)},
       2005.

     * f. sehnke, c. osendorfer, t. r  ckstiess, a. graves, j. peters, j.
       schmidhuber. parameter-exploring policy gradients. neural networks
       23(2), 2010.

     * j. c. spall, introduction to stochastic search and optimization:
       estimation, simulation, and control. hoboken, nj: wiley, 2003.

     * f. su and a. gibbs, on choosing and bounding id203 metrics,
       international statistical review, vol. 70, no. 3, pp. 419-435,
       2002.

     * r. sutton, d. mcallester, s. singh, and y. mansour, policy gradient
       methods for id23 with function approximation,
       advances in neural information processing systems, 2000.

     * r. tedrake, t. w. zhang, and h. s. seung. stochastic policy
       gradient id23 on a simple 3d biped. in
       proceedings of the ieee international conference on intelligent
       robots and systems (iros), vol. 3, pages 2849-2854, sendai, japan,
       september 2004.

     * r. j. williams, simple statistical gradient-following algorithms
       for connectionist id23, machine learning, vol. 8,
       no. 23, 1992.

     * n. vlassis, m. toussaint, g. kontes, and s. piperidis. learning
       model-free robot control by a monte carlo em algorithm. autonomous
       robots, 27(2):123-130, 2009.

see also

   [38]id23
   sponsored by: [39]eugene m. izhikevich, editor-in-chief of
   scholarpedia, the peer-reviewed open-access encyclopedia
   [40]reviewed by: [41]anonymous
   [42]reviewed by: [43]dr. christoph kolodziejski, max planck institute
   for dynamics and self-organization, g  ttingen, germany
   accepted on: [44]2010-10-12 14:11:57 gmt
   retrieved from
   "[45]http://www.scholarpedia.org/w/index.php?title=policy_gradient_meth
   ods&oldid=137199"
   [46]categories:
     * [47]computational intelligence
     * [48]machine learning
     * [49]pattern recognition
     * [50]id23

personal tools

     * [51]log in / create account

namespaces

     * [52]page
     * [53]discussion

variants

views

     * [54]read
     * [55]view source
     * [56]view history

actions

search

   ____________________ (button) search

navigation

     * [57]main page
     * [58]about
     * [59]propose a new article
     * [60]instructions for authors
     * [61]random article
     * [62]faqs
     * [63]help
     * [64]blog

focal areas

     * [65]astrophysics
     * [66]celestial mechanics
     * [67]computational neuroscience
     * [68]computational intelligence
     * [69]dynamical systems
     * [70]physics
     * [71]touch
     * [72]more topics

activity

     * [73]recently published articles
     * [74]recently sponsored articles
     * [75]recent changes
     * [76]all articles
     * [77]list all curators
     * [78]list all users
     * [79]scholarpedia journal

tools

     * [80]what links here
     * [81]related changes
     * [82]special pages
     * [83]printable version
     * [84]permanent link

     * [85][twitter.png?303]
     * [86][gplus-16.png]
     * [87][facebook.png?303]
     * [88][linkedin.png?303]

     * [89]powered by mediawiki [90]powered by mathjax [91]creative
       commons license

     * this page was last modified on 28 october 2013, at 12:59.
     * this page has been accessed 144,435 times.
     * "id189" by [92]jan peters is licensed under a
       [93]creative commons attribution-noncommercial-sharealike 3.0
       unported license. permissions beyond the scope of this license are
       described in the [94]terms of use

     * [95]privacy policy
     * [96]about scholarpedia
     * [97]disclaimers

references

   visible links
   1. http://www.scholarpedia.org/w/opensearch_desc.php
   2. http://www.scholarpedia.org/w/index.php?title=special:recentchanges&feed=atom
   3. http://dx.doi.org/10.4249/scholarpedia.3698
   4. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&action=cite&rev=137199
   5. http://www.scholarpedia.org/article/policy_gradient_methods#mw-head
   6. http://www.scholarpedia.org/article/policy_gradient_methods#p-search
   7. http://www.scholarpedia.org/article/user:jan_peters
   8. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
   9. http://www.scholarpedia.org/article/user:nick_orbeck
  10. http://www.scholarpedia.org/article/user:christoph_kolodziejski
  11. http://www.scholarpedia.org/article/user:benjamin_bronner
  12. http://www.scholarpedia.org/article/user:jan_peters
  13. http://www.scholarpedia.org/article/reinforcement_learning
  14. http://www.scholarpedia.org/article/reward
  15. http://www.scholarpedia.org/article/reinforcement_learning
  16. http://www.scholarpedia.org/article/complexity
  17. http://www.scholarpedia.org/article/policy_gradient_methods#introduction
  18. http://www.scholarpedia.org/article/policy_gradient_methods#assumptions_and_notation
  19. http://www.scholarpedia.org/article/policy_gradient_methods#approaches_to_policy_gradient_estimation
  20. http://www.scholarpedia.org/article/policy_gradient_methods#finite-difference_methods
  21. http://www.scholarpedia.org/article/policy_gradient_methods#likelihood_ratio_methods_and_reinforce
  22. http://www.scholarpedia.org/article/policy_gradient_methods#natural_policy_gradients
  23. http://www.scholarpedia.org/article/policy_gradient_methods#conclusion
  24. http://www.scholarpedia.org/article/policy_gradient_methods#references
  25. http://www.scholarpedia.org/article/policy_gradient_methods#see_also
  26. http://www.scholarpedia.org/article/reinforcement_learning
  27. http://www.scholarpedia.org/w/index.php?title=markov_decision_processes&action=edit&redlink=1
  28. http://www.scholarpedia.org/article/reinforcement_learning
  29. http://www.scholarpedia.org/article/reinforcement_learning
  30. http://www.scholarpedia.org/article/optimization
  31. http://www.scholarpedia.org/article/algorithm
  32. http://www.scholarpedia.org/article/dynamical_systems
  33. http://www.scholarpedia.org/article/stability
  34. http://www.scholarpedia.org/article/finite_difference_method
  35. http://www.scholarpedia.org/article/covariance
  36. http://www.scholarpedia.org/article/neuron
  37. http://www.scholarpedia.org/article/optimal_control
  38. http://www.scholarpedia.org/article/reinforcement_learning
  39. http://www.scholarpedia.org/article/user:eugene_m._izhikevich
  40. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&oldid=82031
  41. http://www.scholarpedia.org/article/user:anonymous
  42. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&oldid=82031
  43. http://www.scholarpedia.org/article/user:christoph_kolodziejski
  44. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&oldid=82031
  45. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&oldid=137199
  46. http://www.scholarpedia.org/article/special:categories
  47. http://www.scholarpedia.org/article/category:computational_intelligence
  48. http://www.scholarpedia.org/article/category:machine_learning
  49. http://www.scholarpedia.org/article/category:pattern_recognition
  50. http://www.scholarpedia.org/article/category:reinforcement_learning
  51. http://www.scholarpedia.org/w/index.php?title=special:userlogin&returnto=policy+gradient+methods
  52. http://www.scholarpedia.org/article/policy_gradient_methods
  53. http://www.scholarpedia.org/article/talk:policy_gradient_methods
  54. http://www.scholarpedia.org/article/policy_gradient_methods
  55. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&action=edit
  56. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&action=history
  57. http://www.scholarpedia.org/article/main_page
  58. http://www.scholarpedia.org/article/scholarpedia:about
  59. http://www.scholarpedia.org/article/special:proposearticle
  60. http://www.scholarpedia.org/article/scholarpedia:instructions_for_authors
  61. http://www.scholarpedia.org/article/special:random
  62. http://www.scholarpedia.org/article/help:frequently_asked_questions
  63. http://www.scholarpedia.org/article/scholarpedia:help
  64. http://blog.scholarpedia.org/
  65. http://www.scholarpedia.org/article/encyclopedia:astrophysics
  66. http://www.scholarpedia.org/article/encyclopedia:celestial_mechanics
  67. http://www.scholarpedia.org/article/encyclopedia:computational_neuroscience
  68. http://www.scholarpedia.org/article/encyclopedia:computational_intelligence
  69. http://www.scholarpedia.org/article/encyclopedia:dynamical_systems
  70. http://www.scholarpedia.org/article/encyclopedia:physics
  71. http://www.scholarpedia.org/article/encyclopedia:touch
  72. http://www.scholarpedia.org/article/scholarpedia:topics
  73. http://www.scholarpedia.org/article/special:recentlypublished
  74. http://www.scholarpedia.org/article/special:recentlysponsored
  75. http://www.scholarpedia.org/article/special:recentchanges
  76. http://www.scholarpedia.org/article/special:allpages
  77. http://www.scholarpedia.org/article/special:listcurators
  78. http://www.scholarpedia.org/article/special:listusers
  79. http://www.scholarpedia.org/article/special:journal
  80. http://www.scholarpedia.org/article/special:whatlinkshere/policy_gradient_methods
  81. http://www.scholarpedia.org/article/special:recentchangeslinked/policy_gradient_methods
  82. http://www.scholarpedia.org/article/special:specialpages
  83. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&printable=yes
  84. http://www.scholarpedia.org/w/index.php?title=policy_gradient_methods&oldid=137199
  85. https://twitter.com/scholarpedia
  86. https://plus.google.com/112873162496270574424
  87. http://www.facebook.com/scholarpedia
  88. http://www.linkedin.com/groups/scholarpedia-4647975/about
  89. http://www.mediawiki.org/
  90. http://www.mathjax.org/
  91. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
  92. http://www.scholarpedia.org/article/policy_gradient_methods
  93. http://creativecommons.org/licenses/by-nc-sa/3.0/deed.en_us
  94. http://www.scholarpedia.org/article/scholarpedia:terms_of_use
  95. http://www.scholarpedia.org/article/scholarpedia:privacy_policy
  96. http://www.scholarpedia.org/article/scholarpedia:about
  97. http://www.scholarpedia.org/article/scholarpedia:general_disclaimer

   hidden links:
  99. http://www.scholarpedia.org/article/file:difference.png
 100. http://www.scholarpedia.org/article/file:difference.png
 101. http://www.scholarpedia.org/article/policy_gradient_methods
 102. http://www.scholarpedia.org/article/policy_gradient_methods
 103. http://www.scholarpedia.org/article/main_page
