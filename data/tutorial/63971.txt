   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id97 and fasttext id27 with gensim

   [16]go to the profile of           steeve huang
   [17]          steeve huang (button) blockedunblock (button) followfollowing
   feb 4, 2018
   [1*r_r38hj9nkbpyb0y7pkhrw.png]
   ([18]https://iwcollege-cdn-pull-zone-theisleofwightco1.netdna-ssl.com/w
   p-content/uploads/2017/05/2dgeravyrm.jpg)

   in natural language processing (nlp), we often map words into vectors
   that contains numeric values so that machine can understand it. word
   embedding is a type of mapping that allows words with similar meaning
   to have similar representation. this article will introduce two
   state-of-the-art id27 methods, id97 and fasttext with
   their implementation in gensim.
     __________________________________________________________________

traditional approach

   a traditional way of representing words is one-hot vector, which is
   essentially a vector with only one target element being 1 and the
   others being 0. the length of the vector is equal to the size of the
   total unique vocabulary in the corpora. conventionally, these unique
   words are encoded in alphabetical order. namely, you should expect the
   one-hot vectors for words starting with    a    with target    1    of lower
   index, while those for words beginning with    z    with target    1    of
   higher index.
   [1*zfzlwr9kmiut5fh9tpwd-q.png]
   [19]https://cdn-images-1.medium.com/max/1600/1*ulfyiwpkgwcecqyzedtl0g.p
   ng   

   though this representation of words is simple and easy to implement,
   there are several issues. first, you cannot infer any relationship
   between two words given their one-hot representation. for instance, the
   word    endure    and    tolerate   , although have similar meaning, their
   targets    1    are far from each other. in addition, sparsity is another
   issue as there are numerous redundant    0    in the vectors. this means
   that we are wasting a lot of space.we need a better representation of
   words to solve these issues.

id97

   id97 is an efficient solution to these problems, which leverages
   the context of the target words. essentially, we want to use the
   surrounding words to represent the target words with a neural network
   whose hidden layer encodes the word representation.

   there are two types of id97, skip-gram and continuous bag of words
   (cbow). i will briefly describe how these two methods work in the
   following paragraphs.

skip-gram

   for skip-gram, the input is the target word, while the outputs are the
   words surrounding the target words. for instance, in the sentence    i
   have a cute dog   , the input would be    a   , whereas the output is    i   ,
      have   ,    cute   , and    dog   , assuming the window size is 5. all the input
   and output data are of the same dimension and one-hot encoded. the
   network contains 1 hidden layer whose dimension is equal to the
   embedding size, which is smaller than the input/ output vector size. at
   the end of the output layer, a softmax activation function is applied
   so that each element of the output vector describes how likely a
   specific word will appear in the context. the graph below visualizes
   the network structure.
   [1*tbjqnqluyew-cgsofydioq.png]
   skip-gram
   ([20]https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count
   -word2veec/)

   the id27 for the target words can obtained by extracting the
   hidden layers after feeding the one-hot representation of that word
   into the network.

   with skip-gram, the representation dimension decreases from the
   vocabulary size (v) to the length of the hidden layer (n). furthermore,
   the vectors are more    meaningful    in terms of describing the
   relationship between words. the vectors obtained by subtracting two
   related words sometimes express a meaningful concept such as gender or
   verb tense, as shown in the following figure (dimensionality reduced).
   [1*jpnko5x0ii8pvdqyfo2z1q.png]
   visualize word vectors
   ([21]https://www.tensorflow.org/images/linear-relationships.png)

cbow

   continuous bag of words (cbow) is very similar to skip-gram, except
   that it swaps the input and output. the idea is that given a context,
   we want to know which word is most likely to appear in it.
   [1*udlfo8hgsx0a1nkkuf_n9q.png]
   cbow
   ([22]https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count
   -word2veec/)

   the biggest difference between skip-gram and cbow is that the way the
   word vectors are generated. for cbow, all the examples with the target
   word as target are fed into the networks, and taking the average of the
   extracted hidden layer. for example, assume we only have two sentences,
      he is a nice guy    and    she is a wise queen   . to compute the word
   representation for the word    a   , we need to feed in these two examples,
      he is nice guy   , and    she is wise queen    into the neural network and
   take the average of the value in the hidden layer. skip-gram only feed
   in the one and only one target word one-hot vector as input.

   it is claimed that skip-gram tends to do better in rare words.
   nevertheless, the performance of skip-gram and cbow are generally
   similar.

implementation

   i will show you how to perform id27 with gensim, a powerful
   nlp toolkit, and a ted talk dataset.

   first, we download the the dataset using urllib, extracting the
   subtitle from the file.

   iframe: [23]/media/6c747bff8142ca43fdf1991679981373?postid=a209c1d3e12c

   let   s take a look at what input_text variable stores, as partially
   shown in the following figure.
   [1*c_3boaohjrnunsbfpa2wmw.png]
   input_text

   clearly, there are some redundant information that is not helpful for
   us to understand the talk, such as the words describing sound in the
   parenthesis and the speaker   s name. we get rid of these words with
   regular expression.

   iframe: [24]/media/9903555c6c459bcf69cda8a31e6f9651?postid=a209c1d3e12c

   now, sentences_ted has been transformed into a two dimensional array
   with each element being a word. let   s print out the first and the
   second element.
   [1*cjsewup5vwygtlrc-arrwa.png]
   sentences_ted

   this is the form that is ready to be fed into the id97 model
   defined in gensim. id97 model can be easily trained with one line
   as the code below.

   iframe: [25]/media/cc042844c9ad57b0225d54c37c8896db?postid=a209c1d3e12c

     * sentences: the list of split sentences.
     * size: the dimensionality of the embedding vector
     * window: the number of context words you are looking at
     * min_count: tells the model to ignore words with total count less
       than this number.
     * workers: the number of threads being used
     * sg: whether to use skip-gram or cbow

   now, let   s try which words are most similar to the word    man   .

   iframe: [26]/media/0269793b863235b70d1787f4ebfe0869?postid=a209c1d3e12c

   [1*m7yb5uxdkgzznfguxzoqea.png]

   it appears words related to men/women/kid are most similar to    man   .

   although id97 successfully handles the issue posed by one-hot
   vector, it has several limitation. the biggest challenge is that it is
   not able to represent words that do not appear in the training dataset.
   even though using a larger training set that contains more vocabulary,
   some rare words used very seldom can never be mapped to vectors.

fasttext

   fasttext is an extension to id97 proposed by facebook in 2016.
   instead of feeding individual words into the neural network, fasttext
   breaks words into several id165s (sub-words). for instance, the
   tri-grams for the word apple is app, ppl, and ple (ignoring the
   starting and ending of boundaries of words). the id27 vector
   for apple will be the sum of all these id165s. after training the
   neural network, we will have id27s for all the id165s given
   the training dataset. rare words can now be properly represented since
   it is highly likely that some of their id165s also appears in other
   words. i will show you how to use fasttext with gensim in the following
   section.

implementation

   similar to id97, we only need one line to specify the model that
   trains the id27.

   iframe: [27]/media/8e5f4603667ca5248418e6d999d9f27d?postid=a209c1d3e12c

   let   s try it with the word gastroenteritis, which is rarely used and
   does not appear in the training dataset.

   iframe: [28]/media/dbb4c13f7233307529acdcfabea89a26?postid=a209c1d3e12c

   [1*gffoztsvowhr8sqcs-j0zq.png]

   even though the word gastroenteritis does not exist in the training
   dataset, it is still capable of figuring out this word is closely
   related to some medical terms. if we try this in the id97 defined
   previously, it would pop out error because such word does not exist in
   the training dataset. although it takes longer time to train a fasttext
   model (number of id165s > number of words), it performs better than
   id97 and allows rare words to be represented appropriately.

conclusion

   you have learned what id97 and fasttext are as well as their
   implementation with gensim toolkit. should you have any problem, feel
   free to leave a comment below. if you like this article, make sure you
   follow me on [29]twitter so you won   t miss out any great machine
   learning/ deep learning blog post!

     * [30]machine learning
     * [31]nlp
     * [32]id27s
     * [33]id97
     * [34]towards data science

   (button)
   (button)
   (button) 2.5k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of           steeve huang

[36]          steeve huang

   co-founder & cto @ rosetta.ai. kaggle competitions master. beng in cs @
   hkust github: [37]https://github.com/khuangaf linkedin:
   [38]https://goo.gl/7defvr

     (button) follow
   [39]towards data science

[40]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.5k
     * (button)
     *
     *

   [41]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [42]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a209c1d3e12c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/word-embedding-with-id97-and-fasttext-a209c1d3e12c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/word-embedding-with-id97-and-fasttext-a209c1d3e12c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_nrkdlqb8a8xi---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@huangkh19951228?source=post_header_lockup
  17. https://towardsdatascience.com/@huangkh19951228
  18. https://iwcollege-cdn-pull-zone-theisleofwightco1.netdna-ssl.com/wp-content/uploads/2017/05/2dgeravyrm.jpg
  19. https://cdn-images-1.medium.com/max/1600/1*ulfyiwpkgwcecqyzedtl0g.png
  20. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  21. https://www.tensorflow.org/images/linear-relationships.png
  22. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
  23. https://towardsdatascience.com/media/6c747bff8142ca43fdf1991679981373?postid=a209c1d3e12c
  24. https://towardsdatascience.com/media/9903555c6c459bcf69cda8a31e6f9651?postid=a209c1d3e12c
  25. https://towardsdatascience.com/media/cc042844c9ad57b0225d54c37c8896db?postid=a209c1d3e12c
  26. https://towardsdatascience.com/media/0269793b863235b70d1787f4ebfe0869?postid=a209c1d3e12c
  27. https://towardsdatascience.com/media/8e5f4603667ca5248418e6d999d9f27d?postid=a209c1d3e12c
  28. https://towardsdatascience.com/media/dbb4c13f7233307529acdcfabea89a26?postid=a209c1d3e12c
  29. https://twitter.com/steeve__huang
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/nlp?source=post
  32. https://towardsdatascience.com/tagged/word-embeddings?source=post
  33. https://towardsdatascience.com/tagged/id97?source=post
  34. https://towardsdatascience.com/tagged/towards-data-science?source=post
  35. https://towardsdatascience.com/@huangkh19951228?source=footer_card
  36. https://towardsdatascience.com/@huangkh19951228
  37. https://github.com/khuangaf
  38. https://goo.gl/7defvr
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/a209c1d3e12c/share/twitter
  45. https://medium.com/p/a209c1d3e12c/share/facebook
  46. https://medium.com/p/a209c1d3e12c/share/twitter
  47. https://medium.com/p/a209c1d3e12c/share/facebook
