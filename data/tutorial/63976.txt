community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   57
   57
   lars hulstaert
   october 19th, 2017
   machine learning

lda2vec: id27s in topic models

   learn more about lda2vec, a model that learns dense word vectors
   jointly with dirichlet-distributed latent document-level mixtures of
   topic vectors.

   [intro_to_lda2vec_jsiplp.jpg]

   this blog post will give you an introduction to lda2vec, a topic model
   [3]published by chris moody in 2016. lda2vec expands the id97
   model, [4]described by mikolov et al. in 2013, with topic and document
   vectors and incorporates ideas from both id27 and topic
   models.

   the general goal of a topic model is to produce interpretable document
   representations which can be used to discover the topics or structure
   in a collection of unlabelled documents. an example of such an
   interpretable id194 is: document x is 20% topic a,
   40% topic b and 40% topic c.

   today's post will start off by introducing id44
   (lda). lda is a probabilistic topic model and it treats documents as a
   bag-of-words, so you're going to explore the advantages and
   disadvantages of this approach first.

   on the other hand, lda2vec builds id194s on top of
   id27s. you'll learn more about id27s and why they
   are currently the preferred building block in natural language
   processing (nlp) models.

   finally, you'll learn more about the general idea behind lda2vec.

id44: introduction

   a topic model takes a collection of unlabelled documents and attempts
   to find the structure or topics in this collection. note that topic
   models often assume that word usage is correlated with topic occurence.
   you could, for example, provide a topic model with a set of news
   articles and the topic model will divide the documents in a number of
   clusters according to word usage.

   topic models are a great way to automatically explore and structure a
   large set of documents: they group or cluster documents based on the
   words that occur in them. as documents on similar topics tend to use a
   similar sub-vocabulary, the resulting clusters of documents can be
   interpreted as discussing different 'topics'.

   id44 (lda) is an example of a probabilistic
   topic model. what this exactly means, you'll learn in the following
   sections: you'll first come to understand how lda starts from a
   bag-of-words description to represent the different documents. then,
   you'll see how these representations are used to find the structure in
   the document collection.

bag-of-words

   traditionally, text documents are represented in nlp as a bag-of-words.

   this means that each document is represented as a fixed-length vector
   with length equal to the vocabulary size. each dimension of this vector
   corresponds to the count or occurrence of a word in a document. being
   able to reduce variable-length documents to fixed-length vectors makes
   them more amenable for use with a large variety of machine learning
   (ml) models and tasks (id91, classification, ...).

   bagwords

   the image above illustrates how a document is represented in a
   bag-of-word model: the word "document" has a count of 1, while the word
   "model" occurs twice in the text.

   although the bag-of-words results in a sparse and high-dimensional
   id194, good results on topic classification are often
   obtained if a lot of data is available. you can always read up on the
   recent [5]facebook paper on topic classification.

   a fixed-length id194 means you can easily feed
   documents with varying length into ml models (id166's, id92, random
   forests, ...). this allows you to perform id91 or topic
   classification on documents. the structural information of the document
   is removed and models have to discover which vector dimensions are
   semantically similar. mapping for example    feline    and    cat    on
   different dimensions is less intuitive, as the model is forced to learn
   the correlation between these different dimensions.

the lda model

   when training an lda model, you start with a collection of documents
   and each of these is represented by a fixed-length vector
   (bag-of-words). lda is a general machine learning (ml) technique, which
   means that it can also be used for other unsupervised ml problems where
   the input is a collection of fixed-length vectors and the goal is to
   explore the structure of this data.

   to implement an lda model, you first start by defining the number of
   'topics' that are present in your collection of documents. this sounds
   straight-forward, but is often less intuitive than it sounds if you are
   working with vast amounts of documents.

   training an lda model on $$n$$ documents with $$m$$ topics corresponds
   with finding the document and topic vectors that best explain the data.

   note that this tutorial will not cover the full theory behind lda in
   detail (see [6]this paper by blei et al. for that), as the focus is
   more getting the general idea across.

   assume that the vocabulary in the documents consists of $$v$$ words.
   lda_input

   caption:

   each of the $$n$$ documents wil be represented in the lda model by a
   vector of length $$m$$ that details which topics occur in that
   document. a document can consist of 75% being 'topic 1' and 25% being
   'topic 2'. often, lda results in document vectors with a lot of zeros,
   which means that there are only a limited number of topics occur per
   document. this corresponds with the idea that documents typically only
   talk about a limited number of topics. this significantly improves the
   human interpretability of these document vectors.

   each of the $$m$$ topics is represented by a vector of length $$v$$
   that details which words are likely to occur, given a document on that
   topic. so for topic 1, 'learning', 'modelling' and 'statistics' might
   be some of the most common words. this means that you could then say
   that this is the 'data science' topic. for topic 2, the words 'gpu',
   'compute' and 'storage' could be the most common words. you could
   interpret this as the 'computing' topic.

   the following image illustrates the lda model visually. the goal of the
   model is to find the topic and document vectors that explain the
   original bag-of-word representation of the different documents.
   lda_output

   caption:

   it is important to notice that you are relying on the assumption that
   the topic vectors will be interpretable, otherwise the output of the
   model is pretty much garbage. essentially, you are assuming that the
   model, given enough data, will figure out which words tend to co-occur,
   and will cluster them into distinct 'topics'.

   lda is a simple probabilistic model that tends to work pretty good. the
   document vectors are often sparse, low-dimensional and highly
   interpretable, highlighting the pattern and structure in documents. you
   have to determine a good estimate of the number of topics that occur in
   the collection of the documents. in addition, you have to manually
   assign a distinct nominator/   topic    to the different topic vector. as a
   bag-of-words model is used to represent the documents, lda can suffer
   from the same disadvantages as the bag-of-words model. the lda model
   learns a document vector that predicts words inside of that document
   while disregarding any structure or how these words interact on a local
   level.

id27s

   one of the problems of the bag-of-words representation is that the
   model is responsible for figuring out which dimensions in the document
   vectors are semantically related. one might imagine that leveraging
   information on how words are semantically correlated to each other will
   improve a model   s performance and this is exactly what id27s
   promise.

   with id27s, words are represented as fixed-length vectors or
   embeddings. several different models exist to construct embeddings, but
   they are all based on the distributional hypothesis. that means that "a
   word is characterised by the company it keeps".

   the goal of id27s is to capture semantic and syntactic
   regularities in language from large unsupervised sets of documents,
   such as wikipedia. words that occur in the same context are represented
   by vectors in close proximity to each other. id27s

        image taken from [7]"visualizing id27s with id167"

   the image above is a projection of the id27 space to 2d space
   using t-distributed stochastic neighbor embedding (id167). id167 is a
   id84 method that you can use to visualise
   high-dimensional data. the method takes the id27s as input,
   and projects them onto two-dimensional space which can be easily
   visualised in a plot. only a subsection of the word space is
   investigated, focussing on words close to 'teacher   . instead of
   representing words by uninformative dimensions in a vector, words can
   be represented by semantically correlated vectors using word
   embeddings.

   when using id27s, an ml model can leverage information from a
   large a collection of documents, also known as a "corpus", by embedding
   it in the vector representations. this is not possible with
   bag-of-words models, which can hurt model performance when not a lot of
   data is available. id27s lead to id194s
   that are not fixed-length anymore. instead, documents are represented
   by a variable-length sequence of word vector embeddings. while some
   deep learning techniques, such as long short-term memory (lstm)'s,
   convolutional nets with adaptive pooling, ..., are able to deal with
   variable length sequences, a lot of data is often necessary to properly
   train them.

id97

   as you read in the introduction, id97 is highly popular word
   embedding model, developed by mikolov et al. note that several other
   id27 models exist within the field of distributional
   semantics. although several tricks are required to obtain high-quality
   id27s, this tutorial will only focus on the core idea behind
   id97.

   the following training procedure is used in id97 to obtain the word
   embeddings.
    1. select a (pivot) word in the text. the context words of the current
       pivot word are the words that occur around the pivot word. this
       means that you're working within a fixed-length window of words.
       the combinations of the pivot and context words constitute a set of
       word-context pairs. the image below was taken from the [8]chris
       moody's blog on lda2vec. in this text fragment, 'awesome' is the
       pivot word and the words around it are taken as context words,
       resulting in 7 word-context pairs. pivot

       image taken from [9]"introducing our hybrid lda2vec algorithm"
    2. two variants of the id97 model exist: a. in the bag-of-words
       architecture (cbow) the pivot word is predicted based on a set of
       surrounding context words (i.e. given 'thank', 'such', 'you', 'top'
       the model has to predict 'awesome'). this is called a bag-of-words
       architecture as the order of context words does not matter. b. in
       the skip-gram architecture, the pivot word is used to predict the
       surrounding context words (i.e. given 'awesome' predict 'thank',
       'such', 'you', 'top' ). the following image depicts the two
       different id97 architectures. note that a relatively simple
       (two layer) neural model is used (compared to deep neural models in
       id161).
       id97

    image taken from [10]"efficient estimation of word representations in
                    vector space" (mikolov et al., 2013)

   by training the model on a large corpus, you will obtain word
   embeddings (the weights in the projection layer) that encode semantical
   information as well as some interesting properties: it is possible to
   perform vector arithmetic, such as $$king - man + woman = queen$$.

   word vectors are a useful representation compared to, for example, a
   simple one-hot encoded representation. they allow to encode statistical
   information from a large corpus into other models, such as topic
   classification or dialogue systems. the word vectors are often dense,
   high-dimensional and uninterpretable. consider the following example: [
   -0.65, -1.223, ..., -0.252, +3.2]. while in lda dimensions correspond
   approximately to topics, this is typically not the case for word
   vectors. each word is assigned a context-independent word vector. the
   semantic meaning of words is, however, highly dependent on context. the
   id97 model learns a word vector that predicts context words across
   different documents. as a result, document-specific information is
   mixed together in the id27s.

lda2vec

   inspired by id44 (lda), the id97 model is
   expanded to simultaneously learn word, document and topic vectors.

   lda2vec is obtained by modifying the skip-gram id97 variant. in the
   original skip-gram method, the model is trained to predict context
   words based on a pivot word. in lda2vec, the pivot word vector and a
   document vector are added to obtain a context vector. this context
   vector is then used to predict context words.

   in the next section, you will see how these document vectors are
   constructed and how they can be used similarly as document vectors in
   lda.

lda2vec architecture

   the idea of integrating context vectors in the id97 model is not a
   new idea. [11]paragraph vectors, for example, also explored this idea
   in order to learn fixed-length representations of variable-length text
   fragments. in their work, for each text fragment (size of a paragraph)
   a dense vector representation is learned, similar to the learned word
   vectors.

   the downside of this approach is that the context/paragraph vectors
   resemble typical word vectors, making them less interpretable as, for
   example, the output of lda.

   the lda2vec model goes one step beyond the paragraph vector approach by
   working with document-sized text fragments and decomposing the document
   vectors into two different components. in the same spirit as the lda
   model, a document vector is decomposed into a document weight vector
   and a topic matrix. the document weight vector represents the
   percentage of the different topics, whereas the topic matrix consists
   of the different topic vectors. a context vector is thus constructed by
   combining the different topic vectors that occur in a document.

   consider the following example: in the original id97 model, if the
   pivot word is 'french', then possible context words might be 'german',
   'dutch', 'english'. without any global (document-related) information,
   these would be the most plausible guesses.

   by providing an additional context vector in the lda2vec model, it is
   possible to make better guesses of context words.

   if the document vector is a combination of the    food    and    drinks   
   topics, then 'baguette', 'cheese' and 'wine' might be more suitable. if
   the document vector is similar to the 'city' and    geography    topics,
   then 'paris', 'lyon' and 'grenoble' might be more suitable.

   note that these topic vectors are learned in word space, which allows
   for easy interpretation: you simply look at the word vectors that are
   closest to the topic vectors. in addition, constraints are put on the
   document weight vectors, to obtain a sparse vector (similar to lda),
   instead of a dense vector. this enables easy interpretation of the
   topic content of different documents.

   in short, the end-result of the lda2vec is a set of sparse document
   weight vectors, as well as easily interpretable topic vectors.

   although the performance tends to be similar to traditional lda, using
   automatic differentiation methods makes the method scalable to very
   vast datasets. in addition, by combining the context vector and word
   vector, you obtain    specialised    word vectors, which can be used in
   other models (and might outperform more    generic    word vectors).

lda2vec libraries

   lda2vec is a fairly new and specialised nlp technique. as it builds on
   existing methods, any id97 implementation could be extended into
   lda2vec. chris moody implemented the method in chainer, but other
   automatic differentiation frameworks could also be used (cntk, theano,
   ...). a tensorflow implementation was also made publicly [12]available.

   an overview of the lda2vec python module can be found [13]here. as
   training lda2vec can be computationally intensive, gpu support is
   recommended for larger corpora. in addition, in order to speed up
   training, the different word vectors are often initialised with
   pre-trained id97 vectors.

   finally, lda2vec was discussed as a topic model, but the idea of adding
   context vectors to the id97 model can also be defined more
   generally. consider for example documents written by different authors
   from different regions. then author and region vectors could also be
   added to the context vector, resulting in an unsupervised method to
   obtain document, region and author vector representations.

conclusion

   this blog post only provided a quick overview of lda, id97 and
   lda2vec. note that the original author also published a great [14]blog
   post on the technical details of lda2vec.
   57
   57
   [15]0
   (button)
   post a comment

   [16]subscribe to rss
   [17]about[18]terms[19]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/lda2vec-topic-model#comments
   3. http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/
   4. https://arxiv.org/abs/1301.3781
   5. https://arxiv.org/abs/1607.01759
   6. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
   7. http://nlp.yvespeirsman.be/blog/visualizing-word-embeddings-with-tsne/
   8. http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/
   9. http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec
  10. https://arxiv.org/abs/1301.3781
  11. https://cs.stanford.edu/~quocle/paragraph_vector.pdf
  12. https://github.com/meereeum/lda2vec-tf
  13. http://lda2vec.readthedocs.io/en/latest/?badge=latest
  14. http://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec
  15. https://www.datacamp.com/community/tutorials/lda2vec-topic-model#comments
  16. https://www.datacamp.com/community/rss.xml
  17. https://www.datacamp.com/about
  18. https://www.datacamp.com/terms-of-use
  19. https://www.datacamp.com/privacy-policy

   hidden links:
  21. https://www.datacamp.com/
  22. https://www.datacamp.com/community
  23. https://www.datacamp.com/community/tutorials
  24. https://www.datacamp.com/community/data-science-cheatsheets
  25. https://www.datacamp.com/community/open-courses
  26. https://www.datacamp.com/community/podcast
  27. https://www.datacamp.com/community/chat
  28. https://www.datacamp.com/community/blog
  29. https://www.datacamp.com/community/tech
  30. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  31. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  32. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  33. https://www.datacamp.com/profile/larshulstaert
  34. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  35. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  36. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/lda2vec-topic-model
  37. https://www.facebook.com/pages/datacamp/726282547396228
  38. https://twitter.com/datacamp
  39. https://www.linkedin.com/company/datamind-org
  40. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
