   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

building a deep neural net in google sheets

   [16]go to the profile of blake west
   [17]blake west (button) blockedunblock (button) followfollowing
   feb 9, 2018
   [1*4ttvdct5bnsgupjs3c9kvg.png]

   i want to show you that deep convolutional neural nets are not nearly
   as intimidating as they sound. and i   ll prove it by showing you an
   implementation of one that i made in google sheets. it   s available
   [18]here. copy it (use the file     make a copy option in top left) , and
   you can then play around with it to see how the different levers affect
   the model   s prediction.

   the rest of the article will be a short intro to understand the high
   level intuitions behind convolutional neural nets (id98), and then some
   recommended resources for further information.

   before continuing, i   d like to make a shout out to [19]fastai. i
   recently finished their wonderful [20]deep learning course, and all
   inspiration and credit really goes to them. the fantastic instructor
   [21]jeremy howard, with his co-founder [22]rachel thomas showed the
   class the idea of doing a id98 in excel. but as far as i could tell, the
   spreadsheet was not available online, and also didn   t seem to fully
   complete the network. i   m doing a small extension to their work, and
   putting it on google sheets so it   s easier for everyone to play around
   with.

how did i build it?

   i trained a (very) simple id98 on the [23]mnist data set, which is a
   bunch of black and white images of handwritten digits. each image is
   28x28 pixels. each pixel gets represented as a number between 0 (no
   ink) and 1 (maximum ink). it   s a classic data set to use because it   s
   small enough to be quick, but real enough to show the complexity of
   machine learning. the job of the model is to determine what number the
   image is. each image will always be of exactly one number 0   9.
   [1*wxyohdprqjfauv8mxylzwa.png]
   example image from mnist. 28x28 pixels. note: i added conditional
   formatting in sheets so pixels with more    ink    show up more red.

   i trained the model using a popular deep learning library called
   [24]keras (see that code [25]here), and then put the trained weights
   from my model into sheets. the trained weights are just numbers. to put
   it into sheets, this meant literally copying and pasting a bunch of
   numbers from my model into sheets. the last step was adding formulas to
   replicate what the model does, which is just regular old multiplication
   and addition. let me reiterate that: the math to replicate a deep
   learning model   s predictions stops at multiplication and addition [1].

   there are weights (a.k.a.    parameters   ) in each layer of the model.
   weights are automatically learned by any machine learning model. this
   model has around 1000 weights. more complex models can easily have
   hundreds of millions. you can see all 1000 of this model   s weights
   below:
   [1*y_iltwxixqmrdvtdwgtyha.png]

when to use convolutional neural nets?

   you use id98   s to find patterns in sequential data where you   re pretty
   positive patterns exist, yet you find it hard to put those patterns
   into words, or to extract them through simple rules. id98   s assume order
   matters.

   for example, classifying pictures are a prime use case for id98   s,
   because the pixels are logically sequential, and it   s clear to any
   human that there   s loads of patterns. yet, just try putting into words
   exactly what separates a cat from a chihuahua, and you   ll see why id98   s
   are useful.

   on the other hand, if you   ve got current baseball stats between two
   teams, and you want to predict the winner, then id98   s would be a weird
   choice. the data you have (eg. number of wins, losses, or team batting
   average) is not inherently sequential. order doesn   t matter there, and
   we have already extracted the patterns we believe to be useful. so
   id98   s won   t be helpful.

intuitions behind id98   s

   to understand these beasts, let   s break apart the deep convolutional
   neural net into it   s constituent parts of    deep   ,    convolutional    and
      neural net   .

convolutions

   imagine for a minute that you   re blind. but your job is to figure out
   what digit this handwritten image is of. you   re allowed to talk to
   someone who sees the image, but they have no idea what numbers are. so
   all you can ask them is simple questions. what could you do?

   one approach you could take is to ask things like,    is it mostly
   straight at the top?   ,    diagonal from right to left?   , etc. with enough
   questions like that, you could actually take a pretty good guess that
   it   s a 7, or a 2, or whatever.

   intuitively, that   s what the convolutions are doing. the computer is
   blind, so it does what it can and asks lots of little pattern
   questions.
   [1*yxhwfisdkc0zk_h7_ub3mq.png]
   box 1 is multiplied by box 2. sum the result, and you get box 3. that   s
   a convolution.

   to ask those questions, every pixel in the image gets run through a
   function (a.k.a.    convolution   ) that produces a corresponding pixel,
   which answers one of those little pattern questions. convolutions use
   filters to find patterns. for example, notice how the filter above (no.
   2 in the screenshot) is more red on the right side, and less red on the
   left. that filter will essentially look for left edges.

   it may not be obvious why it will find left edges, but [26]play with
   the spreadsheet and you   ll see for yourself that   s how the math works
   out. filters find things that look like themselves. and id98   s will
   often use hundreds of filters, so you get lots of little    scores    for
   each pixel, kind of like a left-edge score, top-edge score, diagonals,
   corners, etc.

deep

   ok, so asking about edges is cool, but what about more complex shapes?   
   this is where the    deep    multiple layers thing comes in. because now
   that we have a    left-edge   ,    top-edge   , and other simple    filters    of
   the image, we can add another layer, and run convolutions on all the
   previous filters, and combine them! so combining 50/50 a left edge and
   top edge could give you a rounded left corner. cool, huh?
   [1*nxmtw_ryojvn0fqttapsaa.png]
   the second convolution takes corresponding pixels from the previous
   convolution layer and multiplies each by it   s own filter. just like
   before, we sum the result, and that produces a new corresponding pixel
   for the second convolution layer.

   serious id98   s will have many layers, which allow the model to build up
   increasingly abstract and complex shapes. even after only 4 or 5
   layers, your model could start finding faces, animals, and all kinds of
   meaningful shapes.

neural nets

   now you might be asking yourself,    so that   s all great, but coming up
   with all the right filters sounds really tedious.       and what about at
   the end? how do i combine all the answers from those filters into
   something useful?   .

   first it   s useful to realize that at a high level, our id98 really has
   two    sections    to it. the first section, the convolutions, finds useful
   features in our image data for us. the second section, the    dense   
   layers (so named because there are so many weights tying to every
   neuron) towards the end of the spreadsheet, do the classifying for us.
   once you have the features, the dense layers really aren   t that
   dissimilar from running a bunch of id75s and combining
   them into a score for each possible number. the highest score is the
   model   s guess.
   [1*kkywp-tlbxss77d_ev6ffq.png]
   matrix 1 is the output from our convolutions. then every pixel from
   matrix 1 is multiplied by a corresponding number in matrix 2 . the
   summation of that produces number 3. you repeat that process again for
   the boxes in green. you end up with 8 outputs, or    neurons    in deep
   learning jargon.

   figuring out all the right weights to use for the filters and the dense
   layers at the end would be really annoying. luckily figuring out those
   weights automatically is kind of the entire point of nn   s, so we don   t
   need to worry about that. but if you   re curious, you should google
      back propagation   .

summary

   there   s roughly two parts to every id98. the convolutions, which always
   go at the beginning to find useful features in the image, and the
   layers at the end, often called    dense    layers, which classify things
   based on those features.

   to get a real feel for them, i would encourage you to play with [27]the
   spreadsheet. trace a pixel from beginning to end. mess with the
   filters, and see what happens. i also explain more technical details in
   the comments of the spreadsheet.

resources

   to learn more, i recommend the following resources:

   [28]interactive convolutions         a killer interactive tutorial on
   convolutions (ie. just the c part, not the nn part) by victor powell.

   [29]practical deep learning for coders         the course from fast.ai which
   i took, and learned a lot from. it   s online and completely free.

   [30]great video showing basics of id98         this is from jeremy howard
   (fastai founder), and is a 20 minute video going over id98   s. it   s
   excellent. the video is embedded in that page. start from minute 21
   when you open the video.

notes

   [1]         the math required to train the id98 includes calculus, so it can
   automatically adjust the weights. but once the model is trained, it
   does actually only require multiplication and addition to do
   predictions. and in practice, the calculus is handled by whatever deep
   learning library you   re using.

     * [31]artificial intelligence
     * [32]neural networks
     * [33]machine learning
     * [34]convolutional network
     * [35]towards data science

   (button)
   (button)
   (button) 3.1k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of blake west

[37]blake west

   senior engineer [38]@coinbase. 1st hire [39]@hinthealth. musician. ml
   enthusiast

     (button) follow
   [40]towards data science

[41]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.1k
     * (button)
     *
     *

   [42]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [43]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/49cdaf466da0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-deep-neural-net-in-google-sheets-49cdaf466da0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-deep-neural-net-in-google-sheets-49cdaf466da0&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_pllngil3m3ii---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@bwest87?source=post_header_lockup
  17. https://towardsdatascience.com/@bwest87
  18. https://docs.google.com/spreadsheets/d/1swfvctd4tjdn2s8bl09ktpqn_41saryzd3nehyr-8z0/edit?usp=sharing
  19. http://www.fast.ai/
  20. http://course.fast.ai/index.html
  21. https://medium.com/@jeremyphoward
  22. https://medium.com/@racheltho
  23. https://en.wikipedia.org/wiki/mnist_database
  24. https://keras.io/
  25. https://gist.github.com/blakewest/5aa2a93beac46f3848a32dc7c32ffb23
  26. https://docs.google.com/spreadsheets/d/1swfvctd4tjdn2s8bl09ktpqn_41saryzd3nehyr-8z0/edit?usp=sharing
  27. https://docs.google.com/spreadsheets/d/1swfvctd4tjdn2s8bl09ktpqn_41saryzd3nehyr-8z0/edit?usp=sharing
  28. http://setosa.io/ev/image-kernels/
  29. http://course.fast.ai/
  30. https://www.usfca.edu/data-institute/certificates/deep-learning-part-one
  31. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  32. https://towardsdatascience.com/tagged/neural-networks?source=post
  33. https://towardsdatascience.com/tagged/machine-learning?source=post
  34. https://towardsdatascience.com/tagged/convolutional-network?source=post
  35. https://towardsdatascience.com/tagged/towards-data-science?source=post
  36. https://towardsdatascience.com/@bwest87?source=footer_card
  37. https://towardsdatascience.com/@bwest87
  38. http://twitter.com/coinbase
  39. http://twitter.com/hinthealth
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://medium.com/p/49cdaf466da0/share/twitter
  46. https://medium.com/p/49cdaf466da0/share/facebook
  47. https://medium.com/p/49cdaf466da0/share/twitter
  48. https://medium.com/p/49cdaf466da0/share/facebook
