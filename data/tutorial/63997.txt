   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                          introducing id22

   how capsnets can overcome some shortcomings of id98s, including
   requiring less training data, preserving image details, and handling
   ambiguity.

   by [27]aur  lien g  ron

   february 6, 2018

   identification identification (source: [28]pixabay)

   id22 (capsnets) are a hot new neural net architecture that
   may well have a profound impact on deep learning, in particular for
   id161. wait, isn't id161 pretty much solved
   already? haven't we all seen fabulous examples of convolutional neural
   networks (id98s) reaching super-human level in various id161
   tasks, such as classification, localization, id164, semantic
   segmentation or instance segmentation (see figure 1)?
   main id161 tasks figure 1. some of the main id161
   tasks. today, each of these tasks requires a very different id98
   architecture, for example resnet for classification, yolo for object
   detection, mask r-id98 for instance segmentation, and so on. image by
   aur  lien g  ron.

   well, yes, we   ve seen fabulous id98s, but:
     * they were trained on huge numbers of images (or they reused parts
       of neural nets that had). capsnets can generalize well using much
       less training data.
     * id98s don   t handle ambiguity very well. capsnets do, so they can
       perform well even on crowded scenes (although, they still struggle
       with backgrounds right now).
     * id98s lose plenty of information in the pooling layers. these layers
       reduce the spatial resolution (see figure 2), so their outputs are
       invariant to small changes in the inputs. this is a problem when
       detailed information must be preserved throughout the network, such
       as in semantic segmentation. today, this issue is addressed by
       building complex architectures around id98s to recover some of the
       lost information. with capsnets, detailed pose information (such as
       precise object position, rotation, thickness, skew, size, and so
       on) is preserved throughout the network, rather than lost and later
       recovered. small changes to the inputs result in small changes to
       the outputs   information is preserved. this is called
       "equivariance." as a result, capsnets can use the same simple and
       consistent architecture across different vision tasks.
     * finally, id98s require extra components to automatically identify
       which object a part belongs to (e.g., this leg belongs to this
       sheep). capsnets give you the hierarchy of parts for free.

   deeplab2 pipeline for image segmentation figure 2. the deeplab2
   pipeline for image segmentation, by liang-chieh chen, et al.: notice
   that the output of the id98 (top right) is very coarse, making it
   necessary to add extra steps to recover some of the lost details. from
   the [29]paper deeplab: semantic image segmentation with deep
   convolutional nets, atrous convolution, and fully connected crfs,
   figure reproduced with the kind permission of the authors. see this
   [30]great post by s. chilamkurthy to see how diverse and complex the
   architectures for semantic segmentation can get.

   capsnets were first introduced in 2011 by geoffrey hinton, et al., in a
   paper called [31]transforming autoencoders, but it was only a few
   months ago, in november 2017, that sara sabour, nicholas frosst, and
   geoffrey hinton published a paper called [32]dynamic routing between
   capsules, where they introduced a capsnet architecture that reached
   state-of-the-art performance on mnist (the famous data set of
   handwritten digit images), and got considerably better results than
   id98s on multimnist (a variant with overlapping pairs of different
   digits). see figure 3.
   multimnist images and their reconstructions figure 3. multimnist images
   (white) and their reconstructions by a capsnet (red+green).    r    =
   reconstructions;    l    = labels. for example, the predictions for the
   first example (top left) are correct, and so are the reconstructions.
   but in the fifth example, the prediction is wrong: (5,7) instead of
   (5,0). therefore, the 5 is correctly reconstructed, but not the 0. from
   the [33]paper: dynamic routing between capsules, figure reproduced with
   the kind permission of the authors.

   despite all their qualities, capsnets are still far from perfect.
   firstly, for now they don't perform as well as id98s on larger images
   such as cifar10 or id163. moreover, they are computationally
   intensive, and they cannot detect two objects of the same type when
   they are too close to each other (this is called the "crowding
   problem," and it has been shown that [34]humans have it, too). but the
   key ideas are extremely promising, and it seems likely that they just
   need a few tweaks to reach their full potential. after all, modern id98s
   were invented in 1998, yet they only beat the state of the art on
   id163 in 2012, after a few tweaks.

so, what are capsnets exactly?

   in short, a capsnet is composed of capsules rather than neurons. a
   capsule is a small group of neurons that learns to detect a particular
   object (e.g., a rectangle) within a given region of the image, and it
   outputs a vector (e.g., an 8-dimensional vector) whose length
   represents the estimated id203 that the object is
   present[35]^[1], and whose orientation (e.g., in 8d space) encodes the
   object's pose parameters (e.g., precise position, rotation, etc.). if
   the object is changed slightly (e.g., shifted, rotated, resized, etc.)
   then the capsule will output a vector of the same length, but oriented
   slightly differently. thus, capsules are equivariant.

   much like a regular neural network, a capsnet is organized in multiple
   layers (see figure 4). the capsules in the lowest layer are called
   primary capsules: each of them receives a small region of the image as
   input (called its receptive field), and it tries to detect the presence
   and pose of a particular pattern, for example a rectangle. capsules in
   higher layers, called routing capsules, detect larger and more complex
   objects, such as boats.
   two-layer capsnet figure 4. a two-layer capsnet. in this example, the
   primary capsule layer has two maps of 5x5 capsules, while the second
   capsule layer has two maps of 3x3 capsules. each capsule outputs a
   vector. each arrow represents the output of a different capsule. blue
   arrows represent the output of a capsule that tries to detect
   triangles, black arrows represent the output of a capsule that tries to
   detect rectangles, and so on. image by aur  lien g  ron.

   the primary capsule layer is implemented using a few regular
   convolutional layers. for example, in the paper, they use two
   convolutional layers that output 256 6x6 features maps containing
   scalars. they reshape this output to get 32 6x6 maps containing
   8-dimensional vectors. finally, they use a novel squashing function to
   ensure these vectors have a length between 0 and 1 (to represent a
   id203). and that's it: this gives the output of the primary
   capsules.

   the capsules in the next layers also try to detect objects and their
   pose, but they work very differently, using an algorithm called routing
   by agreement. this is where most of the magic of capsnets lies. let's
   look at an example.

   suppose there are just two primary capsules: one rectangle capsule and
   one triangle capsule, and suppose they both detected what they were
   looking for. both the rectangle and the triangle could be part of
   either a house or a boat (see figure 5). given the pose of the
   rectangle, which is slightly rotated to the right, the house and the
   boat would have to be slightly rotated to the right as well. given the
   pose of the triangle, the house would have to be almost upside down,
   whereas the boat would be slightly rotated to the right. note that both
   the shapes and the whole/part relationships are learned during
   training. now notice that the rectangle and the triangle agree on the
   pose of the boat, while they strongly disagree on the pose of the
   house. so, it is very likely that the rectangle and triangle are part
   of the same boat, and there is no house.
   routing-by-agreement figure 5. routing by agreement, step 1   predict the
   presence and pose of objects based on the presence and pose of object
   parts, then look for agreement between the predictions. image by
   aur  lien g  ron.

   since we are now confident that the rectangle and triangle are part of
   the boat, it makes sense to send the outputs of the rectangle and
   triangle capsules more to the boat capsule, and less to the house
   capsule: this way, the boat capsule will receive more useful input
   signal, and the house capsule will receive less noise. for each
   connection, the routing-by-agreement algorithm maintains a routing
   weight (see figure 6): it increases routing weight when there is
   agreement, and decreases it in case of disagreement.
   routing-by-agreement, step 2     update the routing weights figure 6.
   routing by agreement, step 2   update the routing weights. image by
   aur  lien g  ron.

   the routing-by-agreement algorithm involves a few iterations of
   agreement-detection + routing-update (note that this happens for each
   prediction, not just once, and not just at training time). this is
   especially useful in crowded scenes: for example, in figure 7, the
   scene is ambiguous because you could see an upside-down house in the
   middle, but this would leave the bottom rectangle and top triangle
   unexplained, so the routing-by-agreement algorithm will most likely
   converge to a better explanation: a boat at the bottom, and a house at
   the top. the ambiguity is said to be "explained away": the lower
   rectangle is best explained by the presence of a boat, which also
   explains the lower triangle, and once these two parts are explained
   away, the remaining parts are easily explained as a house.
   routing by agreement can parse crowded scenes figure 7. routing by
   agreement can parse crowded scenes, such as this ambiguous image, which
   could be misinterpreted as an upside-down house plus some unexplained
   parts. instead, the lower rectangle will be routed to the boat, and
   this will also pull the lower triangle into the boat as well. once that
   boat is    explained away,    it   s easy to interpret the top part as a
   house. image by aur  lien g  ron.

   and that   s it   you know the key ideas behind capsnets! if you want more
   details, check out my two [36]videos on capsnets (one on the
   architecture and another on the implementation) and my [37]commented
   tensorflow implementation (jupyter notebook). please don   t hesitate to
   comment on the videos, file issues on github if you see any, or
   [38]contact me on twitter @aureliengeron. i hope you found this post
   useful!

   [39]^[1] this is the original architecture proposed in the paper
   dynamic routing with capsules, by s. sabour, n. frosst, and g. hinton,
   but since then, they proposed a more general architecture where the
   object   s presence id203 and pose parameters are encoded
   differently in the output vector. the ideas remain the same, however.
   article image: identification (source: [40]pixabay).

   share
    1. [41]tweet
    2.
    3.
     __________________________________________________________________

   [42]aurelien geron

[43]aur  lien g  ron

   aur  lien g  ron is a machine learning consultant, author of the o   reilly
   book hands-on machine learning with scikit-learn and tensorflow. a
   former googler, he led youtube   s video classification team from 2013 to
   2016. he was also a founder and cto of wifirst from 2002 to 2012, a
   leading wireless isp in france, and a founder and cto of polyconseil in
   2001, the firm that now manages the electric car sharing service
   autolib. before this, he worked as a software engineer in a variety of
   domains: finance, defense, health care, and more.
   [44]more
     __________________________________________________________________

   [45]bots landscape.

   [46]ai

[47]infographic: the bot platform ecosystem

   by [48]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [49]vertical forest, milan.

   [50]ai

[51]evolve ai

   by [52]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [53]welcome sign at o'reilly ai conference 2016

   [54]ai

[55]highlights from the o'reilly ai conference in new york 2016

   by [56]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [57]close up of uber's self driving car in pittsburgh.

   [58]ai

[59]how ai is propelling driverless cars, the future of surface transport

   by [60]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [61]our company
     * [62]teach/speak/write
     * [63]careers
     * [64]customer service
     * [65]contact us

site map

     * [66]ideas
     * [67]learning
     * [68]topics
     * [69]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [70]terms of service     [71]privacy policy     [72]editorial independence

   animal

   iframe: [73]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/78d46-aurelien-geron
  28. https://pixabay.com/en/ball-round-alone-different-407081/
  29. https://arxiv.org/abs/1606.00915
  30. http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review
  31. https://scholar.google.com/scholar?hl=en&as_sdt=0,5&as_ylo=2011&as_yhi=2011&q=transforming+autoencoders+author:hinton+author:krizhevsky+author:wang&btng=
  32. https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf
  33. https://arxiv.org/abs/1710.09829
  34. https://scholar.google.com/scholar?hl=en&as_sdt=0,5&q="visual+crowding"&btng=
  35. https://www.oreilly.com/ideas/introducing-capsule-networks#_ftn1
  36. https://www.youtube.com/watch?v=ppn8d0e3900&list=plutyjxw7aat3hlcatbokkxtifcvam0o_a
  37. https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb
  38. https://twitter.com/aureliengeron
  39. https://www.oreilly.com/ideas/introducing-capsule-networks#_ftn1
  40. https://pixabay.com/en/ball-round-alone-different-407081/
  41. https://twitter.com/share
  42. https://www.oreilly.com/people/78d46-aurelien-geron
  43. https://www.oreilly.com/people/78d46-aurelien-geron
  44. https://www.oreilly.com/people/78d46-aurelien-geron
  45. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  46. https://www.oreilly.com/topics/ai
  47. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  48. https://www.oreilly.com/people/b1d73-jon-bruner
  49. https://www.oreilly.com/ideas/evolve-ai
  50. https://www.oreilly.com/topics/ai
  51. https://www.oreilly.com/ideas/evolve-ai
  52. https://www.oreilly.com/people/14d38-naveen-rao
  53. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  54. https://www.oreilly.com/topics/ai
  55. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  56. https://www.oreilly.com/people/0d2c1-mac-slocum
  57. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  58. https://www.oreilly.com/topics/ai
  59. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  60. https://www.oreilly.com/people/c7521-shahin-farshchi
  61. http://oreilly.com/about/
  62. http://oreilly.com/work-with-us.html
  63. http://oreilly.com/careers/
  64. http://shop.oreilly.com/category/customer-service.do
  65. http://shop.oreilly.com/category/customer-service.do
  66. https://www.oreilly.com/ideas
  67. https://www.oreilly.com/topics/oreilly-learning
  68. https://www.oreilly.com/topics
  69. https://www.oreilly.com/all
  70. http://oreilly.com/terms/
  71. http://oreilly.com/privacy.html
  72. http://www.oreilly.com/about/editorial_independence.html
  73. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  75. https://www.oreilly.com/
  76. https://www.facebook.com/oreilly/
  77. https://twitter.com/oreillymedia
  78. https://www.youtube.com/user/oreillymedia
  79. https://plus.google.com/+oreillymedia
  80. https://www.linkedin.com/company/oreilly-media
  81. https://www.oreilly.com/
