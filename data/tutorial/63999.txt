community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   18
   18
   kasper van lombeek
   february 2nd, 2018
   machine learning

finding similar names with id105

   applying id105 on user clicks on hundreds of names on
   the recommender system namesilike.com reveal an unseen structure in our
   first names.

   it's no secret that one of our hobby projects is the first name
   recommender system [3]namesilike.com. this website recommends first
   names to the user, after asking 20 questions to find out their taste.
   it is mostly used by future parents who are in the process to pick a
   name for their expected child.

   recommender systems such as this one are hot in the data-science field!
   famous examples are the recommender systems of spotify, netflix,
   pandora, last.fm and youtube.

   one of the challenges of recommender systems is to pick up the taste of
   the user as fast as possible. users generally have a short attention
   span when surfing the web and expect to see results fast. we decided to
   learn about the user's taste by asking 20 multiple choice questions.
   each question lists 4 names, out of which the user can indicate their
   favorite (without thinking about it too much). the screenshot below
   provides an example:

   [screenshot_namesilike_small_aczc1x.png]

   in december 2017, names i like received around 40.000 visitors thanks
   to a viral effect. the clicks of all these users generated a nice
   dataset, as we keep track of the series of questions presented to each
   user as well as the option that was chosen.

   after some data manipulation, we can hence construct a user-feedback
   table with columns "q" for question number, "f" for feedback, and "n"
   for name. the feedback is either "c", which stands for clicked, or "nc"
   which stands for not-clicked. this table we call a user-feedback table.

   the table below shows the first 10 lines of a user-feedback table for a
   random user:
   f n q
   0 nc lia 1
   1 c bo 1
   2 nc heleen 1
   3 nc zehra 1
   4 nc lou 2
   5 nc lise 2
   6 nc laurence 2
   7 c annelies 2
   8 nc june 3
   9 c liv 3

   the user-item matrix, also known as the utility matrix, is derived by
   concatenating all the user-feedback tables. items are first names in
   our case (for netflix items are movies and for spotify items are
   songs). the value in each cell_ij tells us whether the user_i clicked
   on the product_j.

   we encoded a click on a name with a value of 3, and a non-click with a
   value of -1, so that way the sum of the feedback for one user is zero.

   the table below shows a fragment of the user-item matrix.
           name         amelia clara elise emma lilly linde lise lou
         user_id
   0044ddn5mh1v0o9pwa8c 3.0    -1.0  -1.0  nan  -1.0  -1.0  -1.0 nan
   00jcvtsdj6wqjp4ymt40 nan    -1.0  nan   3.0  -1.0  nan   3.0  3.0
   00xjdchhxwgas64zrsze -1.0   -1.0  nan   nan  3.0   -1.0  3.0  nan
   010blu21dwkpbzsvppzp -1.0   -1.0  -1.0  -1.0 3.0   nan   3.0  3.0
   01hiqb25apojyrngoeod nan    -1.0  -1.0  nan  -1.0  3.0   -1.0 -1.0
   01ib76ott49y3wuh0feo nan    nan   -1.0  -1.0 nan   nan   nan  3.0
   01mgzumlfh4opu5fc6xd 3.0    -1.0  nan   -1.0 nan   -1.0  nan  -1.0
   01hl2eyyneqa9cqbcksm 3.0    nan   3.0   -1.0 -1.0  -1.0  nan  nan
   0294v1c2qygq35tfkjkf -1.0   nan   3.0   -1.0 -1.0  -1.0  -1.0 nan
   02jmn1zx5aj3ojbdx1qb -1.0   nan   3.0   nan  nan   nan   3.0  3.0

   this table has of course a lot of missing values (nan), as not every
   user saw every name. the art of recommender algorithms is to complete
   this matrix by predicting the missing values. these predictions
   represent the feedback that we expect a user to give for an item which
   was not seen so far. we can then simply rank these predictions, and
   recommend the item (first name) with the highest prediction.

   there are many ways of completing this matrix, but we focus here on
   completing the matrix with a latent factor model. explaining the
   details of this method goes too far for this blog post, but we
   encourage you to watch this excellent [4]stanford tutorial.

   in short, the goal of the latent factor model is to estimate the
   user-item matrix through a product of two (smaller) matrixes, the first
   of which has the same number of rows as users, and a number of columns
   representing "latent features".

   the second matrix has the same number of columns as there are items,
   and the latent features now encoded over the rows. a perfect estimate
   of the original user-item matrix is impossible through this
   decomposition, but we try to construct the two matrixes so that their
   product comes as close as possible to the original. their latent
   features then encode a decomposed, compact representation of the
   original information.

   id105

   what are these latent features (or hidden characteristics)? what do
   they represent?

   that is hard to say as they are chosen through an optimization process.

   recall that the latent features are constructed in such a way such that
   the product of the latent feature matrices is as close as possible to
   the complete matrix. or, in other words, the latent features are chosen
   in such a way that the information loss going from the complete
   user-item matrix to approximation with two smaller latent feature
   matrices is as small as possible. the graph below visualizes the values
   of the first two latent features for each female first name item:

   [matrix_factorization_female_names_watermark_yed488.png]

   by visually inspecting this graph, we do observe that "similar" names
   are positioned close to each other in this new latent feature space:
     * french names, such as (camille, alic, amelie) seem to have low
       values for latent feature 2;
     * very flemish names (fien, fenne, nore, janne) seem to have high
       values for latent feature 2 and high values for latent feaure 1;
     * short names (lia, liz, lou) seem to have low values for latent
       feature 1 and high values for latent features 2.

   we believe this is magical. although for us humans it is logical that
   the name juliette is close to estelle and celeste, it seems odd that an
   algorithm can understand this subjective taste. but this is exactly
   what happend : the algorithm understood the subjective taste present in
   the original ratings of all our users, and was able to extract
   similarities between first names purely based on the clicks of users on
   our website.

   these latent features help us to do two things: improve the accuracy of
   the predictions and setting up a sampling scheme for the
   multiple-choice questions. more on that in a later blog post.

   18
   18
   [5]0

   related posts
   must read
   python
   +1

[6]python machine learning: scikit-learn tutorial

   karlijn willems
   february 25th, 2019
   must read
   machine learning
   +4

[7]detecting fake news with scikit-learn

   katharine jarmul
   august 24th, 2017

   (button)
   post a comment

   [8]subscribe to rss
   [9]about[10]terms[11]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/matrix-factorization-names#comments
   3. https://www.namesilike.com/
   4. https://www.youtube.com/watch?v=e8amcwmqstg
   5. https://www.datacamp.com/community/tutorials/matrix-factorization-names#comments
   6. https://www.datacamp.com/community/tutorials/machine-learning-python
   7. https://www.datacamp.com/community/tutorials/scikit-learn-fake-news
   8. https://www.datacamp.com/community/rss.xml
   9. https://www.datacamp.com/about
  10. https://www.datacamp.com/terms-of-use
  11. https://www.datacamp.com/privacy-policy

   hidden links:
  13. https://www.datacamp.com/
  14. https://www.datacamp.com/community
  15. https://www.datacamp.com/community/tutorials
  16. https://www.datacamp.com/community/data-science-cheatsheets
  17. https://www.datacamp.com/community/open-courses
  18. https://www.datacamp.com/community/podcast
  19. https://www.datacamp.com/community/chat
  20. https://www.datacamp.com/community/blog
  21. https://www.datacamp.com/community/tech
  22. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  23. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  24. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  25. https://www.datacamp.com/profile/kaspervanlombeek
  26. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  27. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  28. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/matrix-factorization-names
  29. https://www.datacamp.com/profile/karlijn
  30. https://www.datacamp.com/profile/katharinecc6b90c27e1b40129c2745c9215cc689
  31. https://www.facebook.com/pages/datacamp/726282547396228
  32. https://twitter.com/datacamp
  33. https://www.linkedin.com/company/datamind-org
  34. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
