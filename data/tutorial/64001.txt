community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   99
   99
   hugo bowne-anderson
   october 13th, 2017
   python
   +5

web scraping & nlp in python

   learn to scrape novels from the web and plot word frequency
   distributions; you will gain experience with python packages requests,
   beautifulsoup and nltk.

                                 nltk python

   earlier this week, i did a facebook live code along session. in it, we
   used some basic natural language processing to plot the most frequently
   occurring words in the novel moby dick. in doing so, we also see the
   efficacy of thinking in terms of the following data science pipeline
   with a constant regard for process:
    1. state your question;
    2. get your data;
    3. wrangle your data to answer your question;
    4. answer your question;
    5. present your solution so that others can understand it.

   in this post, you'll learn how to build a data science pipeline to plot
   frequency distributions of words in moby dick, among many other novels.

   tip: if you'd like to rewatch the facebook live, check out the
   following video; you can skip to the 12th minute where hugo comes on
   and the session actually starts.

                                   iframe:
   [3]https://www.facebook.com/plugins/video.php?href=https%3a%2f%2fwww.fa
   cebook.com%2f726282547396228%2fvideos%2f1762301357127670%2f&show_text=0
                                 &width=560

   we won't give you the novels: you'll learn to scrape them from the
   website [4]project gutenberg (which basically contains a large corpus
   of books) using the python package requests and how to extract the
   novels from this web data using beautifulsoup. then you'll dive in to
   analyzing the novels using the natural language toolkit (nltk). in the
   process, you'll learn about important aspects of natural language
   processing (nlp) such as id121 and stopwords.

   you'll come out being able to visualize word frequency distributions of
   any novel that you can find on project gutenberg. the nlp skills you
   develop, however, will be applicable to much of the data that data
   scientists encounter as the vast proportion of the world's data is
   unstructured data and includes a great deal of text.

   for example, what would the following word frequency distribution be
   from?

   [d-x_vvb7fi.png]

   this post was generated from a jupyter notebook; you can find it in
   [5]this repository. if you have any thoughts, responses and/or
   ruminations, feel free to reach out to me on twitter: [6]@hugobowne.

pre-steps

   follow the instructions in the readme.md to get your system set up and
   ready to go.

1. state your question

   what are the most frequent words in the novel moby dick and how often
   do they occur?

2. get your data

   your raw data is the text of melville's novel moby dick. how would you
   go about getting the text of this ~800 word book into python?

   well, there are several ways to do this but first realize that the text
   is freely available online at [7]project gutenberg. let's head there,
   try to find moby dick and then store the relevant url in your python
   namespace:
# store url
url = 'https://www.gutenberg.org/files/2701/2701-h/2701-h.htm'

   now that you have the url, you need to fetch the html of the website.

   note that html stands for hypertext markup language and is the standard
   markup language for the web.

   you're going to use [8]requests to do this, one of the [9]most popular
   and useful python packages out there. you can find out more in
   datacamp's [10]importing data in python (part 2) course.

   beautifulsoup python

   according to the requests package website:

     requests allows you to send organic, grass-fed http/1.1 requests,
     without the need for manual labor.

   and the following organizations claim to use requests internally:

     her majesty's government, amazon, google, twilio, npr, obama for
     america, twitter, sony, and federal u.s. institutions that prefer to
     be unnamed.

   moreover,

     requests is one of the most downloaded python packages of all time,
     pulling in over 13,000,000 downloads every month. all the cool kids
     are doing it!

   you'll be making a get request from the website, which means you're
   getting data from it. this is what you're doing through a browser when
   visiting a webpage using a browser. there are other types of requests,
   such as post requests, but we won't concern ourselves with them here.

   requests make this easy with its get function. make the request here
   and check the object type returned.
# import `requests`
import requests

# make the request and check object type
r = requests.get(url)
type(r)

requests.models.response

   this is a response object. you can see in the [11]requests kickstart
   guide that a response object has an attribute text that allows you to
   get the html from it! let's do this and print the html to check it out:
# extract html from response object and print
html = r.text
#print(html)

   ok! this html is not quite what you want. however, it does contain what
   you want: the text of moby dick. what you need to do now is wrangle
   this html to extract the novel.

3. wrangle the data to answer the question

part 1: get the text from the html

   here you'll use the package [12]beautifulsoup. the package website
   says:

   [bs4_ejxx4e.png]

   this looks promising!

   firstly, a word on the name of the package: beautiful soup? in web
   development, the term "tag soup" refers to structurally or
   syntactically incorrect html code written for a web page. what
   beautiful soup does best is to make tag soup beautiful again and to
   extract information from it with ease! in fact, the main object created
   and queried when using this package is called beautifulsoup. after
   creating the soup, we can use its .get_text() method to extract the
   text.
# import beautifulsoup from bs4
from bs4 import beautifulsoup


# create a beautifulsoup object from the html
soup = beautifulsoup(html, "html5lib")
type(soup)

bs4.beautifulsoup

   from these soup objects, you can extract all types of interesting
   information about the website you're scraping, such as title:
# get soup title
soup.title

<title>
      moby dick; or the whale, by herman melville
    </title>

   or the title as a string:
# get soup title as string
soup.title.string

'\n      moby dick; or the whale, by herman melville\n    '

   or all urls found within a page   s < a > tags (hyperlinks):
# get hyperlinks from soup and check out first several
soup.findall('a')[:8]

[<a href="#link2h_4_0002"> etymology. </a>,
 <a href="#link2h_4_0003"> extracts (supplied by a sub-sub-librarian).
         </a>,
 <a href="#link2hch0001"> chapter 1. loomings. </a>,
 <a href="#link2hch0002"> chapter 2. the carpet-bag. </a>,
 <a href="#link2hch0003"> chapter 3. the spouter-inn. </a>,
 <a href="#link2hch0004"> chapter 4. the counterpane. </a>,
 <a href="#link2hch0005"> chapter 5. breakfast. </a>,
 <a href="#link2hch0006"> chapter 6. the street. </a>]

   what you want to do is to extract the text from the soup and there's a
   super helpful .get_text() method precisely for this.

   get the text, print it out and have a look at it. is it what you want?
# get the text out of the soup and print it
text = soup.get_text()
#print(text)

   notice that this is now nearly what you want.

   it is the text of the novel with some unwanted stuff at the start and
   some unwanted stuff at the end. you could remove it if you wanted.
   however, this content is so much smaller in amount than the text of
   moby dick that, to a first approximation, it is fine to leave in and
   this will be the approach here. to get robust results, i'd suggest
   removing it.

   now that you have the text of interest, it's time for you to count how
   many times each word appears and to plot the frequency histogram that
   you want: natural language processing to the rescue!

part 2: extract words from your text with nlp

   you'll now use nltk, the natural language toolkit, to
    1. tokenize the text (fancy term for splitting into tokens, such as
       words);
    2. remove stopwords (words such as 'a' and 'the' that occur a great
       deal in ~ nearly all english language texts.

step 1: tokenize

   you want to tokenize your text, that is, split it into a list a words.
   essentially, you want to split off the parts off the text that are
   separated by whitespaces.

   to do this, you're going to use a powerful tool called regular
   expressions. a regular expression, or regex for short, is a sequence of
   characters that define a search pattern. they are notoriously confusing
   and best introduced by example.
     * you have the string 'peter piper picked a peck of pickled peppers'
       and you want to extract from the list of all words in it that start
       with a 'p'.

   the regular expression that matches all words beginning with 'p' is
   'p\w+'. let's unpack this:
     * the 'p' at the beginning of the regular expression means that
       you'll only match sequences of characters that start with a 'p';
     * the '\w' is a special character that will match any alphanumeric
       a-z, a-z, 0-9, along with underscores;
     * the '+' tells you that the previous character in the regex can
       appear as many times as you want in strings that you;re trying to
       match. this means that '\w+' will match arbitrary sequences of
       alphanumeric characters and underscores.

   put this all together and the regular expression 'p\w+' will match all
   substrings that start with a 'p' and are followed by alphanumeric
   characters and underscores. in most english language texts that make
   sense, this will correspond to words beginning with 'p'.

   you'll now use the built-in python package re to extract all words
   beginning with 'p' from the sentence 'peter piper picked a peck of
   pickled peppers' as a warm-up.
# import regex package
import re

# define sentence
sentence = 'peter piper pick a peck of pickled peppers'

# define regex
ps = 'p\w+'


# find all words in sentence that match the regex and print them
re.findall(ps, sentence)

['peter', 'piper', 'pick', 'peck', 'pickled', 'peppers']

   this looks pretty good. now, if 'p\w+' is the regex that matches words
   beginning with 'p', what's the regex that matches all words?

   it's your job to now do this for our toy peter piper sentence above.
# find all words and print them
re.findall('\w+', sentence)

['peter', 'piper', 'pick', 'a', 'peck', 'of', 'pickled', 'peppers']

   now you can do the same with text, the string that contains moby dick:
# find all words in moby dick and print several
tokens = re.findall('\w+', text)
tokens[:8]

['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']

   note that there is also a way to do this with nltk, the [13]natural
   language toolkit:
# import regexptokenizer from nltk.tokenize
from nltk.tokenize import regexptokenizer

# create tokenizer
tokenizer = regexptokenizer('\w+')



# create tokens
tokens = tokenizer.tokenize(text)
tokens[:8]

['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']

   ok! you're nearly there. note, though, that in the above, 'or' has a
   capital 'o' and that in other places it may not but both 'or' and 'or'
   you will want to count as the same word. for this reason, you will need
   to build a list of all words in moby dick in which all capital letters
   have been made lower case. you'll find the string method .lower()
   handy:
# initialize new list
words = []


# loop through list tokens and make lower case
for word in tokens:
    words.append(word.lower())


# print several items from list as sanity check
words[:8]

['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']

step 2: remove stop words

   it is common practice to remove words that appear alot in the english
   language such as 'the', 'of' and 'a' (known as stopwords) because
   they're not so interesting. for more on all of these techniques, check
   out our [14]natural language processing fundamentals in python course.

   the package nltk has a list of stopwords in english which you'll now
   store as sw and of which you'll print the first several elements.

   if you get an error here, run the command nltk.download('stopwords') to
   install the stopwords on your system.
# import nltk
import nltk

# get english stopwords and print some of them
sw = nltk.corpus.stopwords.words('english')
sw[:5]

['i', 'me', 'my', 'myself', 'we']

   you want the list of all words in words that are not in sw. one way to
   get this list is to loop over all elements of words and add the to a
   new list if they are not in sw:
# initialize new list
words_ns = []

# add to words_ns all words that are in words but not in sw
for word in words:
    if word not in sw:
        words_ns.append(word)

# print several list items as sanity check
words_ns[:5]

['moby', 'dick', 'whale', 'herman', 'melville']

4. answer your question

   our question was 'what are the most frequent words in the novel moby
   dick and how often do they occur?'

   you can now plot a frequency histogram of words in moby dick in two
   line of code using nltk. to do this,
     * you create a frequency distribution object using the function
       nltk.freqdist();
     * you use the plot() method of the resulting object.

#import datavis libraries
import matplotlib.pyplot as plt
import seaborn as sns

# figures inline and set visualization style
%matplotlib inline
sns.set()

# create freq dist and plot
freqdist1 = nltk.freqdist(words_ns)
freqdist1.plot(25)

   nlp pipeline python

5. present your solution

   the cool thing is that, in using nltk to answer our question, we
   actually already presented our solution in a manner that can be
   communicated to other: a frequency distribution plot! you can read off
   the most common words, along with their frequency. for example, 'whale'
   is the most common word in the novel (go figure), excepting stopwords,
   and it occurs a whopping >1200 times!
     __________________________________________________________________

bonus material

   as you have seen that there are lots of novels on project gutenberg, we
   can make these word frequency histograms of, it makes sense to write
   your own function that does all of this:
def plot_word_freq(url):
    """takes a url (from project gutenberg) and plots a word frequency
    distribution"""
    # make the request and check object type
    r = requests.get(url)
    # extract html from response object and print
    html = r.text
    # create a beautifulsoup object from the html
    soup = beautifulsoup(html, "html5lib")
    # get the text out of the soup and print it
    text = soup.get_text()
    # create tokenizer
    tokenizer = regexptokenizer('\w+')
    # create tokens
    tokens = tokenizer.tokenize(text)
    # initialize new list
    words = []
    # loop through list tokens and make lower case
    for word in tokens:
        words.append(word.lower())
    # get english stopwords and print some of them
    sw = nltk.corpus.stopwords.words('english')
    # initialize new list
    words_ns = []
    # add to words_ns all words that are in words but not in sw
    for word in words:
        if word not in sw:
            words_ns.append(word)
    # create freq dist and plot
    freqdist1 = nltk.freqdist(words_ns)
    freqdist1.plot(25)

   now use the function to plot word frequency distributions from other
   texts on project gutenberg:
     * pride and prejudice:

plot_word_freq('https://www.gutenberg.org/files/42671/42671-h/42671-h.htm')

   web scraping python
     * robinson crusoe

plot_word_freq('https://www.gutenberg.org/files/521/521-h/521-h.htm')

   word distribution tokenize
     * the king james bible

plot_word_freq('https://www.gutenberg.org/files/10/10-h/10-h.htm')

   word frequency

conclusion

   in this post, you learnt how to build a data science pipeline to plot
   frequency distributions of words in moby dick, among many other novels.
   you learnt to scrape them from the website [15]project gutenberg (large
   corpus of books) using the python package requests and how to extract
   the novels from this web data using beautifulsoup. then you jumped in
   to analyze the novels using the natural language toolkit (nltk). in the
   process you learnt about important aspects of natural language
   processing (nlp) such as id121 and stopwords. you're now able to
   visualize word frequency distributions of any novel that you can find
   on project gutenberg. the nlp skills you developed are also applicable
   to much of the data that data scientists encounter as the vast
   proportion of the world's data is unstructured data and includes a
   great deal of text.

   this post was generated from a jupyter notebook; you can find it in
   [16]this repository. if you have any thoughts, responses and/or
   ruminations, feel free to reach out to me on twitter: [17]@hugobowne.
   let me know what cool projects you build.
   99
   99
   [18]0
   (button)
   post a comment

   [19]subscribe to rss
   [20]about[21]terms[22]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/web-scraping-python-nlp#comments
   3. https://www.facebook.com/plugins/video.php?href=https://www.facebook.com/726282547396228/videos/1762301357127670/&show_text=0&width=560
   4. https://www.gutenberg.org/
   5. https://github.com/datacamp/datacamp_facebook_live_nlp/
   6. https://twitter.com/hugobowne
   7. https://www.gutenberg.org/
   8. http://docs.python-requests.org/en/master/
   9. https://pythontips.com/2013/07/30/20-python-libraries-you-cant-live-without/
  10. https://www.datacamp.com/courses/importing-data-in-python-part-2
  11. http://docs.python-requests.org/en/master/user/quickstart/
  12. https://www.crummy.com/software/beautifulsoup/
  13. http://www.nltk.org/
  14. https://www.datacamp.com/courses/nlp-fundamentals-in-python
  15. https://www.gutenberg.org/
  16. https://github.com/datacamp/datacamp_facebook_live_nlp/
  17. https://twitter.com/hugobowne
  18. https://www.datacamp.com/community/tutorials/web-scraping-python-nlp#comments
  19. https://www.datacamp.com/community/rss.xml
  20. https://www.datacamp.com/about
  21. https://www.datacamp.com/terms-of-use
  22. https://www.datacamp.com/privacy-policy

   hidden links:
  24. https://www.datacamp.com/
  25. https://www.datacamp.com/community
  26. https://www.datacamp.com/community/tutorials
  27. https://www.datacamp.com/community/data-science-cheatsheets
  28. https://www.datacamp.com/community/open-courses
  29. https://www.datacamp.com/community/podcast
  30. https://www.datacamp.com/community/chat
  31. https://www.datacamp.com/community/blog
  32. https://www.datacamp.com/community/tech
  33. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  34. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  35. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  36. https://www.datacamp.com/profile/hugobowneanderson
  37. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  38. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  39. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/web-scraping-python-nlp
  40. https://www.facebook.com/pages/datacamp/726282547396228
  41. https://twitter.com/datacamp
  42. https://www.linkedin.com/company/datamind-org
  43. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
