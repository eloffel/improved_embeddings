community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   196
   196
   karlijn willems
   february 4th, 2019
   must read
   python
   +4

keras tutorial: deep learning in python

   this keras tutorial introduces you to deep learning in python: learn to
   preprocess your data, model, evaluate and optimize neural networks.
   [keras-watermark_vhhlzl.png]

deep learning

   by now, you might already know machine learning, a branch in computer
   science that studies the design of algorithms that can learn. today,
   you   re going to focus on deep learning, a subfield of machine learning
   that is a set of algorithms that is inspired by the structure and
   function of the brain. these algorithms are usually called artificial
   neural networks (ann). deep learning is one of the hottest fields in
   data science with many case studies that have astonishing results in
   robotics, image recognition and artificial intelligence (ai).

   one of the most powerful and easy-to-use python libraries for
   developing and evaluating deep learning models is keras; it wraps the
   efficient numerical computation libraries theano and tensorflow. the
   advantage of this is mainly that you can get started with neural
   networks in an easy and fun way.

   today   s keras tutorial for beginners will introduce you to the basics
   of python deep learning:
     * you   ll first learn what [3]id158s are
     * then, the tutorial will show you step-by-step how to use python and
       its libraries to [4]understand, explore and visualize your data,
     * how to [5]preprocess your data: you   ll learn how to split up your
       data in train and test sets and how you can standardize your data,
     * how to [6]build up multi-layer id88s for classification
       tasks,
     * how to [7]compile and fit the data to these models,
     * how to use your model to [8]predict target values, and
     * how to [9]validate the models that you have built.
     * lastly, you   ll also see how you can build up [10]a model for
       regression tasks, and you   ll learn how you can fine-tune the model
       that you   ve built.

   would you like to take a course on keras and deep learning in python?
   consider taking datacamp   s [11]deep learning in python course!

   also, don   t miss our [12]keras cheat sheet, which shows you the six
   steps that you need to go through to build neural networks in python
   with code examples!

introducing id158s

   before going deeper into keras and how you can use it to get started
   with deep learning in python, you should probably know a thing or two
   about neural networks. as you briefly read in the previous section,
   neural networks found their inspiration and biology, where the term
      neural network    can also be used for neurons. the human brain is then
   an example of such a neural network, which is composed of a number of
   neurons.

   and, as you all know, the brain is capable of performing quite complex
   computations, and this is where the inspiration for artificial neural
   networks comes from. the network a whole is a powerful modeling tool.

id88s

   the most simple neural network is the    id88   , which, in its
   simplest form, consists of a single neuron. much like biological
   neurons, which have dendrites and axons, the single artificial neuron
   is a simple tree structure which has input nodes and a single output
   node, which is connected to each input node. here   s a visual comparison
   of the two:

   [content_content_neuron.png]

   as you can see from the picture, there are six components to artificial
   neurons. from left to right, these are:
    1. input nodes. as it so happens, each input node is associated with a
       numerical value, which can be any real number. remember that real
       numbers make up the full spectrum of numbers: they can be positive
       or negative, whole or decimal numbers.
    2. connections. similarly, each connection that departs from the input
       node has a weight associated with it, and this can also be any real
       number.
    3. next, all the values of the input nodes and weights of the
       connections are brought together: they are used as inputs for a
       weighted sum: \(y = f(\sum_{i=1}^{d} w_i*x_i)\), or, stated
       differently, \(y = f(w_1*x_1 + w_2*x_2 + ... w_d*x_d)\).
    4. this result will be the input for a transfer or activation
       function. in the simplest but trivial case, this transfer function
       would be an identity function, \(f(x)=x\) or \(y=x\). in this case,
       \(x\) is the weighted sum of the input nodes and the connections.
       however, just like a biological neuron only fires when a certain
       threshold is exceeded, the artificial neuron will also only fire
       when the sum of the inputs exceeds a threshold, let   s say for
       example 0. this is something that you can   t find back in an
       identity function! the most intuitive way that one can think of is
       by devising a system like the following:
       \(f(x) = 0\) if \(x<0\)
       \(f(x) = 0.5\) if \(x=0\)
       \(f(x) = 1\) if \(x>0\)
    5. of course, you can already imagine that the output is not going to
       be a smooth line: it will be a discontinuous function. because this
       can cause problems in the mathematical processing, a continuous
       variant, the sigmoid function, is often used. an example of a
       sigmoid function that you might already know is the logistic
       function. using this function results in a much smoother result!
    6. as a result, you have the output node, which is associated with the
       function (such as the sigmoid function) of the weighted sum of the
       input nodes. note that the sigmoid function is a mathematical
       function that results in an    s    shaped curve; you   ll read more
       about this later.
    7. lastly, the id88 may be an additional parameter, called a
       bias, which you can consider as the weight associated with an
       additional input node that is permanently set to 1. the bias value
       is critical because it allows you to shift the activation function
       to the left or right, which can make a determine the success of
       your learning.

   note that the logical consequence of this model is that id88s
   only work with numerical data. this implies that you should convert any
   nominal data into a numerical format.

   now that you know that id88s work with thresholds, the step to
   using them for classification purposes isn   t that far off: the
   id88 can agree that any output above a certain threshold
   indicates that an instance belongs to one class, while an output below
   the threshold might result in the input being a member of the other
   class. the straight line where the output equals the threshold is then
   the boundary between the two classes.

   [13]learn python for data science with datacamp

multi-layer id88s

   networks of id88s are multi-layer id88s, and this is what
   this tutorial will implement in python with the help of keras!
   multi-layer id88s are also known as    feed-forward neural
   networks   . as you sort of guessed by now, these are more complex
   networks than the id88, as they consist of multiple neurons that
   are organized in layers. the number of layers is usually limited to two
   or three, but theoretically, there is no limit!

   the layers act very much like the biological neurons that you have read
   about above: the outputs of one layer serve as the inputs for the next
   layer.

   among the layers, you can distinguish an input layer, hidden layers,
   and an output layer. multi-layer id88s are often fully connected.
   this means that there   s a connection from each id88 in a specific
   layer to each id88 in the next layer. even though the
   connectedness is no requirement, this is typically the case.

   note that while the id88 could only represent linear separations
   between classes, the multi-layer id88 overcomes that limitation
   and can also represent more complex decision boundaries.

predicting wine types: red or white?

   for this tutorial, you   ll use the wine quality data set that you can
   find in the [14]wine quality data set from the uci machine learning
   repository. ideally, you perform deep learning on bigger data sets, but
   for the purpose of this tutorial, you will make use of a smaller one.
   this is mainly because the goal is to get you started with the library
   and to familiarize yourself with how neural networks work.

   you might already know this data set, as it   s one of the most popular
   data sets to get started on learning how to work out machine learning
   problems. in this case, it will serve for you to get started with deep
   learning in python with keras.

   let   s get started now!

understanding the data

   however, before you start loading in the data, it might be a good idea
   to check how much you really know about wine (in relation to the
   dataset, of course). most of you will know that there are, in general,
   two very popular types of wine: red and white.

   (i   m sure that there are many others, but for simplicity and because of
   my limited knowledge of wines, i   ll keep it at this. i   m sorry if i   m
   disappointing the true connoisseurs among you :)).

   knowing this is already one thing, but if you want to analyze this
   data, you will need to know just a little bit more.

   first, check out the data description folder to see which variables
   have been included. this is usually the first step to understanding
   your data. go to [15]this page to check out the description or keep on
   reading to get to know your data a little bit better.

   the data consists of two datasets that are related to red and white
   variants of the portuguese    vinho verde    wine. as stated in the
   description, you   ll only find physicochemical and sensory variables
   included in this data set. the data description file lists the 12
   variables that are included in the data, but for those who, like me,
   aren   t really chemistry experts either, here   s a short description of
   each variable:
    1. fixed acidity: acids are major wine properties and contribute
       greatly to the wine   s taste. usually, the total acidity is divided
       into two groups: the volatile acids and the nonvolatile or fixed
       acids. among the fixed acids that you can find in wines are the
       following: tartaric, malic, citric, and succinic. this variable is
       expressed in g(\(tartaric acid\))/\(dm^3\) in the data sets.
    2. volatile acidity: the volatile acidity is basically the process of
       wine turning into vinegar. in the u.s, the legal limits of volatile
       acidity are 1.2 g/l for red table wine and 1.1 g/l for white table
       wine. in these data sets, the volatile acidity is expressed in
       g(\(acetic acid\))/\(dm^3\).
    3. citric acid is one of the fixed acids that you   ll find in wines.
       it   s expressed in g/\(dm^3\) in the two data sets.
    4. residual sugar typically refers to the sugar remaining after
       fermentation stops, or is stopped. it   s expressed in g/\(dm^3\) in
       the red and white data.
    5. chlorides can be a significant contributor to saltiness in wine.
       here, you   ll see that it   s expressed in g(\(sodium
       chloride\))/\(dm^3\).
    6. free sulfur dioxide: the part of the sulfur dioxide that is added
       to a wine and that is lost into it is said to be bound, while the
       active part is said to be free. the winemaker will always try to
       get the highest proportion of free sulfur to bind. this variable is
       expressed in mg/\(dm^3\) in the data.
    7. total sulfur dioxide is the sum of the bound and the free sulfur
       dioxide (so2). here, it   s expressed in mg/\(dm^3\). there are legal
       limits for sulfur levels in wines: in the eu, red wines can only
       have 160mg/l, while white and rose wines can have about 210mg/l.
       sweet wines are allowed to have 400mg/l. for the us, the legal
       limits are set at 350mg/l, and for australia, this is 250mg/l.
    8. density is generally used as a measure of the conversion of sugar
       to alcohol. here, it   s expressed in g/\(cm^3\).
    9. ph or the potential of hydrogen is a numeric scale to specify the
       acidity or basicity the wine. as you might know, solutions with a
       ph less than 7 are acidic, while solutions with a ph greater than 7
       are basic. with a ph of 7, pure water is neutral. most wines have a
       ph between 2.9 and 3.9 and are therefore acidic.
   10. sulfates are to wine as gluten is to food. you might already know
       sulfites from the headaches that they can cause. they are a regular
       part of the winemaking around the world and are considered
       necessary. in this case, they are expressed in g(\(potassium
       sulphate\))/\(dm^3\).
   11. alcohol: wine is an alcoholic beverage, and as you know, the
       percentage of alcohol can vary from wine to wine. it shouldn   t be
       surprised that this variable is included in the data sets, where
       it   s expressed in % vol.
   12. quality: wine experts graded the wine quality between 0 (very bad)
       and 10 (very excellent). the eventual number is the median of at
       least three evaluations made by those same wine experts.

   this all, of course, is some very basic information that you might need
   to know to get started. if you   re a true wine connoisseur, you probably
   know all of this and more!

   now, it   s time to get your data!

loading in the data

   this can be easily done with the python data manipulation library
   pandas. you follow the import convention and import the package under
   its alias, pd.

   next, you make use of the read_csv() function to read in the csv files
   in which the data is stored. additionally, use the sep argument to
   specify that the separator, in this case, is a semicolon and not a
   regular comma.

   try it out in the datacamp light chunk below:
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0ihbhbmrhcybcbmltcg9
   ydcbwyw5kyxmgyxmgcgrcblxuiybszwfkigluihdoaxrlihdpbmugzgf0ysbcbndoaxrlid
   0gx19fx19fx19fx18oxcjodhrwoi8vyxjjagl2zs5py3mudwnplmvkds9tbc9tywnoaw5ll
   wxlyxjuaw5nlwrhdgfiyxnlcy93aw5llxf1ywxpdhkvd2luzxf1ywxpdhktd2hpdguuy3n2
   xcisihnlcd0noycpxg5cbimgumvhzcbpbibyzwqgd2luzsbkyxrhifxucmvkid0gx19fx19
   fx19fx18oxcjodhrwoi8vyxjjagl2zs5py3mudwnplmvkds9tbc9tywnoaw5llwxlyxjuaw
   5nlwrhdgfiyxnlcy93aw5llxf1ywxpdhkvd2luzxf1ywxpdhktcmvklmnzdlwilcbzzxa9j
   zsnksisinnvbhv0aw9uijoiiybjbxbvcnqgcgfuzgfzifxuaw1wb3j0ihbhbmrhcybhcybw
   zfxuxg4jifjlywqgaw4gd2hpdgugd2luzsbkyxrhifxud2hpdgugpsbwzc5yzwfkx2nzdih
   cimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpbmutbgvhcm5pbmctzgf0yw
   jhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5jc3zciiwgc2vwpsc7jylcb
   lxuiybszwfkigluihjlzcb3aw5ligrhdgegxg5yzwqgpsbwzc5yzwfkx2nzdihcimh0dha6
   ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpbmutbgvhcm5pbmctzgf0ywjhc2vzl3d
   pbmutcxvhbgl0es93aw5lcxvhbgl0es1yzwquy3n2xcisihnlcd0noycpiiwic2n0ijoirx
   goks50zxn0x2ltcg9ydchcinbhbmrhc1wikvxurxgoks50zxn0x29iamvjdchcindoaxrlx
   cipxg5fecgplnrlc3rfb2jqzwn0kfwicmvkxcipxg5zdwnjzxnzx21zzz1cilblcmzly3qh
   iflvdsdyzsbyzwfkesb0bybnbyfciij9

   awesome! that was a piece of cake, wasn   t it?

   you have probably done this a million times by now, but it   s always an
   essential step to get started. now you   re completely set to begin
   exploring, manipulating and modeling your data!

data exploration

   with the data at hand, it   s easy for you to learn more about these
   wines! one of the first things that you   ll probably want to do is to
   start with getting a quick view on both of your dataframes:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioiijifbyaw50igluz
   m8gb24gd2hpdgugd2luzvxux19fx18od2hpdguux19fxygpkvxuxg4jifbyaw50igluzm8g
   b24gcmvkihdpbmvcbl9fx19fkhjlzc5fx19fkckpiiwic29sdxrpb24ioiijifbyaw50igl
   uzm8gb24gd2hpdgugd2luzvxuchjpbnqod2hpdguuaw5mbygpkvxuxg4jifbyaw50igluzm
   8gb24gcmvkihdpbmvcbnbyaw50khjlzc5pbmzvkckpiiwic2n0ijoirxgoks50zxn0x2z1b
   mn0aw9ukfw hpdguuaw5mb1wikvxurxgoks50zxn0x2z1bmn0aw9ukfwichjpbnrciiwg
   aw5kzxg9mslcbkv4kckudgvzdf9mdw5jdglvbihcinjlzc5pbmzvxcipxg5fecgplnrlc3r
   fznvuy3rpb24oxcjwcmludfwilcbpbmrled0ykvxuc3vjy2vzc19tc2coxcjxzwxsigrvbm
   uhxcipin0=

   now is the time to check whether your import was successful: double
   check whether the data contains all the variables that the data
   description file of the uci machine learning repository promised you.
   besides the number of variables, also check the quality of the import:
   are the data types correct? did all the rows come through? are there
   any null values that you should take into account when you   re cleaning
   up the data?

   you might also want to check out your data with more than just info():
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioiijiezpid980ihjvd
   3mgb2ygyhjlzgagxg5yzwqux19fxygpxg5cbimgtgfzdcbyb3dzig9migb3agl0zwbcbndo
   axrlll9fx18okvxuxg4jifrha2ugysbzyw1wbgugb2ygnsbyb3dzig9migbyzwrgxg5yzwq
   ux19fx19fkdupxg5cbimgrgvzy3jpymugyhdoaxrlyfxud2hpdguux19fx19fx18okvxuxg
   4jiervdwjszsbjagvjaybmb3igbnvsbcb2ywx1zxmgaw4gyhjlzgbcbnbklmlzbnvsbchyz
   wqpiiwic29sdxrpb24ioiijiezpid980ihjvd3mgb2ygyhjlzgagxg5yzwquagvhzcgpxg5c
   bimgtgfzdcbyb3dzig9migb3agl0zwbcbndoaxrllnrhawwokvxuxg4jifrha2ugysbzyw1
   wbgugb2ygnsbyb3dzig9migbyzwrgxg5yzwquc2ftcgxlkdupxg5cbimgrgvzy3jpymugyh
   doaxrlyfxud2hpdguuzgvzy3jpymuokvxuxg4jiervdwjszsbjagvjaybmb3igbnvsbcb2y
   wx1zxmgaw4gyhjlzgbcbnbklmlzbnvsbchyzwqpiiwic2n0ijoii2nozwnrighlywqgxg5f
   ecgplnrlc3rfznvuy3rpb24oxcjyzwquagvhzfwikvxuiybjagvjayb0ywlsxg5fecgplnr
   lc3rfznvuy3rpb24oxcj3agl0zs50ywlsxcipxg4jignozwnrihnhbxbszvxurxgoks50zx
   n0x2z1bmn0aw9ukfwicmvklnnhbxbszvwikvxuiybjagvjaybkzxnjcmlizvxurxgoks50z
   xn0x2z1bmn0aw9ukfw hpdguuzgvzy3jpymvciilcbimgy2hly2sgaxnudwxsifxudgvz
   df9mdw5jdglvbihcinbhbmrhcy5pc251bgxciilcbnn1y2nlc3nfbxnnkfwir3jlyxqgam9
   iivwiksj9

   a brief recap of all these pandas functions: you see that head(),
   tail() and sample() are fantastic because they provide you with a quick
   way of inspecting your data without any hassle.

   next, describe() offers some summary statistics about your data that
   can help you to assess your data quality. you see that some of the
   variables have a lot of difference in their min and max values. this is
   something that you   ll deal with later, but at this point, it   s just
   imperative to be aware of this.

   lastly, you have double checked the presence of null values in red with
   the help of isnull(). this is a function that always can come in handy
   when you   re still in doubt after having read the results of info().

   tip: also check out whether the wine data contains null values. you can
   do this by using the ipython shell of the datacamp light chunk which
   you see right above.

   now that you have already inspected your data to see if the import was
   successful and correct, it   s time to dig a little bit deeper.

visualizing the data

   one way to do this is by looking at the distribution of some of the
   dataset   s variables and make scatter plots to see possible
   correlations. of course, you can take this all to a much higher level
   if you would use this data for your own project.

alcohol

   one variable that you could find interesting at first sight is alcohol.
   it   s probably one of the first things that catches your attention when
   you   re inspecting a wine data set. you can visualize the distributions
   with any data visualization library, but in this case, the tutorial
   makes use of matplotlib to plot the distributions quickly:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioijpbxbvcnqgbwf0c
   gxvdgxpyi5wexbsb3qgyxmgcgx0xg5cbmzpzywgyxggpsbwbhquc3vicgxvdhmomswgmilc
   blxuyxhbmf0uaglzdchyzwquywxjb2hvbcwgmtasigzhy2vjb2xvcj0ncmvkjywgywxwage
   9mc41lcbsywjlbd1ciljlzcb3aw5lxcipxg5hefsxxs5oaxn0khdoaxrllmfsy29ob2wsid
   ewlcbmywnly29sb3i9j3doaxrljywgzwm9xcjibgfja1wilcbsdz0wljusigfscghhptaun
   swgbgfizww9xcjxagl0zsb3aw5lxcipxg5cbmzpzy5zdwjwbg90c19hzgp1c3qobgvmdd0w
   lcbyawdodd0xlcbib3r0b209mcwgdg9wptaunswgahnwywnlptaumdusihdzcgfjzt0xkvx
   uyxhbmf0uc2v0x3lsaw0owzasidewmdbdkvxuyxhbmf0uc2v0x3hsywjlbchcikfsy29ob2
   wgaw4gjsbwb2xciilcbmf4wzbdlnnldf95bgfizwwoxcjgcmvxdwvuy3lciilcbmf4wzfdl
   nnldf94bgfizwwoxcjbbgnvag9sigluicugvm9sxcipxg5hefsxxs5zzxrfewxhymvskfwi
   rnjlcxvlbmn5xcipxg4jyxhbmf0ubgvnzw5kkgxvyz0nymvzdccpxg4jyxhbmv0ubgvnzw5
   kkgxvyz0nymvzdccpxg5mawcuc3vwdgl0bguoxcjeaxn0cmlidxrpb24gb2ygqwxjb2hvbc
   bpbialifzvbfwikvxuxg5wbhquc2hvdygpin0=

   as you can see in the image below, you see that the alcohol levels
   between the red and white wine are mostly the same: they have around 9%
   of alcohol. of course, there are also a considerable amount of
   observations that have 10% or 11% of alcohol percentage.

   distribution alcohol for a neural network model

   note that you can double check this if you use the histogram() function
   from the numpy package to compute the histogram of the white and red
   data, just like this:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioijpbxbvcnqgbnvtc
   hkgyxmgbnbcbnbyaw50kg5wlmhpc3rvz3jhbshyzwquywxjb2hvbcwgymlucz1bnyw4ldks
   mtasmtesmtismtmsmtqsmtvdkslcbnbyaw50kg5wlmhpc3rvz3jhbsh3agl0zs5hbgnvag9
   slcbiaw5zpvs3ldgsoswxmcwxmswxmiwxmywxncwxnv0pksj9

   if you   re interested in matplotlib tutorials, make sure to check out
   datacamp   s [16]matplotlib tutorial for beginners and [17]viewing 3d
   volumetric data tutorial, which shows you how to make use of
   matplotlib   s event handler api.

sulfates

   next, one thing that interests me is the relation between the sulfates
   and the quality of the wine. as you have read above, sulfates can cause
   people to have headaches, and i   m wondering if this influences the
   quality of the wine. what   s more, i often hear that women especially
   don   t want to drink wine precisely because it causes headaches. maybe
   this affects the ratings for the red wine?

   let   s take a look.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioijpbxbvcnqgbwf0c
   gxvdgxpyi5wexbsb3qgyxmgcgx0xg5cbmzpzywgyxggpsbwbhquc3vicgxvdhmomswgmiwg
   zmlnc2l6zt0oocwgnckpxg5cbmf4wzbdlnnjyxr0zxiocmvkwydxdwfsaxr5j10sihjlzft
   cinn1bhboyxrlc1wixswgy29sb3i9xcjyzwrciilcbmf4wzfdlnnjyxr0zxiod2hpdgvbj3
   f1ywxpdhknxswgd2hpdgvbj3n1bhboyxrlcyddlcbjb2xvcj1cindoaxrlxcisigvkz2vjb
   2xvcnm9xcjibgfja1wilcbsdz0wljupxg5cbmf4wzbdlnnldf90axrszshciljlzcbxaw5l
   xcipxg5hefsxxs5zzxrfdgl0bguoxcjxagl0zsbxaw5lxcipxg5hefswxs5zzxrfegxhymv
   skfwiuxvhbgl0evwikvxuyxhbmv0uc2v0x3hsywjlbchcilf1ywxpdhlciilcbmf4wzbdln
   nldf95bgfizwwoxcjtdwxwagf0zxnciilcbmf4wzfdlnnldf95bgfizwwoxcjtdwxwagf0z
   xnciilcbmf4wzbdlnnldf94bgltkfswldewxslcbmf4wzfdlnnldf94bgltkfswldewxslc
   bmf4wzbdlnnldf95bgltkfswldiunv0pxg5hefsxxs5zzxrfewxpbshbmcwyljvdkvxuzml
   nlnn1ynbsb3rzx2fkanvzdch3c3bhy2u9mc41kvxuzmlnlnn1chrpdgxlkfwiv2luzsbrdw
   fsaxr5igj5ieftb3vudcbvzibtdwxwagf0zxnciilcblxucgx0lnnob3coksj9

   as you can see in the image below, the red wine seems to contain more
   sulfates than the white wine, which has fewer sulfates above 1
   g/\(dm^3\). for the white wine, there only seem to be a couple of
   exceptions that fall just above 1 g/\(dm^3\), while this is definitely
   more for the red wines. this could maybe explain the general saying
   that red wine causes headaches, but what about the quality?

   you can clearly see that there is white wine with a relatively low
   amount of sulfates that gets a score of 9, but for the rest, it   s
   difficult to interpret the data correctly at this point.

   of course, you need to take into account that the difference in
   observations could also affect the graphs and how you might interpret
   them.

   deep learning python with keras

acidity

   apart from the sulfates, the acidity is one of the major and vital wine
   characteristics that is necessary to achieve quality wines. great wines
   often balance out acidity, tannin, alcohol, and sweetness. some more
   research taught me that in quantities of 0.2 to 0.4 g/l, volatile
   acidity doesn   t affect a wine   s quality. at higher levels, however,
   volatile acidity can give the wine a sharp, vinegary tactile sensation.
   extreme volatile acidity signifies a seriously flawed wine.

   let   s put the data to the test and make a scatter plot that plots the
   alcohol versus the volatile acidity. the data points should be colored
   according to their rating or quality label:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioijpbxbvcnqgbwf0c
   gxvdgxpyi5wexbsb3qgyxmgcgx0xg5pbxbvcnqgbnvtchkgyxmgbnbcblxubnaucmfuzg9t
   lnnlzwqontcwkvxuxg5yzwrsywjlbhmgpsbucc51bmlxdwuocmvkwydxdwfsaxr5j10pxg5
   3agl0zwxhymvscya9ig5wlnvuaxf1zsh3agl0zvsncxvhbgl0esddkvxuxg5pbxbvcnqgbw
   f0cgxvdgxpyi5wexbsb3qgyxmgcgx0xg5mawcsigf4id0gcgx0lnn1ynbsb3rzkdesidisi
   gzpz3npemu9kdgsidqpkvxucmvky29sb3jzid0gbnaucmfuzg9tlnjhbmqoniw0kvxud2hp
   dgvjb2xvcnmgpsbucc5hchblbmqocmvky29sb3jzlcbucc5yyw5kb20ucmfuzcgxldqplcb
   heglzptapxg5cbmzvcibpigluihjhbmdlkgxlbihyzwrjb2xvcnmpktpcbiagicbyzwr5id
   0gcmvkwydhbgnvag9sj11bcmvklnf1ywxpdhkgpt0gcmvkbgfizwxzw2ldxvxuicagihjlz
   hggpsbyzwrbj3zvbgf0awxligfjawrpdhknxvtyzwqucxvhbgl0esa9psbyzwrsywjlbhnb
   av1dxg4gicagyxhbmf0uc2nhdhrlcihyzwr4lcbyzwr5lcbjpxjlzgnvbg9yc1tpxslcbmz
   vcibpigluihjhbmdlkgxlbih3agl0zwnvbg9ycykpolxuicagihdoaxrlesa9ihdoaxrlwy
   dhbgnvag9sj11bd2hpdguucxvhbgl0esa9psb3agl0zwxhymvsc1tpxv1cbiagicb3agl0z
   xggpsb3agl0zvsndm9syxrpbgugywnpzgl0esddw3doaxrllnf1ywxpdhkgpt0gd2hpdgvs
   ywjlbhnbav1dxg4gicagyxhbmv0uc2nhdhrlcih3agl0zxgsihdoaxrleswgyz13agl0zwn
   vbg9yc1tpxslcbiagicbcbmf4wzbdlnnldf90axrszshciljlzcbxaw5lxcipxg5hefsxxs
   5zzxrfdgl0bguoxcjxagl0zsbxaw5lxcipxg5hefswxs5zzxrfegxpbshbmcwxljddkvxuy
   xhbmv0uc2v0x3hsaw0owzasms43xslcbmf4wzbdlnnldf95bgltkfs1lde1ljvdkvxuyxhb
   mv0uc2v0x3lsaw0owzusmtuunv0pxg5hefswxs5zzxrfegxhymvskfwivm9syxrpbgugqwn
   pzgl0evwikvxuyxhbmf0uc2v0x3lsywjlbchcikfsy29ob2xciilcbmf4wzfdlnnldf94bg
   fizwwoxcjwb2xhdglszsbby2lkaxr5xcipxg5hefsxxs5zzxrfewxhymvskfwiqwxjb2hvb
   fwiksbcbinhefswxs5szwdlbmqocmvkbgfizwxzlcbsb2m9j2jlc3qnlcbiym94x3rvx2fu
   y2hvcj0oms4zlcaxkslcbmf4wzfdlmxlz2vuzch3agl0zwxhymvscywgbg9jpsdizxn0jyw
   gymjvef90b19hbmnob3i9kdeumywgmskpxg4jzmlnlnn1chrpdgxlkfwiqwxjb2hvbcatif
   zvbgf0awxliefjawrpdhlciilcbmzpzy5zdwjwbg90c19hzgp1c3qodg9wptauodusihdzc
   gfjzt0wljcpxg5cbnbsdc5zag93kckifq==

   note that the colors in this image are randomly chosen with the help of
   the numpy random module. you can always change this by passing a list
   to the redcolors or whitecolors variables. make sure that they are the
   same (except for 1 because the white wine data has one unique quality
   value more than the red wine data), though, otherwise your legends are
   not going to match!

   check out the full graph here:

   wine quality binary classification with keras

   in the image above, you see that the levels that you have read about
   above especially hold for the white wine: most wines with label 8 have
   volatile acidity levels of 0.5 or below, but whether or not it has an
   effect on the quality is too difficult to say, since all the data
   points are very densely packed towards one side of the graph.

   this is just a quick data exploration. if you would be interested in
   elaborating this step in your own projects, consider datacamp   s data
   exploration posts, such as [18]python exploratory data analysis and
   [19]python data profiling tutorials, which will guide you through the
   basics of eda.

wrapping up the exploratory data analysis

   this maybe was a lot to digest, so it   s never too late for a small
   recap of what you have seen during your eda that could be important for
   the further course of this tutorial:
     * some of the variables of your data sets have values that are
       considerably far apart. you can and will deal with this in the next
       section of the tutorial.
     * you have an ideal scenario: there are no null values in the data
       sets.
     * most wines that were included in the data set have around 9% of
       alcohol.
     * red wine seems to contain more sulphates than the white wine, which
       has less sulphates above 1 g/\(dm^3\).
     * you saw that most wines had a volatile acidity of 0.5 and below. at
       the moment, there is no direct relation to the quality of the wine.

   up until now, you have looked at the white wine and red wine data
   separately. the two seem to differ somewhat when you look at some of
   the variables from close up, and in other cases, the two seem to be
   very similar. do you think that there could there be a way to classify
   entries based on their variables into white or red wine?

   there is only one way to find out: preprocess the data and model it in
   such a way so that you can see what happens!

preprocess data

   now that you have explored your data, it   s time to act upon the
   insights that you have gained! let   s preprocess the data so that you
   can start building your own neural network!
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jykilcjzyw1wbguioiijiefkzcbgdhlwz
   wagy29sdw1uihrvigbyzwrgihdpdgggdmfsdwugmvxux19fwyd0exblj10gpsaxxg5cbimg
   qwrkigb0exblycbjb2x1bw4gdg8gyhdoaxrlycb3axroihzhbhvlidbcbl9fx19fwyd0exb
   lj10gpsawxg5cbimgqxbwzw5kigb3agl0zwagdg8gyhjlzgbcbndpbmvzid0gcmvkll9fx1
   9fxyh3agl0zswgawdub3jlx2luzgv4pvrydwupiiwic29sdxrpb24ioiijiefkzcbgdhlwz
   wagy29sdw1uihrvigbyzwrgihdpdgggdmfsdwugmvxucmvkwyd0exblj10gpsaxxg5cbimg
   qwrkigb0exblycbjb2x1bw4gdg8gyhdoaxrlycb3axroihzhbhvlidbcbndoaxrlwyd0exb
   lj10gpsawxg5cbimgqxbwzw5kigb3agl0zwagdg8gyhjlzgbcbndpbmvzid0gcmvklmfwcg
   vuzch3agl0zswgawdub3jlx2luzgv4pvrydwupiiwic2n0ijoirxgoks50zxn0x29iamvjd
   chcinjlzfwikvxurxgoks50zxn0x29iamvjdchcindoaxrlxcipxg5fecgplnrlc3rfb2jq
   zwn0kfw luzxnciikifq==

   you set ignore_index to true in this case because you don   t want to
   keep the index labels of white when you   re appending the data to red:
   you want the labels to continue from where they left off in red, not
   duplicate index labels from joining both data sets together.

intermezzo: correlation matrix

   now that you have the full data set, it   s a good idea to also do a
   quick data exploration; you already know some stuff from looking at the
   two data sets separately, and now it   s time to gather some more solid
   insights, perhaps.

   since it can be somewhat difficult to interpret graphs, it   s also a
   good idea to plot a correlation matrix. this will give insights more
   quickly about which variables correlate:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jylcbndpbmvzid0gcmvklmfwcgvuzch3a
   gl0zswgawdub3jlx2luzgv4pvrydwupiiwic2ftcgxlijoiaw1wb3j0ihnlywjvcm4gyxmg
   c25zxg5jb3jyid0gd2luzxmuy29ycigpxg5zbnmuagvhdg1hcchjb3jylcbcbiagicagica
   gicagihh0awnrbgfizwxzpwnvcniuy29sdw1ucy52ywx1zxmsxg4gicagicagicagicb5dg
   lja2xhymvscz1jb3jylmnvbhvtbnmudmfsdwvzkvxuc25zlnbsdc5zag93kckifq==

   as you would expect, there are some variables that correlate, such as
   density and residual sugar. also volatile acidity and type are more
   closely connected than you originally could have guessed by looking at
   the two data sets separately, and it was kind of to be expected that
   free sulfur dioxide and total sulfur dioxide were going to correlate.

   correlation wine quality data

   very interesting!

train and test sets

   imbalanced data typically refers to a problem with classification
   problems where the classes are not represented equally.most
   classification data sets do not have exactly equal number of instances
   in each class, but a small difference often does not matter. you thus
   need to make sure that all two classes of wine are present in the
   training model. what   s more, the amount of instances of all two wine
   types needs to be more or less equal so that you do not favour one or
   the other class in your predictions.

   in this case, there seems to be an imbalance, but you will go with this
   for the moment. afterwards, you can evaluate the model and if it
   underperforms, you can resort to undersampling or oversampling to cover
   up the difference in observations.

   for now, import the train_test_split from sklearn.model_selection and
   assign the data and the target labels to the variables x and y. you   ll
   see that you need to flatten the array of target labels in order to be
   totally ready to use the x and y variables as input for the
   train_test_split() function. off to work, get started in the datacamp
   light chunk below!
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuaw1wb3j0ig51bxb5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxud2hpdg
   ugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2kuzwr1l21sl21hy2hpb
   mutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lcxvhbgl0es13agl0zs5j
   c3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawn
   zlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3
   dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jylcbnjlzfsndhlwzsddid0gmvxud2hpd
   gvbj3r5cgunxsa9idbcbndpbmvzid0gcmvklmfwcgvuzch3agl0zswgawdub3jlx2luzgv4
   pvrydwupiiwic2ftcgxlijoiiybjbxbvcnqgyhryywlux3rlc3rfc3bsaxrgigzyb20gyhn
   rbgvhcm4ubw9kzwxfc2vszwn0aw9uyfxuznjvbsbza2xlyxjulm1vzgvsx3nlbgvjdglvbi
   bpbxbvcnqgdhjhaw5fdgvzdf9zcgxpdfxuxg4jifnwzwnpznkgdghligrhdgegxg5ypxdpb
   mvzlml4wzosmdoxmv1cblxuiybtcgvjawz5ihrozsb0yxjnzxqgbgfizwxzigfuzcbmbgf0
   dgvuihrozsbhcnjhesbcbnk9bnaucmf2zwwod2luzxmudhlwzslcblxuiybtcgxpdcb0agu
   gzgf0ysb1ccbpbib0cmfpbibhbmqgdgvzdcbzzxrzxg5yx3ryywlulcbyx3rlc3qsihlfdh
   jhaw4sihlfdgvzdca9ihryywlux3rlc3rfc3bsaxqowcwgeswgdgvzdf9zaxplptaumzmsi
   hjhbmrvbv9zdgf0zt00mikilcjzb2x1dglvbii6iimgsw1wb3j0igb0cmfpbl90zxn0x3nw
   bgl0ycbmcm9tigbza2xlyxjulm1vzgvsx3nlbgvjdglvbmbcbmzyb20gc2tszwfybi5tb2r
   lbf9zzwxly3rpb24gaw1wb3j0ihryywlux3rlc3rfc3bsaxrcblxuiybtcgvjawz5ihrozs
   bkyxrhifxuwd13aw5lcy5pefs6lda6mtfdxg5cbimgu3bly2lmesb0agugdgfyz2v0igxhy
   mvscybhbmqgzmxhdhrlbib0agugyxjyyxlcbnk9ig5wlnjhdmvskhdpbmvzlnr5cgupxg5c
   bimgu3bsaxqgdghligrhdgegdxagaw4gdhjhaw4gyw5kihrlc3qgc2v0c1xuwf90cmfpbiw
   gwf90zxn0lcb5x3ryywlulcb5x3rlc3qgpsb0cmfpbl90zxn0x3nwbgl0kfgsihksihrlc3
   rfc2l6zt0wljmzlcbyyw5kb21fc3rhdgu9ndipiiwic2n0ijoirxgoks50zxn0x2ltcg9yd
   chcinnrbgvhcm4ubw9kzwxfc2vszwn0aw9ulnryywlux3rlc3rfc3bsaxrciilcbkv4kcku
   dgvzdf9vymply3qoxcjyxcipxg5fecgplnrlc3rfb2jqzwn0kfwievwikvxurxgoks50zxn
   0x29iamvjdchcilhfdhjhaw5ciilcbkv4kckudgvzdf9vymply3qoxcjyx3rlc3rciilcbk
   v4kckudgvzdf9vymply3qoxcj5x3ryywluxcipxg5fecgplnrlc3rfb2jqzwn0kfwiev90z
   xn0xcipin0=

   you   re already well on your way to build your first neural network, but
   there is still one thing that you need to take care of! do you still
   know what you discovered when you were looking at the summaries of the
   white and red data sets?

   indeed, some of the values were kind of far apart. it might make sense
   to do some standardization here.

standardize the data

   standardization is a way to deal with these values that lie so far
   apart. the scikit-learn package offers you a great and quick way of
   getting your data standardized: import the standardscaler module from
   sklearn.preprocessing and you   re ready to scale your train and test
   data!
# import `standardscaler` from `sklearn.preprocessing`
from sklearn.preprocessing import standardscaler

# define the scaler
scaler = standardscaler().fit(x_train)

# scale the train set
x_train = scaler.transform(x_train)

# scale the test set
x_test = scaler.transform(x_test)

   now that you   re data is preprocessed, you can move on to the real work:
   building your own neural network to classify wines.

model data

   before you start modeling, go back to your original question: can you
   predict whether a wine is red or white by looking at its chemical
   properties, such as volatile acidity or sulphates?

   since you only have two classes, namely white and red, you   re going to
   do a binary classification. as you can imagine,    binary    means 0 or 1,
   yes or no. since neural networks can only work with numerical data, you
   have already encoded red as 1 and white as 0.

   a type of network that performs well on such a problem is a multi-layer
   id88. as you have read in the beginning of this tutorial, this
   type of neural network is often fully connected. that means that you   re
   looking to build a fairly simple stack of fully-connected layers to
   solve this problem. as for the activation function that you will use,
   it   s best to use one of the most common ones here for the purpose of
   getting familiar with keras and neural networks, which is the relu
   activation function.

   now how do you start building your multi-layer id88? a quick way
   to get started is to use the keras sequential model: it   s a linear
   stack of layers. you can easily create the model by passing a list of
   layer instances to the constructor, which you set up by running model =
   sequential().

   next, it   s best to think back about the structure of the multi-layer
   id88 as you might have read about it in the beginning of this
   tutorial: you have an input layer, some hidden layers and an output
   layer. when you   re making your model, it   s therefore important to take
   into account that your first layer needs to make the input shape clear.
   the model needs to know what input shape to expect and that   s why
   you   ll always find the input_shape, input_dim, input_length, or
   batch_size arguments in the documentation of the layers and in
   practical examples of those layers.

   in this case, you will have to use a dense layer, which is a fully
   connected layer. dense layers implement the following operation: output
   = activation(dot(input, kernel) + bias). note that without the
   activation function, your dense layer would consist only of two linear
   operations: a dot product and an addition.

   in the first layer, the activation argument takes the value relu. next,
   you also see that the input_shape has been defined. this is the input
   of the operation that you have just seen: the model takes as input
   arrays of shape (12,), or (*, 12). lastly, you see that the first layer
   has 12 as a first value for the units argument of dense(), which is the
   dimensionality of the output space and which are actually 12 hidden
   units. this means that the model will output arrays of shape (*, 12):
   this is is the dimensionality of the output space. don   t worry if you
   don   t get this entirely just now, you   ll read more about it later on!

   the units actually represents the kernel of the above formula or the
   weights matrix, composed of all weights given to all input nodes,
   created by the layer. note that you don   t include any bias in the
   example below, as you haven   t included the use_bias argument and set it
   to true, which is also a possibility.

   the intermediate layer also uses the relu activation function. the
   output of this layer will be arrays of shape (*,8).

   you are ending the network with a dense layer of size 1. the final
   layer will also use a sigmoid activation function so that your output
   is actually a id203; this means that this will result in a score
   between 0 and 1, indicating how likely the sample is to have the target
      1   , or how likely the wine is to be red.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ig51bxb
   5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3ksisinnhbxbszsi6iimgsw1wb3j0igbtzxf1zw
   50awfsycbmcm9tigbrzxjhcy5tb2rlbhngxg5mcm9tigtlcmfzlm1vzgvscybpbxbvcnqgu
   2vxdwvudglhbfxuxg4jieltcg9ydcbgrgvuc2vgigzyb20gygtlcmfzlmxhewvyc2bcbmzy
   b20ga2vyyxmubgf5zxjzigltcg9ydcbezw5zzvxuxg4jieluaxrpywxpemugdghlignvbnn
   0cnvjdg9yxg5tb2rlbca9ifnlcxvlbnrpywwokvxuxg4jiefkzcbhbibpbnb1dcbsyxllci
   bcbm1vzgvslmfkzchezw5zzsgxmiwgywn0axzhdglvbj0ncmvsdscsigluchv0x3noyxblp
   sgxmswpkslcblxuiybbzgqgb25lighpzgrlbibsyxllcibcbm1vzgvslmfkzchezw5zzsg4
   lcbhy3rpdmf0aw9upsdyzwx1jykpxg5cbimgqwrkigfuig91dhb1dcbsyxllcibcbm1vzgv
   slmfkzchezw5zzsgxlcbhy3rpdmf0aw9upsdzawdtb2lkjykpin0=

   all in all, you see that there are two key architecture decisions that
   you need to make to make your model: how many layers you   re going to
   use and how many    hidden units    you will chose for each layer.

   in this case, you picked 12 hidden units for the first layer of your
   model: as you read above, this is is the dimensionality of the output
   space. in other words, you   re setting the amount of freedom that you   re
   allowing the network to have when it   s learning representations. if you
   would allow more hidden units, your network will be able to learn more
   complex representations but it will also be a more expensive operations
   that can be prone to overfitting.

   remember that overfitting occurs when the model is too complex: it will
   describe random error or noise and not the underlying relationship that
   it needs to describe. in other words, the training data is modeled too
   well!

   note that when you don   t have that much training data available, you
   should prefer to use a small network with very few hidden layers
   (typically only one, like in the example above).

   if you want to get some information on the model that you have just
   created, you can use the attributed output_shape or the summary()
   function, among others. some of the most basic ones are listed below.

   try running them to see what results you exactly get back and what they
   tell you about the model that you have just created:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ig51bxb
   5igfzig5wxg5ucc5yyw5kb20uc2vlzcg3kvxuznjvbsbrzxjhcy5tb2rlbhmgaw1wb3j0if
   nlcxvlbnrpywxcbmzyb20ga2vyyxmubgf5zxjzigltcg9ydcbezw5zzvxubw9kzwwgpsbtz
   xf1zw50awfskclcbm1vzgvslmfkzchezw5zzsgxmixhy3rpdmf0aw9upsdyzwx1jywgaw5w
   dxrfc2hhcgu9kdexlckpkvxubw9kzwwuywrkkerlbnnlkdgsywn0axzhdglvbj0ncmvsdsc
   pkvxubw9kzwwuywrkkerlbnnlkdesywn0axzhdglvbj0nc2lnbw9pzccpksisinnhbxbszs
   i6iimgtw9kzwwgb3v0chv0ihnoyxblxg5tb2rlbc5fx19fx19fx19fx1xuxg4jie1vzgvsi
   hn1bw1hcnlcbm1vzgvsll9fx19fx19fx19cblxuiybnb2rlbcbjb25mawdcbm1vzgvslmdl
   df9jb25mawcokvxuxg4jiexpc3qgywxsihdlawdodcb0zw5zb3jzifxubw9kzwwuz2v0x3d
   lawdodhmoksisinnvbhv0aw9uijoiiybnb2rlbcbvdxrwdxqgc2hhcgvcbm1vzgvslm91dh
   b1df9zagfwzvxuxg4jie1vzgvsihn1bw1hcnlcbm1vzgvslnn1bw1hcnkokvxuxg4jie1vz
   gvsignvbmzpz1xubw9kzwwuz2v0x2nvbmzpzygpxg5cbimgtglzdcbhbgwgd2vpz2h0ihrl
   bnnvcnmgxg5tb2rlbc5nzxrfd2vpz2h0cygpiiwic2n0ijoirxgoks50zxn0x29iamvjdf9
   hy2nlc3nlzchcim1vzgvsxcipxg5fecgplnrlc3rfb2jqzwn0x2fjy2vzc2vkkfwibw9kzw
   xciilcbkv4kckudgvzdf9mdw5jdglvbihcim1vzgvslmdldf9jb25mawdciilcbkv4kckud
   gvzdf9mdw5jdglvbihcim1vzgvslmdldf93zwlnahrzxcipxg5zdwnjzxnzx21zzyhcikf3
   zxnvbwugam9iivwiksj9

compile and fit

   next, it   s time to compile your model and fit the model to the data:
   once again, make use of compile() and fit() to get this done.
model.compile(loss='binary_crossid178',
              optimizer='adam',
              metrics=['accuracy'])

model.fit(x_train, y_train,epochs=20, batch_size=1, verbose=1)

   in compiling, you configure the model with the adam optimizer and the
   binary_crossid178 id168. additionally, you can also monitor
   the accuracy during the training by passing ['accuracy'] to the metrics
   argument.

   the optimizer and the loss are two arguments that are required if you
   want to compile the model. some of the most popular optimization
   algorithms used are the stochastic id119 (sgd), adam and
   rmsprop. depending on whichever algorithm you choose, you   ll need to
   tune certain parameters, such as learning rate or momentum. the choice
   for a id168 depends on the task that you have at hand: for
   example, for a regression problem, you   ll usually use the mean squared
   error (mse). as you see in this example, you used binary_crossid178
   for the binary classification problem of determining whether a wine is
   red or white. lastly, with multi-class classification, you   ll make use
   of categorical_crossid178.

   after, you can train the model for 20 epochs or iterations over all the
   samples in x_train and y_train, in batches of 1 sample. you can also
   specify the verbose argument. by setting it to 1, you indicate that you
   want to see progress bar logging.

   in other words, you have to train the model for a specified number of
   epochs or exposures to the training dataset. an epoch is a single pass
   through the entire training set, followed by testing of the
   verification set. the batch size that you specify in the code above
   defines the number of samples that going to be propagated through the
   network. also, by doing this, you optimize the efficiency because you
   make sure that you don   t load too many input patterns into memory at
   the same time.

predict values

   let   s put your model to use! you can make predictions for the labels of
   the test set with it. just use predict() and pass the test set to it to
   predict the labels for the data. in this case, the result is stored in
   y_pred:
y_pred = model.predict(x_test)

   before you go and evaluate your model, you can already get a quick idea
   of the accuracy by checking how y_pred and y_test compare:
y_pred[:5]
array([[0],
       [1],
       [0],
       [0],
       [0]], dtype=int32)
y_test[:5]
array([0, 1, 0, 0, 0])

   you see that these values seem to add up, but what is all of this
   without some hard numbers?

evaluate model

   now that you have built your model and used it to make predictions on
   data that your model hadn   t seen yet, it   s time to evaluate its
   performance. you can visually compare the predictions with the actual
   test labels (y_test), or you can use all types of metrics to determine
   the actual performance. in this case, you   ll use evaluate() to do this.
   pass in the test data and test labels and if you want, put the verbose
   argument to 1. you   ll see more logs appearing when you do this.
score = model.evaluate(x_test, y_test,verbose=1)

print(score)
[0.025217213829228164, 0.99487179487179489]

   the score is a list that holds the combination of the loss and the
   accuracy. in this case, you see that both seem very great, but in this
   case it   s good to remember that your data was somewhat imbalanced: you
   had more white wine than red wine observations. the accuracy might just
   be reflecting the class distribution of your data because it   ll just
   predict white because those observations are abundantly present!

   before you start re-arranging the data and putting it together in a
   different way, it   s always a good idea to try out different evaluation
   metrics. for this, you can rely on scikit-learn (which you import as
   sklearn, just like before when you were making the train and test sets)
   for this.

   in this case, you will test out some basic classification evaluation
   techniques, such as:
     * the confusion matrix, which is a breakdown of predictions into a
       table showing correct predictions and the types of incorrect
       predictions made. ideally, you will only see numbers in the
       diagonal, which means that all your predictions were correct!
     * precision is a measure of a classifier   s exactness. the higher the
       precision, the more accurate the classifier.
     * recall is a measure of a classifier   s completeness. the higher the
       recall, the more cases the classifier covers.
     * the f1 score or f-score is a weighted average of precision and
       recall.
     * the kappa or cohen   s kappa is the classification accuracy
       normalized by the imbalance of the classes in the data.

# import the modules from `sklearn.metrics`
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_
score, cohen_kappa_score

# confusion matrix
confusion_matrix(y_test, y_pred)
array([[1585,    3],
       [   8,  549]])
# precision
precision_score(y_test, y_pred)
0.994565217391
# recall
recall_score(y_test, y_pred)
0.98563734290843807
# f1 score
f1_score(y_test,y_pred)
0.99008115419296661
# cohen's kappa
cohen_kappa_score(y_test, y_pred)
0.98662321692498967

   all these scores are very good! you have made a pretty accurate model
   despite the fact that you have considerably more rows that are of the
   white wine type.

   good job!

some more experiments

   you   ve successfully built your first model, but you can go even further
   with this one. why not try out the following things and see what their
   effect is? like you read above, the two key architectural decisions
   that you need to make involve the layers and the hidden nodes. these
   are great starting points:
     * you used 1 hidden layers. try to use 2 or 3 hidden layers;
     * use layers with more hidden units or less hidden units;
     * take the quality column as the target labels and the rest of the
       data (including the encoded type column!) as your data. you now
       have a multi-class classification problem!

   but why also not try out changing the activation function? instead of
   relu, try using the tanh activation function and see what the result
   is!

   [20]learn python for data science with datacamp

predicting wine quality

   your classification model performed perfectly for a first run!

   but there is so much more that you can do besides going a level higher
   and trying out more complex structures than the multi-layer id88.
   why not try to make a neural network to predict the wine quality?

   in this case, the tutorial assumes that quality is a continuous
   variable: the task is then not a binary classification task but an
   ordinal regression task. it   s a type of regression that is used for
   predicting an ordinal variable: the quality value exists on an
   arbitrary scale where the relative ordering between the different
   quality values is significant. in this scale, the quality scale 0-10
   for    very bad    to    very good    is such an example.

   note that you could also view this type of problem as a classification
   problem and consider the quality labels as fixed class labels.

   in any case, this situation setup would mean that your target labels
   are going to be the quality column in your red and white dataframes for
   the second part of this tutorial. this will require some additional
   preprocessing.

preprocess data

   since the quality variable becomes your target class, you will now need
   to isolate the quality labels from the rest of the data set. you will
   put wines.quality in a different variable y and you   ll put the wines
   data, with exception of the quality column in a variable x.

   next, you   re ready to split the data in train and test sets, but you
   won   t follow this approach in this case (even though you could!). in
   this second part of the tutorial, you will make use of k-fold
   validation, which requires you to split up the data into k partitions.
   usually, k is set at 4 or 5. next, you instantiate identical models and
   train each one on a partition, while also evaluating on the remaining
   partitions. the validation score for the model is then an average of
   the k validation scores obtained.

   you   ll see how to do this later. for now, use standardscaler to make
   sure that your data is in a good place before you fit the data to the
   model, just like before.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxud2hpdgugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2
   kuzwr1l21sl21hy2hpbmutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lc
   xvhbgl0es13agl0zs5jc3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0
   cdovl2fyy2hpdmuuawnzlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxm
   vd2luzs1xdwfsaxr5l3dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jylcbndpbmvzid
   0gcmvklmfwcgvuzch3agl0zswgawdub3jlx2luzgv4pvrydwupiiwic2ftcgxlijoiiybjc
   29syxrlihrhcmdldcbsywjlbhncbnkgpsb3aw5lcy5fx19fx19fx1xuxg4jielzb2xhdgug
   zgf0yvxuwca9ihdpbmvzlmryb3aoj3f1ywxpdhknlcbheglzpv8picisinnvbhv0aw9uijo
   iiybjc29syxrlihrhcmdldcbsywjlbhncbnkgpsb3aw5lcy5xdwfsaxr5xg5cbimgsxnvbg
   f0zsbkyxrhxg5yid0gd2luzxmuzhjvid35ncxvhbgl0escsigf4axm9mskgiiwic2n0ijoir
   xgoks50zxn0x29iamvjdchcinlciilcbkv4kckudgvzdf9vymply3qoxcjyxcipin0=

   remember that you also need to perform the scaling again because you
   had a lot of differences in some of the values for your red, white (and
   consequently also wines) data.

   try this out in the datacamp light chunk below. all the necessary
   libraries have been loaded in for you!
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxud2hpdgugpsbwzc5yzwfkx2nzdihcimh0dha6ly9hcmnoaxzllmljcy51y2
   kuzwr1l21sl21hy2hpbmutbgvhcm5pbmctzgf0ywjhc2vzl3dpbmutcxvhbgl0es93aw5lc
   xvhbgl0es13agl0zs5jc3zciiwgc2vwpsc7jylcbnjlzca9ihbklnjlywrfy3n2kfwiahr0
   cdovl2fyy2hpdmuuawnzlnvjas5lzhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxm
   vd2luzs1xdwfsaxr5l3dpbmvxdwfsaxr5lxjlzc5jc3zciiwgc2vwpsc7jylcbndpbmvzid
   0gcmvklmfwcgvuzch3agl0zswgawdub3jlx2luzgv4pvrydwupxg55id0gd2luzxmucxvhb
   gl0evxuwca9ihdpbmvzlmryb3aoj3f1ywxpdhknlcbheglzptepifxuznjvbsbza2xlyxju
   lnbyzxbyb2nlc3npbmcgaw1wb3j0ifn0yw5kyxjku2nhbgvyiiwic2ftcgxlijoiiybty2f
   szsb0agugzgf0ysb3axroigbtdgfuzgfyzfnjywxlcmbcblggpsbfx19fx19fx19fx19fx1
   9flmzpdf90cmfuc2zvcm0owckilcjzb2x1dglvbii6iimgu2nhbgugdghligrhdgegd2l0a
   cbgu3rhbmrhcmrty2fszxjgxg5yid0gu3rhbmrhcmrty2fszxioks5maxrfdhjhbnnmb3jt
   kfgpiiwic2n0ijoirxgoks50zxn0x29iamvjdchcilhciikifq==

   now you   re again at the point where you were a bit ago. you can again
   start modeling the neural network!

model neural network architecture

   now that you have preprocessed the data again, it   s once more time to
   construct a neural network model, a multi-layer id88. even though
   you   ll use it for a regression task, the architecture could look very
   much the same, with two dense layers.

   don   t forget that the first layer is your input layer. you will need to
   pass the shape of your input data to it. in this case, you see that
   you   re going to make use of input_dim to pass the dimensions of the
   input data to the dense layer.
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ihbhbmr
   hcybhcybwzfxuznjvbsbza2xlyxjulm1vzgvsx3nlbgvjdglvbibpbxbvcnqgdhjhaw5fdg
   vzdf9zcgxpdfxuznjvbsbza2xlyxjulnbyzxbyb2nlc3npbmcgaw1wb3j0ifn0yw5kyxjku
   2nhbgvyxg53agl0zsa9ihbklnjlywrfy3n2kfwiahr0cdovl2fyy2hpdmuuawnzlnvjas5l
   zhuvbwwvbwfjagluzs1szwfybmluzy1kyxrhymfzzxmvd2luzs1xdwfsaxr5l3dpbmvxdwf
   saxr5lxdoaxrllmnzdlwilcbzzxa9jzsnkvxucmvkid0gcgqucmvhzf9jc3yoxcjodhrwoi
   8vyxjjagl2zs5py3mudwnplmvkds9tbc9tywnoaw5llwxlyxjuaw5nlwrhdgfiyxnlcy93a
   w5llxf1ywxpdhkvd2luzxf1ywxpdhktcmvklmnzdlwilcbzzxa9jzsnkvxucmvkwyd0exbl
   j10gpsaxxg53agl0zvsndhlwzsddid0gmfxud2luzxmgpsbyzwquyxbwzw5kkhdoaxrllcb
   pz25vcmvfaw5kzxg9vhj1zslcbnkgpsb3aw5lcy5xdwfsaxr5xg5yid0gd2luzxmuzhjvcc
   gncxvhbgl0escsigf4axm9mskgxg5yid0gu3rhbmrhcmrty2fszxioks5maxrfdhjhbnnmb
   3jtkfgpiiwic2ftcgxlijoiiybjbxbvcnqgyfnlcxvlbnrpywxgigzyb20gygtlcmfzlm1v
   zgvsc2bcbmzyb20ga2vyyxmubw9kzwxzigltcg9ydcbtzxf1zw50awfsxg5cbimgsw1wb3j
   0igbezw5zzwagznjvbsbga2vyyxmubgf5zxjzyfxuznjvbsbrzxjhcy5syxllcnmgaw1wb3
   j0ierlbnnlxg5cbimgsw5pdglhbgl6zsb0agugbw9kzwxcbm1vzgvsid0gu2vxdwvudglhb
   cgpxg5cbimgqwrkigluchv0igxhewvyifxubw9kzwwuywrkkerlbnnlkdy0lcbpbnb1df9k
   aw09mtisigfjdgl2yxrpb249j3jlbhunkslcbiagicbcbimgqwrkig91dhb1dcbsyxllcib
   cbm1vzgvslmfkzchezw5zzsgxkskifq==

   note again that the first layer that you define is the input layer.
   this layer needs to know the input dimensions of your data. you pass in
   the input dimensions, which are 12 in this case (don   t forget that
   you   re also counting the type column which you have generated in the
   first part of the tutorial!). you again use the relu activation
   function, but once again there is no bias involved. the number of
   hidden units is 64.

   your network ends with a single unit dense(1), and doesn   t include an
   activation. this is a typical setup for scalar regression, where you
   are trying to predict a single continuous value).

compile the model, fit the data

   with your model at hand, you can again compile it and fit the data to
   it. but wait. don   t you need the k fold validation partitions that you
   read about before? that   s right.
import numpy as np
from sklearn.model_selection import stratifiedkfold

seed = 7
np.random.seed(seed)

kfold = stratifiedkfold(n_splits=5, shuffle=true, random_state=seed)
for train, test in kfold.split(x, y):
    model = sequential()
    model.add(dense(64, input_dim=12, activation='relu'))
    model.add(dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    model.fit(x[train], y[train], epochs=10, verbose=1)

   use the compile() function to compile the model and then use fit() to
   fit the model to the data. to compile the model, you again make sure
   that you define at least the optimizer and loss arguments. in this
   case, you can use rsmprop, one of the most popular optimization
   algorithms, and mse as the id168, which is very typical for
   regression problems such as yours.

   the additional metrics argument that you define is actually a function
   that is used to judge the performance of your model. for regression
   problems, it   s prevalent to take the mean absolute error (mae) as a
   metric. you   ll read more about this in the next section.

   pass in the train data and labels to fit(), determine how many epochs
   you want to run the fitting, the batch size and if you want, you can
   put the verbose argument to 1 to get more logs because this can take up
   some time.

evaluate model

   just like before, you should also evaluate your model. besides adding
   y_pred = model.predict(x[test]) to the rest of the code above, it might
   also be a good idea to use some of the id74 from sklearn,
   like you also have done in the first part of the tutorial.

   to do this, you can make use of the mean squared error (mse) and the
   mean absolute error (mae). the former, which is also called the    mean
   squared deviation    (msd) measures the average of the squares of the
   errors or deviations. in other words, it quantifies the difference
   between the estimator and what is estimated. this way, you get to know
   some more about the quality of your estimator: it is always
   non-negative, and values closer to zero are better.

   the latter evaluation measure, mae, stands for mean absolute error: it
   quantifies how close predictions are to the eventual outcomes.

   add these lines to the previous code chunk, and be careful with the
   indentations:
mse_value, mae_value = model.evaluate(x[test], y[test], verbose=0)

print(mse_value)
0.522478731072
print(mae_value)
0.561965950103

   note that besides the mse and mae scores, you could also use the r2
   score or the regression score function. here, you should go for a score
   of 1.0, which is the best. however, the score can also be negative!
from sklearn.metrics import r2_score

r2_score(y[test], y_pred)
0.3125092543

   at first sight, these are quite horrible numbers, right? the good thing
   about this, though, is that you can now experiment with optimizing the
   code so that the results become a little bit better.

   that   s what the next and last section is all about!

model fine-tuning

   fine-tuning your model is probably something that you   ll be doing a lot
   because not all problems are as straightforward as the one that you saw
   in the first part of this tutorial. as you read above, there are
   already two critical decisions that you   ll probably want to adjust: how
   many layers you   re going to use and how many    hidden units    you will
   choose for each layer.

   in the beginning, this will indeed be quite a journey.

adding layers

   what would happen if you add another layer to your model? what if it
   would look like this?
model = sequential()
model.add(dense(64, input_dim=12, activation='relu'))
model.add(dense(64, activation='relu'))
model.add(dense(1))

hidden units

   also try out the effect of adding more hidden units to your model   s
   architecture and study the effect on the evaluation, just like this:
model = sequential()
model.add(dense(128, input_dim=12, activation='relu'))
model.add(dense(1))

   note again that, in general, because you don   t have a ton of data, the
   worse overfitting can and will be. that   s why you should use a small
   network.

some more experiments: optimization parameters

   besides adding layers and playing around with the hidden units, you can
   also try to adjust (some of) the parameters of the optimization
   algorithm that you give to the compile() function. up until now, you
   have always passed a string, such as rmsprop, to the optimizer
   argument.

   but that doesn   t always need to be like this!

   try, for example, importing rmsprop from keras.models and adjust the
   learning rate lr. you can also change the default values that have been
   set for the other parameters for rmsprop(), but this is not
   recommended. you can get more information [21]here.
from keras.optimizers import rmsprop
rmsprop = rmsprop(lr=0.0001)
model.compile(optimizer=rmsprop, loss='mse', metrics=['mae'])

   also, try out experimenting with other optimization algorithms, like
   the stochastic id119 (sgd). do you notice an effect?
from keras.optimizers import sgd, rmsprop
sgd=sgd(lr=0.1)
model.compile(optimizer=sgd, loss='mse', metrics=['mae'])

go further!

   this tutorial was just a start in your deep learning journey with
   python and keras. there is still a lot to cover, so why not take
   datacamp   s [22]deep learning in python course? in the meantime, also
   make sure to check out the [23]keras documentation, if you haven   t done
   so already. you   ll find more examples and information on all functions,
   arguments, more layers, etc. it   ll undoubtedly be an indispensable
   resource when you   re learning how to work with neural networks in
   python! if you instead feel like reading a book that explains the
   fundamentals of deep learning (with keras) together with how it's used
   in practice, you should definitely read fran  ois chollet's [24]deep
   learning in python book.

   196
   196
   [25]0

   related posts
   must read
   python
   +1

[26]python machine learning: scikit-learn tutorial

   karlijn willems
   february 25th, 2019
   must read
   python
   +4

[27]tensorflow tutorial for beginners

   karlijn willems
   january 16th, 2019
   python
   +4

[28]convolutional neural networks in python with keras

   aditya sharma
   december 5th, 2017

   (button)
   post a comment

   [29]subscribe to rss
   [30]about[31]terms[32]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/deep-learning-python#comments
   3. https://www.datacamp.com/community/tutorials/deep-learning-python#ann
   4. https://www.datacamp.com/community/tutorials/deep-learning-python#python
   5. https://www.datacamp.com/community/tutorials/deep-learning-python#preprocess
   6. https://www.datacamp.com/community/tutorials/deep-learning-python#modeling
   7. https://www.datacamp.com/community/tutorials/deep-learning-python#compfit
   8. https://www.datacamp.com/community/tutorials/deep-learning-python#predict
   9. https://www.datacamp.com/community/tutorials/deep-learning-python#validate
  10. https://www.datacamp.com/community/tutorials/deep-learning-python#finetune
  11. https://www.datacamp.com/courses/deep-learning-in-python
  12. https://www.datacamp.com/community/blog/keras-cheat-sheet
  13. https://www.datacamp.com/courses/
  14. https://www.datacamp.com/community/tutorials/deep-learning-python
  15. http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names
  16. https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python
  17. https://www.datacamp.com/community/tutorials/matplotlib-3d-volumetric-data
  18. https://www.datacamp.com/community/tutorials/exploratory-data-analysis-python
  19. https://www.datacamp.com/community/tutorials/python-data-profiling
  20. https://www.datacamp.com/courses/
  21. https://keras.io/optimizers/
  22. https://www.datacamp.com/courses/deep-learning-in-python
  23. https://keras.io/
  24. https://www.manning.com/books/deep-learning-with-python
  25. https://www.datacamp.com/community/tutorials/deep-learning-python#comments
  26. https://www.datacamp.com/community/tutorials/machine-learning-python
  27. https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  28. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python
  29. https://www.datacamp.com/community/rss.xml
  30. https://www.datacamp.com/about
  31. https://www.datacamp.com/terms-of-use
  32. https://www.datacamp.com/privacy-policy

   hidden links:
  34. https://www.datacamp.com/
  35. https://www.datacamp.com/community
  36. https://www.datacamp.com/community/tutorials
  37. https://www.datacamp.com/community/data-science-cheatsheets
  38. https://www.datacamp.com/community/open-courses
  39. https://www.datacamp.com/community/podcast
  40. https://www.datacamp.com/community/chat
  41. https://www.datacamp.com/community/blog
  42. https://www.datacamp.com/community/tech
  43. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/deep-learning-python
  44. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/deep-learning-python
  45. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/deep-learning-python
  46. https://www.datacamp.com/profile/karlijn
  47. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/deep-learning-python
  48. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/deep-learning-python
  49. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/deep-learning-python
  50. https://www.datacamp.com/profile/karlijn
  51. https://www.datacamp.com/profile/karlijn
  52. https://www.datacamp.com/profile/adityasharma101993
  53. https://www.facebook.com/pages/datacamp/726282547396228
  54. https://twitter.com/datacamp
  55. https://www.linkedin.com/company/datamind-org
  56. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
