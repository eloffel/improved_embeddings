   #[1]github [2]recent commits to segmentation:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2
     * [35]star [36]13
     * [37]fork [38]7

[39]alexalemi/[40]segmentation

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   [46]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   branch: master
   [48]segmentation/[49]data/segmentation.txt
   [50]find file copy path
   fetching contributors   
   cannot retrieve contributors at this time
   1631 lines (1444 sloc) 50.6 kb
   [51]raw [52]blame [53]history
   (button) (button)
   text segmentation based on semantic id27s
   alexander a alemi
   dept of physics
   cornell university
   aaa244@cornell.edu
   paul ginsparg
   depts of physics and information science
   cornell university
   ginsparg@cornell.edu
   abstract
   we explore the use of semantic id27s [11, 13,
   9] in text segmentation algorithms, including the c99 seg-
   mentation algorithm [3, 4] and new algorithms inspired by
   the distributed word vector representation. by developing
   a general framework for discussing a class of segmentation
   ob jectives, we study the e   ectiveness of greedy versus ex-
   act optimization approaches and suggest a new iterative re-
      nement technique for improving the performance of greedy
   strategies. we compare our results to known benchmarks
   [15, 12, 3, 4], using known metrics [2, 14]. we demonstrate
   state-of-the-art performance for an untrained method with
   our content vector segmentation (cvs) on the choi test
   set. finally, we apply the segmentation procedure to an in-
   the-wild dataset consisting of text extracted from scholarly
   articles in the arxiv.org database.
   categories and subject descriptors
   i.2.7 [natural language processing]: text analysis
   general terms
   information retrieval, id91, text
   keywords
   text segmentation, id111, word vectors
   1.
   introduction
   segmenting text into naturally coherent sections has many
   useful applications in information retrieval and automated
   text summarization, and has received much past attention.
   an early text segmentation algorithm was the texttiling
   method introduced by hearst [8] in 1997. text was scanned
   linearly, with a coherence calculated for each adjacent block,
   and a heuristic was used to determine the locations of cuts.
   in addition to linear approaches, there are text segmenta-
   tion algorithms that optimize some scoring ob jective. an
   permission to make digital or hard copies of all or part of this work
   for
   personal or classroom use is granted without fee provided that copies
   are
   not made or distributed for pro   t or commercial advantage and that
   copies
   bear this notice and the full citation on the    rst page. to copy
   otherwise, to
   republish, to post on servers or to redistribute to lists, requires
   prior speci   c
   permission and/or a fee.
   kdd    15 sydney, australia
   copyright 20xx acm x-xxxxx-xx-x/xx/xx ...$15.00.
   early algorithm in this class was choi   s c99 algorithm [3]
   in 2000, which also introduced a benchmark segmentation
   dataset used by subsequent work. instead of looking only at
   nearest neighbor coherence, the c99 algorithm computes a
   coherence score between all pairs of elements of text,1 and
   searches for a text segmentation that optimizes an ob jective
   based on that scoring by greedily making a succession of
   best cuts. later work by choi and collaborators [4] used dis-
   tributed representations of words rather than a bag of words
   approach, with the representations generated by lsa [6]. in
   2001, utiyama and ishahara introduce a statistical model
   for segmentation and optimized a posterior for the segment
   boundaries. moving beyond the greedy approaches, in 2004
   fragkou et al. [7] attempted to    nd the optimal splitting
   for their own ob jective using id145. more
   recent attempts at segmentation, including misra et al. [12]
   and riedl and biemann [15], use lda based topic models
   to inform the segmentation task.
   for the most part, the non-topic model based segmenta-
   tion approaches have been based on relatively simple rep-
   resentations of the underlying text. recent approaches to
   learning word vectors, including mikolov et al.   s id97
   [11], pennington et al.   s glove [13] and levy and gold-
   berg   s pointwise mutual information [9], have proven re-
   markably successful in solving analogy tasks, machine trans-
   lation [10], and id31 [13]. these word vector
   approaches attempt to learn a log-linear model for word-
   word co-occurrence statistics, such that the id203 of
   two words (w, w (cid:48) ) appearing near one another is propor-
   tional to the exponential of their dot product,
   (cid:80)
   exp(w    w (cid:48) )
   p (w|w
   v exp(v    w (cid:48) )
   the method relies on these word-word co-occurrence statis-
   tics encoding meaningful semantic and syntactic relation-
   ships. arora et al. [1] have shown how the remarkable per-
   formance of these techniques can be understood in terms of
   relatively mild assumptions about corpora statistics, which
   in turn can be recreated with a simple generative model.
   here we explore the utility of word vectors for text seg-
   mentation, both in the context of existing algorithms such
   as c99, and when used to construct new segmentation ob-
   jectives based on a generative model for segment formation.
   we will    rst construct a framework for describing a family
   of segmentation algorithms, then discuss the speci   c algo-
   ) =
   (cid:48)
   .
   (1)
   1by    elements   , we mean the pieces of text combined in order
   to comprise the segments. in the applications to be consid-
   ered, the basic elements will be either sentences or words.
   rithms to be investigated in detail. we then apply our mod-
   i   ed algorithms both to the standard choi test set and to a
   test set generated from arxiv.org research articles.
   2. text segmentation
   the segmentation task is to split a text into contiguous
   coherent sections. we    rst build a representation of the text,
   by splitting it into n basic elements, (cid:126)vi (i = 1, . . . , n ),
   each
   a d-dimensional feature vector vi   (   = 1, . . . , d) repre-
   senting the element. then we assign a score   (i, j ) to each
   candidate segment, comprised of the ith through (j     1)th
   elements, and    nally determine how to split the text into
   the appropriate number of segments.
   denote a segmentation of text into k segments as a list
   of k indices s = (s1 , s1 ,          , sk ), where the k-th segment
   includes the elements (cid:126)vi with sk   1     i < sk , with s0    
   0. for example, the string    aaabbcccdd    considered at the
   character level would be properly split with s = (3, 5, 8, 10)
   into (   aaa   ,    bb   ,    ccc   ,    dd   ).
   2.1 representation
   the text representation thus amounts to turning a plain
   text document t into an (n    d)-dimensional matrix v,
   with n the number of initial elements to be grouped into
   coherent segments and d the dimensionality of the element
   representation. for example, if segmenting at the word level
   then n would be the number of words in the text, and each
   word might be represented by a d-dimensional vector, such
   as those obtained from glove [13]. if segmenting instead at
   the sentence level, then n is the number of sentences in the
   text and we must decide how to represent each sentence.
   there are additional preprocessing decisions, for example
   using a id30 algorithm or removing stop words before
   forming the representation. particular preprocessing deci-
   sions can have a large e   ect on the performance of segmen-
   tation algorithms, but for discussing scoring functions and
   splitting methods those decisions can be abstracted into the
   speci   cation of the n    d matrix v.
   2.2 scoring
   having built an initial representation the text, we next
   specify the coherence of a segment of text with a scoring
   function   (i, j ), which acts on the representation v and re-
   turns a score for the segment running from i (inclusive) to
   j (non-inclusive). the score can be a simple scalar or more
   general ob ject. in addition to the scoring function, we need
   to specify how to return an aggregrate score for the entire
   segmentation. this score aggregation function     can be as
   simple as adding the scores for the individual segments, or
   again some more general function. the score s (s) for an
   overall segmentation is given by aggregating the scores of
   all of the segments in the segmentation:
   s (s) =   (0, s1 )       (s1 , s2 )                    (sk   1 , sk ) .
   (2)
   finally, to frame the segmentation problem as a form of
   gle scalar. the key function ((cid:74)  (cid:75)) returns this single
   number,
   optimization, we need to map the aggregated score to a sin-
   c (s) = (cid:74)s (s)(cid:75) .
   so that the cost for the above segmentation is
   (3)
   for most of the segmentation schemes to be considered,
   the score function itself returns a scalar, so the score aggre-
   gation function     will be taken as simple addition with the
   key function the identity, but the generality here allows us
   to incorporate the c99 segmentation algorithm [3] into the
   same framework.
   2.3 splitting
   having speci   ed the representation of the text and scor-
   ing of the candidate segments, we need to prescribe how to
   choose the    nal segmentation.
   in this work, we consider
   three methods: (1) greedy splitting, which at each step in-
   serts the best available segmentation boundary; (2) dynamic
   programming based segmentation, which uses dynamic pro-
   gramming to    nd the optimal segmentation; and (3) an it-
   erative re   nement scheme, which starts with the greedy seg-
   mentation and then adjusts the boundaries to improve per-
   formance.
   2.3.1 greedy segmentation
   the greedy segmentation approach builds up a segmenta-
   tion into k segments by greedily inserting new boundaries
   at each step to minimize the aggregate score:
   s0 = {n }
   st+1 = arg min
   i   [1,n )
   c (st     {i})
   (4)
   (5)
   until the desired number of splits is reached. many published
   text segmentation algorithms are greedy in nature, including
   the original c99 algorithm [3].
   2.3.2 id145
   the greedy segmentation algorithm is not guaranteed to
      nd the optimal splitting, but id145 meth-
   ods can be used for the text segmentation problem formu-
   lated in terms of optimizing a scoring ob jective. for a de-
   tailed account of id145 and segmentation
   in general, see the thesis by terzi [16]. dynamic program-
   ming as been applied to text segmentation in fragkou et
   al. [7], with much success, but we will also consider here an
   optimizaton of the the c99 segmentation algorithm using a
   id145 approach.
   the goal of the id145 approach is to split
   the segmentation problem into a series of smaller segmenta-
   tion problems, by expressing the optimal segmentation of the
      rst n elements of the sequence into k segments in terms of
   the best choice for the last segmentation boundary. the ag-
   be minimized with respect to the key function (cid:74)  (cid:75):
   gregated score s (n, k) for this optimal segmentation should
   s (n, 1) =   (0, n)
   s (n, k) = (cid:74)  (cid:75)min
   s (l, k     1)       (l, n) .
   l<n
   while the id145 approach yeilds the op-
   timal segmentation for our decomposable score function, it
   can be costly to compute, especially for long texts. in prac-
   tice, both the optimal segmentation score and the resulting
   segmentation can be found in one pass by building up a ta-
   ble of segmentation scores and optimal cut indices one row
   at a time.
   2.3.3
   iterative relaxation
   inspired by the popular lloyd algorithm for id116, we
   attempt to retain the computational bene   t of the greedy
   segmentation approach, but realize additional performance
   gains by iteratively re   ning the segmentation. since text
   (7)
   (6)
   st+1
   k =
   segmentation problems require contiguous blocks of text, a
   natural scheme for relaxation is to try to move each segment
   boundary optimally while keeping the edges to either side of
   (cid:0)  (0, st
   it    xed:
   (cid:74)  (cid:75)
   1 )             
   k )(cid:1) (8)
   arg min
   l   (st
   k   1 , st
   k+1 )
   s (cid:0)st     {st
   k }     {l}(cid:1)
   k+1 )                    (st
   k   1 , l)       (l, st
         (st
   k   1 , st
   (cid:74)  (cid:75)
   (9)
   arg min
   l   (st
   k   1 , st
   k+1 )
   we will see in practice that by 20 iterations it has typically
   converged to a    xed point very close to the optimal dynamic
   programming segmentation.
   =
   (10)
   aij =
   3. scoring functions
   in the experiments to follow, we will test various choices
   for the representation, scoring function, and splitting method
   in the above general framework. the segmentation algo-
   rithms to be considered fall into three groups:
   3.1 c99 segmentation
   choi   s c99 algorithm [3] was an early text segmentation
   algorithm with promising results. the feature vector for an
   element of text is chosen as the pairwise cosine distances
   with other elements of text, where those elements in turn
   (cid:80)
   are represented by a bag of stemmed words vector (after
   preprocessing to remove stop words):
   (cid:113)(cid:80)
   (cid:80)
   w fi,w fj,w
   w f 2
   w f 2
   i,w
   j,w
   with fi,w the frequency of word w in element i. the pair-
   wise cosine distance matrix is noisy for these features, and
   since only the relative values are meaningful, c99 employs a
   ranking transformation, replacing each value of the matrix
   (cid:88)
   (cid:88)
   by the fraction of its neighbors with smaller value:
   i   r/2   l   i+r/2
   j   r/2   m   j+r/2
   l(cid:54)=i
   m (cid:54)=j
   (11)
   where the neighborhood is an r    r block around the en-
   try, the square brackets mean 1 if the inequality is satis   ed
   otherwise 0 (and values o    the end of the matrix are not
   counted in the sum, or towards the id172). each
   element of the text in the c99 algorithm is represented by a
   rank transformed vector of its cosine distances to each other
   element.
   (cid:80)
   the score function describes the average intersentence sim-
   ilarity by taking the overall score to be
   k   k(cid:80)
   where   k = (cid:80)
   (cid:80)
   c (s) =
   k   k
   vij is the sum of all
   sk   1   j<sk
   sk   1   i<sk
   ranked cosine similarities in a segment and   k = (sk+1    sk )2
   is the squared length of the segment. this score function
   vij , (j     i)2(cid:17)
   (cid:16) (cid:88)
   is still decomposable, but requires that we de   ne the local
   (cid:88)
   score function to return a pair,
   i   k<j
   i   k<j
   [aij > alm ] ,
   1
   r2     1
     (i, j ) =
   vij =
   (12)
   (13)
   ,
   ,
   ,
   with score aggregation function de   ned as component addi-
   tion,
   .
     
     
   (15)
   (14)
   (  1 ,   1 )     (  2 ,   2 ) = (  1 +   2 ,   1 +   2 ) ,
   and key function de   ned as division of the two components,
   (cid:74)(   ,   )(cid:75) =
   while earlier work with the c99 algorithm considered only
   a greedy splitting approach, in the experiments that follow
   we will use our more general framework to explore both op-
   timal id145 and re   ned iterative versions
   of c99. followup work by choi et al. [4] explored the e   ect
   of using combinations of lsa word vectors in eq. (10) in
   place of the fi,w . below we will explore the e   ect of using
   combinations of word vectors to represent the elements.
   3.2 average word vector
   to assess the utility of word vectors in segmentation, we
      rst investigate how they can be used to improve the c99
   algorithm, and then consider more general scoring functions
   based on our word vector representation. as the represen-
   (cid:88)
   tation of an element, we take
   w
   fiw vwk ,
   vik =
   (16)
   with fiw representing the frequency of word w in element i,
   and vwk representing the kth component of the word vector
   for word w as learned by a word vector training algorithm,
   such as id97 [11] or glove [13].
   the length of word vectors varies strongly across the vo-
   cabulary and in general correlates with word frequency. in
   order to mitigate the e   ect of common words, we will some-
   times weight the sum by the inverse document frequency
   (cid:88)
   (idf ) of the word in the corpus:
   w
   |d|
   dfw
   fiw log
   vik =
   vwk ,
   (17)
   where dfw is the number of documents in which word w
   appears. we can instead normalize the word vectors before
   (cid:88)
   vwk(cid:112)(cid:80)
   adding them together
   k v2
   wk
   w
   fiw   vwk
     vwk =
   vik =
   (18)
   ,
   or both weight by idf and normalize.
   segmentation is a form of id91, so a natural choice
   for scoring function is the sum of square deviations from the
   (cid:88)
   (cid:88)
   (cid:0)vlk       k (i, j )(cid:1)2
   mean of the segment, as used in id116:
   j   1(cid:88)
   k
   l
   1
   j     i
   l=i
   where   k (i, j ) =
     (i, j ) =
   vlk ,
   (19)
   (20)
   and which we call the euclidean score function. generally,
   however, cosine similarity is used for word vectors, making
   angles between words more important than distances.
   in
   some experiments, we therefore normalize the word vectors
      rst, so that a euclidean distance score better approximates
   the cosine distance (recall |  v       w|2
   2 = |  v |2
   2 + |   w|2
   2     2  v      w =
   2(1       v      w) for normalized vectors).
   (21)
   3.3 content vector segmentation (cvs)
   trained word vectors have a remarkable amount of struc-
   ture. analogy tasks such as man:woman::king:? can be
   solved by    nding the vector closest to the linear query:
   vwoman     vman + vking .
   arora et al. [1] constructed a generative model of text that
   explains how this linear structure arises and can be main-
   tained even in relatively low dimensional vector models. the
   generative model consists of a content vector which under-
   goes a random walk from a stationary distribution de   ned
   to be the product distribution on each of its components ck ,
   uniform on the interval [    1   
   , 1   
   ] (with d the dimension-
   d
   d
   ality of the word vectors). at each point in time, a word
   (cid:88)
   vector is generated by the content vector according to a log-
   linear model:
   p (w|c) =
   v
   exp(w    c) , zc =
   exp(v    c) .
   1
   zc
   (22)
   the slow drift of the content vectors helps to ensure that
   nearby words obey with high id203 a log-linear model
   for their co-occurence id203:
   (cid:107)vw + vw(cid:48) (cid:107)2     2 log z    o(1) ,
   log p (w, w
   (23)
   ) =
   (cid:48)
   1
   2d
   (24)
   for some    xed z .
   to segment text into coherent sections, we will boldly as-
   sume that the content vector in each putative segment is
   constant, and measure the log likelihood that all words in the
   segment are drawn from the same content vector c. (this is
   similar in spirit to the probabilistic segmentation technique
   (cid:88)
   log p (wi |c)     (cid:88)
   proposed by utiyama and isahara [17].) assuming the word
   draws {wi } are independent, we have that the log likelihood
   wi    c
   log p ({wi }|c) =
   i
   i
   is proportional to the sum of the dot products of the word
   vectors wi with the content vector c. we use a maximum
   likelihood estimate for the content vector:
   (cid:16)
   (cid:17)
   log p (c|{wi })
   c = arg max
   c
   log p ({wi }|c) + log p (c)     log p ({wi })
   (cid:88)
   = arg max
   c
   s.t.     1   
   1   
       arg max
   wi    c
   d
   d
   c
   (cid:88)
   (cid:88)
   this determines what we will call the content vector seg-
   mentation (cvs) algorithm, based on the score function
   i   l<j
   k
   wlk ck (i, j ) .
     (i, j ) =
   < ck <
   (26)
   (25)
   (27)
   (28)
   .
   the score   (i, j ) for a segment (i, j ) is the sum of the dot
   products of the word vectors wlk with the maximum likeli-
          1   
          (cid:88)
   hood content vector c(i, j ) for the segment, with components
   given by
   d
   i   l<j
   the maximum likelihood content vector thus has compo-
   nents    1   
   , depending on whether the sum of the word
   d
   vector components in the segment is positive or negative.
   ck (i, j ) = sign
   (29)
   wl,k
   .
   this score function will turn out to generate some of the
   most accurate segmentation results. note that cvs is com-
   pletely untrained with respect to the speci   c text to be seg-
   mented, relying only on a suitable set of word vectors, de-
   rived from some corpus in the language of choice. while
   cvs is most justi   able when working on the word vectors
   directly, we will also explore the e   ect of normalizing the
   word vectors before applying the ob jective.
   4. experiments
   to explore the e   cacy of di   erent segmentation strategies
   and algorithms, we performed segmentation experiments on
   two datasets. the    rst is the choi dataset [3], a common
   benchmark used in earlier segmentation work, and the sec-
   ond is a similarly constructed dataset based on articles up-
   loaded to the arxiv, as will be described in section 4.3. all
   code and data used for these experiments is available on-
   line2 .
   4.1 evaluation
   to evaluate the performance of our algorithms, we use
   two standard metrics: the pk metric and the windowdi   
   (wd) metric. for text segmentation, near misses should
   get more credit than far misses. the pk metric [2], captures
   the id203 for a probe composed of a pair of nearby ele-
   ments (at constant distance positions (i, i + k)) to be placed
   in the same segment by both reference and hypothesized seg-
   mentations. in particular, the pk metric counts the number
   n    k(cid:88)
   (cid:2)  hyp (i, i + k) (cid:54)=   ref (i, i + k)(cid:3)
   of disagreements on the probe elements:
   i=1
       1 ,
   # elements
   1
   k =
   2
   # segments
   nearest
   integer
   1
   n     k
   pk =
   (30)
   where   (i, j ) is equal to 1 or 0 according to whether or not
   both element i and j are in the same segment in hypothe-
   sized and reference segmentations, resp., and the argument
   of the sum tests agreement of the hypothesis and reference
   segmentations. (k is taken to be one less than the integer
   closest to half of the number of elements divided by the
   number of segments in the reference segmentation.) the
   total is then divided by the total number of probes. this
   metric counts the number of disagreements, so lower scores
   indicate better agreement between the two segmentations.
   trivial strategies such as choosing only a single segmen-
   tation, or giving each element its own segment, or giving
   constant boundaries or random boundaries, tend to produce
   values of around 50% [2].
   the pk metric has the disadvantage that it penalizes false
   positives more severely than false negatives, and can su   er
   when the distribution of segment sizes varies. pevzner and
   n    k(cid:88)
   (cid:2)bref (i, i + k) (cid:54)= bhyp (i, i + k)(cid:3) ,
   hearst [14] introduced the windowdi    (wd) metric:
   i=1
   where b(i, j ) counts the number of boundaries between loca-
   tion i and j in the text, and an error is registered if the hy-
   pothesis and reference segmentations disagree on the num-
   ber of boundaries. in practice, the pk and wd scores are
   2 github.com/alexalemi/segmentation
   1
   n     k
   w d =
   (31)
   highly correlated, with pk more prevalent in the literature
       we will provide both for most of the experiments here.
   4.2 choi dataset
   the choi dataset is used to test whether a segmentation
   algorithm can distinguish natural topic boundaries. it con-
   catenates the    rst n sentences from ten di   erent documents
   chosen at random from the brown corpus. the number of
   sentences taken from each document is chosen uniformly at
   random within a range speci   ed by the subset id (in the form
   minimum   maximum #sentences). there are four ranges
   considered: (3   5, 6   8, 9   11, 3   11), the    rst three of which
   have 100 example documents, and the last 400 documents.
   the dataset can be obtained from an archived version of the
   c99 segmentation code release3 . an extract from one of the
   documents in the test set is shown in fig. 1.
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   = = = = = = = = = =
   s o m e o f t h e f e a t u r e s o f t h e t o p p o r t i o n s o f f
   i g u r e 1
   a n d f i g u r e 2 w e r e m e n t i o n e d i n d i s c u s s i n g t
   a b l e 1
   .
   f i r s t , t h e o n s e t p r o f i l e s p r e a d s a c r o s s
   a p p r o x i m a t e l y 1 2 y e a r s f o r b o y s a n d 1 0 y e a r
   s f o r
   g i r l s .
   i n c o n t r a s t , 2 0 o f t h e 2 1 l i n e s i n t h e c o m p l e
   t i o n
   p r o f i l e ( e x c l u d i n g c e n t e r 5 f o r b o y s a n d 4 f
   o r
   g i r l s ) a r e b u n c h e d a n d e x t e n d o v e r a m u c h
   s h o r t e r p e r i o d , a p p r o x i m a t e l y 3 0 m o n t h s f
   o r b o y s
   a n d 4 0 m o n t h s f o r g i r l s .
   t h e m a t u r i t y c h a r t f o r e a c h s e x d e m o n s t r a t
   e s c l e a r l y
   t h a t o n s e t i s a p h e n o m e n o n o f i n f a n c y a n d e a
   r l y
   c h i l d h o o d w h e r e a s c o m p l e t i o n i s a p h e n o m e
   n o n o f
   t h e l a t e r p o r t i o n o f a d o l e s c e n c e .
   = = = = = = = = = =
   t h e m a n y l i n g u i s t i c t e c h n i q u e s f o r r e d u c i
   n g t h e
   a m o u n t o f d i c t i o n a r y i n f o r m a t i o n t h a t h a v
   e b e e n
   p r o p o s e d a l l o r g a n i z e t h e d i c t i o n a r y     s c o
   n t e n t s
   a r o u n d p r e f i x e s , s t e m s , s u f f i x e s , e t c .
   .
   a s i g n i f i c a n t r e d u c t i o n i n t h e v o u m e o f s t o
   r e
   i n f o r m a t i o n i s t h u s r e a l i z e d , e s p e c i a l l y
   f o r a
   h i g h l y i n f l e c t e d l a n g u a g e s u c h a s r u s s i a n
   .
   f o r e n g l i s h t h e r e d u c t i o n i n s i z e i s l e s s s t
   r i k i n g .
   t h i s a p p r o a c h r e q u i r e s t h a t : ( 1 ) e a c h t e x t
   w o r d b e
   s e p a r a t e d i n t o s m a l l e r e l e m e n t s t o e s t a b l
   i s h a
   c o r r e s p o n d e n c e b e t w e e n t h e o c c u r r e n c e a n
   d
   d i c t i o n a r y e n t r i e s , a n d ( 2 ) t h e i n f o r m a t i
   o n
   r e t r i e v e d f r o m s e v e r a l e n t r i e s i n t h e d i c t
   i o n a r y
   b e s y n t h e s i z e d i n t o a d e s c r i p t i o n o f t h e
   p a r t i c u l a r w o r d .
   = = = = = = = = = =
   figure 1: example of two segments from the choi
   dataset, taken from an entry in the 3   5 set. note the
   appearance of a    sentence    with the single character
      .    in the second segment on line 8. these short
   sentences can confound the benchmarks.
   4.2.1 c99 benchmark
   we will explore the e   ect of changing the representation
   and splitting strategy of the c99 algorithm.
   in order to
   give fair comparisons we implemented our own version of
   the c99 algorithm (oc99). the c99 performance depended
   sensitively on the details of the text preprocessing. details
   can be found in appendix a.
   3 http://web.archive.org/web/20010422042459/http://
   www.cs.man.ac.uk/~choif/software/c99- 1.2- release.
   tgz (we thank with martin riedl for pointing us to the
   dataset.)
   4.2.2 effect of word vectors on c99 variant
   the    rst experiment explores the ability of word vectors
   to improve the performance of the c99 algorithm. the word
   vectors were learned by glove [13] on a 42 billion word set of
   the common crawl corpus in 300 dimensions4 . we empha-
   size that these word vectors were not trained on the brown
   or choi datasets directly, and instead come from a general
   corpus of english. these vectors were chosen in order to
   isolate any improvement due to the word vectors from any
   confounding e   ects due to details of the training procedure.
   the results are summarized in table 1 below. the upper
   section cites results from [4], exploring the utility of using
   lsa word vectors, and showed an improvement of a few
   percent over their baseline c99 implementation. the mid-
   dle section shows results from [15], which augmented the
   c99 method by representing each element with a histogram
   of topics learned from lda. our results are in the lower
   section, showing how word vectors improve the performance
   of the algorithm.
   in each of these last experiments, we turned o    the rank
   transformation, pruned the stop words and punctuation, but
   did not stem the vocabulary. word vectors can be incorpo-
   rated in a few natural ways. vectors for each word in a
   sentence can simply be summed, giving results shown in the
   oc99tf row. but all words are not created equal, so the sen-
   tence representation might be dominated by the vectors for
   common words. in the oc99t   df row, the word vectors are
   weighted by idfi = log 500
   (i.e., the log of the inverse doc-
   dfi
   ument frequency of each word in the brown corpus, which
   has 500 documents in total) before summation. we see some
   improvement from using word vectors, for example the pk
   of 14.78% for the oc99t   df method on the 3   11 set, com-
   pared to pk of 15.56% for our baseline c99 implementation.
   on the shorter 3   5 test set, our oc99t   df method achieves
   pk of 10.27% versus the baseline oc99 pk of 14.22% . to
   compare to the various topic model based approaches, e.g.
   [15], we perform spherical id116 id91 on the word
   vectors [5] and represent each sentence as a histogram of
   its word clusters (i.e., as a vector in the space of clusters,
   with components equal to the number of its words in that
   that cluster). in this case, the word topic representations
   (oc99k50 and oc99k200 in table 1) do not perform as well
   as the c99 variants of [15]. but as was noted in [15], those
   topic models were trained on cross-validated subsets of the
   choi dataset, and bene   ted from seeing virtually all of the
   sentences in the test sets already in each training set, so
   have an unfair advantage that would not necessarily convey
   to real world applications. overall, the results in table 1
   illustrate that the word vectors obtained from glove can
   markedly improve existing segmentation algorithms.
   4.2.3 alternative scoring frameworks
   the use of word vectors permits consideration of natural
   scoring functions other than c99-style segmentation scoring.
   the second experiment examines alternative scoring frame-
   works using the same glove word vectors as in the previous
   experiment. to test the utility of the scoring functions more
   directly, for these experiments we used the optimal dynamic
   programming segmentation. results are summarized in ta-
   ble 2, which shows the average pk and wd scores on the
   4obtainable from http://www- nlp.stanford.edu/data/
   glove.42b.300d.txt.gz
   algorithm
   c99 [4]
   c99lsa
   c99 [15]
   c99lda
   oc99
   oc99tf
   oc99t   df
   oc99k50
   oc99k200
   3   5
   12
   9
   14.22
   12.14
   10.27
   20.39
   18.60
   pk
   9   11
   9
   7
   6   8
   11
   10
   11.20
   4.16
   11.59
   12.20
   13.17
   14.60
   15.87
   12.23
   23.76
   21.13
   17.37
   19.42
   3   5
   wd
   6   8
   9   11
   3   11
   3   11
   9
   5
   12.07
   4.89
   11.60
   12.22
   13.34
   15.22
   16.29
   12.30
   23.26
   21.34
   17.42
   19.60
   15.64
   15.22
   14.96
   24.63
   20.97
   15.56
   14.91
   14.78
   24.33
   20.85
   14.22
   12.14
   10.27
   20.39
   18.60
   table 1: e   ect of using word vectors in the c99 text segmentation
   algorithm. pk and wd results are shown
   (smaller values indicate better performance). the top section (c99 vs.
   c99lsa) shows the few percent
   improvement over the c99 baseline reported in [4] of using lsa to
   encode the words. the middle section
   (c99 vs. c99lda) shows the e   ect of modifying the c99 algorithm to work
   on histograms of lda topics in
   each sentence, from [15]. the bottom section shows the e   ect of using
   word vectors trained from glove [13]
   in our oc99 implementation of the c99 segmentation algorithm. the
   oc99tf implementation sums the word
   vectors in each sentence, with no rank transformation, after removing
   stop words and punctuation. oc99t   df
   weights the sum by the log of the inverse document frequency of each
   word. the oc99k models use the word
   vectors to form a topic model by doing spherical id116 on the word
   vectors. oc99k50 uses 50 clusters and
   oc99k200 uses 200.
   algorithm rep
   tf
   t   df
   oc99
   euclidean
   content (cvs)
   tf
   t   df
   tf
   t   df
   pk wd
   n
   11.94
   11.78
   -
   12.19
   12.27
   -
   8.28
   7.68
   f
   10.83
   t
   9.18
   14.27
   f 12.89
   8.95
   8.32
   t
   f
   5.29
   5.39
   5.55
   5.42
   t
   5.87
   5.75
   f
   t
   5.03
   5.12
   table 2: results obtained by varying the scoring
   function. these runs were on the 3   11 set from the
   choi database, with a word cut of 5 applied, after
   preprocessing to remove stop words and punctua-
   tion, but without id30. the cvs method does
   remarkably better than either the c99 method or a
   euclidean distance-based scoring function.
   3   11 subset of the choi dataset. in all cases, we removed
   stop words and punctuation, did not stem, but after prepro-
   cessing removed sentences with fewer than 5 words.
   note    rst that the id145 results for our
   implementation of c99 with tf weights gives pk = 11.78%,
   3% better than the greedy version result of 14.91% reported
   in table 1. this demonstrates that the original c99 algo-
   rithm and its applications can bene   t from a more exact
   minimization than given by the greedy approach. we con-
   sidered two natural score functions: the euclidean scoring
   function (eqn. (20)) which minimizes the sum of the square
   deviations of each vector in a segment from the average vec-
   tor of the segment, and the content vector scoring (cvs)
   (eqn. (28) of section 3.3), which uses an approximate log
   posterior for the words in the segment, as determined from
   its maximum likelihood content vector.
   in each case, we
   consider vectors for each sentence generated both as a strict
   sum of the words comprising it (tf approach), and as a sum
   weighted by the log idf (t   df approach, as in sec. 4.2.2). ad-
   ditionally, we consider the e   ect of normalizing the element
   vectors before starting the score minimization, as indicated
   by the n column.
   the cvs score function eqn. (28) performs the best over-
   all, with pk scores below 6%, indicating an improved seg-
   mentation performance using a score function adapted to
   the choice of representation. while the most principled
   score function would be the content score function using
   tf weighted element vectors without id172, the nor-
   malized t   df scheme actually performs the best. this is
   probably due to the uncharacteristically large e   ect com-
   mon words have on the element representation, which the
   log idf weights and the id172 help to mitigate.
   strictly speaking, the idf weighted schemes cannot claim
   to be completely untrained, as they bene   t from word usage
   statistics in the choi test set, but the raw cvs method still
   demonstrates a marked improvement on the 3   11 subset,
   5.29% pk versus the optimal c99 baseline of 11.78% pk .
   4.2.4 effect of splitting strategy
   to explore the e   ect of the splitting strategy and to com-
   pare with our overall results on the choi test set against
   other published benchmarks, in our third experiment we ran
   the raw cvs method against all of the choi test subsets, us-
   ing all three splitting strategies discussed: greedy, re   ned,
   and id145. these results are summarized
   in table 3.
   overall, our method outperforms all previous untrained
   methods. as commented regarding table 1 (toward the end
   of subsection 4.2.2), we have included the results of the topic
   modelling based approaches m09 [12] and r12 [15] for ref-
   erence. but due to repeat appearance of the same sentences
   throughout each section of the choi dataset, methods that
   split that dataset into test and training sets have unavoid-
   able access to the entirety of the test set during training, al-
   beit in di   erent order.5 these results can therefore only be
   5 in [15], it is observed that    this makes the choi data set
   arti   cially easy for supervised approaches.   
   alg
   tt [3]
   c99 [3]
   c01 [4]
   u00 [17]
   f04 [7]
   g-cvs
   r-cvs
   dp-cvs
   m09 [12]
   r12 [15]
   3   5
   44
   12
   10
   9
   5.5
   5.14
   3.92
   3.41
   2.2
   1.24
   6   8
   43
   9
   7
   7
   3.0
   4.82
   3.75
   3.45
   2.3
   0.76
   9   11
   48
   9
   5
   5
   1.3
   6.38
   5.17
   4.45
   4.1
   0.56
   3   11
   46
   12
   9
   10
   7.0
   6.49
   5.65
   5..29
   2.3
   0.95
   table 3: some published pk results on the choi
   dataset against our raw cvs method. g-cvs uses
   a greedy splitting strategy, r-cvs uses up to 20 it-
   erations to re   ne the results of the greedy strategy,
   and dp-cvs shows the optimal results obtained by
   id145. we include the topic mod-
   eling results m09 and r12 for reference, but for rea-
   sons detailed in the text do not regard them as com-
   parable, due to their mingling of test and training
   samples.
   figure 2: results from last column of table 3 repro-
   duced to highlight the performance of the cvs seg-
   mentation algorithm compared to similar untrained
   algorithms. its superior performance in an unsuper-
   vised setting suggests applications on documents    in
   the wild   .
   compared to other algorithms permitted to make extensive
   use of the test data during cross-validation training. only
   the tt, c99, u00 and raw cvs method can be considered
   as completely untrained. the c01 method derives its lsa
   vectors from the brown corpus, from which the choi test set
   is constructed, but that provides only a weak bene   t, and
   the f04 method is additionally trained on a subset of the
   test set to achieve its best performance, but its use only of
   idf values provides a similarly weak bene   t.
   we emphasize that the raw cvs method is completely
   independent of the choi test set, using word vectors derived
   from a completely di   erent corpus.
   in fig. 2, we repro-
   duce the relevant results from the last column of table 1 to
   highlight the performance bene   ts provided by the semantic
   id27.
   note also the surprising performance of the re   ned split-
   ting strategy, with the r-cvs results in table 3 much lower
   than the greedy g-cvs results, and moving close to the op-
   timal dp-cvs results, at far lower computational cost. in
   particular, taking the id145 segmentation
   as the true segmentation, we can assess the performance of
   r-cvs vs dp-cvs [3]
   3   5
   0.90
   6   8
   0.65
   9   11
   1.16
   3   11
   1.15
   table 4: treating the id145 splits
   as the true answer, the error of the re   ned splits as
   measured in pk across the subsets of the choi test
   set.
   the re   ned strategy. as seen in table 4, the re   ned segmen-
   tation very closely approximates the optimal segmentation.
   this is important in practice since the dynamic program-
   ming segmentation is much slower, taking    ve times longer
   to compute on the 3   11 subset of the choi test set. the
   id145 segmentation becomes computation-
   ally infeasible to do at the scale of word level segmentation
   on the arxiv dataset considered in the next section, whereas
   the re   ned segmentation method remains eminently feasible.
   4.3 arxiv dataset
   performance evaluation on the choi test set implements
   segmentation at the sentence level, i.e., with segments of
   composed of sentences as the basic elements. but text sources
   do not necessarily have well-marked sentence boundaries.
   the arxiv is a repository of scienti   c articles which for prac-
   tical reasons extracts text from pdf documents (typically
   using pdfminer/pdf2txt.py). that postscript-based for-
   mat was originally intended only as a means of formatting
   text on a page, rather than as a network transmission for-
   mat encoding syntactic or semantic information. the result
   is often somewhat corrupted, either due to the handling of
   mathematical notation, the presence of footers and headers,
   or even just font encoding issues.
   to test the segmentation algorithms in a realistic set-
   ting, we created a test set similar to the choi test set, but
   based on text extracted from pdfs retrieved from the arxiv
   database. each test document is composed of a random
   number of contiguous words, uniformly chosen between 100
   and 300, sampled at random from the text obtained from
   arxiv articles. the text was preprocessed by lowercasing
   and inserting spaces around every non-alphanumeric char-
   acter, then splitting on whitespace to tokenize. an example
   of two of the segments of the    rst test document is shown
   in figure 3 below.
   this is a much more di   cult segmentation task: due to
   the presence of numbers and many periods in references,
   there are no clear sentence boundaries on which to initially
   group the text, and no natural boundaries are suggested in
   the test set examples. here segmentation algorithms must
   work directly at the    word    level, where word can mean a
   punctuation mark. the presence of garbled mathematical
   formulae adds to the di   culty of making sense of certain
   streams of text.
   in table 5, we summarize the results of three word vector
   powered approaches, comparing a c99 style algorithm to
   our content vector based methods, both for unnormalized
   and normalized word vectors. since much of the language
   of the scienti   c articles is specialized, the word vectors used
   in this case were obtained from glove trained on a corpus
   of similarly preprocessed texts from 98,392 arxiv articles.
   (since the elements are now words rather than sentences,
   the only issue involves whether or not those word vectors
   are normalized.) as mentioned, the id145
   approach is prohibitively expensive for this dataset.
   024681012pkdp-cvsr-cvsg-cvsf04u00c01c991
   2
   . n a t u r e _ 4 1 4 : 4 4 1 - 4 4 3 . 1 2 s e i n e n , i . a n d s c
   h r a m
   a . 2 0 0 6 . s o c i a l _ s t a t u s a n d g r o u p n o r m s :
   i n d i r e c t _ r e c i p r o c i t y i n a h e l p i n g e x p e r i
   m e n t .
   e u r o p e a n _ e c o n o m i c _ r e v i e w 5 0 : 5 8 1 - 6 0 2 . s
   i l v a ,
   e . r . , j a f f e , k . 2 0 0 2 . e x p a n d e d f o o d
   c h o i c e a s a p o s s i b l e f a c t o r i n t h e e v o l u t i o
   n o f
   e u s o c i a l i t y i n v e s p i d a e s o c i o b i o l o g y 3 9 :
   2 5 - 3 6
   . s m i t h , j . , v a n d y k e n , j . d . , z e e j u n e ,
   p . c . 2 0 1 0 . a g e n e r a l i z a t i o n o f h a m i l t o n     s
   r u l e f o r t h e e v o l u t i o n o f m i c r o b i a l c o o p e r
   a t i o n
   s c i e n c e _ 3 2 8 , 1 7 0 0 - 1 7 0 3 . z h a n g , j . , w a n g ,
   j . , s u n , s . , w a n g , l . , w a n g , z . , x i a ,
   c . 2 0 1 2 . e f f e c t o f g r o w i n g s i z e o f i n t e r a c t
   i o n
   n e i g h b o r s o n t h e e v o l u t i o n o f c o o p e r a t i o n
   i n
   s p a t i a l s n o w d r i f t _ g a m e . c h i n e s e _ s c i e n c
   e b u l l e t i n
   5 7 : 7 2 4 - 7 2 8 . z i m m e r m a n , m . , e g u     i l u z ,
   v . , s a n _ m i g u e l ,
   o f ) e , e q u i p p e d _ w i t h t h e t o p o l o g y o f
   w e a k _ c o n v e r g e n c e . w e w i l l s t a t e s o m e r e s u
   l t s
   a b o u t r a n d o m m e a s u r e s . 1 0 d e f i n i t i o n a . 1 (
   f i r s t t w o m o m e n t m e a s u r e s ) . f o r a
   r a n d o m _ v a r i a b l e z , t a k i n g v a l u e s i n p ( e ) ,
   a n d k = 1 , 2 , . . . , t h e r e i s a
   u n i q u e l y _ d e t e r m i n e d m e a s u r e    ( k ) o n b ( e k
   )
   s u c h t h a t e [ z ( a 1 )   _   _    z ( a k ) ] =    ( k )
   ( a 1       _   _       a k ) f o r a 1 , . . . , a k     b ( e )
   . t h i s i s c a l l e d t h e k t h _ m o m e n t m e a s u r e .
   e q u i v a l e n t l y ,    ( k ) i s t h e u n i q u e m e a s u r e s
   u c h
   t h a t e [ h z ,    1 i    _    _    h z ,    k i ] = h    ( k )
   ,    1   _   _       k i , w h e r e h . , . i d e n o t e s
   i n t e g r a t i o n . l e m m a a . 2 ( c h a r a c t e r i s a t i o
   n o f
   d e t e r m i n i s t i c r a n d o m m e a s u r e s ) . l e t z b e a
   r a n d o m _ v a r i a b l e _ t a k i n g v a l u e s i n p ( e ) w i
   t h t h e
   f i r s t t w o m o m e n t m e a s u r e s    : =    ( 1 ) a n d    (
   2 ) . t h e n t h e
   f o l l o w i n g _ a s s e r t i o n s _ a r e _ e q u i v a l e n t :
   1 . t h e r e
   i s        p ( e ) w i t h z =    , a l m o s t _ s u r e l y . 2 .
   t h e s e c o n d _ m o m e n t m e a s u r e h a s p r o d u c t - f o
   r m , i
   . e .    ( 2 ) =           ( w h i c h i s e q u i v a l e n t t o e
   [ h z ,    1 i    h z ,    2 i ] = h    ,    1 i    h    ,   
   2 i ( t h i s i s i n f a c t e q u i v a l e n t t o e [ h z ,    i 2
   ]
   figure 3: example of two of the segments from a
   document in the arxiv test set.
   alg
   wd
   pk
   s
   oc99 g 47.29    47.31   
   oc99 r 47.60    49.30   
   28.23
   cvs g 26.07
   27.73
   cvs r 25.55
   cvsn g 24.63
   26.69
   cvsn r 24..03 26..15
   table 5:
   results on the arxiv test set for the
   c99 method using word vectors (oc99), our cvs
   method, and cvs method with normalized word
   vectors (cvsn). the pk and wd metrics are given
   for both the greedy (g) and re   ned splitting strate-
   gies (r), with respect to the reference segmentation
   in the test set. the re   ned strategy was allowed up
   to 20 iterations to converge. the re   nement con-
   verged for all of the cvs runs, but failed to converge
   for some documents in the test set under the c99
   method. re   nement improved performance in all
   cases, and our cvs methods improve signi   cantly
   over the c99 method for this task. (*) the oc99 re-
   sults are computed on the    rst 100 test documents
   due to lack of time. the results should still be rep-
   resentative.
   we see that the cvs method performs far better on the
   test set than the c99 style segmentation using word vectors.
   the pk and wd values obtained are not as impressive as
   those obtained on the choi test set, but this test set o   ers
   a much more challenging segmentation task: it requires the
   methods to work at the level of words, and as well includes
   the possibility that natural topic boundaries occur in the test
   set segments themselves. the segmentations obtained with
   the cvs method typically appear sensibly split on section
   boundaries, references and similar formatting boundaries,
   not known in advance to the algorithm.
   as a    nal illustration of the e   ectiveness of our algorithm
   at segmenting scienti   c articles, we   ve applied the best per-
   forming algorithm to the current article. figure 4 shows
   how the algorithm segments the article roughly along sec-
   tion borders.
   5. conclusion
   we have presented a general framework for describing and
   developing segmentation algorithms, and compared some ex-
   isting and new strategies for representation, scoring and
   splitting. we have demonstrated the utility of semantic
   id27s for segmentation, both in existing algo-
   rithms and in new segmentation algorithms. on a real world
   segmentation task at word level, we   ve demonstrated the
   ability to generate useful segmentations of scienti   c articles.
   in future work, we plan to use this segmentation technique
   to facilitate retrieval of documents with segments of con-
   centrated content, and to identify documents with localized
   sections of similar content.
   6. acknowledgements
   this work was supported by nsf iis-1247696. we thank
   james p. sethna for useful discussions and for feedback on
   the manuscript.
   7. references
   [1] s. arora, y. li, t. m. yingyu liang, and a. risteski.
   id93 on context spaces: towards an
   explanation of the mysteries of semantic word
   embeddings. 2015, arxiv:1502.03520.
   [2] d. beeferman, a. berger, and j. la   erty. statistical
   models for text segmentation. machine learning,
   34(1-3):177   210, 1999.
   [3] f. y. choi. advances in domain independent linear
   text segmentation. in proceedings of the 1st north
   american chapter of the association for
   computational linguistics conference, pages 26   33.
   association for computational linguistics, 2000,
   arxiv:cs/0003083.
   [4] f. y. choi, p. wiemer-hastings, and j. moore. latent
   semantic analysis for text segmentation. in in
   proceedings of emnlp. citeseer, 2001.
   [5] a. coates and a. y. ng. learning feature
   representations with id116. in neural networks:
   tricks of the trade, pages 561   580. springer, 2012.
   [6] s. t. dumais. latent semantic analysis. annual review
   of information science and technology, 38(1):188   230,
   2004.
   [7] p. fragkou, v. petridis, and a. kehagias. a dynamic
   programming algorithm for linear text segmentation.
   journal of intel ligent information systems,
   23(2):179   197, 2004.
   [8] m. a. hearst. texttiling: segmenting text into
   multi-paragraph subtopic passages. computational
   linguistics, 23(1):33   64, 1997.
   [9] o. levy and y. goldberg. neural id27 as
   implicit id105. in advances in neural
   information processing systems, pages 2177   2185,
   2014.
   [10] t. mikolov, q. v. le, and i. sutskever. exploiting
   similarities among languages for machine translation.
   arxiv preprint arxiv:1309.4168, 2013.
   [11] t. mikolov, i. sutskever, k. chen, g. s. corrado, and
   j. dean. distributed representations of words and
   phrases and their compositionality. in advances in
   neural information processing systems, pages
   3111   3119, 2013.
   [12] h. misra, f. yvon, j. m. jose, and o. cappe. text
   segmentation via id96: an analytical study.
   in proceedings of the 18th acm conference on
   information and know ledge management, pages
   1553   1556. acm, 2009.
   [13] j. pennington, r. socher, and c. d. manning. glove:
   global vectors for word representation. proceedings of
   the empiricial methods in natural language
   processing (emnlp 2014), 12, 2014.
   [14] l. pevzner and m. a. hearst. a critique and
   improvement of an evaluation metric for text
   segmentation. computational linguistics, 28(1):19   36,
   2002.
   [15] m. riedl and c. biemann. text segmentation with
   topic models. journal for language technology and
   computational linguistics, 27(1):47   69, 2012.
   [16] e. terzi et al. problems and algorithms for sequence
   segmentations. 2006.
   [17] m. utiyama and h. isahara. a statistical model for
   domain-independent text segmentation. in proceedings
   of the 39th annual meeting on association for
   computational linguistics, pages 499   506. association
   for computational linguistics, 2001.
   figure 4: e   ect of applying our segmentation algo-
   rithm to this paper.
   appendix
   a. details of c99 representation
   this set of experiments compare to the results reported in
   [3]. we implemented our own version of the c99 algorithm
   (oc99) and tested it on the choi dataset. we explored the
   e   ect of various changes to the representation part of the
   algorithm, namely the e   ects of removing stop words, cut-
   ting small sentence sizes, id30 the words, and perform-
   ing the rank transformation on the cosine similarity matrix.
   for id30, the implementation of the porter id30
   algorithm from nltk was used. for stopwords, we used the
   list distributed with the c99 code augmented by a list of
   punctuation marks. the results are summarized in table 6.
   while we reproduce the results reported in [4] without
   the rank transformation (c99 in table 6), our results for
   the rank transformed results (last two lines for oc99) show
   better performance without id30. this is likely due to
   particulars relating to details of the text transformations,
   such at the precise id30 algorithm and the stopword
   list. we attempted to match the choices made in [4] as
   much as possible, but still showed some deviations.
   perhaps the most telling deviation is the 1.5% swing in
   results for the last two rows, whose only di   erence was a
   change in the tie breaking behavior of the algorithm. in our
   best result, we minimized the ob jective at each stage, so in
   the case of ties would break at the earlier place in the text,
   whereas for the tbr row, we maximized the negative of the
   ob jective, so in the case of ties would break on the rightmost
   equal value.
   these relatively large swings in the performance on the
   choi dataset suggest that it is most appropriate to com-
   pare di   erences in parameter settings for a particular im-
   plementation of an algorithm. comparing results between
   di   erent articles to assess performance improvements due to
   algorithmic changes hence requires careful attention to the
   implemention details.
   note
   c99 [4]
   oc99
   reps
   tbr
   best
   cut
   0
   0
   0
   0
   0
   0
   0
   5
   0
   0
   0
   5
   5
   5
   5
   5
   5
   5
   5
   5
   stop
   t
   t
   t
   t
   t
   t
   f
   f
   t
   f
   t
   t
   t
   t
   t
   t
   t
   t
   t
   t
   stem rank
   f
   0
   11(?)
   f
   11(?)
   t
   0
   f
   11
   f
   11
   t
   f
   0
   0
   f
   0
   f
   0
   t
   0
   t
   t
   0
   3
   t
   5
   t
   7
   t
   t
   9
   11
   t
   13
   t
   11
   f
   f
   11
   pk (%) wd (%)
   -
   23
   13
   -
   -
   12
   22.52
   22.52
   16.72
   16.69
   19.96
   17.90
   32.26
   32.28
   32.76
   32.73
   22.52
   22.52
   32.28
   32.26
   23.33
   23.33
   23.56
   23.59
   18.30
   18.17
   17.56
   17.44
   17.05
   16.95
   17.12
   17.20
   17.14
   17.07
   17.19
   17.11
   17.12
   17.04
   15.56
   15.64
   table 6: e   ects of text representation on the per-
   formance of the c99 algorithm. the cut column de-
   notes the cuto    for the length of a sentence after pre-
   processing. the stop column denotes whether stop
   words and punctuation are removed. the stem col-
   umn denotes whether the words are passed through
   the porter id30 algorithm. the rank column
   denotes the size of the kernel for the ranking trans-
   formation. evaluations are given both as the pk
   metric and the window di    (wd) score. all ex-
   periments are done on the 400 test documents in
   the 3   11 set of the choi dataset. the upper sec-
   tion cites results contained in the cwm 2000 paper
   [4]. the second section is an attempt to match these
   results with our implementation (oc99). the third
   section attempts to give an overview of the e   ect of
   di   erent parameter choices for the representation
   step of the algorithm. the last section reports our
   best observed result as well as a run (tbr) with
   the same parameter settings, but with a tie-breaking
   strategy that takes right-most rather then left-most
   equal value.
     * copy lines
     * copy permalink
     * [54]view git blame
     * [55]reference in new issue

   ____________________ (button) go

     *    2019 github, inc.
     * [56]terms
     * [57]privacy
     * [58]security
     * [59]status
     * [60]help

     * [61]contact github
     * [62]pricing
     * [63]api
     * [64]training
     * [65]blog
     * [66]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [67]reload to refresh your
   session. you signed out in another tab or window. [68]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/alexalemi/segmentation/commits/master.atom
   3. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/alexalemi/segmentation/blob/master/data/segmentation.txt
  32. https://github.com/join
  33. https://github.com/login?return_to=/alexalemi/segmentation
  34. https://github.com/alexalemi/segmentation/watchers
  35. https://github.com/login?return_to=/alexalemi/segmentation
  36. https://github.com/alexalemi/segmentation/stargazers
  37. https://github.com/login?return_to=/alexalemi/segmentation
  38. https://github.com/alexalemi/segmentation/network/members
  39. https://github.com/alexalemi
  40. https://github.com/alexalemi/segmentation
  41. https://github.com/alexalemi/segmentation
  42. https://github.com/alexalemi/segmentation/issues
  43. https://github.com/alexalemi/segmentation/pulls
  44. https://github.com/alexalemi/segmentation/projects
  45. https://github.com/alexalemi/segmentation/pulse
  46. https://github.com/alexalemi/segmentation/blob/a72cac9a939f6b8bdd07376a6193230f25e35da6/data/segmentation.txt
  47. https://github.com/join?source=prompt-blob-show
  48. https://github.com/alexalemi/segmentation
  49. https://github.com/alexalemi/segmentation/tree/master/data
  50. https://github.com/alexalemi/segmentation/find/master
  51. https://github.com/alexalemi/segmentation/raw/master/data/segmentation.txt
  52. https://github.com/alexalemi/segmentation/blame/master/data/segmentation.txt
  53. https://github.com/alexalemi/segmentation/commits/master/data/segmentation.txt
  54. https://github.com/alexalemi/segmentation/blame/a72cac9a939f6b8bdd07376a6193230f25e35da6/data/segmentation.txt
  55. https://github.com/alexalemi/segmentation/issues/new
  56. https://github.com/site/terms
  57. https://github.com/site/privacy
  58. https://github.com/security
  59. https://githubstatus.com/
  60. https://help.github.com/
  61. https://github.com/contact
  62. https://github.com/pricing
  63. https://developer.github.com/
  64. https://training.github.com/
  65. https://github.blog/
  66. https://github.com/about
  67. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt
  68. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt

   hidden links:
  70. https://github.com/
  71. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt
  72. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt
  73. https://github.com/alexalemi/segmentation/blob/master/data/segmentation.txt
  74. https://github.com/
