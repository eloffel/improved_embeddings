   #[1]facebook research    feed [2]facebook research    comments feed
   [3]alternate [4]alternate

   [5]skip to content

   iframe: [6]https://www.googletagmanager.com/ns.html?id=gtm-wspss3d

   [7]facebook research
   search ____________________ (button) submit
   [8]toggle search (button)

     * [9]research areas (button) show dropdown menu
          + [10]ar/vr
          + [11]connectivity
          + [12]computational photography & intelligent cameras
          + [13]id161
          + [14]data science
          + [15]economics & computation
          + [16]facebook ai research
          + [17]human computer interaction & ux
          + [18]machine learning
          + [19]natural language processing & speech
          + [20]security & privacy
          + [21]systems & networking
     * [22]publications
     * [23]people
     * [24]academic programs (button) show dropdown menu
          + [25]facebook fellowship program
          + [26]research awards
          + [27]research collaborations
          + [28]research award recipients
          + [29]visiting researchers and postdocs
     * [30]downloads & projects
     * [31]careers
     * [32]blog

   search ____________________ (button) submit
   [33]close search

   february 14, 2018

announcing tensor comprehensions

   by: nicolas vasilache, oleksandr zinenko - inria & di ens, theodoros
   theodoridis - eth z  rich, [34]priya goyal, zachary devito, william s.
   moses - mit csail, sven verdoolaege, andrew adams, albert cohen - inria
   & di ens & fair

   today, facebook ai research (fair) is announcing the release of tensor
   comprehensions, a c++ library and mathematical language that helps
   bridge the gap between researchers, who communicate in terms of
   mathematical operations, and engineers focusing on the practical needs
   of running large-scale models on various hardware backends. the main
   differentiating feature of tensor comprehensions is that it represents
   a unique take on just-in-time compilation to produce the
   high-performance codes that the machine learning community needs,
   automatically and on-demand.

order of magnitude productivity gains

   the typical workflow for creating new high-performance machine learning
   (ml) layers can span days or weeks of engineering work through a two
   phase process:
    1. a researcher writes a new layer at a numpy-level abstraction,
       chaining existing operations in a deep learning library like
       pytorch, and tests it in small-scale experiments. the performance
       of the code implementing the validated idea needs to be accelerated
       by an order of magnitude to run large-scale experiments.
    2. an engineer takes the layer and writes efficient code for gpus and
       cpus:

   a. the engineer needs to be a high-performance computing expert of
   which only a limited supply of talent is available

   b. the engineer needs to acquire context, map out a strategy, write and
   debug code

   c. moving the code to the backend involves mundane tasks, such as
   verbose argument checking and adding boilerplate integration code

   as a consequence and over the last few years, the deep learning
   community has grown to rely on high-performance libraries such as
   cublas, mkl, and cudnn to get high-performance code on gpus and cpus.
   experimenting with ideas that deviate from the primitives provided in
   these libraries involves a level and magnitude of engineering that can
   be intimidating to researchers.

   we anticipate great practical value in open-sourcing a package that
   shortens this process from days or weeks to minutes. with tensor
   comprehensions, our vision is for researchers to write their idea out
   in mathematical notation, this notation automatically gets compiled and
   tuned by our system, and the result is specialized code with good
   performance.

   in this release, we provide:
     * a mathematical notation to express a broad family of ml ideas in a
       simple syntax
     * a c++ frontend for this mathematical notation based on halide ir
     * a polyhedral just-in-time (jit) compiler based on integer set
       library (isl)
     * a multi-threaded, multi-gpu autotuner based on evolutionary search

related earlier work

   a recent language that has become popular in the adjacent field of
   high-performance image processing is [35]halide. halide uses similar
   high-level functional syntax to describe an image processing pipeline,
   and then, in a separate block of code, explicitly schedules it onto the
   hardware, specifying in detail how operations are tiled, vectorized,
   parallelized, and fused. this makes it a very productive language for
   people with architectural expertise, but it is difficult to use for
   most ml practitioners. automatic scheduling of halide is an active
   research area, but there is no good solution yet for ml code running on
   a gpu.

   tensor comprehensions uses the halide compiler as a library. we build
   on halide   s intermediate representation (ir) and analysis tools, and
   pair it with polyhedral compilation techniques, so that you can write
   layers using similar high-level syntax but without the need to
   explicitly say how it is going to run. we also found ways to make our
   language even more concise, eliminating the need to specify loop bounds
   for reductions.

the details

   tensor comprehensions use halide and polyhedral compilation techniques
   to automatically synthesize cuda kernels with delegated memory
   management and synchronization. this translation performs optimizations
   for general operator fusion, fast local memory, fast reductions and jit
   specialization for specific sizes. since we do not try to own or
   optimize memory management, our flow is easily and efficiently
   integrated into any ml framework and any language that allows calling
   c++ functions.

   contrary to classical compiler technology and library approaches,
   polyhedral compilation allows tensor comprehensions to schedule
   computation of individual tensor elements on-demand for each new
   network.

   at the cuda level, it combines affine loop transformations,
   fusion/fission and automatic parallelization while ensuring data is
   correctly moved through the memory hierarchy.

   the numbers in the figure show the order in which tensor elements were
   initially computed and arrows represent dependencies between them. in
   this example, the figure rotation corresponds to loop interchange which
   enables deep operator fusion.

   to drive the search procedure, we also provide an integrated
   multi-threaded, multi-gpu autotuning library which uses evolutionary
   search to generate and evaluate thousands of implementation
   alternatives and select the best performing ones. just call the tune
   function on your tensor comprehension and watch the performance
   improve, live; stop when you are satisfied. the best strategy is
   serialized via protobuf and reusable immediately or in offline
   scenarios.

   on the performance side, while we still have many improvements in the
   works, tensor comprehensions can already match or beat the performance
   of current ml frameworks integrated with hand-tuned libraries, in
   favorable cases. this is mainly achieved by the ability to adapt code
   generation strategies to specific problem sizes. the following bar
   chart illustrates performance gains we observed when comparing kernels
   produced automatically by tensor comprehensions against existing
   alternatives in caffe2 and aten (which use vendor library
   implementations such as cudnn). for more details about the performance
   one may currently expect from tensor comprehensions please see our
   [36]accompanying paper.

   as we extend our contribution to more hardware backends, tensor
   comprehensions will complement fast libraries written by hardware
   manufacturers such as nvidia and intel, and will be used in conjunction
   with libraries such as cudnn, mkl or nnpack.

what to expect next

   this release will allow researchers and programmers to write layers in
   a notation that is similar to the maths they use in their papers and
   communicate concisely the intent of their program. they will also be
   able to take that notation and translate it easily into a fast
   implementation in a matter of minutes rather than days. as the
   toolchain grows, we expect usability and performance to increase and
   benefit the whole community.

   we will release pytorch integration for tensor comprehensions at a
   later date.

   we are grateful for frequent exchanges with and feedback from the
   frameworks teams and are looking forward to bringing this exciting new
   technology to your favorite ml framework.

   fair is committed to open science and working with the machine learning
   community to push ai research further. tensor comprehensions is already
   a collaboration between facebook, inria, eth zurich and mit. our work
   is in the early stages and we   re excited to share it early and look
   forward to improving it with feedback from the community.

get started

   [37]tensor comprehensions is available under the apache 2.0 license.
   [38]documentation
   on [39]arxiv
   on [40]slack
   email: [41]tensorcomp@fb.com
   areas: [42]facebook ai research facebook ai research

related content

   [43]

   blog

open-sourcing pytorch-biggraph for faster embeddings of extremely large
graphs

   april 2, 2019
   [44]

   blog

turing award presented to yann lecun, geoffrey hinton and yoshua bengio

   march 27, 2019
   [45]

   blog

q&a with facebook ai residents tatiana likhomanenko and siddharth karamcheti

   march 15, 2019
   [46]

   blog

facebook ai researchers share perspectives on diversity on international
women   s day

   march 8, 2019

   iframe:
   [47]https://www.facebook.com/plugins/page.php?href=https%3a%2f%2fwww.fa
   cebook.com%2facademics&tabs&width=340&height=70&small_header=true&adapt
   _container_width=true&hide_cover=true&show_facepile=false&appid

     * [48]rss feed
     * [49]about
     * [50]careers
     * [51]privacy
     * [52]cookies
     * [53]terms
     * [54]help

   facebook    2019

   to help personalize content, tailor and measure ads, and provide a
   safer experience, we use cookies. by clicking or navigating the site,
   you agree to allow our collection of information on and off facebook
   through cookies. learn more, including about available controls:
   [55]cookies policy

references

   1. https://research.fb.com/feed/
   2. https://research.fb.com/comments/feed/
   3. https://research.fb.com/wp-json/oembed/1.0/embed?url=https://research.fb.com/announcing-tensor-comprehensions/
   4. https://research.fb.com/wp-json/oembed/1.0/embed?url=https://research.fb.com/announcing-tensor-comprehensions/&format=xml
   5. https://research.fb.com/announcing-tensor-comprehensions/#content
   6. https://www.googletagmanager.com/ns.html?id=gtm-wspss3d
   7. https://research.fb.com/
   8. javascript:void(0)
   9. https://research.fb.com/research-areas/
  10. https://research.fb.com/category/augmented-reality-virtual-reality/
  11. https://research.fb.com/category/connectivity/
  12. https://research.fb.com/category/computational-photography-and-intelligent-cameras/
  13. https://research.fb.com/category/computer-vision/
  14. https://research.fb.com/category/data-science/
  15. https://research.fb.com/category/economics-and-computation/
  16. https://research.fb.com/category/facebook-ai-research
  17. https://research.fb.com/category/human-computer-interaction-and-ux/
  18. https://research.fb.com/category/machine-learning/
  19. https://research.fb.com/category/natural-language-processing-and-speech/
  20. https://research.fb.com/category/security-and-privacy/
  21. https://research.fb.com/category/systems-and-networking/
  22. https://research.fb.com/publications/
  23. https://research.fb.com/people/
  24. https://research.fb.com/programs/
  25. https://research.fb.com/programs/fellowship/
  26. https://research.fb.com/programs/research-awards/
  27. https://research.fb.com/programs/research-collaborations/
  28. https://research.fb.com/programs/faculty-awards
  29. https://research.fb.com/programs/post-docs-and-sabbaticals/
  30. https://research.fb.com/downloads/
  31. https://research.fb.com/careers/
  32. https://research.fb.com/blog/
  33. javascript:void(0)
  34. https://research.fb.com/people/goyal-priya/
  35. http://halide-lang.org/
  36. https://arxiv.org/abs/1802.04730
  37. http://github.com/facebookresearch/tensorcomprehensions
  38. https://facebookresearch.github.io/tensorcomprehensions/
  39. https://arxiv.org/abs/1802.04730
  40. https://tensorcomprehensions.herokuapp.com/
  41. mailto:tensorcomp@fb.com
  42. https://research.fb.com/category/facebook-ai-research/
  43. https://ai.facebook.com/blog/open-sourcing-pytorch-biggraph-for-faster-embeddings-of-extremely-large-graphs/
  44. https://research.fb.com/turing-award-presented-to-yann-lecun-geoffrey-hinton-and-yoshua-bengio/
  45. https://research.fb.com/qa-with-facebook-ai-residents-tatiana-likhomanenko-and-siddharth-karamcheti/
  46. https://research.fb.com/facebook-ai-researchers-share-perspectives-on-diversity-on-international-womens-day/
  47. https://www.facebook.com/plugins/page.php?href=https://www.facebook.com/academics&tabs&width=340&height=70&small_header=true&adapt_container_width=true&hide_cover=true&show_facepile=false&appid
  48. https://research.fb.com/feed/
  49. https://www.facebook.com/facebook
  50. https://www.facebook.com/careers/university/
  51. https://www.facebook.com/about/privacy
  52. https://www.facebook.com/help/cookies
  53. https://www.facebook.com/policies
  54. https://www.facebook.com/help
  55. https://www.facebook.com/policies/cookies/
