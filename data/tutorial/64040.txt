   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

spiking neural networks, the next generation of machine learning

   [16]go to the profile of devin soni
   [17]devin soni (button) blockedunblock (button) followfollowing
   jan 10, 2018

   everyone who has been remotely tuned in to recent progress in machine
   learning has heard of the current 2nd generation artificial neural
   networks used for machine learning. these are generally fully
   connected, take in continuous values, and output continuous values.
   although they have allowed us to make breakthrough progress in many
   fields, they are biologically inn-accurate and do not actually mimic
   the actual mechanisms of our brain   s neurons.
   [1*etb_vtruptufjyligflioa.png]

   the 3rd generation of neural networks, spiking neural networks, aims to
   bridge the gap between neuroscience and machine learning, using
   biologically-realistic models of neurons to carry out computation. a
   spiking neural network (snn) is fundamentally different from the neural
   networks that the machine learning community knows. snns operate using
   spikes, which are discrete events that take place at points in time,
   rather than continuous values. the occurrence of a spike is determined
   by differential equations that represent various biological processes,
   the most important of which is the membrane potential of the neuron.
   essentially, once a neuron reaches a certain potential, it spikes, and
   the potential of that neuron is reset. the most common model for this
   is the leaky integrate-and-fire (lif) model. additionally, snns are
   often sparsely connected and take advantage of specialized network
   topologies.
   [1*hiorfahq59k0xktcgrvgyw.png]
   differential equation for membrane potential in the lif model
   [1*vvjgqaftlk2npd7bmd1zvq.gif]
   membrane potential behavior during a spike
   [1*bk_3fg-lc5pyxj5do4lavq.jpeg]
   spike trains for a network of 3 neurons
   [1*vbgk4g8dytygbkoz8ap2nq.jpeg]
   a full spiking neural network

   at first glance, this may seem like a step backwards. we have moved
   from continuous outputs to binary, and these spike trains are not very
   interpretable. however, spike trains offer us enhanced ability to
   process spatio-temporal data, or in other words, real-world sensory
   data. the spatial aspect refers to the fact that neurons are only
   connected to neurons local to them, so these inherently process chunks
   of the input separately (similar to how a id98 would using a filter).
   the temporal aspect refers to the fact that spike trains occur over
   time, so what we lose in binary encoding, we gain in the temporal
   information of the spikes. this allows us to naturally process temporal
   data without the extra complexity that id56s add. it has been proven, in
   fact, that spiking neurons are fundamentally more powerful
   computational units than traditional artificial neurons.

   given that these snns are more powerful, in theory, than 2nd generation
   networks, it is natural to wonder why we do not see widespread use of
   them. the main issue that currently lies in practical use of snns is
   that of training. although we have unsupervised biological learning
   methods such as hebbian learning and stdp, there are no known effective
   supervised training methods for snns that offer higher performance than
   2nd generation networks. since spike trains are not differentiable, we
   cannot train snns using id119 without losing the precise
   temporal information in spike trains. therefore, in order to properly
   use snns for real-world tasks, we would need to develop an effective
   supervised learning method. this is a very difficult task, as doing so
   would involve determining how the human brain actually learns, given
   the biological realism in these networks.

   another issue, that we are much closer to solving, is that simulating
   snns on normal hardware is very computationally-intensive since it
   requires simulating differential equations. however, neuromorphic
   hardware such as ibm   s truenorth aims to solve this by simulating
   neurons using specialized hardware that can take advantage of the
   discrete and sparse nature of neuronal spiking behavior.

   the future of snns therefore remains unclear. on one hand, they are the
   natural successor of our current neural networks, but on the other,
   they are quite far from being practical tools for most tasks. there are
   some current real-world applications of snns in real-time image and
   audio processing, but the literature on practical applications remains
   sparse. most papers on snns are either theoretical, or show performance
   under that of a simple fully-connected 2nd generation network. however,
   there are many teams working on developing snn supervised learning
   rules, and i remain optimistic for the future of snns.
     __________________________________________________________________

   make sure you give this post 50 claps and my blog a follow if you
   enjoyed this post and want to see more.

     * [18]machine learning
     * [19]neural networks
     * [20]data science
     * [21]towards data science
     * [22]artificial intelligence

   (button)
   (button)
   (button) 7.7k claps
   (button) (button) (button) 27 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of devin soni

[24]devin soni

   all opinions are my own     crypto markets, data science     twitter
   [25]@devin_soni     website [26]https://100.github.io/

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 7.7k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/84e167f4eb2b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/spiking-neural-networks-the-next-generation-of-machine-learning-84e167f4eb2b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/spiking-neural-networks-the-next-generation-of-machine-learning-84e167f4eb2b&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_dzctdi8sxx7d---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@devins?source=post_header_lockup
  17. https://towardsdatascience.com/@devins
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/neural-networks?source=post
  20. https://towardsdatascience.com/tagged/data-science?source=post
  21. https://towardsdatascience.com/tagged/towards-data-science?source=post
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/@devins?source=footer_card
  24. https://towardsdatascience.com/@devins
  25. http://twitter.com/devin_soni
  26. https://100.github.io/
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/84e167f4eb2b/share/twitter
  33. https://medium.com/p/84e167f4eb2b/share/facebook
  34. https://medium.com/p/84e167f4eb2b/share/twitter
  35. https://medium.com/p/84e167f4eb2b/share/facebook
