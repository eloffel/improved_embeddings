   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

using deep learning for structured data with entity embeddings

   [16]go to the profile of rutger ruizendaal
   [17]rutger ruizendaal (button) blockedunblock (button) followfollowing
   feb 21, 2018

   showing that deep learning can handle structured data and how to do it.
   [1*lhtlf2pw9apvqm2l2vf8yq.png]
   the idea of embeddings comes from learning them on words in nlp
   (id97), image taken from aylien

   in this blog we will touch on two recurring questions in machine
   learning: the first question revolves around how deep learning performs
   well on images and text, but how can we use it on our tabular data?
   second is a question you must always ask yourself when building a
   machine learning model: how am i going to deal with categorical
   variables in this data set? surprisingly, we can answer both questions
   with the same answer: entity embeddings.

   deep learning has outperformed other machine learning methods on many
   fronts recently: image recognition, audio classification and natural
   language processing are just some of the many examples. these research
   areas all use what is known as    unstructured data   , which is data
   without a predefined structure. generally speaking this data can also
   be organized as a sequence (of pixels, user behavior, text). deep
   learning has become the standard when dealing with unstructured data.
   recently the question has arisen of whether deep learning can also
   perform the best on structured data. structured data is data that is
   organized in a tabular format where the columns represent different
   features and the rows represent different data samples. this is similar
   to how data is represented in an excel sheet. currently, the golden
   standard for structured data sets are gradient boosted tree models
   (chen & guestrin, 2016). they consistently perform the best on kaggle
   competitions, as well as in academic literature. recently deep learning
   has shown that it can match the performance of these boosted tree
   models on structured data. entity embeddings play an important role in
   this.
   [1*iijtk0x4cop6fh1xecqesw.png]
   structured vs. unstructured data

entity embeddings

   entity embeddings have been shown to work successfully when fitting
   neural networks on structured data. for example, the winning solution
   in a kaggle competition on predicting the distance of taxi rides used
   entity embeddings to deal with the categorical metadata of each ride
   (de br  bisson et al., 2015). similarly, the third place solution on the
   task of prediction store sales for rossmann drug stores used a much
   less complicated approach than the number one and two   s solutions. the
   team was able to achieve this success by using a simple feed-forward
   neural network with entity embeddings for the categorical variables.
   this included variables with over a 1000 categories like the store id
   (guo & berkahn, 2016).

   if this is your first time reading about embeddings i suggest you first
   read [18]this post. in short, embeddings refer the representation of
   categories by vectors. let   s show how this works on a short sentence:

        deep learning is deep   

   we can represent each word with a vector, so the word    deep    becomes
   something like [0.20, 0.82, 0.45, 0.67]. in practice, one would replace
   the words by integers like 1 2 3 1, and use a look-up table to find the
   vector linked to each integer. this practice is very common in natural
   language processing and has also been used in data that consists of a
   behavioral sequence, like the journey of an online user. entity
   embeddings refer to using this principle on categorical variables,
   where each category of a categorical variable gets represented by a
   vector. let   s quickly review the two common methods for handling
   categorical variables in machine learning.
     * one-hot encoding: creates binary sub-features like word_deep,
       word_learning, word_is. these are 1 for the category belonging to
       that data point and 0 for the others. so, for the word    deep    the
       feature word_deep will be 1 and word_learning, word_is etc. will be
       0.
     * label encoding: assigning integers like we did in the example
       before, so deep becomes 1, learning becomes 2 etc. this method is
       suitable for tree-based methods, but not for linear models because
       it implies an order in the assigned values.

   entity embeddings basically take the label encoding approach to the
   next level, by not just assigning an integer to a category but a whole
   vector. this vector can be of any size and has to be specified by the
   researcher. you might be wondering what the advantages of these entity
   embeddings are.
    1. entity embeddings solve the disadvantages of one-hot encoding.
       one-hot encoding variables with many categories results in very
       sparse vectors, which are computationally inefficient and make it
       harder to reach optimization. label encoding also solves this
       problem, but can only be used by tree-based models.
    2. embeddings provide information about the distance between different
       categories. the beauty of using embeddings is that the vectors
       assigned to each category are also trained during the training of
       the neural network. therefore, at the end of the training process
       we end up with a vector that represents each category. these
       trained embeddings can then be visualized to provide insights into
       each category. in the rossmann sales prediction task, the
       visualized embeddings of german states showed similar clusters to
       the states    geographical locations. even though none of this
       geographical information was available to the model.
    3. the trained embeddings can be saved and used in non-deep learning
       models. for example, one could train the embeddings for categorical
       features each month and save the embeddings. these embeddings can
       then be used to train a id79 or a gradient boosted trees
       model by loading the learned embeddings for the categorical
       features.
     __________________________________________________________________

choosing the embedding size

   the embedding size refers to the length of the vector representing each
   category and can be set for each categorical feature. similar to the
   tuning process of hyperparameters in a neural network, there are no
   hard rules for choosing the embedding size. in the taxi distance
   prediction task the researchers used an embedding size of 10 for each
   feature. these features had very different dimensionalities ranging
   from 7 (day of the week) to 57106 (client id). choosing the same
   embedding size for each category is an easy and transparent approach,
   but probably not the optimal one.

   for the rossmann store sales prediction task the researchers chose a
   value between 1 and m (the amount of categories) -1 with a maximum
   embedding size of 10. for example, day of the week (7 values) gets an
   embedding size of 6, while store id (1115 values) gets an embedding
   size of 10. however, the authors have no clear rules of choosing the
   size between 1 and m-1.

   jeremy howard rebuilt the solution for the rossmann competition and
   came up with the following solution for choosing embedding sizes:
# c is the amount of categories per feature
embedding_size = (c+1) // 2
if embedding_size > 50:
    embedding_size = 50
     __________________________________________________________________

visualizing embeddings

   an advantage of embeddings is that the learned embeddings can be
   visualized to show which categories are similar to each other. the most
   popular method for this is id167, which is a technique for
   id84 that works particularly well for visualizing
   data sets with high-dimensionality. let   s finish this post with two
   quick examples of visualized embeddings. below are the visualized
   embeddings for home depot products and the category they belong to.
   similar products like oven, refrigerator and microwave are very close
   to each other. the same goes for products like charger, battery and
   drill.
   [1*98ziziysh8euw8gw7r7wjg.png]
   learned embeddings of home depot products.

   another example are the learned state embeddings of german states in
   the rossmann sales prediction task mentioned earlier in this post. the
   proximity between the states in the embeddings is similar to their
   geographical location.
   [1*jhncb7nlivpvs052ozueeq.png]
   example of the learned state embeddings for germany

   i hope that i was able to enthusiast you about embeddings and deep
   learning. if you liked this posts be sure to recommend it so others can
   see it. you can also follow this profile to keep up with my process in
   deep learning. see you there!
     __________________________________________________________________

   i   m currently working as a data scientist for micompany. we are looking
   hard for data engineers and software engineers. we are also recruiting
   data scientists for ourselves and our partners which include some of
   the biggest organisations in the netherlands, israel and some big
   global players!

   contact me on [19]linkedin and join us in amsterdam or tel aviv, or let
   me help you join one of our partner organisations all over the world!
     __________________________________________________________________

   be sure to checkout the rest of my deep learning series:
    1. [20]setting up aws & image recognition
    2. [21]convolutional neural networks
    3. [22]more on id98s & handling overfitting
    4. [23]why you need to start using embedding layers
     __________________________________________________________________

references

   chen, t., & guestrin, c. (2016, august). xgboost: a scalable tree
   boosting system. in proceedings of the 22nd acm sigkdd international
   conference on knowledge discovery and data mining (pp. 785   794). acm.

   de br  bisson, a., simon,   ., auvolat, a., vincent, p., & bengio, y.
   (2015). id158s applied to taxi destination
   prediction. arxiv preprint arxiv:1508.00021.

   guo, c., & berkhahn, f. (2016). entity embeddings of categorical
   variables. arxiv preprint arxiv:1604.06737.

     * [24]machine learning
     * [25]data science
     * [26]deep learning
     * [27]artificial intelligence
     * [28]towards data science

   (button)
   (button)
   (button) 762 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of rutger ruizendaal

[30]rutger ruizendaal

   data science, machine learning & deep learning. visit
   [31]https://musicaldata.com.

     (button) follow
   [32]towards data science

[33]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 762
     * (button)
     *
     *

   [34]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [35]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8d6a278f3088
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-structured-data-8d6a278f3088&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-structured-data-8d6a278f3088&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_obxkcppizb30---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@r.ruizendaal?source=post_header_lockup
  17. https://towardsdatascience.com/@r.ruizendaal
  18. https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12
  19. https://www.linkedin.com/in/rutger-ruizendaal/
  20. https://medium.com/towards-data-science/deep-learning-1-1a7e7d9e3c07
  21. https://medium.com/towards-data-science/deep-learning-2-f81ebe632d5c
  22. https://medium.com/towards-data-science/deep-learning-3-more-on-id98s-handling-overfitting-2bd5d99abe5d
  23. https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/tagged/deep-learning?source=post
  27. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  28. https://towardsdatascience.com/tagged/towards-data-science?source=post
  29. https://towardsdatascience.com/@r.ruizendaal?source=footer_card
  30. https://towardsdatascience.com/@r.ruizendaal
  31. https://musicaldata.com/
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/8d6a278f3088/share/twitter
  38. https://medium.com/p/8d6a278f3088/share/facebook
  39. https://medium.com/p/8d6a278f3088/share/twitter
  40. https://medium.com/p/8d6a278f3088/share/facebook
