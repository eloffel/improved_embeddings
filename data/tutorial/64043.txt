   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    essentials of deep learning : introduction to
   long short term memory comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]deep learning [94]essentials of deep learning :
   introduction to long short term memory

   [95]deep learning[96]python

essentials of deep learning : introduction to long short term memory

   [97]pranjal srivastava, december 10, 2017

introduction

   sequence prediction problems have been around for a long time. they are
   considered as one of the hardest problems to solve in the data science
   industry. these include a wide range of problems; from predicting sales
   to finding patterns in stock markets    data, from understanding movie
   plots to recognizing your way of speech, from language translations to
   predicting your next word on your iphone   s keyboard.

   with the recent breakthroughs that have been happening in data science,
   it is found that for almost all of these sequence prediction problems,
   long short term memory networks, a.k.a lstms have been observed as the
   most effective solution.

   lstms have an edge over conventional feed-forward neural networks and
   id56 in many ways. this is because of their property of selectively
   remembering patterns for long durations of time.  the purpose of this
   article is to explain lstm and enable you to use it in real life
   problems.  let   s have a look!

   note: to go through the article, you must have basic knowledge of
   neural networks and how keras (a deep learning library) works. you can
   refer the mentioned articles to understand these concepts:
     * [98]understanding neural network from scratch
     * [99]fundamentals of deep learning     introduction to recurrent
       neural networks
     * [100]tutorial: optimizing neural networks using keras (with image
       recognition case study)


table of contents

    1. flashback: a look into recurrent neural networks (id56)
    2. limitations of id56s
    3. improvement over id56 : long short term memory (lstm)
    4. architecture of lstm
         1. forget gate
         2. input gate
         3. output gate
    5. text generation using lstms.


1. flashback: a look into recurrent neural networks (id56)

   take an example of sequential data, which can be the stock market   s
   data for a particular stock. a simple machine learning model or an
   id158 may learn to predict the stock prices based
   on a number of features: the volume of the stock, the opening value
   etc. while the price of the stock depends on these features, it is also
   largely dependent on the stock values in the previous days. in fact for
   a trader, these values in the previous days (or the trend) is one major
   deciding factor for predictions.

   in the conventional feed-forward neural networks, all test cases are
   considered to be independent. that is when fitting the model for a
   particular day, there is no consideration for the stock prices on the
   previous days.

   this dependency on time is achieved via recurrent neural networks. a
   typical id56 looks like:

   this may be intimidating at first sight, but once unfolded, it looks a
   lot simpler:

   now it is easier for us to visualize how these networks are considering
   the trend of stock prices, before predicting the stock prices for
   today. here every prediction at time t (h_t) is dependent on all
   previous predictions and the information learned from them.

   id56s can solve our purpose of sequence handling to a great extent but
   not entirely. we want our computers to be good enough to [101]write
   shakespearean sonnets. now id56s are great when it comes to short
   contexts, but in order to be able to build a story and remember it, we
   need our models to be able to understand and remember the context
   behind the sequences, just like a human brain. this is not possible
   with a simple id56.

   why? let   s have a look.


2. limitations of id56s

   recurrent neural networks work just fine when we are dealing with
   short-term dependencies. that is when applied to problems like:

   id56s turn out to be quite effective. this is because this problem has
   nothing to do with the context of the statement. the id56 need not
   remember what was said before this, or what was its meaning, all they
   need to know is that in most cases the sky is blue. thus the prediction
   would be:

   however, vanilla id56s fail to understand the context behind an input.
   something that was said long before, cannot be recalled when making
   predictions in the present. let   s understand this as an example:

   here, we can understand that since the author has worked in spain for
   20 years, it is very likely that he may possess a good command over
   spanish. but, to make a proper prediction, the id56 needs to remember
   this context. the relevant information may be separated from the point
   where it is needed, by a huge load of irrelevant data. this is where a
   recurrent neural network fails!

   the reason behind this is the problem of vanishing gradient. in order
   to understand this, you   ll need to have some knowledge about how a
   feed-forward neural network learns. we know that for a conventional
   feed-forward neural network, the weight updating that is applied on a
   particular layer is a multiple of the learning rate, the error term
   from the previous layer and the input to that layer. thus, the error
   term for a particular layer is somewhere a product of all previous
   layers    errors. when dealing with id180 like the sigmoid
   function, the small values of its derivatives (occurring in the error
   function) gets multiplied multiple times as we move towards the
   starting layers. as a result of this, the gradient almost vanishes as
   we move towards the starting layers, and it becomes difficult to train
   these layers.

   a similar case is observed in recurrent neural networks. id56 remembers
   things for just small durations of time, i.e. if we need the
   information after a small time it may be reproducible, but once a lot
   of words are fed in, this information gets lost somewhere. this issue
   can be resolved by applying a slightly tweaked version of id56s     the
   id137.


3. improvement over id56: lstm (long short-term memory) networks

   when we arrange our calendar for the day, we prioritize our
   appointments right? if in case we need to make some space for anything
   important we know which meeting could be canceled to accommodate a
   possible meeting.

   turns out that an id56 doesn   t do so. in order to add a new information,
   it transforms the existing information completely by applying a
   function. because of this, the entire information is modified, on the
   whole, i. e. there is no consideration for    important    information and
      not so important    information.

   lstms on the other hand, make small modifications to the information by
   multiplications and additions. with lstms, the information flows
   through a mechanism known as cell states. this way, lstms can
   selectively remember or forget things. the information at a particular
   cell state has three different dependencies.

   we   ll visualize this with an example. let   s take the example of
   predicting stock prices for a particular stock. the stock price of
   today will depend upon:
    1. the trend that the stock has been following in the previous days,
       maybe a downtrend or an uptrend.
    2. the price of the stock on the previous day, because many traders
       compare the stock   s previous day price before buying it.
    3. the factors that can affect the price of the stock for today. this
       can be a new company policy that is being criticized widely, or a
       drop in the company   s profit, or maybe an unexpected change in the
       senior leadership of the company.

   these dependencies can be generalized to any problem as:
    1. the previous cell state (i.e. the information that was present in
       the memory after the previous time step)
    2. the previous hidden state (i.e. this is the same as the output of
       the previous cell)
    3. the input at the current time step (i.e. the new information that
       is being fed in at that moment)

   another important feature of lstm is its analogy with conveyor belts!

   that   s right!

   industries use them to move products around for different processes.
   lstms use this mechanism to move information around.

   we may have some addition, modification or removal of information as it
   flows through the different layers, just like a product may be molded,
   painted or packed while it is on a conveyor belt.

   the following diagram explains the close relationship of lstms and
   conveyor belts.

   [102]source

   although this diagram is not even close to the actual architecture of
   an lstm, it solves our purpose for now.

   just because of this property of lstms, where they do not manipulate
   the entire information but rather modify them slightly, they are able
   to forget and remember things selectively. how do they do so, is what
   we are going to learn in the next section?


4. architecture of lstms

   the functioning of lstm can be visualized by understanding the
   functioning of a news channel   s team covering a murder story. now, a
   news story is built around facts, evidence and statements of many
   people. whenever a new event occurs you take either of the three steps.

   let   s say, we were assuming that the murder was done by    poisoning    the
   victim, but the autopsy report that just came in said that the cause of
   death was    an impact on the head   . being a part of this news team what
   do you do? you immediately forget the previous cause of death and all
   stories that were woven around this fact.

   what, if an entirely new suspect is introduced into the picture. a
   person who had grudges with the victim and could be the murderer? you
   input this information into your news feed, right?

   now all these broken pieces of information cannot be served on
   mainstream media. so, after a certain time interval, you need to
   summarize this information and output the relevant things to your
   audience. maybe in the form of    xyz turns out to be the prime
   suspect.   .

   now let   s get into the details of the architecture of lstm network:

   [103]source

   now, this is nowhere close to the simplified version which we saw
   before, but let me walk you through it. a typical lstm network is
   comprised of different memory blocks called cells
   (the rectangles that we see in the image).  there are two states that
   are being transferred to the next cell; the cell state and the hidden
   state. the memory blocks are responsible for remembering things and
   manipulations to this memory is done through three major mechanisms,
   called gates. each of them is being discussed below.

4.1 forget gate

   taking the example of a text prediction problem. let   s assume an lstm
   is fed in, the following sentence:

   as soon as the first full stop after    person    is encountered, the
   forget gate realizes that there may be a change of context in the next
   sentence. as a result of this, the subject of the sentence is forgotten
   and the place for the subject is vacated. and when we start speaking
   about    dan    this position of the subject is allocated to    dan   . this
   process of forgetting the subject is brought about by the forget gate.

   a forget gate is responsible for removing information from the cell
   state. the information that is no longer required for the lstm to
   understand things or the information that is of less importance is
   removed via multiplication of a filter. this is required for optimizing
   the performance of the lstm network.

   this gate takes in two inputs; h_t-1 and x_t.

   h_t-1 is the hidden state from the previous cell or the output of the
   previous cell and x_t is the input at that particular time step. the
   given inputs are multiplied by the weight matrices and a bias is added.
   following this, the sigmoid function is applied to this value. the
   sigmoid function outputs a vector, with values ranging from 0 to 1,
   corresponding to each number in the cell state. basically, the sigmoid
   function is responsible for deciding which values to keep and which to
   discard. if a    0    is output for a particular value in the cell state,
   it means that the forget gate wants the cell state to forget that piece
   of information completely. similarly, a    1    means that the forget gate
   wants to remember that entire piece of information. this vector output
   from the sigmoid function is multiplied to the cell state.


4.2 input gate

   okay, let   s take another example where the lstm is analyzing a
   sentence:

   now the important information here is that    bob    knows swimming and
   that he has served the navy for four years. this can be added to the
   cell state, however, the fact that he told all this over the phone is a
   less important fact and can be ignored. this process of adding some new
   information can be done via the input gate.

   here is its structure:

   the input gate is responsible for the addition of information to the
   cell state. this addition of information is basically
   three-step process as seen from the diagram above.
    1. regulating what values need to be added to the cell state by
       involving a sigmoid function. this is basically very similar to the
       forget gate and acts as a filter for all the information from h_t-1
       and x_t.
    2. creating a vector containing all possible values that can be added
       (as perceived from h_t-1 and x_t) to the cell state. this is done
       using the tanh function, which outputs values from -1 to +1.
    3. multiplying the value of the regulatory filter (the sigmoid gate)
       to the created vector (the tanh function) and then adding this
       useful information to the cell state via addition operation.


   once this three-step process is done with, we ensure that only that
   information is added to the cell state that is important and is not
   redundant.


4.3 output gate

   not all information that runs along the cell state, is fit for being
   output at a certain time. we   ll visualize this with an example:

   in this phrase, there could be a number of options for the empty space.
   but we know that the current input of    brave   , is an adjective that is
   used to describe a noun. thus, whatever word follows, has a strong
   tendency of being a noun. and thus, bob could be an apt output.

   this job of selecting useful information from the current cell state
   and showing it out as an output is done via the output gate. here is
   its structure:

   the functioning of an output gate can again be broken down to three
   steps:
    1. creating a vector after applying tanh function to the cell state,
       thereby scaling the values to the range -1 to +1.
    2. making a filter using the values of h_t-1 and x_t, such that it can
       regulate the values that need to be output from the vector created
       above. this filter again employs a sigmoid function.
    3. multiplying the value of this regulatory filter to the vector
       created in step 1, and sending it out as a output and also to the
       hidden state of the next cell.

   the filter in the above example will make sure that it diminishes all
   other values but    bob   . thus the filter needs to be built on the input
   and hidden state values and be applied on the cell state vector.


5. text generation using lstms

   we have had enough of theoretical concepts and functioning of lstms.
   now we would be trying to build a model that can predict some n number
   of characters after the original text of macbeth. most of the classical
   texts are no longer protected under copyright and can be found
   [104]here. an updated version of the .txt file can be found [105]here.

   we will use the library keras, which is a high-level api for neural
   networks and works on top of tensorflow or theano. so make sure that
   before diving into this code you have keras installed and functional.

   okay, so let   s generate some text!


     * importing dependencies

# importing dependencies numpy and keras
import numpy
from keras.models import sequential
from keras.layers import dense
from keras.layers import dropout
from keras.layers import lstm
from keras.utils import np_utils

   we import all the required dependencies and this is pretty much
   self-explanatory.
     * loading text file and creating character to integer mappings

# load text
filename = "/macbeth.txt"

text = (open(filename).read()).lower()

# mapping characters with integers
unique_chars = sorted(list(set(text)))

char_to_int = {}
int_to_char = {}

for i, c in enumerate (unique_chars):
    char_to_int.update({c: i})
    int_to_char.update({i: c})

   the text file is open, and all characters are converted to lowercase
   letters. in order to facilitate the following steps, we would be
   mapping each character to a respective number. this is done to make the
   computation part of the lstm easier.
     * preparing dataset

# preparing input and output dataset
x = []
y = []

for i in range(0, len(text) - 50, 1):
    sequence = text[i:i + 50]
    label =text[i + 50]
    x.append([char_to_int[char] for char in sequence])
    y.append(char_to_int[label])

   data is prepared in a format such that if we want the lstm to predict
   the    o    in    hello     we would feed in [   h   ,    e    ,    l     ,    l    ] as the
   input and [   o   ] as the expected output. similarly, here we fix the
   length of the sequence that we want (set to 50 in the example) and then
   save the encodings of the first 49 characters in x and the expected
   output i.e. the 50th character in y.
     * reshaping of x

# reshaping, normalizing and one hot encoding
x_modified = numpy.reshape(x, (len(x), 50, 1))
x_modified = x_modified / float(len(unique_chars))
y_modified = np_utils.to_categorical(y)


   a lstm network expects the input to be in the form [samples, time
   steps, features] where samples is the number of data points we have,
   time steps is the number of time-dependent steps that are there in a
   single data point, features refers to the number of variables we have
   for the corresponding true value in y. we then scale the values in
   x_modified between 0 to 1 and one hot encode our true values in
   y_modified.


     * defining the lstm model

# defining the lstm model
model = sequential()
model.add(lstm(300, input_shape=(x_modified.shape[1], x_modified.shape[2]), retu
rn_sequences=true))
model.add(dropout(0.2))
model.add(lstm(300))
model.add(dropout(0.2))
model.add(dense(y_modified.shape[1], activation='softmax'))

model.compile(loss='categorical_crossid178', optimizer='adam')

   a sequential model which is a linear stack of layers is used. the first
   layer is an lstm layer with 300 memory units and it returns sequences.
   this is done to ensure that the next lstm layer receives sequences and
   not just randomly scattered data. a dropout layer is applied after each
   lstm layer to avoid overfitting of the model. finally, we have the last
   layer as a fully connected layer with a    softmax    activation and
   neurons equal to the number of unique characters, because we need to
   output one hot encoded result.
     * fitting the model and generating characters

# fitting the model
model.fit(x_modified, y_modified, epochs=1, batch_size=30)

# picking a random seed
start_index = numpy.random.randint(0, len(x)-1)
new_string = x[start_index]

# generating characters
for i in range(50):
    x = numpy.reshape(new_string, (1, len(new_string), 1))
    x = x / float(len(unique_chars))

    #predicting
    pred_index = numpy.argmax(model.predict(x, verbose=0))
    char_out = int_to_char[pred_index]
    seq_in = [int_to_char[value] for value in new_string]
    print(char_out)

    new_string.append(pred_index)
    new_string = new_string[1:len(new_string)]

   the model is fit over 100 epochs, with a batch size of 30. we then fix
   a random seed (for easy reproducibility) and start generating
   characters. the prediction from the model gives out the character
   encoding of the predicted character, it is then decoded back to the
   character value and appended to the pattern.

   this is how the output of the network would look like

   eventually, after enough training epochs, it will give better and
   better results over the time. this is how you would use lstm to solve a
   sequence prediction task.


end notes

   lstms are a very promising solution to sequence and time series related
   problems. however, the one disadvantage that i find about them, is the
   difficulty in training them. a lot of time and system resources go into
   training even a simple model. but that is just a hardware constraint! i
   hope i was successful in giving you a basic understanding of these
   networks. for any problems or issues related to the blog, please feel
   free to comment below.

[106]learn, [107]engage [108], hack and [109]get hired!


   you can also read this article on analytics vidhya's android app
   [110]get it on google play

share this:

     * [111]click to share on linkedin (opens in new window)
     * [112]click to share on facebook (opens in new window)
     * [113]click to share on twitter (opens in new window)
     * [114]click to share on pocket (opens in new window)
     * [115]click to share on reddit (opens in new window)
     *

like this:

   like loading...

related articles

   [ins: :ins]

   tags : [116]forget gate, [117]input gate, [118]long short term memory
   network, [119]lstm, [120]output gate, [121]id56, [122]sequence
   prediction, [123]text generator, [124]time series
   next article

introduction to computational linguistics and dependency trees in data
science

   previous article

fundamentals of deep learning     introduction to recurrent neural networks

[125]pranjal srivastava

   i am a senior undergraduate at iit (bhu), varanasi and a deep learning
   enthusiast. data is surely going to be the biggest thing of this
   century, instead of witnessing this as a mere spectator, i chose to be
   a part of this revolution.
     *
     *
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [126]discussion portal to get your queries resolved

20 comments

     * bikram kachari says:
       [127]december 11, 2017 at 10:56 am
       very nicely written
       [128]reply
          + pranjal srivastava says:
            [129]december 12, 2017 at 11:23 am
            thanks.
            [130]reply
     * sujatha sivaraman says:
       [131]december 11, 2017 at 11:05 am
       very well articulated article on lstm
       [132]reply
          + pranjal srivastava says:
            [133]december 12, 2017 at 11:26 am
            thank you.
            [134]reply
     * james chibole says:
       [135]december 11, 2017 at 1:18 pm
       simple explanations. thank you.
       [136]reply
          + pranjal srivastava says:
            [137]december 12, 2017 at 12:05 pm
            thanks.
            [138]reply
     * saurabh singh says:
       [139]december 12, 2017 at 9:57 am
       very nice article     and very well explained . thanks for the
       quality article .
       [140]reply
          + pranjal srivastava says:
            [141]december 12, 2017 at 11:19 am
            thanks. glad you liked it.
            [142]reply
     * sasikanth says:
       [143]december 14, 2017 at 9:10 pm
       is there a similar package in r language to invoke lstm?
       [144]reply
          + pranjal srivastava says:
            [145]december 23, 2017 at 4:51 pm
            yes. keras is available for r as well. please checkout
            [146]https://keras.rstudio.com/
            [147]reply
     * richard lucius says:
       [148]december 18, 2017 at 11:31 pm
       your prediction section should be indented. when you copy an pasted
       into this format you last the indenting. the prediction does not
       look like it is part of the for loop.
       [149]reply
          + pranjal srivastava says:
            [150]december 23, 2017 at 4:57 pm
            thanks richard for pointing it out. have made the necessary
            changes.
            [151]reply
     * srinivas says:
       [152]january 11, 2018 at 12:33 pm
       this is the best article i ever found on lstm.you nailed it with
       good examples..waiting for other articles from you
       [153]reply
          + pranjal srivastava says:
            [154]march 23, 2018 at 5:38 pm
            thanks srinivas.
            [155]reply
     * bill says:
       [156]march 8, 2018 at 1:13 pm
       hi
       i   m a starter and interested in this subject. i think i like the
       way you said be part not a spectator. if you could, let me know how
       i could engage into this subject and where do i start?
       [157]reply
     * venkatraju says:
       [158]march 16, 2018 at 9:34 pm
       this is very nice article and explained well . thank you
       [159]reply
     * nehru says:
       [160]march 27, 2018 at 2:03 pm
       actually the question is what is the benfit getting by multiplying
       tanh and sigmoid functions to take input and what is the necessity
       of tanh function
       [161]reply
          + faizan shaikh says:
            [162]march 27, 2018 at 4:20 pm
            hey     there is a whole article written on this topic. i
            suggest you to refer this article    [163]id180
            and their use   
            [164]reply
     * ei ei mon says:
       [165]may 24, 2018 at 9:33 am
       thank you.
       [166]reply
     * shreesh gupta says:
       [167]august 4, 2018 at 12:24 pm
       well written content !!
       thanks dude!!
       [168]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [169]srk       3924
   2    [2.jpg?date=2019-04-06] [170]mark12    3510
   3    [3.jpg?date=2019-04-06] [171]nilabha   3261
   4    [4.jpg?date=2019-04-06] [172]nitish007 3237
   5    [5.jpg?date=2019-04-06] [173]tezdhar   3082
   [174]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [175]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [176]understanding support vector machine algorithm from examples
       (along with code)
     * [177]essentials of machine learning algorithms (with python and r
       codes)
     * [178]a complete tutorial to learn data science with python from
       scratch
     * [179]7 types of regression techniques you should know!
     * [180]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [181]a simple introduction to anova (with applications in excel)
     * [182]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [183]top 5 machine learning github repositories and reddit discussions
   from march 2019

[184]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [185]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[186]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [187]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[188]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [189]16 opencv functions to start your id161 journey (with
   python code)

[190]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [191][ds-finhack.jpg]

   [192][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [193]about us
     * [194]our team
     * [195]career
     * [196]contact us
     * [197]write for us

   [198]about us
   [199]   
   [200]our team
   [201]   
   [202]careers
   [203]   
   [204]contact us

data scientists

     * [205]blog
     * [206]hackathon
     * [207]discussions
     * [208]apply jobs
     * [209]leaderboard

companies

     * [210]post jobs
     * [211]trainings
     * [212]hiring hackathons
     * [213]advertising
     * [214]reach us

   don't have an account? [215]sign up here.

join our community :

   [216]46336 [217]followers
   [218]20224 [219]followers
   [220]followers
   [221]7513 [222]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [223]privacy policy
     * [224]terms of use
     * [225]refund policy

   don't have an account? [226]sign up here

   iframe: [227]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [228](button) join now

   subscribe!

   iframe: [229]likes-master

   %d bloggers like this:

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [230](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/deep-learning/
  94. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/
  95. https://www.analyticsvidhya.com/blog/category/deep-learning/
  96. https://www.analyticsvidhya.com/blog/category/python-2/
  97. https://www.analyticsvidhya.com/blog/author/pranj52/
  98. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  99. https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
 100. https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/
 101. https://www.psfk.com/2014/01/shakespeare-machine-learning-poetry-app.html
 102. https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b
 103. http://colah.github.io/posts/2015-08-understanding-lstms/
 104. https://www.gutenberg.org/
 105. https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/10165151/macbeth.txt
 106. https://www.analyticsvidhya.com/blog
 107. http://discuss.analyticsvidhya.com/
 108. https://datahack.analyticsvidhya.com/
 109. https://www.analyticsvidhya.com/jobs/#/user/
 110. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 111. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?share=linkedin
 112. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?share=facebook
 113. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?share=twitter
 114. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?share=pocket
 115. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?share=reddit
 116. https://www.analyticsvidhya.com/blog/tag/forget-gate/
 117. https://www.analyticsvidhya.com/blog/tag/input-gate/
 118. https://www.analyticsvidhya.com/blog/tag/long-short-term-memory-network/
 119. https://www.analyticsvidhya.com/blog/tag/lstm/
 120. https://www.analyticsvidhya.com/blog/tag/output-gate/
 121. https://www.analyticsvidhya.com/blog/tag/id56/
 122. https://www.analyticsvidhya.com/blog/tag/sequence-prediction/
 123. https://www.analyticsvidhya.com/blog/tag/text-generator/
 124. https://www.analyticsvidhya.com/blog/tag/time-series/
 125. https://www.analyticsvidhya.com/blog/author/pranj52/
 126. https://discuss.analyticsvidhya.com/
 127. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146965
 128. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146965
 129. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147160
 130. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147160
 131. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146966
 132. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146966
 133. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147162
 134. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147162
 135. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146977
 136. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-146977
 137. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147167
 138. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147167
 139. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147144
 140. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147144
 141. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147159
 142. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147159
 143. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147661
 144. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-147661
 145. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-149220
 146. https://keras.rstudio.com/
 147. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-149220
 148. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-148597
 149. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-148597
 150. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-149223
 151. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-149223
 152. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-150719
 153. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-150719
 154. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152118
 155. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152118
 156. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-151767
 157. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-151767
 158. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-151949
 159. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-151949
 160. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152203
 161. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152203
 162. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152206
 163. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/
 164. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-152206
 165. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-153509
 166. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-153509
 167. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-154436
 168. https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/#comment-154436
 169. https://datahack.analyticsvidhya.com/user/profile/srk
 170. https://datahack.analyticsvidhya.com/user/profile/mark12
 171. https://datahack.analyticsvidhya.com/user/profile/nilabha
 172. https://datahack.analyticsvidhya.com/user/profile/nitish007
 173. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 174. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 175. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 176. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 177. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 178. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 179. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 180. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 181. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 182. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 183. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 184. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 185. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 186. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 187. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 188. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 189. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 190. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 191. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 192. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 193. http://www.analyticsvidhya.com/about-me/
 194. https://www.analyticsvidhya.com/about-me/team/
 195. https://www.analyticsvidhya.com/career-analytics-vidhya/
 196. https://www.analyticsvidhya.com/contact/
 197. https://www.analyticsvidhya.com/about-me/write/
 198. http://www.analyticsvidhya.com/about-me/
 199. https://www.analyticsvidhya.com/about-me/team/
 200. https://www.analyticsvidhya.com/about-me/team/
 201. https://www.analyticsvidhya.com/about-me/team/
 202. https://www.analyticsvidhya.com/career-analytics-vidhya/
 203. https://www.analyticsvidhya.com/about-me/team/
 204. https://www.analyticsvidhya.com/contact/
 205. https://www.analyticsvidhya.com/blog
 206. https://datahack.analyticsvidhya.com/
 207. https://discuss.analyticsvidhya.com/
 208. https://www.analyticsvidhya.com/jobs/
 209. https://datahack.analyticsvidhya.com/users/
 210. https://www.analyticsvidhya.com/corporate/
 211. https://trainings.analyticsvidhya.com/
 212. https://datahack.analyticsvidhya.com/
 213. https://www.analyticsvidhya.com/contact/
 214. https://www.analyticsvidhya.com/contact/
 215. https://datahack.analyticsvidhya.com/signup/
 216. https://www.facebook.com/analyticsvidhya/
 217. https://www.facebook.com/analyticsvidhya/
 218. https://twitter.com/analyticsvidhya
 219. https://twitter.com/analyticsvidhya
 220. https://plus.google.com/+analyticsvidhya
 221. https://in.linkedin.com/company/analytics-vidhya
 222. https://in.linkedin.com/company/analytics-vidhya
 223. https://www.analyticsvidhya.com/privacy-policy/
 224. https://www.analyticsvidhya.com/terms/
 225. https://www.analyticsvidhya.com/refund-policy/
 226. https://id.analyticsvidhya.com/accounts/signup/
 227. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 228. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 229. https://widgets.wp.com/likes/master.html?ver=201914#ver=201914
 230. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 232. https://www.facebook.com/analyticsvidhya
 233. https://twitter.com/analyticsvidhya
 234. https://plus.google.com/+analyticsvidhya/posts
 235. https://in.linkedin.com/company/analytics-vidhya
 236. https://www.analyticsvidhya.com/blog/2017/12/introduction-computational-linguistics-dependency-trees/
 237. https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/
 238. https://www.analyticsvidhya.com/blog/author/pranj52/
 239. https://www.analyticsvidhya.com/cdn-cgi/l/email-protection#542426353a3e35387a27263d223527203522357a3731266560143d3d20363c217a35377a3d3a
 240. https://www.facebook.com/pranjal.srivastava3
 241. https://www.linkedin.com/in/pranjal52/
 242. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 243. https://www.facebook.com/analyticsvidhya/
 244. https://twitter.com/analyticsvidhya
 245. https://plus.google.com/+analyticsvidhya
 246. https://plus.google.com/+analyticsvidhya
 247. https://in.linkedin.com/company/analytics-vidhya
 248. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 249. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 250. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 251. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 252. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 253. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 254. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 255. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 256. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 257. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 258. javascript:void(0);
 259. javascript:void(0);
 260. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 261. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 262. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 263. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 264. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 265. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 266. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 267. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 268. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 269. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2017%2f12%2ffundamentals-of-deep-learning-introduction-to-lstm%2f&linkname=essentials%20of%20deep%20learning%20%3a%20introduction%20to%20long%20short%20term%20memory
 270. javascript:void(0);
 271. javascript:void(0);
