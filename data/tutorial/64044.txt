   #[1]adventures in machine learning    keras lstm tutorial     how to
   easily build a powerful deep learning language model comments feed
   [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

keras lstm tutorial     how to easily build a powerful deep learning language
model

   by [9]admin | [10]deep learning

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]keras lstm tutorial     how to easily build a powerful deep
       learning language model

   feb 03
   [14]11
   keras lstm tutorial architecture

   in previous posts, i introduced keras for building [15]convolutional
   neural networks and performing [16]id27. the next natural
   step is to talk about implementing recurrent neural networks in keras.
   in a [17]previous tutorial of mine, i gave a very comprehensive
   introduction to recurrent neural networks and long short term memory
   (lstm) networks, implemented in tensorflow. in this tutorial, i   ll
   concentrate on creating id137 in keras, briefly giving a recap
   or overview of how lstms work. in this keras lstm tutorial, we   ll
   implement a sequence-to-sequence text prediction model by utilizing a
   large text data set called the ptb corpus. all the code in this
   tutorial can be found on this [18]site   s github repository.
     __________________________________________________________________

   recommended book: if you want to learn more about lstms, tensorflow and
   keras, check out this book: [19]coding the deep learning revolution     a
   step by step introduction using python, tensorflow and keras
     __________________________________________________________________

a brief introduction to id137

recurrent neural networks

   a lstm network is a kind of recurrent neural network. a recurrent
   neural network is a neural network that attempts to model time or
   sequence dependent behaviour     such as language, stock prices,
   electricity demand and so on. this is performed by feeding back the
   output of a neural network layer at time t to the input of the same
   network layer at time t + 1. it looks like this:
   recurrent lstm tutorial - id56 diagram with nodes

   recurrent neural network diagram with nodes shown

   recurrent neural networks are    unrolled    programmatically during
   training and prediction, so we get something like the following:
   recurrent lstm tutorial - unrolled id56

   unrolled recurrent neural network

   here you can see that at each time step, a new word is being supplied    
   the output of the previous f (i.e. $h_{t-1}$) is supplied to the
   network at each time step also. if you   re wondering what those example
   words are referring to, it is an example sentence i used in my
   [20]previous lstm tutorial in tensorflow:    a girl walked into a bar,
   and she said    can i have a drink please?   .  the bartender said
      certainly      .

   the problem with vanilla recurrent neural networks, constructed from
   regular neural network nodes, is that as we try to model dependencies
   between words or sequence values that are separated by a significant
   number of other words, we experience the vanishing gradient problem
   (and also sometimes  the exploding gradient problem)     to learn more
   about the vanishing gradient problem, see [21]my post on the topic.
   this is because small gradients or weights (values less than 1) are
   multiplied many times over through the multiple time steps, and the
   gradients shrink asymptotically to zero. this means the weights of
   those earlier layers won   t be changed significantly and therefore the
   network won   t learn long-term dependencies.

   id137 are a way of solving this problem.

id137

   as mentioned previously, in this keras lstm tutorial we will be
   building an lstm network for text prediction. an lstm network is a
   recurrent neural network that has lstm cell blocks in place of our
   standard neural network layers. these cells have various components
   called the input gate, the forget gate and the output gate     these will
   be explained more fully later. here is a graphical representation of
   the lstm cell:
   recurrent neural network lstm tutorial - lstm cell diagram

   lstm cell diagram

   notice first, on the left hand side, we have our new word/sequence
   value $x_t$ being concatenated to the previous output from the
   cell $h_{t-1}$. the first step for this combined input is for it to be
   squashed via a tanh layer. the second step is that this input is passed
   through an input gate. an input gate is a layer of sigmoid activated
   nodes whose output is multiplied by the squashed input. these input
   gate sigmoids can act to    kill off    any elements of the input vector
   that aren   t required. a sigmoid function outputs values between 0 and
   1, so the weights connecting the input to these nodes can be trained to
   output values close to zero to    switch off    certain input values (or,
   conversely, outputs close to 1 to    pass through    other values).

   the next step in the flow of data through this cell is the internal
   state / forget gate loop. lstm cells have an internal state
   variable $s_t$. this variable, lagged one time step
   i.e. $s_{t-1}$ is added to the input data to create an effective layer
   of recurrence. this addition operation, instead of a multiplication
   operation, helps to reduce the risk of vanishing gradients. however,
   this recurrence loop is controlled by a forget gate     this works the
   same as the input gate, but instead helps the network learn which state
   variables should be    remembered    or    forgotten   .

   finally, we have an output layer tanh squashing function, the output of
   which is controlled by an output gate. this gate determines which
   values are actually allowed as an output from the cell $h_t$.

   the mathematics of the lstm cell looks like this:

   input

   first, the input is squashed between -1 and 1 using a tanh activation
   function. this can be expressed by:

   $$g = tanh(b^g + x_tu^g + h_{t-1}v^g)$$

   where $u^g$ and $v^g$ are the weights for the input and previous cell
   output, respectively, and $b^g$ is the input bias. note that the
   exponents g are not a raised power, but rather signify that these are
   the input weights and bias values (as opposed to the input gate, forget
   gate, output gate etc.).

   this squashed input is then multiplied element-wise by the output of
   the input gate, which, as discussed above, is a series of sigmoid
   activated nodes:

   $$i = \sigma(b^i + x_tu^i + h_{t-1}v^i)$$

   the output of the input section of the lstm cell is then given by:

   $$g \circ i$$

   where the $\circ$ operator expresses element-wise multiplication.

   forget gate and state loop

   the forget gate output is expressed as:

   $$f = \sigma(b^f + x_tu^f + h_{t-1}v^f)$$

   the output of the element-wise product of the previous state and the
   forget gate is expressed as $s_{t-1} \circ f$. the output from the
   forget gate / state loop stage is:

   $$s_t = s_{t-1} \circ f + g \circ i$$

   output gate

   the output gate is expressed as:

   $$o = \sigma(b^o + x_tu^o + h_{t-1}v^o)$$

   so the final output of the cell , with the tanh squashing, can be shown
   as:

   $$h_t = tanh(s_t) \circ o$$

lstm id27 and hidden layer size

   it should be remembered that in all of the mathematics above we are
   dealing with vectors i.e. the input $x_t$ and $h_{t-1}$ are not single
   valued scalars, but rather vectors of a certain length. likewise, all
   the weights and bias values are matrices and vectors respectively. now,
   you may be wondering, how do we represent words to input them to a
   neural network? the answer is id27. i   ve written about this
   extensively in previous tutorials, in particular [22]id97 word
   embedding tutorial in python and tensorflow and [23]a id97 keras
   tutorial. basically it involves taking a word and finding a vector
   representation of that word which captures some meaning of the word. in
   id97, this meaning is usually quantified by context     i.e. word
   vectors which are close together in vector space are those words which
   appear in sentences close to the same words.

   the word vectors can be learnt separately, as in [24]this tutorial, or
   they can be learnt during the training of your keras lstm network. in
   the example to follow, we   ll be setting up what is called
   an embedding layer, to convert each word into a meaningful word vector.
   we have to specify the size of the embedding layer     this is the length
   of the vector each word is represented by     this is usually in the
   region of between 100-500. in other words, if the embedding layer size
   is 250, each word will be represented by a 250-length vector i.e.
   [$x_1, x_2, x_3,\ldots, x_{250}$].

   lstm hidden layer size

   we usually match up the size of the embedding layer output with the
   number of hidden layers in the lstm cell. you might be wondering where
   the hidden layers in the lstm cell come from. in my lstm overview
   diagram, i simply showed    data rails    through which our input data
   flowed. however, each sigmoid, tanh or hidden state layer in the cell
   is actually a set of nodes, whose number is equal to the hidden
   layer size. therefore each of the    nodes    in the lstm cell is actually
   a cluster of normal neural network nodes, as in each layer of a
   [25]densely connected neural network.

the keras lstm architecture

   this section will illustrate what a full lstm architecture looks like,
   and show the architecture of the network that we are building in keras.
   this will further illuminate some of the ideas expressed above,
   including the embedding layer and the tensor sizes flowing around the
   network. the proposed architecture looks like the following:


   keras lstm tutorial architecture

   keras lstm tutorial architecture

   the input shape of the text data is ordered as follows : (batch size,
   number of time steps, hidden size). in other words, for each batch
   sample and each word in the number of time steps, there is a 500 length
   embedding word vector to represent the input word. these embedding
   vectors will be learnt as part of the overall model learning. the input
   data is then fed into two    stacked    layers of lstm cells (of 500 length
   hidden size)     in the diagram above, the lstm network is shown as
   unrolled over all the time steps. the output from these unrolled cells
   is still (batch size, number of time steps, hidden size).

   this output data is then passed to a keras layer called
   timedistributed, which will be explained more fully below. finally, the
   output layer has a softmax activation applied to it. this output is
   compared to the training y data for each batch, and the error and
   gradient back propagation is performed from there in keras. the
   training y data in this case is the input x words advanced one time
   step     in other words, at each time step the model is trying to predict
   the very next word in the sequence. however, it does this at every time
   step     hence the output layer has the same number of time steps as the
   input layer. this will be made more clear later.

building the keras lstm model

   in this section, each line of code to create the keras lstm
   architecture shown above will be stepped through and discussed.
   however, i   ll only briefly discuss the text preprocessing code which
   mostly uses the code found on the tensorflow site [26]here. the
   complete code for this keras lstm tutorial can be found at [27]this
   site   s github repository and is called keras_lstm.py. note, you first
   have to download the [28]penn tree bank (ptb) dataset which will be
   used as the training and validation corpus. you   ll need to change
   the data_path variable in the github code to match the location of this
   downloaded data.

the text preprocessing code

   in order to get the text data into the right shape for input into the
   keras lstm model, each unique word in the corpus must be assigned a
   unique integer index. then the text corpus needs to be re-constituted
   in order, but rather than text words we have the integer identifiers in
   order. the three functions which do this in the code are read_words,
   build_vocab and file_to_word_ids. i won   t go into these functions in
   detail, but basically, they first split the given text file into
   separate words and sentence based characters (i.e. end-of-sentence
   <eos>). then, each unique word is identified and assigned a unique
   integer. finally, the original text file is converted into a list of
   these unique integers, where each word is substituted with its new
   integer identifier. this allows the text data to be consumed in the
   neural network.

   the load_data function which i created to run these functions is shown
   below:
def load_data():
    # get the data paths
    train_path = os.path.join(data_path, "ptb.train.txt")
    valid_path = os.path.join(data_path, "ptb.valid.txt")
    test_path = os.path.join(data_path, "ptb.test.txt")

    # build the complete vocabulary, then convert text data to list of integers
    word_to_id = build_vocab(train_path)
    train_data = file_to_word_ids(train_path, word_to_id)
    valid_data = file_to_word_ids(valid_path, word_to_id)
    test_data = file_to_word_ids(test_path, word_to_id)
    vocabulary = len(word_to_id)
    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))

    print(train_data[:5])
    print(word_to_id)
    print(vocabulary)
    print(" ".join([reversed_dictionary[x] for x in train_data[:10]]))
    return train_data, valid_data, test_data, vocabulary, reversed_dictionary

   to call this function, we can run:
train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()

   the three outputs from this function are the training data, validation
   data and test data from the data set, respectively, but with each word
   represented as an integer in a list. some information is printed out
   during the running of load_data(), one of which
   is print(train_data[:5])     this produces the following output:

     [9970, 9971, 9972, 9974, 9975]

   as you can observe, the training data is comprised of a list of
   integers, as expected.

   next, the output vocabulary is simply the size of our text corpus. when
   words are incorporated into the training data, every single unique word
   is not considered     rather, in natural language processing, the text
   data is usually limited to a certain n number of the most common words.
   in this case n = vocabulary = 10,000.

   finally, reversed_dictionary is a python dictionary where the key is
   the unique integer identifier of a word, and the associated value is
   the word in text. this allows us to work backwards from predicted
   integer words that our model will produce, and translate them back to
   real text. for instance, the following code converts the integers
   in train_data back to text which is then printed: print(   
      .join([reversed_dictionary[x] for x in train_data[100:110]])). this
   code snippet produces:

     workers exposed to it more than n years ago researchers

   that   s about all the explanation required with regard to the text
   pre-processing, so let   s progress to setting up the input data
   generator which will feed samples into our keras lstm model.

creating the keras lstm data generators

   when training neural networks, we generally feed data into them in
   small batches, called mini-batches or just    batches    (for more
   information on mini-batch id119, see my tutorial [29]here).
   keras has  some handy functions which can extract training data
   automatically from a pre-supplied python iterator/generator object and
   input it to the model. one of these keras functions is
   called fit_generator. the first argument to fit_generator is the python
   iterator function that we will create, and it will be used to extract
   batches of data during the training process. this function in keras
   will handle all of the data extraction, input into the model, executing
   gradient steps, logging metrics such as accuracy and
   executing callbacks (these will be discussed later). the python
   iterator function needs to have a form like:
while true:
    #do some things to create a batch of data (x, y)
   yield x, y

   in this case, i have created a generator class which contains a method
   which implements such a structure. the initialization of this class
   looks like:
class kerasbatchgenerator(object):

    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):
        self.data = data
        self.num_steps = num_steps
        self.batch_size = batch_size
        self.vocabulary = vocabulary
        # this will track the progress of the batches sequentially through the
        # data set - once the data reaches the end of the data set it will reset
        # back to zero
        self.current_idx = 0
        # skip_step is the number of words which will be skipped before the next
        # batch is skimmed from the data set
        self.skip_step = skip_step

   here the kerasbatchgenerator object takes our data as the first
   argument. note, this data can be either training, validation or test
   data     multiple instances of the same class can be created and used in
   the various stages of our machine learning development cycle    
   training, validation tuning, test. the next argument supplied is
   called num_steps     this is the number of words that we will feed into
   the time distributed input layer of the network. in other words (pun
   intended), this is the set of words that the model will learn from to
   predict the words coming after. the argument batch_size is pretty
   self-explanatory, and we   ve discussed vocabulary already (it is equal
   to 10,000 in this case). finally skip_steps is the number of words we
   want to skip over between training samples within each batch. to make
   this a bit clearer, consider the following sentence:

      the cat sat on the mat, and ate his hat. then he jumped up and spat   

   if num_steps is set to 5, the data consumed as the input data for a
   given sample would be    the cat sat on the   . in this case, because we
   are predicted the very next word in the sequence via our model, for
   each time step, the matching output y or target data would be    cat sat
   on the mat   . finally, the skip_steps is the number of words to skip
   over before the next data batch is taken. if, in this example, it
   is skip_steps=num_steps the next 5 input words for the next batch would
   be    mat and ate his hat   . hopefully that makes sense.

   one final item in the initialization of the class needs to be
   discussed. this is variable current_idx which is initialized at zero.
   this variable is required to track the extraction of data through the
   full data set     once the full data set has been consumed in the
   training, we need to reset current_idx to zero so that the data
   consumption starts from the beginning of the data set again. in other
   words it is basically a data set location pointer.

   ok, now we need to discuss the generator method that will be called
   during fit_generator:
def generate(self):
    x = np.zeros((self.batch_size, self.num_steps))
    y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))
    while true:
        for i in range(self.batch_size):
            if self.current_idx + self.num_steps >= len(self.data):
                # reset the index back to the start of the data set
                self.current_idx = 0
            x[i, :] = self.data[self.current_idx:self.current_idx + self.num_ste
ps]
            temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_
steps + 1]
            # convert all of temp_y into a one hot representation
            y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)
            self.current_idx += self.skip_step
        yield x, y

   in the first couple of lines our x and y output arrays are created. the
   size of variable x is fairly straight forward to understand     it   s
   first dimension is the number of samples we specify in the batch. the
   second dimension is the number of words we are going to base our
   predictions on. the size of variable y is a little more complicated.
   first it has the batch size as the first dimension, then it has the
   number of time steps as the second, as discussed above. however, y has
   an additional third dimension, equal to the size of our vocabulary, in
   this case 10,000.

   the reason for this is that the output layer of our keras lstm network
   will be a standard softmax layer, which will assign a id203 to
   each of the 10,000 possible words. the one word with the highest
   id203 will be the predicted word     in other words, the keras lstm
   network will predict one word out of 10,000 possible categories.
   therefore, in order to train this network, we need to create a training
   sample for each word that has a 1 in the location of the true word, and
   zeros in all the other 9,999 locations. it will look something like
   this: (0, 0, 0,    , 1, 0,    , 0, 0)     this is called a one-hot
   representation, or alternatively, a categorical representation.
   therefore, for each target word, there needs to be a 10,000 length
   vector with only one of the elements in this vector set to 1.

   ok, now onto the while true: yield x, y paradigm that was discussed
   earlier for the generator. in the first line, we enter into a for loop
   of size batch_size, to populate all the data in the batch. next, there
   is a condition to test regarding whether we need to reset
   the current_idx pointer. remember that for each training sample we
   consume num_steps words. therefore, if the current index point
   plus num_steps is greater than the length of the data set, then
   the current_idx pointer needs to be reset to zero to start over with
   the data set.

   after this check is performed, the input data is consumed into
   the x array. the data indices consumed is pretty straight-forward to
   understand     it is the current index to the
   current-index-plus-num_steps number of words. next, a
   temporary y variable is populated which works in pretty much the same
   way     the only difference is that the starting point and the end point
   of the data consumption is advanced by 1 (i.e. + 1). if this is
   confusing, please refer to the    cat sat on the mat etc.    example
   discussed above.

   the final step is converting each of the target words in each sample
   into the one-hot or categorical representation that was discussed
   previously. to do this, you can use the keras to_categorical function.
   this function takes a series of integers as its first arguments and
   adds an additional dimension to the vector of integers     this dimension
   is the one-hot representation of each integer. it   s size is specified
   by the second argument passed to the function. so say we have a series
   of integers with a shape (100, 1) and we pass it to
   the to_categorical function and specify the size to be equal to 10,000
       the returned shape will be (100, 10000). for instance, let   s say the
   series / vector of integers looked like: (0, 1, 2, 3,    .),
   the to_categorical output would look like:

   (1, 0, 0, 0, 0,    .)

   (0, 1, 0, 0, 0,    .)

   (0, 0, 1, 0, 0,    .)

   and so on   

   here the           represents a whole lot of zeroes ensuring that the total
   number of elements associated with each integer is 10,000. hopefully
   that makes sense.

   the final two lines of the generator function are straight-forward    
   first, the current_idx pointer is incremented by skip_step whose role
   was discussed previously. the last line yields the batch
   of x and y data.

   now that the generator class has been created, we need to create
   instances of it. as mentioned previously, we can setup instances of the
   same class to correspond to the training and validation data. in the
   code, this looks like the following:
train_data_generator = kerasbatchgenerator(train_data, num_steps, batch_size, vo
cabulary,
                                           skip_step=num_steps)
valid_data_generator = kerasbatchgenerator(valid_data, num_steps, batch_size, vo
cabulary,
                                           skip_step=num_steps)

   now that the input data for our keras lstm code is all setup and ready
   to go, it is time to create the lstm network itself.

creating the keras lstm structure

   in this example, the sequential way of building deep learning networks
   will be used. this way of building networks was introduced in
   my [30]keras tutorial     build a convolutional neural network in 11
   lines. the alternate way of building networks in keras is the
   functional api, which i used in my [31]id97 keras tutorial.
   basically, the sequential methodology allows you to easily stack layers
   into your network without worrying too much about all the tensors (and
   their shapes) flowing through the model. however, you still have to
   keep your wits about you for some of the more complicated layers, as
   will be discussed below. in this example, it looks like the following:
model = sequential()
model.add(embedding(vocabulary, hidden_size, input_length=num_steps))
model.add(lstm(hidden_size, return_sequences=true))
model.add(lstm(hidden_size, return_sequences=true))
if use_dropout:
    model.add(dropout(0.5))
model.add(timedistributed(dense(vocabulary)))
model.add(activation('softmax'))

   the first step involves creating a keras model with the sequential()
   constructor. the first layer in the network, as per the architecture
   diagram shown previously, is a id27 layer. this will convert
   our words (referenced by integers in the data) into meaningful
   embedding vectors. this embedding() layer takes the size of the
   vocabulary as its first argument, then the size of the resultant
   embedding vector that you want as the next argument. finally, because
   this layer is the first layer in the network, we must specify the
      length    of the input i.e. the number of steps/words in each sample.

   it   s worthwhile keeping track of the tensor shapes in the network     in
   this case, the input to the embedding layer is (batch_size, num_steps)
   and the output is (batch_size, num_steps, hidden_size). note that
   keras, in the sequential model, always maintains the batch size as the
   first dimension. it receives the batch size from the keras fitting
   function (i.e. fit_generator in this case), and therefore it is rarely
   (never?) included in the definitions of the sequential model layers.

   the next layer is the first of our two lstm layers. to specify an lstm
   layer, first you have to provide the number of nodes in the hidden
   layers within the lstm cell, e.g. the number of cells in the forget
   gate layer, the tanh squashing input layer and so on. the next argument
   that is specified in the code above is the return_sequences=true
   argument. what this does is ensure that the lstm cell returns all of
   the outputs from the unrolled lstm cell through time. if this argument
   is left out, the lstm cell will simply provide the output of the lstm
   cell from the last time step. the diagram below shows what i mean:
   keras lstm tutorial - return sequences argument comparison

   keras lstm return sequences argument comparison

   as can be observed in the diagram above, there is only one output
   when return_sequences=false     $h_t$ . however,
   when return_sequences=true all of the unrolled outputs from the lstm
   cells are returned $h_0     h_t$. in this case, we want the latter
   arrangement. why? well, in this example we are trying to predict the
   very next word in the sequence. however, if we are trying to train the
   model, it is best to be able to compare the lstm cell output at each
   time step with the very next word in the sequence     in this way we
   get num_steps sources to correct errors in the model (via
   [32]back-propagation) rather than just one for each sample.

   therefore, for both stacked lstm layers, we want to return all the
   sequences. the output shape of each lstm layer is (batch_size,
   num_steps, hidden_size).

   the next layer in our keras lstm network is a dropout layer to prevent
   overfitting. after that, there is a special keras layer for use in
   recurrent neural networks called timedistributed. this function adds an
   independent layer for each time step in the recurrent model. so, for
   instance, if we have 10 time steps in a model, a timedistributed layer
   operating on a dense layer would produce 10 independent dense layers,
   one for each time step. the activation for these dense layers is set to
   be softmax in the final layer of our keras lstm model.

compiling and running the keras lstm model

   the next step in keras, once you   ve completed your model, is to run the
   compile command on the model. it looks like this:
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['categ
orical_accuracy'])

   in this command, the type of loss that keras should use to train the
   model needs to be specified. in this case, we are using
      categorical_crossid178    which is cross id178 applied in cases
   where there are many classes or categories, of which only one is true.
   next, in this example, the optimizer that will be used is the [33]adam
   optimizer     an effective    all round    optimizer with adaptive stepping.
   finally, a metric is specified        categorical_accuracy   , which can let
   us see how the accuracy is improving during training.

   the next line of code involves creating a keras callback     callbacks
   are certain functions which keras can optionally call, usually after
   the end of a training epoch. for more on callbacks, see my [34]keras
   tutorial. the callback that is used in this example is a model
   checkpoint callback     this callback saves the model after each epoch,
   which can be handy for when you are running long-term training.
checkpointer = modelcheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', v
erbose=1)

   note that the model checkpoint function can include the epoch in its
   naming of the model, which is good for keeping track of things.

   the final step in training the keras lstm model is to call the
   aforementioned fit_generator function. the line below shows you how to
   do this:
model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_siz
e*num_steps), num_epochs,
                        validation_data=valid_data_generator.generate(),
                        validation_steps=len(valid_data)//(batch_size*num_steps)
, callbacks=[checkpointer])

   the first argument to fit_generator is our generator function that was
   explained earlier. the next argument is the number of iterations to run
   for each training epoch. the value
   given len(train_data)//(batch_size*num_steps) ensures that the whole
   data set is run through the model in each epoch. likewise, a generator
   for the smaller validation data set is called, with the same argument
   for the number of iterations to run. at the end of each epoch, the
   validation data will be run through the model and the accuracy will be
   returned. finally, the model checkpoint callback explained above is
   supplied via the callbacks argument in fit_generator. now the model is
   good to go!

   before some results are presented     some caveats are required. first
   the ptb data set is a serious text data set     not a toy problem to
   demonstrate how good lstm models are. therefore, in order to get good
   results, you   ll likely have to run the model over many epochs, and the
   model will need to have a significant level of complexity. therefore,
   it is likely to take a long time on a cpu machine, and i   d suggest
   running it on a machine with a good gpu if you want to try and
   replicate things. if you don   t have a gpu machine yourself, you can
   create an amazon ec2 instance as shown in [35]my amazon aws tutorial.
   another alternative is to use google colaboratory which offers free gpu
   time, see my introduction [36]here. i   m in the latter camp, and wasn   t
   looking to give too many dollars to amazon to train, optimize learning
   parameters and so on. however, i   ve run the model up to 40 epochs and
   gotten some reasonable initial results. my model parameters for the
   results presented below are as follows:

     num_steps=30

     batch_size=20

     hidden_size=500

   after 40 epochs, training data set accuracy was around 40%, while
   validation set accuracy reached approximately 20-25%. this is the sort
   of output you   ll see while running the training session:
   keras lstm tutorial - example training output

   keras lstm tutorial     example training output

the keras lstm results

   in order to test the trained keras lstm model, one can compare the
   predicted word outputs against what the actual word sequences are in
   the training and test data set. the code below is a snippet of how to
   do this, where the comparison is against the predicted model output and
   the training data set (the same can be done with the test_data data).
model = load_model(data_path + "\model-40.hdf5")
dummy_iters = 40
example_training_generator = kerasbatchgenerator(train_data, num_steps, 1, vocab
ulary,
                                                     skip_step=1)
print("training data:")
for i in range(dummy_iters):
    dummy = next(example_training_generator.generate())
num_predict = 10
true_print_out = "actual words: "
pred_print_out = "predicted words: "
for i in range(num_predict):
    data = next(example_training_generator.generate())
    prediction = model.predict(data[0])
    predict_word = np.argmax(prediction[:, num_steps-1, :])
    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i
]] + " "
    pred_print_out += reversed_dictionary[predict_word] + " "
print(true_print_out)
print(pred_print_out)

   in the code above, first the model is reloaded from the trained data
   (in the example above, it is the checkpoint from the 40th epoch of
   training). then another kerasbatchgenerator class is created, as was
   discussed previously     in this case, a batch of length 1 is used, as we
   only want one num_steps worth of text data to compare. then a loop of
   dummy data extractions from the generator is created     this is to
   control where in the data-set the comparison sentences are drawn from.
   the second loop, from 0 to num_predict is where the interesting stuff
   is happening.

   first, a batch of data is extracted from the generator and this is
   passed to the model.predict() method. this returns num_steps worth of
   predicted words     however, each word is represented by a categorical or
   one hot output. in other words, each word is represented by a vector of
   10,000 items, with most being zero and only one element being equal to
   1. the index of this    1    is the integer representation of the actual
   english word. so to extract the index where this    1    occurs, we can use
   the np.argmax() function. this function identifies the index where the
   maximum value occurs in a vector     in this case the maximum value is 1,
   compared to all the zeros, so this is a handy function for us to use.

   once the index has been identified, it can be translated into an actual
   english word by using the reverse_dictionary that was constructed
   during the data pre-processing. this english word is then added to the
   predicted words string, and finally the actual and predicted words are
   returned.

   the output below is the comparison between the actual and predicted
   words after 10 epochs of training on the training data set:
   keras lstm tutorial - comparison on the training data set after 10
   epochs

   comparison on the training data set after 10 epochs of training

   as can be observed, while some words match, after 10 epochs of training
   the match is pretty poor. by the way    <unk>    refers to words not
   included in the 10,000 length vocabulary of the data set.
   alternatively, if we look at the comparison after 40 epochs of training
   (again, just on the training data set):
   keras lstm tutorial - comparison on the training data set after 40
   epochs

   comparison on the training data set after 40 epochs of training

   it can be observed that the match is quite good between the actual and
   predicted words in the training set.

   however, when we look at the test data set, the match after 40 epochs
   of training isn   t quite as good:
   keras lstm tutorial - comparison on the test data set after 40 epochs

   comparison on the test data set after 40 epochs of training

   despite there not being a perfect correspondence between the predicted
   and actual words, you can see that there is a rough correspondence and
   the predicted sub-sentence at least makes some grammatical sense. so
   not so bad after all. however, in order to train a keras lstm network
   which can perform well on this realistic, large text corpus, more
   training and optimization is required. i will leave it up to you, the
   reader, to experiment further if you desire. however, the current code
   is sufficient for you to gain an understanding of how to build a keras
   lstm network, along with an understanding of the theory behind lstm
   networks.

   i hope this (large) tutorial is a help to you in understanding keras
   id137, and id137 in general.
     __________________________________________________________________

   recommended book: if you want to learn more about lstms, tensorflow and
   keras, check out this book: [37]coding the deep learning revolution     a
   step by step introduction using python, tensorflow and keras
     __________________________________________________________________


about the author

     shoujun says:
   [38]february 3, 2018 at 4:25 pm

   your blogs are very helpful!

     li says:
   [39]march 9, 2018 at 2:20 am

   thanks a lot for your awesome explanation, very helpful

     sally says:
   [40]march 28, 2018 at 1:28 am

   thank you very much for your detailed explanation!

     neha says:
   [41]april 8, 2018 at 1:10 pm

   the best description found on lstms.
   could you also explain, encoder-decoder architectures and attention
   mechanisms?
     * andy says:
       [42]april 11, 2018 at 7:39 pm
       thanks neha. i hope to do a tutorial on these networks soon

     kien tran says:
   [43]april 11, 2018 at 12:51 pm

   thanks you for this useful tutorial. do we really need the size of
   hidden layer equal to the dimension of the input (in this case, hidden
   layer size is 500)?
     * andy says:
       [44]april 11, 2018 at 7:38 pm
       thanks for the comment. no, they don   t have to be the same

     [45]yashodhan pawar says:
   [46]april 19, 2018 at 11:17 am

   this is really nice blog..they way you explained concepts with the help
   of elaborative diagrams and simple mathematical formulas is highly
   commendable.

     isaac says:
   [47]june 24, 2018 at 7:19 am

   awesome
     * andy says:
       [48]june 24, 2018 at 7:20 am
       thanks isaac!

     sandipan says:
   [49]february 19, 2019 at 11:12 pm

   okay, thats it, this blog goes into my bookmarks! amazing work.

   ____________________ (button)

   recent posts
     * [50]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [51]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [52]keras, eager and tensorflow 2.0     a new tf paradigm
     * [53]introduction to tensorboard and tensorflow visualization
     * [54]tensorflow eager tutorial

   recent comments
     * andry on [55]neural networks tutorial     a pathway to deep learning
     * sandipan on [56]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [57]neural networks tutorial     a pathway to deep learning
     * martin on [58]neural networks tutorial     a pathway to deep learning
     * uri on [59]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [60]march 2019
     * [61]january 2019
     * [62]october 2018
     * [63]september 2018
     * [64]august 2018
     * [65]july 2018
     * [66]june 2018
     * [67]may 2018
     * [68]april 2018
     * [69]march 2018
     * [70]february 2018
     * [71]november 2017
     * [72]october 2017
     * [73]september 2017
     * [74]august 2017
     * [75]july 2017
     * [76]may 2017
     * [77]april 2017
     * [78]march 2017

   categories
     * [79]amazon aws
     * [80]cntk
     * [81]convolutional neural networks
     * [82]cross id178
     * [83]deep learning
     * [84]gensim
     * [85]gpus
     * [86]keras
     * [87]id168s
     * [88]lstms
     * [89]neural networks
     * [90]nlp
     * [91]optimisation
     * [92]pytorch
     * [93]recurrent neural networks
     * [94]id23
     * [95]tensorboard
     * [96]tensorflow
     * [97]tensorflow 2.0
     * [98]weight initialization
     * [99]id97

   meta
     * [100]log in
     * [101]entries rss
     * [102]comments rss
     * [103]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [104]thrive themes | powered by [105]wordpress

   (button) close dialog

   session expired

   [106]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[107]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/keras-lstm-tutorial/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/keras-lstm-tutorial/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/keras-lstm-tutorial/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/keras-lstm-tutorial/
  14. http://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments
  15. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  16. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  17. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  18. https://github.com/adventuresinml/adventures-in-ml-code
  19. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  20. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  21. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/
  22. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  23. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  24. https://adventuresinmachinelearning.com/gensim-id97-tutorial/
  25. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  26. https://github.com/tensorflow/models/tree/master/tutorials/id56/ptb
  27. https://github.com/adventuresinml/adventures-in-ml-code
  28. http://www.fit.vutbr.cz/~imikolov/id56lm/simple-examples.tgz
  29. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  30. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  31. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  32. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  33. https://arxiv.org/abs/1412.6980
  34. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  35. https://adventuresinmachinelearning.com/tensorflow-amazon-aws/
  36. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  37. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  38. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5104
  39. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5106
  40. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5110
  41. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5111
  42. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5114
  43. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5112
  44. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5113
  45. https://sites.google.com/site/yashodhanpawar/
  46. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5118
  47. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5123
  48. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5124
  49. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  50. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  51. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  52. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  53. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  54. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  55. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  56. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  57. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  58. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  59. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  60. https://adventuresinmachinelearning.com/2019/03/
  61. https://adventuresinmachinelearning.com/2019/01/
  62. https://adventuresinmachinelearning.com/2018/10/
  63. https://adventuresinmachinelearning.com/2018/09/
  64. https://adventuresinmachinelearning.com/2018/08/
  65. https://adventuresinmachinelearning.com/2018/07/
  66. https://adventuresinmachinelearning.com/2018/06/
  67. https://adventuresinmachinelearning.com/2018/05/
  68. https://adventuresinmachinelearning.com/2018/04/
  69. https://adventuresinmachinelearning.com/2018/03/
  70. https://adventuresinmachinelearning.com/2018/02/
  71. https://adventuresinmachinelearning.com/2017/11/
  72. https://adventuresinmachinelearning.com/2017/10/
  73. https://adventuresinmachinelearning.com/2017/09/
  74. https://adventuresinmachinelearning.com/2017/08/
  75. https://adventuresinmachinelearning.com/2017/07/
  76. https://adventuresinmachinelearning.com/2017/05/
  77. https://adventuresinmachinelearning.com/2017/04/
  78. https://adventuresinmachinelearning.com/2017/03/
  79. https://adventuresinmachinelearning.com/category/amazon-aws/
  80. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  81. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  82. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  83. https://adventuresinmachinelearning.com/category/deep-learning/
  84. https://adventuresinmachinelearning.com/category/nlp/gensim/
  85. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  86. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  87. https://adventuresinmachinelearning.com/category/loss-functions/
  88. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  89. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  90. https://adventuresinmachinelearning.com/category/nlp/
  91. https://adventuresinmachinelearning.com/category/optimisation/
  92. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  93. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  94. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  95. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  96. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  97. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  98. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  99. https://adventuresinmachinelearning.com/category/nlp/id97/
 100. https://adventuresinmachinelearning.com/wp-login.php
 101. https://adventuresinmachinelearning.com/feed/
 102. https://adventuresinmachinelearning.com/comments/feed/
 103. https://wordpress.org/
 104. https://www.thrivethemes.com/
 105. http://www.wordpress.org/
 106. https://adventuresinmachinelearning.com/wp-login.php
 107. http://adventuresinmachinelearning.com/keras-lstm-tutorial/

   hidden links:
 109. https://adventuresinmachinelearning.com/author/admin/
