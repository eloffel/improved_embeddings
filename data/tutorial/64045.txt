   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

automatic feature engineering using id3

application to id161 and synthetic financial transactions data

   [16]go to the profile of hamaad shah
   [17]hamaad shah (button) blockedunblock (button) followfollowing
   feb 15, 2018

   the purpose of deep learning is to learn a representation of high
   dimensional and noisy data using a sequence of differentiable
   functions, i.e., geometric transformations, that can perhaps be used
   for supervised learning tasks among other tasks. it has had great
   success in discriminative models while generative models have not fared
   perhaps quite as well due to the limitations of explicit maximum
   likelihood estimation (id113). adversarial learning as presented in the
   generative adversarial network (gan) aims to overcome these problems by
   using implicit id113.

   we will use the mnist id161 dataset and a synthetic financial
   transactions dataset for an insurance task for these experiments using
   gans. gans are a remarkably different method of learning compared to
   explicit id113. our purpose will be to show that the representation
   learnt by a gan can be used for supervised learning tasks such as image
   recognition and insurance loss risk prediction. in this manner we avoid
   the manual process of handcrafted feature engineering by learning a set
   of features automatically, i.e., representation learning.

   this article can be seen as a follow up to this article:
   [18]https://goo.gl/tjb2v3

generative adversarial network

   there are 2 main components to a gan, the generator and the
   discriminator, that play an adversarial game against each other. in
   doing so the generator learns how to create realistic synthetic samples
   from noise, i.e., the latent space z, while the discriminator learns
   how to distinguish between a real sample and a synthetic sample.
   [1*y7ud0x9w4hsur6y7xlle5q.jpeg]

   the representation learnt by the discriminator can be used for other
   supervised learning tasks, i.e., automatic feature engineering or
   representation learning. this can also be viewed through the lens of
   id21. a gan can also be used for semi-supervised learning
   which we will get to in another paper where we will look into using
   id5, ladder networks and adversarial autoencoders
   for this purpose.

id161

   we will use the mnist dataset for this purpose where the raw data is a
   2 dimensional tensor of pixel intensities per image. the image is our
   unit of analysis: we will predict the id203 of each class for
   each image. this is a multiclass classification task and we will use
   the accuracy score to assess model performance on the test fold.
   [1*p0gc8rqyfwywqog-vxqylw@2x.png]
   2 dimensional tensor of pixel intensities per image.

   some examples of handcrafted feature engineering for the computer
   vision task perhaps might be using gabor filters.

insurance

   we will use a synthetic dataset where the raw data is a 2 dimensional
   tensor of historical policy level information per policy-period
   combination: per unit this will be a 4 by 3 dimensional tensor, i.e., 4
   historical time periods and 3 transactions types. the policy-period
   combination is our unit of analysis: we will predict the id203 of
   loss for time period 5 in the future         think of this as a potential
   renewal of the policy for which we need to predict whether it would
   make a loss for us or not hence affecting whether we decided to renew
   the policy and / or adjust the renewal premium to take into account the
   additional risk. this is a binary class classification task and we will
   use the auroc score to assess model performance.
   [1*nem9tvc7ahx_9kt_hakwmq@2x.png]
   2 dimensional tensor of transaction values per policy-period
   combination.

   some examples of handcrafted feature engineering for the insurance task
   perhaps might be using column or row averages.

   the synthetic insurance financial transactions dataset was coded in r.
   all the rest of the work is done in python.

   please note the similarities between the raw data for the computer
   vision task and the raw data for the insurance task. our main goal here
   is to learn a good representation of this raw data using automatic
   feature engineering via gans.

scikit-learn, keras and tensorflow

   we will use the python machine learning library scikit-learn for data
   transformation and the classification task. we will code the gans as
   scikit-learn transformers such that they can be readily used by
   scikit-learn pipelines. the gans will be coded using keras with the
   tensorflow backend. we also use an external gpu, i.e., gtx 1070, on a
   macbook pro.

generator

   assume that we have a prior belief on where the latent space z lies:
   p(z). given a draw from this latent space the generator g, a deep
   learner, outputs a synthetic sample.
   [1*wchsede7qaxdk2vqmx7rsw@2x.png]

discriminator

   the discriminator d is another deep learner and it aims to classify if
   a sample is real or synthetic, i.e., if a sample is from the real data
   distribution,
   [1*4ceyqdoxmkcblbptyx1rag@2x.png]

   or the synthetic data distribution.
   [1*zsswl8ugvtmdu10w8z85nq@2x.png]

   let us denote the discriminator d as follows.
   [1*vpnmrhaoa56zyupnot5nna@2x.png]

   here we assume that the positive examples are from the real data
   distribution while the negative examples are from the synthetic data
   distribution.

game: optimality

   a gan simultaneously trains the discriminator to correctly classify
   real and synthetic examples while training the generator to create
   synthetic examples such that the discriminator incorrectly classifies
   real and synthetic examples. this 2 player minimax game has the
   following objective function.
   [1*v1pa_xq3ng8zm2ilnyjrpw@2x.png]

   please note that the above expression is basically the objective
   function of the discriminator.
   [1*gdebtszmat3z7aj2zte0iw@2x.png]

   it is clear from how the game has been set up that we are trying to
   obtain a solution for d such that it maximises v(d, g) while
   simultaneously we are trying to obtain a solution for g such that it
   minimizes v(d, g).

   we do not simultaneously train d and g. we train them alternately:
   train d and then train g while freezing d. we repeat this for a fixed
   number of steps.

   if the synthetic samples taken from the generator g are realistic then
   implicitly we have learnt the synthetic data distribution. in other
   words, the synthetic data distribution can be seen as a good estimation
   of the real data distribution. the optimal solution will be as follows.
   [1*jb7dpd9exftzyhhjcxpa4a@2x.png]

   to show this let us find the optimal discriminator given a generator g
   and sample x.
   [1*zm0kbtu0dxcwncbuci72iw@2x.png]

   let us take a closer look at the discriminator   s objective function for
   a sample x.
   [1*am6ctq5ssu11taecszzbxg@2x.png]

   we have found the optimal discriminator given a generator. let us focus
   now on the generator   s objective function which is essentially to
   minimize the discriminator   s objective function.
   [1*lsumjh-rykjhsfl07fz2jw@2x.png]

   we will note the kullback   leibler (kl) divergences in the above
   objective function for the generator.
   [1*l7oud9gmi3dldtmty2kyca@2x.png]

   recall the definition of a lambda divergence.
   [1*fz-btbsfzcn1ygx0jy8ccw@2x.png]

   if lambda takes the value of 0.5 this is then called the jensen-shannon
   (js) divergence. this divergence is symmetric and non-negative.
   [1*xqtwwsfvtkqn28cq4oaqsw@2x.png]

   keeping this in mind let us take a look again at the objective function
   of the generator.
   [1*3hzrxwkfawkniputwlnp-w@2x.png]

   it is clear from the objective function of the generator above that the
   global minimum value attained is -log(4) which occurs when the
   following holds.
   [1*jb7dpd9exftzyhhjcxpa4a@2x.png]

   when the above holds the jensen-shannon divergence between the real
   data distribution and the synthetic data distribution will be zero.
   hence we have shown that the optimal solution is as follows.
   [1*jb7dpd9exftzyhhjcxpa4a@2x.png]

game: convergence

   assuming that the discriminator is allowed to reach its optimum given a
   generator, then
   [1*zsswl8ugvtmdu10w8z85nq@2x.png]

   can be shown to converge to
   [1*4ceyqdoxmkcblbptyx1rag@2x.png]

   consider the following objective function which has been previously
   shown to be convex with respect to
   [1*zsswl8ugvtmdu10w8z85nq@2x.png]

   as we found the global minimum at -log(4).
   [1*rdhuzxecgkws9wfm9yiwxw@2x.png]

   id119 is used by the generator to move towards the global
   minimum given an optimal discriminator. we will show that the gradient
   of the generator exists given an optimal discriminator, i.e.,
   [1*vqsaabgz-2stkjewqldolw@2x.png]

   such that convergence of
   [1*zsswl8ugvtmdu10w8z85nq@2x.png]

   to
   [1*4ceyqdoxmkcblbptyx1rag@2x.png]

   is guaranteed.

   note that the following is a supremum of a set of convex functions
   where the set is indexed by the discriminator:
   [1*gusuzdtbd8wxmwsa1m1sza@2x.png]

   remember that the supremum is the least upper bound.

   let us recall a few definitions regarding gradients and subgradients. a
   vector
   [1*koulotrtfdwzkdebfq3nea@2x.png]

   is a subgradient of a function
   [1*mzixszz1pt0dpfpdaogsig@2x.png]

   at a point
   [1*uxeymwssnwx9hv4b2stbwa@2x.png]

   if
   [1*vibi_4mjqiu_dnxgn9khsw@2x.png]

   the following relationship holds:
   [1*luprst5wnuglydgi0tjv2w@2x.png]

   if f is convex and differentiable then its gradient at a point x is
   also the subgradient. most importantly, a subgradient can exist even if
   f is not differentiable.

   the subgradients of the supremum of a set of convex functions include
   the subgradient of the function at the point where the supremum is
   attained. as mentioned earlier, we have already shown that
   [1*viq_wb2ndh_srh9jqnci8w@2x.png]

   is convex.
   [1*poi5fidujx71rkeeh8otug@2x.png]

   the gradient of the generator
   [1*xn3jctntzjwozooesxrsaw@2x.png]

   is used to make incremental improvements to the objective function of
   the generator
   [1*viq_wb2ndh_srh9jqnci8w@2x.png]

   given an optimal discriminator d*. therefore convergence of
   [1*zsswl8ugvtmdu10w8z85nq@2x.png]

   to
   [1*4ceyqdoxmkcblbptyx1rag@2x.png]

   is guaranteed.

results

   in these experiments we show the ability of the generator to create
   realistic synthetic examples for the mnist dataset and the insurance
   dataset. we use a 2-dimensional latent manifold.

   finally we show that using the representation learnt by the
   discriminator we can attain competitive results to using other
   representation learning methods for the mnist dataset and the insurance
   dataset such as a wide variety of autoencoders as shown in this
   article: [19]https://goo.gl/tjb2v3

generating new data

   [1*8a8e3udywbbbp-htyaccra.png]
   [1*myblwkxvvfbxv0ygqxnr6a.png]
   [1*pktv2n-jmezif0whad1kkg.png]

   with image data we can perhaps judge qualitatively whether the
   generated data makes sense. for financial transactions data this is not
   possible. however let us have a look at an example of a generated
   transactions lattice. please note that all financial transactions data
   has been transformed to lie between 0 and 1.
   [1*t2fmkemkx-tl7ijbxsn69a.png]

   if we use the same matplotlib code as applied to the image data to plot
   the above generated transactions lattice we get the following image. we
   can see that where we have the maximum value possible for a
   transaction, i.e., 1, that is colored as black, while where we have the
   minimum value possible for a transaction, i.e., 0, that is colored as
   white. transactions values in between have some gray color.
   [1*prklf8vourahxpnan72hca.png]

   finally let us compare the distributions of actual and generated
   transactions lattices to see whether generated values are similar to
   actual values. this is a simple sanity check and it appears that the
   distributions are fairly similar.

   another way perhaps is to check if the features learnt by the
   discriminator are useful for a supervised learning task. this seems to
   be the case in our insurance data example especially if we compare
   their performance with autoencoders for the same task here:
   [20]https://goo.gl/tjb2v3
   [1*uwvfyhnrergqtwcith9qaw.png]

gan for representation learning

     the accuracy score for the mnist classification task with dcgan:
     98.350000%.

     the accuracy score for the mnist classification task with deeper
     cgan: 99.090000%.

     the auroc score for the insurance classification task with dcgan:
     92.795883%.

   compare these results with using autoencoders for representation
   learning for the same id161 and insurance tasks here:
   [21]https://goo.gl/tjb2v3

code on github

   the code i   ve written for this article is available on my github here:
   [22]https://github.com/hamaadshah/gan_keras

conclusion

   we have shown how to use gans to learn a good representation of raw
   data, i.e., 2 dimensional tensors per unit of analysis, that can then
   perhaps be used for supervised learning tasks in the domain of computer
   vision and insurance. this moves us away from manual handcrafted
   feature engineering towards automatic feature engineering, i.e.,
   representation learning. gans can perhaps be also used for
   semi-supervised learning which will be the topic of another paper.

references

    1. goodfellow, i., bengio, y. and courville a. (2016). deep learning
       (mit press).
    2. geron, a. (2017). hands-on machine learning with scikit-learn &
       tensorflow (o   reilly).
    3. radford, a., luke, m. and chintala, s. (2015). unsupervised
       representation learning with deep convolutional generative
       adversarial networks ([23]https://arxiv.org/abs/1511.06434).
    4. goodfellow, i., pouget-abadie, j., mirza, m., xu, b., warde-farley,
       d., ozair, s., courville, a., bengio, y. (2014). generative
       adversarial networks ([24]https://arxiv.org/abs/1406.2661).
    5. [25]http://scikit-learn.org/stable/#
    6. [26]https://towardsdatascience.com/learning-rate-schedules-and-adap
       tive-learning-rate-methods-for-deep-learning-2c8f433990d1
    7. [27]https://stackoverflow.com/questions/42177658/how-to-switch-back
       end-with-keras-from-tensorflow-to-theano
    8. [28]https://blog.keras.io/building-autoencoders-in-keras.html
    9. [29]https://keras.io
   10. [30]https://github.com/fchollet/keras/blob/master/examples/mnist_ac
       gan.py#l24
   11. [31]https://en.wikipedia.org/wiki/kullback   leibler_divergence
   12. [32]https://goo.gl/tjb2v3
   13. [33]https://see.stanford.edu/materials/lsocoee364b/01-subgradients_
       notes.pdf

     * [34]machine learning
     * [35]deep learning
     * [36]data science
     * [37]bayesian machine learning
     * [38]towards data science

   (button)
   (button)
   (button) 702 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of hamaad shah

[40]hamaad shah

   vp lead data scientist at deutsche bank with expertise and experience
   in machine learning and quantitative analytics applied to financial
   services.

     (button) follow
   [41]towards data science

[42]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 702
     * (button)
     *
     *

   [43]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [44]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8e24b3c16bf3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/automatic-feature-engineering-using-generative-adversarial-networks-8e24b3c16bf3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/automatic-feature-engineering-using-generative-adversarial-networks-8e24b3c16bf3&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_c8iaydnobtwq---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@littlemarble?source=post_header_lockup
  17. https://towardsdatascience.com/@littlemarble
  18. https://goo.gl/tjb2v3
  19. https://goo.gl/tjb2v3
  20. https://goo.gl/tjb2v3
  21. https://goo.gl/tjb2v3
  22. https://github.com/hamaadshah/gan_keras
  23. https://arxiv.org/abs/1511.06434
  24. https://arxiv.org/abs/1406.2661
  25. http://scikit-learn.org/stable/
  26. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1
  27. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano
  28. https://blog.keras.io/building-autoencoders-in-keras.html
  29. https://keras.io/
  30. https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py#l24
  31. https://en.wikipedia.org/wiki/kullback
  32. https://goo.gl/tjb2v3
  33. https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf
  34. https://towardsdatascience.com/tagged/machine-learning?source=post
  35. https://towardsdatascience.com/tagged/deep-learning?source=post
  36. https://towardsdatascience.com/tagged/data-science?source=post
  37. https://towardsdatascience.com/tagged/bayesian-machine-learning?source=post
  38. https://towardsdatascience.com/tagged/towards-data-science?source=post
  39. https://towardsdatascience.com/@littlemarble?source=footer_card
  40. https://towardsdatascience.com/@littlemarble
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/?source=footer_card
  43. https://towardsdatascience.com/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/8e24b3c16bf3/share/twitter
  47. https://medium.com/p/8e24b3c16bf3/share/facebook
  48. https://medium.com/p/8e24b3c16bf3/share/twitter
  49. https://medium.com/p/8e24b3c16bf3/share/facebook
