   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

web scraping tutorial with python: tips and tricks

   [14]go to the profile of jekaterina kokatjuhha
   [15]jekaterina kokatjuhha (button) blockedunblock (button)
   followfollowing
   jan 9, 2018
   [1*womq16d7_myhvysmcemefq.png]
   [1*rah6xoghpmktwkhy2rvbwq.png]
   [16]https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html

   i was searching for flight tickets and noticed that ticket prices
   fluctuate during the day. i tried to find out when the best time to buy
   tickets is, but there was nothing on the web that helped. i built a
   small program to automatically collect the data from the web         a
   so-called scraper. it extracted information for my specific flight
   destination on predetermined dates and notified me when the price got
   lower.

     web scraping is a technique used to extract data from websites
     through an automated process.

   i learned a lot from this experience with web scraping, and i want to
   share it.

   this post is intended for people who are interested to know about the
   common design patterns, pitfalls and rules related to the web scraping.
   the ariticle presents several use cases and a collection of typical
   problems, such as how not to be detected, dos and don   ts, and how to
   speed up (parallelization) your scraper.

   everything will be accompanied by python snippets, so that you can
   start straight away. this document will also go through several useful
   python packages.

use cases

   there are many reasons and use cases why you would want to scrape data.
   let me list some of them:
     * [17]scrape pages of a e-retailer to spot if some of the clothes you
       want to buy got discounted
     * [18]compare prices of several clothes brands by scraping their
       pages
     * price of the flight tickets can vary during the day. one could
       crawl the travel website and get alarmed once the price was lowered
     * analyze the action websites to answer the question if starting bid
       should be low or high to attract more bidders or if the longer
       auction correlates with a higher end bid

tutorial

   structure of the tutorial:
    1. available packages
    2. basic code
    3. pitfalls
    4. dos and dont   s
    5. speed up         parallelization

   before we start: be nice to the servers; you don   t want to crash a
   website.
     __________________________________________________________________

1. available packages and tools

   there is no universal solution for web scraping because the way data is
   stored on each website is usually specific to that site. in fact, if
   you want to scrape the data, you need to understand the website   s
   structure and either build your own solution or use a highly
   customizable one.

   however, you don   t need to reinvent the wheel: there are many packages
   that do the most work for you. depending on your programming skills and
   your intended use case, you might find different packages more or less
   useful.

   1.1 inspect option

   most of the time you will finding yourself inspecting the [19]html the
   website. you can easily do it with an    inspect    [20]option of your
   bowser.
   [1*1nc5h4qmq7vu9qcgtzpttg.png]

   the section of the website that holds my name, my avatar and my
   description is called hero hero--profile u-flextop(how interesting that
   medium calls its writers    heroes    :)). the <h1> class that holds my
   name is calledui-h2 hero-title and the description is contained within
   the <p> class ui-body hero-description.

   you can read more about [21]html tags, and differences between
   [22]classes and [23]ids [24]here.

   1.2 scrapy

   there is a stand-alone ready-to-use data extracting framework called
   [25]scrapy. apart from extracting html the package offers lots of
   functionalities like exporting data in formats, logging etc. it is also
   highly customisable: run different spiders on different processes,
   disable cookies   and set download delays  . it can also be used to
   extract data using api. however, the learning curve is not smooth for
   the new programmers: you need to read tutorials and examples to get
   started.

      some sites use cookies to identify bots.
      the website can get overloaded due to a huge amount of crawling
   requests.

   for my use case it was too much    out of the box   : i just wanted to
   extract the links from all pages, access each link and extract
   information out of it.

   1.3 beautifulsoup with requests

   beautifulsoup is a library that allows you to parse the html source
   code in a beautiful way. along with it you need a request library that
   will fetch the content of the url. however, you should take care of
   everything else like error handling, how to export data, how to
   parallelize the web scraper, etc.

   i chose beautifulsoup as it would force me to figure out a lot of stuff
   that scrapy handles on its own, and hopefully help me learn faster from
   my mistakes.
     __________________________________________________________________

2. basic code

   it   s very straightforward to start scraping a website. most of the time
   you will find yourself inspecting [26]html of the website to access the
   classes and ids you need. lets say we have a following html structure
   and we want to extract the main_price elements. note: discounted_price
   element is optional.
<body>
<div id="listings_prices">
 <div class="item">
  <li class="item_name">watch</li>
  <div class="main_price">price: $66.68</div>
       <div class="discounted_price">discounted price: $46.68</div>
   </div>
   <div class="item">
  <li class="item_name">watch2</li>
  <div class="main_price">price: $56.68</div>
   </div>
</div>
</body>

   the basic code would be to import the libraries, do the request, parse
   the html and then to find the class main_price.

   iframe: [27]/media/62c950b507b44b66b495afee7d58b1c7?postid=db070e70e071

   [28]https://gist.github.com/jkokatjuhha/02af3a28cf512ee8a3096273850fe02
   9

   it can happen that the class main_price is present in another section
   of the website. to avoid extracting unnecessary class main_price from
   any other part of the webpage we could have first addressed the id
   listings_prices and only then find all elements with class main_price.
     __________________________________________________________________

3. pitfalls

   3.1 check robots.txt

   the scraping rules of the websites can be found in the [29]robots.txt
   file. you can find it by writing robots.txt after the main domain, e.g
   [30]www.website_to_scrape.com/robots.txt. these rules identify which
   parts of the websites are not allowed to be automatically extracted or
   how frequently a bot is allowed to request a page. most people don   t
   care about it, but try to be respectful and at least look at the rules
   even if you don   t plan to follow them.

   3.2 html can be evil

   html tags can contain id, class or both. html id specifies a unique id
   and html class is non-unique. changes in the class name or element
   could either break your code or deliver wrong results.

   there are two ways to avoid it or at least to be alerted about it:
     * use specific id rather than class since it is less likely to be
       changed
     * check if the element returns none

   iframe: [31]/media/60336945eebd3252470c17bf94589709?postid=db070e70e071

   [32]https://gist.github.com/jkokatjuhha/392744085cec10b6000a1f82adb462f
   e

   however, because some fields can be optional (like discounted_price in
   our html example), corresponding elements would not appear on each
   listing. in this case you can count the percentage of how many times
   this specific element returned none to the number of listings. if it is
   100%, you might want to check if the element name was changed.

   3.3 user agent spoofing

   everytime you visit a website, it gets your [33]browser information via
   [34]user agent. some websites won   t show you any content unless you
   provide a user agent. also, some sites offer different content to
   different browsers. websites do not want to block genuine users but you
   would look suspicious if you send 200 requests/second with the same
   user agent. a way out might be either to generate (almost) random user
   agent or to set one yourself.

   iframe: [35]/media/28da7b05bb5954f75f6a4c11795acb0c?postid=db070e70e071

   [36]https://gist.github.com/jkokatjuhha/083c1b5e14e64b3b1ff734bb45b859b
   e

   3.4 timeout request

   [37]by default, request will keep waiting for a response indefinitely.
   therefore, it is advised to set the timeout parameter.

   iframe: [38]/media/13814b73c72fc285dfecebbe358bebdb?postid=db070e70e071

   [39]https://gist.github.com/jkokatjuhha/64cecefa0bf31c2b21111373c11fcc6
   6

   3.5 did i get blocked?

   frequent appearance of the [40]status codes like 404 (not found), 403
   (forbidden), 408 (request timeout) might indicate that you got blocked.
   you may want to check for those error codes and proceed accordingly.
   also, be ready to handle exceptions from the request.

   iframe: [41]/media/c80ab1ac34642c4ccaf78e7aeac3b92e?postid=db070e70e071

   [42]https://gist.github.com/jkokatjuhha/a33467fae4c9f7fac64f067501b484a
   c

   3.6 ip rotation

   even if you randomize your user agent, all your requests will be from
   the same ip address. that doesn   t sound abnormal because libraries,
   universities, and also companies have only a few ip addresses. however,
   if there are uncommonly many requests coming from a single ip address,
   a server can detect it.
   using shared [43]proxies, vpns or tor can help you become a ghost ;).

   iframe: [44]/media/e6bfa1c02047234b9696a443a68dfa6a?postid=db070e70e071

   [45]https://gist.github.com/jkokatjuhha/a4df4078aa1f86846511332c472fadb
   f

   by using a shared proxy, the website will see the ip address of the
   proxy server and not yours. a vpn connects you to another network and
   the ip address of the vpn provider will be sent to the website.

   3.7 honeypots

   honeypots are means to detect [46]crawlers or scrapers.

   these can be    hidden    links that are not visible to the users but can
   be extracted by scrapers/spiders. such links will have a css style set
   to display:none, they can be blended by having the color of the
   background, or even be moved off of the visible area of the page. once
   your crawler visits such a link, your ip address can be flagged for
   further investigation, or even be instantly blocked.

   another way to spot crawlers is to add links with infinitely deep
   directory trees. then one would need to limit the number of retrieved
   pages or limit the traversal depth.
     __________________________________________________________________

4. dos and don   ts

     * before scraping, check if there is a public api available. public
       apis provide easier and faster (and legal) data retrieval than web
       scraping. check out [47]twitter api that provides apis for
       different purposes.
     * in case you scrape lots of data, you might want to consider using a
       database to be able to analyze or retrieve it fast. follow [48]this
       tutorial on how to create a local database with python.
     * be polite. as [49]this answer suggests, it is recommended to let
       people know that you are scraping their website so they can better
       respond to the problems your bot might cause.

   again, do not overload the website by sending hundreds of requests per
   second.
     __________________________________________________________________

5. speed up         parallelization

   if you decide to parallelize your program, be careful with your
   implementation so you don   t slam the server. and be sure you read the
   dos and don   ts section. check out the the definitions of
   parallelization vs concurrency, processors and threads [50]here and
   [51]here.

   if you extract a huge amount of information from the page and do some
   preprocessing of the data while scraping, the number of requests per
   second you send to the page can be relatively low.

   for my other project where i scraped apartment rental prices, i did
   heavy preprocessing of the data while scraping, which resulted in 1
   request/second. in order to scrape 4k ads, my program would run for
   about one hour.

   in order to send requests in parallel you might want to use a
   [52]multiprocessing package.

   let   s say we have 100 pages and we want to assign every processor equal
   amount of pages to work with. if n is the number of cpus, you can
   evenly chunk all pages into the n bins and assign each bin to a
   processor. each process will have its own name, target function and the
   arguments to work with. the name of the process can be used afterwards
   to enable writing data to a specific file.

   i assigned 1k pages to each of my 4 cpus which yielded 4
   requests/second and reduced the scraping time to around 17 mins.

   iframe: [53]/media/a9b775836f039cc8cc0b441d5dcb2e70?postid=db070e70e071

   [54]https://gist.github.com/jkokatjuhha/7927b27cf7a831c48e223b7c06fbd40
   1
     __________________________________________________________________

happy scraping!

     * [55]web development
     * [56]software development
     * [57]python
     * [58]programming
     * [59]life hacking

   (button)
   (button)
   (button) 5.2k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [60]go to the profile of jekaterina kokatjuhha

[61]jekaterina kokatjuhha

     (button) follow
   [62]hacker noon

[63]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 5.2k
     * (button)
     *
     *

   [64]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [65]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/db070e70e071
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_kppcsxvratem---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@k.kokatjuhha?source=post_header_lockup
  15. https://hackernoon.com/@k.kokatjuhha
  16. https://www.kdnuggets.com/2018/02/top-news-week-0129-0204.html
  17. https://twitter.com/plloyduk/status/935593622141063169
  18. https://towardsdatascience.com/data-driven-lingerie-shopping-6dc61c57f97f
  19. https://www.w3schools.com/html/html_intro.asp
  20. https://www.lifewire.com/get-inspect-element-tool-for-browser-756549
  21. https://www.w3schools.com/tags/
  22. https://www.w3schools.com/html/html_classes.asp
  23. https://www.w3schools.com/tags/att_global_id.asp
  24. https://www.quora.com/what-is-the-difference-between-class-and-id-in-html
  25. https://scrapy.org/
  26. https://www.w3schools.com/html/html_intro.asp
  27. https://hackernoon.com/media/62c950b507b44b66b495afee7d58b1c7?postid=db070e70e071
  28. https://gist.github.com/jkokatjuhha/02af3a28cf512ee8a3096273850fe029
  29. http://www.robotstxt.org/robotstxt.html
  30. http://www.website_to_scrap.com/robots.txt
  31. https://hackernoon.com/media/60336945eebd3252470c17bf94589709?postid=db070e70e071
  32. https://gist.github.com/jkokatjuhha/392744085cec10b6000a1f82adb462fe
  33. https://www.whoishostingthis.com/tools/user-agent/
  34. https://en.wikipedia.org/wiki/user_agent
  35. https://hackernoon.com/media/28da7b05bb5954f75f6a4c11795acb0c?postid=db070e70e071
  36. https://gist.github.com/jkokatjuhha/083c1b5e14e64b3b1ff734bb45b859be
  37. http://docs.python-requests.org/en/master/user/quickstart/#timeouts
  38. https://hackernoon.com/media/13814b73c72fc285dfecebbe358bebdb?postid=db070e70e071
  39. https://gist.github.com/jkokatjuhha/64cecefa0bf31c2b21111373c11fcc66
  40. https://en.wikipedia.org/wiki/list_of_http_status_codes
  41. https://hackernoon.com/media/c80ab1ac34642c4ccaf78e7aeac3b92e?postid=db070e70e071
  42. https://gist.github.com/jkokatjuhha/a33467fae4c9f7fac64f067501b484ac
  43. https://www.privateinternetaccess.com/pages/tor-vpn-proxy
  44. https://hackernoon.com/media/e6bfa1c02047234b9696a443a68dfa6a?postid=db070e70e071
  45. https://gist.github.com/jkokatjuhha/a4df4078aa1f86846511332c472fadbf
  46. https://en.wikipedia.org/wiki/web_crawler
  47. https://developer.twitter.com/en/docs
  48. http://zetcode.com/db/sqlitepythontutorial/
  49. https://webmasters.stackexchange.com/questions/6205/what-user-agent-should-i-set
  50. https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python
  51. https://code.tutsplus.com/articles/introduction-to-parallel-and-concurrent-programming-in-python--cms-28612
  52. https://docs.python.org/2/library/multiprocessing.html
  53. https://hackernoon.com/media/a9b775836f039cc8cc0b441d5dcb2e70?postid=db070e70e071
  54. https://gist.github.com/jkokatjuhha/7927b27cf7a831c48e223b7c06fbd401
  55. https://hackernoon.com/tagged/web-development?source=post
  56. https://hackernoon.com/tagged/software-development?source=post
  57. https://hackernoon.com/tagged/python?source=post
  58. https://hackernoon.com/tagged/programming?source=post
  59. https://hackernoon.com/tagged/life-hacking?source=post
  60. https://hackernoon.com/@k.kokatjuhha?source=footer_card
  61. https://hackernoon.com/@k.kokatjuhha
  62. https://hackernoon.com/?source=footer_card
  63. https://hackernoon.com/?source=footer_card
  64. https://hackernoon.com/
  65. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  67. https://medium.com/p/db070e70e071/share/twitter
  68. https://medium.com/p/db070e70e071/share/facebook
  69. https://medium.com/p/db070e70e071/share/twitter
  70. https://medium.com/p/db070e70e071/share/facebook
