   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

a guide for time series prediction using recurrent neural networks (lstms)

forecasting future currency exchange rates with long short-term
memory (lstms)

   [16]go to the profile of neelabh pant
   [17]neelabh pant (button) blockedunblock (button) followfollowing
   sep 7, 2017
   [18][1*jdqusr0c9xkln4vqpc3fzq.png]

   the [19]statsbot team has already published the article about using
   [20]time series analysis for anomaly detection. today, we   d like to
   discuss time series prediction with a long short-term memory model
   (lstms). we asked a data scientist, neelabh pant, to tell you about his
   experience of forecasting exchange rates using recurrent neural
   networks.
   [1*mvlugavhiv0upx0a1rygtw.jpeg]

   as an indian guy living in the us, i have a constant flow of money from
   home to me and vice versa. if the usd is stronger in the market, then
   the indian rupee (inr) goes down, hence, a person from india buys a
   dollar for more rupees. if the dollar is weaker, you spend less rupees
   to buy the same dollar.

   if one can predict how much a dollar will cost tomorrow, then this can
   guide one   s decision making and can be very important in minimizing
   risks and maximizing returns. looking at the strengths of a neural
   network, especially a recurrent neural network, i came up with the idea
   of predicting the exchange rate between the usd and the inr.

   there are a lot of methods of forecasting exchange rates such as:
     * purchasing power parity (ppp), which takes the inflation into
       account and calculates inflation differential.
     * relative economic strength approach, which considers the economic
       growth of countries to predict the direction of exchange rates.
     * econometric model is another common technique used to forecast the
       exchange rates which is customizable according to the factors or
       attributes the forecaster thinks are important. there could be
       features like interest rate differential between two different
       countries, gdp growth rates, income growth rates, etc.
     * time series model is purely dependent on the idea that past
       behavior and price patterns can be used to predict future price
       behavior.

   in this article, we   ll tell you how to predict the future exchange rate
   behavior using time series analysis and by making use of machine
   learning with time series.

sequence problems

   let us begin by talking about sequence problems. the simplest machine
   learning problem involving a sequence is a one to one problem.
   [0*7aimlpm1e7hggolz.]
   one to one

   in this case, we have one data input or tensor to the model and the
   model generates a prediction with the given input. id75,
   classification, and even image classification with convolutional
   network fall into this category. we can extend this formulation to
   allow for the model to make use of the pass values of the input and the
   output.

   it is known as the one to many problem. the one to many problem starts
   like the one to one problem where we have an input to the model and the
   model generates one output. however, the output of the model is now fed
   back to the model as a new input. the model now can generate a new
   output and we can continue like this indefinitely. you can now see why
   these are known as recurrent neural networks.
   [0*qfwzfolmh4eyyzxu.]
   one to many

   a recurrent neural network deals with sequence problems because their
   connections form a directed cycle. in other words, they can retain
   state from one iteration to the next by using their own output as input
   for the next step. in programming terms this is like running a fixed
   program with certain inputs and some internal variables. the simplest
   recurrent neural network can be viewed as a fully connected neural
   network if we unroll the time axes.
   [0*x1vmplhmsow0kzvk.]
   id56 unrolled time
   [0*ni39bju15z96htxw.]

   in this univariate case only two weights are involved. the weight
   multiplying the current input xt, which is u, and the weight
   multiplying the previous output yt-1, which is w. this formula is like
   the exponential weighted moving average (ewma) by making its pass
   values of the output with the current values of the input.

   one can build a deep recurrent neural network by simply stacking units
   to one another. a simple recurrent neural network works well only for a
   short-term memory. we will see that it suffers from a fundamental
   problem if we have a longer time dependency.

long short-term neural network

   as we have talked about, a simple recurrent network suffers from a
   fundamental problem of not being able to capture long-term dependencies
   in a sequence. this is a problem because we want our id56s to analyze
   text and answer questions, which involves keeping track of long
   sequences of words.

   in late    90s, [21]lstm was proposed by sepp hochreiter and jurgen
   schmidhuber, which is relatively insensitive to gap length over
   alternatives id56s, id48, and other sequence learning
   methods in numerous applications.
   [0*_rc7uksazzfokpfz.]
   lstm architecture

   this model is organized in cells which include several operations. lstm
   has an internal state variable, which is passed from one cell to
   another and modified by operation gates.

   1. forget gate
   [0*yk0duxow-jly8dzk.]

   it is a sigmoid layer that takes the output at t-1 and the current
   input at time t and concatenates them into a single tensor and applies
   a linear transformation followed by a sigmoid. because of the sigmoid,
   the output of this gate is between 0 and 1. this number is multiplied
   with the internal state and that is why the gate is called a forget
   gate. if ft=0 then the previous internal state is completely forgotten,
   while if ft=1 it will be passed through unaltered.

   2. input gate
   [0*wo-tfx3t3t6l6bfj.]

   the input gate takes the previous output and the new input and passes
   them through another sigmoid layer. this gate returns a value between 0
   and 1. the value of the input gate is multiplied with the output of the
   candidate layer.
   [0*zq_yfpo7eg4wl6qy.]

   this layer applies a hyperbolic tangent to the mix of input and
   previous output, returning a candidate vector to be added to the
   internal state.

   the internal state is updated with this rule:
   [0*9yb45vnf6g47ddv8.]

   .the previous state is multiplied by the forget gate and then added to
   the fraction of the new candidate allowed by the output gate.

   3. output gate
   [0*9wb-rbvyurzkpzhp.]
   [0*vdc6tlu5kbpfn7c9.]

   this gate controls how much of the internal state is passed to the
   output and it works in a similar way to the other gates.

   these three gates described above have independent weights and biases,
   hence the network will learn how much of the past output to keep, how
   much of the current input to keep, and how much of the internal state
   to send out to the output.

   in a recurrent neural network, you not only give the network the data,
   but also the state of the network one moment before. for example, if i
   say    hey! something crazy happened to me when i was driving    there is a
   part of your brain that is flipping a switch that   s saying    oh, this is
   a story neelabh is telling me. it is a story where the main character
   is neelabh and something happened on the road.    now, you carry a little
   part of that one sentence i just told you. as you listen to all my
   other sentences you have to keep a bit of information from all past
   sentences around in order to understand the entire story.

   another example is video processing, where you would again need a
   recurrent neural network. what happens in the current frame is heavily
   dependent upon what was in the last frame of the movie most of the
   time. over a period of time, a recurrent neural network tries to learn
   what to keep and how much to keep from the past, and how much
   information to keep from the present state, which makes it so powerful
   as compared to a simple feed forward neural network.

time series prediction

   i was impressed with the strengths of a recurrent neural network and
   decided to use them to predict the exchange rate between the usd and
   the inr. the dataset used in this project is the exchange rate data
   between january 2, 1980 and august 10, 2017. later, i   ll give you a
   link to download this dataset and experiment with it.
   [0*f70cza2vhe0r_rsq.]
   table 1. dataset example

   the dataset displays the value of $1 in rupees. we have a total of
   13,730 records starting from january 2, 1980 to august 10, 2017.
   [0*uyhldtufptm7yps6.]
   usd vs inr

   over the period, the price to buy $1 in rupees has been rising. one can
   see that there was a huge dip in the american economy during 2007   2008,
   which was hugely caused by the great recession during that period. it
   was a period of general economic decline observed in world markets
   during the late 2000s and early 2010s.

   this period was not very good for the world   s developed economies,
   particularly in north america and europe (including russia), which fell
   into a definitive recession. many of the newer developed economies
   suffered far less impact, particularly china and india, whose economies
   grew substantially during this period.

test-train split

   now, to train the machine we need to divide the dataset into test and
   training sets. it is very important when you do time series to split
   train and test with respect to a certain date. so, you don   t want your
   test data to come before your training data.

   in our experiment, we will define a date, say january 1, 2010, as our
   split date. the training data is the data between january 2, 1980 and
   december 31, 2009, which are about 11,000 training data points.

   the test dataset is between january 1, 2010 and august 10, 2017, which
   are about 2,700 points.
   [0*jxh_d2zd8tomxa1h.]
   train-test split

   the next thing to do is normalize the dataset. you only need to fit and
   transform your training data and just transform your test data. the
   reason you do that is you don   t want to assume that you know the scale
   of your test data.

   normalizing or transforming the data means that the new scale variables
   will be between zero and one.

neural network models

   a fully connected model is a simple neural network model which is built
   as a simple regression model that will take one input and will spit out
   one output. this basically takes the price from the previous day and
   forecasts the price of the next day.

   as a id168, we use mean squared error and stochastic gradient
   descent as an optimizer, which after enough numbers of epochs will try
   to look for a good local optimum. below is the summary of the fully
   connected layer.
   [0*u3xljemm4m-0ucjr.]
   summary of a fully connected layer

   after training this model for 200 epochs or early_callbacks (whichever
   came first), the model tries to learn the pattern and the behavior of
   the data. since we split the data into training and testing sets we can
   now predict the value of testing data and compare them with the ground
   truth.
   [0*6-fjhypogwczges7.]
   ground truth(blue) vs prediction(orange)

   as you can see, the model is not good. it essentially is repeating the
   previous values and there is a slight shift. the fully connected model
   is not able to predict the future from the single previous value. let
   us now try using a recurrent neural network and see how well it does.

long short-term memory

   the recurrent model we have used is a one layer sequential model. we
   used 6 lstm nodes in the layer to which we gave input of shape (1,1),
   which is one input given to the network with one value.
   [0*fdevzbb0ibwhtliw.]
   summary of lstm model

   the last layer is a dense layer where the loss is mean squared error
   with stochastic id119 as an optimizer. we train this model
   for 200 epochs with early_stopping callback. the summary of the model
   is shown above.
   [1*ysq--yj7je3greiix5knbg.png]
   lstm prediction

   this model has learned to reproduce the yearly shape of the data and
   doesn   t have the lag it used to have with a simple feed forward neural
   network. it is still underestimating some observations by certain
   amounts and there is definitely room for improvement in this model.

changes in the model

   there can be a lot of changes to be made in this model to make it
   better. one can always try to change the configuration by changing the
   optimizer. another important change i see is by using the [22]sliding
   time window method, which comes from the field of stream data
   management system.

   this approach comes from the idea that only the most recent data are
   important. one can show the model data from a year and try to make a
   prediction for the first day of the next year. sliding time window
   methods are very useful in terms of fetching important patterns in the
   dataset that are highly dependent on the past bulk of observations.

   try to make changes to this model as you like and see how the model
   reacts to those changes.

dataset

   i made the dataset available on my github account [23]under deep
   learning in python repository. feel free to download the dataset and
   play with it.

useful sources

   i personally follow some of my favorite data scientists like [24]kirill
   eremenko, [25]jose portilla, [26]dan van boxel (better known as dan
   does data), and many more. most of them are available on different
   podcast stations where they talk about different current subjects like
   id56, convolutional neural networks, lstm, and even the most recent
   technology, [27]id63.

   try to keep up with the news of different [28]artificial intelligence
   conferences. by the way, if you are interested, then kirill eremenko is
   [29]coming to san diego this november with his amazing team to give
   talks on machine learning, neural networks, and data science.

conclusion

   lstm models are powerful enough to learn the most important past
   behaviors and understand whether or not those past behaviors are
   important features in making future predictions. there are several
   applications where lstms are highly used. applications like speech
   recognition, music composition, handwriting recognition, and even in my
   current research of human mobility and travel predictions.

   according to me, lstm is like a model which has its own memory and
   which can behave like an intelligent human in making decisions.

   thank you again and happy machine learning!

   iframe: [30]/media/02231cd5403151a2a463e36cc3b88462?postid=807fa6ca7f

you   d also like:

   [31]strategic pricing in retail with machine learning
   building a dynamic pricing strategy in retailblog.statsbot.co
   [32]id108 to improve machine learning results
   how ensemble methods work: id112, boosting and
   stackingblog.statsbot.co
   [33]support vector machines tutorial
   learning id166s from examplesblog.statsbot.c

     * [34]machine learning
     * [35]data science
     * [36]neural networks
     * [37]timeseries
     * [38]recurrent neural network

   (button)
   (button)
   (button) 2.6k claps
   (button) (button) (button) 26 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of neelabh pant

[40]neelabh pant

   i love data science. let   s build some intelligent bots together! ;)

     (button) follow
   [41]stats and bots

[42]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 2.6k
     * (button)
     *
     *

   [43]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [44]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/807fa6ca7f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_gnlwbmgdkcvh---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@neelabhpant?source=post_header_lockup
  17. https://blog.statsbot.co/@neelabhpant
  18. https://cube.dev/
  19. http://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=timeseries_lstm
  20. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  21. http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735
  22. https://en.wikipedia.org/wiki/data_stream_management_system#windows
  23. https://github.com/neelabhpant/deep-learning-in-python
  24. https://www.superdatascience.com/
  25. https://www.udemy.com/user/joseporitlla/
  26. https://www.youtube.com/user/dvbuntu
  27. https://en.wikipedia.org/wiki/neural_turing_machine
  28. http://www.aaai.org/
  29. https://www.datasciencego.com/?utm_source=email&utm_medium=allless_id1&utm_content=em2_earlybirds_imagelogo&utm_campaign=event
  30. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=807fa6ca7f
  31. https://blog.statsbot.co/strategic-pricing-d1adcc2e0fd6
  32. https://blog.statsbot.co/ensemble-learning-d1dcd548e936
  33. https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93
  34. https://blog.statsbot.co/tagged/machine-learning?source=post
  35. https://blog.statsbot.co/tagged/data-science?source=post
  36. https://blog.statsbot.co/tagged/neural-networks?source=post
  37. https://blog.statsbot.co/tagged/timeseries?source=post
  38. https://blog.statsbot.co/tagged/recurrent-neural-network?source=post
  39. https://blog.statsbot.co/@neelabhpant?source=footer_card
  40. https://blog.statsbot.co/@neelabhpant
  41. https://blog.statsbot.co/?source=footer_card
  42. https://blog.statsbot.co/?source=footer_card
  43. https://blog.statsbot.co/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://blog.statsbot.co/strategic-pricing-d1adcc2e0fd6
  47. https://blog.statsbot.co/ensemble-learning-d1dcd548e936
  48. https://blog.statsbot.co/support-vector-machines-tutorial-c1618e635e93
  49. https://medium.com/p/807fa6ca7f/share/twitter
  50. https://medium.com/p/807fa6ca7f/share/facebook
  51. https://medium.com/p/807fa6ca7f/share/twitter
  52. https://medium.com/p/807fa6ca7f/share/facebook
