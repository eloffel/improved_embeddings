   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id119 algorithm and its variants

   [16]go to the profile of imad dabbura
   [17]imad dabbura (button) blockedunblock (button) followfollowing
   dec 21, 2017
   [1*70f9pb-rwfaakqd6lfp4iw.png]
   figure 1: trajectory towards local minimum

   optimization refers to the task of minimizing/maximizing an objective
   function f(x) parameterized by x. in machine/deep learning terminology,
   it   s the task of minimizing the cost/id168 j(w) parameterized
   by the model   s parameters w     r^d. optimization algorithms (in case of
   minimization) have one of the following goals:
     * find the global minimum of the objective function. this is feasible
       if the objective function is convex, i.e. any local minimum is a
       global minimum.
     * find the lowest possible value of the objective function within its
       neighborhood. that   s usually the case if the objective function is
       not convex as the case in most deep learning problems.

   there are three kinds of optimization algorithms:
     * optimization algorithm that is not iterative and simply solves for
       one point.
     * optimization algorithm that is iterative in nature and converges to
       acceptable solution regardless of the parameters initialization
       such as id119 applied to id28.
     * optimization algorithm that is iterative in nature and applied to a
       set of problems that have non-convex cost functions such as neural
       networks. therefore, parameters    initialization plays a critical
       role in speeding up convergence and achieving lower error rates.

   id119 is the most common optimization algorithm in machine
   learning and deep learning. it is a first-order optimization algorithm.
   this means it only takes into account the first derivative when
   performing the updates on the parameters. on each iteration, we update
   the parameters in the opposite direction of the gradient of the
   objective function j(w) w.r.t the parameters where the gradient gives
   the direction of the steepest ascent. the size of the step we take on
   each iteration to reach the local minimum is determined by the learning
   rate   . therefore, we follow the direction of the slope downhill until
   we reach a local minimum.

   in this article, we   ll cover id119 algorithm and its
   variants: batch id119, mini-batch id119, and
   stochastic id119.

   let   s first see how id119 works on id28
   before going into the details of its variants. for the sake of
   simplicity, let   s assume that the id28 model has only
   two parameters: weight w and bias b.

   1. initialize weight w and bias b to any random numbers.

   2. pick a value for the learning rate   . the learning rate determines
   how big the step would be on each iteration.
     * if    is very small, it would take long time to converge and become
       computationally expensive.
     * if    is large, it may fail to converge and overshoot the minimum.

   therefore, plot the cost function against different values of    and
   pick the value of    that is right before the first value that didn   t
   converge so that we would have a very fast learning algorithm that
   converges (see figure 2).
   [1*rcmvcjqvsxrji8y4hpgccw.png]
   figure 2: id119 with different learning rates. [18]source
     * the most commonly used rates are : 0.001, 0.003, 0.01, 0.03, 0.1,
       0.3.

   3. make sure to scale the data if it   s on a very different scales. if
   we don   t scale the data, the level curves (contours) would be narrower
   and taller which means it would take longer time to converge (see
   figure 3).
   [1*vxpodxsx-nslmspoelhovg.png]
   figure 3: id119: normalized versus unnormalized
   level curves.

   scale the data to have    = 0 and    = 1. below is the formula for
   scaling each example:
   [1*2g6dhidpigweuafynhl8iw.png]

   4. on each iteration, take the partial derivative of the cost function
   j(w) w.r.t each parameter (gradient):
   [1*wmufvqbcefdnko2usl_o7a.png]

   the update equations are:
   [1*vdtl0p6ongccm0agdpur_g.png]
     * for the sake of illustration, let   s assume we don   t have bias. if
       the slope of the current value of w > 0, this means that we are to
       the right of optimal w*. therefore, the update will be negative,
       and will start getting close to the optimal values of w*. however,
       if it   s negative, the update will be positive and will increase the
       current values of w to converge to the optimal values of w*(see
       figure 4):

   [1*jnye54ftvoh1203iwyeneg.png]
   figure 4: id119. an illustration of how id119
   algorithm uses the first derivative of the id168 to follow
   downhill it   s minimum.
     * continue the process until the cost function converges. that is,
       until the error curve becomes flat and doesn   t change.
     * in addition, on each iteration, the step would be in the direction
       that gives the maximum change since it   s perpendicular to level
       curves at each step.

   now let   s discuss the three variants of id119 algorithm. the
   main difference between them is the amount of data we use when
   computing the gradients for each learning step. the trade-off between
   them is the accuracy of the gradient versus the time complexity to
   perform each parameter   s update (learning step).

batch id119

   batch id119 is when we sum up over all examples on each
   iteration when performing the updates to the parameters. therefore, for
   each update, we have to sum over all examples:
   [1*nu5id-pd3bncl1kktbxp4g.png]
for i in range(num_epochs):
    grad = compute_gradient(data, params)
    params = params     learning_rate * grad

   the main advantages:
     * we can use fixed learning rate during training without worrying
       about learning rate decay.
     * it has straight trajectory towards the minimum and it is guaranteed
       to converge in theory to the global minimum if the id168 is
       convex and to a local minimum if the id168 is not convex.
     * it has unbiased estimate of gradients. the more the examples, the
       lower the standard error.

   the main disadvantages:
     * even though we can use vectorized implementation, it may still be
       slow to go over all examples especially when we have large
       datasets.
     * each step of learning happens after going over all examples where
       some examples may be redundant and don   t contribute much to the
       update.

mini-batch id119

   instead of going over all examples, mini-batch id119 sums up
   over lower number of examples based on the batch size. therefore,
   learning happens on each mini-batch of b examples:
   [1*9alsfxka6iyqr7qqloqsqq.png]
     * shuffle the training data set to avoid pre-existing order of
       examples.
     * partition the training data set into b mini-batches based on the
       batch size. if the training set size is not divisible by batch
       size, the remaining will be its own batch.

for i in range(num_epochs):
    np.random.shuffle(data)
    for batch in radom_minibatches(data, batch_size=32):
        grad = compute_gradient(batch, params)
        params = params     learning_rate * grad

   the batch size is something we can tune. it is usually chosen as power
   of 2 such as 32, 64, 128, 256, 512, etc. the reason behind it is
   because some hardware such as gpus achieve better run time with common
   batch sizes such as power of 2.

   the main advantages:
     * faster than batch version because it goes through a lot less
       examples than batch (all examples).
     * randomly selecting examples will help avoid redundant examples or
       examples that are very similar that don   t contribute much to the
       learning.
     * with batch size < size of training set, it adds noise to the
       learning process that helps improving generalization error.
     * even though with more examples the estimate would have lower
       standard error, the return is less than linear compared to the
       computational burden we incur.

   the main disadvantages:
     * it won   t converge. on each iteration, the learning step may go back
       and forth due to the noise. therefore, it wanders around the
       minimum region but never converges.
     * due to the noise, the learning steps have more oscillations (see
       figure 4) and requires adding learning-decay to decrease the
       learning rate as we become closer to the minimum.

   [1*5mhkzw3fpur2hbnflrxz-a.png]
   figure 5: id119: batch versus mini-batch id168

   with large training datasets, we don   t usually need more than 2   10
   passes over all training examples (epochs). note: with batch size b = m
   (number of training examples), we get the batch id119.

stochastic id119

   instead of going through all examples, stochastic id119
   (sgd) performs the parameters update on each example (x^i,y^i).
   therefore, learning happens on every example:
   [1*ecof_ywcdje9gfcezkzlpw.png]
     * shuffle the training data set to avoid pre-existing order of
       examples.
     * partition the training data set into m examples.

for i in range(num_epochs):
    np.random.shuffle(data)
    for example in data:
        grad = compute_gradient(example, params)
        params = params     learning_rate * grad

   it shares most of the advantages and the disadvantages with mini-batch
   version. below are the ones that are specific to sgd:
     * it adds even more noise to the learning process than mini-batch
       that helps improving generalization error. however, this would
       increase the run time.
     * we can   t utilize vectorization over 1 example and becomes very
       slow. also, the variance becomes large since we only use 1 example
       for each learning step.

   below is a graph that shows the id119   s variants and their
   direction towards the minimum:
   [1*pv-fcusnld9egtic61h-ig.png]
   figure 6: id119 variants    trajectory towards minimum

   as the figure above shows, sgd direction is very noisy compared to
   mini-batch.

challenges

   below are some challenges regarding id119 algorithm in
   general as well as its variants         mainly batch and mini-batch:
     * id119 is a first-order optimization algorithm, which
       means it doesn   t take into account the second derivatives of the
       cost function. however, the curvature of the function affects the
       size of each learning step. the gradient measures the steepness of
       the curve but the second derivative measures the curvature of the
       curve. therefore, if:

    1. second derivative = 0    the curvature is linear. therefore, the step
       size = the learning rate   .
    2. second derivative > 0     the curvature is going upward. therefore,
       the step size < the learning rate    and may lead to divergence.
    3. second derivative < 0     the curvature is going downward. therefore,
       the step size > the learning rate   .

   as a result, the direction that looks promising to the gradient may not
   be so and may lead to slow the learning process or even diverge.
     * if hessian matrix has poor conditioning number, i.e. the direction
       of the most curvature has much more curvature than the direction of
       the lowest curvature. this will lead the cost function to be very
       sensitive in some directions and insensitive in other directions.
       as a result, it will make it harder on the gradient because the
       direction that looks promising for the gradient may not lead to big
       changes in the cost function (see figure 7).

   [1*xoazp424rr74lzyxpxxk0q.png]
   figure 7: id119 fails to exploit the curvature information
   contained in the hessian matrix. [19]source
     * the norm of the gradient gtg is supposed to decrease slowly with
       each learning step because the curve is getting flatter and
       steepness of the curve will decrease. however, we see that the norm
       of the gradient is increasing, because of the curvature of the
       curve. nonetheless, even though the gradients    norm is increasing,
       we   re able to achieve a very low error rates (see figure 8).

   [1*8_u3k1hsxlvehuovkbpg4a.png]
   figure 8: gradient norm. [20]source
     * in small dimensions, local minimum is common; however, in large
       dimensions, saddle points are more common. saddle point is when the
       function curves up in some directions and curves down in other
       directions. in other words, saddle point looks like a minimum from
       one direction and a maximum from other direction (see figure 9).
       this happens when at least one eigenvalue of the hessian matrix is
       negative and the rest of eigenvalues are positive.

   [1*6h_q_bc_epum6fhoudpgew.png]
   figure 9: saddle point
     * as discussed previously, choosing a proper learning rate is hard.
       also, for mini-batch id119, we have to adjust the
       learning rate during the training process to make sure it converges
       to the local minimum and not wander around it. figuring out the
       decay rate of the learning rate is also hard and changes with
       different data sets.
     * all parameter updates have the same learning rate; however, we may
       want to perform larger updates to some parameters that have their
       directional derivatives more inline with the trajectory towards the
       minimum than other parameters.

   originally published at [21]imaddabbura.github.io on december 21, 2017.

     * [22]artificial intelligence
     * [23]deep learning
     * [24]machine learning
     * [25]data science

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of imad dabbura

[27]imad dabbura

   data scientist

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/10f652806a3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_tgybmhjzk0nj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@imadphd?source=post_header_lockup
  17. https://towardsdatascience.com/@imadphd
  18. http://cs231n.github.io/neural-networks-3/
  19. http://www.deeplearningbook.org/contents/numerical.html
  20. http://www.deeplearningbook.org/contents/optimization.html
  21. https://imaddabbura.github.io/post/gradient_descent_algorithms/
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/tagged/deep-learning?source=post
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/@imadphd?source=footer_card
  27. https://towardsdatascience.com/@imadphd
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/10f652806a3/share/twitter
  34. https://medium.com/p/10f652806a3/share/facebook
  35. https://medium.com/p/10f652806a3/share/twitter
  36. https://medium.com/p/10f652806a3/share/facebook
