   #[1]perception, control, cognition    feed [2]perception, control,
   cognition    comments feed [3]perception, control, cognition    practical
   advice for building deep neural networks comments feed [4]what can you
   do with a rock? [5]eve faq [6]alternate [7]alternate [8]perception,
   control, cognition [9]wordpress.com

   [10]skip to content

[11]perception, control, cognition

combining deep neural networks with bayesian models to advance the potential
of ai

   (button) menu
     * [12]about
     * [13]contact
     * [14]publications
     * [15]people
     * [16]in the news
     * [17]blog

practical advice for building deep neural networks

   posted on [18]october 2, 2017october 10, 2017 by [19]matt h and daniel
   r

   in our machine learning lab, we   ve accumulated tens of thousands of
   training hours across numerous high-powered machines. the computers
   weren   t the only ones to learn a lot in the process, though: we
   ourselves have made a lot of mistakes and fixed a lot of bugs.

   here we present some practical tips for training deep neural networks
   based on our experiences (rooted mainly in tensorflow). some of the
   suggestions may seem obvious to you, but they weren   t to one of us at
   some point. other suggestions may not apply or might even be bad advice
   for your particular task: use discretion!

   we acknowledge these are all well-known methods. we, too, stand on the
   shoulders of giants here! our objective with this article is simply to
   summarize them at a high level for use in practice.

general tips

     * use the adam optimizer. it works really well. prefer it to more
       traditional optimizers such as vanilla id119. tensorflow
       note: if saving and restoring weights, remember to set up the saver
       after setting up the adamoptimizer, because adam has state (namely
       per-weight learning rates) that need to be restored as well.

     * relu is the best nonlinearity (activation function). kind of like
       how sublime is the best text editor. but really, relus are fast,
       simple, and, amazingly, they work, without diminishing gradients
       along the way. while sigmoid is a common textbook activation
       function, it does not propagate gradients well through dnns.

     * do not use an activation function at your output layer. this should
       be obvious, but it is an easy mistake to make if you build each
       layer with a shared function: be sure to turn off the activation
       function at the output.

     * do add a bias in every layer. this is ml 101: a bias essentially
       translates a plane into a best-fitting position. in y=mx+b, b is
       the bias, allowing the line to move up or down into the    best fit   
       position.

     * use variance-scaled initialization. in tensorflow, this looks like
       tf.contrib.layers.variance_scaling_initializer(). in our
       experience, this generalizes/scales better than regular gaussian,
       truncated normal, and xavier. roughly speaking, the variance
       scaling initializer adjusts the variance the initial random weights
       based on the number of inputs or outputs at each layer (default in
       tensorflow is number of inputs), thus helping signals to propagate
       deeper into the network without extra    hacks    like clipping or
       batch id172. xavier is similar, except that the variance is
       nearly the same in all layers; but networks with layers that vary
       greatly in their shapes (common with convolutional networks) may
       not cope as well with the same variance in each layer.

     * whiten (normalize) your input data. for training, subtract the mean
       of the data set, then divide by its standard deviation. the less
       your weights have to be stretched and pulled in every which
       direction, the faster and more easily your network will learn.
       keeping the input data mean-centered with constant variance will
       help with this. you   ll have to perform the same id172 to
       each test input as well, so make sure your training set resembles
       real data.

     * scale input data in a way that reasonably preserves its dynamic
       range. this is related to id172 but should happen before
       normalizing. for example, data x with an actual real-world range of
       [0, 140000000] can often be tamed with tanh(x) or tanh(x/c) where c
       is some constant that stretches the curve to fit more of the input
       range within the dynamic, sloping part of the tanh function.
       especially in cases where your input data may be unbounded on one
       or both ends, the neural net will learn much better between (0,1).

     * don   t bother decaying the learning rate (usually). learning rate
       decay was more common with sgd, but adam takes care of this
       naturally. if you absolutely want to squeeze out every ounce of
       performance: decay the learning rate for a short time at the end of
       training; you   ll probably see a sudden, very small drop in error,
       then it will flatten out again.

     * if your convolution layer has 64 or 128 filters, that   s probably
       plenty. especially for a deep network. like, really, 128 is a lot.
       if you already have a high number of filters, adding more probably
       won   t improve things.

     * pooling is for transform invariance. pooling essentially lets the
       network learn    the general idea    of    that part    of an image. max
       pooling, for example, can help a convolutional network become
       robust against translation, rotation, and scaling of features in
       the image.

debugging a neural network

   if your network isn   t learning (meaning: the loss/accuracy is not
   converging during training, or you   re not getting results you expect),
   try these tips:
     * overfit! the first thing to do if your network isn   t learning is to
       overfit a training point. accuracy should be essentially 100% or
       99.99%, or an error as close to 0. if your neural network can   t
       overfit a single data point, something is seriously wrong with the
       architecture, but it may be subtle. if you can overfit one data
       point but training on a larger set still does not converge, try the
       following suggestions.

     * lower your learning rate. your network will learn slower, but it
       may find its way into a minimum that it couldn   t get into before
       because its step size was too big. (intuitively, think of stepping
       over a ditch on the side of the road, when you actually want to get
       into the lowest part of the ditch, where your error is the lowest.)

     * raise your learning rate. this will speed up training which helps
       tighten the feedback loop, meaning you   ll have an inkling sooner
       whether your network is working. while the network should converge
       sooner, its results probably won   t be great, and the    convergence   
       might actually jump around a lot. (with adam, we found ~0.001 to be
       pretty good in many experiences.)

     * decrease (mini-)batch size. reducing a batch size to 1 can give you
       more granular feedback related to the weight updates, which you
       should report with tensorboard (or some other
       debugging/visualization tool).

     * remove batch id172. along with decreasing batch size to 1,
       doing this can expose diminishing or exploding gradients. for weeks
       we had a network that wasn   t converging, and only when we removed
       batch id172 did we realize that the outputs were all nan by
       the second iteration. batch norm was putting a band-aid on
       something that needed a tourniquet. it has its place, but only
       after you know your network is bug-free.

     * increase (mini-)batch size. a larger batch size   heck, the whole
       training set if you could   reduces variance in gradient updates,
       making each iteration more accurate. in other words, weight updates
       will be in the right direction. but! there   s an effective upper
       bound on its usefulness, as well as physical memory limits.
       typically, we find this less useful than the previous two
       suggestions to reduce batch size to 1 and remove batch norm.

     * check your reshaping. drastic reshaping (like changing an image   s
       x,y dimensions) can destroy spatial locality, making it harder for
       a network to learn since it must also learn the reshape. (natural
       features become fragmented. the fact that natural features appear
       spatially local is why conv nets are so effective!) be especially
       careful if reshaping with multiple images/channels; use
       numpy.stack() for proper alignment.

     * scrutinize your id168. if using a complex function, try
       simplifying it to something like l1 or l2. we   ve found l1 to be
       less sensitive to outliers, making less drastic adjustments when
       hitting a noisy batch or training point.

     * scrutinize your visualizations, if applicable. is your viz library
       (matplotlib, opencv, etc.) adjusting the scale of the values, or
       clipping them? consider using a perceptually-uniform color scheme
       as well.

an example case study

   to help make the process described above more relatable, here are a few
   loss charts (via tensorboard) for some actual regression experiments of
   a convolutional neural network that we built.

   at first, the network was not learning at all:

   longtrain_notlearning

   we tried clipping the values, to prevent them from going out of bounds:

   trainloss_batchnorm_hiding_clip_attempt.png

   huh. look at how crazy the un-smoothed values are. learning rate too
   high? we tried decaying the learning rate and training on just one
   input:

   not_overfitting.png

   you can see where the first few changes to the learning rate occurred
   (at about steps 300 and 3000). obviously, we decayed too quickly. so,
   giving it more time between decays, it did better:

   overfit-decay-lr.png

   you can see we decayed at steps 2000 and 5000. this was better, but
   still not great, because it didn   t go to 0.

   then we disabled lr decay and tried moving the values into a narrower
   range instead by putting the inputs through a tanh. while this
   obviously brought the error values below 1, we still couldn   t overfit
   the training set:

   batchnorm_hiding.png

   this is where we discovered, by removing batch id172, that the
   network was quickly outputting nan after one or two iterations. we left
   batch norm disabled and changed our initialization to variance scaling.
   these made all the difference! we were able to overfit our test set of
   just one or two inputs. while the chart on the bottom clips the y axis,
   the initial error value was well above 5, showing a reduction in error
   by almost 4 orders of magnitude:

   initial_training_lr_001.png

   the top chart is heavily smoothed, but you can see that it overfit the
   test input extremely quickly, and the loss of the whole training set
   marched down below 0.01 over time. this was without decaying the
   learning rate. we then continued training after dropping the learning
   rate by one order of magnitude, and got even better results:

   training_continued.png

   these results were much better! but what if we decayed the learning
   rate geometrically rather than splitting training into two parts?

   by multiplying the learning rate by 0.9995 at each step, the results
   were not as good:

   tanhnorm-lrgeo9995_decay_tooquickly.png

       presumably because the decay was too quick. a multiplier of 0.999995
   did better, but the results were nearly equivalent to not decaying it
   at all. we concluded from this particular sequence of experiments that
   batch id172 was hiding exploding gradients caused by poor
   initialization, and that decaying the learning rate was not
   particularly helpful with the adam optimizer, except perhaps one
   deliberate decay at the end. along with batch norm, clipping the values
   was just masking the real problem. we also tamed our high-variance
   input values by putting them through a tanh.

   we hope you will find these basic tips useful as you become more
   familiar with building deep neural networks. often, it   s just simple
   things that can make all the difference.

share this:

     * [20]twitter
     * [21]facebook
     *

like this:

   like loading...

related

   posted in [22]blog

post navigation

   [23]what can you do with a rock?
   [24]eve faq
     __________________________________________________________________

one thought on    practical advice for building deep neural networks   

    1. pingback: [25]                                                                              -ai                     

   comments are closed.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [26]cancel reblog post

   iframe: [27]likes-master

   %d bloggers like this:

references

   visible links
   1. https://pcc.cs.byu.edu/feed/
   2. https://pcc.cs.byu.edu/comments/feed/
   3. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/feed/
   4. https://pcc.cs.byu.edu/2017/09/23/what-can-you-do-with-a-rock/
   5. https://pcc.cs.byu.edu/2018/02/22/eve-faq/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/&for=wpcom-auto-discovery
   8. https://pcc.cs.byu.edu/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/#content
  11. https://pcc.cs.byu.edu/
  12. https://pcc.cs.byu.edu/about/
  13. https://pcc.cs.byu.edu/contact/
  14. https://pcc.cs.byu.edu/publications/
  15. https://pcc.cs.byu.edu/testimonial/
  16. https://pcc.cs.byu.edu/category/in-the-news/
  17. https://pcc.cs.byu.edu/category/blog/
  18. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/
  19. https://pcc.cs.byu.edu/author/pcclmatt/
  20. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/?share=twitter
  21. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/?share=facebook
  22. https://pcc.cs.byu.edu/category/blog/
  23. https://pcc.cs.byu.edu/2017/09/23/what-can-you-do-with-a-rock/
  24. https://pcc.cs.byu.edu/2018/02/22/eve-faq/
  25. https://www.aigyw.com/2018/07/31/8223.html
  26. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/
  27. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  29. https://pcc.cs.byu.edu/
  30. https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/
