   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

the matrix calculus you need for deep learning (notes from a paper by
[9]terence parr and [10]jeremy howard)

   [11]go to the profile of rohit patil
   [12]rohit patil (button) blockedunblock (button) followfollowing
   feb 27, 2018
   [1*l_zfm3-vjmbvczachwcx-q.jpeg]

table of contents

     * [13]review: scalar derivative rules
     * [14]introduction to vector calculus and partial derivatives
     * [15]matrix calculus
     * [16]generalization of the jacobian
     * [17]derivatives of vector element-wise binary operators
     * [18]vector sum reduction
     * [19]the chain rules
     * [20]resources

   jeremy   s courses show how to become a world-class deep learning
   practitioner with only a minimal level of scalar calculus, thanks to
   leveraging the automatic differentiation built in to modern deep
   learning libraries. but if you really want to really understand what   s
   going on under the hood of these libraries, and grok academic papers
   discussing the latest advances in model training techniques, you   ll
   need to understand certain bits of the field of matrix calculus.

review: scalar derivative rules

   hopefully you remember some of these main scalar derivative rules. if
   your memory is a bit fuzzy on this, have a look at [21]khan academy
   video on scalar derivative rules.
   [1*zr50k2cdpl1um4s-aoewqw.png]
   basic rules of derivatives

   there are other rules for trigonometry, exponential, etc., which you
   can find at [22]khan academy id128 course.

introduction to vector calculus and partial derivatives

   neural network layers are not single functions of a single parameter,
   f(x). so, let   s move on to functions of multiple parameters such as
   f(x,y). for example, what is the derivative of xy (i.e., the
   multiplication of x and y)?

   well, it depends on whether we are changing x or y. we compute
   derivatives with respect to one variable (parameter) at a time, giving
   us two different partial derivatives for this two-parameter function
   (one for x and one for y). instead of using operator d/dx, the partial
   derivative operator is    /    x (a stylized d and not the greek letter   
   ). so    (xy)/    x and    (xy)/    y are the partial derivatives of xy; often,
   these are just called the partials.

   the partial derivative with respect to x is just the usual scalar
   derivative, simply treating any other variable in the equation as a
   constant. consider function f(x,y) = 3x  y. the partial derivative with
   respect to x is written    (3x  y)/    x. there are three constants from the
   perspective of    /    x: 3, 2, and y. therefore,    (3x  y)/    x = 3y   (x  )/    x
   = 3y(2x) = 6xy. the partial derivative with respect to y treats x like
   a constant and we get    (3x  y)/    y = 3x  . you can learn more on [23]khan
   academy video on partials.

   so from above example if f(x,y) = 3x  y, then
   [1*8a9yl0fqmo70v-juo5c3bg.png]
   gradient of f(x,y)

   so the gradient of f(x,y) is simply a vector of its partial.

matrix calculus

   when we move from derivatives of one function to derivatives of many
   functions, we move from the world of vector calculus to matrix
   calculus. let us bring one more function g(x,y) = 2x + y   . so gradient
   of g(x,y) is
   [1*iuponl8zni_fpcdkcfp3bq.png]

   gradient vectors organize all of the partial derivatives for a specific
   scalar function. if we have two functions, we can also organize their
   gradients into a matrix by stacking the gradients. when we do so, we
   get the jacobian matrix (or just the jacobian) where the gradients are
   rows:
   [1*7xkv9d7qxx44gqqbvylbga.png]
   [24]numerator layout of jacobian

generalization of the jacobian

   to define the jacobian matrix more generally, let   s combine multiple
   parameters into a single vector argument: f(x,y,z) => f(x). lowercase
   letters in bold font such as x are vectors and those in italics font
   like x are scalars. xi is the ith element of vector x and is in italics
   because a single vector element is a scalar. we also have to define an
   orientation for vector x. we   ll assume that all vectors are vertical by
   default of size n x 1:
   [1*tnld_rdmvqhgizrlp3jd-q.png]

   with multiple scalar-valued functions, we can combine them all into a
   vector just like we did with the parameters. let y = f(x) be a vector
   of m scalar-valued functions that each take a vector x of length n= |x|
   where |x| is the cardinality (count) of elements in x. each fi function
   within f returns a scalar just as in the previous section
   [1*wyqevfkocbibjzjs1e3moa.png]

     generally speaking, though, the jacobian matrix is the collection of
     all m x n possible partial derivatives (m rows and n columns), which
     is the stack of m gradients with respect to x:

   [1*f75-0xoigirn-kkl2d_veg.png]

derivatives of vector element-wise binary operators

   by    element-wise binary operations    we simply mean applying an operator
   to the first item of each vector to get the first item of the output,
   then to the second items of the inputs for the second item of the
   output, and so forth.we can generalize the element-wise binary
   operations with notation y= f(w) o g(x) where m = n = |y| = |w| = |x|.
   the o symbol represents any element-wise operator (such as +) and not
   the o function composition operator.
   [1*6svhgcqcijq6aeyuoc_yzw.png]

   that   s quite a furball, but fortunately the jacobian is very often a
   diagonal matrix, a matrix that is zero everywhere but the diagonal.

vector sum reduction

   summing up the elements of a vector is an important operation in deep
   learning, such as the network id168, but we can also use it as
   a way to simplify computing the derivative of vector dot product and
   other operations that reduce vectors to scalars.

   let y=sum(f(x)) =    fi(x). notice we were careful here to leave the
   parameter as a vector x because each function fi could use all values
   in the vector, not just xi. the sum is over the results of the function
   and not the parameter. the gradient ( 1 x n jacobian) of vector
   summation is:
   [1*-mehl1gpbjiijxyla9c3wq.png]

the chain rules

   we can   t compute partial derivatives of very complicated functions
   using just the basic matrix calculus rules. part of our goal here is to
   clearly define and name three different chain rules and indicate in
   which situation they are appropriate.

   the chain rule is conceptually a divide and conquer strategy (like
   quicksort) that breaks complicated expressions into sub-expressions
   whose derivatives are easier to compute. its power derives from the
   fact that we can process each simple sub-expression in isolation yet
   still combine the intermediate results to get the correct overall
   result.

   the chain rule comes into play when we need the derivative of an
   expression composed of nested subexpressions. for example, we need the
   chain rule when confronted with expressions like d(sin(x  ))/dx.
     * [25]single-variable chain rule :- chain rules are typically defined
       in terms of nested functions, such as y= f(u) where u= g(x) so y=
       f(g(x)) for single-variable chain rules.

   [1*u3k7gs3vapexukvkwmydzg.png]
   formulation of the single-variable chain rule

   to deploy the single-variable chain rule, follow these steps:
    1. introduce intermediate variables for nested sub-expressions and
       sub-expressions for both binary and unary operators; example, x is
       binary, sin (x) and other trigonometric functions are usually unary
       because there is a single operand. this step normalizes all
       equations to single operators or function applications.
    2. compute derivatives of the intermediate variables with respect to
       their parameters.
    3. combine all derivatives of intermediate variables by multiplying
       them together to get the overall result.
    4. substitute intermediate variables back in if any are referenced in
       the derivative equation.

     * single-variable total-derivative chain rule :- the total derivative
       assumes all variables are potentially codependent whereas the
       partial derivative assumes all variables but x are constants.

   [1*nxfy9u1eh7_sklznwc3u-w.png]

   this chain rule that takes into consideration the total derivative
   degenerates to the single-variable chain rule when all intermediate
   variables are functions of a single variable.

     a word of caution about terminology on the web. unfortunately, the
     chain rule given in this section, based upon the total derivative,
     is universally called    multivariable chain rule    in calculus
     discussions, which is highly misleading! only the intermediate
     variables are multivariate functions.

     * vector chain rule :- vector chain rule for vectors of functions and
       a single parameter mirrors the single-variable chain rule.

   if y= f(g(x)) and x is a vector. the derivative of vector y with
   respect to scalar x is a vertical vector with elements computed using
   the single-variable total-derivative chain rule.
   [1*jtptluhqtbvk_0be5ripxq.png]

   the goal is to convert the above vector of scalar operations to a
   vector operation. so the above rhs matrix can also be implemented as a
   product of vector multiplication.
   [1*airi5dtfm_x_ybgl0uqaya.png]

   that means that the jacobian is the multiplication of two other
   jacobians. to make this formula work for multiple parameters or vector
   x, we just have to change x to vector x in the equation. the effect is
   that    g/    x and the resulting jacobian,    f/    x , are now matrices
   instead of vertical vectors. our complete vector chain rule is:
   [1*sgjplnnyp7leos-lnmrnfg.png]

     please note here that matrix multiply does not commute, the order of
     (   f/    x)(   g/    x) matters.

   for completeness, here are the two jacobian components :-
   [1*g5ph9kqoynrpvyfr1nyyta.png]

   where m = |f|, n = |x| and k = |g|. the resulting jacobian is m x n.
   (an m x k matrix multiplied by a k x n matrix).

   we can simplify further because, for many applications, the jacobians
   are square (m = n) and the off-diagonal entries are zero.

resources

   1.the original [26]paper.

   2. there are some online tools which can differentiate a matrix for
   you:
     * [27]wolfram alpha
     * [28]matrix calculus differentiator.

   3. more [29]matrix calculus.
   [1*xhcj5o5sya_8vbuw_mr_xg.jpeg]

     * [30]machine learning
     * [31]mathematics
     * [32]calculus
     * [33]deep learning
     * [34]differentiation

   (button)
   (button)
   (button) 458 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of rohit patil

[36]rohit patil

   machine learning and software engineer. currently working on
   programming self-driving cars.

     * (button)
       (button) 458
     * (button)
     *
     *

   [37]go to the profile of rohit patil
   never miss a story from rohit patil, when you sign up for medium.
   [38]learn more
   never miss a story from rohit patil
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4f4263b7bb8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@rohitrpatil/the-matrix-calculus-you-need-for-deep-learning-notes-from-a-paper-by-terence-parr-and-jeremy-4f4263b7bb8&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@rohitrpatil/the-matrix-calculus-you-need-for-deep-learning-notes-from-a-paper-by-terence-parr-and-jeremy-4f4263b7bb8&source=--------------------------nav_reg&operation=register
   9. http://parrt.cs.usfca.edu/
  10. http://www.fast.ai/about/#jeremy
  11. https://medium.com/@rohitrpatil?source=post_header_lockup
  12. https://medium.com/@rohitrpatil
  13. https://medium.com/p/4f4263b7bb8#48c0
  14. https://medium.com/p/4f4263b7bb8#d5f1
  15. https://medium.com/p/4f4263b7bb8#3c66
  16. https://medium.com/p/4f4263b7bb8#efec
  17. https://medium.com/p/4f4263b7bb8#59c5
  18. https://medium.com/p/4f4263b7bb8#641f
  19. https://medium.com/p/4f4263b7bb8#ada3
  20. https://medium.com/p/4f4263b7bb8#3788
  21. https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules
  22. https://www.khanacademy.org/math/differential-calculus
  23. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives
  24. https://en.wikipedia.org/wiki/matrix_calculus#layout_conventions
  25. http://m.wolframalpha.com/input/?i=chain+rule
  26. http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html#sec3
  27. http://www.wolframalpha.com/input/?i=d[{x^2,+x^3}.{{1,2},{3,4}}.{x^2,+x^3},+x]
  28. http://www.matrixcalculus.org/
  29. https://atmos.washington.edu/~dennis/matrixcalculus.pdf
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/mathematics?source=post
  32. https://medium.com/tag/calculus?source=post
  33. https://medium.com/tag/deep-learning?source=post
  34. https://medium.com/tag/differentiation?source=post
  35. https://medium.com/@rohitrpatil?source=footer_card
  36. https://medium.com/@rohitrpatil
  37. https://medium.com/@rohitrpatil
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/p/4f4263b7bb8/share/twitter
  41. https://medium.com/p/4f4263b7bb8/share/facebook
  42. https://medium.com/p/4f4263b7bb8/share/twitter
  43. https://medium.com/p/4f4263b7bb8/share/facebook
