   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   exploring recurrent neural networks comments feed [5]a general approach
   to preprocessing text data [6]drexel university: teaching faculty
   positions in data science, computer science, and computing security &
   technology

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]dec    [29]tutorials,
   overviews    exploring recurrent neural networks ( [30]17:n46 )

exploring recurrent neural networks

   [31][prv.gif] previous post
   [32]next post [nxt.gif]


   tags: [33]neural networks, [34]packt publishing, [35]python,
   [36]recurrent neural networks, [37]tensorflow

   we explore recurrent neural networks, starting with the basics, using a
   motivating weather modeling problem, and implement and train an id56 in
   tensorflow.
     __________________________________________________________________

                                                            c [38]comments

   by packtpub.

   in this tutorial, taken from [39]hands-on deep learning with theano by
   dan van boxel, we   ll be exploring recurrent neural networks. we   ll
   start off by looking at the basics, before looking at id56s through a
   motivating weather modeling problem. we   ll also implement and train an
   id56 in tensorflow.

   in a typical model, you have some x input features and some y output
   you want to predict. we usually consider our different training samples
   as independent observations. so, the features from data point one
   shouldn't impact the prediction for data point two. but what if our
   data points are correlated? the most common example is that each data
   point, xt, represents features collected at time t. it's natural to
   suppose that the features at time t and time t+1 will both be important
   to the prediction at time t+1. in other words, history matters.

   now, when modeling, you could just include twice as many input
   features, adding the previous time step to the current ones, and
   computing twice as many input weights. but, if you're going through all
   the effort of building a neural network to compute transform features,
   it would be nice if you could use the intermediate features from the
   previous time step, in the current time step network.

   id56s do exactly this. consider your input, xt as usual, but add in some
   state, st-1 that comes from the previous time step, as additional
   features. now you can compute weights as usual to predict yt, and you
   produce a new internal state, st, to be used in the next time step. for
   the first time step, it's typical to use a default or zero initial
   state. classic id56s are literally this simple, but there are more
   advanced structures common in literature today, such as gated recurrent
   units and long short-term memory circuits. these are beyond the scope
   of this tutorial, but work on the same principles and generally apply
   to the same types of problems.

modeling the weights

   you might be wondering how we'll compute weights with all these
   dependents on the previous time step. computing the gradients does
   involve recursing back through the time computation, but fear not,
   tensorflow handles the tedious stuff and let's us do the modeling:
# read in data
filename = 'weather.npz'
data = np.load(filename)
daily = data['daily']
weekly = data['weekly']

num_weeks = len(weekly)
dates = np.array([datetime.datetime.strptime(str(int(d)),
        '%y%m%d') for d in weekly[:,0]])

   to use id56s, we need a data modeling problem with a time component.

   the font classification problem isn't really appropriate here. so,
   let's take a look at some weather data. the weather.npz file is a
   collection of weather station data from a city in the united states
   over several decades. the daily array contains measurements from every
   day of the year. there are six columns to the data, starting with the
   date. next, is the precipitation, measuring any rainfall in inches that
   day. after this, come two columns for snow   the first is measured snow
   currently on the ground, while the latter is snowfall on that day,
   again, in inches. finally, we have some temperature information, the
   daily high and the daily low in degrees fahrenheit.

   the weekly array, which we'll use, is a weekly summary of the daily
   information. we'll use the middle date to indicate the week, then,
   we'll sum up all rainfall for the week. for snow, however, we'll
   average the snow on the ground, since it doesn't make sense to add snow
   from one cold day to the same snow sitting on the ground the next day.
   snowfall though, we'll total for the week, just like rain. finally,
   we'll average the high and low temperatures for the week respectively.
   now that you've got a handle on the dataset, what shall we do with it?
   one interesting time-based modeling problem would be trying to predict
   the season of a particular week using it's weather information and the
   history of previous weeks.

   in the northern hemisphere, in the united states, it's warmer during
   the months of june through august and colder during december through
   february, with transitions in between. spring months tend to be rainy,
   and winter often includes snow. while one week can be highly variable,
   a history of weeks should provide some predictive power.

understanding id56s

   first, let's read in the data from a compressed numpy array. the
   weather.npz file happens to include the daily data as well, if you wish
   to explore your own model; np.load reads both arrays into a dictionary
   and will set weekly to be our data of interest; num_weeks is naturally
   how many data points we have, here, several decades worth of
   information:
num_weeks = len(weekly)

   to format the weeks, we use a python datetime.datetime object reading
   the storage string in year month day format:
dates = np.array([datetime.datetime.strptime(str(int(d)),
        '%y%m%d') for d in weekly[:,0]])


   we can use the date of each week to assign its season. for this model,
   because we're looking at weather data, we use the meteorological season
   rather than the common astronomical season. thankfully, this is easy to
   implement with the python function. grab the month from the datetime
   object and we can directly compute this season. spring, season zero, is
   march through may, summer is june through august, autumn is september
   through november, and finally, winter is december through february. the
   following is the simple function that just evaluates the month and
   implements that:
def assign_season(date):
  ''' assign season based on meteorological season.
      spring - from mar 1 to may 31
      summer - from jun 1 to aug 31
      autumn - from sep 1 to nov 30
      winter - from dec 1 to feb 28 (feb 29 in a leap year)
  '''
  month = date.month
  # spring = 0
  if 3 <= month < 6:
     season = 0
  # summer = 1
  elif 6 <= month < 9:
    season = 1
  # autumn = 2
  elif 9 <= month < 12:
    season = 2
  # winter = 3
  elif month == 12 or month < 3:
    season = 3
  return season


   let's note that we have four seasons and five input variables and, say,
   11 values in our history state:

# there are 4 seasons
num_classes = 4

# and 5 variables
num_inputs = 5

# and a state of 11 numbers
state_size = 11

   now you're ready to compute the labels:

labels = np.zeros([num_weeks,num_classes])

# read and convert to one-hot

for i,d in enumerate(dates):

labels[i,assign_season(d)] = 1


   we do this directly in one-hot format, by making an all-zeroes array
   and putting a one in the position of the assign season.

   cool! you just summarized decades of time with a few commands.

   as these input features measure very different things, namely rainfall,
   snow, and temperature, on very different scales, we should take care to
   put them all on the same scale. in the following code, we grab the
   input features, skipping the date column of course, and subtract the
   average to center all features at zero:
# extract and scale training data

train = weekly[:,1:]

train = train - np.average(train,axis=0)

train = train / train.std(axis=0)

   then, we scale each feature by dividing by its standard deviation. this
   accounts for temperatures ranging roughly 0 to 100, while rainfall only
   changes between about 0 and 10. nice work on the data prep! it isn't
   always fun, but it's a key part of machine learning and tensorflow.

   let's now jump into the tensorflow model:
# these will be inputs

x = tf.placeholder("float", [none, num_inputs])

# tf likes a funky input to id56

x_ = tf.reshape(x, [1, num_weeks, num_inputs])


   we input our data as normal with a placeholder variable, but then you
   see this strange reshaping of the entire data set into one big tensor.
   don't worry, this is because we technically have one long, unbroken
   sequence of observations. the y_ variable is just our output:

y_ = tf.placeholder("float", [none,num_classes])


   we'll be computing a id203 for every week for each season.

   the cell variable is the key to the recurrent neural network:
cell = tf.nn.id56_cell.basicid56cell(state_size)


   this tells tensorflow how the current time step depends on the
   previous. in this case, we'll use a basic id56 cell. so, we're only
   looking back one week at a time. suppose that it has state size or 11
   values. feel free to experiment with more exotic cells and different
   state sizes.

   to put that cell to use, we'll use tf.nn.dynamic_id56:
outputs, states = tf.nn.dynamic_id56(cell,x_,

dtype=tf.nn.dtypes.float32, initial_state=none)

   this intelligently handles the recursion rather than simply unrolling
   all the time steps into a giant computational graph. as we have
   thousands of observations in one sequence, this is critical to attain
   reasonable speed. after the cell, we specify our input x_, then dtype
   to use 32 bits to store decimal numbers in a float, and then the empty
   initial_state. we use the outputs from this to build a simple model.
   from this point on, the model is almost exactly as you would expect
   from any neural network:

   we'll multiply the output of the id56 cells, some weights, and add a
   bias to get a score for each class for that week:

w1 = tf.variable(tf.truncated_normal([state_size,num_classes],
        stddev=1./math.sqrt(num_inputs)))

b1 = tf.variable(tf.constant(0.1,shape=[num_classes]))

# reshape the output for traditional usage
h1 = tf.reshape(outputs,[-1,state_size])


   our categorical cross_id178 id168 and train optimizer should
   be very familiar to you:
# climb on cross-id178
cross_id178 = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(y + 1e-50
, y_))

# how we train
train_step = tf.train.gradientdescentoptimizer(0.01).minimize(cross_id178)

# define accuracy
correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
accuracy=tf.reduce_mean(tf.cast(correct_prediction, "float"))


   great work setting up the tensorflow model! to train this, we'll use a
   familiar loop:
# actually train
epochs = 100
train_acc = np.zeros(epochs//10)

for i in tqdm(range(epochs), ascii=true):
    if i % 10 == 0:

   # record summary data, and the accuracy
     # check accuracy on train set
     a = accuracy.eval(feed_dict={x: train, y_: labels})
     train_acc[i//10] = a

   train_step.run(feed_dict={x: train, y_: labels})

   since this is a fictitious problem, we'll not worry too much about how
   accurate the model really is. the goal here is just to see how an id56
   works. you can see that it runs just like any tensorflow model:

   if you do look at the accuracy, you can see that it's doing pretty
   well; much better than the 25 percent random guessing, but still has a
   lot to learn.

   we hope you enjoyed this extract from [40]hands on deep learning with
   tensorflow. if you   d like to learn more visit packtpub.com.

   related:
     * [41]a guide for time series prediction using recurrent neural
       networks (lstms)
     * [42]going deeper with recurrent networks: sequence to bag of words
       model
     * [43]7 steps to mastering deep learning with keras
     __________________________________________________________________

   [44][prv.gif] previous post
   [45]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [46]another 10 free must-read books for machine learning and data
       science
    2. [47]9 must-have skills you need to become a data scientist, updated
    3. [48]who is a typical data scientist in 2019?
    4. [49]the pareto principle for data scientists
    5. [50]what no one will tell you about data science job applications
    6. [51]19 inspiring women in ai, big data, data science, machine
       learning
    7. [52]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [53]id158s optimization using genetic algorithm
       with python
    2. [54]who is a typical data scientist in 2019?
    3. [55]8 reasons why you should get a microsoft azure certification
    4. [56]the pareto principle for data scientists
    5. [57]r vs python for data visualization
    6. [58]how to work in data science, ai, big data
    7. [59]the deep learning toolset     an overview

[60]latest news

     * [61]download your datax guide to ai in marketing
     * [62]kdnuggets offer: save 20% on strata in london
     * [63]training a champion: building deep neural nets for big ...
     * [64]building a recommender system
     * [65]predict age and gender using convolutional neural netwo...
     * [66]top tweets, mar 27     apr 02: here is a great ex...

more recent stories

     * [67]top tweets, mar 27     apr 02: here is a great explanat...
     * [68]odsc east is selling out; odsc india announced
     * [69]accelerate ai and data science career expo, may 4, 2019
     * [70]grow your data career at datasciencego, san diego, sep 27-29
     * [71]getting started with nlp using the pytorch framework
     * [72]how to diy your data science education
     * [73]top 8 data science use cases in gaming
     * [74]kdnuggets 19:n13, apr 3: top 10 data scientist coding mista...
     * [75]make better data-driven business decisions
     * [76]top stories, mar 25-31: r vs python for data visualization;
       th...
     * [77]two predictive analytics world events in europe this fall
     * [78]7 qualities your big data visualization tools absolutely must
       ...
     * [79]yeshiva university: tenure-track faculty in ai and machine
       lea...
     * [80]which face is real?
     * [81]yeshiva university: program director / tenure track faculty
       me...
     * [82]top 10 coding mistakes made by data scientists
     * [83]uber   s case study at paw industry 4.0: machine learning ...
     * [84]xai     a data scientist   s mouthpiece
     * [85]what does gpt-2 think about the ai arms race?
     * [86]openclassrooms: data freelance online course creator
       [telecomm...

   [87]kdnuggets home    [88]news    [89]2017    [90]dec    [91]tutorials,
   overviews    exploring recurrent neural networks ( [92]17:n46 )
      2019 kdnuggets. [93]about kdnuggets.  [94]privacy policy. [95]terms
   of service

   [96]subscribe to kdnuggets news
   [97][tw_c48.png] [98]facebook [99]linkedin
   x
   [envelope.png] [100]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html/feed
   5. https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html
   6. https://www.kdnuggets.com/jobs/17/12-01-drexel-teaching-faculty.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/12/index.html
  29. https://www.kdnuggets.com/2017/12/tutorials.html
  30. https://www.kdnuggets.com/2017/n46.html
  31. https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html
  32. https://www.kdnuggets.com/jobs/17/12-01-drexel-teaching-faculty.html
  33. https://www.kdnuggets.com/tag/neural-networks
  34. https://www.kdnuggets.com/tag/packt-publishing
  35. https://www.kdnuggets.com/tag/python
  36. https://www.kdnuggets.com/tag/recurrent-neural-networks
  37. https://www.kdnuggets.com/tag/tensorflow
  38. https://www.kdnuggets.com/2017/12/exploring-recurrent-neural-networks.html#comments
  39. https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach
  40. https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow?utm_source=kdnuggets&utm_medium=referral&utm_campaign=outreach
  41. https://www.kdnuggets.com/2017/10/guide-time-series-prediction-recurrent-neural-networks-lstms.html
  42. https://www.kdnuggets.com/2017/08/deeper-recurrent-networks-sequence-bag-words-model.html
  43. https://www.kdnuggets.com/2017/10/seven-steps-deep-learning-keras.html
  44. https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html
  45. https://www.kdnuggets.com/jobs/17/12-01-drexel-teaching-faculty.html
  46. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  47. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  48. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  49. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  50. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  51. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  52. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  53. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  54. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  55. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  56. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  57. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  58. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  59. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  60. https://www.kdnuggets.com/news/index.html
  61. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  62. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  63. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  64. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  65. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  66. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  67. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  68. https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html
  69. https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html
  70. https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html
  71. https://www.kdnuggets.com/2019/04/nlp-pytorch.html
  72. https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html
  73. https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html
  74. https://www.kdnuggets.com/2019/n13.html
  75. https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html
  76. https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html
  77. https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html
  78. https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html
  79. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html
  80. https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html
  81. https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html
  82. https://www.kdnuggets.com/2019/04/top-10-coding-mistakes-data-scientists.html
  83. https://www.kdnuggets.com/2019/04/paw-uber-case-study-machine-learning-enforce-mobile-performance.html
  84. https://www.kdnuggets.com/2019/04/xai-data-scientist.html
  85. https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html
  86. https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html
  87. https://www.kdnuggets.com/
  88. https://www.kdnuggets.com/news/index.html
  89. https://www.kdnuggets.com/2017/index.html
  90. https://www.kdnuggets.com/2017/12/index.html
  91. https://www.kdnuggets.com/2017/12/tutorials.html
  92. https://www.kdnuggets.com/2017/n46.html
  93. https://www.kdnuggets.com/about/index.html
  94. https://www.kdnuggets.com/news/privacy-policy.html
  95. https://www.kdnuggets.com/terms-of-service.html
  96. https://www.kdnuggets.com/news/subscribe.html
  97. https://twitter.com/kdnuggets
  98. https://facebook.com/kdnuggets
  99. https://www.linkedin.com/groups/54257
 100. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
 102. https://www.kdnuggets.com/
 103. https://www.kdnuggets.com/
