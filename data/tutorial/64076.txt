   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

optimization in python cookbook: bowl, plate and valley functions

   go to the profile of alexandr honchar
   [9]alexandr honchar (button) blockedunblock (button) followfollowing
   mar 3, 2018
   [1*w5lsncdrz4mytimqws6via.png]

   hi all again! in [10]one of my posts in the end of last year i
   mentioned optimization as one of the most interesting field for machine
   learning researchers. i am a huge fan of numerical optimization since
   my bachelor degree and i decided to recap some basics first of all for
   myself and tell you, guys, some recipes on how to solve optimization
   problems for machine learning and not only.

   i want to start with simple functions optimization, after move on to
   more complex functions with several local minimums or that are just
   difficult to find minima in, constrained optimization and geometrical
   optimization problems. i also want to use totally different methods for
   optimization: starting with gradient based methods and finishing with
   evolutionary algorithms and latest ideas from deep learning
   battlefront. of course, machine learning applications will go
   alongside, but the real goal is to show big landscape of problems and
   algorithms in numerical optimization and to give you understanding,
   what   s really happening inside your favorite adamoptimizer().

   another moment, that i personally like a lot: i want to concentrate on
   visualizations of different algorithms behavior to understand intuition
   behind math and code, so in this series of posts there will be a lot of
   gifs         for [11]zero order methods, [12]first order in scipy, [13]first
   order in tensorflow and [14]second order methods. as always, i strongly
   recommend you to check out [15]source code too.

functions for optimization

   first of all, let   s define set of functions for today. i decided to
   start with the simplest ones, that supposed to be very easy to optimize
   and to show general walkthrough with different tools. the whole list of
   functions and formulas you can find [16]here, i just chose some of
   them. i also need to notice, that code for visualization is used from
   this [17]amazing tutorial.

bowl functions

   bohachevsky function and trid functions
def bohachevsky(x, y):
    return x**2 + 2*y**2 - 0.3*np.cos(3*3.14*x) -     0.4*np.cos(4*3.14*y) + 0.7
def trid(x, y):
    return (x-1)**2 + (y-1)**2 - x*y

   [1*b2dkl_bvdjcw2gz2pkj_aa.png]
   [1*ic4cxwpp1pqbikchawxfta.png]
   bohachevsky function (on the left) and trid function (on the right)

plate functions

   booth, matyas and zakharov functions
def booth(x, y):
    return (x + 2*y - 7)**2 + (2*x + y - 5)**2
def matyas(x, y):
    return 0.26*(x**2 + y**2) - 0.48*x*y
def zakharov(x, y):
    return (x**2 + y**2) + (0.5*x + y)**2 + (0.5*x + y)**4

   [1*ze1wqttu7nydzdfcmlkm8a.png]
   [1*emfz0jbmbhndkdxumditgg.png]
   [1*rwo962c21xobgzhfhyojva.png]
   booth (left), matyas (center) and zakharov (right) functions

valley functions

   rozenbrock, beale and six hump camel functions
def rozenbrock(x, y):
    return (1-x)**2 + 100*(y - x**2)**2
def beale(x, y):
    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.65 - x + x*y**3)**2
def six_hump(x, y):
    return (4 - 2.1*x**2 + x**4/3)*x**2 + x*y + (-4 + 4*y**2)*y**2

   [1*i67cxeelate5l6zhurgo-w.png]
   [1*6tu3akx44sbsd394aqwg8a.png]
   [1*o9x2xwc1dlkfk-th4zy3dg.png]
   rozenbrock (left), beale (center) and six hump camel (right) functions

algorithms

   in this article we will review just basic algorithms for optimization
   from scipy and tensorflow.

optimization without gradients

   very often our cost function is noisy, or non-differentiable, and,
   hence, we can   t apply methods that use gradient in this case. in this
   tutorial we compare nelder-mead and powell algorithms as ones, that
   don   t compute gradients. first approach builds (n+1)-dimensional
   simplex and finds a minimum on it, sequentially updating the simplex.
   powell method makes one-dimensional search on each of the basis vectors
   of the space. we use scipy implementations for them:
minimize(fun, x0, method='nelder-mead', tol=none, callback=make_minimize_cb)

   callback is used for saving intermediate results, taken from [18]here,
   check the full code on my [19]github. read more about [20]nelder-mead
   and [21]powell methods.

first order algorithms

   this family of algorithms you probably know much better from machine
   learning frameworks like tensorflow. the idea behind all of them is
   moving in direction of anti-gradient that leads to the minima of the
   function. but the details of moving to this minima differ a lot. we
   will review the following from tensorflow: id119 with
   momentum (and without), adam and rmsprop. for tf we will define
   functions like this (whole code is [22]here):
x = tf.variable(8., trainable=true)
y = tf.variable(8., trainable=true)
f = tf.add_n([
    tf.add(tf.square(x), tf.square(y)),
    tf.square(tf.add(tf.multiply(0.5, x), y)),
    tf.pow(tf.multiply(0.5, x), 4.)
])

   and optimize them following way:
opt = tf.train.gradientdescentoptimizer(0.01)
grads_and_vars = opt.compute_gradients(f, [x, y])
clipped_grads_and_vars = [(tf.clip_by_value(g, xmin, xmax), v) for g, v in grads
_and_vars]
train = opt.apply_gradients(clipped_grads_and_vars)
sess = tf.session()
sess.run(tf.global_variables_initializer())
points_tf = []
for i in range(100):
    points_tf.append(sess.run([x, y]))
    sess.run(train)

   from scipy we will use conjugated gradient, newton conjugated gradient,
   truncated newton, sequential least squares programming methods. you can
   read more about this kind of algorithms in a [23]free online book by
   [24]stephen boyd and [25]lieven vandenberghe. another cool comparison
   of optimization for machine learning is in this [26]blog post.

   the scipy api is more or less the same as with zero-order methods, you
   can check it [27]here.

second order algorithms

   we also will touch couple of algorithms that use second order
   derivatives for faster convergence: dog-leg trust-region, nearly exact
   trust-region. these algorithms sequentially solve sub-optimization
   tasks, where regions of search (usually spheres) are being found. as we
   know, these algorithms require a hessian (or its approximation), so we
   will use numdifftools library to compute them and pass into scipy
   optimizer (inspired by [28]click, whole code [29]click):
from numdifftools import jacobian, hessian
def fun(x):
    return (x[0]**2 + x[1]**2) + (0.5*x[0] + x[1])**2 + (0.5*x[0] + x[1])**4
def fun_der(x):
    return jacobian(lambda x: fun(x))(x).ravel()
def fun_hess(x):
    return hessian(lambda x: fun(x))(x)
minimize(fun, x0, method='dogleg', jac=fun_der, hess=fun_hess)

optimization without gradients

   in this post i want to evaluate results first of all from visual point
   of view, i believe that it   s very important to see with your eyes
   trajectories first and after all the formulas will be much more clear.

   here are just some of the visualizations, you can find more on this
   [30]imgur.
   [1*fof8d6qbbnckbzubuaxwqa.gif]
   [1*aclmpl-oy2azkxsxdej4xw.gif]
   [1*i4yqc3cruvy31t0doesena.gif]
   bohachevsky, matyas and trid functions are optimized with nelder-mead
   and powell

optimization with jacobian

   here you can see comparison of gradient based methods from scipy and
   tensorflow, more gifs are [31]here and [32]here. some of these
   algorithms may look too    slow   , but it depends a lot on the choice of
   hyperparameters, we take a look on them a bit later.
   [1*cdi6bbge2vpctyp1ofqfkg.gif]
   [1*ainiykfj2pqet3ttkkggnq.gif]
   [1*pttrle5aty2ppt90s8hjga.gif]
   booth, rosenbrok and six hump functions in scipy
   [1*la6oii56_tftmzb8rxxsdq.gif]
   [1*pbn06z8rola1hvs2tqw5pa.gif]
   [1*xocper0pxrl09lmlz1q6pa.gif]
   booth, rosenbrok and six hump functions in tensorflow

optimization with hessian

   using second derivative almost immediately leads us to the minima for
      good    looking quadratic functions, but it   s not that simple for other   
   for example for bohachevsky function it comes close to the minima, but
   not exactly there. more examples (also diverging!) of other functions
   are [33]here.
   [1*r3z-brivrlzldq0x5zmsvq.gif]
   [1*ftrwuewlyenw_qxclm4maq.gif]
   [1*mxtjex0hnordngwakytnbw.gif]
   bohachevsky, matyas and trid functions optimization trajectories

notes on hyperparameters

learning rate

   first of all, i think you have noticed, that such popular adaptive
   algorithms as adam and rmsprop were very slow comparing even to sgd,
   but they were designed to be faster? this is because of the too small
   learning rate exactly for these loss surfaces. this parameter has to be
   tuned for each problem separately. on the images below you can see what
   happens if we increase it to the values of 1.
   [1*uffmjbnqnjxvuglal9rx4g.gif]
   [1*hlmqjxntfmiotv7m7wyy5a.gif]
   increased learning rate for adam and rmsprop

starting point

   usually we just start search of minima from a random point (or with
   some smart initializers like in neural networks), but in general it   s
   not correct strategy. on the example below you can see, how even second
   order methods can diverge if started from the wrong point. one of the
   ways to overcome this issue in pure optimization problem is use of
   global search algorithms to estimate area of global minima (we will do
   it later).
   [1*r5-t_-oiaar2u3istkhnlq.gif]
   [1*ioxian87wblmtvhedcy8bq.gif]
   diverging second order methods from the wrong starting point

notes on machine learning

   most probably, right now you want to try some of scipy algorithms to
   train your machine learning models in tensorflow. you even don   t need
   to build a custom optimizer, because tf.contrib.opt already [34]has it
   for you! it allows to use the same algorithms and their parameters:
vector = tf.variable([7., 7.], 'vector')
# make vector norm as small as possible.
loss = tf.reduce_sum(tf.square(vector))
optimizer = scipyoptimizerinterface(loss, options={'maxiter': 100}, method='slsq
p')
with tf.session() as session:
  optimizer.minimize(session)

conclusion

   this article is just introduction to the field of optimization, but
   even from these results we can see, that it   s not easy at all. using
      cool    second order or adaptive rate algorithms doesn   t guarantee to us
   convergence to the minima, moreover, we have some hyperparameters as
   learning rate and starting point to find. anyway, now you have some
   intuition, of how all these algorithms work. for example, you can be
   sure that it   s good idea to use second order methods for quadratic-like
   functions or not to be shy to use algorithms, that doesn   t require
   derivatives, they also work well and must be part of your optimization
   arsenal.

   we also need to remember, that these functions were not really
   difficult at all! what will happen when we will encounter something
   like this    with constraints    and stochasticity?
   [1*x_hwb_1ywhu5pwhkxopz2q.png]
   rastrigin function

   if you want to know, stay tuned!

   p.s.
   follow me also in [35]facebook for ai articles that are too short for
   medium, [36]instagram for personal stuff and [37]linkedin!

     * [38]machine learning
     * [39]optimization
     * [40]deep learning
     * [41]artificial intelligence
     * [42]mathematics

   (button)
   (button)
   (button) 523 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of alexandr honchar

[43]alexandr honchar

   medium member since oct 2018

                     developing ai for biosignal analysis and finance, consulting,
   giving public speeches and blogging. contact me to collaborate
   [44]rachnogstyle@gmail.com

     * (button)
       (button) 523
     * (button)
     *
     *

   go to the profile of alexandr honchar
   never miss a story from alexandr honchar, when you sign up for medium.
   [45]learn more
   never miss a story from alexandr honchar
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/262aa5555b61
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@alexrachnog/optimization-cookbook-1-262aa5555b61&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@alexrachnog/optimization-cookbook-1-262aa5555b61&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@alexrachnog
  10. https://blog.goodaudience.com/ai-in-2018-for-researchers-8955df0caaf9
  11. https://imgur.com/a/pfpyx
  12. https://imgur.com/a/cqn0z
  13. https://imgur.com/a/swfwq
  14. https://imgur.com/a/pmybq
  15. https://github.com/rachnog/optimization-cookbook/tree/master/chapter1
  16. https://www.sfu.ca/~ssurjano/optimization.html
  17. http://tiao.io/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/
  18. http://tiao.io/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/
  19. https://github.com/rachnog/optimization-cookbook/blob/master/chapter1/zero order methods.ipynb
  20. https://en.wikipedia.org/wiki/nelder   mead_method
  21. https://en.wikipedia.org/wiki/powell's_method
  22. https://github.com/rachnog/optimization-cookbook/blob/master/chapter1/tensorflow methods.ipynb
  23. https://web.stanford.edu/~boyd/cvxbook/
  24. http://www.stanford.edu/~boyd/
  25. http://www.ee.ucla.edu/~vandenbe/
  26. http://ruder.io/optimizing-gradient-descent/
  27. https://github.com/rachnog/optimization-cookbook/blob/master/chapter1/first order methods.ipynb\
  28. https://stackoverflow.com/questions/41137092/jacobian-and-hessian-inputs-in-scipy-optimize-minimize
  29. https://github.com/rachnog/optimization-cookbook/blob/master/chapter1/tensorflow methods.ipynb
  30. https://imgur.com/a/pfpyx
  31. https://imgur.com/a/cqn0z
  32. https://imgur.com/a/swfwq
  33. https://imgur.com/a/pmybq
  34. https://www.tensorflow.org/api_docs/python/tf/contrib/opt/scipyoptimizerinterface
  35. https://www.facebook.com/rachnogstyle.blog
  36. http://instagram.com/rachnogstyle
  37. https://www.linkedin.com/in/alexandr-honchar-4423b962/
  38. https://medium.com/tag/machine-learning?source=post
  39. https://medium.com/tag/optimization?source=post
  40. https://medium.com/tag/deep-learning?source=post
  41. https://medium.com/tag/artificial-intelligence?source=post
  42. https://medium.com/tag/mathematics?source=post
  43. https://medium.com/@alexrachnog
  44. mailto:rachnogstyle@gmail.com
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://medium.com/@alexrachnog?source=post_header_lockup
  48. https://medium.com/p/262aa5555b61/share/twitter
  49. https://medium.com/p/262aa5555b61/share/facebook
  50. https://medium.com/@alexrachnog?source=footer_card
  51. https://medium.com/p/262aa5555b61/share/twitter
  52. https://medium.com/p/262aa5555b61/share/facebook
  53. https://medium.com/@alexrachnog
