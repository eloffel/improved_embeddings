   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

another twitter id31 with python         part 10 (neural network with
doc2vec/id97/glove)

   [16]go to the profile of ricky kim
   [17]ricky kim (button) blockedunblock (button) followfollowing
   feb 9, 2018
   [1*lr1etlrpttcyrnj9-yeftq.jpeg]

   this is the 10th part of my ongoing twitter id31 project.
   you can find the previous posts from the below links.
     * [18]part 1: data cleaning
     * [19]part 2: eda, data visualisation
     * [20]part 3: zipf   s law, data visualisation
     * [21]part 4: feature extraction (count vectorizer), id165,
       confusion matrix
     * [22]part 5: feature extraction (tfidf vectorizer), machine learning
       model comparison, lexical approach
     * [23]part 6: doc2vec
     * [24]part 7: phrase modeling + doc2vec
     * [25]part 8: id84 (chi2, pca)
     * [26]part 9: neural networks with tfidf vectors

   in the previous post, i implemented neural network modelling with
   tf-idf vectors, but found that with high-dimensional sparse data,
   neural network did not perform well. in this post, i will see if
   feeding document vectors from doc2vec models or word vectors is any
   different from tf-idf vectors.

   *in addition to short code blocks i will attach, you can find the link
   for the whole jupyter notebook at the end of this post.

neural networks with doc2vec

   before i jump into neural network modelling with the vectors i got from
   doc2vec, i would like to give you some background on how i got these
   document vectors. i have implemented doc2vec using gensim library in
   the [27]6th part of this series.

   there are three different methods used to train doc2vec. distributed
   bag of words, distributed memory (mean), distributed memory
   (concatenation). these models were trained with 1.5 million tweets
   through 30 epochs and the output of the models are 100 dimension
   vectors for each tweet. after i got document vectors from each model, i
   have tried concatenating these (so the concatenated document vectors
   have 200 dimensions) in combination: dbow + dmm, dbow + dmc, and saw an
   improvement to the performance when compared with models with any
   single pure method. using different methods of training and
   concatenating them to improve the performance has already been
   demonstrated by [28]le and mikolov (2014) in their research paper.

   finally, i have applied phrase modelling to detect bigram phrase and
   trigram phrase as a pre-step of doc2vec training and tried different
   combination across id165s. when tested with a id28
   model, i got the best performance result from    unigram dbow + trigram
   dmm    document vectors.

   i will first start by loading gensim   s doc2vec, and define a function
   to extract document vectors, then load the doc2vec model i trained.

   iframe: [29]/media/6b053a4016392c4b127934a7ea29b346?postid=a6441269aa3c

   when fed to a simple id28, the concatenated document
   vectors (unigram dbow + trigram dmm) yields 75.90% training set
   accuracy, and 75.76% validation set accuracy.

   i will try different numbers of hidden layers, hidden nodes to compare
   the performance. in the below code block, you see i first define the
   seed as    7    but not setting the random seed,    np.random.seed()    will be
   defined at the start of each model. this is for a reproducibility of
   various results from different model structures.

   *side note (reproducibility): to be honest, this took me a while to
   figure out. i first tried by setting the random seed before i import
   keras, and ran one model after another. however, if i define the same
   model structure after it has run, i couldn   t get the same result. but i
   also realised if i restart the kernel, and re-run code blocks from
   start it gives me the same result as the last kernel. so i figured,
   after running a model the random seed changes, and that is the reason
   why i cannot get the same result with the same structure if i run them
   in the same kernel consecutively. anyway, that is why i set the random
   seed every time i try a different model. for your information, i am
   running keras with theano backend, and only using cpu not gpu. if you
   are on the same setting, this should work. i explicitly specified
   backend as theano by launching jupyter notebook in the command line as
   follows:    keras_backend=theano jupyter notebook   

   please note that not all of the dependencies loaded in the below cell
   has been used for this post, but imported for later use.

   defining different model structures will be quite repetitive codes, so
   i will just give you two examples so that you can understand how to
   define model structures with keras.

   iframe: [30]/media/923ce09a02605478a6929bbab28f47b7?postid=a6441269aa3c

   after trying 12 different models with a range of hidden layers (from 1
   to 3) and a range of hidden nodes for each hidden layer (64, 128, 256,
   512), below is the result i got. best validation accuracy (79.93%) is
   from    model_d2v_09    at epoch 7, which has 3 hidden layers of 256 hidden
   nodes for each hidden layer.
   [1*g7xidcmczpon2rtepjn8tw.png]

   now i know which model gives me the best result, i will run the final
   model of    model_d2v_09   , but this time with callback functions in
   keras. i was not quite familiar with callback functions in keras before
   i received a comment in my previous post. after i got the comment, i
   did some digging and found all the useful functions in keras callbacks.
   thanks to [31]@rcshubha for the comment. with my final model of doc2vec
   below, i used    checkpoint    and    earlystop   . you can set the
      checkpoint    function with options, and with the below parameter
   setting,    checkpoint    will save the best performing model up until the
   point of running, and only if a new epoch outperforms the saved model
   it will save it as a new model. and    early_stop    i defined it as to
   monitor validation accuracy, and if it doesn   t outperform the best
   validation accuracy so far for 5 epochs, it will stop.

   iframe: [32]/media/f4e26c7205ea16121f1b2f96e8032c26?postid=a6441269aa3c

   [1*i_rw2e4rrmebwwsw04w7rq.png]

   if i evaluate the model i just run, it will give me the result as same
   as i got from the last epoch.
model_d2v_09_es.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)

   [1*9bo71byy7tbx5oohwctxuq.png]

   but if i load the saved model at the best epoch, then this model will
   give me the result at that epoch.
from keras.models import load_model
loaded_model = load_model('d2v_09_best_weights.07-0.7993.hdf5')
loaded_model.evaluate(x=validation_vecs_ugdbow_tgdmm, y=y_validation)

   [1*eblaq_jqzhwx1xztk3arnq.png]

   if you remember the validation accuracy with the same vector
   representation of the tweets with a id28 model (75.76%),
   you can see that feeding the same information to neural networks yields
   a significantly better result. it   s amazing to see how neural network
   can boost the performance of dense vectors, but the best validation
   accuracy is still lower than the tfidf vectors + id28
   model, which gave me 82.92% validation accuracy.

   if you have read my posts on doc2vec, or familiar with doc2vec, you
   might know that you can also extract word vectors for each word from
   the trained doc2vec model. i will move on to id97, and try
   different methods to see if any of those can outperform the doc2vec
   result (79.93%), ultimately outperform the tfidf + id28
   model (82.92%).

id97

   to make use of word vectors extracted from doc2vec model, i can no
   longer use the concatenated vectors of different id165s, since they
   will not consist of the same vocabularies. thus below, i load the model
   for unigram dmm and create concatenated vectors with unigram dbow of
   200 dimensions for each word in the vocabularies.

   what i will do first before i try neural networks with document
   representations computed from word vectors is that i will fit logistic
   regressions with various methods of id194 and with
   the one that gives me the best validation accuracy, i will finally
   define a neural network model.

   i will also give you the summary of result from all the different word
   vectors fit with id28 as a table.

word vectors extracted from doc2vec models (average/sum)

   there could be a number of different ways to come up with document
   representational vectors with individual word vectors. one obvious
   choice is to average them. for every word in a tweet, see if trained
   doc2vec has word vector representation of the word, if so, sum them up
   throughout the document while counting how many words were detected as
   having word vectors, and finally by dividing the summed vector by the
   count you get the averaged word vector for the whole document which
   will have the same dimension (200 in this case) as the individual word
   vectors.

   another method is just the sum of the word vectors without averaging
   them. this might distort the vector representation of the document if
   some tweets only have a few words in the doc2vec vocabulary and some
   tweets have most of the words in the doc2vec vocabulary. but i will try
   both summing and averaging and compare the results.

   iframe: [33]/media/5f030dfab67709fc8ef3962e187996c9?postid=a6441269aa3c

   the validation accuracy with averaged word vectors of unigram dbow +
   unigram dmm is 71.74%, which is significantly lower than document
   vectors extracted from unigram dbow + trigram dmm (75.76%), and also
   from the results i got from the 6th part of this series, i know that
   document vectors extracted from unigram dbow + unigram dmm will give me
   75.51% validation accuracy.

   i also tried scaling the vectors using scikitlearn   s scale function and
   saw significant improvement in computation time and a slight
   improvement of the accuracy.

   let   s see how summed word vectors perform compared to the averaged
   counterpart.

   iframe: [34]/media/ad0aa6960f817a065b46fbc4eb91fb15?postid=a6441269aa3c

   the summation method gave me higher accuracy without scaling compared
   to the average method. but the simple id28 with the
   summed vectors took more than 3 hours to run. so again i tried scaling
   these vectors.

   iframe: [35]/media/6b06352bc952c7f0c664c63a2a7dd64b?postid=a6441269aa3c

   surprising! with scaling, id28 fitting only took 3
   minutes! that   s quite a difference. validation accuracies with scaled
   word vectors are 72.42% with averaging, 72.51% with summing.

word vectors extracted from doc2vec models with tfidf weighting (average/sum)

   in the [36]5th part of this series, i have already explained what
   tf-idf is. tf-idf is a way of weighting each word by calculating the
   product of relative term frequency and inverse document frequency.
   since it gives one scalar value for each word in the vocabulary, this
   can also be used as a weighting factor of each word vectors. correa jr.
   et al (2017) has implemented this tf-idf weighting in their paper
      [37]nilc-usp at semeval-2017 task 4: a multi-view ensemble for twitter
   id31   

   in order to get the tfidf value for each word, i first fit and
   transform the training set with tfidfvectorizer and create a dictionary
   containing    word   ,    tfidf value    pairs. also, i defined a function
      get_w2v_general    to get either averaged word vectors or summed word
   vectors with given id97 model. finally, i calculate the
   multiplication of word vectors with corresponding tfidf values. from
   below code, the tfidf multiplication part took quite a bit of time. to
   be honest, i am still not sure why it took so long to compute the tfidf
   weighting of the word vectors, but after 5 hours it finally finished
   computing. you can also see later that i tried another method of
   weighting but that took less than 10 seconds. if you have an answer to
   this, any insight would be appreciated.

   iframe: [38]/media/9195e3c3f166855640920c1af1ed6496?postid=a6441269aa3c

   then i can call    get_w2v_general    function in the same way i did with
   the above word vectors without weighting factors, then fit a logistic
   regression model. the validation accuracy for mean is 70.57%, for sum
   is 70.32%. the results are not what i expected, especially after 5
   hours of waiting. by weighting word vectors with tfidf values, the
   validation accuracy dropped around 2% both for averaging and summing.

word vectors extracted from doc2vec models with custom weighting
(average/sum)

   in the [39]3rd part of this series, i have defined a custom metric
   called    pos_normcdf_hmean   , which is a metric borrowed from the
   presentation by [40]jason kessler in pydata 2017 seattle. if you want
   to know more in detail about the calculation, you can either check
   [41]my previous post or you can also watch [42]jason kessler   s
   presentation. to give you a high-level intuition, by calculating
   harmonic mean of cdf(cumulative distribution function) transformed
   values of term frequency rate within the whole document and the term
   frequency within a class, you can get a meaningful metric which shows
   how each word is related to a certain class.

   i have used this metric to visualise tokens in the 3rd part of the
   series, and also used this again to create custom lexicon to be used
   for classification purpose in the [43]5th part. i will use this again
   as a weighting factor for the word vectors and see how it affects the
   performance.

   iframe: [44]/media/33180c8f1746bf45dfd45cdc30060b2c?postid=a6441269aa3c

   the validation accuracy for mean is 73.27%, and for sum is 70.94%.
   unlike tfidf weighting, this time with custom weighting it actually
   gave me some performance boost when used with averaging method. but
   with summing, this weighting has performed no better than the word
   vectors without weighting.

word vectors extracted from pre-trained glove (average/sum)

   glove is another kind of word representation in vectors proposed by
   [45]pennington et al. (2014) from the stanford nlp group.

   the difference between id97 and glove is how the two models compute
   the word vectors. in id97, the word vectors you are getting is a
   kind of a by-product of a shallow neural network, when it tries to
   predict either centre word given surrounding words or vice versa. but
   with glove, the word vectors you are getting is the object matrix of
   glove model, and it calculates this using term co-occurrence matrix and
   id84.

   the good news is you can now easily load and use the pre-trained glove
   vectors from gensim thanks to its latest update (gensim 3.2.0). in
   addition to some pre-trained word vectors, new datasets are also added
   and this also can be easily downloaded using their downloader api. if
   you want to know more about this, please check [46]this blog post by
   rare technologies.

   the stanford nlp group has made their pre-trained glove vectors
   publicly available, and among them, there are glove vectors trained
   specifically with tweets. this sounds like something definitely worth
   trying. they have four different versions of tweet vectors each with
   different dimensions (25, 50, 100, 200) trained on 2 billion tweets.
   you can find more detail on [47]their website.

   for this post, i will use 200 dimensions pre-trained glove vectors.
   (you might need to update gensim if your version of gensim is lower
   than 3.2.0)

   iframe: [48]/media/e4bce0bc41da31e25e7d8887c72e8db8?postid=a6441269aa3c

   by using pre-trained glove vectors, i can see that the validation
   accuracy significantly improved. so far the best validation accuracy
   was from the averaged word vectors with custom weighting, which gave me
   73.27% accuracy, and compared to this, glove vectors yield 76.27%,
   76.60% for average and sum respectively.

word vectors extracted from pre-trained google news id97 (average/sum)

   with new updated gensim, i can also load the famous pre-trained google
   news word vectors. these word vectors are trained using id97 model
   on google news dataset (about 100 billion words) and published by
   google. the model contains 300-dimensional vectors for 3 million words
   and phrases. you can find more detail in the [49]google project
   archive.

   iframe: [50]/media/ec8e934c12a11d608c2fa9f60e457366?postid=a6441269aa3c

   the validation accuracy for mean is 74.96%, and for sum is 74.92%. even
   though it gives me a better result than the word vectors extracted from
   custom trained doc2vec models, but it fails to outperform glove
   vectors. and the vector dimension is even larger in google news word
   vectors.

   but, this is trained with google news, and glove vector i used was
   trained specifically with tweets, thus it is hard to compare each other
   directly. what if id97 is specifically trained with tweets?

separately trained id97 (average/sum)

   i know i have already tried word vectors i extracted from doc2vec
   models, but what if i train separate id97 models? even though
   doc2vec models gave good representational vectors of document level,
   would it be more efficiently learning word vectors if i train pure
   id97?

   in order to answer my own questions, i trained two id97 models
   using cbow (continuous bag of words) and skip gram models. in terms of
   parameter setting, i set the same parameters i used for doc2vec.
     * size of vectors: 100 dimensions
     * negative sampling: 5
     * window: 2
     * minimum word count: 2
     * alpha: 0.065 (decrease alpha by 0.002 per epoch)
     * number of epochs: 30

   with above settings, i defined cbow model by passing    sg=0   , and skip
   gram model by passing    sg=1   .

   and once i get the results from two models, i concatenate vectors of
   two models for each word so that the concatenated vectors will have
   200-dimensional representation of each word.

   please note that in the 6th part, where i trained doc2vec, i used
      labeledsentence    function imported from gensim. this has now been
   deprecated, thus for this post i used    taggeddocument    function
   instead. the usage is the same.

   iframe: [51]/media/bddf5e9491f8b3a7a7a3d0dc8749b5ef?postid=a6441269aa3c

   the concatenated vectors of unigram cbow and unigram skip gram models
   has yielded 76.50%, 76.75% validation accuracy respectively with mean
   and sum method. these results are even higher than the results i got
   from glove vectors.

   but please do not confuse this as a general statement. this is an
   empirical finding in this particular setting.

separately trained id97 with custom weighting (average/sum)

   as a final step, i will apply the custom weighting i have implemented
   above and see if this affects the performance.

   iframe: [52]/media/d2b92ba5dde185336e1002d98b5fcfda?postid=a6441269aa3c

   finally i get the best performing word vectors. averaged word vectors
   (separately trained id97 models) weighted with custom metric has
   yielded the best validation accuracy of 77.97%! below is the table of
   all the results i tried above.
   [1*q1xdzzkszzhxizttxgiszw.png]

neural network with id97

   the best performing word vectors with id28 was chosen to
   feed to a neural network model. this time i did not try various
   different architecture. based on what i have observed during trials of
   different architectures with doc2vec document vectors, the best
   performing architecture was one with 3 hidden layers with 256 hidden
   nodes at each hidden layer.
   i will finally fit a neural network with early stopping and checkpoint
   so that i can save the best performing weights on validation accuracy.

   iframe: [53]/media/ee2fe098d5a2289addcd1ac061fce97d?postid=a6441269aa3c

   [1*cvduubtq9w8ew7j6ziutvq.png]
from keras.models import load_model
loaded_w2v_model = load_model('w2v_01_best_weights.10-0.8048.hdf5')
loaded_w2v_model.evaluate(x=validation_w2v_final, y=y_validation)

   [1*gim0l60bl07uc7vmmjhbcw.png]

   the best validation accuracy is 80.48%. surprisingly this is even
   higher than the best accuracy i got by feeding document vectors to
   neural network models in the above.

   it took quite some time for me to try different settings, different
   calculations, but i learned some valuable lessons through all the trial
   and errors. specifically trained id97 with carefully engineered
   weighting can even outperform doc2vec in the classification task.

   in the next post, i will try more sophisticated neural network model,
   convolutional neural network. again i hope this will give me some boost
   of the performance.

   thank you for reading, and you can find the jupyter notebook from the
   below link.

   [54]https://github.com/tthustla/twitter_sentiment_analysis_part10/blob/
   master/capstone_part10.ipynb

     * [55]machine learning
     * [56]data science
     * [57]nlp
     * [58]id31
     * [59]id97

   (button)
   (button)
   (button) 197 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [60]go to the profile of ricky kim

[61]ricky kim

   the rickest ricky. love data, beer, coffee, and good memes in no
   particular order.

     (button) follow
   [62]towards data science

[63]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 197
     * (button)
     *
     *

   [64]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [65]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a6441269aa3c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_kfgohv6frwq7---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rickykim78?source=post_header_lockup
  17. https://towardsdatascience.com/@rickykim78
  18. https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90
  19. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913
  20. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7
  21. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5
  22. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd
  23. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504
  24. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-7-phrase-modeling-doc2vec-592a8a996867
  25. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3
  26. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7
  27. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504
  28. https://cs.stanford.edu/~quocle/paragraph_vector.pdf
  29. https://towardsdatascience.com/media/6b053a4016392c4b127934a7ea29b346?postid=a6441269aa3c
  30. https://towardsdatascience.com/media/923ce09a02605478a6929bbab28f47b7?postid=a6441269aa3c
  31. http://twitter.com/rcshubha
  32. https://towardsdatascience.com/media/f4e26c7205ea16121f1b2f96e8032c26?postid=a6441269aa3c
  33. https://towardsdatascience.com/media/5f030dfab67709fc8ef3962e187996c9?postid=a6441269aa3c
  34. https://towardsdatascience.com/media/ad0aa6960f817a065b46fbc4eb91fb15?postid=a6441269aa3c
  35. https://towardsdatascience.com/media/6b06352bc952c7f0c664c63a2a7dd64b?postid=a6441269aa3c
  36. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd
  37. http://www.aclweb.org/anthology/s17-2100
  38. https://towardsdatascience.com/media/9195e3c3f166855640920c1af1ed6496?postid=a6441269aa3c
  39. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7
  40. https://youtu.be/h7x9ca2pwko
  41. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7
  42. https://youtu.be/h7x9ca2pwko
  43. https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd
  44. https://towardsdatascience.com/media/33180c8f1746bf45dfd45cdc30060b2c?postid=a6441269aa3c
  45. https://nlp.stanford.edu/pubs/glove.pdf
  46. https://rare-technologies.com/new-download-api-for-pretrained-nlp-models-and-datasets-in-gensim/
  47. https://nlp.stanford.edu/projects/glove/
  48. https://towardsdatascience.com/media/e4bce0bc41da31e25e7d8887c72e8db8?postid=a6441269aa3c
  49. https://code.google.com/archive/p/id97/
  50. https://towardsdatascience.com/media/ec8e934c12a11d608c2fa9f60e457366?postid=a6441269aa3c
  51. https://towardsdatascience.com/media/bddf5e9491f8b3a7a7a3d0dc8749b5ef?postid=a6441269aa3c
  52. https://towardsdatascience.com/media/d2b92ba5dde185336e1002d98b5fcfda?postid=a6441269aa3c
  53. https://towardsdatascience.com/media/ee2fe098d5a2289addcd1ac061fce97d?postid=a6441269aa3c
  54. https://github.com/tthustla/twitter_sentiment_analysis_part10/blob/master/capstone_part10.ipynb
  55. https://towardsdatascience.com/tagged/machine-learning?source=post
  56. https://towardsdatascience.com/tagged/data-science?source=post
  57. https://towardsdatascience.com/tagged/nlp?source=post
  58. https://towardsdatascience.com/tagged/sentiment-analysis?source=post
  59. https://towardsdatascience.com/tagged/id97?source=post
  60. https://towardsdatascience.com/@rickykim78?source=footer_card
  61. https://towardsdatascience.com/@rickykim78
  62. https://towardsdatascience.com/?source=footer_card
  63. https://towardsdatascience.com/?source=footer_card
  64. https://towardsdatascience.com/
  65. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  67. https://medium.com/p/a6441269aa3c/share/twitter
  68. https://medium.com/p/a6441269aa3c/share/facebook
  69. https://medium.com/p/a6441269aa3c/share/twitter
  70. https://medium.com/p/a6441269aa3c/share/facebook
