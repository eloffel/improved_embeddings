   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

tensorflow tutorial: image classifier using convolutional neural network

   [9]go to the profile of manan oza
   [10]manan oza (button) blockedunblock (button) followfollowing
   dec 15, 2017
   [1*3mnryx8lz2sgp_cvq-izha.jpeg]

   in this tensorflow tutorial, we shall build a convolutional neural
   network based image classifier using tensorflow. if you are just
   getting started with tensorflow, then it would be a good idea to
   [11]read the basic tensoflow tutorial here.

   to demonstrate how to build a convolutional neural network based image
   classifier, we shall build a 6 layer neural network that will identify
   and separate images of dogs from that of cats. this network that we
   shall build is a very small network that you can run on a cpu as well.
   traditional neural networks that are very good at doing image
   classification have many more paramters and take a lot of time if
   trained on cpu. however, in this post, my objective is to show you how
   to build a real-world convolutional neural network using tensorflow
   rather than participating in [12]ilsvrc. before we start with
   tensorflow tutorial, let   s cover basics of convolutional neural
   network. if you are already familiar with conv-nets(and call them
   conv-nets), you can move to part-2 i.e. tensorflow tutorial

part-1: basics of convolutional neural networks:

   neural networks are essentially mathematical models to solve an
   optimization problem. they are made of neurons, the basic computation
   unit of neural networks. a neuron takes an input(say x), do some
   computation on it(say: multiply it with a variable w and adds another
   variable b ) to produce a value (say; z= wx+b). this value is passed to
   a non-linear function called activation function(f) to produce the
   final output(activation) of a neuron. there are many kinds of
   id180. one of the popular activation function is
   sigmoid, which is:
   [1*tqwpundxrkvezf7hc1c4tw.png]

   the neuron which uses sigmoid function as an activation function will
   be called sigmoid neuron. depending on the id180,
   neurons are named and there are many kinds of them like relu, tanh, etc
   (remember this). one neuron can be connected to multiple neurons, like
   this:
   [1*ch5zu5rr7-0i1hjtw-shiq.png]

   in this example, you can see that the weights are the property of the
   connection, i.e. each connection has a different weight value while
   bias is the property of the neuron. this is the complete picture of a
   sigmoid neuron which produces output y:
   [1*wqb75ijgcrcw-oa7lohila.png]

layers:

   if you stack neurons in a single line, it   s called a layer; which is
   the next building block of neural networks.
   [1*-yxxs6n3nga5ncbjpzxotg.png]

   as you can see above, the neurons in green make 1 layer which is the
   first layer of the network through which input data is passed to the
   network. similarly, the last layer is called output layer as shown in
   red. the layers in between input and output layer are called hidden
   layers. in this example, we have only 1 hidden layer shown in blue. the
   networks which have many hidden layers tend to be more accurate and are
   called deep network and hence machine learning algorithms which uses
   these deep networks are called deep learning.

types of layers:

   typically, all the neurons in one layer, do similar kind of
   mathematical operations and that   s how that a layer gets its
   name(except for input and output layers as they do little mathematical
   operations). here are the most popular kinds of layers you should know
   about:

1. convolutional layer:

   convolution is a mathematical operation that   s used in single
   processing to filter signals, find patterns in signals etc. in a
   convolutional layer, all neurons apply convolution operation to the
   inputs, hence they are called convolutional neurons. the most important
   parameter in a convolutional neuron is the filter size, let   s say we
   have a layer with filter size 5*5*3. also, assume that the input that   s
   fed to convolutional neuron is an input image of size of 32*32 with 3
   channels.
   [1*qx1yvn8in1hlkysgghw8va.png]

   let   s pick one 5*5*3(3 for number of channels in a colored image) sized
   chunk from image and calculate convolution(dot product) with our
   filter(w). this one convolution operation will result in a single
   number as output. we shall also add the bias(b) to this output.
   [1*laaxoh1wnitlp7nrplhbow.png]

   in order to calculate the dot product, it   s mandatory for the 3rd
   dimension of the filter to be same as the number of channels in the
   input. i.e. when we calculate the dot product it   s a matrix
   multiplication of 5*5*3 sized chunk with 5*5*3 sized filter.

   we shall slide convolutional filter over whole input image to calculate
   this output across the image as shown by a schematic below:
   [1*hh8jo1ugusqcybjhns_tcg.gif]

   in this case, we slide our window by 1 pixel at a time. if some cases,
   people slide the windows by more than 1 pixel. this number is called
   stride.

   if you concatenate all these outputs in 2d, we shall have an output
   activation map of size 28*28(can you think of why 28*28 from 32*32 with
   the filter of 5*5 and stride of 1). typically, we use more than 1
   filter in one convolution layer. if we have 6 filters in our example,
   we shall have an output of size 28*28*6.
   [1*j8kyhtq1e5csafrabcjofg.png]

   as you can see, after each convolution, the output reduces in size(as
   in this case we are going from 32*32 to 28*28). in a deep neural
   network with many layers, the output will become very small this way,
   which doesn   t work very well. so, it   s a standard practice to add zeros
   on the boundary of the input layer such that the output is the same
   size as input layer. so, in this example, if we add a padding of size 2
   on both sides of the input layer, the size of the output layer will be
   32*32*6 which works great from the implementation purpose as well.
   let   s say you have an input of size n*n, filter size is f, you are
   using s as stride and input is added with 0 pad of size p. then, the
   output size will be:

     (n-f+2p)/s+1

2. pooling layer:

   pooling layer is mostly used immediately after the convolutional layer
   to reduce the spatial size(only width and height, not depth). this
   reduces the number of parameters, hence computation is reduced. also,
   less number of parameters avoid overfitting(don   t worry about it now,
   will describe it little later). the most common form of pooling is max
   pooling where we take a filter of size f*f and apply the maximum
   operation over the f*f sized part of the image.
   [1*ujnsk8arhkcmiw7sgapo0a.png]

   if you take the average in place of taking maximum, it will be called
   average pooling, but it   s not very popular.

   if your input is of size w1*h1*d1 and the size of the filter is f*f
   with stride s. then the output sizes w2*h2*d2 will be:

   w2= (w1-f)/s +1

   h2=(h1-f)/s +1

   d2=d1

   most common pooling is done with the filter of size 2*2 with a stride
   of 2. as you can calculate using the above formula, it essentially
   reduces the size of input by half.
   [1*d0bwvcsujcnhxgti0hx13g.png]

3. fully connected layer:

   if each neuron in a layer receives input from all the neurons in the
   previous layer, then this layer is called fully connected layer. the
   output of this layer is computed by id127 followed by
   bias offset.

understanding training process:

   deep neural networks are nothing but mathematical models of
   intelligence which to a certain extent mimic human brains. when we are
   trying to train a neural network, there are two fundamental things we
   need to do:

i. the architecture of the network:

   when designing the architecture of a neural network you have to decide
   on: how do you arrange layers? which layers to use? how many neurons to
   use in each layer etc.? designing the architecture is slightly
   complicated and advanced topic and takes a lot of research. there are
   many standard architectures which work great for many standard
   problems. examples being alexnet, googlenet, inceptionresnet, vgg etc.
   in the beginning, you should only use the standard network
   architectures. you could start designing networks after you get a lot
   of experience with neural nets. hence, let   s not worry about it now.

ii. correct weights/parameters:

   once you have decided the architecture of the network; the second
   biggest variable is the weights(w) and biases(b) or the parameters of
   the network. the objective of the training is to get the best possible
   values of the all these parameters which solve the problem reliably.
   for example, when we are trying to build the classifier between dog and
   cat, we are looking to find parameters such that output layer gives out
   id203 of dog as 1(or at least higher than cat) for all images of
   dogs and id203 of cat as 1((or at least higher than dog) for all
   images of cats.

   you can find the best set of parameters using a process called backward
   propagation, i.e. you start with a random set of parameters and keep
   changing these weights such that for every training image we get the
   correct output. there are many optimizer methods to change the weights
   that are mathematically quick in finding the correct weights.
   gradientdescent is one such method(backward propagation and optimizer
   methods to change the gradient is a very complicated topic. but we
   don   t need to worry about it now as tensorflow takes care of it).

   so, let   s say, we start with some initial values of parameters and feed
   1 training image(in reality multiple images are fed together) of dog
   and we calculate the output of the network as 0.1 for it being a dog
   and 0.9 of it being a cat. now, we do backward propagation to slowly
   change the parameters such that the id203 of this image being a
   dog increases in the next iteration. there is a variable that is used
   to govern how fast do we change the parameters of the network during
   training, it   s called learning rate. if you think about it, we want to
   maximise the total correct classifications by the network i.e. we care
   for the whole training set; we want to make these changes such that the
   number of correct classifications by the network increases. so we
   define a single number called cost which indicates if the training is
   going in the right direction. typically cost is defined in such a way
   that; as the cost is reduced, the accuracy of the network increases.
   so, we keep an eye on the cost and we keep doing many iterations of
   forward and backward propagations(10s of thousands sometimes) till cost
   stops decreasing. there are many ways to define cost. one of the simple
   one is mean root square cost. let   s say yprediction is the vector
   containing the output of the network for all the training images and
   yactual is the vector containing actual values(also called ground
   truth) of these labeled images. so, if we minimize the distance between
   these two variables, it would be a good indicator of the training. so,
   we define the cost as the average of these distances for all the
   images:
   [1*ackaq8wt3paqehn54b9tia.png]

   this is a very simple example of cost, but in actual training, we use
   much more complicated cost measures, like cross-id178 cost. but
   tensorflow implements many of these costs so we don   t need to worry
   about the details of these costs at this point in time.

   after training is done, these parameters and architecture will be saved
   in a binary file(called model). in production set-up when we get a new
   image of dog/cat to classify, we load this model in the same network
   architecture and calculate the id203 of the new image being a
   cat/dog. this is called id136 or prediction.

   for computational simplicity, not all training data is fed to the
   network at once. rather, let   s say we have total 1600 images, we divide
   them in small batches say of size 16 or 32 called batch-size. hence, it
   will take 100 or 50 rounds(iterations) for complete data to be used for
   training. this is called one epoch, i.e.in one epoch the networks sees
   all the training images once. there are a few more things that are done
   to improve accuracy but let   s not worry about everything at once.

part-2: tensorflow tutorial -> building a small neural network based image
classifier:

   network that we will implement in this tutorial is smaller and simpler
   (than the ones that are used to solve real-world problems) so that you
   can train this on your cpu as well. while training, images from both
   the classes(dogs/cats) are fed to a convolutional layer which is
   followed by 2 more convolutional layers. after convolutional layers, we
   flatten the output and add two fully connected layer in the end. the
   second fully connected layer has only two outputs which represent the
   id203 of an image being a cat or a dog.
   [1*mcw9e9nyyxd6_cykndrzqa.png]

a) pre-requisites:

    1. opencv: we use opencv to read images of cats/dogs so you will have
       to install it.
    2. shape function: if you have multi-dimensional tensor in tf, you can
       get the shape of it by doing this

   iframe: [13]/media/d3566bd1e46106543051c43cd8524824?postid=fbe4028b0df6

   output will be: array([ 16, 128, 128, 3], dtype=int32)

   you can reshape this to a new 2d tensor of shape[16 128*128*3]= [16
   49152].

   iframe: [14]/media/382b0c277b86bebe9feebac94a2a2fb6?postid=fbe4028b0df6

   output: array([16, 49152], dtype=int32)

   3. softmax: is a function that converts k-dimensional vector    x   
   containing real values to the same shaped vector of real values in the
   range of (0,1), whose sum is 1. we shall apply the softmax function to
   the output of our convolutional neural network in order to, convert the
   output to the id203 for each class.
   [1*mc_umoe2yli5jvg6vpngeq.png]

b) reading inputs:

   i have used 1336 images of dogs and cats each from a dataset created
   using google search results but you could use any n image folders on
   your computer which contain different kinds of objects. typically, we
   divide our input data into 3 parts:
    1. training data: we shall use 80% i.e. 1069 images for training.
    2. validation data: 20% images will be used for validation. these
       images are taken out of training data to calculate accuracy
       independently during the training process.
    3. test set: separate independent data for testing which has around
       400 images. sometimes due to something called overfitting; after
       training, neural networks start working very well on the training
       data(and very similar images) i.e. the cost becomes very small, but
       they fail to work well for other images. for example, if you are
       training a classifier between dogs and cats and you get training
       data from someone who takes all images with white backgrounds. it   s
       possible that your network works very well on this validation
       data-set, but if you try to run it on an image with a cluttered
       background, it will most likely fail. so, that   s why we try to get
       our test-set from an independent source.

   iframe: [15]/media/80abeebe9dce5c33d6f4c1a8aae55ec6?postid=fbe4028b0df6

   dataset is a class that i have created to read the input data. this is
   a simple python code that reads images from the provided training and
   testing data folders.

   the objective of our training is to learn the correct values of
   weights/biases for all the neurons in the network that work to do
   classification between dog and cat. the initial value of these weights
   can be taken anything but it works better if you take normal
   distributions(with mean zero and small variance). there are other
   methods to initialize the network but normal distribution is more
   prevalent. let   s create functions to create initial weights quickly
   just by specifying the shape.

   iframe: [16]/media/4e23362aec772555cd8c8de7e945884f?postid=fbe4028b0df6

c) creating network layers:

    1. building convolution layer in tensorflow:

   tf.nn.conv2d function can be used to build a convolutional layer which
   takes these inputs:

   input= the output(activation) from the previous layer. this should be a
   4-d tensor. typically, in the first convolutional layer, you pass n
   images of size width*height*num_channels, then this has the size [n
   width height num_channels]

   filter= trainable variables defining the filter. we start with a random
   normal distribution and learn these weights. it   s a 4d tensor whose
   specific shape is predefined as part of network design. if your filter
   is of size filter_size and input fed has num_input_channels and you
   have num_filters filters in your current layer, then filter will have
   following shape:

   [filter_size filter_size num_input_channels num_filters]

   strides = defines how much you move your filter when doing convolution.
   in this function, it needs to be a tensor of size>=4 i.e. [batch_stride
   x_stride y_stride depth_stride]. batch_stride is always 1 as you don   t
   want to skip images in your batch. x_stride and y_stride are same
   mostly and the choice is part of network design and we shall use them
   as 1 in our example. depth_stride is always set as 1 as you don   t skip
   along the depth.

   padding=same means we shall 0 pad the input such a way that output x,y
   dimensions are same as that of input.

   after convolution, we add the biases of that neuron, which are also
   learnable/trainable. again we start with random normal distribution and
   learn these values during training.

   now, we apply max-pooling using tf.nn.max_pool function that has a very
   similar signature as that of conv2d function.

   iframe: [17]/media/d65c02c501f97f9566ac2d228568e20f?postid=fbe4028b0df6

   notice that we are using k_size/filter_size as 2*2 and stride of 2 in
   both x and y direction. if you use the formula (w2= (w1-f)/s +1;
   h2=(h1-f)/s +1 ) mentioned earlier we can see that output is exactly
   half of input. these are most commonly used values for max pooling.

   finally, we use a relu as our activation function which simply takes
   the output of max_pool and applies relu using tf.nn.relu

   all these operations are done in a single convolution layer. let   s
   create a function to define a complete convolutional layer.

   iframe: [18]/media/d2b8d2b81dd40e3434470e350d92ce7e?postid=fbe4028b0df6

   2. flattenig layer:

   the output of a convolutional layer is a multi-dimensional tensor. we
   want to convert this into a one-dimensional tensor. this is done in the
   flattening layer. we simply use the reshape operation to create a
   single dimensional tensor as defined below:

   iframe: [19]/media/ee98a2b3628086d28edae46e08a10106?postid=fbe4028b0df6

   3. fully connected layer:

   now, let   s define a function to create a fully connected layer. just
   like any other layer, we declare weights and biases as random normal
   distributions. in fully connected layer, we take all the inputs, do the
   standard z=wx+b operation on it. also sometimes you would want to add a
   non-linearity(relu) to it. so, let   s add a condition that allows the
   caller to add relu to the layer.

   iframe: [20]/media/e5d55d7460928916d04b7346b08d1f55?postid=fbe4028b0df6

   so, we have finished defining the building blocks of the network.

   4. placeholders and input:

   now , let   s create a placeholder that will hold the input training
   images. all the input images are read in dataset.py file and resized to
   128 x 128 x 3 size. input placeholder x is created in the shape of
   [none, 128, 128, 3]. the first dimension being none means you can pass
   any number of images to it. for this program, we shall pass images in
   the batch of 16 i.e. shape will be [16 128 128 3]. similarly, we create
   a placeholder y_true for storing the predictions. for each image, we
   have two outputs i.e. probabilities for each class. hence y_pred is of
   the shape [none 2] (for batch size 16 it will be [16 2]).

   iframe: [21]/media/d4d7f7e788dbfee99e36a7b95e57df46?postid=fbe4028b0df6

   5. network design:

   we use the functions defined above to create various layers of the
   network.

   iframe: [22]/media/eb2612be9f364a0784db2216c0038e1b?postid=fbe4028b0df6

   6. predictions:

   as mentioned above, you can get the id203 of each class by
   applying softmax to the output of fully connected layer.
y_pred = tf.nn.softmax(layer_fc2,name=   y_pred   )

   y_pred contains the predicted id203 of each class for each input
   image. the class having higher id203 is the prediction of the
   network.
y_pred_cls = tf.argmax(y_pred, dimension=1)

   now let   s define the cost that will be minimized to reach the optimum
   value of weights. we will use a simple cost that will be calculated
   using a tensorflow function softmax_cross_id178_with_logits which
   takes the output of last fully connected layer and actual labels to
   calculate cross_id178 whose average will give us the cost.

   iframe: [23]/media/be757b5f51b438d09088b9745bddf82d?postid=fbe4028b0df6

   7. optimization:

   tensorflow implements most of the optimisation functions. we shall use
   adamoptimizer for gradient calculation and weight optimization. we
   shall specify that we are trying to minimise cost with a learning rate
   of 0.0001.
optimizer = tf.train.adamoptimizer(learning_rate=1e-4).minimize(cost)

   iframe: [24]/media/66a4305576877e5fa2e0f867f273f9c1?postid=fbe4028b0df6

   where next_batch is a simple python function in dataset.py file that
   returns the next 16 images to be passed for training. similarly, we
   pass the validation batch of images independently to in another
   session.run() call.

   iframe: [25]/media/edc1b12618ee024e6556a235cbcab6b3?postid=fbe4028b0df6

   note that in this case, we are passing cost in the session.run() with a
   batch of validation images as opposed to training images. in order to
   calculate the cost, the whole network(3 convolution+1 flattening+2 fc
   layers) will have to be executed to produce layer_fc2(which is required
   to calculate cross_id178, hence cost). however, as opposed to
   training, this time optimization optimizer =
   tf.train.adamoptimizer(learning_rate=1e-4).minimize(cost) will not be
   run(as we only have to calculate cost). this is what changes the
   gradients and weights and is very computationally expensive. we can
   calculate the accuracy on validataion set using true labels(y_true) and
   predicted labels(y_pred).

   iframe: [26]/media/5e5bcb3bd93d5d09974a9d8842e6fb7a?postid=fbe4028b0df6

   we can calculate the validation accuracy by passing accuracy in
   session.run() and providing validation images in a feed_dict.
val_acc = session.run(accuracy,feed_dict=feed_dict_validate)

   similarly we also report the accuracy for the training images.
acc = session.run(accuracy, feed_dict=feed_dict_train)

   as, training images along with labels are used for training, so in
   general training accuracy will be higher than validation. we report
   training accuracy to know that we are at least moving in the right
   direction and are at least improving accuracy in the training dataset.
   after each epoch, we report the accuracy numbers and save the model
   using saver object in tensorflow.

   iframe: [27]/media/edf82ad62ad264b51a09453f9b898505?postid=fbe4028b0df6

   so, this is how the complete train function looks like:

   iframe: [28]/media/ecfb1fc8f06d580899206f758659b754?postid=fbe4028b0df6

   the complete code is [29]here on my github profile.
   [1*-prlc0iyio2rnepdcsr3eq.png]

   this is a small network and is not state-of-the-art to build an image
   classifier but it   s very good for learning specially when you are just
   getting started. for our training, we get more than 80% accuracy on
   validation set. as we save the model during training, we shall use this
   to run on our own images.

prediction:

   after you are done with training, you shall notice that there are many
   new files in the folder:
    1. dogs-cats-model.meta
    2. dogs-cats-model.data-00000-of-00001
    3. dogs-cats-model.index
    4. checkpoint

   file dogs-cats-model.meta contains the complete network graph and we
   can use this to recreate the graph later. we shall use a saver object
   provided by tensorflow to do this.

   iframe: [30]/media/dac3aa7185b8164184ccd3dc407e23ef?postid=fbe4028b0df6

   the file dogs-cats-model.data-00000-of-00001 contains the trained
   weights(values of variables) of the network. so, once we have recreated
   the graph, we shall restore the weights.

   iframe: [31]/media/c0cb4c93a1b5c740522447cd548ae441?postid=fbe4028b0df6

   in order to get the prediction of the network, we need to read &
   pre-process the input image in the same way(as training), get hold of
   y_pred on the graph and pass it the new image in a feed dict. so, let   s
   do that:

   iframe: [32]/media/c2e73968fa8d76cd5d55f5529f503f26?postid=fbe4028b0df6

   output contains the probabilities of the input image being a dog or a
   cat. in this example, id203 of being dog is much higher than that
   of cat.

   congratulations! you have learnt how to build and train an image
   classifier using convolutional neural networks.

   the complete code is available [33]here. please let me know your
   questions and feedback in the comments below. these comments and
   feedback are my motivation to work harder for my future projects and
   posts     .

     * [34]machine learning
     * [35]tensorflow
     * [36]convolutional network
     * [37]deep learning
     * [38]image recognition

   (button)
   (button)
   (button) 3 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of manan oza

[40]manan oza

   student

     * (button)
       (button) 3
     * (button)
     *
     *

   [41]go to the profile of manan oza
   never miss a story from manan oza, when you sign up for medium.
   [42]learn more
   never miss a story from manan oza
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fbe4028b0df6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@mananoza/tensorflow-tutorial-image-classifier-using-convolutional-neural-network-fbe4028b0df6&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@mananoza/tensorflow-tutorial-image-classifier-using-convolutional-neural-network-fbe4028b0df6&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@mananoza?source=post_header_lockup
  10. https://medium.com/@mananoza
  11. https://www.tensorflow.org/get_started/
  12. http://image-net.org/challenges/lsvrc/
  13. https://medium.com/media/d3566bd1e46106543051c43cd8524824?postid=fbe4028b0df6
  14. https://medium.com/media/382b0c277b86bebe9feebac94a2a2fb6?postid=fbe4028b0df6
  15. https://medium.com/media/80abeebe9dce5c33d6f4c1a8aae55ec6?postid=fbe4028b0df6
  16. https://medium.com/media/4e23362aec772555cd8c8de7e945884f?postid=fbe4028b0df6
  17. https://medium.com/media/d65c02c501f97f9566ac2d228568e20f?postid=fbe4028b0df6
  18. https://medium.com/media/d2b8d2b81dd40e3434470e350d92ce7e?postid=fbe4028b0df6
  19. https://medium.com/media/ee98a2b3628086d28edae46e08a10106?postid=fbe4028b0df6
  20. https://medium.com/media/e5d55d7460928916d04b7346b08d1f55?postid=fbe4028b0df6
  21. https://medium.com/media/d4d7f7e788dbfee99e36a7b95e57df46?postid=fbe4028b0df6
  22. https://medium.com/media/eb2612be9f364a0784db2216c0038e1b?postid=fbe4028b0df6
  23. https://medium.com/media/be757b5f51b438d09088b9745bddf82d?postid=fbe4028b0df6
  24. https://medium.com/media/66a4305576877e5fa2e0f867f273f9c1?postid=fbe4028b0df6
  25. https://medium.com/media/edc1b12618ee024e6556a235cbcab6b3?postid=fbe4028b0df6
  26. https://medium.com/media/5e5bcb3bd93d5d09974a9d8842e6fb7a?postid=fbe4028b0df6
  27. https://medium.com/media/edf82ad62ad264b51a09453f9b898505?postid=fbe4028b0df6
  28. https://medium.com/media/ecfb1fc8f06d580899206f758659b754?postid=fbe4028b0df6
  29. https://github.com/ozamanan/tf-image-classification.git
  30. https://medium.com/media/dac3aa7185b8164184ccd3dc407e23ef?postid=fbe4028b0df6
  31. https://medium.com/media/c0cb4c93a1b5c740522447cd548ae441?postid=fbe4028b0df6
  32. https://medium.com/media/c2e73968fa8d76cd5d55f5529f503f26?postid=fbe4028b0df6
  33. https://github.com/ozamanan/tf-image-classification.git
  34. https://medium.com/tag/machine-learning?source=post
  35. https://medium.com/tag/tensorflow?source=post
  36. https://medium.com/tag/convolutional-network?source=post
  37. https://medium.com/tag/deep-learning?source=post
  38. https://medium.com/tag/image-recognition?source=post
  39. https://medium.com/@mananoza?source=footer_card
  40. https://medium.com/@mananoza
  41. https://medium.com/@mananoza
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/fbe4028b0df6/share/twitter
  45. https://medium.com/p/fbe4028b0df6/share/facebook
  46. https://medium.com/p/fbe4028b0df6/share/twitter
  47. https://medium.com/p/fbe4028b0df6/share/facebook
