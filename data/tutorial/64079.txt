   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

how to think in graphs: an illustrative introduction to id207 and its
applications

   [11]go to the profile of vardan grigoryan (vardanator)
   [12]vardan grigoryan (vardanator) (button) blockedunblock (button)
   followfollowing
   feb 21, 2018
   [1*z6qfdmozczheitxecpoqjq.png]
   id207 can be difficult to understand

   id207 represents one of the most important and interesting areas
   in computer science. but at the same time it   s one of the most
   misunderstood (at least it was to me).

   understanding, using and thinking in graphs makes us better
   programmers. at least that   s how we   re supposed to think. a graph is a
   set of vertices v and a set of edges e, comprising an ordered pair
   g=(v, e).

   while trying to studying id207 and implementing some algorithms,
   i was regularly getting stuck, just because it was so boring.

   the best way to understand something is to understand its applications.
   in this article, we   re going to demonstrate various applications of
   id207. but more importantly, these applications will contain
   detailed illustrations. so lets get started and dive in.

   while this approach might seem too detailed (to seasoned programmers),
   but believe me, as someone who was once there and tried to understand
   id207, detailed explanations are always preferred over succinct
   definitions.

   so, if you   ve been looking for a    id207 and everything about it
   tutorial for absolute unbelievable dummies   , then you   ve come to the
   right place. or at least i hope. so lets get started and dive in.
   [1*oq1wosxcfpi8n8tirhslwq.png]
   he meant monte cristo

table of contents

     * [13]disclaimers
     * [14]seven bridges of k  nigsberg
     * graph representation: intro
     * [15]intro to graph representation and binary trees (airbnb example)
     * [16]graph representation: outro
     * [17]twitter example: tweet delivery problem
     * [18]graph algorithms: intro
     * [19]netflix and amazon: inverted index example
     * [20]traversals: dfs and bfs
     * [21]uber and the shortest path problem (dijkstra   s algorithm)
     __________________________________________________________________

disclaimers

   disclaimer 1: i am not an expert in cs, algorithms, data structures and
   especially in id207. i am not involved in any project for the
   companies discussed in this article. solutions to the problems are not
   final and could be improved drastically. if you find any issue or
   something unreasonable, you are more than welcome to leave a comment.
   if you work at one of the mentioned companies or are involved in
   corresponding software projects, please respond with the actual
   solution (it will be helpful to others). to all others, be patient
   readers, this is a pretty long article.

   disclaimer 2: this article is somewhat different in the style that
   information is provided. sometimes it might seem a bit digressed from
   the sub-topic, but patient readers will eventually find themselves with
   a complete understanding of the bigger picture.

   disclaimer 3: this article is written for a broad audience of
   programmers. while having junior programmers as the target audience, i
   hope it will be interesting to experienced professionals as well.

seven bridges of k  nigsberg

   let   s start with something that i used to regularly encounter in graph
   theory books that discuss    the origins of id207   , the [22]seven
   bridges of k  nigsberg (not really sure, but you can pronounce it as
      qyonigsberg   ). there were seven bridges in [23]kaliningrad, connecting
   two big islands surrounded by the pregolya river and two portions of
   mainlands divided by the same river.
   [1*yiwa1lzpj6xhaxw3g9kcqa.png]
   [24]our area of interest

   in the 18th century this was called k  nigsberg (part of prussia) and
   the area above had a lot more bridges. the problem or just a brain
   teaser with k  nigsberg   s bridges was to be able to walk through the
   city by crossing all the seven bridges only once. they didn   t have an
   internet connection at that time, so it should have been entertaining.
   here   s the illustrated view of the seven bridges of k  nigsberg in 18th
   century.
   [1*_8_s3lgba5fylf9eppxpaa.png]
   seven bridges of k  nigsberg

   try it. see if you can walk through the city by crossing each bridge
   only once.
     * there should not be any uncrossed bridge(s).
     * each bridge must not be crossed more than once.

   if you are familiar with this problem, you know that it   s impossible to
   do it. although you were trying hard enough and you may try even harder
   now, you   ll eventually give up.
   [1*c3vl36-jiebzq_rsnyml5a.jpeg]
   leonhard euler (photo from wikipedia)

   sometimes it   s reasonable to give up fast. that   s how euler solved this
   problem - he gave up pretty soon. instead of trying to solve it, he
   adopted a different approach of trying to prove that it   s not possible
   to walk through the city by crossing each bridge one and only time.

   let   s try to understand how euler was thinking and how he came up with
   the solution (if there isn   t a solution, it still needs a proof). that
   is a real challenge here, because walking through the thought process
   of such a venerable mathematician is kind of dishonorable. (venerable
   so much that [25]knuth and friends dedicated their book to [26]leonhard
   euler). we rather will pretend to    think like euler   . let   s start with
   picturing the impossible.
   [1*rpipdvczbwm0hy519yozia.png]

   there are four distinct places, two islands and two parts of mainland.
   and seven bridges. it   s interesting to find out if there is any pattern
   regarding the number of bridges connected to islands or mainland (we
   will use the term    land    to refer to the four distinct places).
   [1*tqzvw18zspyiuofy5zjjjg.png]
   number of bridges

   at a first glance, there seems to be some sort of a pattern. there are
   an odd number of bridges connected to each land. if you have to cross
   each bridge once, then you can enter a land and leave it if it has 2
   bridges.
   [1*jdq0lcode1ln3_27yvcnjg.png]
   [1*tqshipv13iktzizafiej_q.png]
   [1*vs7zeh7lhhaaxglxh2lyhw.png]
   examples of 2 bridge lands

   it   s easy to see in the illustrations above that if you enter a land by
   crossing one bridge, you can always leave the land by crossing its
   second bridge. whenever a third bridge appears, you won   t be able to
   leave a land once you enter it by crossing all its bridges. if you try
   to generalize this reasoning for a single piece of land, you   ll be able
   to show that, in case of an even number of bridges it   s always possible
   to leave the land and in case of an odd number of bridges it isn   t. try
   it in your mind!

   let   s add a new bridge to see how the number of overall connected
   bridges changes and whether it solves the problem.
   [1*olodxmfjsej28jf70sg9dw.png]
   notice the new bridge

   now that we have two even (4 and 4) and two odd (3 and 5) number of
   bridges connecting the four pieces of land, let   s draw a new route with
   the addition of this new bridge.
   [1*0tigvhob7ugpedzx1znlug.png]
   wow

   we saw that the number of even and odd number of bridges played a role
   in determining if the solution was possible. here   s a question. does
   the number of bridges solve the problem? should it be even all the
   time? turns out that it   s not the case. that   s what euler did. he found
   a way to show that the number of bridges matter. and more
   interestingly, the number of pieces of land with an odd number of
   connected bridges also matters. that   s when euler started to    convert   
   lands and bridges into something we know as graphs. here   s how a graph
   representing the k  nigsberg bridges problem could look like (note that
   our    temporarily    added bridge isn   t there).
   [1*dz0h0t88ztzlnqhgalpztg.png]
   lines are a bit twisty

   one important thing to note is the generalization/abstraction of a
   problem. whenever you solve a specific problem, the most important
   thing is to generalize the solution for similar problems. in this
   particular case, euler   s task was to generalize the bridge crossing
   problem to be able to solve similar problems in the future, i.e. for
   all the bridges in the world. visualization also helps to view the
   problem at a different angle. the following graphs are all various
   representations of the same k  nigsberg bridge problem shown above.
   [1*h8-y7kfly81iez61y6rhpa.png]

   so yes, visually graphs are a good choice for picturing problems. but
   now we need to find out how the k  nigsberg problem can be solved using
   graphs. pay attention to the number of lines coming out of each circle.
   and yes, let   s name them as seasoned professionals would do, from now
   on we will call circles, vertices and the lines connecting them, edges.
   you might   ve seen letter notations, v for (vendetta?) vertex, e for
   edge.
   [1*uliw_ftvf-urbtjhfu-mdg.png]

   the next important thing is the so-called degree of a vertex, the
   number of edges incident connected to the vertex. in our example above,
   the number of bridges connected to lands can be expressed as degrees of
   the graph vertex.
   [1*esq5g9vi988ahhxta1cflw.png]

   in his endeavor euler showed that the possibility of a walk through
   graph (city) traversing each edge (bridge) one and only one time is
   strictly dependent on the degrees of vertices (lands). the path
   consisting of such edges called (in his honor) an euler path. the
   length of an euler path is the number of edges. get ready for some
   strict language.     

     an euler path of a finite undirected graph g(v, e) is a path such
     that every edge of g appears on it once. if g has an euler path,
     then it is called an euler graph. [1]

     theorem. a finite undirected connected graph is an euler graph if
     and only if exactly two vertices are of odd degree or all vertices
     are of even degree. in the latter case, every euler path of the
     graph is a circuit, and in the former case, none is. [1]

   [1*v-tkqftu_snzgkjiydkkwa.png]
   [1*mq6gk_iryp3jweik33apqa.png]
   exactly two vertices have an odd degree in the illustration at the
   left, and all vertices are of an odd degree in illustration at
   the right

   i used    euler path    instead of    eulerian path    just to be consistent
   with the referenced books [1] definition. if you know someone who
   differentiates euler path and eulerian path, and euler graph and
   eulerian graph, let them know to leave a comment.

   first of all, let   s clarify the new terms in the above definition and
   theorem.
     * undirected graph - a graph that doesn   t have a particular direction
       for edges.
     * directed graph - a graph in which edges have a particular
       direction.
     * connected graph - a graph where there is no unreachable vertex.
       there must be a path between every pair of vertices.
     * disconnected graph - a graph where there are unreachable vertices.
       there is not a path between every pair of vertices.
     * finite graph - a graph with a finite number of nodes and edges.
     * infinite graph - a graph where an end of the graph in a particular
       direction(s) extends to infinity.

   we   ll discuss some of these terms in the coming paragraphs.

   graphs can be directed and undirected, and that   s one of the
   interesting properties of graphs. you must   ve seen a popular facebook
   vs twitter example for directed and undirected graphs. a facebook
   friendship relation may be easily represented as an undirected graph,
   because if alice is a friend with bob, then bob must be a friend with
   alice, too. there is no direction, both are friends with each other.
   [1*nssblc2ssmmniutbl3jrnq.png]
   undirected graph

   also note the vertex labeled as    patrick   , it is kind of special (he   s
   got no friends), as it doesn   t have any incident edges. it is still a
   part of the graph, but in this case we will say that this graph is not
   connected, it is a disconnected graph (same goes with    john   ,    ashot   
   and    beth    as they are interconnected with each other but separated
   from others). in a connected graph there is no unreachable vertex,
   there must be a path between every pair of vertices.

   contrary to the facebook example, if alice follows bob on twitter, that
   doesn   t require bob to follow alice back. so a    follow    relation must
   have a direction indicator, showing which vertex (user) has a directed
   edge (follows) to the other vertex.
   [1*nogzj2zbrtrwh43ph-xaiq.png]

   now, knowing what is a finite connected undirected graph, let   s get
   back to euler   s graph:
   [1*h_s6hgd0y1udl9pltn8qyw.png]

   so why did we discuss k  nigsberg bridges problem and euler graphs in
   the first place? well, it   s not so boring and by investigating the
   problem and foregoing solution we touched the elements behind graphs
   (vertex, edge, directed, undirected) avoiding a dry theoretical
   approach. and no, we are not done with euler graphs and the problem
   above, yet.     

   we should now move on to the computer representation of graphs as that
   is the topic of interest for us programmers. by representing a graph in
   a computer program, we will be able to devise an algorithm for tracing
   graph path(s), and therefore find out if it is an euler path. before
   that, try to think of a good application for an euler graph (besides
   fiddling around with bridges).

graph representation: intro

   now this is quite a tedious task, so be patient. remember the fight
   between arrays and linked lists? use arrays if you need fast element
   access, use lists if you need fast element insertion/deletion, etc. i
   hardly believe you ever struggled with something like    how to represent
   lists   . well, in case of graphs the actual representation is really
   bothering, because first you should decide how exactly are you going to
   represent a graph. and believe me, you are not going to like this.
   adjacency list, adjacency matrix, maybe edge lists? toss a coin.

   you should have tossed hard, because we are starting with a tree. you
   must have seen a binary tree (or bt for short) at least once (the
   following is not a binary search tree).
   [1*xwon44k1_pdtg8pwzeutxw.png]
   just a sample

   just because it consists of vertices and edges, it   s a graph. you also
   may recall how most commonly a binary tree is represented (at least in
   textbooks).

   iframe: [27]/media/53aae7ddbd72868c01a72cad737e5c43?postid=1c96572a1401

   it might seem too basic for people who are already familiar with binary
   trees, but i still have to illustrate it to make sure we are on the
   same page (note that we are still dealing with pseudocode).

   iframe: [28]/media/1ac1c0208c644f4dde7546f35a1bf6a1?postid=1c96572a1401

   if you are new to trees, read the pseudocode above carefully, then
   follow the steps in the illustration below.
   [1*qgss9h5tjrkc226e1tr5ba.png]
   colors are just for bright visualization

   while a binary tree is a simple    collection    of nodes, each of which
   has left and right child nodes. a binary search tree is much more
   useful as it applies one simple rule which allows fast key lookups.
   binary search trees (bst) keep their keys in sorted order. you are free
   to implement your bt with any rule you want (although it might change
   its name based on the rule, for instance, min-heap or max-heap). the
   most important expectation for a bst is that it satisfies the binary
   search property (that   s where the name comes from). each node   s key
   must be greater than any key in its left sub-tree and less than any key
   in its right sub-tree.

   i   d like to point out a very interesting point regarding the statement
      greater than    that   s crucial to understand how bst   s function.
   whenever you change the property to    greater than or equal   , your bst
   will be able to save duplicate keys when inserting new nodes, otherwise
   it will keep only nodes with unique keys. you can find really good
   articles on the web about binary search trees. we won   t be providing a
   full implementation of a binary search tree, but for the sake of
   consistency, we   ll illustrate a simple binary search tree here.
   [1*gb8tsj_motcivzlyc2hkmq.png]

intro to graph representation and binary trees (airbnb example)

   trees are very useful data structures. you might not have implemented a
   tree from scratch in your projects. but you   ve probably used them even
   without noticing. let   s look at an artificial yet valuable example and
   try to answer the    why    question,    why use a binary search tree in the
   first place   .

   as you   ve noticed, there is a    search    in binary search tree. so
   basically, everything that needs a fast lookup, should be placed in a
   binary search tree.    should    doesn   t mean must, the most important
   thing to keep in mind in programming is to solve a problem with proper
   tools. there are tons of cases where a simple linked list with its o(n)
   lookup might be more preferable than a bst with its o(logn) lookup.

   typically we would use a library implementation of a bst, most likely
   std::set or std::map in c++. however in this tutorial we are free to
   reinvent our own wheel. bsts are implemented in almost any
   general-purpose programming language library. you can find them in the
   corresponding documentation of your favorite language. approaching a
      real-life example   , here   s the problem we   ll try to tackle - airbnb
   home search.
   [1*2wwx3k9jjhelx2d8hyz39q.png]
   a glimpse on airbnb homes search

   how do we search for homes based on some query with a bunch of filters
   as fast as possible. this is a hard task. it becomes harder if we
   consider that airbnb stores [29]4 millions listings.
   [1*-fxzco9efvu5cpt-ubsk3q.png]

   so when users search for homes, there is a chance that they might
      touch    4 million records stored in the database. sure the results are
   limited to the    top listings    shown on the website   s home page and a
   user almost is never curious    enough    to view millions of listings. i
   don   t have any analytics regarding airbnb, but we can use a powerful
   tool in programming called    assumptions   . so we will assume that a
   single user finds a good home by viewing at most ~1k homes.

   the most important factor here is the number of real-time users, as it
   makes a difference in data structures and database(s) choices and the
   project architecture overall. as obvious as it might seem, if there are
   just 100 users overall, then we may not bother at all.

   on the contrary, if the number of users overall and real-time users in
   particular is far beyond the million threshold, we have to think really
   wisely about each decision.    each    is used exactly right, that   s why
   companies hire the best while striving for excellence in service
   provision.

   google, facebook, airbnb, netflix, amazon, twitter, and many others
   deal with huge amounts of data and the right choice to serve millions
   of bytes of data each second to millions of real-time users starts from
   hiring the right engineers. that   s why we, the programmers, struggle
   with these data structures, algorithms and problem solving in possible
   interviews, because all they need is the engineer having the ability to
   solve such big problems in the fastest and most efficient possible way.

   so here   s a use case. a user visits the home page (we   re still talking
   about airbnb) and tries to filter out homes to find the best possible
   fit. how would we deal with this problem? (note that this problem is
   rather backend-side, so we won   t care about front-end or the network
   traffic or https over http or amazon ec2 over home cluster and so on).

   first of all, as we are already familiar with one of the most powerful
   tools in a programmers    inventory (talking about assumptions rather
   than abstractions), let   s start with a few assumptions:
     * we   re dealing with data that completely fits in the ram.
     * our ram is big enough.

   big enough to hold, id48, how much? well that   s a good question. how
   much memory will be required to store the actual data. if we are
   dealing with 4 million units of data (again, am assumption), and if we
   probably know each unit   s size, then we can easily derive the required
   memory size, i.e. 4m * sizeof(one_unit).

   let   s consider a    home    object and its properties. actually, let   s
   consider at least those properties that we will deal with while solving
   our problem (a    home    is our unit). we will represent it as a c++
   structure in some pseudocode. you can easily convert it to a mongodb
   schema object or anything you want. we just discuss the property names
   and types (try to think about using bitfields or bitsets for space
   economy).

   iframe: [30]/media/7b6bdfc58f0d6eefaa8c4acc63fa51a0?postid=1c96572a1401

   the above structure is not perfect (obviously) and there are many
   assumptions and/or incomplete parts. i just looked at airbnb   s filters
   and devised property lists that should exist to satisfy search queries.
   it   s just an example.

   now we should calculate how many bytes in memory will take each
   airbnbhome object.
     * home name -name is a wstring to support multilingual names/titles,
       which means each character will take 2 bytes (we may not bother
       with character size if we would use other language, but in c++ char
       is 1-byte character and wchar is 2-byte character). a quick look at
       airbnb   s listings allows us to assume that the name of a home
       should take up to 100 characters (though mostly it is around 50,
       rather than 100), we   ll assume 100 characters as a maximum value,
       which leads to ~200 bytes of memory. uint is 4 bytes, uchar is 1
       byte, ushort is 2 bytes (again, in our assumptions).
     * photos - photos are residing within some storage service, like
       amazon s3 (as far as i know, this assumption is most likely to be
       true for airbnb, but again, amazon s3 is just an assumption)
     * photo urls - we have those photo urls, and considering the fact
       that there is no standard size limit on the urls, but there is in
       fact a well-known limit of 2083 characters, we   ll take it as a max
       size of any url. so taking into account that each home has 5 photos
       in average, it would take up to ~10kb.
     * photo ids - let   s have a rethink about this. usually storage
       services serve content with the same base urls, like
       http(s)://s3.amazonaws.com/<bucket>/<object>, i.e. there is a
       common pattern for constructing urls and we need to store only the
       actual photo id. let   s say we use some unique id generator, which
       returns a 20 byte unique string id where photo objects and the url
       pattern for particular photo looks like
       https://s3.amazonaws.com/some-know-bucket/<unique-photo-id>. this
       gives us good space efficiency, so for storing string ids of five
       photos we will need only 100 bytes of memory.
     * host id - the same    trick    (above) could be done with the host_id,
       i.e. the user id who hosts the home, takes 20 bytes of memory
       (actually we could just use integer ids for users, but considering
       that some db systems like mongodb have rather specific unique id
       generator, we   re assuming a 20 byte length string id as some
          median    value which fits into almost any db system with a little
       change. mongo   s id length is 24 bytes). and finally, we   ll take a
       bitset of up to 32 bits in size as a 4 bytes object and a bitset of
       size between 32 and 64 bits, as a 8 byte object. mind the
       assumptions. we used bitset in this example for any property that
       expresses an enum, but is able to take more than one value, in
       other words a kind of multiple choice checkbox.

   [1*yctk591iy-mzqte5jnxfrg.png]
   airbnb house amenities

   amenities - each airbnb home keeps a list of available amenities, e.g.
      iron   ,    washer   ,    tv   ,    wifi   ,    hangers   ,    smoke detector    and even
      laptop friendly workspace    and so on. there might be more than 20
   amenities, we stick to the 20 just because it   s the number of
   filterable amenities on the airbnb filters page. bitset saves us some
   good space, if we keep proper ordering for amenities. for instance, if
   a home has all above mentioned amenities (see checked ones in the
   screenshot), we will just set a bit at corresponding position in the
   bitset.
   [1*lj3odnq70fdpot_lkfdd2w.png]
   bitset allows to save 20 different values using just 20 bits

   for example, checking if a home has a    washer   :

   iframe: [31]/media/22eaf78f74af2761568ab3e47a55a4f1?postid=1c96572a1401

   or a little more professionally:

   iframe: [32]/media/9a129659839e3dee564ed9fb0e3d6887?postid=1c96572a1401

   you can improve the code as much as you want (and fix compile errors).
   we just wanted to emphasize the idea behind bitsets in this problem
   context.
     * house rules, home type - the same idea (that we implemented for the
       amenities field) goes with    house rules   ,    home type    and others.
     * country code, city name - finally, the country code and city name.
       as mentioned in the comments of the code above (see remarks), we
       won   t store latitude and longitude to avoid geo-spatial queries (a
       subject of another article). instead, we save country code and city
       name to narrow down the search by a location (omitting streets for
       the sake of simplicity, please forgive me). [33]country code could
       be represented as 2 characters, 3 characters or 3 digits, we   ll
       save a numeric representation and will use an ushort for it.
       (un)fortunately there are many more cities than countries, so we
       can   t use a    city code    (though we can make one for internal use).
       instead we   ll store actual city name, preserving 50 bytes in
       average for a city name and for super-specific cases like
       [34]taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronuk
       upokaiwhenuakitanatahu (85 letter city). we better use an
       additional boolean variable which indicates that this is that
       specific super-long city (don   t try to pronounce it). so, keeping
       in mind the memory overhead of strings and vectors. we   ll add an
       additional 32 bytes (just in case) to the final size of the struct.
       we also will assume that we work on a 64-bit system, although we
       chose very compact values for int and short.

   iframe: [35]/media/083d9e2d4f5ecde3bc0fed0f0af2b620?postid=1c96572a1401

   so, 420+ bytes with an overhead of 32 bytes, 452 bytes and considering
   the fact that some of you might just be obsessed with the aligning
   factor, let   s round up it to 500 bytes. so each    home    object takes up
   to 500 bytes, and for all home listings (there could be some confusing
   moments with the listings count and actual home count, just let me know
   if i got something wrong), 500 bytes * 4 million = 1.86gb ~ 2gb. seems
   plausible. we made many assumptions while constructing the struct,
   making it cheaper to save in memory, i really expected much more than 2
   gigabytes and if i did a mistake in calculations, let me know. anyway,
   moving forward, so whatever we gonna do with this data, we will need at
   least 2 gb of memory. if you got bored, deal with it. we are just
   starting.

   now the hardest part of the task. choosing the right data structure for
   this problem (filter the listings as efficiently as possible) is not
   the hardest task. the hardest task is (for me) to search listings by a
   bunch of filters. if there would be just one search key (just one
   filter) we would easily solve it. suppose the only thing users care is
   the price, so all we need is to find airbnbhome objects with prices
   falling in the provided range. if we   ll use a binary search tree for
   that, here   s how it might look.
   [1*6cm5jxjsovsulzquqno2ww.png]

   if you imagine all 4 millions objects, this tree grows very very big.
   by the way, the memory overhead grows as well, just because we used a
   bst to store objects. as each parent tree node has two additional
   pointers to its left and right child it adds up to 8 additional bytes
   for each child pointer (assuming a 64-bit system). for 4 million nodes
   it sums up to ~62 mb, which in comparison to 2gb of object data looks
   quite small, though it is not something that we can    omit    easily.

   the tree in the last illustration so far shows that any item can be
   easily found in o(logn) complexity. if you aren   t familiar or are not
   sure enough to chit-chat in big-ohs, we   ll clarify it below, otherwise
   skip the complexity subsection.

   algorithmic complexity - let   s make this quick as there will be a long
   and detailed explanation in an upcoming article:    algorithmic
   complexity and software performance: the missing manual   . for most of
   the cases finding the big o complexity for an algorithm is somewhat
   easy. first thing to note is that we always consider the worst case,
   i.e. the maximum number of operations that an algorithm does to produce
   a positive outcome (to solve the problem).

   suppose an array has 100 elements in an unsorted order. how many
   comparisons would it take to find an element (also taking into account
   that the required element could be missing)? it will take up to 100
   comparisons as we should compare each element   s value with the value we
   are looking for. and despite the fact that the element might be the
   first element in the array (leading to a single comparison), we will
   consider only the worst possible case (element is either missing or is
   residing at the last position of the array).
   [1*ty0mol97rjvfnecai3vxqq.png]

   the point of    calculating    algorithmic complexity is finding a
   dependency between the number of operations and the size of input, for
   instance the array above had 100 elements and the number of operations
   were also 100, if the number of array elements (its input) will
   increase to 1423, the number of operations to find any element will
   also increase to 1423 (the worst case). so the thin line between input
   and number of operations is clear in this case, it is so-called linear,
   the number of operations grows as much as grows array   s input. growth.
   that   s the key point in complexity, we say that searching for an
   element in an unsorted array takes o(n) time to emphasize that the
   process of finding it will take up to n operations (or even up to n
   operations times some constant value such as 3n). on the other hand,
   accessing any element in an array takes a constant time, i.e. o(1).
   that   s because of an array   s structure. it   s a contiguous data
   structure, and holds elements of the same type (mind js arrays), so
      jumping    to a particular element requires only calculating its
   relative position to the array   s first element.
   [1*i8cuohrcknf8fuznv6-dea.png]

   one thing is very clear. a binary search tree keeps its nodes in sorted
   order. so what would be the algorithmic complexity of searching an
   element in a binary search tree? we should calculate the number of
   operations required to find an element (in the worst case).

   look at the illustration above. when starting our search at the root,
   the first comparison may lead to three cases,
    1. the node is found.
    2. the comparison continues to node   s left sub-tree if the required
       element is less than the node   s value
    3. the comparison continues to the node   s right sub-tree if the value
       we search for is greater than the node   s value.

   at each step we reduce the size of nodes needed to be considered by
   half. the number of operations (i.e. comparisons) needed to find an
   element in the bst equals the height of the tree. the height of a tree
   is the number of nodes on the longest path. in this case it   s 4. and
   the height is [base 2] logn + 1, as shown. so the complexity of search
   is o(logn + 1) = o(logn). this means that searching something in 4
   million nodes requires log4000000 = ~22 comparisons in the worst case.

   back to the tree - element access time in a binary search tree is
   o(logn). why not use hashtables? hashtables have constant access time,
   which makes it reasonable to use hashtables almost everywhere.
   [1*moszx3_liqtpkhuqg5a0iw.png]

   in this problem we must take into account an important requirement. we
   must be able to make range searches, e.g. homes with prices from $80 to
   $162. in case of a bst, it   s easy to get all the nodes in a range just
   by doing an inorder traversal of the tree and keeping a counter. for a
   hashtable it is somewhat expensive which makes it reasonable to stick
   with bsts in this case.

   though there is another spot, which leads us to rethink hashtables. the
   density. prices won   t go up    forever   , most of the homes reside at the
   same price range. look at the screenshot, the histogram shows us the
   real picture of the prices, millions of homes are in the same range
   (+/- $18 - $212), they have the same average price. simple arrays may
   play a good role. assuming the index of an array as the price and the
   value as the list of homes, we might access any price range in constant
   time (well, almost constant). here   s how it looks (way abstract):
   [1*gwjoqqisi3hi5f3ic5p6_q.png]

   just like a hashtable, we are accessing each set of homes by its price.
   all homes having the same price are grouped under a separate bst. it
   will also save us some space if we store home ids instead of the whole
   object defined above (the airbnbhome struct). the most possible
   scenario is to save all homes full objects in a hashtable mapping home
   id to home full object and storing another hashtable (or better, an
   array), which maps prices with homes ids.

   so when users request a price range, we fetch home ids from the price
   table, cut the results to a fixed size (i.e. the pagination, usually
   around 10 - 30 items are shown on one page), fetch the full home
   objects using each home id.

   just keep another thing in mind (think of it in the background).
   balancing is crucial for a bst, because it   s the only guarantee of
   having tree operations done in o(logn). the problem of unbalanced bst
   is obvious when you insert elements in sorted order. eventually, the
   tree becomes just a linked list, which obviously leads to linear-time
   operations. forget this for now, suppose all our trees are perfectly
   balanced. take a look at the illustration above once again. each array
   element represents a big tree. what if we change the illustration to
   something like this:
   [1*cpvsz7h8iugrfdoa4usksq.png]
   more like a graph

   it resembles a    more realistic    graph. this illustration represents the
   most disguised data structures and graphs, which takes us to the next
   section.

graph representation: outro

   the bad news about graphs is that there isn   t a single definition for
   the graph representation. that   s why you can   t find a std::graph in the
   library. we already had a chance to represent a    special    graph called
   bst. the point is, tree is a graph, but graph is not a tree. the last
   illustration shows us that we have a lot of trees under a single
   abstraction,    prices vs homes    and some of the vertices    differ    in
   their type, prices are graph nodes having only the price value and
   refer to the whole tree of home ids (home vertices) that satisfy the
   particular price. it is much like a hybrid data structure, than a
   simple graph that we   re used to seeing in textbook examples.

   that   s the key point in graph representation, there isn   t a fixed and
      de jure    structure for graph representation (unlike bsts with their
   specified node-based representation with left/right child pointers,
   though you can represent a bst with a single array). you can represent
   a graph in the most convenient way you wish (most convenient to
   particular problem), the main thing is that you    see    it as a graph.
   and by    seeing a graph    we mean applying algorithms that are specific
   to graphs.

   what about an n-ary tree, it is more likely to resemble a graph.
   [1*wagdbjyglj73ueporuh1ba.png]

   and the first thing that comes into mind to represent an n-ary tree
   node is something like this:

   iframe: [36]/media/79f48cf78a1a8e787ac6fcf756651a12?postid=1c96572a1401

   this structure represents just a single node of a tree. the full tree
   looks more like this:

   iframe: [37]/media/a860978d734c22164210bf6eb7f56adc?postid=1c96572a1401

   this class is an abstraction around a single tree node named root_ .
   that   s all we need to build a tree of any size. that   s the starting
   point of the tree. for adding a new tree node we need to allocate a
   memory to it and add that node to the root of the tree.

   a graph is much like an n-ary tree, with a slight difference. try to
   spot it.
   [1*cu05nugy1sbx3zzeh8slgw.png]

   is this a graph? no. i mean yes, but it   s the same n-ary tree from the
   previous illustration, just a little rotated. as a rule of a thumb,
   whenever you see a tree (even if it is an apple tree, lemon tree or
   binary search tree), you can be sure that it is also a graph. so,
   devising a structure for a graph node (graph vertex), we can come up
   with the same structure:

   iframe: [38]/media/9119087f1c12d3415bd0c3328416ab65?postid=1c96572a1401

   is this enough to construct a graph? well, no. and here   s why. look at
   these two graphs from previous illustrations, find a difference:
   [1*cpvsz7h8iugrfdoa4usksq.png]
   [1*cu05nugy1sbx3zzeh8slgw.png]
   both are graphs

   the graph in the illustration at the left side has no single point to
      enter    (it   s rather a forest than a single tree), in the contrary, the
   graph in the right illustration doesn   t have unreachable vertices.
   sounds familiar.

     a graph is connected when there is a path between every pair of
     vertices. [[39]wikipedia]

   obviously, there isn   t a path between every pair of vertices for the
      prices vs homes    graph (if it isn   t obvious from the illustration,
   just assume that prices are not connected with each other). as much as
   it   s just an example to show that we aren   t able to construct a graph
   with a single graphnode struct, there are cases that we have to deal
   with disconnected graphs like this. take a look at this class:

   iframe: [40]/media/538b05968609aa8d01e5d253ee10cf2f?postid=1c96572a1401

   just like an n-ary tree is built around a single node (the root node),
   a connected graph also can be built around a root node. it   s said that
   trees are    rooted   , i.e. they have a starting point. a connected graph
   can be represented as a rooted tree (with a couple of more properties),
   it   s already obvious, but keep in mind that the actual representation
   may differ from algorithm to algorithm, from problem to problem even
   for a connected graph. however, considering node-based nature of
   graphs, a disconnected graph can be represented like this:

   iframe: [41]/media/7a1662b7cbfa9a1bcb569e174493ce0e?postid=1c96572a1401

   for graph traversals like dfs/bfs it   s natural to use a tree-like
   representation. helps a lot. however, cases like efficient path tracing
   require a different representation. remember euler   s graph? to track
   down a graph   s    eulerness   , we should trace an euler path within it.
   that means visiting all vertices by traversing each edge only once, and
   when the tracing finishes and we have untraversed edges, then the graph
   doesn   t have an euler path, and therefore is not an euler graph.

   there is even faster method, we can check the degrees of vertices
   (suppose each vertex stores its degree) and just as the definition
   says, if a graph has vertices of odd degree and there aren   t exactly
   two of them, then it is not an euler graph. the complexity of such a
   check is o(|v|), where |v| is the number of graph vertices. we can
   track down odd/even degrees while inserting new edges to increase
   odd/even degree checks to o(1). lightning fast. never mind, we   re just
   going to trace a graph, that   s it. below is both the representation of
   a graph and the trace() function returning a path.

   iframe: [42]/media/0bca1e8a3b9d7f1d6066c9cafcdba082?postid=1c96572a1401

   mind the bugs, bugs are everywhere. this code contains a lot of
   assumptions, for instance, the labeling, so by a vertex we understand a
   string label. sure you can easily update it to be anything you want.
   doesn   t matter in the context of this example. next, the naming. as
   mentioned in the comments, velograph is for vertex edge label only
   graph (i made this up). the point is, this graph representation
   contains a table for mapping a vertex label with edges incident to that
   vertex, and a list of edges containing a pair of vertices (connected by
   a particular edge) and a flag which is used only by the trace()
   function. take a look at the trace() function implementation. it uses
   edge   s flag to mark an already traversed edge (flags should be reset
   after any trace() call).

twitter example: tweet delivery problem

   here   s another representation called an adjacency matrix, which could
   be useful in directed graphs, like one we used for twitter follower
   graph.
   [1*nogzj2zbrtrwh43ph-xaiq.png]
   directed graph

   there are 8 vertices in this twitter example. so all we need to
   represent this graph is a |v|x|v| square matrix (|v| number of rows and
   |v| number of columns). if there is a directed edge from v to u, then
   matrix   s [v][u] is true, otherwise it   s false.
   [1*s--ueetmqg9f8lhhhgfkta.png]
   twitter   s example

   as you can see, this matrix is way too sparse, its trade off is the
   fast access. to see if patrick follows sponge bob, we should just check
   the value of matrix["patrick"]["sponge bob"]. to get the list of ann   s
   followers, we just process the entire    ann    column (title is in
   yellow). to find who are being followed (sounds strange) by sponge bob,
   we process the entire row    sponge bob   . adjacency matrix could be used
   for undirected graphs as well, and instead of settings 1   s if a there
   is an edge from v to u, we should set both values to 1, e.g.
   adj_matrix[v][u] = 1, adj_matrix[u][v] = 1. undirected graph   s
   adjacency matrix is symmetric.

   note that instead of storing ones and zeroes in an adjacency matrix, we
   can store something    more useful   , like edge weights. one of the best
   examples might be a graph of places with distance information.
   [1*gmt-thrl-ferk-p2-oyprg.png]
   [1*zh3p8q22ygh-ufzsrj4vda.png]

   the graph above represents distances between houses of patrick, sponge
   bob and others (also known as weighted graph). we put    infinity    signs
   if there isn   t a direct route between vertices. that doesn   t mean that
   there are no routes at all, and at the same time that doesn   t mean that
   there must necessarily be routes. it might be defined while applying an
   algorithm for finding a route between vertices (there is even better
   way to store vertices and edges incident to it, called an incidence
   matrix).
   [1*mxda1tfizi5ohpamzvz-cq.jpeg]
   82000tb. [43]photo source

   while adjacency matrix seemed a good use for twitter   s followes graph,
   keeping a square matrix for nearly 300 million users (monthly active
   users) takes 300 * 300 * 1 bytes (storing boolean values). that is,
   ~82000 tb (terabytes), which is 1024 * 82000 gb. well, don   t know about
   your home cluster, my laptop doesn   t have so much ram. bitsets? a
   [44]bitboard could help us a little, reducing the required size to
   ~10000 tb. still way too big. as mentioned above, an adjacency matrix
   is too sparse. it forces us to use more space than actually needed.
   that   s why using a list of edges incident to vertices may be useful.
   the point is, an adjacency matrix allows us to keep both    follows    and
      doesn   t follow    information, while all we need is to know information
   about the followes, something like this:
   [1*jtt7z3ihfr7xsfskx95dta.png]
   [1*qqtfxxc44_2aqx4dehpkqw.png]
   adjacency matrix vs adjacency list

   the illustration at the right side is called an [45]adjacency list.
   each list describes the set of neighbors of a vertex in the graph. by
   the way, the actual implementation of the graph representation as an
   adjacency list, again, differs (ridiculous facts). in the illustration,
   we highlighted a hashtable usage, which is reasonable, as the access of
   any vertex will be o(1), and for the list of neighbor vertices we
   didn   t mention the exact data structure, deviating from linked lists to
   vectors. choice is yours.

   the point is, to find out whether patrick does follow liz, we should
   access the hashtable (constant time) and go through all items in the
   list comparing each element with    liz    element (linear time). linear
   time isn   t that bad at this point, because we have to loop over only a
   fixed amount of vertices adjacent to    patrick   . what about the space
   complexity, is it ok to use at twitter? well, we need at least 300
   million hashtable records, each of which points to a vector (choosing
   vector to avoid memory overhead of linked lists    left/right pointers)
   containing, how much? no stats here, found just an average number of
   twitter followers, 707 (googled).

   so if we consider that each hashtable record points to an array of 707
   user ids (each weighing 8 byte), and let   s assume that hashtable   s
   overhead is only its keys, which are again, user ids, so the hashtable
   itself takes 300 million * 8 bytes. overall, we have 300 million * 8
   bytes for hashtable + 707 * 8 bytes for each hashtable key, that is 300
   million * 8 * 707 * 8 bytes = ~12 tb. well, can   t say that feels
   better, but yes, feels much better than 10,000 tb.

   honestly, i don   t know whether this 12tb is a reasonable number. but
   considering the fact that i   m spending around $30 on a dedicated server
   machine with 32 gb of ram, then, storing (sharded) 12 tb requires at
   least 385 such servers, plus a couple of control servers (for data
   distribution control) rounds up to 400. so it will cost me $12k
   (monthly).

   well, considering the fact that the data should be replicated, and that
   something always can go wrong, we   ll triple the number of servers and
   then again, add some control servers, let   s say we need at least 1500
   servers, which will cost us $45k monthly. well, definitely not good for
   me as i hardly can keep one server, but seems okay for twitter (it   s
   really nothing compared to real twitter servers). let   s assume it is
   really okay for twitter.

   now, are we okay here? not yet, that was just the data regarding the
   followers. what is the main thing in twitter? i mean, technically, what
   is its biggest problem? you won   t be alone if you say it   s the fast
   delivery of tweets. i will definitely second that. and not fast, but
   lightning fast. say patrick tweeted something about his thoughts on
   food, all his followers should receive that very tweet in a reasonable
   time. how long will it take? we are free of making any assumption here,
   and use any abstractions we want, but we are interested in the real
   world production systems, so, let   s dig. here   s what   s typically
   happens when someone tweets.
   [1*rxzgqg1aw6lslhil9squga.png]

   again, don   t know much about how long it takes for one tweet to reach
   all followers, but publicly available statistics tell us that there are
   about 500 million daily tweets. daily!     

   so the process above happens 500 million times every day. i can   t
   really find anything on tweet delivery speed. i vaguely recall
   something about a maximum of 5 seconds for a tweet to reach all its
   followers. and also note the    heavy cases   , celebrities with more than
   a million followers. they might tweet something about their wonderful
   breakfast at the beach house, but twitter sweats much to deliver that
   super-useful content to millions of followers.

   to solve tweet delivery problem we don   t really need the graph of
   followers, instead we need a graph of followers. the previous graph
   (with a hashtable and a bunch of lists) allows us to efficiently find
   all users followed by any particular user. but it does not allow us to
   efficiently find all users who are following one particular user, for
   that case we have to scan all the hashtable keys. that   s why we should
   construct another graph, which is kind of a symmetric opposite to the
   one we constructed for followers. this new graph will again consist of
   a hashtable containing all 300 million vertices, each of which points
   to a list of adjacent vertices (the structure remains the same), but
   this time, the list of adjacent vertices will represent followers.
   [1*axs6z0f87lcnxthjxonpkw.png]

   so based on this illustration, whenever liz tweets something, sponge
   bob and ann must see that very tweet on their timelines. a common
   technique to accomplish this is by keeping separate structures for each
   user   s timeline. in case of twitter   s 300+ million users, we might
   assume there are at least 300+ million timelines (for each user).
   basically, whenever a user tweets, we should get the list of user   s
   followers and update their timelines (insert that same tweet into each
   one of them). a timeline might be represented as a linked list, or a
   balanced tree (tweet datetimes as node keys).

   iframe: [46]/media/a112441177141c5c9709526c2a0e4008?postid=1c96572a1401

   this is just a basic idea we abstracted from actual timeline
   representation and of course, we can make the actual delivery faster if
   we use multithreading. this is crucial for    heavy cases   , because for
   millions of followers the ones that reside closer to the end are being
   processed later than the ones residing closer to the front of the list.

   the following pseudocode tries to illuminate this multithreading
   delivery idea:

   iframe: [47]/media/730335f911c6b06587653a3c8955c5ae?postid=1c96572a1401

   so whenever followers refresh their timelines, they will receive the
   new tweet.

   it will be fair to say, that we merely touched the tip of the iceberg
   of real problems at airbnb or twitter. it takes a really long time and
   the hard work of really talented engineers to accomplish such great
   results in complex systems like twitter, google, facebook, amazon,
   airbnb and others. just keep this in mind while reading this article.
   [1*28qw4dwbrh3kljvxoulgjw.png]

   the point of demonstrating twitter   s tweet delivery problem is to
   embrace the usage of graphs, even though we didn   t use any graph
   algorithm, we just used a representation of the graph. sure we
   pseudocoded a function for delivering tweets, but that is something we
   came up during the process of searching for a solution.

   what i meant by    any graph algorithm    is any algorithm [48]from this
   list. as something big enough to make programmers cry, id207 and
   graph algorithm applications are somewhat different to spot at a
   glimpse. we were discussing the airbnb homes and efficient filtering
   before finishing with graph representations, and the main obvious thing
   was the inability to efficiently filter homes with more than one filter
   key. is there anything that could be done using graph algorithms? well,
   we can   t tell for sure, but at least we can try. what if we represent
   each filter as a separate vertex?

   literally each filter, even all the prices from $10 to $1000+, all city
   names, country codes, amenities (tv, wi-fi, and all others), number of
   adults, and each number as a separate graph vertex.
   [1*r756x_xp1va-vlqu6leljq.png]
   excerpt of airbnb filters

   we can even make this set of vertices more    friendly    if we add    type   
   vertices too, like    amenities    connected to all vertices representing
   an amenity filter.
   [1*k4sfmofsfg-v2yu6zbwwxw.png]
   airbnb filters with types

   now, what if we represent airbnb homes as vertices and then connect
   each home with    filter    vertex if that home supports the corresponding
   filter (for example, connecting    home 1    with    kitchen    if    home 1    has
      kitchen    in its amenities)?
   [1*sozj2rceng1ca-hua1324w.png]
   looks messy

   a subtle change of this illustration makes it more likely to resemble a
   special type of graph, called a bipartite graph.
   [1*rjdmni0vk44xoiwieqem-a.png]
   number of vertices are more than it may appear

   bipartite graph or just bigraph is a graph whose vertices can be
   divided into two disjoint and independent sets such that every edge
   connects a vertex in one set to one in other set. - [49]wikipedia.

   in our example one of the sets represents filters (we   ll denote it by
   f) and the other is a homes set (denoted by h). for example, if there
   are 100 thousand homes with the price value $62, then price vertex
   labeled    $62    will have 100 thousand edges incident to each homes
   vertices. if we measure the worst case of space complexity, i.e. each
   home has all the properties satisfying to all the filters, than the
   total amount of edges to be stored will be 70,000 * 4 million. if we
   represent each edge as a pair of two ids: {filter_id; home_id} and if
   we rethink about ids and use a 4 byte (int) numeric id for filters and
   8 byte (long) id for homes, then each edge would require at least 12
   bytes. so storing 70,000 * 4 million 12 bytes values will require
   around 3tb of memory. we made a small mistake in calculation, you see.

   the number of filters is around 70,000 because of the 65 thousand
   cities active in airbnb ([50]stats). and the good news is that the same
   home cannot be located in more than one city. that is, our actual
   number of edges pairing with cities is 4 million (each home located in
   one city). so we   ll calculate for 70k - 65k = 5 thousand filters, that
   means we need 5000 * 4 million * 12 bytes of memory, which is less than
   0.3 tb. sounds good. but what gives us this bipartite graph? most
   commonly a website/mobile request will consist of several filters, for
   example like this:
house_type: "entire_place",
adults_number: 2,
price_range_start: 56,
price_range_end: 80,
beds_number: 2,
amenities: ["tv", "wifi", "laptop friendly workspace"],
facilities: ["gym"]

   and all we need is to find all the    filter vertices    above and process
   all the    home vertices    that are adjacent to these    filter vertices   .
   this takes us to a scary subject.

graph algorithms: intro

   any processing done with graphs might be categorized as a    graph
   algorithm   . you literally can implement a function printing all the
   vertices of a graph and name it    <your name here>   s vertex printing
   algorithm   . most of us are scared of the graph algorithms listed in
   textbooks ([51]see the full list here). let   s try to apply a bipartite
   graph matching algorithm, such as [52]hopcroft-karp algorithm to our
   airbnb homes filtering problem:

     given a bipartite graph of airbnb homes (h) and filters (f), where
     every element (vertex) of h can have more than one adjacent elements
     (vertex) of f (sharing a common edge). find a subset of h consisting
     of vertices that are adjacent to vertices in a subset of f.

   confusing problem definition, however we can   t be sure at this point
   whether hopcroft-karp algorithm solves our problem. but i assure you
   that this journey will teach us many crucial ideas behind graph
   algorithms. and the journey is not so short, so be patient.

   the [53]hopcroft   [54]karp algorithm is an algorithm that takes as
   input, a bipartite graph and produces as output, a maximum cardinality
   matching - a set of as many edges as possible with the property that no
   two edges share an endpoint - [55]wikipedia.

   readers familiar with this algorithm are already aware that this
   doesn   t solve our problem, because matching requires that no two edges
   share a common vertex.

   let   s look at an example illustration, where there are just 4 filters
   and 8 homes (for the sake of simplicity).
     * homes are denoted by letters from a through h, filters are chosen
       randomly.
     * home a has a price ($50), and 1 bed, (that   s all we got for the
       price).
     * all homes from a through h have a $50 per night price tag and 1
       bed, but few of them have    wi-fi    and/or    tv   .

   so the following illustration tries to show which homes should we
      return    for the request asking for homes that have all four filters
   available (for example, they cost $50 per night, they have 1 bed and
   also they have wi-fi and tv).
   [1*r2g1a2oog8ketmqimr4sxq.png]

   the solution to our problem requires edges with common vertices leading
   to distinct home vertices that are incident to the same filter subset,
   while hopcroft-karp algorithm eliminates such edges with common
   endpoints and produces edges incident to vertices in both subsets.

   take a look at the illustration above, all we need are homes d and g,
   which both satisfy to all four filter values. what we really need is to
   get all matching edges which share a common endpoint.

   we could devise an algorithm for this approach, but its processing time
   is arguably not relevant to users needs (users needs = lightning fast,
   right here, right now). probably it would be faster to create a
   balanced binary search tree with multiple sort keys, almost like a
   database index file, which maps primary/foreign keys with a set of
   satisfying records.

   balanced binary search trees and database indexing will be discussed in
   a separate article, where we will return to the airbnb home filtering
   problem again.

   the hopcroft-karp algorithm (and many others) are based on both dfs
   (depth-first search) and bfs (breadth-first search) graph traversal
   algorithms. to be honest, the actual reason to introduce the
   hopcroft-karp algorithm here is to surreptitiously switch to graph
   traversals, which is better to start from the nice graphs, binary
   trees.

   binary tree traversals are really beautiful, mostly because of their
   recursive nature. there are three basic traversals called in-order,
   post-order and pre-order (you may come up with your own traversal
   algorithm). they are easy to understand if you have ever traversed a
   linked list. in linked lists you just print the current node   s value
   (named item in the code below) and continue to the next node.

   iframe: [56]/media/eeaaacbc62116bf4ab620d3102280302?postid=1c96572a1401

   almost the same goes with binary trees, you print the node value (or
   whatever else you need to do with it) and then continue to the next
   node, but in this case, there are    two next    nodes, left and right. so
   you should do the same for both left and right nodes. but you have
   three different choices here:
     * print the node value then go to the left node, and then go to the
       right node, or
     * go to the left node, print the node value, and then go to the right
       node, or
     * go to the left node, then go to the right node, and then print the
       value of the node.

   iframe: [57]/media/0f9ce1d7d9e771b78a77beb58a698f37?postid=1c96572a1401

   [1*_reeybwygacxuog91npfdq.png]
   detailed tracing of pre-order traversal
   [1*b8dfeer0ikes0ogkcr5mdq.png]

   obviously, recursive functions look very elegant though they are so
   expensive. each time we call a function recursively, it means we call a
   completely    new    function (see the illustration above). and by    new    we
   mean that another stack memory space should be    allocated    for the
   function arguments and local variables. that   s why recursive calls are
   expensive (the extra stack space allocations and the many function
   calls) and dangerous (mind the stack overflow) and it is obviously
   suggested to use iterative implementations. in mission critical systems
   programming (aircraft, nasa rovers and so on) a recursion is completely
   prohibited (no stats, no experience, just telling you the rumors).

netflix and amazon: inverted index example

   let   s say we want to store all netflix movies in a binary search tree
   with movie titles as sort keys. so whenever a user types something like
      inter   , we will return a list of movies starting with    inter    (for
   instance,    interstellar   ,    interceptor   ,    interrogation of walter
   white   ).

   now, it would be great if we   ll return every movie that contains
      inter    in its title (not only ones that start with    inter   ), and the
   list would be sorted by movie ratings or something that is relevant to
   that particular user (like thrillers more than drama). the point of
   this example is to make efficient range queries to a bst.

   but as usual, we won   t dive deeper into the cold water to spot the rest
   of the iceberg. basically, we need a fast lookup by search keywords and
   then get a list of results sorted by some key, which most likely should
   be a movie rating and/or some internal ranking based on a user   s
   personalized data. we   ll try to stick to the kisk principle (keep it
   simple, karl) as much as possible.

        kisk    or    let   s keep it simple    or    for the sake of simplicity   , a
     super excuse for tutorial writers to abstract from the real problem
     and make tons of assumptions by bringing an    abc    easy example and
     its solution in pseudocode that works even on your grandma   s laptop.

   this problem could be easily applied to amazon   s product search as we
   most commonly search something in amazon by typing a text describing
   our interest (like    graph algorithms   ) and get results sorted by
   product ratings. i haven   t experienced personalized results in amazon   s
   search results. but i   m pretty sure amazon does that stuff too. so, it
   will be fair to change the title of this subtopic to   

   netflix and amazon. netflix serves movies, amazon serves products,
   we   ll name them    items   , so whenever you read an    item    think of a
   movie in netflix or any [viable] product in amazon.

   what is most commonly done with the items is the parsing of their title
   and description (we   ll stick to the title only), so if an operator
   (usually a human being inserting item   s data into netflix/amazon
   database via an admin dashboard) inserts a new item into the database,
   its title is being processed by some    itemtitleprocessor    to produce
   keywords.
   [1*shk85qlf7ubbv3fv0tepqw.png]
   not the best illustration, i know (and has a typo)

   each item has its unique id, which is being linked to the keyword found
   in its title. this is what search engines do while crawling websites
   all over the world. they analyze each document   s content, tokenize it
   (break it into smaller entities called words) and add it to a table,
   which maps each token (word) to the document id (website) where the
   token has been    seen   .

   so whenever you search for    hello   , the search engine fetches all
   documents mapped to the keyword    hello    (reality is much complex,
   because the most important thing is the search relevancy, which is why
   google search is so awesome). so a similar table for netflix/amazon may
   look like this (again, think of movies or products when reading items).
   [1*fpei4aynsqh8weu2sdcoag.png]
   inverted index

   hashtables, again. yes, we will keep a hashtable for this inverted
   index (index structure storing a mapping from content - [58]wikipedia).
   the hashtable will map a keyword to a bst of items. why bst? because we
   want to keep them sorted and at the same time serve them (respond to
   frontend requests) in sequential sorted portions, (for instance, 100
   items at a request using pagination). not really something that shows
   the power of bsts. but let   s pretend that we also need a fast lookup in
   the search result, say you want all 3 star movies with the keyword
      machine   .
   [1*dinmllvjp8dfkyj4iuhgcq.png]

   note that it   s okay to have duplicate items in different trees, because
   an item usually can be found with more than one keyword.

   we   ll operate with items defined as follows:

   iframe: [59]/media/79585ae445447ded39385b0c183c6734?postid=1c96572a1401

   each time a new item is inserted into a database, the title is
   processed and added to the big index table, which maps a keyword to an
   item. there could be many items sharing the same keyword, so we keep
   these items in a bst sorted by their rating.

   when users search for some keyword, they get a list of items sorted by
   their rating. how can we get a list from a tree in a sorted order? by
   doing an in-order traversal.

   iframe: [60]/media/335ad1fcb684abd18f56935d1a94e8a8?postid=1c96572a1401

   here   s how an implementation of inorderproducevector() might look:

   iframe: [61]/media/31738da745f9cb4b55b9b9b97ade582d?postid=1c96572a1401

   but, but    we need the highest rated items first, and our in-order
   traversal produces the lowest rated items first. that   s because of its
   nature. in-order traversal works    bottom up   , from the lowest to the
   highest item. to get what we wanted, i.e. the list in descending order
   instead of ascending, we should take a look at the in-order traversal
   implementation a bit closer.

   what we are doing is going through the left node, then printing the
   current node   s value and then going through the right node. the left
   most node is the node with the smallest value. so simply changing the
   implementation to go through the right node first will lead us to a
   descending order of the list. we   ll name it as others do, a reverse
   in-order traversal.

   let   s update the code above (introducing in a single listing). warning
   - bugs ahead!

   iframe: [62]/media/5b524962453cc44796523fffa4db6551?postid=1c96572a1401

   fetching movies or products by keyword in sorted order (by rating)

   that   s it. we can serve item search results pretty fast. as mentioned
   above, inverted indexing is used mostly in search engines, like google.
   although google search is a very complex search engine, it does use
   some simple ideas (way too modernized though) to match search queries
   to documents and serve the results as fast as possible.

   we used tree traversals to serve results in sorted order. at this point
   it might seem that pre/in/post-order traversals are more than enough,
   but sometimes there is a need for another type of traversal.

   let   s tackle this well-known programming interview question,    how would
   you print a [binary] tree level by level?   .
   [1*n8leoidshv0s5jesdlwccw.png]

traversals: dfs and bfs

   if you are not familiar with this problem, think of some data structure
   that you could use to store nodes while traversing the tree. if we
   compare level-by-level traversal of a tree with the others above (pre,
   in, post order traversals), we   ll eventually devise two main traversals
   of graphs, that is a depth-first search (dfs) and breadth-first search
   (bfs).
   [1*binjy5q9c9x0opcheraqtg.png]

   depth-first search hunts for the farthest node, breadth-first search
   explores nearest nodes first.
     * depth-first search - more actions, less thoughts.
     * breadth-first search - take a good look around you before going
       farther.

   [1*weftmoewfdw2zljsenznug.png]

   dfs is much like pre, in, post-order traversals. while bfs is what we
   need if we want to print a tree   s nodes level-by-level.

   to accomplish this, we would need a queue (data structure) to store the
      level    of the graph while printing (visiting) its    parent level   . in
   the previous illustration nodes that are placed in the queue are in
   light blue.

   basically, going level by level, nodes on each level are fetched from
   the queue, and while visiting each fetched node, we also should insert
   its children into the queue (for the next level). the following code is
   simple enough to get the main idea of bfs. it is assumed that the graph
   is connected, although it can be modified to apply to disconnected
   graphs.

   iframe: [63]/media/577240205af8eadc13b6f537c44c9fc6?postid=1c96572a1401

   the basic idea is easy to show on a node-based connected graph
   representation. just keep in mind that the implementation of the graph
   traversal differs from representation to representation.

   bfs and dfs are important tools in tackling graph searching problems
   (but remember that there are tons of graph search algorithms). while
   dfs has elegant recursive implementation, it is reasonable to implement
   it iteratively. for the iterative implementation of bfs we used a
   queue, for dfs you will need a stack. one of the most popular problems
   in graphs and at the same time one of the most possible reasons you
   read in this article is the problem of finding the shortest path
   between graph vertices. and this takes us to our last thought
   experiment.

uber and the shortest path problem (dijkstra   s algorithm)

   with its 50 million users and 7 million drivers ([64]source), one of
   the most important things that is critical to uber   s functioning is the
   ability to match drivers with riders in an efficient way. the problem
   starts with locations.

   the backend should process millions of user requests, sending each of
   the requests to one or more (usually more) drivers nearby. while it is
   easier and sometimes even smarter to send the user request to all
   nearby drivers, some pre-processing might actually help.
   [1*qprbzuwr78qzgizysowv4q.png]

   besides processing incoming requests and finding the location area
   based on the user coordinates and then finding drivers with nearest
   coordinates, we also need to find the right driver for the ride. to
   avoid geospatial request processing (fetching nearby cars by comparing
   their current coordinates with user   s coordinates), let   s say we
   already have a segment of the map with user and several nearby cars.

   something like this:
   [1*6kgesmbss6faqplrlrq2da.png]

   possible paths from a car to a user are in yellow. the problem is to
   calculate the minimum required distance for the car to reach the user,
   in other words, find the shortest path between them. while this is more
   about google maps rather than uber, we   ll try to solve it for this
   particular and very simplified case mostly because there are usually
   more than one drivers car and uber might want to calculate the nearest
   car with the highest rating to send it to the user.

   for this illustration that means calculating for all three cars the
   shortest path reaching to the user and decide which car would be the
   optimal to send. to make things really simple, we   ll discuss the case
   with just one car. here are some possible routes to reach to the user.
   [1*yv50usktygpygn4nbf8ckw.png]
   [1*fxrxgd8zx4wphcaw9wlkdg.png]
   [1*nyl2fdd3aobuyd4bp9tijq.png]
   possible variants to reach the user

   cutting to the chase, we   ll represent this segment as a graph:
   [1*h8eghvyr-h2rfdclewf3pa.png]

   this is an undirected weighted graph (edge-weighted, to be more
   specific). to find the shortest path between vertices b (the car) and a
   (the user), we should find a route between them consisting of edges
   with possibly minimum weights. you are free to devise your version of
   the solution. we   ll stick with [65]dijkstra   s version. the following
   steps of dijkstra   s algorithm are from [66]wikipedia.

   let the node at which we are starting be called the initial node. let
   the distance of node y be the distance from the initial node to y.
   dijkstra   s algorithm will assign some initial distance values and will
   try to improve them step by step.
    1. mark all nodes unvisited. create a set of all the unvisited nodes
       called the unvisited set.
    2. assign to every node a tentative distance value: set it to zero for
       our initial node and to infinity for all other nodes. set the
       initial node as current.
    3. for the current node, consider all of its unvisited neighbors and
       calculate their tentative distances through the current node.
       compare the newly calculated tentative distance to the current
       assigned value and assign the smaller one. for example, if the
       current node a is marked with a distance of 6, and the edge
       connecting it with a neighbor b has length 2, then the distance to
       b through a will be 6 + 2 = 8. if b was previously marked with a
       distance greater than 8 then change it to 8. otherwise, keep the
       current value.
    4. when we are done considering all of the neighbors of the current
       node, mark the current node as visited and remove it from the
       unvisited set. a visited node will never be checked again.
    5. if the destination node has been marked visited (when planning a
       route between two specific nodes) or if the smallest tentative
       distance among the nodes in the unvisited set is infinity (when
       planning a complete traversal; occurs when there is no connection
       between the initial node and remaining unvisited nodes), then stop.
       the algorithm has finished.
    6. otherwise, select the unvisited node that is marked with the
       smallest tentative distance, set it as the new    current node   , and
       go back to step 3.

   applying this to our example, we   ll start with vertex b (the car) as
   the initial node. for first two steps:
   [1*tjlybxa2sd73xkk_g3zfdg.png]

   our unvisited set consists of all vertices. also note the table at the
   left side of the illustration. for all vertices, it will contain all
   the shortest distances from b and the previous (marked    prev   ) vertex
   that lead to the vertex. for instance the distance is 20 from b to f,
   and the previous vertex is b.
   [1*nntyjcutyrq3owuwfcfskq.png]

   we are marking b as visited and move it to its neighbor f.
   [1*-q06mg5a5yirmu2qhzgl0q.png]

   now we are marking f as visited and choosing the next unvisited node
   with smallest tentative distance, which is g. also note the table at
   the left side. in the previous illustration nodes c, f and g already
   have their tentative distances set with the previous nodes which lead
   to the mentioned nodes.
   [1*8y39ufj1feyqjpbjq9apca.png]

   as stated in the algorithm, if the destination node has been marked
   visited (when planning a route between two specific nodes as in our
   case) then we can stop. so our next step stops the algorithm with the
   following values.
   [1*mxxxszr1m82fetkgchghgg.png]

   so we have both the shortest distance from b to a and the route through
   f and g nodes.

   this is really the simplest possible example of potential problems at
   uber, comparing this to our iceberg analogy, we are at the tip of the
   tip of the iceberg. however, this is a good first start to explore the
   real world of id207 and its applications. i didn   t complete what
   i initially planned for in this article, but in the near future, most
   probably, this will be continued (also including database indexing
   internals).

   there is still so much to tell about graphs (still need to study). take
   this article as another tip of the iceberg. if you have read this far,
   you definitely deserve a cookie. don   t forget to clap and share. thank
   you.
     __________________________________________________________________

resources

   [1] [67]sh. even, g. even, graph algorithms

further reading

   [68]r. sedgewick, k. wayne, algorithms

   [69]t. cormen, ch. leiserson, r. rivest, c. stein, introduction to
   algorithms

   airbnb engineering, [70]airbnbeng

   netflix tech blog, [71]netflix technology blog

   [72]twitter engineering blog

   [73]uber engineering blog

     * [74]programming
     * [75]algorithms
     * [76]data
     * [77]tech
     * [78]web development

   (button)
   (button)
   (button) 13.4k claps
   (button) (button) (button) 39 (button) (button)

     (button) blockedunblock (button) followfollowing
   [79]go to the profile of vardan grigoryan (vardanator)

[80]vardan grigoryan (vardanator)

   backend engineer, [81]http://bit.ly/vardanator

     (button) follow
   [82]freecodecamp.org

[83]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 13.4k
     * (button)
     *
     *

   [84]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [85]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1c96572a1401
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/i-dont-understand-graph-theory-1c96572a1401&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_vyc7jnwoaglk---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@vardanator?source=post_header_lockup
  12. https://medium.freecodecamp.org/@vardanator
  13. https://medium.com/p/1c96572a1401#0239
  14. https://medium.com/p/1c96572a1401#48f5
  15. https://medium.com/p/1c96572a1401#0374
  16. https://medium.com/p/1c96572a1401#fb0c
  17. https://medium.com/p/1c96572a1401#0cd4
  18. https://medium.com/p/1c96572a1401#fb0c
  19. https://medium.com/p/1c96572a1401#cdde
  20. https://medium.com/p/1c96572a1401#45f6
  21. https://medium.com/p/1c96572a1401#aa4d
  22. https://en.wikipedia.org/wiki/seven_bridges_of_k  nigsberg
  23. https://en.wikipedia.org/wiki/kaliningrad
  24. https://www.google.com.au/maps/@54.7066964,20.5100082,16z
  25. https://www.amazon.com/concrete-mathematics-foundation-computer-science/dp/0201558025/
  26. https://en.wikipedia.org/wiki/leonhard_euler
  27. https://medium.freecodecamp.org/media/53aae7ddbd72868c01a72cad737e5c43?postid=1c96572a1401
  28. https://medium.freecodecamp.org/media/1ac1c0208c644f4dde7546f35a1bf6a1?postid=1c96572a1401
  29. https://press.atairbnb.com/app/uploads/2017/08/4-million-listings-announcement-1.pdf
  30. https://medium.freecodecamp.org/media/7b6bdfc58f0d6eefaa8c4acc63fa51a0?postid=1c96572a1401
  31. https://medium.freecodecamp.org/media/22eaf78f74af2761568ab3e47a55a4f1?postid=1c96572a1401
  32. https://medium.freecodecamp.org/media/9a129659839e3dee564ed9fb0e3d6887?postid=1c96572a1401
  33. https://en.wikipedia.org/wiki/country_code
  34. https://en.wikipedia.org/wiki/taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronukupokaiwhenuakitanatahu
  35. https://medium.freecodecamp.org/media/083d9e2d4f5ecde3bc0fed0f0af2b620?postid=1c96572a1401
  36. https://medium.freecodecamp.org/media/79f48cf78a1a8e787ac6fcf756651a12?postid=1c96572a1401
  37. https://medium.freecodecamp.org/media/a860978d734c22164210bf6eb7f56adc?postid=1c96572a1401
  38. https://medium.freecodecamp.org/media/9119087f1c12d3415bd0c3328416ab65?postid=1c96572a1401
  39. https://en.wikipedia.org/wiki/connectivity_(graph_theory)
  40. https://medium.freecodecamp.org/media/538b05968609aa8d01e5d253ee10cf2f?postid=1c96572a1401
  41. https://medium.freecodecamp.org/media/7a1662b7cbfa9a1bcb569e174493ce0e?postid=1c96572a1401
  42. https://medium.freecodecamp.org/media/0bca1e8a3b9d7f1d6066c9cafcdba082?postid=1c96572a1401
  43. http://www.independent.co.uk/arts-entertainment/tv/news/the-simpsons-death-episode-will-be-bigger-than-game-of-thrones-9304810.html
  44. https://github.com/vardanator/ultron/blob/master/src/arrays/bitboard.h
  45. https://en.wikipedia.org/wiki/adjacency_list
  46. https://medium.freecodecamp.org/media/a112441177141c5c9709526c2a0e4008?postid=1c96572a1401
  47. https://medium.freecodecamp.org/media/730335f911c6b06587653a3c8955c5ae?postid=1c96572a1401
  48. https://en.wikipedia.org/wiki/list_of_algorithms#graph_algorithms
  49. https://en.wikipedia.org/wiki/bipartite_graph
  50. https://press.atairbnb.com/fast-facts/
  51. https://en.wikipedia.org/wiki/list_of_algorithms#graph_algorithms
  52. https://en.wikipedia.org/wiki/hopcroft   karp_algorithm
  53. https://en.wikipedia.org/wiki/john_hopcroft
  54. https://en.wikipedia.org/wiki/richard_m._karp
  55. https://en.wikipedia.org/wiki/hopcroft   karp_algorithm
  56. https://medium.freecodecamp.org/media/eeaaacbc62116bf4ab620d3102280302?postid=1c96572a1401
  57. https://medium.freecodecamp.org/media/0f9ce1d7d9e771b78a77beb58a698f37?postid=1c96572a1401
  58. https://en.wikipedia.org/wiki/inverted_index
  59. https://medium.freecodecamp.org/media/79585ae445447ded39385b0c183c6734?postid=1c96572a1401
  60. https://medium.freecodecamp.org/media/335ad1fcb684abd18f56935d1a94e8a8?postid=1c96572a1401
  61. https://medium.freecodecamp.org/media/31738da745f9cb4b55b9b9b97ade582d?postid=1c96572a1401
  62. https://medium.freecodecamp.org/media/5b524962453cc44796523fffa4db6551?postid=1c96572a1401
  63. https://medium.freecodecamp.org/media/577240205af8eadc13b6f537c44c9fc6?postid=1c96572a1401
  64. https://expandedramblings.com/index.php/uber-statistics/
  65. https://en.wikipedia.org/wiki/dijkstra's_algorithm
  66. https://en.wikipedia.org/wiki/dijkstra's_algorithm
  67. https://www.amazon.com/graph-algorithms-shimon-even-ebook/dp/b00inyg6o4/
  68. https://www.amazon.com/algorithms-4th-robert-sedgewick/dp/032157351x/
  69. https://www.amazon.com/introduction-algorithms-3rd-mit-press/dp/0262033844/
  70. https://medium.com/@airbnbeng
  71. https://medium.com/@netflixtechblog
  72. https://blog.twitter.com/engineering/en_us.html
  73. https://eng.uber.com/
  74. https://medium.freecodecamp.org/tagged/programming?source=post
  75. https://medium.freecodecamp.org/tagged/algorithms?source=post
  76. https://medium.freecodecamp.org/tagged/data?source=post
  77. https://medium.freecodecamp.org/tagged/tech?source=post
  78. https://medium.freecodecamp.org/tagged/web-development?source=post
  79. https://medium.freecodecamp.org/@vardanator?source=footer_card
  80. https://medium.freecodecamp.org/@vardanator
  81. http://bit.ly/vardanator
  82. https://medium.freecodecamp.org/?source=footer_card
  83. https://medium.freecodecamp.org/?source=footer_card
  84. https://medium.freecodecamp.org/
  85. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  87. https://medium.com/p/1c96572a1401/share/twitter
  88. https://medium.com/p/1c96572a1401/share/facebook
  89. https://medium.com/p/1c96572a1401/share/twitter
  90. https://medium.com/p/1c96572a1401/share/facebook
