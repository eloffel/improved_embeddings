   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

how to train your own object detector with tensorflow   s object detector api

   [16]go to the profile of dat tran
   [17]dat tran (button) blockedunblock (button) followfollowing
   jul 28, 2017

   this is a follow-up post on [18]   building a real-time object
   recognition app with tensorflow and opencv    where i focus on training
   my own classes. specifically, i trained my own raccoon detector on a
   dataset that i collected and labeled by myself. the full dataset is
   available on [19]my github repo.

   by the way, here is the raccoon detector in action:
   [1*pydwef7qautbrq3lcw5xew.gif]
   the raccoon detector.

   if you want to know the details, you should continue reading!

motivation

   after my last post, a lot of people asked me to write a guide on how
   they can use [20]tensorflow   s new object detector api to train an
   object detector with their own dataset. i found some time to do it. in
   this post, i will explain all the necessary steps to train your own
   detector. in particular, i created an object detector that is able to
   recognize racoons with relatively good results.

     what the heck? why raccoooons    ????

   nothing special     they are one of my favorite animals and somehow they
   are also my neighbors! i swear, there are so many potential use cases
   with the raccoon detector. for example, now you can detect if a raccoon
   is knocking on your door while you   re not at home. the system could
   send you a push message to your mobile phone so that you know that you
   have some visitors.
   [1*suv3cdoggy5qohdie2d7tw.png]
   full video: [21]https://youtu.be/bl-qy84hojs

creating the dataset

   so let   s get serious! the first thing i needed to do was to create my
   own dataset:
     * tensorflow id164 api uses the [22]tfrecord file format,
       so at the end we need to convert our dataset to this file format
     * there are several options to generate the tfrecord files. either
       you have a dataset that has a similar structure to the [23]pascal
       voc dataset or the [24]oxford pet dataset, then they have
       ready-made scripts for this case (see
       [25]create_pascal_tf_record.py and [26]create_pet_tf_record.py). if
       you don   t have one of those structures you need to write your own
       script to generate the tfrecords (they also provide [27]an
       explanation for this). this is what i did!
     * to prepare the input file for the api you need to consider two
       things. firstly, you need an rgb image which is encoded as jpeg or
       png and secondly you need a list of bounding boxes (xmin, ymin,
       xmax, ymax) for the image and the class of the object in the
       bounding box. in terms of me, this was easy as i only had one
       class.
     * i scraped 200 raccoon images (mainly jpegs and a few pngs) from
       [28]google images and [29]pixabay where i made sure that the images
       have a large variations in scale, pose and lighting. here is a
       subset of the raccoon image dataset that i collected:

   [1*bss_ixhpmbdqinfcwgrrfg.jpeg]
   subset of the raccoon image dataset.
     * afterwards, i hand-labeled them manually with [30]labelimg.
       labelimg is a graphical image annotation tool that is written in
       python und uses qt for the graphical interface. it supports python
       2 and 3 but i built it from source with python 2 and qt4 as i had
       problems with python 3 and qt5. it   s super easy to use and the
       annotations are saved as xml files in the pascal voc format which
       means that i could also use the [31]create_pascal_tf_record.py
       script but i didn   t do this as i wanted to create my own script.
     * somehow, labelimg had problems with opening the jpegs on mac osx so
       i had to convert them to pngs and then later back to jpegs.
       actually, i could leave them in pngs as well as the api should also
       support this. i figured out this too late. this is what i will do
       next time.
     * finally, after labeling the images i wrote a script that converted
       the xml files to a csv and then created the tfrecords. i used 160
       images for training (train.records) and 40 images for testing
       (test.records). the script is also [32]available on my repo.

   notes:
     * i found another annotation tool called [33]fiat (fast image data
       annotation tool) that seems to be good as well. in the future, i
       might try this out.
     * for image processing on the command line like converting multiple
       images to different file formats, [34]imagemagick is a very good
       tool. in case, you haven   t used, it   s worth trying it out.
     * usually, creating the dataset is the most painful part. it took me
       2hrs to sort out the images and labeling them. and this was just
       for one class.
     * make sure that the image size is medium (see google images to see
       what medium means). if images are too large, you might run in
       out-of-memory errors during training in particular when you don   t
       change the default batch size settings.

training the model

   after i created the required input file for the api, i now can train my
   model.

   for training, you need the following:
     * an [35]id164 training pipeline. they also provide
       [36]sample config files on the repo. for my training, i used
       ssd_mobilenet_v1_pets.config as basis. i needed to adjust the
       num_classes to one and also set the path (path_to_be_configured)
       for the model checkpoint, the train and test data files as well as
       the label map. in terms of other configurations like the learning
       rate, batch size and many more, i used their default settings.

   note: the data_augmentation_option is very interesting if your dataset
   doesn   t have much of variability like different scale, pose etc.. a
   full list of options can be found [37]here (see
   preprocessing_function_map).
     * the dataset (tfrecord files) and its corresponding label map.
       example of how to create label maps can be found [38]here. here is
       also my label map which was very simple since i had only one class:

item {
  id: 1
  name: 'raccoon'
}

   note: it   s very important that your label map should always start from
   id 1. the index 0 is a placeholder index (see also this [39]discussion
   for more information on this topic).
     * (optional) pre-trained model checkpoint. it is recommended to use a
       checkpoint as it   s always better to start from from pre-trained
       models as training from scratch can take days before we get good
       results. they provide [40]several model checkpoint on their repo.
       in my case, i used the [41]ssd_mobilenet_v1_coco model as the model
       speed was more important for me than accuracy.

   now you can start the training:
     * training can be either done [42]locally or on the [43]cloud (aws,
       google cloud etc.). if you have gpu (at least more than 2 gb) at
       home then you can do it locally otherwise i would recommend to go
       with the cloud. in my case, i went with [44]google cloud this time
       and essentially followed all the steps described in [45]their
       documentation.
     * for google cloud, you need to define a yaml configuration file. a
       [46]sample file is also provided and i basically just took the
       default values.
     * it is also recommended during the training to start the evaluation
       job. you can then monitor the process of the training and
       evaluation jobs by running [47]tensorboard on your local machine.

tensorboard     logdir=gs://${your_cloud_bucket}

   here are the results from my training and evaluation jobs. in total, i
   ran it over about one hour/22k steps with a batch size of 24 but i
   already achieved good results in about 40mins.

   this is how the total loss evolved:
   [1*mbk-cbczxjbes1ixdt0ipw.png]
   total loss decreased pretty fast due to the pre-trained model.

   since i only had one class, it was enough to just look at total map
   (mean average precision):
   [1*siam74rv9k5ffsgrq8qoow.png]
   the map hit 0.8 at around 20k steps which is quite good.

   and here is an example for the evaluation of one image while training
   the model:
   [1*qxmwwfd2qa_ma8nrn-bpew.gif]
   the detected box around the raccoon got much better over time.

exporting the model

     * after finishing with training, i exported the trained model to a
       single file (tensorflow graph proto) so that i can use it for
       id136.
     * in my case, i had to copy the model checkpoints from the google
       cloud bucket to my local machine and then used the [48]provided
       script to export the model. the model can be found [49]on my repo,
       just in case if you really want to use it in production;)

   bonus:

   iframe: [50]/media/4b19a2155f09a9a445084973ccf5fe96?postid=bec72ecfe1d9

   i applied the trained model on a video that i found on youtube.
     * if you   ve watched the video, you   ll see that not every raccoon is
       detected or there are some misclassifications. this is logical as
       we only trained the model on a small dataset. to create a more
       generalized and robust raccoon detector that is, for example, also
       able to the detect the most famous raccoon on earth which is rocket
       raccoon from the guardians of the galaxy, we just need much more
       data. that   s just one of the limitations of ai right now!

   [1*h6x-atke8cq3kgj7zdedja.png]
   most famous raccoon on earth.

conclusion

   i hope you liked this post. give me a        if you do. hopefully, you can
   now train your own object detector. in this article, i only used one
   class because i was lazy labeling more data. there are services like
   [51]crowdflower, [52]crowdai or [53]amazon   s mechanical turk that offer
   data labeling services but that would be too much for this article.

   i obtained quite decent results for such a short training time but this
   is due to the fact that the detector was trained on a single class
   only. for more classes, total map won   t be as good as the one that i
   got and definitely longer training time would be needed to get good
   results. in fact, i also trained an object detector on [54]the
   annotated driving dataset (dataset 1) provided by udacity. it took me
   quite a while to train a model that is decently able to recognize cars,
   trucks and pedestrians. in many other cases, even the model that i used
   would be too simple to capture all the variability across multiple
   classes so that more complicated models must be used. there is a also
   the tradeoff between model speed and model accuracy that one must
   consider. however, this is a different story and actually could be
   another independent article.

   follow me here on medium [55]dat tran or on twitter [56]@datitran to
   stay up-to-date with my work.

     * [57]tensorflow
     * [58]machine learning
     * [59]tech
     * [60]data science
     * [61]towards data science

   (button)
   (button)
   (button) 8.4k claps
   (button) (button) (button) 208 (button) (button)

     (button) blockedunblock (button) followfollowing
   [62]go to the profile of dat tran

[63]dat tran

   head of data science @ idealo ([64]https://www.idealo.de/). more on
   [65]@datitran.

     (button) follow
   [66]towards data science

[67]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 8.4k
     * (button)
     *
     *

   [68]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [69]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bec72ecfe1d9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_tcnq8ojvu9wb---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@datitran?source=post_header_lockup
  17. https://towardsdatascience.com/@datitran
  18. https://medium.com/towards-data-science/building-a-real-time-object-recognition-app-with-tensorflow-and-opencv-b7a2b4ebdc32
  19. https://github.com/datitran/raccoon-dataset
  20. https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html
  21. https://youtu.be/bl-qy84hojs
  22. https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details
  23. http://host.robots.ox.ac.uk/pascal/voc/
  24. http://www.robots.ox.ac.uk/~vgg/data/pets/
  25. https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py
  26. https://github.com/tensorflow/models/blob/master/research/object_detection/create_pet_tf_record.py
  27. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md
  28. https://images.google.com/
  29. https://pixabay.com/
  30. https://github.com/tzutalin/labelimg
  31. https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py
  32. https://github.com/datitran/raccoon-dataset
  33. https://github.com/christopher5106/fastannotationtool
  34. http://imagemagick.org/
  35. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md
  36. https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs
  37. https://github.com/tensorflow/models/blob/a4944a57ad2811e1f6a7a87589a9fc8a776e8d3c/object_detection/builders/preprocessor_builder.py
  38. https://github.com/tensorflow/models/tree/master/research/object_detection/data
  39. https://github.com/tensorflow/models/issues/1696
  40. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
  41. http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz
  42. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md
  43. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md
  44. http://cloud.google.com/
  45. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_cloud.md
  46. https://github.com/tensorflow/models/blob/master/research/object_detection/samples/cloud/cloud.yml
  47. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  48. https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/exporting_models.md
  49. https://github.com/datitran/raccoon-dataset/tree/master/training
  50. https://towardsdatascience.com/media/4b19a2155f09a9a445084973ccf5fe96?postid=bec72ecfe1d9
  51. https://www.crowdflower.com/
  52. https://crowdai.com/
  53. https://www.mturk.com/mturk/welcome
  54. https://github.com/udacity/self-driving-car/tree/master/annotations
  55. https://medium.com/@datitran
  56. https://twitter.com/datitran
  57. https://towardsdatascience.com/tagged/tensorflow?source=post
  58. https://towardsdatascience.com/tagged/machine-learning?source=post
  59. https://towardsdatascience.com/tagged/tech?source=post
  60. https://towardsdatascience.com/tagged/data-science?source=post
  61. https://towardsdatascience.com/tagged/towards-data-science?source=post
  62. https://towardsdatascience.com/@datitran?source=footer_card
  63. https://towardsdatascience.com/@datitran
  64. https://www.idealo.de/
  65. http://twitter.com/datitran
  66. https://towardsdatascience.com/?source=footer_card
  67. https://towardsdatascience.com/?source=footer_card
  68. https://towardsdatascience.com/
  69. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  71. https://medium.com/p/bec72ecfe1d9/share/twitter
  72. https://medium.com/p/bec72ecfe1d9/share/facebook
  73. https://medium.com/p/bec72ecfe1d9/share/twitter
  74. https://medium.com/p/bec72ecfe1d9/share/facebook
