   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

understanding hinton   s id22. part iv: capsnet architecture

   [9]go to the profile of max pechyonkin
   [10]max pechyonkin (button) blockedunblock (button) followfollowing
   feb 21, 2018

part of understanding hinton   s id22 series:

   part i: [11]intuition
   part ii: [12]how capsules work
   part iii: [13]dynamic routing between capsules
   part iv: [14]capsnet architecture (you are reading it now)
     __________________________________________________________________

   quick announcement about our new publication [15]ai  . we are getting
   the best writers together to talk about the theory, practice, and
   business of ai and machine learning. follow it to stay up to date on
   the latest trends.
     __________________________________________________________________

introduction

   in this part, i will walk through the architecture of the capsnet. i
   will also offer my shot at calculating the number of trainable
   parameters in the capsnet. my resulting number is around 8.2 million of
   trainable parameters which is different from the 11.36 officially
   referred to in the paper. the paper itself is not very detailed and
   hence it leaves some open questions about specifics of the network
   implementation that are as of today still unanswered because the
   authors did not provide their code. nonetheless, i still think that
   counting parameters in a network is a good exercise for purely learning
   purposes as it allows one to practice understanding of all building
   blocks of a particular architecture.

   the capsnet has 2 parts: encoder and decoder. the first 3 layers are
   encoder, and the second 3 are decoder:

   layer 1. convolutional layer
   layer 2. primarycaps layer
   layer 3. digitcaps layer
   layer 4. fully connected #1
   layer 5. fully connected #2
   layer 6. fully connected #3

part i. encoder.

   [1*ajryyzttfiomrb73jzycog.png]
   capsnet encoder architecture. source: [16]original paper.

   encoder part of the network takes as input a 28 by 28 mnist digit image
   and learns to encode it into a 16-dimensional vector of instantiation
   parameters (as explained in the previous posts of this series), this is
   where the capsules do their job. the output of the network during
   prediction is a 10-dimensional vectors of lengths of digitcaps   
   outputs. the decoder has 3 layers: two of them are convolutional and
   the last one is fully connected.

layer 1. convolutional layer

   input: 28x28 image (one color channel).
   output: 20x20x256 tensor.
   number of parameters: 20992.

   convolutional layer   s job is to detect basic features in the 2d image.
   in the capsnet, the convolutional layer has 256 kernels with size of
   9x9x1 and stride 1, followed by relu activation. if you don   t know what
   this means, [17]here [18]are [19]some [20]awesome resources that will
   allow you to quickly pick up key ideas behind convolutions. to
   calculate the number of parameters, we need to also remember that each
   kernel in a convolutional layer has 1 bias term. hence this layer has
   (9x9 + 1)x256 = 20992 trainable parameters in total.

layer 2. primarycaps layer

   input: 20x20x256 tensor.
   output: 6x6x8x32 tensor.
   number of parameters: 5308672.

   this layer has 32 primary capsules whose job is to take basic features
   detected by the convolutional layer and produce combinations of the
   features. the layer has 32    primary capsules    that are very similar to
   convolutional layer in their nature. each capsule applies eight 9x9x256
   convolutional kernels (with stride 2) to the 20x20x256 input volume and
   therefore produces 6x6x8 output tensor. since there are 32 such
   capsules, the output volume has shape of 6x6x8x32. doing calculation
   similar to the one in the previous layer, we get 5308672 trainable
   parameters in this layer.

layer 3. digitcaps layer

   input: 6x6x8x32 tensor.
   output: 16x10 matrix.
   number of parameters: 1497600.

   this layer has 10 digit capsules, one for each digit. each capsule
   takes as input a 6x6x8x32 tensor. you can think of it as 6x6x32
   8-dimensional vectors, which is 1152 input vectors in total. as per the
   inner workings of the capsule (as described [21]here), each of these
   input vectors gets their own 8x16 weight matrix that maps 8-dimensional
   input space to the 16-dimensional capsule output space. so, there are
   1152 matrices for each capsule, and also 1152 c coefficients and 1152 b
   coefficients used in the dynamic routing. multiplying: 1152 x 8 x 16 +
   1152 + 1152, we get 149760 trainable parameters per capsule, then we
   multiply by 10 to get the final number of parameters for this layer.

the id168

   the id168 might look complicated at first sight, but it really
   is not. it is very similar to the [22]id166 id168. in order to
   understand the main idea about how it works, recall that the output of
   the digitcaps layer is 10 sixteen-dimensional vectors. during training,
   for each training example, one loss value will be calculated for each
   of the 10 vectors according to the formula below and then the 10 values
   will be added together to calculate the final loss. because we are
   talking about supervised learning, each training example will have the
   correct label, in this case it will be a ten-dimensional [23]one-hot
   encoded vector with 9 zeros and 1 one at the correct position. in the
   id168 formula, the correct label determines the value of t_c:
   it is 1 if the correct label corresponds with the digit of this
   particular digitcap and 0 otherwise.
   [1*y-bvfuilreqssdmdz6wama.png]
   color coded id168 equation. source: author, based on
   original paper.

   suppose the correct label is 1, this means the first digitcap is
   responsible for encoding the presence of the digit 1. for this
   digitcap   s id168 t_c will be one and for all remaining nine
   digitcaps t_c will be 0. when t_c is 1 then the first term of the loss
   function is calculated and the second becomes zero. for our example, in
   order to calculate the first digitcap   s loss we take the output vector
   of this digitcap and subtract it from m+, which is fixed at 0.9. then
   we keep the resulting value only in the case when it is greater than
   zero and square it. otherwise, return 0. in other words, the loss will
   be zero if the correct digitcap predicts the correct label with greater
   than 0.9 id203, and it will be non-zero if the id203 is
   less than 0.9.
   [1*9t2t_c5c1rjidlw58rl0sa.png]
   id168 value for correct and incorrect digitcap. note that the
   red graph is    squashed    vertically compared to the green one. this is
   due to the lambda multiplier from the formula. source: author.

   for digitcaps who do not match with the correct label, t_c will be zero
   and therefore the second term will be evaluated (corresponding to
   (1         t_c) part). in this case we can see that the loss will be zero if
   the mismatching digitcap predicts an incorrect label with id203
   less than 0.1 and non-zero if it predicts an incorrect label with
   id203 more than 0.1.

   finally, in the formula lambda coefficient is included for numerical
   stability during training (its value is fixed at 0.5). the two terms in
   the formula have squares because this id168 has l2 norm and the
   authors apparently consider this norm to work better.

part ii. decoder.

   [1*zqrtvmuxjadddzjerlawbq.png]
   capsnet decoder architecture. source: [24]original paper.

   decoder takes a 16-dimensional vector from the correct digitcap and
   learns to decode it into an image of a digit (note that it only uses
   the correct digitcap vector during training and ignores the incorrect
   ones). decoder is used as a regularizer, it takes the output of the
   correct digitcap as input and learns to recreate an 28 by 28 pixels
   image, with the id168 being euclidean distance between the
   reconstructed image and the input image. decoder forces capsules to
   learn features that are useful for reconstructing the original image.
   the closer the reconstructed image to the input image, the better.
   examples of reconstructed images can be seen in the image below.
   [1*pq0zhlqhkd-ftdoo_77jca.png]
   top row: original images. bottom row: reconstructed images. source:
   [25]original paper.

layer 4. fully connected #1

   input: 16x10.
   output: 512.
   number of parameters: 82432.

   each output of the lower level gets weighted and directed into each
   neuron of the fully connected layer as input. each neuron also has a
   bias term. for this layer there are 16x10 inputs that are all directed
   to each of the 512 neurons of this layer. therefore, there are (16x10 +
   1)x512 trainable parameters.

   for the following two layers calculation is the same: number of
   parameters = (number of inputs + bias) x number of neurons in the
   layer. this is why there is no explanation for fully connected layers 2
   and 3.

layer 5. fully connected #2

   input: 512.
   output: 1024.
   number of parameters: 525312.

layer 6. fully connected #3

   input: 1024.
   output: 784 (which after reshaping gives back a 28x28 decoded image).
   number of parameters: 803600.

   total number of parameters in the network: 8238608.

conclusion

   this wraps up the series on the capsnet. there are many very good
   resources around the internet. if you would like to learn more on this
   fascinating topic, please have a look at [26]this awesome compilation
   of links about capsnets.
     __________________________________________________________________

thanks for reading! if you enjoyed it, hit that clap button below and
[27]subscribe to updates on [28]my website! it would mean a lot to me and
encourage me to write more stories like this.

   you can [29]follow me on twitter. let   s also connect on [30]linkedin.

     * [31]machine learning
     * [32]geoffrey hinton
     * [33]neural networks
     * [34]deep learning
     * [35]artificial intelligence

   (button)
   (button)
   (button) 3.6k claps
   (button) (button) (button) 29 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of max pechyonkin

[37]max pechyonkin

   [38]https://pechyonkin.me/

     * (button)
       (button) 3.6k
     * (button)
     *
     *

   [39]ai   | theory, practice, business
   never miss a story from ai   | theory, practice, business, when you sign
   up for medium. [40]learn more
   never miss a story from ai   | theory, practice, business
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6a64422f7dce
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@pechyonkin/part-iv-capsnet-architecture-6a64422f7dce&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@pechyonkin/part-iv-capsnet-architecture-6a64422f7dce&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@pechyonkin?source=post_header_lockup
  10. https://medium.com/@pechyonkin
  11. https://pechyonkin.me/capsules-1/
  12. https://pechyonkin.me/capsules-2/
  13. https://pechyonkin.me/capsules-3/
  14. https://pechyonkin.me/capsules-4/
  15. https://medium.com/ai  -theory-practice-business?source=logo-bc670b9a1aca---ab837e78463f
  16. https://arxiv.org/abs/1710.09829
  17. https://www.youtube.com/watch?v=acu-t9l4_li
  18. https://arxiv.org/pdf/1603.07285.pdf
  19. http://colah.github.io/posts/2014-07-understanding-convolutions/
  20. http://setosa.io/ev/image-kernels/
  21. https://pechyonkin.me/capsules-2/
  22. http://cs231n.github.io/linear-classify/
  23. https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/
  24. https://arxiv.org/abs/1710.09829
  25. https://arxiv.org/abs/1710.09829
  26. https://github.com/aisummary/awesome-capsule-networks
  27. https://pechyonkin.me/subscribe/
  28. https://pechyonkin.me/
  29. https://twitter.com/max_pechyonkin
  30. https://www.linkedin.com/in/maxim-pechyonkin-phd/
  31. https://medium.com/tag/machine-learning?source=post
  32. https://medium.com/tag/geoffrey-hinton?source=post
  33. https://medium.com/tag/neural-networks?source=post
  34. https://medium.com/tag/deep-learning?source=post
  35. https://medium.com/tag/artificial-intelligence?source=post
  36. https://medium.com/@pechyonkin?source=footer_card
  37. https://medium.com/@pechyonkin
  38. https://pechyonkin.me/
  39. https://medium.com/ai  -theory-practice-business
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/6a64422f7dce/share/twitter
  43. https://medium.com/p/6a64422f7dce/share/facebook
  44. https://medium.com/p/6a64422f7dce/share/twitter
  45. https://medium.com/p/6a64422f7dce/share/facebook
