   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

understanding id22         ai   s alluring new architecture

   go to the profile of nick bourdakos
   [11]nick bourdakos (button) blockedunblock (button) followfollowing
   feb 12, 2018
   [1*e2y2v1vcy6oo5of7x0guua.jpeg]
      [12]science    by [13]alex reynolds

   convolutional neural networks have done an amazing job, but are rooted
   in problems. it   s time we started thinking about new solutions or
   improvements         and now, enter capsules.

   previously, i briefly discussed how [14]id22 combat some of
   these traditional problems. for the past for few months, i   ve been
   submerging myself in all things capsules. i think it   s time we all try
   to get a deeper understanding of how capsules actually work.

   in order to make it easier to follow along, i have built a
   visualization tool that allows you to see what is happening at each
   layer. this is paired with a simple implementation of the network. all
   of it can be found on github [15]here.

   this is the capsnet architecture. don   t worry if you don   t understand
   what any of it means yet. i   ll be going through it layer by layer, with
   as much detail as i can possibly conjure up.
   [1*ajhygzzpyf44uzmffpkkfw.png]

part 0: the input

   the input into capsnet is the actual image supplied to the neural net.
   in this example the input image is 28 pixels high and 28 pixels wide.
   but images are actually 3 dimensions, and the 3rd dimension contains
   the color channels.

   the image in our example only has one color channel, because it   s black
   and white. most images you are familiar with have 3 or 4 channels, for
   red-green-blue and possibly an additional channel for alpha, or
   transparency.
   [1*t3vqz3rtde7ipk8wr4wyea.png]

   each one of these pixels is represented as a value from 0 to 255 and
   stored in a 28x28x1 matrix [28, 28, 1]. the brighter the pixel, the
   larger the value.

part 1a: convolutions

   the first part of capsnet is a traditional convolutional layer. what is
   a convolutional layer, how does it work, and what is its purpose?

   the goal is to extract some extremely basic features from the input
   image, like edges or curves.

   how can we do this?

   let   s think about an edge:
   [1*dydazxcjlclsxhstzq4sra.png]

   if we look at a few points on the image, we can start to pick up a
   pattern. focus on the colors to the left and right of the point we are
   looking at:
   [1*n41scjbm5icq2krcdgnt7a.png]

   you might notice that they have a larger difference if the point is an
   edge:
255 - 114 = 141
114 - 153 = -39
153 - 153 = 0
255 - 255 = 0

   what if we went through each pixel in the image and replaced its value
   with the value of the difference of the pixels to the left and right of
   it? in theory, the image should become all black except for the edges.

   we could do this by looping through every pixel in the image:
for pixel in image {
  result[pixel] = image[pixel - 1] - image[pixel + 1]
}

   but this isn   t very efficient. we can instead use something called a
      convolution.    technically speaking, it   s a    cross-correlation,    but
   everyone likes to call them convolutions.

   a convolution is essentially doing the same thing as our loop, but it
   takes advantage of matrix math.

   a convolution is done by lining up a small    window    in the corner of
   the image that only lets us see the pixels in that area. we then slide
   the window across all the pixels in the image, multiplying each pixel
   by a set of weights and then adding up all the values that are in that
   window.

   this window is a matrix of weights, called a    kernel.   

   we only care about 2 pixels, but when we wrap the window around them it
   will encapsulate the pixel between them.
window:
                                                                                                                     
    left_pixel middle_pixel right_pixel    
                                                                                                                     

   can you think of a set of weights that we can multiply these pixels by
   so that their sum adds up to the value we are looking for?
window:
                                                                                                                     
    left_pixel middle_pixel right_pixel    
                                                                                                                     
(w1 * 255) + (w2 * 255) + (w3 * 114) = 141

   spoilers below!
                                  
                                  
                                  
                                  
                                  
\   /          \   /          \   /
 v            v            v

   we can do something like this:
window:
                                                                                                                     
    left_pixel middle_pixel right_pixel    
                                                                                                                     
(1 * 255) + (0 * 255) + (-1 * 114) = 141

   with these weights, our kernel will look like this:
kernel = [1  0 -1]

   however, kernels are generally square         so we can pad it with more
   zeros to look like this:
kernel = [
  [0  0  0]
  [1  0 -1]
  [0  0  0]
]

   here   s a nice gif to see a convolution in action:
   [1*ss2pdokfkcyi2uwliwabdg.gif]

   note: the dimension of the output is reduced by the size of the kernel
   plus 1. for example:(7         3) + 1 = 5 (more on this in the next section)

   here   s what the original image looks like after doing a convolution
   with the kernel we crafted:
   [1*a44ifeqtidxo1as-t8tylg.png]

   you might notice that a couple edges are missing. specifically, the
   horizontal ones. in order to highlight those, we would need another
   kernel that looks at pixels above and below. like this:
kernel = [
  [0  1  0]
  [0  0  0]
  [0 -1  0]
]

   also, both of these kernels won   t work well with edges of other angles
   or edges that are blurred. for that reason, we use many kernels (in our
   capsnet implementation, we use 256 kernels). and the kernels are
   normally larger to allow for more wiggle room (our kernels will be
   9x9).

   this is what one of the kernels looked like after training the model.
   it   s not very obvious, but this is just a larger version of our edge
   detector that is more robust and only finds edges that go from bright
   to dark.
kernel = [
  [ 0.02 -0.01  0.01 -0.05 -0.08 -0.14 -0.16 -0.22 -0.02]
  [ 0.01  0.02  0.03  0.02  0.00 -0.06 -0.14 -0.28  0.03]
  [ 0.03  0.01  0.02  0.01  0.03  0.01 -0.11 -0.22 -0.08]
  [ 0.03 -0.01 -0.02  0.01  0.04  0.07 -0.11 -0.24 -0.05]
  [-0.01 -0.02 -0.02  0.01  0.06  0.12 -0.13 -0.31  0.04]
  [-0.05 -0.02  0.00  0.05  0.08  0.14 -0.17 -0.29  0.08]
  [-0.06  0.02  0.00  0.07  0.07  0.04 -0.18 -0.10  0.05]
  [-0.06  0.01  0.04  0.05  0.03 -0.01 -0.10 -0.07  0.00]
  [-0.04  0.00  0.04  0.05  0.02 -0.04 -0.02 -0.05  0.04]
]

   note: i   ve rounded the values because they are quite large, for example
   0.01783941

   luckily, we don   t have to hand-pick this collection of kernels. that is
   what training does. the kernels all start off empty (or in a random
   state) and keep getting tweaked in the direction that makes the output
   closer to what we want.

   this is what the 256 kernels ended up looking like (i colored them as
   pixels so it   s easier to digest). the more negative the numbers, the
   bluer they are. 0 is green and positive is yellow:
   [1*ulhrj-tq2wnljuckrvwu2a.png]
   256 kernels (9x9)

   after we filter the image with all of these kernels, we end up with a
   fat stack of 256 output images.

part 1b: relu

   relu (formally known as rectified linear unit) may sound complicated,
   but it   s actually quite simple. relu is an activation function that
   takes in a value. if it   s negative it becomes zero, and if it   s
   positive it stays the same.

   in code:
x = max(0, x)

   and as a graph:
   [1*mcz45vgoyz9cmpuclnflqw.png]

   we apply this function to all of the outputs of our convolutions.

   why do we do this? if we don   t apply some sort of activation function
   to the output of our layers, then the entire neural net could be
   described as a linear function. this would mean that all this stuff we
   are doing is kind of pointless.

   adding a non-linearity allows us to describe all kinds of functions.
   there are many different types of function we could apply, but relu is
   the most popular because it   s very cheap to perform.

   here are the outputs of relu conv1 layer:
   [1*otjfxsnxjna4an5o4npafq.png]
   256 outputs (20x20 pixels)

part 2a: primarycaps

   the primarycaps layer starts off as a normal convolution layer, but
   this time we are convolving over the stack of 256 outputs from the
   previous convolutions. so instead of having a 9x9 kernel, we have a
   9x9x256 kernel.

   so what exactly are we looking for?

   in the first layer of convolutions we were looking for simple edges and
   curves. now we are looking for slightly more complex shapes from the
   edges we found earlier.

   this time our    stride    is 2. that means instead of moving 1 pixel at a
   time, we take steps of 2. a larger stride is chosen so that we can
   reduce the size of our input more rapidly:
   [1*x30mdjs0wvnsfhj1fiwzhw.gif]

   note: the dimension of the output would normally be 12, but we divide
   it by 2, because of the stride. for example: ((20         9) + 1) / 2 = 6

   we will convolve over the outputs another 256 times. so we will end up
   with a stack of 256 6x6 outputs.

   but this time we aren   t satisfied with just some lousy plain old
   numbers.

   we   re going to cut the stack up into 32 decks with 8 cards each deck.

   we can call this deck a    capsule layer.   

   each capsule layer has 36    capsules.   

   if you   re keeping up (and are a math wiz), that means each capsule has
   an array of 8 values. this is what we can call a    vector.   

   here   s what i   m talking about:
   [1*m-qjswvlotkijewd2z1jug.png]

   these    capsules    are our new pixel.

   with a single pixel, we could only store the confidence of whether or
   not we found an edge in that spot. the higher the number, the higher
   the confidence.

   with a capsule we can store 8 values per location! that gives us the
   opportunity to store more information than just whether or not we found
   a shape in that spot. but what other kinds of information would we want
   to store?

   when looking at the shape below, what can you tell me about it? if you
   had to tell someone else how to redraw it, and they couldn   t look at
   it, what would you say?
   [1*wbpjvxipzyogchphp-gm-a.png]

   this image is extremely basic, so there are only a few details we need
   to describe the shape:
     * type of shape
     * position
     * rotation
     * color
     * size

   we can call these    instantiation parameters.    with more complex images
   we will end up needing more details. they can include pose (position,
   size, orientation), deformation, velocity, albedo, hue, texture, and so
   on.

   you might remember that when we made a kernel for edge detection, it
   only worked on a specific angle. we needed a kernel for each angle. we
   could get away with it when dealing with edges because there are very
   few ways to describe an edge. once we get up to the level of shapes, we
   don   t want to have a kernel for every angle of rectangles, ovals,
   triangles, and so on. it would get unwieldy, and would become even
   worse when dealing with more complicated shapes that have 3 dimensional
   rotations and features like lighting.

   that   s one of the reasons why traditional neural nets don   t handle
   unseen rotations very well:
   [1*9vai2xlyusj4hs-kbvuzig.png]

   as we go from edges to shapes and from shapes to objects, it would be
   nice if we had more room to store this extra useful information.

   here is a simplified comparison of 2 capsule layers (one for rectangles
   and the other for triangles) vs 2 traditional pixel outputs:
   [1*_cihxggvpmn7g_hfh9znvq.png]

   like a traditional 2d or 3d vector, this vector has an angle and a
   length. the length describes the id203, and the angle describes
   the instantiation parameters. in the example above, the angle actually
   matches the angle of the shape, but that   s not normally the case.

   in reality it   s not really feasible (or at least easy) to visualize the
   vectors like above, because these vectors are 8 dimensional.

   since we have all this extra information in a capsule, the idea is that
   we should be able to recreate the image from them.

   sounds great, but how do we coax the network into actually wanting to
   learn these things?

   when training a traditional id98, we only care about whether or not the
   model predicts the right classification. with a capsule network, we
   have something called a    reconstruction.    a reconstruction takes the
   vector we created and tries to recreate the original input image, given
   only this vector. we then grade the model based on how close the
   reconstruction matches the original image.

   i will go into more detail on this in the coming sections, but here is
   a simple example:
   [1*zhjakulfuc2pxciu2poara.png]

part 2b: squashing

   after we have our capsules, we are going to perform another
   non-linearity function on it (like relu), but this time the equation is
   a bit more involved. the function scales the values of the vector so
   that only the length of the vector changes, not the angle. this way we
   can make the vector between 0 and 1 so it   s an actual id203.
   [1*frarvy1y1ih9ykcscbve_g.png]

   this is what lengths of the capsule vectors look like after squashing.
   at this point it   s almost impossible to guess what each capsule is
   looking for.
   [1*aprf1ogewdttav9bjsw4bg.png]
   keep in mind that each pixel is actually a vector of length 8

part 3: routing by agreement

   the next step is to decide what information to send to the next level.
   in traditional networks, we would probably do something like    max
   pooling.    max pooling is a way to reduce size by only passing on the
   highest activated pixel in the region to the next layer.
   [1*vthaax_6rb9goxbdfjvb1a.png]

   however, with id22 we are going to do something called
   routing by agreement. the best example of this is the boat and house
   example illustrated by aur  lien g  ron in [16]this excellent video. each
   capsule tries to predict the next layer   s activations based on itself:
   [1*khwyeh3zlj5ionbuzbbm_a.png]

   looking at these predictions, which object would you choose to pass on
   to the next layer (not knowing the input)? probably the boat, right?
   both the rectangle capsule and the triangle capsule agree on what the
   boat would look like. but they don   t agree on how the house would look,
   so it   s not very likely that the object is a house.

   with routing by agreement, we only pass on the useful information and
   throw away the data that would just add noise to the results. this
   gives us a much smarter selection than just choosing the largest
   number, like in max pooling.

   with traditional networks, misplaced features don   t faze it:
   [1*h-be4uj-a7n6_qx-k03kkq.png]

   with id22, the features wouldn   t agree with each other:
   [1*jck173lt9oddzcnpbjmgxq.png]

   hopefully, that works intuitively. however, how does the math work?

   we have 10 different digit classes that we are predicting:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9

   note: in the boat and house example we were predicting 2 objects, but
   now we are predicting 10.

   unlike in the boat and the house example, the predictions aren   t
   actually images. instead, we are trying to predict the vector that
   describes the image.

   the capsule   s predictions for each class are made by multiplying it   s
   vector by a matrix of weights for each class that we are trying to
   predict.

   remember that we have 32 capsule layers, and each capsule layer has 36
   capsules. that means we have a total of 1,152 capsules.
cap_1 * weight_for_0 = prediction
cap_1 * weight_for_1 = prediction
cap_1 * weight_for_2 = prediction
cap_1 * ...
cap_1 * weight_for_9 = prediction
cap_2 * weight_for_0 = prediction
cap_2 * weight_for_1 = prediction
cap_2 * weight_for_2 = prediction
cap_2 * ...
cap_2 * weight_for_9 = prediction
...
cap_1152 * weight_for_0 = prediction
cap_1152 * weight_for_1 = prediction
cap_1152 * weight_for_2 = prediction
cap_1152 * ...
cap_1152 * weight_for_9 = prediction

   you will end up with a list of 11,520 predictions.

   each weight is actually a 16x8 matrix, so each prediction is a matrix
   multiplication between the capsule vector and this weight matrix:
   [1*wnsn7kbxrtbf06nilyytow.png]

   as you can see, our prediction is a 16 degree vector.

   where does the 16 come from? it   s an arbitrary choice, just like 8 was
   for our original capsules.

   but it should be noted that we want to increase the number of
   dimensions of our capsules the deeper we get into the network. this
   should make sense intuitively, because the deeper we go the more
   complex our features become and the more parameters we need to recreate
   them. for example, you will need more information to describe an entire
   face than just a person   s eye.

   the next step is to figure out which of these 11,520 predictions agree
   with each other the most.

   it can be difficult to visualize a solution to this when we think in
   terms of high dimensional vectors. for the sake of sanity, let   s start
   off by pretending our vectors are just points in 2 dimensional space:
   [1*ucoytc3gsrjvd2vdqytv_a.png]

   we start off by calculating the mean of all of the points. each point
   starts out with equal importance:
   [1*kq5cjsn5d6xlfhpebyideg.png]

   we then can measure the distance between every point from the mean. the
   further the point is away from the mean, the less important that point
   becomes:
   [1*0ccocpifvk6gqra3beku2g.png]

   we then recalculate the mean, this time taking into account the point   s
   importance:
   [1*llll4gxxacai5ad5c5exea.png]

   we end up going through this cycle 3 times:
   [1*xwqit5ktjzhdjihygc1kha.gif]

   as you can see, as we go through this cycle, the points that don   t
   agree with the others start to disappear. the highest agreeing points
   end up getting passed on to the next layer with the highest
   activations.

part 4: digitcaps

   after agreement, we end up with ten 16 dimensional vectors, one vector
   for each digit. this matrix is our final prediction. the length of the
   vector is the confidence of the digit being found         the longer the
   better. the vector can also be used to generate a reconstruction of the
   input image.

   this is what the lengths of the vectors look like with the input of 4:
   [1*bj5wkhl3edkrhp-zzejw0a.png]

   the fifth block is the brightest, which means high confidence. remember
   that 0 is the first class, meaning 4 is our predicted class.

part 5: reconstruction

   the reconstruction portion of the implementation isn   t very
   interesting. it   s just a few fully connected layers. but the
   reconstruction itself is very cool and fun to play around with.

   if we reconstruct our 4 input from its vector, this is what we get:
   [1*ki3syogj-nqgubvg_lcdlg.png]

   if we manipulate the sliders (the vector), we can see how each
   dimension affects the 4:
   [1*t_6wawja2-9a07g1t22ata.gif]

   i recommend cloning the visualization repo to play around with
   different inputs and see how the sliders affect the reconstruction:
git clone [17]https://github.com/bourdakos1/capsnet-visualization.git
cd capsnet-visualization
pip install -r requirements.txt

   run the tool:
python run_visualization.py

   then point your browser to: [18]http://localhost:5000

final thoughts

   i think that the reconstructions from id22 are stunning.
   even though the current model is only trained on simple digits, it
   makes my mind run with the possibilities that a matured architecture
   trained on a larger dataset could achieve.

   i   m very curious to see how manipulating the reconstruction vectors of
   a more complicated image would affect it. for that reason, my next
   project is to get id22 to work with the cifar and smallnorb
   datasets.
     __________________________________________________________________

   thanks for reading! if you have any questions, feel free to reach out
   at bourdakos1@gmail.com, connect with me on [19]linkedin, or follow me
   on [20]medium and [21]twitter.

   if you found this article helpful, it would mean a lot if you gave it
   some applause     and shared to help others find it! and feel free to
   leave a comment below.

     * [22]machine learning
     * [23]ai
     * [24]tech
     * [25]programming
     * [26]technology

   (button)
   (button)
   (button) 7.2k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of nick bourdakos

[27]nick bourdakos

   medium member since sep 2017

   id161 addict at ibm watson

     (button) follow
   [28]freecodecamp.org

[29]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 7.2k
     * (button)
     *
     *

   [30]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [31]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bdb228173ddc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_on3shu0tgrn4---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@bourdakos1
  12. https://www.instagram.com/p/baiocbql-h_/?taken-by=areynolds_illo
  13. https://alex-reynolds-art.squarespace.com/
  14. https://hackernoon.com/capsule-networks-are-shaking-up-ai-heres-how-to-use-them-c233a0971952
  15. https://github.com/bourdakos1/capsnet-visualization
  16. https://www.youtube.com/watch?v=ppn8d0e3900
  17. https://github.com/bourdakos1/capsnet-visualization.git
  18. http://localhost:5000/
  19. https://www.linkedin.com/in/nicholasbourdakos
  20. https://medium.com/@bourdakos1
  21. https://twitter.com/bourdakos1
  22. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  23. https://medium.freecodecamp.org/tagged/ai?source=post
  24. https://medium.freecodecamp.org/tagged/tech?source=post
  25. https://medium.freecodecamp.org/tagged/programming?source=post
  26. https://medium.freecodecamp.org/tagged/technology?source=post
  27. https://medium.freecodecamp.org/@bourdakos1
  28. https://medium.freecodecamp.org/?source=footer_card
  29. https://medium.freecodecamp.org/?source=footer_card
  30. https://medium.freecodecamp.org/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.freecodecamp.org/@bourdakos1?source=post_header_lockup
  34. https://medium.com/p/bdb228173ddc/share/twitter
  35. https://medium.com/p/bdb228173ddc/share/facebook
  36. https://medium.freecodecamp.org/@bourdakos1?source=footer_card
  37. https://medium.com/p/bdb228173ddc/share/twitter
  38. https://medium.com/p/bdb228173ddc/share/facebook
