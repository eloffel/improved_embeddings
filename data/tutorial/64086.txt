   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

attention for neural (connectionist) machine translation

   [9]go to the profile of gautam karmakar
   [10]gautam karmakar (button) blockedunblock (button) followfollowing
   feb 23, 2018

   a quick recap of encoder-decoder (id195) model: in part 1 of this
   series
   [11]https://medium.com/@gautam.karmakar/id195-model-for-language-a099
   387ce837 i tried to explain id56 encoder decoder model first proposed by
   cho et al in 2014. it uses two recurrent neural network. one encodes
   the input sequence into a fixed length vector representation and
   another decodes that representations into another sequence of symbols.
   they are jointly trained to increase id155 of the
   target sequence given the input sequence. in addition to standard log
   loss of recurrent neural network using conditional probabilities of
   phrase pairs computed by id56 encoder-decoder found to improve empirical
   performance.
   [0*h3oremg7z1sgllaq.]

   image courtesy: https://smerity.com/articles/2016/google_id4_arch.html

   in part 2 of the series
   [12]https://medium.com/@gautam.karmakar/learning-phrase-representation-
   using-id56-encoder-decoder-for-machine-translation-9171cd6a6574 we saw
   how vanilla encoder-decoder model is used for neural machine
   translation

   problem with encoder-decoder model: the problem with this model is that
   decoder has to learn to predict target sequence just from a fixed
   length vector created at the end of encoder steps. to encode all the
   relevant information of the input sequence especially for longer
   sequence to a single vector is problematic. cho et al, 2014a showed
   that this problem of encoding information into a single vector
   increases or in other words performance of decoder decreases almost
   linearly as length of input sequence increases. also seeing how human
   perform translation tasks it doesn   t make sense that encoder needs to
   encode all the information once and decoder has to predict everything
   from that vector, because human translators does pay attention in parts
   of source sentence and then go back to source sentence as many times as
   needed to translate complete sentence. hence intuitively it make sense
   for model not to depend on soley one vector of encoder to predict the
   complete translation.attention model simulates this pattern of learning
   by looking back at the source when decoding. this provides shorter
   distance for decoder to source removing information bottleneck and we
   are no longer constrained to one single hidden state to learn decode.

   image courtesy:https://smerity.com/articles/2016/google_id4_arch.html
   [0*upleatkqp5lkdkuz.]

   attention model: attention model unlike basic encoder-decoder model
   pays attention to a subset of hidden state from encoder step for each
   word in the sentence. it adaptively learns to choose this subset at
   every decoding step. it also continue to use previous prediction like
   basic encoder-decoder model. originally developed by sutsekever et al,
   2014 and cho et al, 2014a called this alignment but later on attention
   caught on.

   in this model at each hidden state h_i a attention score is calculated
   alpha<i> such that alpha_ti adds up to 1. then it produces a context
   vector c_i which is weighted hidden state from time step i.

   in encoder-decoder model encoder produces a fixed context vector c from
   input sequence {x1, x2,    .x_tx} where hiddent state at time step t,

   h_t = f(x_t, h_t-1)

   c = q({h_1, h_2,       h_tx})

   normally a lstm model is used for f and q. decoder predicts next word
   in output sequence based on context vector c and previous predictions
   {y1, y2,    ..y_ty}

   references: [13]https://arxiv.org/abs/1409.0473

   [14]https://nlp.stanford.edu/pubs/emnlp15_attn.pdf
   [0*bsfoerv031h-1vmo.]

   there are also cases in papers where encoder is created using id98
   architecture instead of standard lstm.

   learning to align and translate:

   as shown above in attentional mechanism of encoder-decoder architecture
   in practice a bidirectional stacked lstm is used as encoder and similar
   decoder using stacked lstm is designed.

   the context vector is calculated as weighted sum of all input hidden
   states h_i. note that each h_i encodes input upto that step and hence
   store local context and learned weights alpha_ti decides how important
   that hidden state is in order for decoder to predict next sequence as
   output.
   [0*cs4gf2kgyactccxv.]

   the weights are calculated as none other than a softmax as shown below.
   [0*afv4fznqiiois5sn.]
   [0*zhfqsdybvuzg4qck.]
   [0*h4appkdobnlqgxtd.]

   this is called alignment model in the paper. this scores how well
   inputs around position j and position i match. the score is based on
   the id56 hidden state s_i -1. in this model alignment is learned using
   feedforward network and not considered as latent variable.

   the id203 alpha_ij or its associated energy e_ij reflects to
   previous hidden state s_i-1 in deciding the next state s_i and
   generating y_i. intuitively it acts as a attention in the decoder.
   decoder learns to pay attention to subset of encoder hidden state at
   each time step and relieves encoder from the burden of storing all
   information of input sequence into a fixed length vector.

   information can be stored in sequence of annotations of hidden states
   that can be adaptively selected or rejected while decoding.

     * [15]machine learning
     * [16]id4

   (button)
   (button)
   (button) 117 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [17]go to the profile of gautam karmakar

[18]gautam karmakar

   hungry, learner, fascinated by ml & ai

     * (button)
       (button) 117
     * (button)
     *
     *

   [19]go to the profile of gautam karmakar
   never miss a story from gautam karmakar, when you sign up for medium.
   [20]learn more
   never miss a story from gautam karmakar
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b833d1e085a3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@gautam.karmakar/attention-for-neural-connectionist-machine-translation-b833d1e085a3&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@gautam.karmakar/attention-for-neural-connectionist-machine-translation-b833d1e085a3&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@gautam.karmakar?source=post_header_lockup
  10. https://medium.com/@gautam.karmakar
  11. https://medium.com/@gautam.karmakar/id195-model-for-language-a099387ce837
  12. https://medium.com/@gautam.karmakar/learning-phrase-representation-using-id56-encoder-decoder-for-machine-translation-9171cd6a6574
  13. https://arxiv.org/abs/1409.0473
  14. https://nlp.stanford.edu/pubs/emnlp15_attn.pdf
  15. https://medium.com/tag/machine-learning?source=post
  16. https://medium.com/tag/id4?source=post
  17. https://medium.com/@gautam.karmakar?source=footer_card
  18. https://medium.com/@gautam.karmakar
  19. https://medium.com/@gautam.karmakar
  20. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  22. https://medium.com/p/b833d1e085a3/share/twitter
  23. https://medium.com/p/b833d1e085a3/share/facebook
  24. https://medium.com/p/b833d1e085a3/share/twitter
  25. https://medium.com/p/b833d1e085a3/share/facebook
