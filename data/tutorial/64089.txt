   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

training recurrent networks

a concise introduction to problem faced during training id56s

   [16]go to the profile of prakhar mishra
   [17]prakhar mishra (button) blockedunblock (button) followfollowing
   feb 13, 2018
   [1*u4ieqnyg9a7qc29qkswwsa.jpeg]
   [[18]source]

   this post assumes you to be having an introductory knowledge of the
   recurrent neural networks. if not, read this [19]great article and then
   come back here.
     __________________________________________________________________

   training a [20]recurrent neural networks solves our problem of learning
   patterns from the sequential data to some extent. the architectural
   design helps it achieves the same but with a downside, as it suffers
   from the [21]vanishing gradient problem. we will be discussing this
   problem in detail in the later part of this blog.

recall that   

   recurrent nets were designed to model sequential data and capture the
   [22]temporal patterns in the data fed into it.

   the below diagram shows id56 cell at two timestamps t=0 and t=1.
   [1*ebyybbgmt-iduntk5tukoa.png]

   the below diagram shows unfolded id56 at multiple timestamps. at any
   time stamp (t) the state (s) of the cell is

     activation(input*weight1 + state_previous_time*weight2)

   [1*zrlhni-dnpethkph_mqtcw.png]
   id56 unfolded over

   if you recall, u and w parameters are shared across the sequence over
   time, so as to capture sentences with same semantics and different word
   ordering. otherwise, it would be hard for the network to learn these
   representations of sentences, as it will have to relearn every time
   when sentences are fed in.

back propagation through time (bptt)

   [1*2dic4bdemb4snhj1mdpd9w.png]

goal     

    1. calculate the gradient of error w.r.t our current parameters u, v,
       w and relearn them to minimize the error function using
       optimization algorithms like [23]stochastic id119 or
       [24]adams optimizer.

assumptions     

    1. total error of the network is the summation of error at each
       time-stamp.
    2. total gradient is the summation of gradient at each time
       time-stamp.

derivatives     

   let   s demonstrate the process by considering error at t=3.

   we sum up the contributions of each time-step to the gradients. since,
   w, is used in every step till the output, we need to back-propagate
   error from t=3 to t=0.

   before that, let   s define few variables in-hand:
     * w = weight matrix at input edge.
     * y_ = predicted output

   [1*2kbs9far-iwouqdomxia2w.png]

   the multiplicative factor,
   [1*idjrmosljsx1k1nbthaeig.png]

   can   t be seen as constant because s3 depends on previous states
   recursively. also, the
   [1*iowjlb7tsndxkevk4nmqpw.png]
   error propagation from t=0 to t=3
   [1*cqir0j5kd2swxlb1g7b44g.png]
   error propagation from t=1 to t=3

   similarly, the equation will be expanded for k=[0, 1, 2, 3, 4]

vanishing gradient problem     

   if you understand the derivative in the above section then you can
   think of the chain rule expanding to extremely long for longer
   sentences. id180 (tanh or sigmoid) tend to go flat (i.e.
   derivate equals to zero) for very large and very small values of linear
   summation at every node. multiplying small values over long sequences
   makes them vanish (or what we call as neurons going dead).

solving vanishing gradient problem     

    1. use [25]relu instead of tanh or sigmoid activation function.
    2. properly initialize the w matrix. (ex. using [26]xavier
       initialization)
    3. use [27]lstm/[28]gru cells instead of simple id56.
     __________________________________________________________________

   recently, i have made a presentation about [29]neural networks. read it
   for better understanding of this blog.

   feel free to comment and correct at any point. spread the word if you
         ed it. also, you can     to me at prakharmishra137 [at] gmail [dot] com

     * [30]machine learning
     * [31]id26
     * [32]neural networks
     * [33]artificial intelligence
     * [34]recurrent neural network

   (button)
   (button)
   (button) 165 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of prakhar mishra

[36]prakhar mishra

   moving from medium to [37]https://prakhartechviz.blogspot.com/

     (button) follow
   [38]towards data science

[39]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 165
     * (button)
     *
     *

   [40]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [41]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/523d3b3bad3c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/training-recurrent-networks-523d3b3bad3c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/training-recurrent-networks-523d3b3bad3c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_jagipvritzwy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@prakhar.mishra?source=post_header_lockup
  17. https://towardsdatascience.com/@prakhar.mishra
  18. https://i.ytimg.com/vi/4rg8iskdc3u/maxresdefault.jpg
  19. http://colah.github.io/posts/2015-08-understanding-lstms/
  20. https://en.wikipedia.org/wiki/recurrent_neural_network
  21. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  22. http://www.ifp.illinois.edu/~hong/research/pattern_modeling.../temporal_pattern.htm
  23. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  24. https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/
  25. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  26. http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization
  27. https://en.wikipedia.org/wiki/long_short-term_memory
  28. https://en.wikipedia.org/wiki/gated_recurrent_unit
  29. https://www.slideshare.net/prakharmishra9/neural-networks-87485385
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/id26?source=post
  32. https://towardsdatascience.com/tagged/neural-networks?source=post
  33. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  34. https://towardsdatascience.com/tagged/recurrent-neural-network?source=post
  35. https://towardsdatascience.com/@prakhar.mishra?source=footer_card
  36. https://towardsdatascience.com/@prakhar.mishra
  37. https://prakhartechviz.blogspot.com/
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/523d3b3bad3c/share/twitter
  44. https://medium.com/p/523d3b3bad3c/share/facebook
  45. https://medium.com/p/523d3b3bad3c/share/twitter
  46. https://medium.com/p/523d3b3bad3c/share/facebook
