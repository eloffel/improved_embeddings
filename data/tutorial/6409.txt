   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    ml algorithms addendum: passive aggressive
   algorithms comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

ml algorithms addendum: passive aggressive algorithms

   [25]10/06/201710/08/2017[26]artificial intelligence, [27]generic,
   [28]machine learning, [29]machine learning algorithms addenda,
   [30]python, [31]scikit-learn[32]4 comments

   passive aggressive algorithms are a family of online learning
   algorithms (for both classification and regression) proposed by crammer
   at al. the idea is very simple and their performance has been proofed
   to be superior to many other alternative methods like[33] online
   id88 and [34]mira (see the original paper in the reference
   section).

classification

   let   s suppose to have a dataset:

   the index t has been chosen to mark the temporal dimension. in this
   case, in fact, the samples can continue arriving for an indefinite
   time. of course, if they are drawn from same data generating
   distribution, the algorithm will keep learning (probably without large
   parameter modifications), but if they are drawn from a completely
   different distribution, the weights will slowly forget the previous one
   and learn the new distribution. for simplicity, we also assume we   re
   working with a binary classification based on bipolar labels.

   given a weight vector w, the prediction is simply obtained as:

   all these algorithms are based on the hinge id168 (the same
   used by id166):

   the value of l is bounded between 0 (meaning perfect match) and k
   depending on f(x(t),  ) with k>0 (completely wrong prediction). a
   passive-aggressive algorithm works generically with this update rule:

   to understand this rule, let   s assume the slack variable   =0 (and l
   constrained to be 0). if a sample x(t) is presented, the classifier
   uses the current weight vector to determine the sign. if the sign is
   correct, the id168 is 0 and the argmin is w(t). this means that
   the algorithm is passive when a correct classification occurs. let   s
   now assume that a misclassification occurred:

   the angle    > 90  , therefore, the dot product is negative and the
   sample is classified as -1, however, its label is +1. in this case, the
   update rule becomes very aggressive, because it looks for a new w which
   must be as close as possible as the previous (otherwise the existing
   knowledge is immediately lost), but it must satisfy l=0 (in other
   words, the classification must be correct).

   the introduction of the slack variable allows to have soft-margins
   (like in id166) and a degree of tolerance controlled by the parameter c.
   in particular, the id168 has to be l <=   , allowing a larger
   error. higher c values yield stronger aggressiveness (with a consequent
   higher risk of destabilization in presence of noise), while lower
   values allow a better adaptation. in fact, this kind of algorithms,
   when working online, must cope with the presence of noisy samples (with
   wrong labels). a good robustness is necessary, otherwise, too rapid
   changes produce consequent higher misclassification rates.

   after solving both update conditions, we get the closed-form update
   rule:

   this rule confirms our expectations: the weight vector is updated with
   a factor whose sign is determined by y(t) and whose magnitude is
   proportional to the error. note that if there   s no misclassification
   the nominator becomes 0, so w(t+1) = w(t), while, in case of
   misclassification, w will rotate towards x(t) and stops with a loss l
   <=   . in the next figure, the effect has been marked to show the
   rotation, however, it   s normally as smallest as possible:

   after the rotation,    < 90   and the dot product becomes negative, so
   the sample is correctly classified as +1. scikit-learn implements
   passive aggressive algorithms, but i preferred to implement the code,
   just to show how simple they are. in next snippet (also available in
   this [35]gist), i first create a dataset, then compute the score with a
   id28 and finally apply the pa and measure the final
   score on a test set:
   view the code on [36]gist.

regression

   for regression, the algorithm is very similar, but it   s now based on a
   slightly different hinge id168 (called   -insensitive):

   the parameter    determines a tolerance for prediction errors. the
   update conditions are the same adopted for classification problems and
   the resulting update rule is:

   just like for classification, scikit-learn implements also a
   regression, however, in the next snippet (also available in this
   [37]gist), there   s a custom implementation:
   view the code on [38]gist.

   the error plot is shown in the following figure:

   the quality of the regression (in particular, the length of the
   transient period when the error is high) can be controlled by picking
   better c and    values. in particular, i suggest checking different
   range centers for c (100, 10, 1, 0.1, 0.01), in order to determine
   whether a higher aggressiveness is preferable.

   references:
     * crammer k., dekel o., keshet j., shalev-shwartz s., singer y.,
       [39]online passive-aggressive algorithms, journal of machine
       learning research 7 (2006) 551   585

   see also:

[40]ml algorithms addendum: instance based learning     giuseppe bonaccorso

     contrary to the majority of machine learning algorithms, instance
     based learning is model-free, meaning that there are strong
     assumptions about the structure of regressors, classifiers or
     id91 functions. they are    simply    determined by the data,
     according to an affinity induced by a distance metric (the most
     common name for this approach is nearest neighbors).

share:

     * [41]click to share on twitter (opens in new window)
     * [42]click to share on facebook (opens in new window)
     * [43]click to share on linkedin (opens in new window)
     * [44]click to share on pocket (opens in new window)
     * [45]click to share on tumblr (opens in new window)
     * [46]click to share on reddit (opens in new window)
     * [47]click to share on pinterest (opens in new window)
     * [48]click to share on skype (opens in new window)
     * [49]click to share on whatsapp (opens in new window)
     * [50]click to share on telegram (opens in new window)
     * [51]click to email this to a friend (opens in new window)
     * [52]click to print (opens in new window)
     *

you can also be interested in these articles:

   [53]algorithms[54]classification[55]passive-aggressive[56]python[57]reg
   ression[58]scikit-learn

post navigation

   [59]a brief (and comprehensive) guide to stochastic id119
   algorithms
   [60]a glimpse into the self-organizing maps (som)

4 thoughts on    ml algorithms addendum: passive aggressive algorithms   

    1. no
       10/07/2017 at 14:22
       are you sure that l is bounded between 0 and 1? $\max(0, 1    
       (-1)(+1))=2$
       [61]reply
          + [62]giuseppe bonaccorso
            10/07/2017 at 15:27
            you   re right. it was a typo. it   s a k>0. thank you, even if i
            don   t see your name that i could have written.
            [63]reply
    2. no
       10/07/2017 at 14:33
       the minimization in the update rule is unaffected by $c$. why are
       they there? your explanation of it is not consistent with the
       update rule. you can also take the $\xi$ out of the argmin.
       [64]reply
          + [65]giuseppe bonaccorso
            10/07/2017 at 15:41
            the minimization is subject to the second inequality. in my
            explanation xi=0, therefore l=0. when xi>0, the update rule
            has to find the minimum w(t+1) that satisfies the argmin (+
            c*xi) and l(w(t+1)) <= xi.
            [66]reply

leave a reply [67]cancel reply

   iframe: [68]jetpack_remote_comment

follow me

     * [69]linkedin
     * [70]twitter
     * [71]facebook
     * [72]github
     * [73]instagram
     * [74]amazon
     * [75]medium
     * [76]rss

search articles

   ____________________ (button)

latest blog posts

     * [77]machine learning algorithms     second edition 08/28/2018
     * [78]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [79]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [80]a book that every data scientist should read 06/22/2018
     * [81]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [82]my tweets

   copyright    2019 [83]giuseppe bonaccorso. all rights reserved.
   [84]privacy policy - [85]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [86]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/generic/
  28. https://www.bonaccorso.eu/category/machine-learning/
  29. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
  30. https://www.bonaccorso.eu/category/software/python/
  31. https://www.bonaccorso.eu/category/machine-learning/scikit-learn/
  32. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#comments
  33. https://en.wikipedia.org/wiki/id88
  34. https://en.wikipedia.org/wiki/margin-infused_relaxed_algorithm
  35. https://gist.github.com/giuseppebonaccorso/d700d7bd48b1865990d2f226759686b1
  36. https://gist.github.com/giuseppebonaccorso/d700d7bd48b1865990d2f226759686b1
  37. https://gist.github.com/giuseppebonaccorso/d459e15308b4faeb3a63bbbf8a6c9462
  38. https://gist.github.com/giuseppebonaccorso/d459e15308b4faeb3a63bbbf8a6c9462
  39. http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf
  40. https://www.bonaccorso.eu/2017/08/29/ml-algorithms-addendum-instance-based-learning/
  41. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=twitter
  42. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=facebook
  43. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=linkedin
  44. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=pocket
  45. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=tumblr
  46. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=reddit
  47. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=pinterest
  48. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=skype
  49. https://api.whatsapp.com/send?text=ml algorithms addendum: passive aggressive algorithms https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  50. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=telegram
  51. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/?share=email
  52. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#print
  53. https://www.bonaccorso.eu/tag/algorithms/
  54. https://www.bonaccorso.eu/tag/classification/
  55. https://www.bonaccorso.eu/tag/passive-aggressive/
  56. https://www.bonaccorso.eu/tag/python/
  57. https://www.bonaccorso.eu/tag/regression/
  58. https://www.bonaccorso.eu/tag/scikit-learn/
  59. https://www.bonaccorso.eu/2017/10/03/a-brief-and-comprehensive-guide-to-stochastic-gradient-descent-algorithms/
  60. https://www.bonaccorso.eu/2017/10/22/a-glimpse-into-self-organizing-maps-som/
  61. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#comment-45
  62. https://www.bonaccorso.eu/
  63. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#comment-48
  64. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#comment-47
  65. https://www.bonaccorso.eu/
  66. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#comment-49
  67. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#respond
  68. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1690&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=f002880a1592d493bc7042c8169c0b18c05abbf7#parent=https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/
  69. https://www.linkedin.com/in/giuseppebonaccorso/
  70. https://twitter.com/giuseppeb/
  71. https://www.facebook.com/giuseppe.bonaccorso/
  72. https://github.com/giuseppebonaccorso/
  73. https://www.instagram.com/giuseppebonaccorso/
  74. https://www.amazon.com/author/giuseppebonaccorso
  75. https://medium.com/@giuseppe.bonaccorso
  76. https://www.bonaccorso.eu/feed/
  77. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  78. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  79. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  80. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  81. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  82. https://twitter.com/giuseppeb
  83. https://www.bonaccorso.eu/
  84. https://www.iubenda.com/privacy-policy/331721
  85. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  86. https://www.bonaccorso.eu/2017/10/06/ml-algorithms-addendum-passive-aggressive-algorithms/#cancel

   hidden links:
  88. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
