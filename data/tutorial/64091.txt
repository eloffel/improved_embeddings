   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [0*1q1g_6krbxurgnni.]

beyond accuracy: precision and recall

   [16]go to the profile of will koehrsen
   [17]will koehrsen (button) blockedunblock (button) followfollowing
   mar 3, 2018

   choosing the right metrics for classification tasks

   would you believe someone who claimed to create a model entirely in
   their head to identify terrorists trying to board flights with greater
   than 99% accuracy? well, here is the model: simply label every single
   person flying from a us airport as not a terrorist. given the [18]800
   million average passengers on us flights per year and the [19]19
   (confirmed) terrorists who boarded us flights from 2000   2017, this
   model achieves an astounding accuracy of 99.9999999%! that might sound
   impressive, but i have a suspicion the us department of homeland
   security will not be calling anytime soon to buy this model. while this
   solution has nearly-perfect accuracy, this problem is one in which
   accuracy is clearly not an adequate metric!

   the terrorist detection task is an [20]imbalanced classification
   problem: we have two classes we need to identify         terrorists and not
   terrorists         with one category representing the overwhelming majority
   of the data points. another imbalanced classification problem occurs in
   disease detection when the rate of the disease in the public is very
   low. in both these cases the positive class         disease or terrorist         is
   greatly outnumbered by the negative class. these types of problems are
   examples of the fairly common case in data science when accuracy is not
   a good measure for assessing model performance.
     __________________________________________________________________

   intuitively, we know that proclaiming all data points as negative in
   the terrorist detection problem is not helpful and, instead, we should
   focus on identifying the positive cases. the metric our intuition tells
   us we should maximize is known in statistics as [21]recall, or the
   ability of a model to find all the relevant cases within a dataset. the
   precise definition of recall is the number of true positives divided by
   the number of true positives plus the number of false negatives. true
   positives are data point classified as positive by the model that
   actually are positive (meaning they are correct), and false negatives
   are data points the model identifies as negative that actually are
   positive (incorrect). in the terrorism case, true positives are
   correctly identified terrorists, and false negatives would be
   individuals the model labels as not terrorists that actually were
   terrorists. recall can be thought as of a model   s ability to find all
   the data points of interest in a dataset.
   [1*gscg4jdjnyu5qkqndqbg_w.png]

   you might notice something about this equation: if we label all
   individuals as terrorists, then our recall goes to 1.0! we have a
   perfect classifier right? well, not exactly. as with most concepts in
   data science, there is a trade-off in the metrics we choose to
   maximize. in the case of recall, when we increase the recall, we
   decrease the precision. again, we intuitively know that a model that
   labels 100% of passengers as terrorists is probably not useful because
   we would then have to ban every single person from flying. statistics
   provides us with the vocabulary to express our intuition: this new
   model would suffer from low [22]precision, or the ability of a
   classification model to identify only the relevant data points.

   precision is defined as the number of true positives divided by the
   number of true positives plus the number of false positives. false
   positives are cases the model incorrectly labels as positive that are
   actually negative, or in our example, individuals the model classifies
   as terrorists that are not. while recall expresses the ability to find
   all relevant instances in a dataset, precision expresses the proportion
   of the data points our model says was relevant actually were relevant.
   [1*fkxzf6dysp2mv4hubftrgg.png]

   now, we can see that our first model which labeled all individuals as
   not terrorists wasn   t very useful. although it had near-perfect
   accuracy, it had 0 precision and 0 recall because there were no true
   positives! say we modify the model slightly, and identify a single
   individual correctly as a terrorist. now, our precision will be 1.0 (no
   false positives) but our recall will be very low because we will still
   have many false negatives. if we go to the other extreme and classify
   all passengers as terrorists, we will have a recall of 1.0         we   ll
   catch every terrorist         but our precision will be very low and we   ll
   detain many innocent individuals. in other words, as we increase
   precision we decrease recall and vice-versa.
   [0*xeo3pwaee7tbt_d1.png]
   the precision-recall trade-off ([23]source)

combining precision and recall

   in some situations, we might know that we want to maximize either
   recall or precision at the expense of the other metric. for example, in
   preliminary disease screening of patients for follow-up examinations,
   we would probably want a recall near 1.0         we want to find all patients
   who actually have the disease         and we can accept a low precision if
   the cost of the follow-up examination is not significant. however, in
   cases where we want to find an optimal blend of precision and recall we
   can combine the two metrics using what is called the[24] f1 score.

   the f1 score is the harmonic mean of precision and recall taking both
   metrics into account in the following equation:
   [1*ujxvqlnbsj42erhaskeloa.png]

   we use the [25]harmonic mean instead of a simple average because it
   punishes extreme values. a classifier with a precision of 1.0 and a
   recall of 0.0 has a simple average of 0.5 but an f1 score of 0. the f1
   score gives equal weight to both measures and is a specific example of
   the general f   metric where    can be adjusted to give more weight to
   either recall or precision. (there are other metrics for combining
   precision and recall, such as the [26]geometric mean of precision and
   recall, but the f1 score is the most commonly used.) if we want to
   create a balanced classification model with the optimal balance of
   recall and precision, then we try to maximize the f1 score.

visualizing precision and recall

   i   ve thrown a couple new terms at you and we   ll walk through an example
   to show how they are used in practice. before we can get there though
   we need to briefly talk about tw concepts used for showing precision
   and recall.

   first up is the [27]confusion matrix which is useful for quickly
   calculating precision and recall given the predicted labels from a
   model. a confusion matrix for binary classification shows the four
   different outcomes: true positive, false positive, true negative, and
   false negative. the actual values form the columns, and the predicted
   values (labels) form the rows. the intersection of the rows and columns
   show one of the four outcomes. for example, if we predict a data point
   is positive, but it actually is negative, this is a false positive.
   [1*cpno_bcdbe8fxtejqiv2dg.png]

   going from the confusion matrix to the recall and precision requires
   finding the respective values in the matrix and applying the equations:
   [1*6nkn_lins2erxgvj9rkpua.png]

   the other main visualization technique for showing the performance of a
   classification model is the [28]receiver operating characteristic (roc)
   curve. don   t let the complicated name scare you off! the idea is
   relatively simple: the roc curve shows how the recall vs precision
   relationship changes as we vary the threshold for identifying a
   positive in our model. the threshold represents the value above which a
   data point is considered in the positive class. if we have a model for
   identifying a disease, our model might output a score for each patient
   between 0 and 1 and we can set a threshold in this range for labeling a
   patient as having the disease (a positive label). by altering the
   threshold, we can try to achieve the right precision vs recall balance.

   an roc curve plots the true positive rate on the y-axis versus the
   false positive rate on the x-axis. the true positive rate (tpr) is the
   recall and the false positive rate (fpr) is the id203 of a false
   alarm. both of these can be calculated from the confusion matrix:
   [1*uh9yup632ktsd75bzdeb0q.png]

   a typical roc curve is shown below:
   [0*2ihr8dfxev5gwo_f.png]
   receiver operating characteristic curve ([29]source)

   the black diagonal line indicates a random classifier and the red and
   blue curves show two different classification models. for a given
   model, we can only stay on one curve, but we can move along the curve
   by adjusting our threshold for classifying a positive case. generally,
   as we decrease the threshold, we move to the right and upwards along
   the curve. with a threshold of 1.0, we would be in the lower left of
   the graph because we identify no data points as positives leading to no
   true positives and no false positives (tpr = fpr = 0). as we decrease
   the threshold, we identify more data points as positive, leading to
   more true positives, but also more false positives (the tpr and fpr
   increase). eventually, at a threshold of 0.0 we identify all data
   points as positive and find ourselves in the upper right corner of the
   roc curve (tpr = fpr = 1.0).

   finally, we can quantify a model   s roc curve by calculating the total
   [30]area under the curve (auc), a metric which falls between 0 and 1
   with a higher number indicating better classification performance. in
   the graph above, the auc for the blue curve will be greater than that
   for the red curve, meaning the blue model is better at achieving a
   blend of precision and recall. a random classifier (the black line)
   achieves an auc of 0.5.

recap

   we   ve covered a few terms, none of which are difficult on their own,
   but which combined can be a little overwhelming! let   s do a quick recap
   and then walk through an example to solidly the new ideas we learned.

   four outcomes of binary classification
     * true positives: data points labeled as positive that are actually
       positive
     * false positives: data points labeled as positive that are actually
       negative
     * true negatives: data points labeled as negative that are actually
       negative
     * false negatives: data points labeled as negative that are actually
       positive

   recall and precision metrics
     * recall: ability of a classification model to identify all relevant
       instances
     * precision: ability of a classification model to return only
       relevant instances
     * f1 score: single metric that combines recall and precision using
       the harmonic mean

   visualizing recall and precision
     * confusion matrix: shows the actual and predicted labels from a
       classification problem
     * receiver operating characteristic (roc) curve: plots the true
       positive rate (tpr) versus the false positive rate (fpr) as a
       function of the model   s threshold for classifying a positive
     * area under the curve (auc): metric to calculate the overall
       performance of a classification model based on area under the roc
       curve

example application

   our task will be to diagnose 100 patients with a disease present in 50%
   of the general population. we will assume a black box model, where we
   put in information about patients and receive a score between 0 and 1.
   we can alter the threshold for labeling a patient as positive (has the
   disease) to maximize the classifier performance. we will evaluate
   thresholds from 0.0 to 1.0 in increments of 0.1, at each step
   calculating the precision, recall, f1, and location on the roc curve.
   following are the classification outcomes at each threshold:
   [1*3sjx3lalufj3yf7xu1qhma.png]
   outcome of model at each threshold

   we   ll do one sample calculation of the recall, precision, true positive
   rate, and false positive rate at athreshold of 0.5. first we make the
   confusion matrix:
   [1*gdld_wahnfjkh49nsfq72q.png]
   confusion matrix for threshold of 0.5

   we can use the numbers in the matrix to calculate the recall,
   precision, and f1 score:
   [1*xqoyd2mhehyvvr9h-entiq.png]

   then we calculate the true positive and false positive rate to find the
   y and x coordinates for the roc curve.
   [1*hzwxvbikctib-qtfb48woq.png]

   to make the entire roc curve, we carry out this process at each
   threshold. as you might think, this is pretty tedious, so instead of
   doing it by hand, we use a language like python to do it for us! the
   [31]jupyter notebook with the calculations is on github for anyone to
   see the implementation. the final roc curve is shown below with the
   thresholds above the points.
   [1*zah33g5fd9xyzradgmqwvw.png]

   here we can see all the concepts come together! at a threshold of 1.0,
   we classify no patients as having the disease and hence have a recall
   and precision of 0.0. as the threshold decreases, the recall increases
   because we identify more patients that have the disease. however, as
   our recall increases, our precision decreases because in addition to
   increasing the true positives, we increase the false positives. at a
   threshold of 0.0, our recall is perfect         we find all patients with the
   disease         but our precision is low because we have many false
   positives. we can move along the curve for a given model by changing
   the threshold and select the threshold that maximizes the f1 score. to
   shift the entire curve, we would need to build a different model.

   final model statistics at each threshold are below:
   [1*tesjafburn7rvxyb5kdoxg.png]

   based on the f1 score, the overall best model occurs at a threshold of
   0.5. if we wanted to emphasize precision or recall to a greater extent,
   we could choose the corresponding model that performs best on those
   measures.

conclusions

   we tend to use accuracy because everyone has an idea of what it means
   rather than because it is the best tool for the task! although
   better-suited metrics such as recall and precision may seem foreign, we
   already have an intuitive sense of why they work better for some
   problems such as imbalanced classification tasks. statistics provides
   us with the formal definitions and the equations to calculate these
   measures. [32]data science is about knowing the right tools to use for
   a job, and often we need to go beyond accuracy when developing
   classification models. knowing about recall, precision, f1, and the roc
   curve allows us to assess classification models and should make us
   think skeptically about anyone touting only the accuracy of a model,
   especially for imbalanced problems. as we have seen, accuracy does not
   provide a useful assessment on several crucial problems, but now we
   know how to employ smarter metrics!

   as always, i welcome constructive criticism, feedback, and discussion.
   i can be reached on twitter [33]@koehrsen_will.

     * [34]machine learning
     * [35]data science
     * [36]education
     * [37]statistics
     * [38]towards data science

   (button)
   (button)
   (button) 8.1k claps
   (button) (button) (button) 35 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of will koehrsen

[40]will koehrsen

   data scientist at cortex intel, data science communicator

     (button) follow
   [41]towards data science

[42]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 8.1k
     * (button)
     *
     *

   [43]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [44]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3da06bea9f6c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vfjy6kvwp2ng---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@williamkoehrsen?source=post_header_lockup
  17. https://towardsdatascience.com/@williamkoehrsen
  18. https://www.rita.dot.gov/bts/press_releases/bts018_16
  19. https://en.wikipedia.org/wiki/list_of_aircraft_hijackings#2000s
  20. https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/
  21. https://en.wikipedia.org/wiki/precision_and_recall
  22. https://en.wikipedia.org/wiki/precision_and_recall
  23. http://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=2610217
  24. https://en.wikipedia.org/wiki/f1_score
  25. https://stackoverflow.com/questions/26355942/why-is-the-f-measure-a-harmonic-mean-and-not-an-arithmetic-mean-of-the-precision
  26. https://en.wikipedia.org/wiki/fowlkes   mallows_index
  27. http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
  28. http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
  29. http://www.statisticshowto.com/c-statistic/
  30. https://en.wikipedia.org/wiki/receiver_operating_characteristic#area_under_the_curve
  31. https://github.com/willkoehrsen/data-analysis/blob/master/recall_precision/recall_precision_example.ipynb
  32. https://www.datacamp.com/community/podcast/data-science-astronomy
  33. https://twitter.com/koehrsen_will
  34. https://towardsdatascience.com/tagged/machine-learning?source=post
  35. https://towardsdatascience.com/tagged/data-science?source=post
  36. https://towardsdatascience.com/tagged/education?source=post
  37. https://towardsdatascience.com/tagged/statistics?source=post
  38. https://towardsdatascience.com/tagged/towards-data-science?source=post
  39. https://towardsdatascience.com/@williamkoehrsen?source=footer_card
  40. https://towardsdatascience.com/@williamkoehrsen
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/?source=footer_card
  43. https://towardsdatascience.com/
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/3da06bea9f6c/share/twitter
  47. https://medium.com/p/3da06bea9f6c/share/facebook
  48. https://medium.com/p/3da06bea9f6c/share/twitter
  49. https://medium.com/p/3da06bea9f6c/share/facebook
