    #[1]new tech dojo    feed [2]alternate [3]alternate

   [4]new tech dojo new tech dojo
   (button)
     * [5]home
     * [6]blog
     * [7]expert interview

list of machine learning algorithms

   march 6, 2018 [8]newtechdojo [9]machine learning
   [10]machine learning algorithm list machine learning algorithm list
   [1x1.trans.gif] [machine-learning-algorithm-list.jpg]

     * [11]machine learning algorithms:
          + [12]the machine learning algorithm list includes:
          + [13]the three categories of these machine learning algorithms
            are:
     * [14]supervised learning
     * [15]id90
     * [16]naive bayes classification
     * [17]support vector machines for classification problems (id166)
     * [18]id79 for classification and regression problems
     * [19]id75 for regression problems
     * [20]ordinary least squares regression
     * [21]id28
     * [22]ensemble methods
          + [23]id112
          + [24]boosting
          + [25]stacking
     * [26]unsupervised learning
     * [27]id116 id91 algorithm
     * [28]apriori algorithm for association rule learning problems
     * [29]principal component analysis
     * [30]singular value decomposition
     * [31]independent component analysis
     * [32]reinforcement or semi-supervised machine learning
     * [33]must have machine learning algorithm cheat sheet

machine learning algorithms:

   there is a distinct list of machine learning algorithms. the method of
   how and when you should be using them. by learning about the list of
   machine learning algorithm you learn furthermore about ai and designing
   machine learning system.

the machine learning algorithm list includes:

    1. id75
    2. id28
    3. support vector machines
    4. id79
    5. na  ve bayes classification
    6. ordinary least square regression
    7. id116
    8. ensemble methods
    9. apriori algorithm
   10. principal component analysis
   11. singular value decomposition
   12. reinforcement or semi-supervised machine learning
   13. independent component analysis

   these are the most important algorithms in machine learning. if you are
   aware of these algorithms then you can use them well to apply in almost
   any data problem. data scientists and the machine learning enthusiasts
   use these algorithms for creating various functional machine learning
   projects. then comes the 3 types of machine learning technique or
   category which are used in these machine learning algorithms.

the three categories of these machine learning algorithms are:

    1. [34]supervised learning
    2. [35]unsupervised learning
    3. [36]id23

   to understand it better, you would need to understand each algorithm
   which will let you pick the right one which will match your problem and
   learning requirement.
   ypes of machine learning     at a glance - list of machine learning
   algorithms ypes of machine learning     at a glance - list of machine
   learning algorithms

supervised learning

                                   iframe:
   [37]https://www.youtube.com/embed/hfo6irj-gzo?rel=0&controls=0&showinfo
                                     =0

   the supervised learning method is used by maximum machine learning
   users. there is a basic fundamental on why it is called supervised
   learning. it is called supervised learning because the way an
   algorithm   s learning process is done, it is a training dataset. and
   while using training dataset, the process can be thought of as a
   teacher supervising the learning process. the correct answer is known
   and stored in the system already.  the algorithm helps in making
   predictions about the data that is in training process and gets the
   correction done by the teacher itself. there is an end to the learning
   only when the algorithm has achieved an acceptable degree or level of
   performance.

   there are two types of supervised learning problems. these supervised
   problems can be further grouped into regression and classification
   problems.
     * classification problems: classification problem can be defined as
       the problem that brings output variable which falls just in
       particular categories, such as the    red    or    blue    or it could be
          disease    and    no disease   .
     * regression: a regression problem is when the output variable is a
       real value, such as    dollars    or it could be    weight   .

    there are some problems which you get to observe in the data type. the
   common problems which occur or gets built on the head of the
   classification problems and the regression problem. the common problems
   include the time-series prediction and recommendation respectively.

   there are few really popular supervised machine learning algorithms,
   such as:
    1. [38]id90
    2. [39]naive bayes classification
    3. [40]support vector machines for classification problems
    4. [41]id79 for classification and regression problems
    5. [42]id75 for regression problems
    6. [43]ordinary least squares regression
    7. [44]id28
    8. [45]ensemble methods

   [46]learn how supervised machine learning works - list of machine
   learning algorithms learn how supervised machine learning works - list
   of machine learning algorithms

   how supervised machine learning works? - image source:
   [47]boozallen.com

id90

   well, a lot is noticeable when you read the name decision tree, in
   simple terms a decision tree lends you the help to make a decision
   about the data item. for instance, in case, if you are a banker you get
   to take the decision whether you should give a loan to a person or not
   on the basis of his age, occupation and education level. you can do
   this by using a decision tree. while considering any decision tree, we
   have to start the process from the root node and go on answering a
   particular question at each node and take the branch that corresponds
   to the particular answer. well, following this mannerism, we traverse
   from the root node then to a leaf and then form conclusions in context
   to the data item. let us consider an example based on a decision tree
   below.

   visual representation of learn decision tree algorithm visual
   representation of learn decision tree algorithm visual representation
   of learn decision tree algorithm visual representation of learn
   decision tree algorithm
   fig: a tree showing the survival of passengers on the titanic (   sibsp   
   is the number of spouses or siblings aboard). (source: [48]wikipedia).

   iframe:
   [49]https://www.youtube.com/embed/ldrbo9a6xpu?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [50]learn more

naive bayes classification

   heard about the [51]bayes    theorem?  so this is a classification
   technique dependent on the bayes    theorem. this is based on the
   assumption which has independence amongst the predictors. in simple
   terms, this could be put up as naive bayes classifier which assumes
   that a particular feature in a class is not exactly directly related to
   any other feature.

   iframe:
   [52]https://www.youtube.com/embed/sjudljfdnkm?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   considering the example, a fruit can be considered an apple only based
   on its color i.e. if the color is red if it is round in shape and if it
   is about 3 inches in terms of diameter. even if these features are
   interdependent and each of the features exist because of the other
   feature. all these properties got to contribute independently to the
   id203 of the outcome of fruit that it is an apple and the reason
   being it would be naive.

   naive bayes model isn   t difficult to build and is really useful for
   very large datasets. along with simplicity, naive bayes is also
   considered to have outperformed all the highly sophisticated
   classification methods.

support vector machines for classification problems (id166)

   support vector machine is proved to be a supervised machine learning
   method. this is considered to be used in solving both regression and
   the classification problems. generally, support vector is used as a
   classifier so that we can discuss id166 as how it is a classifier. well,
   like other machines it doesn   t have gears, valves, or different
   electronic parts nevertheless; it does what it can with normal machines
   to do: it takes the input, does the manipulation of the input and then
   provides the output.

   iframe:
   [53]https://www.youtube.com/embed/n1vogolbjsc?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [54]learn more

   to be apt, in a given labeled training data id166 outputs, it applies an
   optimal hyperplane. this later helps in categorizing new examples.

id79 for classification and regression problems

   you have probably already guessed the answer having learned about
   id90. yes, just the way a forest is a collection of trees, a
   id79 is also a collection of id90. [55]decision
   trees that are grown very deep often indulge in overfitting the
   training data so they can show high variation even on a small change in
   an input data.

   iframe:
   [56]https://www.youtube.com/embed/d_2lkhmjcfy?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [57]learn more

   they are always sensitive to the specific data on which they can be
   trained so that they can remain error-prone to test data sets. the
   id79 algorithm helps to grow many such id90 and
   provide the average of the different classification trees (or the
   mode). this reduces the variance. the different classification trees
   are trained on the basis of different parts of the training dataset. in
   order to classify a new object from an input vector, put the input
   vector down, with each of the trees in the forest. each tree gives a
   classification, the forest then chooses the classification of having
   the most votes or the average of all the trees in the forest.

id75 for regression problems

   as the name indicates this already, id75 is well known to
   be an approach for modeling the relationship that lies in between a
   dependent variable    y    and another or more independent variables that
   are denoted as    x    and expressed in a linear form. the word linear
   indicates that the dependent variable is directly proportional to the
   independent variables. there are other things that are to be kept in
   mind.

   iframe:
   [58]https://www.youtube.com/embed/fibvs5gbblq?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [59]learn more

   it has to be constant as if x is increased/decreased then y also
   changes linearly. mathematically the relationship is based and
   expressed in the simplest form as: this is

   y = ax + b

   here a and b are considered to be the constant factors. the goal hidden
   behind the supervised learning using id75 is to find the
   exact value of the constants    a    and    b    with the help of the data
   sets. then these values, i.e. the value of the constants will be
   helpful in predicting the values of    y    in the future for any values of
      x   . now, the cases where there is a single and independent variable it
   is termed as simple id75, while if there is the chance of
   more than one independent variable, then this process is called
   multiple id75.

ordinary least squares regression

   the ordinary least squares regression or call it ordinary least squares
   (ols). the linear least squares. when we consider the statistics, this
   is a method where we estimate the unknown parameters. this is known as
   the id75 model, it comes with the goal which minimizes the
   differences of the observed responses in some arbitrary dataset.

   iframe:
   [60]https://www.youtube.com/embed/szxbuo3bvrk?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   also, minimizes the responses that are very well predicted by the
   linear approximation of the data (visually this can be seen as the sum,
   which is of the vertical distances falling in between each data point
   in the set and the corresponding points on the regression line     it is
   observed that the smaller the differences are, the better would be the
   model that fits the data). the resulting estimator can be expressed in
   the form of a simple formula, especially when this falls in the case of
   a single regressor and is on the right-hand side. the ols estimators
   are known to be really consistent whereas the regressors are exogenous
   and there lies no perfect multicollinearity, and this remains optimal
   in the class of the linear unbiased estimators. while there are errors,
   these are homoscedastic and serially uncorrelated. under these
   conditions, there is a method of ols. it provides with the
   minimum-variance, there is a mean-unbiased estimation, here the errors
   would have finite variances. under these additional assumptions, there
   are errors that could be normally distributed. the ols algorithm is the
   maximum likelihood estimator. the ols is mostly used in the subject
   matter such as economics (econometrics), in political science and then
   electrical engineering (control theory and the signal processing),
   there are many other areas of application. the multi-fractional order
   estimator is known to be an expanded version of the ols.

id28

   id28 is a supervised machine learning algorithm used for
   classification. though the    regression    in its name can be somehow
   misleading let   s not mistake it as some sort of regression algorithm.
   the name id28 came from a special function called
   logistic function which plays a central role in this method.

   iframe:
   [61]https://www.youtube.com/embed/qsthzvn8hzs?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [62]learn more

   a id28 model is termed as a probabilistic model. it
   helps in finding the id203 that a new instance belongs to a
   certain class. since it is id203, the output lies between 0 and
   1. whenever we are using the id28 as a binary classifier
   (classification done into two classes), we can consider the classes to
   be a positive class and a negative class. we then find the id203.
   higher the id203 (greater than 0.5), it is likelier that it falls
   into the positive class. similarly, if the id203 is low (less
   than 0.5), we can classify this into the negative class.

   let   s consider an example of classifying emails into the spam malignant
   and ham (not spam). we assume that the malignant spam would be falling
   in the positive class and benign ham would be in the negative class.
   what we can do in the beginning is to take several labeled examples of
   emails and then use it to train the model. after training it, this can
   be used really well to predict the class of new email based examples.
   when we feed the examples to our model, it returns to us a value, say
   it is y such that 0   y   1. suppose, the value we get is 0.8. from this
   value, we can say or predict that there is  80% id203 that tested
   examples are a kind of spam. thus this can be classified it in the form
   of a spam mail.

ensemble methods

   ensemble methods are the meta-algorithms that combine several machine
   learning algorithms and techniques into one predictive model in order
   to decrease the variance (id112), bias (boosting) or improve the
   predictions (stacking).

   iframe:
   [63]https://www.youtube.com/embed/m-s9hojj1as?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [64]learn more

   the ensemble methods can be divided into two groups:
     * the sequential ensemble methods are derived totally from where the
       base learners. and then this is generated sequentially (e.g.
       adaboost).
     * the primary motivation of sequential methods is mainly to exploit
       the dependence that falls in between the base learners. the overall
       performance can be increased and boosted by weighing all the
       previously mislabeled examples with higher weight.
     * the parallel ensemble methods where the base learners are generated
       in parallel (e.g. id79).
     * then there is the basic motivation called the parallel methods
       which help to exploit independence that falls in between the base
       learners since the error here can be reduced dramatically by
       averaging.
     * most ensemble methods make use of a single base learning algorithm
       to produce homogeneous base learners, i.e. learners who fall in the
       same type, leading to homogeneous ensembles.

   there are also some methods that are continuously using heterogeneous
   learners, i.e. learners that are of different types, this leads to
   heterogeneous ensembles. in order for ensemble methods to be more
   accurate than any of its individual members, the base learners should
   have to be as accurate as possible and even as diverse as possible.

id112

   the term id112 stands for bootstrap aggregation. one way which is
   known to reduce the variance of an estimate is by the average, to
   average together the multiple estimates. for example, we can train m
   the different trees on different subsets of the data (which is chosen
   randomly with replacement) and compute the ensemble:

   id112 is in the list of machine learning algorithms id112 is in the
   list of machine learning algorithms [1x1.trans.gif]
   [id112-is-in-the-list-of-machine-learning-algorithms.png]

boosting

   the term boosting here refers to a family of algorithms that are able
   and successful to convert weak learners into strong learners. the main
   principle of boosting is to fit a sequence that is made out of weak
   learners    models that are only slightly better than any random
   guessing, such as in the form of small id90    to the weighted
   versions of the data. more weight is now given to the examples that
   were misclassified in the earlier rounds.

   the predictions are later combined through a weighted of majority vote
   (classification) or it can be a weighted sum (regression) to help
   produce the final prediction. the principal difference that is found in
   between boosting and the committee methods are such as id112. and
   this says it is the base learners who are trained in sequence on a
   weighted version of the data.

   well, the algorithm below describes the most widely used form of
   boosting algorithm i.e called the adaboost, which basically stands for
   adaptive boosting.

stacking

   stacking is known to be an id108 technique this helps
   combine the multiple classifications or regression models via a
   meta-classifier or it could be a meta-regresser. well, these base level
   models are well trained. and this completely depends on a training set
   and after that, the meta-model is trained in a way which is based on
   the outputs that are received by the base level model as features.

   the base level is known to be consisting of different learning
   algorithms and these algorithms are therefore stacking ensembles that
   are often considered to be known as heterogeneous. the algorithm given
   below summarizes stacking.

unsupervised learning

   unsupervised learning is that algorithm where you only have to
   insert/put the input data (x) and no corresponding output variables are
   to be put.

   the major goal for the unsupervised learning is to help model the
   underlying structure or maybe in the distribution of the data in order
   to help the learners learn more about the data.

   these are termed as unsupervised learning because unlike supervised
   learning which is shown above there are no correct answers and there is
   no teacher to this. algorithms are left to their own devices to help
   discover and present the interesting structure that is present in the
   data.

   iframe:
   [65]https://www.youtube.com/embed/idsywhn-u08?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   unsupervised learning problems can even be grouped ahead into
   id91 and association problems.
    1. id91: a id91 is that problem which indicates what you
       want to discover and this helps in the inherent groupings of the
       data, such as grouping the customers based on their purchasing
       behavior.
    2. association:  an association rule is termed to be the learning
       problem. this is where you would be discovering the exact rules
       that will describe the large portions of your data. example: people
       who buy x are also the one who tends to buy y.

   some popular examples of unsupervised learning algorithms are:
     * [66]id116 for id91 problems
     * [67]apriori algorithm for association rule learning problems
     * [68]principal component analysis
     * [69]singular value decomposition
     * [70]independent component analysis

   [71]learn how unsupervised machine learning works - list of machine
   learning algorithms learn how unsupervised machine learning works -
   list of machine learning algorithms

   how unsupervised machine learning works?     image source: boozallen.com

id116 id91 algorithm

   id116, it is one of the simplest unsupervised learning algorithms
   that will solve the most well-known id91 problem. the procedure
   can be grouped as the one which follows a simple and very easy way to
   classify a given data set with the help of a certain number of clusters
   (assume k clusters) fixed apriori. the main idea here is to define k
   centers, which takes one for each cluster. these centers should now be
   planned and placed in an absolute cunning way because it has got
   various locations leading or causing a different result. so, there is a
   better choice, which is to place them very far away from each other. as
   far as possible. then comes the next step which is to take each point
   that is belonging to a given data set and can be associated with the
   nearest center. when there is no point pending, the first step is
   already completed and a complete early group age is done. this is the
   point, where we all need to do the re-calculation. here,    k    is the
   complete new centroids as barycenter of the clusters which actually
   results from the previous or the earlier step. also, after we have got
   these k new centroids, a new binding has to be done. this will need to
   be in between the same data set points and the nearest new center. a
   loop has to be generated. as a result of this loop, we may notice that
   the k centers will be changing the location step by step. this will
   continue until no more changes are to be done or in other words, can
   say the centers do not move anymore. finally, this algorithm is always
   aiming at minimizing an objective function which is known to be as
   squared error function given and explained as such: id116 is in the
   list of machine learning algorithms id116 is in the list of machine
   learning algorithms [1x1.trans.gif] [id116-algorithms.jpg]

   where,
                                 ||x[i ]    v[j]||    is the euclidean distance
   between x[i] and v[j.]

                                 c[i]    is the number of data points
   in i^th cluster.

                                 c    is the number of cluster centers.

   iframe:
   [72]https://www.youtube.com/embed/4r8nwdh-wa0?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [73]learn more

apriori algorithm for association rule learning problems

   apriori is considered an algorithm for frequent itemset mining and
   association rule learning over transactional databases. it proceeds
   just by identifying the frequent individual items in the database and
   then extending them to larger and larger item sets. the observation is,
   for as long as those itemsets appear sufficiently often in the
   database. the frequent itemsets that were determined by apriori can be
   later used to determine about the association rules which highlights
   all the general trends that are being used in the database: this has
   got applications that fall in the domains such as the market basket
   analysis.

   iframe:
   [74]https://www.youtube.com/embed/wglmls_yydk?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [75]learn more

principal component analysis

   the main idea which falls behind the principal component analysis (pca)
   is to help in reducing the dimensionality of the dataset which consists
   of many variables, that are always correlated with each other, either
   in a heavy or light manner, while retaining the variation which is
   present in the dataset, up to its maximum extent. the same thing is
   repeated and done by transforming and bringing the variables to a whole
   new set of variables, which are called the principal components (or
   simply, the pcs) and are even termed to be orthogonal, ordered in such
   a way that the retention of variation which is  present in the original
   variables can be decreased as we try to  move down in the proper order.
   so, by following this particular way, the 1st principal component
   retains the most and maximum variation that was earlier present in the
   original components. the principal components are basically known to be
   the eigenvectors of a covariance matrix, and hence they are even called
   the orthogonal.

   most importantly, the dataset which is based on what the pca techniques
   are to be used and must be scaled. the result also turns out to be
   sensitive based on the relative scaling. as a layman, it can be termed
   as a method of summarizing data. just imagine having some wine bottles
   on your dining table. each wine would be described only by its
   attributes, that are like colour, age, strength, etc. but eventually,
   redundancy will arise maybe because many of them would be measured
   based on the related properties. so what does pca  have to do or has to
   offer in this case? it will basically summarize each wine in the stock
   with really fewer characteristics.

   iframe:
   [76]https://www.youtube.com/embed/d1itn4pafki?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [77]learn more on pca

singular value decomposition

   in id202, you can call the id166 (svd)
   as a factorization of maybe real or complex matrix. it is the
   generalization of the eigendecomposition, that is the origin of
   a positive semidefinite normal matrix is done somewhere over here (for
   example, take a symmetric matrix which has actually got the positive
   eigenvalues) to any {\displaystyle m\times n} m\times n m\times n
   [1x1.trans.gif] [12b23d207d23dd430b93320539abbb0bde84870d]  matrix via
   an extension which is lying under the polar decomposition. it has many
   useful applications that are signal processing and are into statistics.

   iframe:
   [78]https://www.youtube.com/embed/mbclrguafuk?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [79]learn more on svd

   the id166 can be computed easily by making the
   use of the following observations:
     * the left-singular vectors of m are considered to be a set
       of orthonormal eigenvectors of mm^   .
     * the right-singular vectors of m are actually the set of orthonormal
       eigenvectors of m^   m.
     * the non-zero singular values of m (that are found on the diagonal
       entries of   ) are considered to be the square roots of the
       non-zero eigenvalues of both m^   m and mm^   .

   applications that help to employ the svd include computing of
   the pseudoinverse, the least squares fitting of data, multivariable
   control, matrix approximation, and determining the rank, range and null
   space of a matrix.

independent component analysis

   now, consider the independent component analysis (ica), it is
   considered to be a statistical and computational technique. it helps to
   bring our or in revealing hidden factors that underlie in the sets of
   random variables, measurements, or signals.

   ica helps to define a generative model. this model stands for the
   observed multivariate data. it is typically recognized in the form of a
   large database of samples. well, in the model, the data variables are
   assumed to be the linear mixtures of few less known

   known or you can call it as unknown latent variables, and even the
   mixing system is also unknown. then comes the latent variables. these
   variables are actually assumed to be the nongaussian. they are even the
   mutually independent ones. these could be termed as the independent
   components belonging in the category of the observed data. these
   independent components, also termed as the sources or factors, can be
   found by the ica.

   ica, the term is basically superficially related to the principal
   component analysis and then to the factor analysis. ica is considered
   and supposedly it is  a much more powerful technique. still, however
   this would be always capable of finding the underlying factors. it can
   even be the sources if possible by any chance, if these classic methods
   fail completely anyhow.

   the data which is analyzed by the ica could be originating from various
   kinds of application fields, this could be including digital images,
   the document databases, the economic indicators and then the
   psychometric measurements. in many cases, these measurements are given
   to be considered as a set of parallel signals or time series; the term
   blind source separation is then used in this to characterize this
   problem. typical examples are actually the mixtures of simultaneous
   speech signals that have been picked up by several microphones, these
   are the brain waves that is recorded by multiple sensors and then the
   interfering radio signals that arriving at a mobile phone, or maybe the
   parallel time series which is obtained from performing some industrial
   process.

   iframe:
   [80]https://www.youtube.com/embed/wilrddnbxdo?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   [81]learn more on ica

reinforcement or semi-supervised machine learning

   there are problems where you   ll find yourself that you   ve found a large
   amount of input data. let   s consider it as (x) and then later some of
   the data is labeled as (y). these are termed as semi-supervised
   learning problems.

   iframe:
   [82]https://www.youtube.com/embed/e3jy2vshroe?autoplay=0&showinfo=1&con
   trols=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque

   these problems will actually sit in between supervised learning and
   then the unsupervised learning.

   a good example would be to photo archive the places where only some of
   the images are labeled, (e.g. dog, cat, person) and the majority of the
   place is unlabeled.

   many of the realistic-world machine learning related problems fall into
   this category. this is because it could be really expensive or maybe
   time-consuming. to label this data as it may require the access to get
   through the domain experts. the unlabeled data is cheap and
   comparatively easy to collect and store.

   you can use these unsupervised learning techniques to do wonders. this
   can help you discover and learn the various valid structures that are
   in the input variables.

   you can also use the supervised learning techniques to make the best of
   the guess predictions which would be belonging to the unlabeled data.
   you can then feed that data back into the supervised learning algorithm
   as training data does and then later use the model to make predictions
   based on new unseen data.

must have machine learning algorithm cheat sheet

   [83]machine learning algorithm cheat sheet machine learning algorithm
   cheat sheet
   [84]download the machine learning algorithm cheat sheet from here
   tags:[85]machine learning algorithms

most people like this blog

   [86]machine learning vs robotic process automation machine learning vs
   robotic process automation [1x1.trans.gif]
   [machine-learning-vs-robotic-process-automation.jpg]
   [87]january 3, 2018 [88]machine learning

[89]machine learning vs robotic process automation

   [90]read more

   [91]tapping into rpa for social good tapping into rpa for social good
   [1x1.trans.gif] [tapping-into-rpa-for-social-good.jpg]
   [92]january 18, 2018 [93]machine learning

[94]tapping into robotic process automation for the social good

   [95]read more

   [96]deep learning subset of machine learning deep learning subset of
   machine learning [1x1.trans.gif]
   [deep-learning-subset-of-machine-learning.jpg]
   [97]february 13, 2018 [98]machine learning

[99]deep learning     in demand subset of machine learning

   [100]read more

   ntd logo

   newtechdojo is an on-demand marketplace to learn from the best and
   experienced industry experts. get trained from the top data science
   consultants and programmers. take this opportunity, explore your career
   in data science and learn from the skilled and upbeat mentors.

latest news

     * [101]power bi for the beginners
     * [102]unpivot columns in power bi
     * [103]what is data science and it   s tools
     * [104]sas introduction & overview
     * [105]what is data mining?

tags

   [106]amd radeon [107]artificial intelligence [108]coca cola
   [109]id161 [110]deep learning [111]deep learning using python
   [112]drone industry [113]facebook [114]fashion trends [115]gpu for
   machine learning [116]ibm watson [117]jobs [118]learning python
   [119]machine learning [120]machine learning algorithms [121]machine
   learning with tensorflow [122]marketers [123]microsoft azure
   [124]neural network [125]powerbi [126]predictive analytics [127]pwc
   [128]python machine learning [129]ted talks on machine learning
   [130]tensorflow course [131]tensorflow deep learning [132]tensorflow
   tutorial [133]youtube

   newtechdojo. all rights reserved.
     * [134]home
     * [135]blog
     * [136]expert interview

   (button) (button) (button)
   (button) (button)

references

   1. https://www.newtechdojo.com/feed/
   2. https://www.newtechdojo.com/wp-json/oembed/1.0/embed?url=https://www.newtechdojo.com/list-machine-learning-algorithms/
   3. https://www.newtechdojo.com/wp-json/oembed/1.0/embed?url=https://www.newtechdojo.com/list-machine-learning-algorithms/&format=xml
   4. https://www.newtechdojo.com/
   5. https://www.newtechdojo.com/
   6. https://www.newtechdojo.com/blog/
   7. https://www.newtechdojo.com/expert-interview/
   8. https://www.newtechdojo.com/author/admin/
   9. https://www.newtechdojo.com/category/machine-learning/
  10. https://www.newtechdojo.com/list-machine-learning-algorithms/
  11. https://www.newtechdojo.com/list-machine-learning-algorithms/#machine_learning_algorithms
  12. https://www.newtechdojo.com/list-machine-learning-algorithms/#the_machine_learning_algorithm_list_includes
  13. https://www.newtechdojo.com/list-machine-learning-algorithms/#the_three_categories_of_these_machine_learning_algorithms_are
  14. https://www.newtechdojo.com/list-machine-learning-algorithms/#supervised_learning
  15. https://www.newtechdojo.com/list-machine-learning-algorithms/#decision_trees
  16. https://www.newtechdojo.com/list-machine-learning-algorithms/#naive_bayes_classification
  17. https://www.newtechdojo.com/list-machine-learning-algorithms/#support_vector_machines_for_classification_problems_id166
  18. https://www.newtechdojo.com/list-machine-learning-algorithms/#random_forest_for_classification_and_regression_problems
  19. https://www.newtechdojo.com/list-machine-learning-algorithms/#linear_regression_for_regression_problems
  20. https://www.newtechdojo.com/list-machine-learning-algorithms/#ordinary_least_squares_regression
  21. https://www.newtechdojo.com/list-machine-learning-algorithms/#logistic_regression
  22. https://www.newtechdojo.com/list-machine-learning-algorithms/#ensemble_methods
  23. https://www.newtechdojo.com/list-machine-learning-algorithms/#id112
  24. https://www.newtechdojo.com/list-machine-learning-algorithms/#boosting
  25. https://www.newtechdojo.com/list-machine-learning-algorithms/#stacking
  26. https://www.newtechdojo.com/list-machine-learning-algorithms/#unsupervised_learning
  27. https://www.newtechdojo.com/list-machine-learning-algorithms/#id116_id91_algorithm
  28. https://www.newtechdojo.com/list-machine-learning-algorithms/#apriori_algorithm_for_association_rule_learning_problems
  29. https://www.newtechdojo.com/list-machine-learning-algorithms/#principal_component_analysis
  30. https://www.newtechdojo.com/list-machine-learning-algorithms/#singular_value_decomposition
  31. https://www.newtechdojo.com/list-machine-learning-algorithms/#independent_component_analysis
  32. https://www.newtechdojo.com/list-machine-learning-algorithms/#reinforcement_or_semi-supervised_machine_learning
  33. https://www.newtechdojo.com/list-machine-learning-algorithms/#must_have_machine_learning_algorithm_cheat_sheet
  34. https://www.newtechdojo.com/list-machine-learning-algorithms/#supervised learning
  35. https://www.newtechdojo.com/list-machine-learning-algorithms/#unsupervised learning
  36. https://www.newtechdojo.com/list-machine-learning-algorithms/#id23
  37. https://www.youtube.com/embed/hfo6irj-gzo?rel=0&controls=0&showinfo=0
  38. https://www.newtechdojo.com/list-machine-learning-algorithms/#id90
  39. https://www.newtechdojo.com/list-machine-learning-algorithms/#naive bayes classification
  40. https://www.newtechdojo.com/list-machine-learning-algorithms/#support vector machines for classification problems
  41. https://www.newtechdojo.com/list-machine-learning-algorithms/#id79 for classification and regression problems
  42. https://www.newtechdojo.com/list-machine-learning-algorithms/#id75 for regression problems
  43. https://www.newtechdojo.com/list-machine-learning-algorithms/#ordinary least squares regression
  44. https://www.newtechdojo.com/list-machine-learning-algorithms/#id28
  45. https://www.newtechdojo.com/list-machine-learning-algorithms/#ensemble methods
  46. https://www.newtechdojo.com/wp-content/uploads/2018/03/how-supervised-machine-learning-works.jpg
  47. https://www.boozallen.com/content/dam/boozallen_site/sig/pdf/publications/machine-intelligence-quick-guide-to-how-machines-learn.pdf
  48. http://wikipedia.org/
  49. https://www.youtube.com/embed/ldrbo9a6xpu?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  50. https://www.newtechdojo.com/learn-decision-tree-algorithm-using-excel/
  51. https://en.wikipedia.org/wiki/bayes'_theorem
  52. https://www.youtube.com/embed/sjudljfdnkm?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  53. https://www.youtube.com/embed/n1vogolbjsc?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  54. https://www.newtechdojo.com/learn-support-vector-machine-using-excel/
  55. https://www.newtechdojo.com/learn-decision-tree-algorithm-using-excel/
  56. https://www.youtube.com/embed/d_2lkhmjcfy?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  57. https://www.newtechdojo.com/learn-random-forest-using-excel/
  58. https://www.youtube.com/embed/fibvs5gbblq?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  59. https://www.newtechdojo.com/learn-linear-regression-using-excel/
  60. https://www.youtube.com/embed/szxbuo3bvrk?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  61. https://www.youtube.com/embed/qsthzvn8hzs?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  62. https://www.newtechdojo.com/learn-logistic-regression-using-excel/
  63. https://www.youtube.com/embed/m-s9hojj1as?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  64. https://blog.statsbot.co/ensemble-learning-d1dcd548e936
  65. https://www.youtube.com/embed/idsywhn-u08?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  66. https://www.newtechdojo.com/list-machine-learning-algorithms/#id116 for id91 problems
  67. https://www.newtechdojo.com/list-machine-learning-algorithms/#apriori algorithm for association rule learning problems
  68. https://www.newtechdojo.com/list-machine-learning-algorithms/#principal component analysis
  69. https://www.newtechdojo.com/list-machine-learning-algorithms/#singular value decomposition
  70. https://www.newtechdojo.com/list-machine-learning-algorithms/#independent component analysis
  71. https://www.newtechdojo.com/wp-content/uploads/2018/03/how-unsupervised-machine-learning-works.jpg
  72. https://www.youtube.com/embed/4r8nwdh-wa0?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  73. https://sites.google.com/site/dataid91algorithms/id116-id91-algorithm
  74. https://www.youtube.com/embed/wglmls_yydk?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  75. https://en.wikipedia.org/wiki/apriori_algorithm
  76. https://www.youtube.com/embed/d1itn4pafki?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  77. https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial
  78. https://www.youtube.com/embed/mbclrguafuk?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  79. https://en.wikipedia.org/wiki/singular-value_decomposition
  80. https://www.youtube.com/embed/wilrddnbxdo?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  81. http://cis.legacy.ics.tkk.fi/aapo/papers/ijid9899_tutorialweb/
  82. https://www.youtube.com/embed/e3jy2vshroe?autoplay=0&showinfo=1&controls=1&autohide=1&rel=0&loop=0&modestbranding=0&vq=auto&wmode=opaque
  83. https://www.newtechdojo.com/wp-content/uploads/2018/02/machine-learning-algorithm-cheat-sheet-1.png
  84. http://download.microsoft.com/download/a/6/1/a613e11e-8f9c-424a-b99d-65344785c288/microsoft-machine-learning-algorithm-cheat-sheet-v6.pdf
  85. https://www.newtechdojo.com/tag/machine-learning-algorithms/
  86. https://www.newtechdojo.com/machine-learning-vs-robotic-process-automation/
  87. https://www.newtechdojo.com/machine-learning-vs-robotic-process-automation/
  88. https://www.newtechdojo.com/category/machine-learning/
  89. https://www.newtechdojo.com/machine-learning-vs-robotic-process-automation/
  90. https://www.newtechdojo.com/machine-learning-vs-robotic-process-automation/
  91. https://www.newtechdojo.com/tapping-into-robotic-process-automation-for-the-social-good/
  92. https://www.newtechdojo.com/tapping-into-robotic-process-automation-for-the-social-good/
  93. https://www.newtechdojo.com/category/machine-learning/
  94. https://www.newtechdojo.com/tapping-into-robotic-process-automation-for-the-social-good/
  95. https://www.newtechdojo.com/tapping-into-robotic-process-automation-for-the-social-good/
  96. https://www.newtechdojo.com/deep-learning-in-demand-subset-of-machine-learning/
  97. https://www.newtechdojo.com/deep-learning-in-demand-subset-of-machine-learning/
  98. https://www.newtechdojo.com/category/machine-learning/
  99. https://www.newtechdojo.com/deep-learning-in-demand-subset-of-machine-learning/
 100. https://www.newtechdojo.com/deep-learning-in-demand-subset-of-machine-learning/
 101. https://www.newtechdojo.com/power-bi-for-the-beginners/
 102. https://www.newtechdojo.com/unpivot-columns-in-power-bi/
 103. https://www.newtechdojo.com/what-is-data-science-and-its-tools/
 104. https://www.newtechdojo.com/overview-of-sas-download-certification/
 105. https://www.newtechdojo.com/what-is-data-mining/
 106. https://www.newtechdojo.com/tag/amd-radeon/
 107. https://www.newtechdojo.com/tag/artificial-intelligence/
 108. https://www.newtechdojo.com/tag/coca-cola/
 109. https://www.newtechdojo.com/tag/computer-vision/
 110. https://www.newtechdojo.com/tag/deep-learning/
 111. https://www.newtechdojo.com/tag/deep-learning-using-python/
 112. https://www.newtechdojo.com/tag/drone-industry/
 113. https://www.newtechdojo.com/tag/facebook/
 114. https://www.newtechdojo.com/tag/fashion-trends/
 115. https://www.newtechdojo.com/tag/gpu-for-machine-learning/
 116. https://www.newtechdojo.com/tag/ibm-watson/
 117. https://www.newtechdojo.com/tag/jobs/
 118. https://www.newtechdojo.com/tag/learning-python/
 119. https://www.newtechdojo.com/tag/machine-learning/
 120. https://www.newtechdojo.com/tag/machine-learning-algorithms/
 121. https://www.newtechdojo.com/tag/machine-learning-with-tensorflow/
 122. https://www.newtechdojo.com/tag/marketers/
 123. https://www.newtechdojo.com/tag/microsoft-azure/
 124. https://www.newtechdojo.com/tag/neural-network/
 125. https://www.newtechdojo.com/tag/powerbi/
 126. https://www.newtechdojo.com/tag/predictive-analytics/
 127. https://www.newtechdojo.com/tag/pwc/
 128. https://www.newtechdojo.com/tag/python-machine-learning/
 129. https://www.newtechdojo.com/tag/ted-talks-on-machine-learning/
 130. https://www.newtechdojo.com/tag/tensorflow-course/
 131. https://www.newtechdojo.com/tag/tensorflow-deep-learning/
 132. https://www.newtechdojo.com/tag/tensorflow-tutorial/
 133. https://www.newtechdojo.com/tag/youtube/
 134. https://www.newtechdojo.com/
 135. https://www.newtechdojo.com/blog/
 136. https://www.newtechdojo.com/expert-interview/
