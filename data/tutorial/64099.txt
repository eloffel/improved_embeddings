   #[1]real python

   [2]real python (button)
     * [3]start here
     * [4]tutorials
     * [5]store
       [6]become a member [7]python basics book [8]python tricks book
       [9]the real python course [10]managing python dependencies
       [11]sublime text + python setup [12]pythonic wallpapers pack
       [13]python mugs, t-shirts, and more [14]pythonista cafe community
       [15]browse all   
     * [16]more
       [17]video courses [18]python quizzes [19]learning paths
       [20]newsletter [21]resources [22]the team [23]write for us
       [24]python job board

   ____________________
     * [25]join
     * [26]sign in

   [27]

   watch video lessons on real python (new feature)   

   python web scraping

practical introduction to web scraping in python

   by [28]colin okeefe [29]basics [30]web-scraping

   table of contents
     * [31]setting up your python web scraper
     * [32]making web requests
     * [33]wrangling html with beautifulsoup
     * [34]using beautifulsoup to get mathematician names
     * [35]getting the popularity score
     * [36]putting it all together
     * [37]conclusion & next steps

   what is web scraping all about?

   imagine that one day, out of the blue, you find yourself thinking    gee,
   i wonder who the five most popular mathematicians are?   

   you do a bit of thinking, and you get the idea to use [38]wikipedia   s
   xtools to measure the popularity of a mathematician by equating
   popularity with pageviews. for example, look at the [39]page on henri
   poincar  . there, you can see that poincar     s pageviews for the last 60
   days are, as of december 2017, around 32,000.

   next, you google    famous mathematicians    and find [40]this resource
   that lists 100 names. now you have a page listing mathematicians    names
   as well as a website that provides information about how    popular    that
   mathematician is. now what?

   this is where python and web scraping come in. web scraping is about
   downloading structured data from the web, selecting some of that data,
   and passing along what you selected to another process.

   in this tutorial, you will be writing a python program that downloads
   the list of 100 mathematicians and their xtools pages, selects data
   about their popularity, and finishes by telling us the top 5 most
   popular mathematicians of all time! let   s get started.

   important: we   ve received an email from an xtools maintainer informing
   us that scraping xtools is harmful and that automation apis should be
   used instead:

     this article on your site is essentially a guide to scraping xtools
     [   ] this is not necessary, and it   s causing problems for us. we have
     [41]apis that should be used for automation, and furthermore, for
     pageviews specifically folks should be using the [42]official
     pageviews api.

   the example code in the article was modified to no longer make requests
   to the xtools website. the web scraping techniques demonstrated here
   are still valid, but please do not use them on web pages of the xtools
   project. use the provided automation api instead.

setting up your python web scraper

   you will be using python 3 and python [43]virtual environments
   throughout the tutorial. feel free to set things up however you like.
   here is how i tend to do it:
$ python3 -m venv venv
$ . ./venv/bin/activate

   you will need to install only these two packages:
     * [44]requests for performing your http requests
     * [45]beautifulsoup4 for handling all of your html processing

   let   s install these dependencies with pip:
$ pip install requests beautifulsoup4

   finally, if you want to follow along, fire up your favorite text editor
   and create a file called mathematicians.py. get started by including
   these import statements at the top:
from requests import get
from requests.exceptions import requestexception
from contextlib import closing
from bs4 import beautifulsoup

making web requests

   your first task will be to download web pages. the requests package
   comes to the rescue. it aims to be an easy-to-use tool for doing all
   things http in python, and it doesn   t dissappoint. in this tutorial,
   you will need only the requests.get() function, but you should
   definitely checkout the [46]full documentation when you want to go
   further.

   first, here   s your function:
def simple_get(url):
    """
    attempts to get the content at `url` by making an http get request.
    if the content-type of response is some kind of html/xml, return the
    text content, otherwise return none.
    """
    try:
        with closing(get(url, stream=true)) as resp:
            if is_good_response(resp):
                return resp.content
            else:
                return none

    except requestexception as e:
        log_error('error during requests to {0} : {1}'.format(url, str(e)))
        return none


def is_good_response(resp):
    """
    returns true if the response seems to be html, false otherwise.
    """
    content_type = resp.headers['content-type'].lower()
    return (resp.status_code == 200
            and content_type is not none
            and content_type.find('html') > -1)


def log_error(e):
    """
    it is always a good idea to log errors.
    this function just prints them, but you can
    make it do anything.
    """
    print(e)

   the simple_get() function accepts a single url argument. it then makes
   a get request to that url. if nothing goes wrong, you end up with the
   raw html content for the page you requested. if there were any problems
   with your request (like the url is bad, or the remote server is down),
   then your function returns none.

   you may have noticed the use of the closing() function in your
   definition of simple_get(). the closing() function ensures that any
   network resources are freed when they go out of scope in that with
   block. using closing() like that is good practice and helps to prevent
   fatal errors and network timeouts.

   you can test simple_get() like this:
   >>>
>>> from mathematicians import simple_get
>>> raw_html = simple_get('https://realpython.com/blog/')
>>> len(raw_html)
33878

>>> no_html = simple_get('https://realpython.com/blog/nope-not-gonna-find-it')
>>> no_html is none
true

wrangling html with beautifulsoup

   once you have raw html in front of you, you can start to select and
   extract. for this purpose, you will be using beautifulsoup. the
   beautifulsoup constructor parses raw html strings and produces an
   object that mirrors the html document   s structure. the object includes
   a slew of methods to select, view, and manipulate dom nodes and text
   content.

   consider the following quick and contrived example of an html document:
<!doctype html>
<html>
<head>
  <title>contrived example</title>
</head>
<body>
<p id="eggman"> i am the egg man </p>
<p id="walrus"> i am the walrus </p>
</body>
</html>

   if the above html is saved in the file contrived.html, then you can use
   beautifulsoup like this:
   >>>
>>> from bs4 import beautifulsoup
>>> raw_html = open('contrived.html').read()
>>> html = beautifulsoup(raw_html, 'html.parser')
>>> for p in html.select('p'):
...     if p['id'] == 'walrus':
...         print(p.text)

'i am the walrus'

   breaking down the example, you first parse the raw html by passing it
   to the beautifulsoup constructor. beautifulsoup accepts multiple
   back-end parsers, but the standard back-end is 'html.parser', which you
   supply here as the second argument. (if you neglect to supply that
   'html.parser', then the code will still work, but you will see a
   warning print to your screen.)

   the select() method on your html object lets you use [47]css selectors
   to locate elements in the document. in the above case, html.select('p')
   returns a list of paragraph elements. each p has html attributes that
   you can access like a dict. in the line if p['id'] == 'walrus', for
   example, you check if the id attribute is equal to the string 'walrus',
   which corresponds to <p id="walrus"> in the html.

using beautifulsoup to get mathematician names

   now that you have given the select() method in beautifulsoup a short
   test drive, how do you find out what to supply to select()? the fastest
   way is to step out of python and into your web browser   s developer
   tools. you can use your browser to examine the document in some detail.
   i usually look for id or class element attributes or any other
   information that uniquely identifies the information i want to extract.

   to make matters concrete, turn to the [48]list of mathematicians you
   saw earlier. if you spend a minute or two looking at this page   s
   source, you can see that each mathematician   s name appears inside the
   text content of an <li> tag. to make matters even simpler, <li> tags on
   this page seem to contain nothing but names of mathematicians.

   here   s a quick look with python:
   >>>
>>> raw_html = simple_get('http://www.fabpedigree.com/james/mathmen.htm')
>>> html = beautifulsoup(raw_html, 'html.parser')
>>> for i, li in enumerate(html.select('li')):
        print(i, li.text)

0  isaac newton
 archimedes
 carl f. gauss
 leonhard euler
 bernhard riemann

1  archimedes
 carl f. gauss
 leonhard euler
 bernhard riemann

2  carl f. gauss
 leonhard euler
 bernhard riemann

 3  leonhard euler
 bernhard riemann

4  bernhard riemann

# 5 ... and many more...

   the above experiment shows that some of the <li> elements contain
   multiple names separated by newline characters, while others contain
   just a single name. with this information in mind, you can write your
   function to extract a single list of names:
def get_names():
    """
    downloads the page where the list of mathematicians is found
    and returns a list of strings, one per mathematician
    """
    url = 'http://www.fabpedigree.com/james/mathmen.htm'
    response = simple_get(url)

    if response is not none:
        html = beautifulsoup(response, 'html.parser')
        names = set()
        for li in html.select('li'):
            for name in li.text.split('\n'):
                if len(name) > 0:
                    names.add(name.strip())
        return list(names)

    # raise an exception if we failed to get any data from the url
    raise exception('error retrieving contents at {}'.format(url))

   the get_names() function downloads the page and iterates over the <li>
   elements, picking out each name that occurs. next, you add each name to
   a python set, which ensures that you don   t end up with duplicate names.
   finally, you convert the set to a list and return it.

getting the popularity score

   nice, you   re nearly done! now that you have a list of names, you need
   to pick out the pageviews for each one. the function you write is
   similar to the function you made to get the list of names, only now you
   supply a name and pick out an integer value from the page.

   again, you should first check out an [49]example page in your browser   s
   developer tools. it looks as if the text appears inside an <a> element,
   and the href attribute of that element always contains the string
   'latest-60' as a substring. that   s all the information you need to
   write your function:
def get_hits_on_name(name):
    """
    accepts a `name` of a mathematician and returns the number
    of hits that mathematician's wikipedia page received in the
    last 60 days, as an `int`
    """
    # url_root is a template string that is used to build a url.
    url_root = 'url_removed_see_notice_at_start_of_article'
    response = simple_get(url_root.format(name))

    if response is not none:
        html = beautifulsoup(response, 'html.parser')

        hit_link = [a for a in html.select('a')
                    if a['href'].find('latest-60') > -1]

        if len(hit_link) > 0:
            # strip commas
            link_text = hit_link[0].text.replace(',', '')
            try:
                # convert to integer
                return int(link_text)
            except:
                log_error("couldn't parse {} as an `int`".format(link_text))

    log_error('no pageviews found for {}'.format(name))
    return none

putting it all together

   you have reached a point where you can finally find out which
   mathematician is most beloved by the public! the plan is simple:
     * get a list of names
     * iterate over the list to get a    popularity score    for each name
     * finish by sorting the names by popularity

   simple, right? well, there   s one thing that hasn   t been mentioned yet:
   errors.

   working with real-world data is messy, and trying to force messy data
   into a uniform shape will invariably result in the occasional error
   jumping in to mess with your nice clean vision of how things ought to
   be. ideally, you would like to keep track of errors when they occur in
   order to get a better sense of the of quality your data.

   for your present purposes, you will track instances in which you could
   not find a popularity score for a given mathematician   s name. at the
   end of the script, you will print a message showing the number of
   mathematicians who were left out of the rankings.

   here   s the code:
if __name__ == '__main__':
    print('getting the list of names....')
    names = get_names()
    print('... done.\n')

    results = []

    print('getting stats for each name....')

    for name in names:
        try:
            hits = get_hits_on_name(name)
            if hits is none:
                hits = -1
            results.append((hits, name))
        except:
            results.append((-1, name))
            log_error('error encountered while processing '
                      '{}, skipping'.format(name))

    print('... done.\n')

    results.sort()
    results.reverse()

    if len(results) > 5:
        top_marks = results[:5]
    else:
        top_marks = results

    print('\nthe most popular mathematicians are:\n')
    for (mark, mathematician) in top_marks:
        print('{} with {} pageviews'.format(mathematician, mark))

    no_results = len([res for res in results if res[0] == -1])
    print('\nbut we did not find results for '
          '{} mathematicians on the list'.format(no_results))

   that   s it!

   when you run the script, you should see at the following report:
the most popular mathematicians are:

albert einstein with 1089615 pageviews
isaac newton with 581612 pageviews
srinivasa ramanujan with 407141 pageviews
aristotle with 399480 pageviews
galileo galilei with 375321 pageviews

but we did not find results for 19 mathematicians on our list

conclusion & next steps

   web scraping is a big field, and you have just finished a brief tour of
   that field, using python as you guide. you can get pretty far using
   just requests and beautifulsoup, but as you followed along, you may
   have come up with few questions:
     * what happens if page content loads as a result of asynchronous
       javascript requests? (check out [50]selenium   s python api.)
     * how do i write a web spider or search engine bot that traverses
       large portions of the web?
     * what is this [51]scrapy thing i keep hearing about?

   these are topics for another post    keep your eyes peeled! there will be
   a followup that uses selenium and a headless browser to deal with
   dynamic content:

   get notified: don't miss the follow up to this tutorial   [52]click here
   to join the real python newsletter and you'll know when the next
   installment comes out.

   until then, happy scraping!

        python tricks     

   get a short & sweet python trick delivered to your inbox every couple
   of days. no spam ever. unsubscribe any time. curated by the real python
   team.
   python tricks dictionary merge
   ____________________
   (button) send me python tricks   

   about colin okeefe
   [53]colin okeefe

   colin is a freelance software creative who travels the unixverse in the
   good ship python.
   [54]   more about colin
     __________________________________________________________________

   each tutorial at real python is created by a team of developers so that
   it meets our high quality standards. the team members who worked on
   this tutorial are:
   [55]aldren santos
   [56]

   aldren
   [57]dan bader
   [58]

   dan
   [59]joanna jablonski
   [60]

   joanna

   what do you think?

   real python comment policy: the most useful comments are those written
   with the goal of learning from or helping out other readers   after
   reading the whole article and all the earlier comments. complaints and
   insults generally won   t make the cut here.

   keep learning

   related tutorial categories: [61]basics [62]web-scraping

       free email series    

        python tricks     

   python tricks dictionary merge
   ____________________
   (button) get python tricks   

        no spam. unsubscribe any time.

   all tutorial topics
   [63]advanced [64]api [65]basics [66]best-practices [67]community
   [68]databases [69]data-science [70]devops [71]django [72]docker
   [73]flask [74]front-end [75]intermediate [76]machine-learning
   [77]python [78]testing [79]tools [80]web-dev [81]web-scraping

   table of contents
     * [82]setting up your python web scraper
     * [83]making web requests
     * [84]wrangling html with beautifulsoup
     * [85]using beautifulsoup to get mathematician names
     * [86]getting the popularity score
     * [87]putting it all together
     * [88]conclusion & next steps

   almost there! complete this form and click the button below to gain
   instant access:
   (button)   
   [real-python-logo-square.28474fda9228.png]

   join the real python community newsletter (more than 45,468 python
   developers have subscribed)
   ____________________
   (button) get the newsletter   

      2012   2019 real python     [89]newsletter     [90]youtube     [91]twitter    
   [92]facebook     [93]instagram
   [94]python tutorials     [95]search     [96]privacy policy     [97]advertise
       [98]contact
          happy pythoning!

   [tr?id=2220911568135371&ev=pageview&noscript=1]

references

   visible links
   1. https://realpython.com/atom.xml
   2. https://realpython.com/
   3. https://realpython.com/start-here/
   4. https://realpython.com/
   5. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more
   6. https://realpython.com/account/join/
   7. https://realpython.com/products/python-basics-book/
   8. https://realpython.com/products/python-tricks-book/
   9. https://realpython.com/products/real-python-course/
  10. https://realpython.com/products/managing-python-dependencies/
  11. https://realpython.com/products/sublime-python/
  12. https://realpython.com/products/pythonic-wallpapers/
  13. https://nerdlettering.com/
  14. https://www.pythonistacafe.com/
  15. https://realpython.com/products/
  16. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more
  17. https://realpython.com/courses/
  18. https://realpython.com/quizzes/
  19. https://realpython.com/learning-paths/
  20. https://realpython.com/newsletter/
  21. https://realpython.com/resources/
  22. https://realpython.com/team/
  23. https://realpython.com/write-for-us/
  24. https://www.pythonjobshq.com/
  25. https://realpython.com/account/join/
  26. https://realpython.com/account/login/
  27. https://realpython.com/courses/
  28. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#author
  29. https://realpython.com/tutorials/basics/
  30. https://realpython.com/tutorials/web-scraping/
  31. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#setting-up-your-python-web-scraper
  32. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#making-web-requests
  33. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#wrangling-html-with-beautifulsoup
  34. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#using-beautifulsoup-to-get-mathematician-names
  35. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#getting-the-popularity-score
  36. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#putting-it-all-together
  37. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#conclusion-next-steps
  38. https://www.mediawiki.org/wiki/xtools
  39. https://xtools.wmflabs.org/articleinfo/en.wikipedia.org/henri_poincar  
  40. http://www.fabpedigree.com/james/mathmen.htm
  41. https://xtools.readthedocs.io/en/stable/api/index.html
  42. https://wikimedia.org/api/rest_v1/#!/pageviews_data/get_metrics_pageviews_per_article_project_access_agent_article_granularity_start_end
  43. https://realpython.com/python-virtual-environments-a-primer/
  44. http://docs.python-requests.org/en/master/
  45. https://www.crummy.com/software/beautifulsoup/bs4/doc/
  46. http://docs.python-requests.org/en/master/
  47. https://en.wikipedia.org/wiki/cascading_style_sheets#selector
  48. http://www.fabpedigree.com/james/mathmen.htm
  49. https://xtools.wmflabs.org/articleinfo/en.wikipedia.org/henri_poincar  
  50. http://seleniumhq.github.io/selenium/docs/api/py/
  51. https://realpython.com/web-scraping-and-crawling-with-scrapy-and-mongodb/
  52. https://realpython.com/optins/view/newsletter-dont-miss-updates/
  53. https://realpython.com/team/cokeefe/
  54. https://realpython.com/team/cokeefe/
  55. https://realpython.com/team/asantos/
  56. https://realpython.com/team/asantos/
  57. https://realpython.com/team/dbader/
  58. https://realpython.com/team/dbader/
  59. https://realpython.com/team/jjablonski/
  60. https://realpython.com/team/jjablonski/
  61. https://realpython.com/tutorials/basics/
  62. https://realpython.com/tutorials/web-scraping/
  63. https://realpython.com/tutorials/advanced/
  64. https://realpython.com/tutorials/api/
  65. https://realpython.com/tutorials/basics/
  66. https://realpython.com/tutorials/best-practices/
  67. https://realpython.com/tutorials/community/
  68. https://realpython.com/tutorials/databases/
  69. https://realpython.com/tutorials/data-science/
  70. https://realpython.com/tutorials/devops/
  71. https://realpython.com/tutorials/django/
  72. https://realpython.com/tutorials/docker/
  73. https://realpython.com/tutorials/flask/
  74. https://realpython.com/tutorials/front-end/
  75. https://realpython.com/tutorials/intermediate/
  76. https://realpython.com/tutorials/machine-learning/
  77. https://realpython.com/tutorials/python/
  78. https://realpython.com/tutorials/testing/
  79. https://realpython.com/tutorials/tools/
  80. https://realpython.com/tutorials/web-dev/
  81. https://realpython.com/tutorials/web-scraping/
  82. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#setting-up-your-python-web-scraper
  83. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#making-web-requests
  84. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#wrangling-html-with-beautifulsoup
  85. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#using-beautifulsoup-to-get-mathematician-names
  86. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#getting-the-popularity-score
  87. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#putting-it-all-together
  88. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#conclusion-next-steps
  89. https://realpython.com/newsletter/
  90. https://www.youtube.com/realpython
  91. https://twitter.com/realpython
  92. https://facebook.com/learnrealpython
  93. https://www.instagram.com/realpython/
  94. https://realpython.com/
  95. https://realpython.com/search
  96. https://realpython.com/privacy-policy/
  97. https://realpython.com/sponsorships/
  98. https://realpython.com/contact/

   hidden links:
 100. https://realpython.com/python-web-scraping-practical-introduction/?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more#reader-comments
