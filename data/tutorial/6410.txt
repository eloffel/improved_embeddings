   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a soft introduction to neural networks

   [16]go to the profile of shubhang desai
   [17]shubhang desai (button) blockedunblock (button) followfollowing
   jun 20, 2017
   [1*m2gdbt_nc-ie7r4am3shbq.jpeg]

   over the last few years, neural networks have become synonymous with
   machine learning. recently, we have been able to make neural nets which
   can produce life-like faces, transfer dynamic art style, and even    age   
   a picture of a person by years. if you want to know how the intuition
   behind neural networks work, but have no idea where to start, you   ve
   come to the right place!

   by the end of the tutorial, you can expect to understand the basics of
   how and why neural networks work. you will also create a very simple
   neural network from scratch using only numpy. experience in linear
   algebra/multivariate calculus and python programming are prerequisites
   to this tutorial.

the forward pass

   the project of any neural network is to generate a prediction based on
   some input. this sounds very vague, but this is because the task of
   every neural network is slightly different, so this definition is a
   one-size-fits-all. some examples of inputs might be an image in the
   form of a three-dimensional tensor, an input feature vector of
   properties, or a vector of dictionary embeddings; some examples of
   outputs might be a predicted class label (classification) or a
   predicted non-discrete value (regression).

give me the data

   let   s take a simple example to make this concept less vague. we have
   this toy dataset of sixteen data points, each with four features and
   two possible class labels (0 or 1):
   [1*cshz_fidzgdhj5hdqaxbgg.png]

   and here is the dataset in code:
import numpy as np
x = np.array([[1, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 1,
0], [1, 0, 1, 1], [0, 0, 0, 0], [1, 1, 1, 0], [0, 0, 1, 1], [1, 1, 0, 1], [0, 0,
 1, 0], [1, 0, 0, 0], [1, 1, 1, 1], [0, 1, 1, 1], [1, 0, 0, 1], [1, 0, 0, 1]])
y = np.array([[0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [
1], [0], [0]])

   as you can see, the class label is exactly the same as the value for
   the third feature. we want to make the network accurately transform the
   features into predicted class label probabilities         that is, the output
   should have the probabilities of each of the two possible classes. if
   our network is accurate, we will have a high predicted id203 for
   the correct class and a low predicted id203 for the incorrect
   classes.

   one important note before we continue: we can   t use all of x to train
   our network. this is because we expect our network to do well on data
   it has seen; we are really interested in seeing how it does on data
   it   s hasn   t seen. we want two types of unseen data: a dataset upon
   which we can periodically evaluate our network, known as the validation
   set, and a dataset which simulates    real world data    that we evaluate
   on only once, known as the test set. the rest of the data used for
   training is known as the training set. here   s the code to perform that
   split:
x_train = x[:8, :]
x_val = x[8:12, :]
x_test = x[12:16, :]
y_train = y[:8]
y_val = y[8:12]
y_test = y[12:16]

network layers

   the word    layers    get thrown around in the context of machine learning
   quite a bit. truth be told, it is nothing more than a mathematical
   operation that often times involves a multiplicative weight matrix and
   an additive bias vector.

   let   s take just one of the data points, which is a set of four feature
   values. we want to turn the input data point, which is a matrix of
   dimension 1 by 4, into a vector of label probabilities, which is a
   vector of dimension 1 by 2. to do this, we simply have to multiply the
   input by a 4 by 2 weight matrix. that   s it! well, not quite         we also
   add a 1 by 2 bias vector to the product too, just to add an additional
   parameter to learn.

   here   s the code:
w = np.random.randn(4, 2)
b = np.zeros(2)
linear_cache = {}
def linear(input):
    output = np.matmul(input, w) + b
    linear_cache["input"] = input
    return output

   (don   t worry about the cache for now; that will be explained in the
   next section)

   a really cool result of the way this operation works is that we don   t
   need to give the layer just one data point as an input! if we take,
   say, four data points instead, the input is 4 by 4 and the output is 4
   by 2 (four sets of class probabilities). a subset of your dataset used
   at train-time is called a batch.

   the class with the highest id203 is the network   s guess. great!
   except that the network would be horribly wrong         all the weights are
   random, after all! said concisely: now that we have the network
   guessing at all, we need to get it guessing correctly.

the backward pass

   to make the network   s guesses better, we need to find    good    values for
   the weight and bias so that the amount of correct guesses is maximized.
   we do this by finding one scalar value that represents how    wrong    our
   guess was, known as the loss value, and minimizing it using
   multivariate calculus.

the id168 (or,    taking an l   )

   before we compute a scalar loss, we want to turn our arbitrarily valued
      probabilities    into a proper id203 distribution. we do this by
   computing the softmax function over the values:
   [1*bdrnieqs4lo1ddo3qtvmxg.png]

   for every value f_j in the linear output. to get the scalar loss value,
   compute the cross-id178 of the correct class:
   [1*fcuwynxivgwjxqh8mktu3w.png]

   for the correct class value f_i. here   s the code:
softmax_cache = {}
def softmax_cross_id178(input, y):
    batch_size = input.shape[1]
    indeces = np.arange(batch_size)
    exp = np.exp(input)
    norm = (exp.t / np.sum(exp, axis=1)).t
    softmax_cache["norm"], softmax_cache["y"], softmax_cache["indeces"] = norm,
y, indeces
    losses = -np.log(norm[indeces, y])
    return np.sum(losses)/batch_size

id26

   we are now in the business of minimizing the loss value, and we need to
   change the weights to do this. we use the beauty of chain rule to
   accomplish this.

   let   s take a super simple example to demonstrate this concept. let   s
   define a function: l(x) = 3x   + 2. here is function as a graph:
   [1*afsjsmr5nl4xer_exrziqw.png]

   we want to take the derivative of l with respect to the x. we can do
   this by treating each graph node at the subfunction, taking the partial
   derivative at each of these subfunctions, and multiplying by the
   incoming derivative:
   [1*hpseqojoo3b9marwyofaza.png]

   let   s break this down. the derivative of l with respect to l is just 1.
   the derivative of n + 2 is 1; multiplied by 1 (the incoming gradient)
   is 1. the derivative 3n is 3; multiplied by 1 is 3. and finally, the
   derivative of n   is 2n; multiplied by 3 is 6n. indeed, we know that the
   derivative of 3x   +2 is 6x. finding the derivative with respect to
   parameters by recursive use of the chain rule is called
   id26. we can do the same thing with our complex neural
   network by first drawing out the network as a graph:
   [1*ppi68wluvenbx5y_tqxi5q.png]

   now we can backpropagate through the network to find the derivative
   (known as a gradient when the variable is non-scalar) with respect to
   the weight and bias:
   [1*-huffnkzogtzbvst1mpacq.png]

   the id26 through this graph has the same underlying rules as
   in the graph above. however, there   s still quite a bit going on here,
   so let   s again break it down. the second gradient is the gradient for
   the entire softmax/cross-id178 function; it basically state that the
   derivative is the same as the output of the softmax from the forward
   pass, except that we subtract 1 from the correct class. the derivation
   for the gradient is not in the scope of this article, but you can read
   more about it [18]here. also, b is not the same dimension as q, so we
   need to sum across a dimension to make sure the dimensions match.
   finally, x^t is the transpose of the input matrix x. hopefully is is
   now clear why we needed to cache certain variables: some values
   used/computed in the forward pass are needed to compute gradients in
   the backward pass. here is the code for the backward passes:
def softmax_cross_id178_backward():
    norm, y, indeces = softmax_cache["norm"], softmax_cache["y"], softmax_cache[
"indeces"]
    dloss = norm
    dloss[indeces, y] -= 1
    return dloss
def linear_backward(dout):
    input = linear_cache["input"]
    dw = np.matmul(input.t, dout)
    db = np.sum(dout, axis=0)
    return dw, db

update rules

   id26 with respect to the parameters gives is the steepest
   direction of change. so, if we move in the opposite direction, then we
   will reduce the value of the function. the simplest algorithm to move
   in the direction of steepest decrease is called gradient
   descent         multiplying the gradient by some value alpha and subtracting
   it from the parameter:
   [1*qqcazyimvuttkzlut3eipa.png]

   the multiplicative value alpha is very important, because if it   s too
   large then we may shoot past the minimum, but if it   s too small we may
   never converge. the size of step that we take in a weight update is
   known as the learning rate. the learning rate is a hyperparameter, a
   value that we can vary to yield different results in our trained
   network.

   note the section commented    parameter updates    in the code for our
   training regime:
def eval_accuracy(output, target):
    pred = np.argmax(output, axis=1)
    target = np.reshape(target, (target.shape[0]))
    correct = np.sum(pred == target)
    accuracy = correct / pred.shape[0] * 100
    return accuracy
# training regime
for i in range(4000):
    indeces = np.random.choice(x_train.shape[0], 4)
    batch = x_train[indeces, :]
    target = y_train[indeces]
    # forward pass
    linear_output = linear(batch)
    loss = softmax_cross_id178(linear_output, target)
    # backward pass
    dloss = softmax_cross_id178_backward()
    dw, db = linear_backward(dloss)
    # weight updates
    w -= 1e-2 * dw
    b -= 1e-2 * db
    # evaluation
    if (i+1) % 100 == 0:
        accuracy = eval_accuracy(linear_output, target)
        print ("training accuracy: %f" % accuracy)
    if (i+1) % 500 == 0:
        accuracy = eval_accuracy(linear(x_val), y_val)
        print("validation accuracy: %f" % accuracy)
# test evaluation
accuracy = eval_accuracy(linear(x_test), y_test)
print("test accuracy: %f" % accuracy)

   here is the complete gist with the entire code:

   iframe: [19]/media/4bbb7b23109a2738af7a2a27a5034fe2?postid=6986b5e3a127

   it can also be found here:
   [20]https://gist.github.com/shubhangdesai/72023174e0d54f8fb60ed87a3a58e
   c7c

   that   s it! we have a very simple, one-hidden-layer neural network that
   can be trained to yield a 100% validation and test accuracy on our toy
   dataset.

next steps

   the type of layer that we have used is called a linear or
   fully-connected layer. i   ll be writing more articles this summer about
   other types of layers and network architectures, as well as articles on
   way cooler applications than toy datasets. be on the lookout for those!

   there are some awesome online courses on machine learning available for
   free. the [21]coursera ml course is a classic, of course, but i   d also
   recommend the [22]course materials for stanford cs 231n. it is a
   master   s level course which i had the privilege to take last quarter;
   it is incredibly taught and intensive.

   i would also recommend looking into some of the beginner tutorials for
   [23]tensorflow and [24]pytorch. they   re the most popular open-source
   deep learning libraries, and rightfully so. the tutorials are in-depth
   and easy to follow.

   now that you have the basics of neural networks down, you have been
   officially inducted into an exciting and fast-changing field. go,
   explore the field! i really hope that machine learning inspires as much
   awe and amazement in you as it has in me.

   if you liked this article, please be sure to give me a clap and follow
   me to see my future articles in your feed! also, check out my
   [25]personal blog and follow my [26]twitter for more of my musings.

     * [27]machine learning
     * [28]python
     * [29]artificial intelligence
     * [30]deep learning
     * [31]towards data science

   (button)
   (button)
   (button) 427 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of shubhang desai

[33]shubhang desai

   passionate about machine learning, comics & edm     stanford 2020

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 427
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/6986b5e3a127
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-soft-introduction-to-neural-networks-6986b5e3a127&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-soft-introduction-to-neural-networks-6986b5e3a127&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rjeocy5eyidx---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@shubhang.desai?source=post_header_lockup
  17. https://towardsdatascience.com/@shubhang.desai
  18. https://stackoverflow.com/questions/37790990/derivative-of-a-softmax-function-explanation
  19. https://towardsdatascience.com/media/4bbb7b23109a2738af7a2a27a5034fe2?postid=6986b5e3a127
  20. https://gist.github.com/shubhangdesai/72023174e0d54f8fb60ed87a3a58ec7c
  21. https://www.coursera.org/learn/machine-learning
  22. http://cs231n.stanford.edu/
  23. https://www.tensorflow.org/tutorials/
  24. http://pytorch.org/tutorials/
  25. https://shubhangdesai.github.io/blog/
  26. https://twitter.com/shubhangdesai
  27. https://towardsdatascience.com/tagged/machine-learning?source=post
  28. https://towardsdatascience.com/tagged/python?source=post
  29. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  30. https://towardsdatascience.com/tagged/deep-learning?source=post
  31. https://towardsdatascience.com/tagged/towards-data-science?source=post
  32. https://towardsdatascience.com/@shubhang.desai?source=footer_card
  33. https://towardsdatascience.com/@shubhang.desai
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/6986b5e3a127/share/twitter
  40. https://medium.com/p/6986b5e3a127/share/facebook
  41. https://medium.com/p/6986b5e3a127/share/twitter
  42. https://medium.com/p/6986b5e3a127/share/facebook
