   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

deep learning with python

   [16]go to the profile of vihar kurama
   [17]vihar kurama (button) blockedunblock (button) followfollowing
   feb 10, 2018

   the human brain imitation.
   [1*5c661k4fuel010pjfqq4ca.png]

   the main reason behind deep learning is the idea that, artificial
   intelligence should draw inspiration from the brain. this perspective
   gave rise to the    neural network    terminology. the brain contains
   billions of neurons with tens of thousands of connections between them.
   deep learning algorithms resemble the brain in many conditions, as both
   the brain and deep learning models involve a vast number of computation
   units (neurons) that are not extraordinarily intelligent in isolation
   but become intelligent when they interact with each other.

     i think people need to understand that deep learning is making a lot
     of things, behind-the-scenes, much better. deep learning is already
     working in google search, and in image search; it allows you to
     image search a term like    hug.       geoffrey hinton

neurons

   the basic building block for neural networks are artificial neurons,
   which imitate human brain neurons. these are simple, powerful
   computational units that have weighted input signals and produce an
   output signal using an activation function. these neurons are spread
   across the several layers in the neural network.
   [1*tobr7hysy-nj85zpxwcuoq.png]

how does id158 works?

   deep learning consists of id158s that are modelled
   on similar networks present in the human brain. as data travels through
   this artificial mesh, each layer processes an aspect of the data,
   filters outliers, spots familiar entities, and produces the final
   output.
   [1*dw0ccmj1hz0ovsxi7kz5mq.jpeg]

   input layer : this layer consists of the neurons that do nothing than
   receiving the inputs and pass it on to the other layers. the number of
   layers in the input layer should be equal to the attributes or features
   in the dataset.

   output layer:the output layer is the predicted feature, it basically
   depends in the type of model you   re building.

   hidden layer: in between input and output layer there will be hidden
   layers based on the type of model. hidden layers contain vast number of
   neurons. the neurons in the hidden layer apply transformations to the
   inputs and before passing them. as the network is trained the weights
   get updated, to be more predictive.

neuron weights

   weights refer to the strength or amplitude of a connection between two
   neurons, if you are familiar with id75 you can compare
   weights on inputs like coefficients we use in a regression
   equation.weights are often initialized to small random values, such as
   values in the range 0 to 1.

feedforward deep networks

   feedforward supervised neural networks were among the first and most
   successful learning algorithms. they are also called deep networks,
   multi-layer id88 (mlp), or simply neural networks and the vanilla
   architecture with a single hidden layer is illustrated. each neuron is
   associated with other neuron with some weight,

   the network processes the input upward activating neurons as it goes to
   finally produce an output value.this is called a forward pass on the
   network.
   [1*abaxzqnallmdkmdnoabvpq.png]

activation function

   an activation function is a mapping of summed weighted input to the
   output of the neuron. it is called an activation/ transfer function
   because it governs the inception at which the neuron is activated and
   the strength of the output signal.

   mathematically,
   [1*arbdotdqbjkprtwtxtyrna.png]

   we have many id180, out of which most used are relu,
   tanh, solfplus.

   the cheat sheet for id180 in given below.
   [1*tmfljgow0tq8yjnzu7ywda.png]
   [18]credits

back propagation

   the predicted value of the network is compared to the expected output,
   and an error is calculated using a function. this error is then
   propagated back within the whole network, one layer at a time, and the
   weights are updated according to the value that they contributed to the
   error. this clever bit of math is called the back-propagation
   algorithm. the process is repeated for all of the examples in your
   training data. one round of updating the network for the entire
   training dataset is called an epoch. a network may be trained for tens,
   hundreds or many thousands of epochs.
   [1*xaot8rhwvzg_e0whxf8gya.png]

cost function and id119

   the cost function is the measure of    how good    a neural network did for
   it   s given training input and the expected output. it also may depend
   on attributes such as weights and biases.

   a cost function is single-valued, not a vector because it rates how
   well the neural network performed as a whole. using the gradient
   descent optimization algorithm, the weights are updated incrementally
   after each epoch.

compatible cost function:

   mathematically,

   sum of squared errors (sse)
   [1*qfuyhl50an-o81hs-ewyuq.png]

   the magnitude and direction of the weight update is computed by taking
   a step in the opposite direction of the cost gradient.
   [1*30ha74go8c3lm0mjkrwkhw.png]
   where    is the learning rate.

   where   w is a vector that contains the weight updates of each weight
   coefficient w, which are computed as follows:
   [1*p8okpkkiaszq0obekm-tsq.png]

   graphically, considering cost function with single coefficient

   we calculate the id119 until the derivative reaches the
   minimum error, and each step is determined by the steepness of the
   slope (gradient).
   [1*qwnoh6rkipcvidrvbplroa.png]

multi layer id88s (forward propagation)

   this class of networks consists of multiple layers of neurons, usually
   interconnected in a feed-forward way (moving in a forward direction).
   each neuron in one layer has direct connections to the neurons of the
   subsequent layer. in many applications, the units of these networks
   apply a sigmoid or relu (rectified linear activation) function as an
   activation function.

   now consider a problem to find the number of transactions, given
   accounts and family members as input.

   to solve this first, we need to start with creating a forward
   propagation neural network. our input layer will be the number of
   family members and accounts, the number hidden layers are one, and the
   output layer will be the number of transactions.

   given weights as shown in the figure from the input layer to hidden
   layer with the number of family members 2 and number of accounts 3 as
   inputs.

   now the values of hidden layer (i, j) and output layer (k) will be
   calculated using using forward propagation by the following steps.

process

    1. multiply         add process.
    2. dot product (inputs * weights).
    3. forward propagation for one data point at a time.
    4. output is the prediction for that data point.

   [1*fymzqrhch5xdpvisyrllkw.png]

   value of i will be calculated from input value and the weights
   corresponding to the neuron connected.

   i = (2 * 1) + (3 * 1)

       i = 5

   similarly,

   j = (2 * -1) + (3 * 1)

       j = 1

   k = (5 * 2) + (1 * -1)

       k = 9
   [1*2u709l8kis2jkyqq8kjcnq.png]

solving the multi layer id88 problem in python

   iframe: [19]/media/f0871878d3298dc7af3a32c930f98da2?postid=703e26853820

$python dl_multilayer_id88.py
enter the two values for input layers
a =
3
b =
4
node 0_hidden: 7
node_1_hidden: 1
output layer : 13

using activation function

   for neural network to achieve their maximum predictive power we need to
   apply an activation function for the hidden layers.it is used to
   capture the non-linearities. we apply them to the input layers, hidden
   layers with some equation on the values.

   here we use rectified linear activation (relu)
   [1*14yudookmthozjfr3kw8ag.png]

   iframe: [20]/media/f211c3e7bbc5c2199aaf37ed663b2fda?postid=703e26853820

$python dl_fp_activation.py
enter the two values for input layers
a =
3
b =
4
44

developing first neural network with keras

about keras:

   keras is a high-level neural networks api, written in python and
   capable of running on top of [21]tensorflow, [22]cntk, or [23]theano.

   to install keras on your machine using pip, run the following command.
sudo pip install keras

steps to implement your deep learning program in keras

    1. load data.
    2. define model.
    3. compile model.
    4. fit model.
    5. evaluate model.
    6. tie it all together.

developing your keras model

   fully connected layers are described using the dense class. we can
   specify the number of neurons in the layer as the first argument, the
   initialisation method as the second argument as init and determine the
   activation function using the activation argument. now that the model
   is defined, we can compile it. compiling the model uses the efficient
   numerical libraries under the covers (the so-called backend) such as
   theano or tensorflow. so far we have defined our model and compiled it
   set for efficient computation. now it is time to run the model on the
   pima data. we can train or fit our model on our data by calling the
   fit() function on the model.

   let   s get started with our program in keras,

   iframe: [24]/media/2739d010627e73b234fcc400d39d0591?postid=703e26853820

$python keras_pima.py
768/768 [==============================] - 0s - loss: 0.6776 - acc: 0.6510
epoch 2/150
768/768 [==============================] - 0s - loss: 0.6535 - acc: 0.6510
epoch 3/150
768/768 [==============================] - 0s - loss: 0.6378 - acc: 0.6510
.
.
.
.
.
epoch 149/150
768/768 [==============================] - 0s - loss: 0.4666 - acc: 0.7786
epoch 150/150
768/768 [==============================] - 0s - loss: 0.4634 - acc: 0.773432/768

[>.............................] - eta: 0sacc: 77.73%

   the neural network trains until 150 epochs and returns the accuracy
   value.
     __________________________________________________________________

closing notes

   thanks for reading. if you found this story helpful, please click the
   below      to spread the love.

important links

   introduction to machine learning
   [25]introduction to machine learning
   machine learning is an idea to learn from examples and experience,
   without being explicitly programmed. instead
   of   towardsdatascience.com

   supervised learning in python
   [26]supervised learning with python
   why artificial intelligence and machine
   learning ?towardsdatascience.com

   python programming in 15 min.
   [27]python programming in 15 min part 1
   about pythontowardsdatascience.com

   code for this article can be found [28]here.

     * [29]machine learning
     * [30]programming
     * [31]data science
     * [32]neural networks
     * [33]towards data science

   (button)
   (button)
   (button) 2.5k claps
   (button) (button) (button) 9 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of vihar kurama

[35]vihar kurama

   philomat

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.5k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/703e26853820
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-with-python-703e26853820&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/deep-learning-with-python-703e26853820&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_x0tg3slhrxhc---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@vihar.kurama?source=post_header_lockup
  17. https://towardsdatascience.com/@vihar.kurama
  18. http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
  19. https://towardsdatascience.com/media/f0871878d3298dc7af3a32c930f98da2?postid=703e26853820
  20. https://towardsdatascience.com/media/f211c3e7bbc5c2199aaf37ed663b2fda?postid=703e26853820
  21. https://github.com/tensorflow/tensorflow
  22. https://github.com/microsoft/cntk
  23. https://github.com/theano/theano
  24. https://towardsdatascience.com/media/2739d010627e73b234fcc400d39d0591?postid=703e26853820
  25. https://towardsdatascience.com/introduction-to-machine-learning-db7c668822c4
  26. https://towardsdatascience.com/supervised-learning-with-python-cf2c1ae543c1
  27. https://towardsdatascience.com/python-programming-in-15-min-part-1-3ad2d773834c
  28. https://github.com/vihar/deep-learning-with-python
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/programming?source=post
  31. https://towardsdatascience.com/tagged/data-science?source=post
  32. https://towardsdatascience.com/tagged/neural-networks?source=post
  33. https://towardsdatascience.com/tagged/towards-data-science?source=post
  34. https://towardsdatascience.com/@vihar.kurama?source=footer_card
  35. https://towardsdatascience.com/@vihar.kurama
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://towardsdatascience.com/introduction-to-machine-learning-db7c668822c4
  42. https://towardsdatascience.com/supervised-learning-with-python-cf2c1ae543c1
  43. https://towardsdatascience.com/python-programming-in-15-min-part-1-3ad2d773834c
  44. https://medium.com/p/703e26853820/share/twitter
  45. https://medium.com/p/703e26853820/share/facebook
  46. https://medium.com/p/703e26853820/share/twitter
  47. https://medium.com/p/703e26853820/share/facebook
