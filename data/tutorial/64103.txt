   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

adversarial examples in deep learning

   [16]go to the profile of gr  gory ch  tel
   [17]gr  gory ch  tel (button) blockedunblock (button) followfollowing
   jun 13, 2017

   this post will contain essentially the same information as [18]the talk
   i gave during the last [19]deep learning paris meetup. i feel that as
   more and more fields start to use deep learning in critical systems, it
   is important to bring awareness on how neural networks can be fooled to
   produce strange and potentially dangerous behaviors. the goal of this
   post is not to be as mathematically precise and exhaustive as possible
   but to give you a basic understanding of adversarial examples and
   hopefully to sensibilize you to one of the problems of modern day deep
   learning.

   before starting this post, i would also like to mention that i am not
   part of the researchers who did this work. i am just a really curious
   guy from the fast.ai mooc who   s passionate about deep learning.
     __________________________________________________________________

   so first, what are adversarial examples?

   an adversarial example is a sample of input data which has been
   modified very slightly in a way that is intended to cause a machine
   learning to misclassify it.

   an example should help anchor the idea.
   [1*uk7fxkstkw095uwh8sfl0q.png]
   an image classified as    suit    by the vgg16 neural network (left), a
   perturbation created specifically using the image on the left (middle)
   and the resulting perturbed image classified as a wig (right).

   in this figure, we can see that the classification of the perturbed
   image (rightmost) is clearly absurd. we can also notice that the
   difference between the original and modified images is very slight.

   with article like [20]   california   s finally ready for truly driverless
   cars    or [21]   the pentagon   s    terminator conundrum   : robots that could
   kill on their own   , applications of adversarial examples are not
   exactly hard to come up with   

   in this post i will first explain how such images are created and then
   go through the main defenses that have been published. for more details
   and more rigorous information, please refer to the research papers
   referenced.
     __________________________________________________________________

   first, a quick reminder on id119. id119 is an
   optimization algorithm used to find a local minimum of a differentiable
   function.
   [1*yziuvzmfy-7aapeghryh5a.png]
   a curve and a tangent to this curve.

   in the figure on the left, you can see a simple curve. suppose that we
   want to find a local minimum (a value of x for which f(x) is locally
   minimal).

   the id119 consists in the following steps: first pick an
   initial value for x, then compute the derivative f    of f according to x
   and evaluate it for our initial guess. f   (x) is the slope of the
   tangent to the curve at x. according to the sign of this slope, we know
   whether we have to increase or decrease x to make f(x) decrease. in the
   example on the left, the slope is negative so we should increase the
   value of x to make f(x) decrease. as the tangent is a good
   approximation of the curve in a tiny neighborhood, the value change
   applied to x is very small to ensure that we do not jump too far.

   now, we can use this algorithm to train machine learning models.
   [1*r5vobcx_n1sdtbo4hni9qq.png]
   a set of data points and a linear model initialized randomly.

   suppose that we have a set of points and we want to find a line that is
   a reasonable approximation for these values.

   our machine learning model will be the line y = ax + b and the model
   parameters will be a and b.

   now to use the id119, we are going to define a function of
   which we will want to find a local minimum. this is our id168.
   [1*uyjcjvv2onvcbecdg2sjhg.png]

   this loss takes a data point x, its corresponding value y and the model
   parameters a and b. the loss is the squared difference of the real
   value y and ax + b, the prediction of our model. the bigger the
   difference between the real and predicted values is, the bigger the
   value of the id168 will be. intuitively, we chose the squaring
   operation to penalize big differences between real and predicted values
   more than small ones.

   now we compute the derivative of our function l according to the
   parameters of the model a and b.
   [1*6gznzhlnygeabl2fgvrejw.png]

   and as before, we can evaluate this derivatives with our current values
   of a and b for each data point (x, y), which will give us the slopes of
   the tangents to the id168 and use these slopes to update a and
   b in order to minimize l.

   ok, that   s cool and all but that   s not how we   re going to generate our
   adversarial examples   

   well, in fact it is exactly how we are going to do it. suppose now that
   the model is fixed (you can   t change a and b) and you want to increase
   the value of the loss. the only thing left to modify are the data
   points (x, y). as modifying the ys does not really make sense, we will
   modify the xs.

   we could just replace the x by random values and the loss value would
   increase by a tremendous amount but that   s not really subtle, in
   particular, it would be really obvious to a human plotting the data
   points. to make our changes in a way that is not obviously detected by
   an observer, we will compute the derivative of the id168
   according to x.
   [1*erkqoyunv2v01rjlxbz-dq.png]

   and now, just as before, we can evaluate this derivative on our data
   points, get the slope of the tangent and update the x values by a small
   amount accordingly. the loss will increase and, as we are modifying all
   the points by a small amount, our perturbation will be hard to detect.
     __________________________________________________________________

   well that was a very simple model that we   ve just messed with, deep
   learning is much more complicated than that   

   guess what? it   s not. everything we just did has a direct equivalent in
   the world of deep learning. when we are training a neural network to
   classify images, the id168 is usually a categorical cross
   id178, the model parameters are the weights of the network and the
   inputs are the pixel values of the image.

   the basic algorithm of adversarial sample generation, called fast
   gradient sign method (from [22]this paper), is exactly what i described
   above. let   s explain it and run it on an example.

   let x be the original image, y the class of x,    the weights of the
   network and l(  , x, y) the id168 used to train the network.
   [1*mkxllqkfuhdk--wto6zviq.png]

   first, we compute the gradient of the id168 according to the
   input pixels. the     operator is just a concise mathematical way of
   taking the derivatives of a function according to many of its
   parameters. you can think of as a matrix of shape [width, height,
   channels] containing the slopes of the tangents.
   [1*uvz269e0po81wkezspxjdg.png]

   as before, we are only interested in the sign of the slopes to know if
   we want to increase or decrease the pixel values. we multiply these
   signs by a very small value    to ensure that we do not go too far on
   the id168 surface and that the perturbation will be
   imperceptible. this will be our perturbation.
   [1*zjy45116ogs82ynzjnqjkg.png]

   our final image is just our original image to which we add the
   perturbation   .

   let   s run it on an example:
   [1*z-iezcsrycgjselnq6li5g.png]
   fgsm applied to an image. the original is classified as    king penguin   
   with 100% confidence and the perturbed one is classified as    tripod   
   with 71% confidence.

   the family of attack where you are able to use compute gradients using
   the target model are called white-box attacks.
     __________________________________________________________________

   now you could tell me that the attack i   ve just presented is not really
   realistic as you   re unlikely to get access to the gradients of the loss
   function on a self-driving car. researchers thought exactly the same
   thing and, in [23]this paper, they found a way to deal with it.

   in a more realistic context, you would want to attack a system having
   only access to its outputs. the problem with this is that you would not
   be able to apply the fgsm algorithm anymore as you would not have
   access to the network itself.

   the solution proposed is to train a new neural network m    to solve the
   same classification task as the target model m. then, when m    is
   trained, use it to generate adversarial samples using fgsm (which we
   now can do since it is our own network) and ask m to classify them.

   what they found is that m will very often misclassify adversarial
   samples generated using m   . moreover, if we do not have access to a
   proper training set for m   , we can build one using m predictions as
   truth values. the authors call this synthetic inputs. this is an
   excerpt of their article in which they describe their attack on the
   metamind network to which they did not have access:

      after labeling 6,400 synthetic inputs to train our substitute (an
   order of magnitude smaller than the training set used by metamind) we
   find that their dnn misclassifies adversarial examples crafted with our
   substitute at a rate of 84.24%   .

   this kind of attack is called black-box attack as you see the target
   model as a black-box.
     __________________________________________________________________

   so, even when the attacker does not have access to the internals of the
   model he can still produce adversarial sample that will fool it but
   still, this attack context is not realistic either. in a real scenario,
   the attacker would not be allowed to provide its own image files, the
   neural network would take camera pictures as input. that   s the problem
   the authors of [24]this article are trying to solve.

   what they noticed is that when you print adversarial samples which have
   been generated with a high-enough    and then take a picture of the
   print and classify it, the neural network is still fooled a significant
   portion of the time. the authors recorded a video to showcase their
   results:

   iframe: [25]/media/a02178ad8f91bb83debd3a7937892eef?postid=be0b08a94953

   adversarial example in the physical world.

      we used images taken from a cell-phone camera as a input to an
   inception v3 image classification neural network. we showed that in
   such a set-up, a significant fraction of adversarial images crafted
   using the original network are misclassified even when fed to the
   classifier through the camera.   

   now that the potential for an attack in a realistic context seems
   reasonable, let   s review a few defense strategies.
     __________________________________________________________________

   the first idea that comes to mind when trying to defend adversarial
   examples is usually to generate a lot of them and run more training
   passes on the network with these images and the correct class as
   targets. this strategy is called adversarial training.

   while this strategy improves robustness to fgsm attacks, it has
   multiple downside:
     * it does not help against more sophisticated white-box attacks like
       [26]rand+fgsm, which, as explained in the article, we cannot use to
       adversarially train a network.
     * it does not help against black-box attacks either.

   this last point is quite surprising but true and have been observed in
   multiple contexts. the reasons of this behavior are explored in
   [27]this paper. proper white-box defense is something that we do not
   know how to do perfectly as i   m writing this post. as the authors note,
   the fact that adversarially trained networks are still vulnerable to
   black-box attacks is often not taken into account in articles that
   presents new defense strategies.
     __________________________________________________________________

   while white-box defense seems like a difficult problem, the hypothesis
   that the attacker has access to the model weights is big. researchers
   tried to produce networks that are robust to black-box attacks in
   [28]this paper.

   since adversarially trained networks are still vulnerable to black-box
   attacks, the authors propose ensemble adversarial training, a strategy
   which consists in adversarially training a model using modified samples
   from an ensemble of other models (usually 2 to 5). for example, they
   train a model to classify mnist digits using adversarial examples
   crafted using 5 other pre-trained models. the error rate goes from
   15.5% for an adversarially trained model to 3.9% for an ensemble
   adversarially trained model for a black-box attack.

   this algorithm is the best so far at black-box defense.
     __________________________________________________________________

   as a conclusion on the defense part, i will cite a [29]blog post from
   i. goodfellow and n. papernot:

      most defenses against adversarial examples that have been proposed so
   far just do not work very well at all, but the ones that do work are
   not adaptive. this means it is like they are playing a game of
   whack-a-mole: they close some vulnerabilities, but leave others open.   
     __________________________________________________________________

   i hope you have enjoyed reading this blog post. if you notice any
   spelling or mathematical mistake, don   t hesitate to leave a comment.

     * [30]machine learning
     * [31]deep learning
     * [32]adversarial
     * [33]artificial intelligence
     * [34]towards data science

   (button)
   (button)
   (button) 269 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of gr  gory ch  tel

[36]gr  gory ch  tel

   phd in computer science, lead r&d @ disaitek, intel ai software
   innovator.

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 269
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/be0b08a94953
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/adversarial-examples-in-deep-learning-be0b08a94953&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/adversarial-examples-in-deep-learning-be0b08a94953&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_agcyl162r8l9---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@chatel.gregory?source=post_header_lockup
  17. https://towardsdatascience.com/@chatel.gregory
  18. https://github.com/rodgzilla/deep-learning-presentation/blob/master/adversarial_samples/gchatel_adversarial_samples.pdf
  19. https://www.meetup.com/deep-learning-paris-meetup/
  20. https://www.wired.com/2017/03/californias-finally-ready-truly-driverless-cars/
  21. https://www.nytimes.com/2016/10/26/us/pentagon-artificial-intelligence-terminator.html
  22. https://arxiv.org/abs/1412.6572
  23. https://arxiv.org/abs/1602.02697
  24. https://arxiv.org/abs/1607.02533
  25. https://towardsdatascience.com/media/a02178ad8f91bb83debd3a7937892eef?postid=be0b08a94953
  26. https://arxiv.org/abs/1705.07204
  27. https://arxiv.org/abs/1705.07204
  28. https://arxiv.org/abs/1705.07204
  29. http://www.cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/deep-learning?source=post
  32. https://towardsdatascience.com/tagged/adversarial?source=post
  33. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  34. https://towardsdatascience.com/tagged/towards-data-science?source=post
  35. https://towardsdatascience.com/@chatel.gregory?source=footer_card
  36. https://towardsdatascience.com/@chatel.gregory
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/be0b08a94953/share/twitter
  43. https://medium.com/p/be0b08a94953/share/facebook
  44. https://medium.com/p/be0b08a94953/share/twitter
  45. https://medium.com/p/be0b08a94953/share/facebook
