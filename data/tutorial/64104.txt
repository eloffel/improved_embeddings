   #[1]wildml    feed [2]wildml    comments feed [3]wildml    introduction to
   learning to trade with id23 comments feed [4]ai and
   deep learning in 2017     a year in review [5]alternate [6]alternate

   [7]skip to content

   [8]wildml

   artificial intelligence, deep learning, and nlp

   (button) menu
     * [9]home
     * [10]ai newsletter
     * [11]deep learning glossary
     * [12]contact
     * [13]about

   posted on [14]february 10, 2018february 10, 2018 by [15]denny britz

introduction to learning to trade with id23

   thanks a lot to [16]@aerinykim, [17]@suzatweet and [18]@hardmaru for
   the useful feedback!

   the academic deep learning research community has largely stayed away
   from the financial markets. maybe that   s because the finance industry
   has a bad reputation, the problem doesn   t seem interesting from a
   research perspective, or because data is difficult and expensive to
   obtain.

   in this post, i   m going to argue that training id23
   agents to trade in the financial (and cryptocurrency) markets can be an
   extremely interesting research problem. i believe that it has not
   received enough attention from the research community but has the
   potential to push the state-of-the art of many related fields. it is
   quite similar to training agents for multiplayer games such as dota,
   and many of the same research problems carry over. knowing virtually
   nothing about trading, i have spent the past few months working on a
   project in this field.

   this is not a    price prediction using deep learning    post. so, if
   you   re looking for example code and models you may be disappointed.
   instead, i want to talk on a more high level about why learning to
   trade using machine learning is difficult, what some of the challenges
   are, and where i think id23 fits in. if there   s
   enough interest in this area i may follow up with another post that
   includes concrete examples.

   i expect most readers to have no background in trading, just like i
   didn   t, so i will start out with covering some of the basics. i   m by no
   means an expert, so please let me know in the comments so if you find
   mistakes. i will use cryptocurrencies as a running example in this
   post, but the same concepts apply to most of the financial markets. the
   reason to use cryptocurrencies is that data is free, public, and easily
   accessible. anyone can sign up to trade. the barriers to trading in the
   financial markets are a little higher, and data can be expensive. and
   well, there   s more hype so it   s more fun :)

basics of market microstructure

   trading in the cryptocurrency (and most financial) markets happens in
   what   s called a continuous double auction with an open order book on an
   exchange. that   s just a fancy way of saying that there are buyers and
   sellers that get matched so that they can trade with each other. the
   exchange is responsible for the matching. there are [19]dozens of
   exchanges and each may carry slightly different products (such as
   bitcoin or ethereum versus u.s. dollar). interface-wise, and in terms
   of the data they provide, they all look pretty much the same.

   let   s take a look at gdax, one of the more popular u.s.-based
   exchanges. let   s assume you want to trade btc-usd (bitcoin for u.s.
   dollar). you would go to [20]this page and see something like this:

   there   s a lot of information here, so let   s go over the basics:

   price chart (middle)

   the current price is the price of the most recent trade. it varies
   depending on whether that trade was a buy or a sell (more on that
   below). the price chart is typically displayed as a [21]candlestick
   chart that shows the open/start (o), high (h), low (l) and close/end
   (c) prices for a given time window. in the picture above, that period
   is 5 minutes, but you can change it using the dropdown. the bars below
   the price chart show the volume (v), which is the total volume of all
   trades that happened in that period. the volume is important because it
   gives you a sense of the liquidity of the market. if you want to buy
   $100,000 worth if bitcoin, but there is nobody willing to sell, the
   market is illiquid. you simply can   t buy. a high trade volume indicates
   that many people are willing to transact, which means that you are
   likely to able to buy or sell when you want to do so. generally
   speaking, the more money you want to invest, the more trade volume you
   want. volume also indicates the    quality    of a price trend. high volume
   means you can rely on the price movement more than if there was low
   volume. high volume is often (but not always, as in the case of market
   manipulation) the consensus of a large number of market participants.

   trade history (right)

   the right side shows a history of all recent trades. each trade has a
   size, price, timestamp, and direction (buy or sell). a trade is a match
   between two parties, a taker and a maker. more on that below.

   order book (left)

   the left side shows the order book, which contains information about
   who is willing to buy and sell at what price. the order book is made up
   of two sides: asks (also called offers), and bids. asks are people
   willing to sell, and bids are people willing to buy. by definition, the
   best ask, the lowest price that someone is willing to sell at, is
   larger than the best bid, the highest price that someone is willing to
   buy at. if this was not the case, a trade between these two parties
   would   ve already happened. the difference between the best ask and best
   bid is called the spread.

   each level of the order book has a price and a volume. for example, a
   volume of 2.0 at a price level of $10,000 means that you can buy 2 btc
   for $10,000. if you want to buy more, you would need to pay a higher
   price for the amount that exceeds 2 btc. the volume at each level is
   cumulative, which means that you don   t know how many people, or orders,
   that 2 btc consists of. there could one person selling 2 btc, or there
   could be 100 people selling 0.02 btc each (some exchanges provide this
   level of information, but most don   t). let   s look at an example:

   so what happens when you send an order to buy 3 btc? you would be
   buying (rounding up) 0.08 btc at $12,551.00, 0.01btc at $12,551.6 and
   2.91 btc at $12,552.00. on gdax, you would also be paying a 0.3% taker
   fee, for a total of about 1.003 * (0.08 * 12551 + 0.01 * 12551.6 + 2.91
   * 12552) = $37,768.88 and an average price per btc of 37768.88 / 3 =
   $12,589.62. it   s important to note that what you are actually paying is
   much higher than $12,551.00, which was the current price! the 0.3% fee
   on gdax is extremely high compared to fees in the financial markets,
   and also much higher than the fees of many other cryptocurrency
   exchanges, which are often between 0% and 0.1%.

   also note that your buy order has consumed all the volume that was
   available at the $12,551.00 and $12,551.60 levels. thus, the order book
   will    move up   , and the best ask will become $12,552.00. the current
   price will also become $12,552.00, because that is where the last trade
   happened. selling works analogously, just that you are now operating on
   the bid side of the order book, and potentially moving the order book
   (and price) down. in other words, by placing buy and sell orders, you
   are removing volume from the order book. if your orders are large
   enough, you may shift the order book by several levels. in fact, if you
   placed a very large order for a few million dollars, you would shift
   the order book and price significantly.

   how do orders get into the order book? that   s the difference between
   market and limit orders. in the above example, you   ve issued a market
   order, which basically means    buy/sell x amount of btc at the best
   price possible, right now   . if you are not careful about what   s in the
   order book you could end up paying significantly more than the current
   price shows. for example, imagine that most of the lower levels in the
   order book only had a volume at 0.001 btc available. most of your buy
   volume would then get matched at a much higher, more expensive, price
   level. if you submit a limit order, also called a passive order, you
   specify the price and quantity you   re willing to buy or sell at. the
   order will be placed into the book, and you can cancel it as long as it
   has not been matched.  for example, let   s assume the bitcoin price is
   at $10,000, but you want to sell at $10,010. you place a limit order.
   first, nothing happens. if the price keeps moving down your order will
   just sit there, do nothing, and will never be matched. you can cancel
   it anytime. however, if the price moves up, your order will at some
   point become the best price in the book, and the next person submitting
   a market order for a sufficient quantity will match it.

   limit_order

   market orders take liquidity from the market. by matching with orders
   from the order book, you are taking away the option to trade to from
   other people     there   s less volume left! that   s also why market orders,
   or market takers, often need to pay higher fees than market makers, who
   put orders into the book. limit orders providing liquidity because they
   are giving others the option to trade. at the same time, limit orders
   guarantee that you will not pay more than the price specified in the
   limit order. however, you don   t know when, or if, someone will match
   your order. you are also giving the market information about what you
   believe the price should be. this can also be used to manipulate the
   other participants in the market, who may act a certain way based on
   the orders you are executing or putting into the book. because they
   provide the option to trade and give away information, market makers
   typically pay lower fees than market takers. some exchanges also
   provide stop orders, which allow you to set a maximum price for your
   market orders.

   this was a very short introduction of how order books works and
   matching works. there are many more subtleties as well other, much more
   complex, order types. if the above was not clear, you can find a wealth
   of information about order book mechanics, and research in that area,
   through google.

data

   the main reasons i am using cryptocurrencies in this post is because
   data is public, free, and easy to obtain. most exchanges have streaming
   apis that allow you to receive market updates in real-time. we   ll use
   gdax ([22]api documentation) as an example again, but the data for
   other exchanges looks very similar. let   s go over the basic types of
   events you would use to build a machine learning model.

   trade

   a new trade has happened. each trade has a timestamp, a unique id
   assigned by the exchange, a price, size, and side, as discussed above.
   if you wanted to plot the price graph of an asset, you would simply
   plot the price of all trades. if you wanted to plot the candlestick
   chart, you would window the trade events for a certain period, such as
   five minutes, and then plot the windows.

   1
   2
   3
   4
   5
   6
   7
   {
       "time": "2014-11-07t22:19:28.578544z",
       "trade_id": 74,
       "price": "10.00000000",
       "size": "0.01000000",
       "side": "buy"
   }

   bookupdate

   one or more levels in the order book were updated. each level is made
   up of the side (buy=bid, sell=ask), the price/level, and the new
   quantity at that level. note that these are changes, or deltas, and you
   must construct the full order book yourself by merging them.

   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   {
       "type": "l2update",
       "product_id": "btc-usd",
       "changes": [
           ["buy", "10000.00", "3"],
           ["sell", "10000.03", "1"],
           ["sell", "10000.04", "2"],
           ["sell", "10000.07", "0"]
       ]
   }

   booksnapshot

   similar to a bookupdate, but a snapshot of the complete order book.
   because the full order book can be very large, it is faster and more
   efficient to use the bookupdate events instead. however, having an
   occasional snapshot can be useful.

   1
   2
   3
   4
   5
   6
   {
       "type": "snapshot",
       "product_id": "btc-eur",
       "bids": [["10000.00", "2"]],
       "asks": [["10000.02", "3"]]
   }

   that   s pretty much all you need in terms of market data. a stream of
   the above events contains all the information you saw in the gui
   interface. you can imagine how you could make prediction based on a
   stream of the above events.

a few trading strategy metrics

   when developing trading algorithms, what do you optimize for? the
   obvious answer is profit, but that   s not the whole story. you also need
   to compare your trading strategy to baselines, and compare its risk and
   volatility to other investments. here are a few of the most basic
   metrics that traders are using. i won   t go into detail here, so feel
   free to follow the links for more information.

   net pnl (net profit and loss)

   simply how much money an algorithm makes (positive) or loses (negative)
   over some period of time, minus the trading fees.

   alpha and beta

   [23]alpha defines how much better, in terms of profit, your strategy is
   when compared to an alternative, relatively risk-free, investment, like
   a government bond. even if your strategy is profitable, you could be
   better off investing in a risk-free alternative. [24]beta is closely
   related, and tells you how volatile your strategy is compared to the
   market. for example, a beta of 0.5 means that your investment moves $1
   when the market moves $2.

   sharpe ratio

   the [25]sharpe ratio measures the excess return per unit of risk you
   are taking. it   s basically your return on capital over the standard
   deviation, adjusted for risk. thus, the higher the better. it takes
   into account both the volatility of your strategy, as well as an
   alternative risk-free investment.

   maximum drawdown

   the [26]maximum drawdown is the maximum difference between a local
   maximum and the subsequent local minimum, another measure of risk. for
   example, a maximum drawdown of 50% means that you lose 50% of your
   capital at some point. you then need to make a 100% return to get back
   to your original amount of capital. clearly, a lower maximum drawdown
   is better.

   value at risk (var)

   [27]value at risk is a risk metric that quantifies how much capital you
   may lose over a given time frame with some id203, assuming normal
   market conditions. for example, a 1-day 5% var of 10% means that there
   is a 5% chance that you may lose more than 10% of an investment within
   a day.

supervised learning

   before looking at the problem from a id23
   perspective, let   s understand how we would go about creating a
   profitable trading strategy using a supervised learning approach. then
   we will see what   s problematic about this, and why we may want to use
   id23 techniques.

   the most obvious approach we can take is price prediction. if we can
   predict that the market will move up we can buy now, and sell once the
   market has moved. or, equivalently, if we predict the market goes down,
   we can go short (borrowing an asset we don   t own) and then buy once the
   market has moved. however, there are a few problems with this.

   first of all, what price do we actually predict? as we   ve seen above,
   there is not a    single    price we are buying at. the final price we pay
   depends on the volume available at different levels of the order book,
   and the fees we need to pay. a naive thing to do is to predict the mid
   price, which is the mid-point between the best bid and best ask. that   s
   what most researchers do. however, this is just a theoretical price,
   not something we can actually execute orders at, and could differ
   significantly from the real price we   re paying.

   the next question is time scale. do we predict the price of the next
   trade? the price at the next second? minute? hour? day? intuitively,
   the further in the future we want to predict, the more uncertainty
   there is, and the more difficult the prediction problem becomes.

   let   s look at an example. let   s assume the btc price is $10,000 and we
   can accurately predict that the    price    moves up from $10,000 to
   $10,050 in the next minute. so, does that mean you can make $50 of
   profit by buying and selling? let   s understand why it doesn   t.
     * we buy when the best ask is $10,000. most likely we will not be
       able to get all our 1.0 btc filled at that price because the order
       book does not have the required volume. we may be forced to buy 0.5
       btc at $10,000 and 0.5 btc at $10,010, for an average price of
       $10,005. on gdax, we also pay a 0.3% taker fee, which corresponds
       to roughly $30.
     * the price is now at $10,050, as predicted. we place the sell order.
       because the market moves very fast, by the time the order is
       delivered over the network the price has slipped already. let   s say
       it   s now at $10,045. similar to above, we most likely cannot sell
       all of your 1 btc at that price. perhaps we are forced to sell 0.5
       btc are $10,045 and 0.5 btc at $10,040, for an average price of
       $10,042.5. then we pay another 0.3% taker fee, which corresponds to
       roughly $30.

   so, how much money have we made? -10005 - 30 - 30 + 10,042.5 = -$22.5.
   instead of making $50, we have lost $22.5, even though we accurately
   predicted a large price movement over the next minute! in the above
   example there were three reasons for this: no liquidity in the best
   order book levels, network latencies, and fees, none of which the
   supervised model could take into account.

   what is the lesson here? in order to make money from a simple price
   prediction strategy, we must predict relatively large price movements
   over longer periods of time, or be very smart about our fees and order
   management. and that   s a very difficult prediction problem. we could
   have saved on the fees by using limit instead of market orders, but
   then we would have no guarantees about our orders being matched, and we
   would need to build a complex system for order management and
   cancellation.

   but there   s another problem with supervised learning: it does not imply
   a policy. in the above example we bought because we predicted that the
   price moves up, and it actually moved up. everything went according to
   plan. but what if the price had moved down? would you have sold? kept
   the position and waited? what if the price had moved up just a little
   bit and then moved down again? what if we had been uncertain about the
   prediction, for example 65% up and 35% down? would you still have
   bought? how do you choose the threshold to place an order?

   thus, you need more than just a price prediction model (unless your
   model is extremely accurate and robust). we also need a rule-based
   policy that takes as input your price predictions and decides what to
   actually do: place an order, do nothing, cancel an order, and so on.
   how do we come up with such a policy? how do we optimize the policy
   parameters and decision thresholds? the answer to this is not obvious,
   and many people use simple heuristics or human intuition.

a typical strategy development workflow

   luckily, there are solutions to many of the above problems. the bad
   news is, the solutions are not very effective. let   s look a typical
   workflow for trading strategy development. it looks something like
   this:

   supervised trading strategy development
    1. data analysis: you perform exploratory data analysis to find
       trading opportunities. you may look at various charts, calculate
       data statistics, and so on. the output of this step is an    idea   
       for a trading strategy that should be validated.
    2. supervised model training: if necessary, you may train one or more
       supervised learning models to predict quantities of interest that
       are necessary for the strategy to work. for example, price
       prediction, quantity prediction, etc.
    3. policy development: you then come up with a rule-based policy that
       determines what actions to take based on the current state of the
       market and the outputs of supervised models. note that this policy
       may also have parameters, such as decision thresholds, that need to
       be optimized. this optimization is done later.
    4. strategy backtesting: you use a simulator to test an initial
       version of the strategy against a set of historical data. the
       simulator can take things such as order book liquidity, network
       latencies, fees, etc into account.  if the strategy performs
       reasonably well in backtesting, we can move on and do parameter
       optimization.
    5. parameter optimization: you can now perform a search, for example a
       grid search, over possible values of strategy parameters like
       thresholds or coefficient, again using the simulator and a set of
       historical data. here, overfitting to historical data is a big
       risk, and you must be careful about using proper validation and
       test sets.
    6. simulation & paper trading: before the strategy goes live,
       simulation is done on new market data, in real-time. that   s called
       paper trading and helps prevent overfitting. only if the strategy
       is successful in paper trading, it is deployed in a live
       environment.
    7. live trading: the strategy is now running live on an exchange.

   that   s a complex process. it may vary slightly depending on the firm or
   researcher, but something along those lines typically happens when new
   trading strategies are developed. now, why do i think this process is
   not effective? there are a couple of reasons.
    1. iteration cycles are slow. step 1-3 are largely based on intuition,
       and you don   t know if your strategy works until the optimization in
       step 4-5 is done, possibly forcing you to start from scratch. in
       fact, every step comes with the risk of failing and forcing you to
       start from scratch.
    2. simulation comes too late. you do not explicitly take into account
       environmental factors such as latencies, fees, and liquidity until
       step 4. shouldn   t these things directly inform your strategy
       development or the parameters of your model?
    3.  policies are developed independently from supervised models even
       though they interact closely. supervised predictions are an input
       to the policy. wouldn   t it make sense to jointly optimize them?
    4. policies are simple. they are limited to what humans can come up
       with.
    5. parameter optimization is inefficient. for example, let   s assume
       you are optimizing for a combination of profit and risk, and you
       want to find parameters that give you a high [28]sharpe ratio.
       instead of using an efficient gradient-based approach you are doing
       an inefficient grid search and hope that you   ll find something good
       (while not overfitting).

   let   s take a look at how a id23 approach can solve
   most of these problems.

deep id23 for trading

   remember that the traditional id23 problem can be
   formulated as a markov decision process (mdp). we have an agent acting
   in an environment. each time step t the agent receives as the input the
   current state s_t , takes an action a_t , and receives a reward r_{t+1}
   and the next state s_{t+1} . the agent chooses the action based on some
   policy \pi : a_t = \pi(s_t) . it is our goal to find a policy that
   maximizes the cumulative reward \sum r_t over some finite or infinite
   time horizon.

   id23

   let   s try to understand what these symbols correspond to in the trading
   setting.

   agent

   let   s start with the easy part. the agent is our trading agent. you can
   think of the agent as a human trader who opens the gui of an exchange
   and makes trading decision based on the current state of the exchange
   and his or her account.

   environment

   here it gets a little hairy. the obvious answer would be that the
   exchange is our environment. but the important thing to note is that
   there are many other agents, both human and algorithmic market players,
   trading on the same exchange. let   s assume for a moment that we are
   taking actions on a minutely scale (more on that below). we take some
   action, wait a minute, get a new state, take another action, and so on.
   when we observe a new state it will be the response of the market
   environment, which includes the response of the other agents. thus,
   from the perspective of our agent, these agents are also part of the
   environment. they   re not something we can control.

   however, by putting other agents together into some big complex
   environment we lose the ability to explicitly model them. for example,
   one can imagine that we could learn to reverse-engineer the algorithms
   and strategies that other traders are running and then learn to exploit
   them. doing so would put us into a multi-agent id23
   (marl) problem setting, which is an active research area. i   ll talk
   more about that below. for simplicity, let   s just assume we don   t do
   this, and assume we   re interacting with a single complex environment
   that includes the behavior of all other agents.

   state

   in the case of trading on an exchange, we do not observe the complete
   state of the environment. for example, we don   t know about the other
   agents are in the environment, how many there are, what their account
   balances are, or what their open limit orders are. this means, we are
   dealing with a partially observable markov decision process (pomdp).
   what the agent observes is not the actual state s_t of the environment,
   but some derivation of that. let   s call that the observation x_t ,
   which is calculated using some function of the full state x_t \sim
   o(s_t) .

   in our case, the observation at each timestep  t is simply the history
   of all exchange events (described in the data section above) received
   up to time t . this event history can be used to build up the current
   exchange state. however, in order for our agent to make decisions,
   there are a few other things that the observation must include, such as
   the current account balance, and open limit orders, if any.

   time scale

   we need to decide what time scale we want to act on. days? hours?
   minutes? seconds? milliseconds? nanoseconds? variables scales? all of
   these require different approaches. someone buying an asset and holding
   it for several days, weeks or months is often making a long-term bet
   based on analysis, such as    will bitcoin be successful?   . often, these
   decisions are driven by external events, news, or a fundamental
   understanding of the assets value or potential. because such an
   analysis typically requires an understanding of how the world works, it
   can be difficult to automate using machine learning techniques. on the
   opposite end, we have high frequency trading (hft) techniques, where
   decisions are based almost entirely on market microstructure signals.
   decisions are made on nanosecond timescales and trading strategies use
   dedicated connections to exchanges and extremely fast but simple
   algorithms running of fpga hardware. another way to think about these
   two extremes is in term of    humanness   . the former requires a big
   picture view and an understanding of how the world works, human
   intuition and high-level analysis, while the latter is all about
   simple, but extremely fast, pattern matching.

   neural networks are popular because, given a lot of data, they can
   learn more complex representations than algorithms such as linear
   regression or naive bayes. but deep neural nets are also slow,
   relatively speaking. they can   t make predictions on nanosecond time
   scales and thus cannot compete with the speed of hft algorithms. that   s
   why i think the sweet spot is somewhere in the middle of these two
   extremes. we want to act on a time scale where we can analyze data
   faster than a human possibly could, but where being smarter allows us
   to beat the    fast but simple    algorithms. my guess, and it really is
   just a guess, is that this corresponds to acting on timescales
   somewhere between a few milliseconds and a few minutes. humans traders
   can act on these timescales as well, but not as quickly as algorithms.
   and they certainly cannot synthesize the same amount of information
   that an algorithm can in that same time period. that   s our advantage.

   another reason to act on relatively short timescales is that patterns
   in the data may be more apparent. for example, because most human
   traders look at the exact same (limited) graphical user interfaces
   which have pre-defined market signals (like the [29]macd signal that is
   built into many exchange guis), their actions are restricted to the
   information present in those signals, resulting in certain action
   patterns. similarly, algorithms running in the market act based on
   certain patterns. our hope is that deep rl algorithms can pick up those
   patterns and exploit them.

   note that we could also act on variable time scales, based on some
   signal trigger. for example, we could decide to take an action whenever
   a large trade occurred in the market. such as trigger-based agent would
   still roughly correspond to some time scale, depending on the frequency
   of the trigger event.

   action space

   in id23, we make a distinction between discrete
   (finite) and continuous (infinite) action spaces. depending on how
   complex we want our agent to be, we have a couple of choices here. the
   simplest approach would be to have three actions: buy, hold, and sell.
   that works, but it limits us to placing market orders and to invest a
   deterministic amount of money at each step.  the next level of
   complexity would be to let our agent learn how much money to invest,
   for example, based on the uncertainty of our model. that would put us
   into a continuous action space, as we need to decide on both the
   (discrete) action and the (continuous) quantity. an even more complex
   scenario arises when we want our agent to be able to place limit
   orders. in that case our agent must decide the level (price) and the
   quantity of the order, both of which are continuous quantities. it must
   also be able to cancel open orders that have not yet been matched.

   reward function

   this is another tricky one. there are several possible reward functions
   we can pick from. an obvious one would the realized pnl (profit and
   loss). the agent receives a reward whenever it closes a position, e.g.
   when it sells an asset it has previously bought, or buys an asset it
   has previously borrowed. the net profit from that trade can be positive
   or negative. that   s the reward signal. as the agent maximizes the total
   cumulative reward, it learns to trade profitably. this reward function
   is technically correct and leads to the optimal policy in the limit.
   however, rewards are sparse because buy and sell actions are relatively
   rare compared to doing nothing. hence, it requires the agent to learn
   without receiving frequent feedback.

   an alternative with more frequent feedback would be the unrealized pnl,
   which the net profit the agent would get if it were to close all of its
   positions immediately. for example, if the price went down after the
   agent placed a buy order, it would receive a negative reward even
   though it hasn   t sold yet. because the unrealized pnl may change at
   each time step, it gives the agent more frequent feedback signals.
   however, the direct feedback may also bias the agent towards short-term
   actions when used in conjunction with a decay factor.

   both of these reward functions naively optimize for profit. in reality,
   a trader may want to minimize risk. a strategy with a slightly lower
   return but significantly lower volatility is preferably over a highly
   volatile but only slightly more profitable strategy. using
   the [30]sharpe ratio is one simple way to take risk into account, but
   there are many others. we may also want to take into account something
   like [31]maximum drawdown, described above.  one can image a wide range
   of complex reward function that trade-off between profit and risk.

the case for id23

   now that we have an idea of how id23 can be used in
   trading, let   s understand why we want to use it over supervised
   techniques. developing trading strategies using rl looks something like
   this. much simpler, and more principled than the approach we saw in the
   previous section.

   trading strategy development with rl

   end-to-end optimization of what we care about

   in the traditional strategy development approach we must go through
   several steps, a pipeline, before we get to the metric we actually care
   about. for example, if we want to find a strategy with a maximum
   drawdown of 25%, we need to train supervised model, come up with a
   rule-based policy using the model, backtest the policy and optimize its
   hyperparameters, and finally assess its performance through simulation.

   id23 allows for end-to-end optimization and maximizes
   (potentially delayed) rewards. by adding a term to the reward function,
   we can for example directly optimize for this drawdown, without needing
   to go through separate stages. for example, you could imagine giving a
   large negative reward whenever a drawdown of more than 25% happens,
   forcing the agent to look for a different policy. of course, we can
   combine drawdown with many other metrics you care about. this is not
   only easier, but also a much more powerful model.

   learned policies

   instead of needing to hand-code a rule-based policy, reinforcement
   learning directly learns a policy. there   s no need for us to specify
   rules and thresholds such as    buy when you are more than 75% sure that
   the market will move up   . that   s baked in the rl policy, which
   optimizes for the metric we care about. we   re removing a full step from
   the strategy development process! and because the policy can be
   parameterized by a complex model, such as a deep neural network, we can
   learn policies that are more complex and powerful than any rules a
   human trader could possibly come up with. and as we   ve seen above, the
   policies implicitly take into account metrics such as risk, if that   s
   something we   re optimizing for.

   trained directly in simulation environments

   we needed separate backtesting and parameter optimization steps because
   it was difficult for our strategies to take into account environmental
   factors, such as order book liquidity, fee structures, latencies, and
   others, when using a supervised approach. it is not uncommon to come up
   with a strategy, only to find out much later that it does not work,
   perhaps because the latencies are too high and the market is moving too
   quickly so that you cannot get the trades you expected to get.

   because id23 agents are trained in a simulation, and
   that simulation can be as complex as you want, taking into account
   latencies, liquidity and fees, we don   t have this problem! getting
   around environmental limitations is part of the optimization process.
   for example, if we simulate the latency in the id23
   environment, and this results in the agent making a mistake, the agent
   will get a negative reward, forcing it to learn to work around the
   latencies.

   we could take this a step further and simulate the response of the
   other agents in the same environment, to model impact of our own
   orders, for example. if the agent   s actions move the price in a
   simulation that   s based on historical data, we don   t know how the real
   market would have responded to this. typically, simulators ignore this
   and assume that orders do not have market impact. however, by learning
   a model of the environment and performing rollouts using techniques
   like a id169 (mcts), we could take into account
   potential reactions of the market (other agents). by being smart about
   the data we collect from the live environment, we can continuously
   improve our model. there exists an interesting exploration/exploitation
   tradeoff here: do we act optimally in the live environment to generate
   profits, or do we act suboptimally to gather interesting information
   that we can use to improve the model of our environment and other
   agents?

   that   s a very powerful concept. by building an increasingly complex
   simulation environment that models the real world you can train very
   sophisticated agents that learn to take environment constraints into
   account.

   learning to adapt to market conditions

   intuitively, certain strategies and policies will work better in some
   market environments than others. for example, a strategy may work well
   in a bearish environment, but lose money in a bullish environment.
   partly, this is due to the simplistic nature of the policy, which does
   not have a parameterization powerful enough to learn to adapt to
   changing market conditions.

   because rl agents are learning powerful policies parameterized by
   neural networks, they can also learn to adapt to various market
   conditions by seeing them in historical data, given that they are
   trained over a long time horizon and have sufficient memory. this
   allows them to be much more robust to changing markets. in facts, we
   can directly optimize them to become robust to changes in market
   conditions, by putting appropriate penalties into your reward function.

   ability to model other agents

   a unique ability of id23 is that we can explicitly
   take into account other agents. so far we   ve always talked about    how
   the market reacts   , ignoring that the market is really just a group of
   agents and algorithms, just like us. however, if we explicitly modeled
   the other agents in the environment, our agent could learn to exploit
   their strategies. in essence, we are reformulating the problem from
      market prediction    to    agent exploitation   . this is much more similar
   to what we are doing in multiplayer games, like dota.

the case for trading agent research

   my goal with this post is not only to give an introduction to
   id23 for trading, but also to convince more
   researchers to take a look at the problem. let   s take a look what makes
   trading an interesting research problem.

   live testing and fast iteration cycle

   when training id23 agents, it is often difficult or
   expensive to deploy them in the real world and get feedback. for
   example, if you trained an agent to play starcraft 2, how would you let
   it play against a larger number of human players? same for chess,
   poker, or any other game that is popular in the rl community. you would
   probably need to somehow enter a tournament and let your agent play
   there.

   trading agents have characteristics very similar to those for
   multiplayer games. but you can easily test them live! you can deploy
   your agent on an exchange through their api and immediately get
   real-world market feedback. if your agent does not generalize and loses
   money you know that you have probably overfit to the training data. in
   other words, the iteration cycle can be extremely fast.

   large multiplayer environments

   the trading environment is essentially a multiplayer game with
   thousands of agents acting simultaneously. this is an active research
   area. we are now making progress at multiplayer games such as poker,
   dota2, and others, and many of the same techniques will apply here. in
   fact, the trading problem is a much more difficult one due to the sheer
   number of simultaneous agents who can leave or join the game at any
   time. understanding how to build models of other agents is only one
   possible direction one can go into. as mentioned earlier, one could
   choose to perform actions in a live environment with the goal
   maximizing the information gain with respect to kind policies the other
   agents may be following.

   learning to exploit other agents & manipulate the market

   closely related is the question of whether we can learn to exploit
   other agents acting in the environment. for example, if we knew exactly
   what algorithms were running in the market we can trick them into
   taking actions they should not take and profit from their mistakes.
   this also applies to human traders, who typically act based on a
   combination of well-known market signals, such as exponential moving
   averages or order book pressures.

   disclaimer: don   t allow your agent to do anything illegal! do comply
   with all applicable laws in your jurisdiction. and finally, past
   performance is no guarantee of future results.

   sparse rewards & exploration

   trading agents typically receive sparse rewards from the market. most
   of the time you will do nothing. buy and sell actions typically account
   for a tiny fraction of all actions you take. naively applying
      reward-hungry    id23 algorithms will fail. this opens
   up the possibility for new algorithms and techniques, especially
   model-based ones, that can efficiently deal with sparse rewards.

   a similar argument can be made for exploration. many of today   s
   standard algorithms, such as id25 or a3c, use a very naive approach to
   exploration, basically adding random noise to the policy. however, in
   the trading case, most states in the environment are bad, and there are
   only a few good ones. a naive random approach to exploration will
   almost never stumble upon those good state-actions pairs. a new
   approach is necessary here.

   multi-agent self-play

   similar to how self-play is applied to two-player games such as chess
   or go, one could apply self-play techniques to a multiplayer
   environment. for example, you could imagine simultaneously training a
   large number of competing agents, and investigate whether the resulting
   market dynamic somehow resembles the dynamics found in the real world.
   you could also mix the types of agents you are training, from different
   rl algorithms, the evolution-based ones, and deterministic ones. one
   could also use the real-world market data as a supervised feedback
   signal to    force    the agents in the simulation to collectively behave
   like the real world.

   continuous time

   because markets change on micro- to milliseconds times scales, the
   trading domain is a good approximation of a continuous time domain. in
   our example above we   ve fixed a time period and made that decision for
   the agent. however, you could imagine making this part of the agent
   training. thus, the agent would not only decide what actions to take,
   but also when to take an action.  again, this is an active research
   area useful for many other domains, including robotics.

   nonstationary, lifelong learning, and catastrophic forgetting

   the trading environment is inherently nonstationary. market conditions
   change and other agent join, leave, and constantly change their
   strategies. can we train agents that learn to automatically adjust to
   changing market conditions, without    forgetting    what they have learned
   before? for example, can an agent successfully transition from a bear
   to a bull market and then back to a bear market, without needing to be
   re-trained? can an agent adjust to other agent joining and learning to
   exploit them automatically?

   id21 and auxiliary tasks

   training id23 from scratch in complex domains can
   take a very long time because they not only need to learn to make good
   decisions, but they also need to learn the    rules of the game   . there
   are many ways to speed up the training of id23
   agents, including id21, and using auxiliary tasks. for
   example, we could imagine pre-training an agent with an expert policy,
   or adding [32]auxiliary tasks, such as price prediction, to the agent   s
   training objective, to speed up the learning.

conclusion

   the goal was to give an introduction to id23 based
   trading agents, make an argument for why they are superior to current
   trading strategy development models, and make an argument for why i
   believe more researcher should be working on this. i hope i achieved
   some this in this post. please let me know in the comments what you
   think, and feel free to get in touch to ask questions.

   thanks for reading all the way to the end :)

   categories[33]deep learning, [34]neural networks, [35]reinforcement
   learning, [36]trading

post navigation

   [37]previous postprevious ai and deep learning in 2017     a year in
   review

subscribe to blog via email

   enter your email address to subscribe to this blog and receive
   notifications of new posts by email.

   email address ____________________

   (button) subscribe

recent posts

     * [38]introduction to learning to trade with id23
     * [39]ai and deep learning in 2017     a year in review
     * [40]hype or not? some perspective on openai   s dota 2 bot
     * [41]learning id23 (with code, exercises and
       solutions)
     * [42]id56s in tensorflow, a practical guide and undocumented features
     * [43]deep learning for chatbots, part 2     implementing a
       retrieval-based model in tensorflow
     * [44]deep learning for chatbots, part 1     introduction
     * [45]attention and memory in deep learning and nlp

archives

     * [46]february 2018
     * [47]december 2017
     * [48]august 2017
     * [49]october 2016
     * [50]august 2016
     * [51]july 2016
     * [52]april 2016
     * [53]january 2016
     * [54]december 2015
     * [55]november 2015
     * [56]october 2015
     * [57]september 2015

categories

     * [58]conversational agents
     * [59]convolutional neural networks
     * [60]deep learning
     * [61]gpu
     * [62]id38
     * [63]memory
     * [64]neural networks
     * [65]news
     * [66]nlp
     * [67]recurrent neural networks
     * [68]id23
     * [69]id56s
     * [70]tensorflow
     * [71]trading
     * [72]uncategorized

meta

     * [73]log in
     * [74]entries rss
     * [75]comments rss
     * [76]wordpress.org

   [77]proudly powered by wordpress

references

   visible links
   1. http://www.wildml.com/feed/
   2. http://www.wildml.com/comments/feed/
   3. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/feed/
   4. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
   5. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
   6. http://www.wildml.com/wp-json/oembed/1.0/embed?url=http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/&format=xml
   7. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/#content
   8. http://www.wildml.com/
   9. http://www.wildml.com/
  10. https://www.getrevue.co/profile/wildml
  11. http://www.wildml.com/deep-learning-glossary/
  12. mailto:dennybritz@gmail.com
  13. http://www.wildml.com/about/
  14. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  15. http://www.wildml.com/author/dennybritz/
  16. https://twitter.com/aerinykim
  17. https://twitter.com/suzatweet
  18. https://twitter.com/hardmaru
  19. https://coinmarketcap.com/exchanges/volume/24-hour/
  20. https://www.gdax.com/trade/btc-usd
  21. https://www.investopedia.com/articles/technical/02/121702.asp
  22. https://docs.gdax.com/
  23. https://en.wikipedia.org/wiki/alpha_(finance)
  24. https://en.wikipedia.org/wiki/beta_(finance)
  25. https://en.wikipedia.org/wiki/sharpe_ratio#use_in_finance
  26. https://en.wikipedia.org/wiki/drawdown_(economics)
  27. https://en.wikipedia.org/wiki/value_at_risk
  28. https://www.investopedia.com/articles/07/sharpe_ratio.asp
  29. https://www.investopedia.com/terms/m/macd.asp
  30. https://www.quantinsti.com/blog/sharpe-ratio-applications-algorithmic-trading/
  31. https://www.investopedia.com/terms/d/drawdown.asp
  32. https://arxiv.org/abs/1611.05397
  33. http://www.wildml.com/category/deep-learning/
  34. http://www.wildml.com/category/neural-networks/
  35. http://www.wildml.com/category/reinforcement-learning/
  36. http://www.wildml.com/category/trading/
  37. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  38. http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/
  39. http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/
  40. http://www.wildml.com/2017/08/hype-or-not-some-perspective-on-openais-dota-2-bot/
  41. http://www.wildml.com/2016/10/learning-reinforcement-learning/
  42. http://www.wildml.com/2016/08/id56s-in-tensorflow-a-practical-guide-and-undocumented-features/
  43. http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/
  44. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  45. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  46. http://www.wildml.com/2018/02/
  47. http://www.wildml.com/2017/12/
  48. http://www.wildml.com/2017/08/
  49. http://www.wildml.com/2016/10/
  50. http://www.wildml.com/2016/08/
  51. http://www.wildml.com/2016/07/
  52. http://www.wildml.com/2016/04/
  53. http://www.wildml.com/2016/01/
  54. http://www.wildml.com/2015/12/
  55. http://www.wildml.com/2015/11/
  56. http://www.wildml.com/2015/10/
  57. http://www.wildml.com/2015/09/
  58. http://www.wildml.com/category/conversational-agents/
  59. http://www.wildml.com/category/neural-networks/convolutional-neural-networks/
  60. http://www.wildml.com/category/deep-learning/
  61. http://www.wildml.com/category/gpu/
  62. http://www.wildml.com/category/language-modeling/
  63. http://www.wildml.com/category/memory/
  64. http://www.wildml.com/category/neural-networks/
  65. http://www.wildml.com/category/news/
  66. http://www.wildml.com/category/nlp/
  67. http://www.wildml.com/category/neural-networks/recurrent-neural-networks/
  68. http://www.wildml.com/category/reinforcement-learning/
  69. http://www.wildml.com/category/id56s/
  70. http://www.wildml.com/category/tensorflow/
  71. http://www.wildml.com/category/trading/
  72. http://www.wildml.com/category/uncategorized/
  73. http://www.wildml.com/wp-login.php
  74. http://www.wildml.com/feed/
  75. http://www.wildml.com/comments/feed/
  76. https://wordpress.org/
  77. https://wordpress.org/

   hidden links:
  79. http://www.wildml.com/wp-content/uploads/2018/01/screen-shot-2018-01-20-at-2.46.01-pm.png
  80. http://www.wildml.com/wp-content/uploads/2018/01/screen-shot-2018-01-21-at-7.56.13-pm.png
