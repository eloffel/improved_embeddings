   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

             how neural networks learn distributed representations

   deep learning   s effectiveness is often attributed to the ability of
   neural networks to learn rich representations of data.

   by [27]garrett hoffman

   february 13, 2018

   neurons neurons (source: [28]pixabay)

   the concept of distributed representations is often central to deep
   learning, particularly as it applies to natural language tasks. those
   beginning in the field may quickly understand this as simply a vector
   that represents some piece of data. while this is true, understanding
   distributed representations at a more conceptual level increases our
   appreciation of the role they play in making deep learning so
   effective.

   to examine different types of representation, we can do a simple
   thought exercise. let   s say we have a bunch of    memory units    to store
   information about shapes. we can choose to represent each individual
   shape with a single memory unit, as demonstrated in figure 1.
   sparse or local non-distributed representation of shapes figure 1.
   sparse or local, non-distributed representation of shapes. image by
   garrett hoffman.

   this non-distributed representation, referred to as "sparse" or
   "local," is inefficient in multiple ways. first, the dimensionality of
   our representation will grow as the number of shapes we observe grows.
   more importantly, it doesn   t provide any information about how these
   shapes relate to each other. this is the true value of a distributed
   representation: its ability to capture meaningful    semantic similarity   
   between between data through concepts.
   distributed representation of shapes figure 2. distributed
   representation of shapes. image by garrett hoffman.

   figure 2 shows a distributed representation of this same set of shapes
   where information about the shape is represented with multiple    memory
   units    for concepts related to orientation and shape. now the    memory
   units    contain information both about an individual shape and how each
   shape relates to each other. when we come across a new shape with our
   distributed representation, such as the circle in figure 3, we don   t
   increase the dimensionality and we also know some information about the
   circle, as it relates to the other shapes, even though we haven   t seen
   it before.
   distributed representation of a circle figure 3. distributed
   representation of a circle; this representation is more useful as it
   provides us with information about how this new shape is related to our
   other shapes. image by garrett hoffman.

   while this shape example is oversimplified, it serves as a great
   high-level, abstract introduction to distributed representations.
   notice, in the case of our distributed representation for shapes, that
   we selected four concepts or features (vertical, horizontal, rectangle,
   ellipse) for our representation. in this case, we were required to know
   what these important and distinguishing features were beforehand, and
   in many cases, this is a difficult or impossible thing to know. it is
   for this reason that feature engineering is such a crucial task in
   classical machine learning techniques. finding a good representation of
   our data is critical to the success of downstream tasks like
   classification or id91. one of the reasons that deep learning has
   seen tremendous success is a neural networks' ability to learn rich
   distributed representations of data.

   to examine this, we will revisit the [29]problem we tackled in our lstm
   tutorial   predicting stock market sentiment from social media posts from
   [30]stocktwits. in this tutorial, we built a multi-layered lstm to
   predict the sentiment of a message from the raw body of text. when
   processing our message data, we created a mapping of our vocabulary to
   an integer index.

   this mapping of vocabulary to integer is a non-distributed sparse
   representation of our data. for instance, the word buy is mapped to
   index 25 and the word long is represented as index 68. note that this
   is an equivalent representation to a    one-hot encoded    vector of length
   vocab_size with a 1 in the index representing the word and a 0
   everywhere else. these are two independent representations that have no
   relational information between the two words despite their semantic
   similarity when it comes to investing   both words represent a position
   of owning a stock.

   the canonical methodology for learning a distributed representation of
   words is the [31]id97 model. the id97 skip-gram model, whose
   architecture is pictured in figure 4, takes in a single word, passes
   this to a single linear hidden layer unique to that word, and uses a
   softmax activation layer to predict the words that occur in a window
   around it.
   distributed representation of shapes figure 4. distributed
   representation of shapes. image by google from [32]   distributed
   representations of words and phrases and their compositionality   , used
   with permission.

   the id97 model uses the j.r. firth philosophy      you shall know a
   word by the company it keeps,    and can be [33]implemented very easily
   in tensorflow. by learning hidden weights, which will be used as our
   distributed representation, words that appear in similar context will
   have a similar representation. id97 is a model designed
   specifically for learning distributed representations of words, also
   called "id27s," from their context. oftentimes, these
   embeddings are pre trained with id97 and then used as inputs to
   other models performing language tasks.

   alternatively, distributed representations can be learned in an
   end-to-end fashion as part of the model training process for an
   arbitrary task. this is how we learned our id27 in our
   [34]stock market sentiment lstm model. recall the model architecture
   (see figure 5), where we input our sparse representation of words into
   an embedding layer.
   unrolled single-layer lstm network with embedding layer figure 5.
   unrolled single-layer lstm network with embedding layer. image courtesy
   of udacity, used with permission.

   trained under this paradigm, distributed representations will
   specifically learn to represent items as they relate to the learning
   task   in our case, our distributed representation should specifically
   learn semantic context around sentiments of words. we can examine this
   by extracting our id27s and looking at some examples.

   we visualize the relationship between a few    bearish-bullish pairs    by
   reducing the dimensionality of our representations [35]using tsne (see
   figure 6). note the concept of sentiment represented by the
   left-to-right direction between word pairs (i.e., bearish vs. bullish,
   overvalued vs. undervalued, short vs. long, etc.).
   visualization of id27s figure 6. visualization of word
   embeddings demonstrating the semantic relationship of the concept of
   sentiment captured by our distributed representation. image by garrett
   hoffman.

   while these aren   t perfect   ideally, we would want to see the pairings
   more vertically aligned, and we also have some pairs where sentiment is
   reversed   they are pretty good given limited training. our model's
   ability to learn this type of representation is a major reason it is
   able to achieve high accuracy when predicting sentiment.

   a neural network's ability to learn distributed representation of data
   is one of the main reasons that deep learning is so effective for so
   many different types of problems. the power and beauty of this concept
   makes representation learning one of the most exciting and active areas
   of deep learning research. methods for learning shared representations
   across multiple modals (e.g., words and images, words in different
   languages) are enabling advancements in image captioning and
   translation. we can be sure that better understanding these types of
   representations will continue to be a major factor in driving ai
   forward.

   this post is a collaboration between o'reilly and [36]tensorflow.
   [37]see our statement of editorial independence.
   article image: neurons (source: [38]pixabay).

   share
    1. [39]tweet
    2.
    3.
     __________________________________________________________________

[40]garrett hoffman

   garrett hoffman is a senior data scientist at stocktwits, where he
   leads efforts to use data science and machine learning to understand
   social dynamics and develop research and discovery tools that are used
   by a network of over one million investors. garrett has a technical
   background in math and computer science but gets most excited about
   approaching data problems from a people-first perspective   using what we
   know or can learn about complex systems to drive optimal decisions,
   experiences, and outcomes.
   [41]more
     __________________________________________________________________

   [42]bots landscape.

   [43]ai

[44]infographic: the bot platform ecosystem

   by [45]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [46]vertical forest, milan.

   [47]ai

[48]evolve ai

   by [49]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [50]welcome sign at o'reilly ai conference 2016

   [51]ai

[52]highlights from the o'reilly ai conference in new york 2016

   by [53]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [54]close up of uber's self driving car in pittsburgh.

   [55]ai

[56]how ai is propelling driverless cars, the future of surface transport

   by [57]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [58]our company
     * [59]teach/speak/write
     * [60]careers
     * [61]customer service
     * [62]contact us

site map

     * [63]ideas
     * [64]learning
     * [65]topics
     * [66]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [67]terms of service     [68]privacy policy     [69]editorial independence

   animal

   iframe: [70]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/d3807-garrett-hoffman
  28. https://pixabay.com/en/neurons-brain-cells-brain-structure-1773922/
  29. https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow
  30. https://stocktwits.com/
  31. https://arxiv.org/pdf/1310.4546.pdf
  32. https://arxiv.org/pdf/1310.4546.pdf
  33. https://www.tensorflow.org/tutorials/id97
  34. https://www.oreilly.com/ideas/introduction-to-lstms-with-tensorflow
  35. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  36. https://www.tensorflow.org/
  37. http://www.oreilly.com/about/editorial_independence.html
  38. https://pixabay.com/en/neurons-brain-cells-brain-structure-1773922/
  39. https://twitter.com/share
  40. https://www.oreilly.com/people/d3807-garrett-hoffman
  41. https://www.oreilly.com/people/d3807-garrett-hoffman
  42. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  43. https://www.oreilly.com/topics/ai
  44. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  45. https://www.oreilly.com/people/b1d73-jon-bruner
  46. https://www.oreilly.com/ideas/evolve-ai
  47. https://www.oreilly.com/topics/ai
  48. https://www.oreilly.com/ideas/evolve-ai
  49. https://www.oreilly.com/people/14d38-naveen-rao
  50. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  51. https://www.oreilly.com/topics/ai
  52. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  53. https://www.oreilly.com/people/0d2c1-mac-slocum
  54. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  55. https://www.oreilly.com/topics/ai
  56. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  57. https://www.oreilly.com/people/c7521-shahin-farshchi
  58. http://oreilly.com/about/
  59. http://oreilly.com/work-with-us.html
  60. http://oreilly.com/careers/
  61. http://shop.oreilly.com/category/customer-service.do
  62. http://shop.oreilly.com/category/customer-service.do
  63. https://www.oreilly.com/ideas
  64. https://www.oreilly.com/topics/oreilly-learning
  65. https://www.oreilly.com/topics
  66. https://www.oreilly.com/all
  67. http://oreilly.com/terms/
  68. http://oreilly.com/privacy.html
  69. http://www.oreilly.com/about/editorial_independence.html
  70. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  72. https://www.oreilly.com/
  73. https://www.facebook.com/oreilly/
  74. https://twitter.com/oreillymedia
  75. https://www.youtube.com/user/oreillymedia
  76. https://plus.google.com/+oreillymedia
  77. https://www.linkedin.com/company/oreilly-media
  78. https://www.oreilly.com/
