   #[1]rss feed for off the convex path

   [2][logo.jpg]

   [3]about [4]contact [5]subscribe

can increasing depth serve to accelerate optimization?

   nadav cohen       mar 2, 2018       12 minute read

      how does depth help?    is a fundamental question in the theory of deep
   learning. conventional wisdom, backed by theoretical studies (e.g.
   [6]eldan & shamir 2016; [7]raghu et al. 2017; [8]lee et al. 2017;
   [9]cohen et al. 2016; [10]daniely 2017; [11]arora et al. 2018), holds
   that adding layers increases expressive power. but often this
   expressive gain comes at a price    optimization is harder for deeper
   networks (viz., [12]vanishing/exploding gradients). recent works on
      landscape characterization    implicitly adopt this worldview (e.g.
   [13]kawaguchi 2016; [14]hardt & ma 2017; [15]choromanska et al. 2015;
   [16]haeffele & vidal 2017; [17]soudry & carmon 2016; [18]safran &
   shamir 2017). they prove theorems about local minima and/or saddle
   points in the objective of a deep network, while implicitly assuming
   that the ideal landscape would be convex (single global minimum, no
   other critical point). my [19]new paper with sanjeev arora and elad
   hazan makes the counterintuitive suggestion that sometimes, increasing
   depth can accelerate optimization.

   our work can also be seen as one more piece of evidence for a nascent
   belief that overparameterization of deep nets may be a good thing. by
   contrast, classical statistics discourages training a model with more
   parameters than necessary [20]as this can lead to overfitting.

$\ell_p$ regression

   let   s begin by considering a very simple learning problem - scalar
   id75 with $\ell_p$ loss (our theory and experiments will
   apply to $p>2$):

   $s$ here stands for a training set, consisting of pairs
   $(\mathbf{x},y)$ where $\mathbf{x}$ is a vector representing an
   instance and $y$ is a (numeric) scalar standing for its label;
   $\mathbf{w}$ is the parameter vector we wish to learn. let   s convert
   the linear model to an extremely simple    depth-2 network   , by replacing
   the vector $\mathbf{w}$ with a vector $\mathbf{w_1}$ times a scalar
   $\omega_2$. clearly, this is an overparameterization that does not
   change expressiveness, but yields the (non-convex) objective:

   we show in the paper, that if one applies id119 over
   $\mathbf{w_1}$ and $\omega_2$, with small learning rate and near-zero
   initialization (as customary in deep learning), the induced dynamics on
   the overall (end-to-end) model $\mathbf{w}=\mathbf{w_1}\omega_2$ can be
   written as follows:

   where $\rho^{(t)}$ and $\mu^{(t,\tau)}$ are appropriately defined
   (time-dependent) coefficients. thus the seemingly benign addition of a
   single multiplicative scalar turned plain id119 into a
   scheme that somehow has a memory of past gradients    the key feature of
   [21]momentum methods    as well as a time-varying learning rate. while
   theoretical analysis of the precise benefit of momentum methods is
   never easy, a simple experiment with $p=4$, on [22]uci machine learning
   repository   s [23]   gas sensor array drift at different concentrations   
   dataset, shows the following effect:

   l4 regression experiment

   not only did the overparameterization accelerate id119, but
   it has done so more than two well-known, explicitly designed
   acceleration methods     [24]adagrad and [25]adadelta (the former did not
   really provide a speedup in this experiment). we observed similar
   speedups in other settings as well.

   what is happening here? can non-convex objectives corresponding to deep
   networks be easier to optimize than convex ones? is this phenomenon
   common or is it limited to toy problems as above? we take a first crack
   at addressing these questions   

overparameterization: decoupling optimization from expressiveness

   a general study of the effect of depth on optimization entails an
   inherent difficulty - deeper networks may seem to converge faster due
   to their superior expressiveness. in other words, if optimization of a
   deep network progresses more rapidly than that of a shallow one, it may
   not be obvious whether this is a result of a true acceleration
   phenomenon, or simply a byproduct of the fact that the shallow model
   cannot reach the same loss as the deep one. we resolve this conundrum
   by focusing on models whose representational capacity is oblivious to
   depth - linear neural networks, the subject of many recent studies.
   with linear networks, adding layers does not alter expressiveness; it
   manifests itself only in the replacement of a matrix parameter by a
   product of matrices - an overparameterization. accordingly, if this
   leads to accelerated convergence, one can be certain that it is not an
   outcome of any phenomenon other than favorable properties of depth for
   optimization.

implicit dynamics of depth

   suppose we are interested in learning a linear model parameterized by a
   matrix $w$, through minimization of some training loss $l(w)$. instead
   of working directly with $w$, we replace it by a depth $n$ linear
   neural network, i.e. we overparameterize it as
   $w=w_{n}w_{n-1}\cdots{w_1}$, with $w_j$ being weight matrices of
   individual layers. in the paper we show that if one applies gradient
   descent over $w_{1}\ldots{w}_n$, with small learning rate $\eta$, and
   with the condition:

   satisfied at optimization commencement (note that this approximately
   holds with standard near-zero initialization), the dynamics induced on
   the overall end-to-end mapping $w$ can be written as follows:

   we validate empirically that this analytically derived update rule
   (over classic linear model) indeed complies with deep network
   optimization, and take a series of steps to theoretically interpret it.
   we find that the transformation applied to the gradient $\nabla{l}(w)$
   (multiplication from the left by $[ww^\top]^\frac{j-1}{n}$, and from
   the right by $[w^\top{w}]^\frac{n-j}{n}$, followed by summation over
   $j$) is a particular preconditioning scheme, that promotes movement
   along directions already taken by optimization. more concretely, the
   preconditioning can be seen as a combination of two elements:
     * an adaptive learning rate that increases step sizes away from
       initialization; and
     * a    momentum-like    operation that stretches the gradient along the
       azimuth taken so far.

   an important point to make is that the update rule above, referred to
   hereafter as the end-to-end update rule, does not depend on widths of
   hidden layers in the linear neural network, only on its depth ($n$).
   this implies that from an optimization perspective, overparameterizing
   using wide or narrow networks has the same effect - it is only the
   number of layers that matters. therefore, acceleration by depth need
   not be computationally demanding - a fact we clearly observe in our
   experiments (previous figure for example shows acceleration by orders
   of magnitude at the price of a single extra scalar parameter).

   end-to-end update rule

beyond id173

   the end-to-end update rule defines an optimization scheme whose steps
   are a function of the gradient $\nabla{l}(w)$ and the parameter $w$. as
   opposed to many acceleration methods (e.g. [26]momentum or [27]adam)
   that explicitly maintain auxiliary variables, this scheme is
   memoryless, and by definition born from id119 over something
   (overparameterized objective). it is therefore natural to ask if we can
   represent the end-to-end update rule as id119 over some
   id173 of the loss $l(w)$, i.e. over some function of $w$. we
   prove, somewhat surprisingly, that the answer is almost always negative
   - as long as the loss $l(w)$ does not have a critical point at $w=0$,
   the end-to-end update rule, i.e. the effect of overparameterization,
   cannot be attained via any regularizer.

acceleration

   so far, we analyzed the effect of depth (in the form of
   overparameterization) on optimization by presenting an equivalent
   preconditioning scheme and discussing some of its properties. we have
   not, however, provided any theoretical evidence in support of
   acceleration (faster convergence) resulting from this scheme. full
   characterization of the scenarios in which there is a speedup goes
   beyond the scope of our paper. nonetheless, we do analyze a simple
   $\ell_p$ regression problem, and find that whether or not increasing
   depth accelerates depends on the choice of $p$: for $p=2$ (square loss)
   adding layers does not lead to a speedup (in accordance with previous
   findings by [28]saxe et al. 2014); for $p>2$ it can, and this may be
   attributed to the preconditioning scheme   s ability to handle large
   plateaus in the objective landscape. a number of experiments, with $p$
   equal to 2 and 4, and depths ranging between 1 (classic linear model)
   and 8, support this conclusion.

non-linear experiment

   as a final test, we evaluated the effect of overparameterization on
   optimization in a non-idealized (yet simple) deep learning setting -
   the [29]convolutional network tutorial for mnist built into tensorflow.
   we introduced overparameterization by simply placing two matrices in
   succession instead of the matrix in each dense layer. with an addition
   of roughly 15% in number of parameters, optimization accelerated by
   orders of magnitude:

   tensorflow mnist id98 experiment

   we note that similar experiments on other convolutional networks also
   gave rise to a speedup, but not nearly as prominent as the above.
   empirical characterization of conditions under which
   overparameterization accelerates optimization in non-linear settings is
   potentially an interesting direction for future research.

conclusion

   our work provides insight into benefits of depth in the form of
   overparameterization, from the perspective of optimization. many open
   questions and problems remain. for example, is it possible to
   rigorously analyze the acceleration effect of the end-to-end update
   rule (analogously to, say, [30]nesterov 1983 or [31]duchi et al. 2011)?
   treatment of non-linear deep networks is of course also of interest, as
   well as more extensive empirical evaluation.

   [32]nadav cohen
   subscribe to our [33]rss feed.
   spread the word:

comments

   please enable javascript to view the [34]comments powered by disqus.

   theme available on [35]github.

references

   visible links
   1. http://www.offconvex.org/feed.xml
   2. http://offconvex.github.io/
   3. http://www.offconvex.org/about/
   4. http://www.offconvex.org/contact/
   5. http://www.offconvex.org/subscribe/
   6. http://proceedings.mlr.press/v49/eldan16.pdf
   7. http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf
   8. http://proceedings.mlr.press/v65/lee17a/lee17a.pdf
   9. http://proceedings.mlr.press/v49/cohen16.pdf
  10. http://proceedings.mlr.press/v65/daniely17a/daniely17a.pdf
  11. https://openreview.net/pdf?id=b1j_rgwrw
  12. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  13. https://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf
  14. https://openreview.net/pdf?id=ryxb0rtxx
  15. http://proceedings.mlr.press/v38/choromanska15.pdf
  16. http://openaccess.thecvf.com/content_cvpr_2017/papers/haeffele_global_optimality_in_cvpr_2017_paper.pdf
  17. https://arxiv.org/pdf/1605.08361.pdf
  18. https://arxiv.org/pdf/1712.08968.pdf
  19. https://arxiv.org/pdf/1802.06509.pdf
  20. https://www.rasch.org/rmt/rmt222b.htm
  21. https://distill.pub/2017/momentum/
  22. https://archive.ics.uci.edu/ml/index.php
  23. https://archive.ics.uci.edu/ml/datasets/gas+sensor+array+drift+dataset
  24. http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
  25. https://arxiv.org/pdf/1212.5701.pdf
  26. https://distill.pub/2017/momentum/
  27. https://arxiv.org/pdf/1412.6980.pdf
  28. https://arxiv.org/pdf/1312.6120.pdf
  29. https://github.com/tensorflow/models/tree/master/tutorials/image/mnist
  30. http://www.cis.pku.edu.cn/faculty/vision/zlin/1983-a method of solving a convex programming problem with convergence rate o(k^(-2))_nesterov.pdf
  31. http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
  32. http://www.cohennadav.com/
  33. http://www.offconvex.org/feed.xml
  34. http://disqus.com/?ref_noscript
  35. https://github.com/johnotander/pixyll

   hidden links:
  37. https://facebook.com/sharer.php?u=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/
  38. https://twitter.com/intent/tweet?text=can%20increasing%20depth%20serve%20to%20accelerate%20optimization?&url=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/
  39. https://plus.google.com/share?url=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/
  40. http://www.linkedin.com/sharearticle?url=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/&title=can%20increasing%20depth%20serve%20to%20accelerate%20optimization?
  41. http://reddit.com/submit?url=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/&title=can%20increasing%20depth%20serve%20to%20accelerate%20optimization?
  42. https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/2018/03/02/acceleration-overparameterization/&t=can%20increasing%20depth%20serve%20to%20accelerate%20optimization?
