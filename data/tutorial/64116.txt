   #[1]adventures in machine learning    id23 tutorial
   using python and keras comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

id23 tutorial using python and keras

   by [9]admin | [10]keras

     * you are here:
     * [11]home
     * [12]keras
     * [13]id23 tutorial using python and keras

   mar 03
   [14]18
   id23 - python and keras - nchain environment

   in this post, i   m going to introduce the concept of reinforcement
   learning, and show you how to build an autonomous agent that can
   successfully play a simple game. id23 is an active
   and interesting area of machine learning research, and has been spurred
   on by recent successes such as the [15]alphago system, which has
   convincingly beat the best human players in the world. this occurred in
   a game that was thought too difficult for machines to learn. in this
   tutorial, i   ll first detail some background theory while dealing with a
   toy game in the open ai gym toolkit. we   ll then create a q table of
   this game using simple python, and then create a q network using keras.
   if you   d like to scrub up on keras, check out my [16]introductory keras
   tutorial. all code present in this tutorial is available on this site   s
   [17]github page.
     __________________________________________________________________

   recommended online course     if you   re more of a video based learner,
   i   d recommend the following inexpensive udemy online course in
   id23: [18]artificial intelligence: reinforcement
   learning in python
   [show?id=jbc0n5zkdzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0]
     __________________________________________________________________

id23     the basics

   id23 can be considered the third genre of the machine
   learning triad     unsupervised learning, supervised learning and
   id23. in supervised learning, we supply the machine
   learning system with curated (x, y) training pairs, where the intention
   is for the network to learn to map x to y. in id23,
   we create an agent which performs actions in an environment and the
   agent receives various rewards depending on what state it is in when it
   performs the action. in other words, an agent explores a kind of game,
   and it is trained by trying to maximize rewards in this game. this
   cycle is illustrated in the figure below:
   id23 with python and keras - id23
   environment

   id23 environment

   as can be observed above, the agent performs some action in the
   environment. an interpreter views this action in the environment, and
   feeds back an updated state that the agent now resides in, and also the
   reward for taking this action. the environment is not known by the
   agent beforehand, but rather it is discovered by the agent taking
   incremental steps in time. so, for instance, at time t the agent, in
   state $s_{t}$,  may take action a. this results in a new
   state $s_{t+1}$ and a reward r. this reward can be a positive real
   number, zero, or a negative real number. it is the goal of the agent to
   learn which state dependent action to take which maximizes its rewards.
   the way which the agent optimally learns is the subject of
   id23 theory and methodologies.

   to more meaningfully examine the theory and possible approaches behind
   id23, it is useful to have a simple example in which
   to work through. this simple example will come from an environment
   available on [19]open ai gym called nchain.

open ai gym example

   the nchain example on open ai gym is a simple 5 state environment.
   there are two possible actions in each state, move forward (action 0)
   and move backwards (action 1). when action 1 is taken, i.e. move
   backwards, there is an immediate reward of 2 given to the agent     and
   the agent is returned to state 0 (back to the beginning of the chain).
   however, when a move forward action is taken (action 0), there is no
   immediate reward until state 4. when the agent moves forward while in
   state 4, a reward of 10 is received by the agent. the agent stays in
   state 4 at this point also, so the reward can be repeated. there is
   also a random chance that the agent   s action is    flipped    by the
   environment (i.e. an action 0 is flipped to an action 1 and vice
   versa). the diagram below demonstrates this environment:
   id23 - python and keras - nchain environment

   open ai gym   s nchain environment

   you can play around with this environment by first installing the open
   ai gym python package     see instructions [20]here. then simply open up
   your python command prompt and have a play     see the figure below for
   an example of some of the commands available:
   id23 - python and keras - nchain python playaround

   nchain python playaround

   if you examine the code above, you can observe that first the python
   module is imported, and then the environment is loaded via the
   gym.make() command. the first step is to initalize / reset the
   environment by running env.reset()     this command returns the initial
   state of the environment     in this case 0. the first command i then run
   is env.step(1)     the value in the bracket is the action id. as
   explained previously, action 1 represents a step back to the beginning
   of the chain (state 0). the step() command returns 4 variables in a
   tuple, these are (in order):
     * the new state after the action
     * the reward due to the action
     * whether the game is    done    or not     the nchain game is done after
       1,000 steps
     * debugging information     not relevant in this example

   as can be observed, starting in state 0 and taking step(1) action, the
   agent stays in state 0 and gets 2 for its reward. next, i sent a series
   of action 0 commands. after every action 0 command, we would expect the
   progression of the agent along the chain, with the state increasing in
   increments (i.e. 0 -> 1 -> 2 etc.). however, you   ll observe after the
   first step(0) command, that the agent stays in state 0 and gets a 2
   reward. this is because of the random tendency of the environment to
      flip    the action occasionally, so the agent actually performed a 1
   action. this is just unlucky.

   nevertheless, i persevere and it can be observed that the state
   increments as expected, but there is no immediate reward for doing so
   for the agent until it reaches state 4. when in state 4, an action of 0
   will keep the agent in step 4 and give the agent a 10 reward. not only
   that, the environment allows this to be done repeatedly, as long as it
   doesn   t produce an unlucky    flip   , which would send the agent back to
   state 0     the beginning of the chain.

   now that we understand the environment that will be used in this
   tutorial, it is time to consider what method can be used to train the
   agent.

a first naive heuristic for id23

   in order to train the agent effectively, we need to find a good policy
   $\pi$ which maps states to actions in an optimal way to maximize
   reward. there are various ways of going about finding a good or
   optimal policy, but first, let   s consider a naive approach.

   let   s conceptualize a table, and call it a reward table, which looks
   like this:

   $$
   \begin{bmatrix}
   r_{s_0,a_0} & r_{s_0,a_1} \\
   r_{s_1,a_0} & r_{s_1,a_1} \\
   r_{s_2,a_0} & r_{s_2,a_1} \\
   r_{s_3,a_0} & r_{s_3,a_1} \\
   r_{s_4,a_0} & r_{s_4,a_1} \\
   \end{bmatrix}
   $$

   each of the rows corresponds to the 5 available states in the nchain
   environment, and each column corresponds to the 2 available actions in
   each state     forward and backward, 0 and 1. the value in each of these
   table cells corresponds to some measure of reward that the agent has
      learnt    occurs when they are in that state and perform that action.
   so, the value $r_{s_0,a_0}$ would be, say, the sum of the rewards that
   the agent has received when in the past they have been in state 0 and
   taken action 0. this table would then let the agent choose between
   actions based on the summated (or average, median etc.     take your
   pick) amount of reward the agent has received in the past when taking
   actions 0 or 1.

   this might be a good policy     choose the action resulting in the
   greatest previous summated reward. let   s give it a try, the code looks
   like:
def naive_sum_reward_agent(env, num_episodes=500):
    # this is the table that will hold our summated rewards for
    # each action in each state
    r_table = np.zeros((5, 2))
    for g in range(num_episodes):
        s = env.reset()
        done = false
        while not done:
            if np.sum(r_table[s, :]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with highest cummulative reward
                a = np.argmax(r_table[s, :])
            new_s, r, done, _ = env.step(a)
            r_table[s, a] += r
            s = new_s
    return r_table

   in the function definition, the environment is passed as the first
   argument, then the number of episodes (or number of games) that we will
   train the r_table on. we first create the r_table matrix which i
   presented previously and which will hold our summated rewards for each
   state and action. then there is an outer loop which cycles through the
   number of episodes. the env.reset() command starts the game afresh each
   time a new episode is commenced. it also returns the starting state of
   the game, which is stored in the variable s.

   the second, inner loop continues until a    done    signal is returned
   after an action is passed to the environment. the if statement on the
   first line of the inner loop checks to see if there are any existing
   values in the r_table for the current state     it does this by
   confirming if the sum across the row is equal to 0. if it is zero, then
   an action is chosen at random     there is no better information
   available at this stage to judge which action to take.

   this condition will only last for a short period of time. after this
   point, there will be a value stored in at least one of the actions for
   each state, and the action will be chosen based on which column value
   is the largest for the row state s. in the code, this choice of the
   maximum column is executed by the numpy argmax function     this function
   returns the index of the vector / matrix with the highest value. for
   example, if the agent is in state 0 and we have the r_table with values
   [100, 1000] for the first row, action 1 will be selected as the index
   with the highest value is column 1.

   after the action has been selected and stored in a, this action is fed
   into the environment with env.step(a). this command returns the new
   state, the reward for this action, whether the game is    done    at this
   stage and the debugging information that we are not interested in. in
   the next line, the r_table cell corresponding to state s and
   action a is updated by adding the reward to whatever is already
   existing in the table cell.

   finally the state s is updated to new_s     the new state of the agent.

   if we run this function, the r_table will look something like:

   examining the results above, you can observe that the most common state
   for the agent to be in is the first state, seeing as any action 1 will
   bring the agent back to this point. the least occupied state is state
   4, as it is difficult for the agent to progress from state 0 to 4
   without the action being    flipped    and the agent being sent back to
   state 0. you can get different results if you run the function multiple
   times, and this is because of the stochastic nature of both the
   environment and the algorithm.

   clearly     something is wrong with this table. one would expect that in
   state 4, the most rewarding action for the agent would be to choose
   action 0, which would reward the agent with 10 points, instead of the
   usual 2 points for an action of 1. not only that, but it has chosen
   action 0 for all states     this goes against intuition     surely it would
   be best to sometimes shoot for state 4 by choosing multiple action 0   s
   in a row, and that way reap the reward of multiple possible 10 scores.

   in fact, there are a number of issues with this way of doing
   id23:
     * first, once there is a reward stored in one of the columns, the
       agent will always choose that action from that point on. this will
       lead to the table being    locked in    with respect to actions after
       just a few steps in the game.
     * second, because no reward is obtained for most of the states when
       action 0 is picked, this model for training the agent has no way to
       encourage acting on delayed reward signal when it is appropriate
       for it to do so.

   let   s see how these problems could be fixed.

delayed reward id23

   if you want to be a medical doctor, you   re going to have to go through
   some pain to get there. you   ll be studying a long time before you   re
   free to practice on your own, and the rewards will be low while you are
   doing so. however, once you get to be a fully fledged md, the rewards
   will be great. during your time studying, you would be operating under
   a delayed reward or delayed gratification paradigm in order to reach
   that greater reward. however, you might only be willing to undertake
   that period of delayed reward for a given period of time     you wouldn   t
   want to be studying forever, or at least, for decades.

   we can bring these concepts into our understanding of reinforcement
   learning. let   s say we are in state 3     in the previous case, when the
   agent chose action 0 to get to state 3, the reward was zero and
   therefore r_table[3, 0] = 0. obviously the agent would not see this as
   an attractive step compared to the alternative for this state i.e.
   r_table[3, 1] >= 2. but what if we assigned to this state the reward
   the agent would received if it chose action 0 in state 4? it would look
   like this: r_table[3, 0] = r + 10 = 10     a much more attractive
   alternative!

   this idea of propagating possible reward from the best possible actions
   in future states is a core component of what is called id24. in q
   learning, the q value for each action in each state is updated when the
   relevant information is made available. the id24 rule is:

   $$q(s, a) = q(s, a) + \alpha (r + \gamma \max\limits_{a   } q(s   , a   )    
   q(s, a))$$

   first, as you can observe, this is an updating rule     the existing q
   value is added to, not replaced. ignoring the $\alpha$ for the moment,
   we can concentrate on what   s inside the brackets. the first term, r, is
   the reward that was obtained when action a was taken in state s. next,
   we have an expression which is a bit more complicated. ignore
   the $\gamma$ for the moment and focus on $\max\limits_{a   } q(s   , a   )$.
   what this means is that we look at the next state s    after action a and
   return the maximum possible q value in the next state. in other words,
   return the maximum q value for the best possible action in the next
   state. in this way, the agent is looking forward to determine the best
   possible future rewards before making the next step a.

   the $\gamma$ value is called the discounting factor     this decreases
   the impact of future rewards on the immediate decision making in
   state s. this is important, as this represents a limited patience in
   the agent     it won   t study forever to get that medical degree. so
   $\gamma$ will always be less than 1. the     q(s, a) term acts to
   restrict the growth of the q value as the training of the agent
   progresses through many iterations. finally, this whole sum is
   multiplied by a learning rate $\alpha$ which restricts the updating to
   ensure it doesn   t    race    to a solution     this is important for optimal
   convergence (see my  [21]neural networks tutorial for more on learning
   rate).

   note that while the learning rule only examines the best action in the
   following state, in reality, discounted rewards still cascade down from
   future states. for instance, if we think of the cascading rewards from
   all the 0 actions (i.e. moving forward along the chain) and start at
   state 3, the q reward will be $r + \gamma \max_a q(s   , a   ) = 0 + 0.95 *
   10 = 9.5$ (with a $\gamma$ = 0.95). if we work back from state 3 to
   state 2 it will be 0 + 0.95 * 9.5 = 9.025. likewise, the cascaded,
   discounted reward from to state 1 will be 0 + 0.95 * 9.025 = 8.57, and
   so on. therefore, while the immediate updating calculation only looks
   at the maximum q value for the next state,    upstream    rewards that have
   previously been discovered by the agent still cascade down into the
   present state and action decision. this is a simplification, due to the
   learning rate and random events in the environment, but represents the
   general idea.

   now that you (hopefully) understand id24, let   s see what it looks
   like in practice:
def q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    lr = 0.8
    for i in range(num_episodes):
        s = env.reset()
        done = false
        while not done:
            if np.sum(q_table[s,:]) == 0:
                # make a random selection of actions
                a = np.random.randint(0, 2)
            else:
                # select the action with largest q value in state s
                a = np.argmax(q_table[s, :])
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a]
)
            s = new_s
    return q_table

   this function is almost exactly the same as the previous naive r_table
   function that was discussed. the additions and changes are:
     * the variables y which specifies the discounting factor $\gamma$
       and lr which is the q table updating learning rate
     * the line:
q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])

   this line executes the id24 rule that was presented previously.
   the np.max(q_table[new_s, :]) is an easy way of selecting the maximum
   value in the q_table for the row new_s. after this function is run, an
   example q_table output is:

   this output is strange, isn   t it? again, we would expect at least the
   state 4     action 0 combination to have the highest q score, but it
   doesn   t.  we might also expect the reward from this action in this
   state to have cascaded down through the states 0 to 3. something has
   clearly gone wrong     and the answer is that there isn   t
   enough exploration going on within the agent training method.

id24 with $\epsilon$-greedy action selection

   if we think about the previous iteration of the agent training model
   using id24, the action selection policy is based solely on the
   maximum q value in any given state. it is conceivable that, given the
   random nature of the environment, that the agent initially makes    bad   
   decisions. the q values arising from these decisions may easily be
      locked in        and from that time forward, bad decisions may continue to
   be made by the agent because it can only ever select the maximum q
   value in any given state, even if these values are not necessarily
   optimal. this action selection policy is called a greedy policy.

   so we need a way for the agent to eventually always choose the    best   
   set of actions in the environment, yet at the same time allowing the
   agent to not get    locked in    and giving it some space to explore
   alternatives. what is required is the $\epsilon$-greedy policy.

   the $\epsilon$-greedy policy in id23 is basically the
   same as the greedy policy, except that there is a value $\epsilon$
   (which may be set to decay over time) where, if a random number is
   selected which is less than this value, an action is chosen completely
   at random. this step allows some random exploration of the value of
   various actions in various states, and can be scaled back over time to
   allow the algorithm to concentrate more on exploiting the best
   strategies that it has found. this mechanism can be expressed in code
   as:
def eps_greedy_q_learning_with_table(env, num_episodes=500):
    q_table = np.zeros((5, 2))
    y = 0.95
    eps = 0.5
    lr = 0.8
    decay_factor = 0.999
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        done = false
        while not done:
            # select the action with highest cummulative reward
            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])
            # pdb.set_trace()
            new_s, r, done, _ = env.step(a)
            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s
, a])
            s = new_s
    return q_table

   this code shows the introduction of the $\epsilon$ value     eps. there
   is also an associated eps decay_factor which exponentially decays eps
   with each episode eps *= decay_factor. the $\epsilon$-greedy based
   action selection can be found in this code:
            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(q_table[s, :])

   the first component of the if statement shows a random number being
   selected, between 0 and 1, and determining if this is below eps. if so,
   the action will be selected randomly from the two possible actions in
   each state. the second part of the if statement is a random selection
   if there are no values stored in the q_table so far. if neither of
   these conditions hold true, the action is selected as per normal by
   taking the action with the highest q value.

   the rest of the code is the same as the standard greedy implementation
   with id24 discussed previously. this code produces a q_table
   which looks something like the following:

   finally we have a table which favors action 0 in state 4     in other
   words what we would expect to happen given the reward of 10 that is up
   for grabs via that action in that state. notice also that, as opposed
   to the previous tables from the other methods, that there are no
   actions with a 0 q value     this is because the full action space has
   been explored via the randomness introduced by the $\epsilon$-greedy
   policy.

comparing the methods

   let   s see if the last agent training model actually produces an agent
   that gathers the most rewards in any given game. the code below shows
   the three models trained and then tested over 100 iterations to see
   which agent performs the best over a test game. the models are trained
   as well as tested in each iteration because there is significant
   variability in the environment which messes around with the efficacy of
   the training     so this is an attempt to understand average performance
   of the different models. the main testing code looks like:
def test_methods(env, num_iterations=100):
    winner = np.zeros((3,))
    for g in range(num_iterations):
        m0_table = naive_sum_reward_agent(env, 500)
        m1_table = q_learning_with_table(env, 500)
        m2_table = eps_greedy_q_learning_with_table(env, 500)
        m0 = run_game(m0_table, env)
        m1 = run_game(m1_table, env)
        m2 = run_game(m2_table, env)
        w = np.argmax(np.array([m0, m1, m2]))
        winner[w] += 1
        print("game {} of {}".format(g + 1, num_iterations))
    return winner

   first, this method creates a numpy zeros array of length 3 to hold the
   results of the winner in each iteration     the winning method is the
   method that returns the highest rewards after training and playing. the
   run_game function looks like:
def run_game(table, env):
    s = env.reset()
    tot_reward = 0
    done = false
    while not done:
        a = np.argmax(table[s, :])
        s, r, done, _ = env.step(a)
        tot_reward += r
    return tot_reward

   here, it can be observed that the trained table given to the function
   is used for action selection, and the total reward accumulated during
   the game is returned. a sample outcome from this experiment (i.e. the
   vector w) is shown below:
   [13, 22, 65]


   as can be observed, of the 100 experiments the $\epsilon$-greedy, q
   learning algorithm (i.e. the third model that was presented) wins 65 of
   them. this is followed by the standard greedy implementation of q
   learning, which won 22 of the experiments. finally the naive
   accumulated rewards method only won 13 experiments. so as can be seen,
   the $\epsilon$-greedy id24 method is quite an effective way of
   executing id23.

   so far, we have been dealing with explicit tables to hold information
   about the best actions and which actions to choose in any given state.
   however, while this is perfectly reasonable for a small environment
   like nchain, the table gets far too large and unwieldy for more
   complicated environments which have a huge number of states and
   potential actions.

   this is where neural networks can be used in id23.
   instead of having explicit tables, instead we can train a neural
   network to predict q values for each action in a given state. this will
   be demonstrated using [22]keras in the next section.

id23 with keras

   to develop a neural network which can perform id24, the input
   needs to be the current state (plus potentially some other information
   about the environment) and it needs to output the relevant q values for
   each action in that state. the q values which are output should
   approach, as training progresses, the values produced in the id24
   updating rule. therefore, the loss or cost function for the neural
   network should be:

   $$\text{loss} = (\underbrace{r + \gamma \max_{a   } q'(s   ,
   a   )}_{\text{target}}     \underbrace{q(s, a)}_{\text{prediction}})^2$$

   the id23 architecture that we are going to build in
   keras is shown below:
   id23 python keras - architecture

   id23 keras architecture

   the input to the network is the one-hot encoded state vector. for
   instance, the vector which corresponds to state 1 is [0, 1, 0, 0, 0]
   and state 3 is [0, 0, 0, 1, 0]. in this case, a hidden layer of 10
   nodes with sigmoid activation will be used. the output layer is a
   linear activated set of two nodes, corresponding to the two q values
   assigned to each state to represent the two possible actions. linear
   activation means that the output depends only on the linear summation
   of the inputs and the weights, with no additional function applied to
   that summation. for more on neural networks, check out my
   [23]comprehensive neural network tutorial.

   building this network is easy in keras     to learn more about how to use
   keras, check out [24]my tutorial. the code below shows how it can be
   done in a few lines:
model = sequential()
model.add(inputlayer(batch_input_shape=(1, 5)))
model.add(dense(10, activation='sigmoid'))
model.add(dense(2, activation='linear'))
model.compile(loss='mse', optimizer='adam', metrics=['mae'])

   first, the model is created using the keras sequential api. then an
   input layer is added which takes inputs corresponding to the one-hot
   encoded state vectors. then the sigmoid activated hidden layer with 10
   nodes is added, followed by the linear activated output layer which
   will yield the q values for each action. finally the model is compiled
   using a mean-squared error id168 (to correspond with the loss
   function defined previously) with the [25]adam optimizer being used in
   its default keras state.

   to use this model in the training environment, the following code is
   run which is similar to the previous $\epsilon$-greedy id24
   methodology with an explicit q table:
    # now execute the id24
    y = 0.95
    eps = 0.5
    decay_factor = 0.999
    r_avg_list = []
    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        if i % 100 == 0:
            print("episode {} of {}".format(i + 1, num_episodes))
        done = false
        r_sum = 0
        while not done:
            if np.random.random() < eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))
            new_s, r, done, _ = env.step(a)
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1
]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs
=1, verbose=0)
            s = new_s
            r_sum += r
        r_avg_list.append(r_sum / 1000)

   the first major difference in the keras implementation is the following
   code:
            if np.random.random() < eps:
                a = np.random.randint(0, 2)
            else:
                a = np.argmax(model.predict(np.identity(5)[s:s + 1]))

   the first condition in the if statement is the implementation of the
   $\epsilon$-greedy action selection policy that has been discussed
   already. the second condition uses the keras model to produce the two q
   values     one for each possible state. it does this by calling the
   model.predict() function. here the numpy identity function is used,
   with vector slicing, to produce the one-hot encoding of the current
   state s. the standard numpy argmax function is used to select the
   action with the highest q value returned from the keras model
   prediction.

   the second major difference is the following four lines:
            target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1
]))
            target_vec = model.predict(np.identity(5)[s:s + 1])[0]
            target_vec[a] = target
            model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs
=1, verbose=0)

   the first line sets the target as the id24 updating rule that has
   been previously presented. it is the reward r plus the discounted
   maximum of the predicted q values for the new state, new_s. this is the
   value that we want the keras model to learn to predict for state s and
   action a i.e. q(s,a). however, our keras model has an output for each
   of the two actions     we don   t want to alter the value for the other
   action, only the action a which has been chosen. so on the next
   line, target_vec is created which extracts both predicted q values for
   state s. on the following line, only the q value corresponding to the
   action a is changed to target     the other action   s q value is left
   untouched.

   the final line is where the keras model is updated in a single training
   step. the first argument is the current state     i.e. the one-hot
   encoded input to the model. the second is our target vector which is
   reshaped to make it have the required dimensions of (1, 2). the third
   argument tells the fit function that we only want to train for a single
   iteration and finally the verbose flag simply tells keras not to print
   out the training progress.

   running this training over 1000 game episodes reveals the following
   average reward for each step in the game:
   id23 python keras - training improvement in reward

   id23 in keras     average reward improvement over
   number of episodes trained

   as can be observed, the average reward per step in the game increases
   over each game episode, showing that the keras model is learning well
   (if a little slowly).

   we can also run the following code to get an output of the q values for
   each of the states     this is basically getting the keras model to
   reproduce our explicit q table that was generated in previous methods:

   state 0     action [[62.734287 61.350456]]

   state 1     action [[66.317955 62.27209 ]]

   state 2     action [[70.82501 63.262383]]

   state 3     action [[76.63797 64.75874]]

   state 4     action [[84.51073 66.499725]]

   this output looks sensible     we can see that the q values for each
   state will favor choosing action 0 (moving forward) to shoot for those
   big, repeated rewards in state 4. intuitively, this seems like the best
   strategy.

   so there you have it     you should now be able to understand some basic
   concepts in id23, and understand how to build q
   learning models in keras. this is just scraping the surface of
   id23, so stay tuned for future posts on this topic
   (or check out the recommended course below) where more interesting
   games are played!
     __________________________________________________________________

   recommended online course     if you   re more of a video based learner,
   i   d recommend the following inexpensive udemy online course in
   id23: [26]artificial intelligence: reinforcement
   learning in python
   [show?id=jbc0n5zkdzk&amp;bids=323058.1080408&amp;type=2&amp;subid=0]
     __________________________________________________________________


about the author

     clemens tolboom says:
   [27]may 26, 2018 at 4:08 pm

   thanks andy for this comprehensive rl tutorial. the parts read from
      id23: an introduction    from sutton and barto got
   some substance now     

     steve nguyen says:
   [28]june 26, 2018 at 1:33 pm

   thanks fortune. your article worth a lot more than all of lessons i
   have paid (or freely attended on-line) combined together. now i can
   move on strongly with advanced ones. thank you and please keep writing
   such great articles.

   regards,
   steve

     ilya says:
   [29]june 29, 2018 at 6:48 pm

   thank you for the amazing tutorial

     ilya says:
   [30]june 29, 2018 at 6:49 pm

   thank you for the amazing tutorial! this is very helpful.
     * andy says:
       [31]june 29, 2018 at 8:00 pm
       you   re welcome ilya, glad it was helpful

     greeny says:
   [32]july 4, 2018 at 10:05 am

   it is a great tutorial. thank you so much.
     * andy says:
       [33]july 4, 2018 at 11:29 am
       you   re welcome, glad it was useful for you

     nenad says:
   [34]july 26, 2018 at 10:26 pm

   thank you for this tutorial. it is a great introduction for rl.

   i really enjoyed the progression. first you showed the importance of
   exploration and then delved into incorporating keras.

   good job and keep up the good work!
     * andy says:
       [35]july 26, 2018 at 11:51 pm
       thanks nenad, glad it was useful

     oswaldo castro says:
   [36]august 22, 2018 at 5:28 pm

   andy, really impressive tutorial   
   i   m taking the course on udemy as cited on your recomendation. it was
   great too but your article is fantastic in giving the high (and middle)
   level concepts necessary to understand rl. it is simply an obrigatory
   read to take off on this subject.
   really thanks
     * andy says:
       [37]august 22, 2018 at 8:15 pm
       you   re welcome oswaldo, thanks for the feedback and i   m really glad
       it was a help

     saigayatri says:
   [38]september 4, 2018 at 12:05 pm

   a great tutorial for beginners!! thanks for writing



   neo says:
   [39]september 8, 2018 at 6:43 am

   so good. thanks

     * andy says:
       [40]september 8, 2018 at 6:45 am
       your welcome neo, glad it was a help



   elias says:
   [41]october 25, 2018 at 11:35 am

   yeah i have to chip in, great tutorial! you know, not everyone has the
   gift to explain principles and basics in a way so that the concept is
   conveyed and the student gets slightly booted and prepared for more
   advanced stuff! cudos to you! +++

     * andy says:
       [42]october 25, 2018 at 11:42 am
       thanks elias, appreciate the feedback. i   m glad it was useful for
       you



   miguel says:
   [43]january 21, 2019 at 7:55 pm

   i   ve seen multiple tutorials on the topic and by far this was the one
   which explained it in the most understandable way, by showing the steps
   and where the nn go into the topic. thank you for your work

     * andy says:
       [44]january 21, 2019 at 7:58 pm
       thanks miguel, glad you found it useful

   ____________________ (button)

   recent posts
     * [45]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [46]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [47]keras, eager and tensorflow 2.0     a new tf paradigm
     * [48]introduction to tensorboard and tensorflow visualization
     * [49]tensorflow eager tutorial

   recent comments
     * andry on [50]neural networks tutorial     a pathway to deep learning
     * sandipan on [51]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [52]neural networks tutorial     a pathway to deep learning
     * martin on [53]neural networks tutorial     a pathway to deep learning
     * uri on [54]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [55]march 2019
     * [56]january 2019
     * [57]october 2018
     * [58]september 2018
     * [59]august 2018
     * [60]july 2018
     * [61]june 2018
     * [62]may 2018
     * [63]april 2018
     * [64]march 2018
     * [65]february 2018
     * [66]november 2017
     * [67]october 2017
     * [68]september 2017
     * [69]august 2017
     * [70]july 2017
     * [71]may 2017
     * [72]april 2017
     * [73]march 2017

   categories
     * [74]amazon aws
     * [75]cntk
     * [76]convolutional neural networks
     * [77]cross id178
     * [78]deep learning
     * [79]gensim
     * [80]gpus
     * [81]keras
     * [82]id168s
     * [83]lstms
     * [84]neural networks
     * [85]nlp
     * [86]optimisation
     * [87]pytorch
     * [88]recurrent neural networks
     * [89]id23
     * [90]tensorboard
     * [91]tensorflow
     * [92]tensorflow 2.0
     * [93]weight initialization
     * [94]id97

   meta
     * [95]log in
     * [96]entries rss
     * [97]comments rss
     * [98]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [99]thrive themes | powered by [100]wordpress

   (button) close dialog

   session expired

   [101]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[102]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  13. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
  14. http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments
  15. https://en.wikipedia.org/wiki/alphago
  16. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  17. https://github.com/adventuresinml/adventures-in-ml-code
  18. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.1080408&type=2&murl=https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python/
  19. https://gym.openai.com/envs/nchain-v0/
  20. https://gym.openai.com/docs/
  21. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  22. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  23. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  24. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  25. https://arxiv.org/abs/1412.6980
  26. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.1080408&type=2&murl=https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python/
  27. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5162
  28. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5163
  29. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5164
  30. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5165
  31. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5166
  32. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5167
  33. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5168
  34. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5176
  35. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5177
  36. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5178
  37. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5179
  38. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5181
  39. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5182
  40. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5183
  41. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5186
  42. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5187
  43. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5202
  44. https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/#comments/5203
  45. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  46. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  47. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  48. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  49. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  50. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  51. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  52. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  53. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  54. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  55. https://adventuresinmachinelearning.com/2019/03/
  56. https://adventuresinmachinelearning.com/2019/01/
  57. https://adventuresinmachinelearning.com/2018/10/
  58. https://adventuresinmachinelearning.com/2018/09/
  59. https://adventuresinmachinelearning.com/2018/08/
  60. https://adventuresinmachinelearning.com/2018/07/
  61. https://adventuresinmachinelearning.com/2018/06/
  62. https://adventuresinmachinelearning.com/2018/05/
  63. https://adventuresinmachinelearning.com/2018/04/
  64. https://adventuresinmachinelearning.com/2018/03/
  65. https://adventuresinmachinelearning.com/2018/02/
  66. https://adventuresinmachinelearning.com/2017/11/
  67. https://adventuresinmachinelearning.com/2017/10/
  68. https://adventuresinmachinelearning.com/2017/09/
  69. https://adventuresinmachinelearning.com/2017/08/
  70. https://adventuresinmachinelearning.com/2017/07/
  71. https://adventuresinmachinelearning.com/2017/05/
  72. https://adventuresinmachinelearning.com/2017/04/
  73. https://adventuresinmachinelearning.com/2017/03/
  74. https://adventuresinmachinelearning.com/category/amazon-aws/
  75. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  76. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  77. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  78. https://adventuresinmachinelearning.com/category/deep-learning/
  79. https://adventuresinmachinelearning.com/category/nlp/gensim/
  80. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  81. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  82. https://adventuresinmachinelearning.com/category/loss-functions/
  83. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  84. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  85. https://adventuresinmachinelearning.com/category/nlp/
  86. https://adventuresinmachinelearning.com/category/optimisation/
  87. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  88. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  89. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  90. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  91. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  92. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  93. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  94. https://adventuresinmachinelearning.com/category/nlp/id97/
  95. https://adventuresinmachinelearning.com/wp-login.php
  96. https://adventuresinmachinelearning.com/feed/
  97. https://adventuresinmachinelearning.com/comments/feed/
  98. https://wordpress.org/
  99. https://www.thrivethemes.com/
 100. http://www.wordpress.org/
 101. https://adventuresinmachinelearning.com/wp-login.php
 102. http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/

   hidden links:
 104. https://adventuresinmachinelearning.com/author/admin/
