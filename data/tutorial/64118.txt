   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [0*l3b0nwqwp4yazyu1.]

id115 in python

   [16]go to the profile of will koehrsen
   [17]will koehrsen (button) blockedunblock (button) followfollowing
   feb 9, 2018

   a complete real-world implementation

   the past few months, i encountered one term again and again in the data
   science world: id115. in my research lab, in
   podcasts, in articles, every time i heard the phrase i would nod and
   think that sounds pretty cool with only a vague idea of what anyone was
   talking about. several times i tried to learn mcmc and bayesian
   id136, but every time i started reading the books, i soon gave up.
   exasperated, i turned to the best method to learn any new skill: apply
   it to a problem.

   using some of my sleep data i had been meaning to explore and a
   hands-on application-based book (bayesian methods for hackers,
   [18]available free online), i finally learned id115
   through a real-world project. as usual, it was much easier (and more
   enjoyable) to understand the technical concepts when i applied them to
   a problem rather than reading them as abstract ideas on a page. this
   article walks through the introductory implementation of markov chain
   monte carlo in python that finally taught me this powerful modeling and
   analysis tool.

   the [19]full code and data for this project is on github. i encourage
   anyone to take a look and use it on their own data. this article
   focuses on applications and results, so there are a lot of topics
   covered at a high level, but i have tried to provide links for those
   wanting to learn more!

introduction

   my garmin vivosmart watch tracks when i fall asleep and wake up based
   on heart rate and motion. it   s not 100% accurate, but real-world data
   is never perfect, and we can still extract useful knowledge from noisy
   data with the right model!
   [1*_v59tsixl9hrqmj4z7otxq.png]
   typical sleep data

   the objective of this project was to use the sleep data to create a
   model that specifies the posterior id203 of sleep as a function
   of time. as time is a continuous variable, specifying the entire
   posterior distribution is intractable, and we turn to methods to
   approximate a distribution, such as id115 (mcmc).

choosing a id203 distribution

   before we can start with mcmc, we need to determine an appropriate
   function for modeling the posterior id203 distribution of sleep.
   one simple way to do this is to visually inspect the data. the
   observations for when i fall asleep as a function of time are shown
   below.
   [1*x-cm51t-ob-iddyg2poppg.png]
   sleeping data

   every data point is represented as a dot, with the intensity of the dot
   showing the number of observations at the specific time. my watch
   records only the minute at which i fall asleep, so to expand the data,
   i added points to every minute on both sides of the precise time. if my
   watch says i fell asleep at 10:05 pm, then every minute before is
   represented as a 0 (awake) and every minute after gets a 1 (asleep).
   this expanded the roughly 60 nights of observations into 11340 data
   points.

   we can see that i tend to fall asleep a little after 10:00 pm but we
   want to create a model that captures the transition from awake to
   asleep in terms of a id203. we could use a simple step function
   for our model that changes from awake (0) to asleep (1) at one precise
   time, but this would not represent the uncertainty in the data. i do
   not go to sleep at the same time every night, and we need a function to
   that models the transition as a gradual process to show the
   variability. the best choice given the data is a logistic function
   which is smoothly transitions between the bounds of 0 and 1. following
   is a logistic equation for the id203 of sleep as a function of
   time
   [1*vhrer2bx6uyzk3gmgmxmag.png]

   here,    (beta) and    (alpha) are the parameters of the model that we
   must learn during mcmc. a logistic function with varying parameters is
   shown below.
   [1*se70kyjxwlie7udnwapq6w.png]

   a logistic function fits the data because the id203 of being
   asleep transitions gradually, capturing the variability in my sleep
   patterns. we want to be able to plug in a time t to the function and
   get out the id203 of sleep, which must be between 0 and 1. rather
   than a straight yes or no answer to the question am i asleep at 10:00
   pm, we can get a id203. to create this model, we use the data to
   find the best alpha and beta parameters through one of the techniques
   classified as id115.

id115

   [20]id115 refers to a class of methods for sampling
   from a id203 distribution in order to construct the most likely
   distribution. we cannot directly calculate the logistic distribution,
   so instead we generate thousands of values         called samples         for the
   parameters of the function (alpha and beta) to create an approximation
   of the distribution. the idea behind mcmc is that as we generate more
   samples, our approximation gets closer and closer to the actual true
   distribution.

   there are two parts to a id115 method. [21]monte
   carlo refers to a general technique of using repeated random samples to
   obtain a numerical answer. monte carlo can be thought of as carrying
   out many experiments, each time changing the variables in a model and
   observing the response. by choosing random values, we can explore a
   large portion of the parameter space, the range of possible values for
   the variables. a parameter space for our problem using normal priors
   for the variables (more on this in a moment) is shown below.
   [1*mqwbinb8ayxgpgwhxca3lq.png]

   clearly we cannot try every single point in these plots, but by
   randomly sampling from regions of higher id203 (red) we can
   create the most likely model for our problem.

markov chain

   a [22]markov chain is a process where the next state depends only on
   the current state. (a state in this context refers to the assignment of
   values to the parameters). a markov chain is memoryless because only
   the current state matters and not how it arrived in that state. if
   that   s a little difficult to understand, consider an everyday
   phenomenon, the weather. if we want to predict the weather tomorrow we
   can get a reasonable estimate using only the weather today. if it
   snowed today, we look at historical data showing the distribution of
   weather on the day after it snows to estimate probabilities of the
   weather tomorrow. the concept of a markov chain is that we do not need
   to know the entire history of a process to predict the next output, an
   approximation that works well in many real-world situations.

   putting together the ideas of markov chain and monte carlo, mcmc is a
   method that repeatedly draws random values for the parameters of a
   distribution based on the current values. each sample of values is
   random, but the choices for the values are limited by the current state
   and the assumed prior distribution of the parameters. mcmc can be
   considered as a random walk that gradually converges to the true
   distribution.

   in order to draw random values of alpha and beta, we need to assume a
   prior distribution for these values. as we have no assumptions about
   the parameters ahead of time, we can use a normal distribution. the
   normal, or gaussian distribution, is defined by the mean, showing the
   location of the data, and the variance, showing the spread. several
   normal distributions with different means and spreads are below:
   [1*1kqmne_e7ysdpjkilu3cpg.png]

   the specific mcmc algorithm we are using is called [23]metropolis
   hastings. in order to connect our observed data to the model, every
   time a set of random values are drawn, the algorithm evaluates them
   against the data. if they do not agree with the data (i   m simplifying a
   little here), the values are rejected and the model remains in the
   current state. if the random values are in agreement with the data, the
   values are assigned to the parameters and become the current state.
   this process continues for a specified number of steps, with the
   accuracy of the model improving with the number of steps.

   putting it all together, the basic procedure for markov chain monte
   carlo in our problem is as follows:
    1. select an initial set of values for alpha and beta, the parameters
       of the logistic function.
    2. randomly assign new values to alpha and beta based on the current
       state.
    3. check if the new random values agree with the observations. if they
       do not, reject the values and return to the previous state. if they
       do, accept the values as the new current state.
    4. repeat steps 2 and 3 for the specified number of iterations.

   the algorithm returns all of the values it generates for alpha and
   beta. we can then use the average of these values as the most likely
   final values for alpha and beta in the logistic function. mcmc cannot
   return the    true    value but rather an approximation for the
   distribution. the final model for the id203 of sleep given the
   data will be the logistic function with the average values of alpha and
   beta.

python implementation

   the above details went over my head many times until i applied them in
   python! seeing the results first-hand is a lot more helpful than
   reading someone else describe. to implement mcmc in python, we will use
   the [24]pymc3 bayesian id136 library. it abstracts away most of the
   details, allowing us to create models without getting lost in the
   theory.

   the following code creates the full model with the parameters, alpha
   and beta, the id203, p, and the observations, observed the step
   variable refers to the specific algorithm, and the sleep_trace holds
   all of the values of the parameters generated by the model.

   iframe: [25]/media/a842af6681fbf27b151e6e0b27caf12f?postid=44f7e609be98

   (check out the notebook for the full code)

   to get a sense of what occurs when we run this code, we can look at all
   the value of alpha and beta generated during the model run.
   [1*nonqehwdksfqb5_4xfuzla.png]

   these are called trace plots. we can see that each state is correlated
   to the previous         the markov chain         but the values oscillate
   significantly         the monte carlo sampling.

   in mcmc, it is common practice to discard up to 90% of the trace. the
   algorithm does not immediately converge to the true distribution and
   the initial values are often inaccurate. the later values for the
   parameters are generally better which means they are what we should use
   for building our model. we used 10000 samples and discarded the first
   50%, but an industry application would likely use hundreds of thousands
   or millions of samples.

   mcmc converges to the true value given enough steps, but assessing
   convergence can be difficult. i will leave that topic out of this post
   (one way is by measuring the [26]auto-correlation of the traces) but it
   is an important consideration if we want the most accurate results.
   pymc3 has built in functions for assessing the quality of models,
   including trace and autocorrelation plots.
pm.traceplot(sleep_trace, ['alpha', 'beta'])
pm.autocorrplot(sleep_trace, ['alpha', 'beta'])

   [1*i3_lpzoxqptsveq6ifhmbg.png]
   [1*bywap4xuhpladdheacduow.png]
   trace (left) and autocorrelation (right) plots

sleep model

   after finally building and running the model, it   s time to use the
   results. we will the the average of the last 5000 alpha and beta
   samples as the most likely values for the parameters which allows us to
   create a single curve modeling the posterior sleep id203:
   [1*bztuqaxir5f6n_7sc-a2ow.png]

   the model represents the data well. moreover, it captures the inherent
   variability in my sleep patterns. rather than a single yes or no
   answer, the model gives us a id203. for example, we can query the
   model to find out the id203 i am asleep at a given time and find
   the time at which the id203 of being asleep passes 50%:
9:30  pm id203 of being asleep: 4.80%.
10:00 pm id203 of being asleep: 27.44%.
10:30 pm id203 of being asleep: 73.91%.
the id203 of sleep increases to above 50% at 10:14 pm.

   although i try to go to bed at 10:00 pm, that clearly does not happen
   most nights! we can see that the average time i go to bed is around
   10:14 pm.

   these values are the most likely estimates given the data. however,
   there is uncertainty associated with these probabilities because the
   model is approximate. to represent this uncertainty, we can make
   predictions of the sleep id203 at a given time using all of the
   alpha and beta samples instead of the average and then plot a histogram
   of the results.
   [1*e8effkyqccthheofzc6ula.png]
   [1*5ecofhqbamosfrsg_7xcgg.png]

   these results give a better indicator of what an mcmc model really
   does. the method does not find a single answer, but rather a sample of
   possible values. bayesian id136 is useful in the real-world because
   it expresses predictions in terms of probabilities. we can say there is
   one most likely answer, but the more accurate response is that there
   are a range of values for any prediction.

wake model

   i can use the waking data to find a similar model for when i wake up in
   the morning. i try to always be up at 6:00 am with my alarm, but we can
   see that does not always happen! the following image shows the final
   model for the transition from sleeping to waking along with the
   observations.
   [1*innfl-fllxd6zvdy_x0jma.png]

   we can query the model to find the id203 i   m asleep at a given
   time and the most likely time for me to wake up.
id203 of being awake at 5:30 am: 14.10%.
id203 of being awake at 6:00 am: 37.94%.
id203 of being awake at 6:30 am: 69.49%.
the id203 of being awake passes 50% at 6:11 am.

   looks like i have some work to do with that alarm!

duration of sleep

   a final model i wanted to create         both out of curiosity and for the
   practice         was my duration of sleep. first, we need to find a function
   to model the distribution of the data. ahead of time, i think it would
   be normal, but we can only find out by examining the data!
   [1*mj1i2z2aotdj1yk_3bzosg.png]

   a normal distribution would work, but it would not capture the outlying
   points on the right side (times when i severely slept in). we could use
   two separate normal distributions to represent the two modes, but
   instead, i will use a skewed normal. the skewed normal has three
   parameters, the mean, the variance, and alpha, the skew. all three of
   these must be learned from the mcmc algorithm. the following code
   creates the model and implements the metropolis hastings sampling.

   iframe: [27]/media/904ab2bf8b050d8c8e094bd395e94bc4?postid=44f7e609be98

   now, we can use the average values of the three parameters to construct
   the most likely distribution. following is the final skewed normal
   distribution on top of the data.
   [1*bjig1nhlcvdnsm12txjvcq.png]

   it looks like a nice fit! we can query the model to find the likelihood
   i get at least a certain amount of sleep and the most likely duration
   of sleep:
id203 of at least 6.5 hours of sleep = 99.16%.
id203 of at least 8.0 hours of sleep = 44.53%.
id203 of at least 9.0 hours of sleep = 10.94%.
the most likely duration of sleep is 7.67 hours.

   i   m not entirely pleased with those results, but what can you expect as
   a graduate student?

conclusions

   once again, completing this project showed me the [28]importance of
   solving problems, preferably ones with real world applications! along
   the way to building an end-to-end implementation of bayesian id136
   using id115, i picked up many of the fundamentals
   and enjoyed myself in the process. not only did i learn a little bit
   about my habits (and what i need to improve), but now i can finally
   understand what everyone is talking about when they say mcmc and
   bayesian id136. data science is about constantly adding tools to
   your repertoire and the most effective way to do that is to find a
   problem and get started!

   as always, i welcome feedback and constructive criticism. i can be
   reached on twitter [29]@koehrsen_will.

     * [30]machine learning
     * [31]data science
     * [32]programming
     * [33]python
     * [34]education

   (button)
   (button)
   (button) 5.6k claps
   (button) (button) (button) 25 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of will koehrsen

[36]will koehrsen

   data scientist at cortex intel, data science communicator

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 5.6k
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/44f7e609be98
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_jbuxcoh86dmj---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@williamkoehrsen?source=post_header_lockup
  17. https://towardsdatascience.com/@williamkoehrsen
  18. https://github.com/camdavidsonpilon/probabilistic-programming-and-bayesian-methods-for-hackers
  19. https://github.com/willkoehrsen/ai-projects/blob/master/markov_chain_monte_carlo/markov_chain_monte_carlo.ipynb
  20. https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/
  21. http://statweb.stanford.edu/~owen/mc/
  22. https://brilliant.org/wiki/markov-chains/
  23. http://www.mit.edu/~ilkery/papers/metropolishastingssampling.pdf
  24. http://docs.pymc.io/notebooks/getting_started.html
  25. https://towardsdatascience.com/media/a842af6681fbf27b151e6e0b27caf12f?postid=44f7e609be98
  26. http://www.itl.nist.gov/div898/handbook/eda/section3/eda35c.htm
  27. https://towardsdatascience.com/media/904ab2bf8b050d8c8e094bd395e94bc4?postid=44f7e609be98
  28. https://towardsdatascience.com/how-to-master-new-skills-656d42d0e09c
  29. https://twitter.com/koehrsen_will
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/data-science?source=post
  32. https://towardsdatascience.com/tagged/programming?source=post
  33. https://towardsdatascience.com/tagged/python?source=post
  34. https://towardsdatascience.com/tagged/education?source=post
  35. https://towardsdatascience.com/@williamkoehrsen?source=footer_card
  36. https://towardsdatascience.com/@williamkoehrsen
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/44f7e609be98/share/twitter
  43. https://medium.com/p/44f7e609be98/share/facebook
  44. https://medium.com/p/44f7e609be98/share/twitter
  45. https://medium.com/p/44f7e609be98/share/facebook
