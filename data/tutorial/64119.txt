   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

introduction to markov chains

what are markov chains, when to use them, and how they work

   [16]go to the profile of devin soni
   [17]devin soni (button) blockedunblock (button) followfollowing
   mar 5, 2018
   [1*jcbuf7dahairs8nfulntow.gif]
   (generated from [18]http://setosa.io/ev/markov-chains/)

   markov chains are a fairly common, and relatively simple, way to
   statistically model random processes. they have been used in many
   different domains, ranging from text generation to financial modeling.
   a popular example is [19]r/subredditsimulator, which uses markov chains
   to automate the creation of content for an entire subreddit. overall,
   markov chains are conceptually quite intuitive, and are very accessible
   in that they can be implemented without the use of any advanced
   statistical or mathematical concepts. they are a great way to start
   learning about probabilistic modeling and data science techniques.

scenario

   to begin, i will describe them with a very common example:
imagine that there were two possible states for weather: sunny or cloudy. you ca
n always directly observe the current weather state, and it is guaranteed to alw
ays be one of the two aforementioned states.
now, you decide you want to be able to predict what the weather will be like tom
orrow. intuitively, you assume that there is an inherent transition in this proc
ess, in that the current weather has some bearing on what the next day   s weather
 will be. so, being the dedicated person that you are, you collect weather data
over several years, and calculate that the chance of a sunny day occurring after
 a cloudy day is 0.25. you also note that, by extension, the chance of a cloudy
day occurring after a cloudy day must be 0.75, since there are only two possible
 states.
you can now use this distribution to predict weather for days to come, based on
what the current weather state is at the time.

   this example illustrates many of the key concepts of a markov chain. a
   markov chain essentially consists of a set of transitions, which are
   determined by some id203 distribution, that satisfy the markov
   property.

   observe how in the example, the id203 distribution is obtained
   solely by observing transitions from the current day to the next. this
   illustrates the markov property, the unique characteristic of markov
   processes that renders them memoryless. this typically leaves them
   unable to successfully produce sequences in which some underlying trend
   would be expected to occur. for example, while a markov chain may be
   able to mimic the writing style of an author based on word frequencies,
   it would be unable to produce text that contains deep meaning or
   thematic significance since these are developed over much longer
   sequences of text. they therefore lack the ability to produce
   context-dependent content since they cannot take into account the full
   chain of prior states.
   [1*frksgjinf5otjx7wl81u3w.png]
   a visualization of the weather example

the model

   formally, a markov chain is a probabilistic automaton. the id203
   distribution of state transitions is typically represented as the
   markov chain   s transition matrix. if the markov chain has n possible
   states, the matrix will be an n x n matrix, such that entry (i, j) is
   the id203 of transitioning from state i to state j. additionally,
   the transition matrix must be a stochastic matrix, a matrix whose
   entries in each row must add up to exactly 1. this makes complete
   sense, since each row represents its own id203 distribution.
   [1*oqbd8elkxyu-h0ahv-m73w.png]
   general view of a sample markov chain, with states as circles, and
   edges as transitions
   [1*laz0aaq84xgcwz9d9ux0wg.png]
   sample transition matrix with 3 possible states

   additionally, a markov chain also has an initial state vector,
   represented as an n x 1 matrix (a vector), that describes the
   id203 distribution of starting at each of the n possible states.
   entry i of the vector describes the id203 of the chain beginning
   at state i.
   [1*y8zkbhqm4d9rf3bl-ok9_w.gif]
   initial state vector with 4 possible states

   these two entities are typically all that is needed to represent a
   markov chain.

   we now know how to obtain the chance of transitioning from one state to
   another, but how about finding the chance of that transition occurring
   over multiple steps? to formalize this, we now want to determine the
   id203 of moving from state i to state j over m steps. as it turns
   out, this is actually very simple to find out. given a transition
   matrix p, this can be determined by calculating the value of entry (i,
   j) of the matrix obtained by raising p to the power of m. for small
   values of m, this can easily be done by hand with repeated
   multiplication. however, for large values of m, if you are familiar
   with simple id202, a more efficient way to raise a matrix to a
   power is to first diagonalize the matrix.

conclusion

   now that you know the basics of markov chains, you should now be able
   to easily implement them in a language of your choice. if coding is not
   your forte, there are also many more advanced properties of markov
   chains and markov processes to dive into. in my opinion, the natural
   progression along the theory route would be toward hidden markov
   processes or mcmc. simple markov chains are the building blocks of
   other, more sophisticated, modeling techniques, so with this knowledge,
   you can now move onto various techniques within topics such as belief
   modeling and sampling.
     __________________________________________________________________

   make sure you give this post 50 claps and my blog a follow if you
   enjoyed this post and want to see more!

     * [20]machine learning
     * [21]artificial intelligence
     * [22]data science
     * [23]statistics
     * [24]towards data science

   (button)
   (button)
   (button) 18.9k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of devin soni

[26]devin soni

   all opinions are my own     crypto markets, data science     twitter
   [27]@devin_soni     website [28]https://100.github.io/

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 18.9k
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/50da3645a50d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_zyuv4qp2fbeg---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@devins?source=post_header_lockup
  17. https://towardsdatascience.com/@devins
  18. http://setosa.io/ev/markov-chains/
  19. https://www.reddit.com/r/subredditsimulator/
  20. https://towardsdatascience.com/tagged/machine-learning?source=post
  21. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  22. https://towardsdatascience.com/tagged/data-science?source=post
  23. https://towardsdatascience.com/tagged/statistics?source=post
  24. https://towardsdatascience.com/tagged/towards-data-science?source=post
  25. https://towardsdatascience.com/@devins?source=footer_card
  26. https://towardsdatascience.com/@devins
  27. http://twitter.com/devin_soni
  28. https://100.github.io/
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/50da3645a50d/share/twitter
  35. https://medium.com/p/50da3645a50d/share/facebook
  36. https://medium.com/p/50da3645a50d/share/twitter
  37. https://medium.com/p/50da3645a50d/share/facebook
