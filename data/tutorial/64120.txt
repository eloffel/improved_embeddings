   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning from scratch: part 1

attributes and patterns

   [16]go to the profile of sebastian kwiatkowski
   [17]sebastian kwiatkowski (button) blockedunblock (button)
   followfollowing
   feb 19, 2018

table of contents

     * part 1: attributes and patterns
     * [18]part 2: collections and data
     * [19]part 3: arrays and representations
     * [20]part 4: functions and classifiers
     __________________________________________________________________

   this is the first article in a brand new series on machine learning.
   each article will be based on five core principles:
    1. the main goal i have is to equip the reader with an in-depth
       understanding of the fundamentals of applied machine learning. if
       you would like to build a solid foundation to analyze the
       implications of artificial intelligence for your industry and your
       personal life, then this series is for you.
    2. the plan is to cover the the most successful machine learning
       models as well as some of the latest validated research trends. i
       will not discuss any approaches that have either failed to gain
       traction or any speculative ideas that have yet to receive
       empirical support.
    3. the material is self-contained and develops the foundations of
       applied machine learning one step at a time. saying that
       high-school mathematics is a prerequisite would be an
       overstatement. some articles will cover topics as basic as sets,
       derivatives and linear models.
    4. each concept will be explained with at least one example. i think
       that one of the most effective strategies to approach an abstract
       topic is to distill it down to a simple case, make it concrete with
       vivid examples and finally reconstruct the big picture. when it
       seems useful, the text will completed with visuals.
    5. everything else is subject to feedback, experimentation and
       learning. i welcome questions and constructive feedback at any
       time!

   let   s get started.
     __________________________________________________________________

what is machine learning?

   machine learning uses computers to predict unknown object attributes
   through the recognition of patterns in data.

   the most important reason for the rise of machine learning is its
   success across a broad range of different domains, including images,
   sound, videos, text, software code, particle collisions, chemical
   descriptors, dna, social networks and financial data.

   interestingly, the number of building blocks that make up most machine
   learning systems is smaller than one might imagine. this is especially
   true for many of the deep neural networks that will be covered in
   detail in later episodes.

   in other words, we can combine a small number of basic elements into
   systems that can be of tremendous value for applications as diverse as
   image classification, id53 and the synthesis of
   natural-sounding speech.

   while machine learning can be applied in many different domains, most
   systems in production are optimized for a single purpose and process
   objects of one particular type.

   id161 systems, for example, specialize in tasks related to
   images or videos, such as traffic sign recognition[1] and the
   classification of medical images[2].
     __________________________________________________________________

objects

   since machine learning deals with unknown object attributes, a natural
   starting point is to talk about objects.

   for purposes of this tutorial, an object is anything that has
   attributes.

   i   ve promised that i will make abstract notions concrete and use
   visuals where it seems appropriate. here is a photo that shows three
   exemplary objects (of desire):
   [1*oulu4dcm84d__miboubfmg.jpeg]
   fig. 1: image by [21]rawan hussein

   among other attributes, each of these scoops of ice cream has a
   characteristic taste, a range of colors it is presented in, a certain
   level of popularity, a price it sells for, etc.

   (an attribute that three three scoops of ice cream share is that they
   have more interesting attributes than the waffle cone.)

attributes

   an attribute is something that characterizes an object.

   let me warn you ahead of time. i love concept trees. consequently, you
   will see many of them over the course of this series. here is the first
   one:
   [1*v2wpgykhgxu1jfh-5qq7na.png]
   fig. 2

   at a high level, we can distinguish between measured attributes and
   unmeasured attributes.

   many attributes are measurable. simple examples include the weight of a
   scoop of ice cream and the price at which a unit is sold at a
   particular place and time. out of the attributes that are measurable,
   only a fraction will actually be measured as part of the data
   collection activities during a project.

   other attributes will not be measured because they are either
   irrelevant, beyond our current understanding or outside of our budget.
   the brain, presumably, has attributes that we could measure to create
   the perfect new ice cream flavor. in practice, however, such an effort
   is constrained by our imperfect understanding of how the brain
   generates the subjective experience of tasting delicious ice cream and
   the price tag put on the required scanning equipment.

measured attributes

   let   s further break down measured attributes into two sub-types:
     * targets
     * features

   the target is the attribute that we care about and that we would like
   to predict.

   some of terms that are largely synonymous are dependent variable,
   unobserved variable, explained variable, output variable and outcome
   measurement.

   [i should note that some of the concepts used in machine learning are
   known under different names in other communities. in many ways, machine
   learning is a descendant of statistics and there is a strong overlap
   between the two fields. consequently, you may encounter variants that
   are popular with statisticians as you dive deeper into the literature.

   i will mention some of the frequently used synonyms when a new term is
   introduced and then consistently use the variant that is most popular
   within the machine learning community and fits in best with the overall
   picture.]

   often times, the target is known for some objects and unknown for
   others. for example, we might want to predict the unknown future sales
   figures for a product based, in part, on known historic sales figures.

   features are attributes that are (potentially) related to the target.

   the price of a product, for example, is related to the demand for the
   product. this relationship may be strong or weak depending on the
   circumstances[3], but price is certainly one of the first features that
   should come to mind.

   in other communities, features are often referred to as independent
   variables, observed variables, explanatory variables and input
   variables.

attribute values

   an attribute value is the value of an attribute with regard to a
   particular object.

   if target are the sales figures for the next quarter, an example of a
   target value could be 8,000 units.

   analogous to targets, we can use the term feature value to refer to the
   value of a specific feature with regard to a particular object. a value
   of the price feature in us dollars might be 99.99.

pattern recognition

   machine learning is based on the premise that there are relationships
   between features and targets that repeat in a predictable manner. let   s
   refer to these relationships as patterns.

   if we were living in a world without patterns, there would be no use
   for machine learning and this tutorial would neither have been written
   nor read. luckily, the universe that we find ourselves in is highly
   structured. the success of empirical science[4] is a testament to the
   idea that patterns exist and that they can be discovered.

   in the not-so-distant past, herculean efforts were made to engineer
   features for specific tasks. for example, there is a large literature
   on features that were developed for certain id161 problems,
   such as the ability of robots to recognize objects in a room.[5]

   over the last decade, progress in machine learning has made it possible
   to automate the search for features to some extent.[6]

an example from id161

   to wrap up this article, let   s use a concrete example to gain more
   insight into the relationship between features and targets.
   [1*smad7jp15353hscx2la-tg.jpeg]
   fig. 3: photo by [22]patrick berry / cc by-sa 2.0

   suppose you   ve just started to work on a system with the objective to
   automatically recognize animals in photos.

   the photo above has the attribute of showing an orangutan mother and
   her infant, a fact that is obvious to us.

   to the id161 system, however, this is far from obvious. at
   the beginning of the development, it does not have any knowledge about
   orangutans, mothers or infants.

   before i describe what we can do to change this, i would like to ask
   you a question.

   how do you solve this task? that is, how do you understand that the
   photo above shows two orangutans?

   human vision (and primate vision in general) is so efficient and occurs
   with so little conscious effort that it can take a while to come up
   with a precise and compelling answer.

   i would suggest you pause and think about it for a moment. in doing so,
   you might anticipate some of ideas described below and eventually
   realize that at least some forms of machine learning are more intuitive
   than they may seem.
     __________________________________________________________________

   part of the answer is that we have seen similar photos of orangutans in
   the past.

   in some cases, we were presented with relevant images, but without much
   in terms of a description or a context. for example, we might have
   briefly seen these great apes while flipping through tv channels or
   scrolling through our facebook feed.

   in other cases, images of orangutans were presented together with
   certain information that allowed us to easily recognize patterns. this
   could have been during documentaries, visits to the zoo, school lessons
   or other learning opportunities.

   one of these patterns is that orangutans have orange-reddish brown
   hair. the presence of this salient feature makes many other
   possibilities substantially less likely.

   another relevant aspect is their semi-solitary lifestyle. outside of
   the strong mother-infant bonds they form, orangutans are usually seen
   alone. all other things being equal, the presence of many individuals
   on the same photo could be a feature that points to the absence of
   orangutans.

   yet another source of information we can attend to is the background.
   as the largest tree-dwelling animals, orangutans often appear on photos
   that show the canopy of the forests they live in.

   this is an intuitive account of some of the high-level features we
   might use to detect our evolutionary relatives in images. later
   tutorials will describe how machine learning models learn hierarchies
   of features to classify images. remarkably, some of these features will
   turn out to be easy to interpret and have a degree of similarity to
   those that are used in primate vision.

supervised machine learning

   the workflow of collecting relevant data and recognizing patterns to
   generate predictions is the essence of what is called supervised
   (machine) learning. in this setting, the target values (e.g., the
   presence of absence of certain content) are known for some objects
   (e.g., images) and unknown for others.

   in the context of id161, we as teachers show the learner (the
   computer/robot) example images, provide brief descriptions of the
   content, emphasize certain features and select strategies that help the
   learner    see    the patterns in the data.

   with a few adjustments, the same process can be used in the many
   different domains that were mentioned at the outset, from text and
   sound to chemical structures and social networks.

   we will gradually make this intuition precise and integrate it into a
   network of concepts that will allow you to understand machine learning
   applications.
     __________________________________________________________________

   the focus in this article was on attributes, patterns and computer
   vision. [23]part 2 looks at collections of data using examples from
   natural language processing.

thank you for reading! if you   ve enjoyed this article, hit the clap button
and follow me to get the next articles in this series.
     __________________________________________________________________

references

   [1] cire  an, d., meier, u., masci, j. and schmidhuber, j., 2012.
   multi-column deep neural network for traffic sign classification.
   neural networks, 32, pp.333   338.

   [2] esteva, a., kuprel, b., novoa, r.a., ko, j., swetter, s.m., blau,
   h.m. and thrun, s., 2017. dermatologist-level classification of skin
   cancer with deep neural networks. nature, 542(7639), p.115.

   [3] [24]https://en.wikipedia.org/wiki/price_elasticity_of_demand

   [4] [25]https://en.wikipedia.org/wiki/list_of_equations

   [5] lowe, d.g., 1999. object recognition from local scale-invariant
   features. in id161, 1999. the proceedings of the seventh ieee
   international conference on (vol. 2, pp. 1150   1157). ieee.

   [6] krizhevsky, a., sutskever, i. and hinton, g.e., 2012. id163
   classification with deep convolutional neural networks. in advances in
   neural information processing systems (pp. 1097   1105).

     * [26]machine learning
     * [27]neural networks
     * [28]artificial intelligence
     * [29]business
     * [30]towards data science

   (button)
   (button)
   (button) 5.8k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of sebastian kwiatkowski

[32]sebastian kwiatkowski

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 5.8k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/76603dececa6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-1-76603dececa6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-1-76603dececa6&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rkkitiqeqxmo---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sekwiatkowski?source=post_header_lockup
  17. https://towardsdatascience.com/@sekwiatkowski
  18. https://towardsdatascience.com/machine-learning-from-scratch-part-2-99ce4c78a3cc
  19. https://towardsdatascience.com/machine-learning-from-scratch-part-3-ed572330367d
  20. https://towardsdatascience.com/machine-learning-from-scratch-part-4-10117c005a28
  21. https://www.123rf.com/photo_29581169_chocolate-vanilla-and-strawberry-ice-cream-in-the-cone-on-white-background-with-clipping-path-.html
  22. https://www.flickr.com/photos/pbarry/4429503485
  23. https://towardsdatascience.com/machine-learning-from-scratch-part-2-99ce4c78a3cc
  24. https://en.wikipedia.org/wiki/price_elasticity_of_demand
  25. https://en.wikipedia.org/wiki/list_of_equations
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/neural-networks?source=post
  28. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  29. https://towardsdatascience.com/tagged/business?source=post
  30. https://towardsdatascience.com/tagged/towards-data-science?source=post
  31. https://towardsdatascience.com/@sekwiatkowski?source=footer_card
  32. https://towardsdatascience.com/@sekwiatkowski
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/76603dececa6/share/twitter
  39. https://medium.com/p/76603dececa6/share/facebook
  40. https://medium.com/p/76603dececa6/share/twitter
  41. https://medium.com/p/76603dececa6/share/facebook
