   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning from scratch: part 3

arrays and representations

   [16]go to the profile of sebastian kwiatkowski
   [17]sebastian kwiatkowski (button) blockedunblock (button)
   followfollowing
   mar 5, 2018

table of contents

     * [18]part 1: attributes and patterns
     * [19]part 2: collections and data
     * part 3: arrays and representations
     * [20]part 4: functions and classifiers
     __________________________________________________________________

   part 3 introduces arrays. this family of higher-order collections
   allows us to describe images and text documents in a format that can be
   processed by machine learning algorithms.

   along the way, we will discuss id31, an important
   application of natural language processing that is used for market
   research and reputation management.
     __________________________________________________________________

arrays

   last time, we introduced the idea of higher-order collections:
   collections that are organized in collections.

   arrays are the most important family of higher-order collections in
   machine learning. they are used to represent images, text documents and
   many other types of data.

   arrays have three important properties. i will first enumerate them and
   then go into more detail:
    1. arrays are lists of one or more dimensions.
    2. all lists on a particular level have the same format.
    3. we will assume that all of the elements in an array are numbers.

vectors, matrices and 3d arrays

   a one-dimensional array is simply a list. an array of two dimensions is
   a list of lists. and a three-dimensional array is a list of lists of
   lists. we will not use any arrays with more than three dimensions in
   this article.

   one-dimensional arrays are called vectors. two-dimensional arrays are
   called matrices. we will not introduce a special term for
   three-dimensional arrays and simply refer to them as 3d arrays.

same level, same format

   in an array, all lists on a given level have the same format.

   consider the following example: [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ]. this
   is a list of three lists. each of the three inner lists has the same
   number of elements: 2. this collection, therefore, qualifies as an
   array.

   in contrast, the collection [ [ 1, 2, 3], [ 4 ], [ 5, 6 ] ] does not
   qualify as an array, because the inner lists have different lengths: 3,
   1 and 2, respectively.

numbers only

   the individual items in an array are known as its entries (the term i
   will use) or elements.

   all entries are assumed to be numbers.
     __________________________________________________________________

image representation

   we will first discuss how matrices are used to represent grayscale
   images. a later section will extend our discussion to color images.
   true to the name of the series, we will proceed one step at a time.

   an image is a collection of pixels arranged on a grid.

   pixels are the smallest elements presented on a screen and the lowest
   level of analysis in id161. a pixel is characterized by the
   following attributes:
    1. a position on the grid
    2. one or more measurements related to the intensity of light

representing a grayscale image

   consider the following example of the digit 3:
   [0*ofo4tndnlu3kwrxm.]
   fig. 1

   to break this image down into pixels, we can impose a grid that divides
   up the image into cells:
   [0*fmfg2eze37hdb9hr.]
   fig. 2

   each cell in this grid will be considered as one pixel.

   pixels differ in the intensity of light which we can measure on a
   grayscale from 0 to 255. a value of 0 corresponds to black and a value
   of 255 is white. the values in-between are different shades of gray.

   using measurements on this scale, we can switch from the visual
   representation to a numerical representation:
   [0*lscw3ompb4cshidq.]
   fig. 3

generalizing to other images

   so far, we looked at one specific image with a height of 7 pixels and a
   width of 6 pixels.

   to generalize this representation to all images of this format, the
   specific value can be replaced with variables:
   [0*stw53_u422ti6g2n.]
   fig. 4

   we can denote the height of the image with the letter m and the width
   of the image with the letter n. in the previous example, we had m = 7
   and n = 6.

   using this notation, we can now generalize from a 7 x 6 image to an m x
   n image.
   [0*hkk7nyw3ydf7mb2o.]
   fig. 5

   if remove the gray borders and add square brackets around the grid, we
   arrive at a matrix representation.
     __________________________________________________________________

matrices as tables of numbers

   [1*cijmt_jlw_8s6kqksh26sa.png]
   fig. 6

   a matrix is an outer list of one or more inner lists.

   fig. 6 illustrates that the entries in a matrix form m rows and n
   column. there are two ways to look at this: we can either think of the
   rows as the inner lists and consider the matrix as a list of rows or we
   can focus on the columns and regard the matrix as a list of columns.

   viewed from the first perspective, the inner lists are stacked on top
   of each other. for example, in the matrix [ [ 1, 2 ], [3, 4] ], the
   list [ 1, 2 ] can be stacked on top of the list [ 3, 4 ] to form a
   table. alternatively, we can consider the two lists as the columns of
   the table.

   an individual entry in a matrix is denoted as a_ij. [i   m using the
   underscore    _    to indicate a subscript.]

   the letter i stands for the index of the row and the letter j
   corresponds to the column. for example, the entry in row 3 and column 2
   is denoted as a_32. in fig. 3, we have a_32 = 255.
     __________________________________________________________________

3d arrays for rgb images

   color images are most often described through the rgb model in which
   the color of each pixel is expressed as a triple (r, g, b), with the
   three items corresponding to the intensity of red light, green light
   and blue light, respectively.

   to numerically represent an rgb image, we use a 3d array. whereas a
   two-dimensional array forms a table, a 3d array can be thought of as a
   cube:
   [0*ayinp5ck7hn3g91z.]
   fig. 7

   this covers the fundamentals of image representation for machine
   learning approaches to id161.

   next, we will discuss how vectors relate to matrices and why they are
   suitable for the representation of text documents.
     __________________________________________________________________

vectors as special cases of matrices

   recall the following the two facts:
     * a vector is a list of numbers.
     * a matrix can be thought of as a table, arranged in rows and
       columns.

   combining both facts, a vector can be seen as a particular row or
   column in a matrix.

   a row vector is a matrix with a single row (m = 1). a column vector is
   a matrix with a single column (n = 1).

   row vectors, column vectors and matrices can be enclosed in square
   brackets or parentheses. commas are usually left out.

   the matrix in fig. 3 consists of 7 row vectors and 6 column vectors. we
   can, for example, extract the following row vector from the second row
   of this matrix: [ 255 0 0 0 0 255 ].

   we denote an individual vector entry with x_i and the number of entries
   with d.

   vectors and matrices are arguably the two most important data
   structures in machine learning. here is a quick comparison:
   [0*6nsavotet9fm-efl.]
   fig. 8
     __________________________________________________________________

id31

   when you publish an opinion about a brand online, computational systems
   will likely analyze your statement, find out whether it expresses a
   positive or a negative view and use the result for market research and
   reputation management purposes. this application of natural language
   processing (nlp) is called id31.

   an important empirical finding is that simple approaches to sentiment
   analysis can achieve excellent performance and are hard to
   outperform.[1, 2] after all, many customer make it a point to be
   understood by anyone willing to read.

   i will use id31 to explain how vectors can be used to
   represent documents.
     __________________________________________________________________

id194

nlp at the movies

   movie reviews are a popular data source in id31
   research.[3] they are easy to obtain in large numbers, pose interesting
   challenges that will be discussed later in articles and can help
   predict box-office revenues[4].
   [1*sizvcisgadrtyyjt8drwag.jpeg]
   copyright: [21]nyul / 123rf stock photo

   while the approach outlined in this section can be effectively applied
   to hundreds of thousands of reviews of different lengths, we will use
   only a small collection of reviews that express clear opinions. the
   focus here is on the concepts. the machines can do the heavy lifting.

   the plan is to start with an initial vocabulary and pass the words in
   the vocabulary through a series of operations that yields a set of
   relevant features. one vector is generated for every document in the
   corpus and each entry in a document vector will correspond to one of
   the selected features. but first we should clarify some terms.

corpora, documents and words

   a corpus is a collection of documents.

   in nlp, a document is any collection of words that has an attribute
   that we are interested in.

   a document can be as short as a single word or sentence and as long as
   an entire book series or website.

   in the case of id31, we are interested in the polarity of
   the opinion expressed in the document. does it express a positive or a
   negative opinion?

   by the way, i will use the term word to refer to sequences of symbols.
   under this definitions, numbers and punctuation marks count as words.

initial vocabulary

   here is the corpus that we will work with:
     * i looove this movie.
     * i hate this movie.
     * best movie i   ve ever seen.
     * what a disappointing movie.

   the alphabetically sorted vocabulary of this corpus contains the
   following words:

   { a, best, disappointing, ever, hate, i, i   ve, looove, movie, seen,
   this, what, . }

   before we proceed, i suggest you pause for a moment and think about the
   following question: would you use every single word in this vocabulary
   to perform id31? or might there be some words that we can
   delete or change to improve pattern recognition?
     __________________________________________________________________

pipelines

   in many nlp projects, the initial vocabulary is passed through a
   pipeline: a series of operations in which the output of one step is the
   input to the next step.

   the particular sequence of operations described in this section
   simplifies the data and eliminates irrelevant words.

   warning! proceed with caution! while many of the steps mentioned below
   work quite reliably across different tasks and domains, they can filter
   out relevant words when applied too aggressively.

   function words, such as pronouns, are a case in point. these are words
   that mainly contribute to the syntax of a sentence. in most
   applications, we can remove function words without causing a
   significant loss of information. for some psycholinguistic
   applications, however, these words can provide useful features.[5] for
   example, one study found that similarity in the use of function words
   predicts romantic interest and relationship stability.[6]

   with the obligatory warning out of the way, here is an overview of the
   entire process:
   [0*51d2tnhj9mg6a--q.]
   fig. 9

   the first step we apply can be called id172. contracted forms
   (such as i   ve) are expanded (i have), while elongated words (such as
   looove) are reduced to their conventional form (love). personally, i
   have nothing anything against looove. it   s just that patterns easier to
   recognize when all expressions of love have the same number of o   s. the
   normalized vocabulary looks like this:

   { a, best, disappointing, ever, hate, have, i, love, movie, seen, this,
   what, . }

   the next step is conversion to lowercase. does it make a difference for
   id31 whether the first letter in the words best and what
   is uppercase?         no, not really:

   { a, best, disappointing, ever, hate, have, i, love, movie, see, this,
   what, . }

   another common step is to remove non-alphabetic words. the full stop,
   for example, carries little relevant information. let   s get rid of it:

   { a, best, disappointing, ever, hate, have, i, love, movie, see, this,
   what }

   part 2 mentioned that some words, including articles, determiners,
   prepositions and basic verbs occur in just about every text: in
   positive reviews and in negative reviews, in movie reviews and in
   non-movie reviews, in the previous sentence, in this sentence, in the
   next sentence     you get the idea. these words are called stop words.
   let   s delete them:

   { best, disappointing, ever, hate, love, movie, see }

   and there are some words that behave like stop words within a
   particular domain, even though they are not considered to be stop words
   in general. for example, the words movie and see occur in a large
   fraction of movie reviews, without providing clues about the polarity
   of an opinion. i will refer to these words as domain-specific stop
   words. deleting these words is the final step in the pipeline and
   yields the following result:

   { best, disappointing, ever, hate, love }

   overall, we have eliminated 8 out of 13 members of the vocabulary and
   have arrived at what i think is an intuitively plausible set of words
   that are relevant for id31.

binary document vectors

   using the five remaining words (best, disappointing, ever, hate, love)
   as features, we can now represent each of the documents as a vector
   with five entries. for every feature, there is a corresponding vector
   entry.

   binary vectors are an appropriate choice for short documents. these are
   vectors whose entries are all 0 or 1.

   we assign a feature value of 1 if the feature (word) is present in the
   document and a value of 0 if the feature is absent.

   consider the third document as an example (best movie i   ve ever seen.).
   two of the five features are present in this document: the first
   feature (best) and the third one (ever). consequently, we set the first
   and the third entry to 1 and the other entries to 0. this gives us the
   vector [ 1 0 1 0 0 ].

   applying the same procedure to the every document in the corpus, we
   obtain the following vectorial representations:
     * i looove this movie.     [ 0 0 0 0 1]
     * i hate this movie.     [ 0 0 0 1 0 ]
     * best movie i   ve ever seen.     [ 1 0 1 0 0 ]
     * what a disappointing movie.     [ 0 1 0 0 0 ]
     __________________________________________________________________

   congratulations! if you are reading this, you have learned how to
   represent images and text documents in a format that can be processed
   by machine learning algorithms.

   next time, we will discuss functions         and i want you to be as
   enthusiastic about this subject as i am.

   the models used in machine learning to predict unknown attributes all
   have the form of functions. neural networks, including the deep neural
   networks you may be reading about on a regular basis, are essentially
   sequences of functions that are applied to arrays of numbers.

thank you for reading! if you   ve enjoyed this article, hit the clap button
and follow me to get the next articles in this series.
     __________________________________________________________________

references

   [1] wang, s. and manning, c.d., 2012, july. baselines and bigrams:
   simple, good sentiment and topic classification. in proceedings of the
   50th annual meeting of the association for computational linguistics:
   short papers-volume 2 (pp. 90   94). association for computational
   linguistics.

   [2] li, b., zhao, z., liu, t., wang, p. and du, x., 2016. weighted
   neural bag-of-id165s model: new baselines for text classification. in
   proceedings of coling 2016, the 26th international conference on
   computational linguistics: technical papers (pp. 1591   1600).

   [3] maas, a.l., daly, r.e., pham, p.t., huang, d., ng, a.y. and potts,
   c., 2011, june. learning word vectors for id31. in
   proceedings of the 49th annual meeting of the association for
   computational linguistics: human language technologies-volume 1 (pp.
   142   150). association for computational linguistics.

   [4] asur, s. and huberman, b.a., 2010, august. predicting the future
   with social media. in proceedings of the 2010 ieee/wic/acm
   international conference on web intelligence and intelligent agent
   technology-volume 01 (pp. 492   499). ieee computer society.

   [5] pennebaker, j.w., francis, m.e. and booth, r.j., 2001. linguistic
   inquiry and word count: liwc 2001. mahway: lawrence erlbaum associates,
   71(2001), p.2001.

   [6] ireland, m.e., slatcher, r.b., eastwick, p.w., scissors, l.e.,
   finkel, e.j. and pennebaker, j.w., 2011. language style matching
   predicts relationship initiation and stability. psychological science,
   22(1), pp.39   44.

     * [22]machine learning
     * [23]artificial intelligence
     * [24]business
     * [25]movies
     * [26]towards data science

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of sebastian kwiatkowski

[28]sebastian kwiatkowski

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ed572330367d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-3-ed572330367d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-3-ed572330367d&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_3vhm9y8wygi7---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sekwiatkowski?source=post_header_lockup
  17. https://towardsdatascience.com/@sekwiatkowski
  18. https://towardsdatascience.com/machine-learning-from-scratch-part-1-76603dececa6
  19. https://towardsdatascience.com/machine-learning-from-scratch-part-2-99ce4c78a3cc
  20. https://towardsdatascience.com/machine-learning-from-scratch-part-4-10117c005a28
  21. https://www.123rf.com/profile_nyul
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  24. https://towardsdatascience.com/tagged/business?source=post
  25. https://towardsdatascience.com/tagged/movies?source=post
  26. https://towardsdatascience.com/tagged/towards-data-science?source=post
  27. https://towardsdatascience.com/@sekwiatkowski?source=footer_card
  28. https://towardsdatascience.com/@sekwiatkowski
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/ed572330367d/share/twitter
  35. https://medium.com/p/ed572330367d/share/facebook
  36. https://medium.com/p/ed572330367d/share/twitter
  37. https://medium.com/p/ed572330367d/share/facebook
