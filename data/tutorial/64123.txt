   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id119 in a nutshell

   [16]go to the profile of niklas donges
   [17]niklas donges (button) blockedunblock (button) followfollowing
   mar 7, 2018

   id119 is by far the most popular optimization strategy, used
   in machine learning and deep learning at the moment. it is used while
   training your model, can be combined with every algorithm and is easy
   to understand and implement. therefore, everyone who works with machine
   learning should understand it   s concept. after reading this posts you
   will understand how id119 works, what types of it are used
   today and what are their advantages and tradeoffs.

   table of contents:
     * introduction
     * what is a gradient?
     * how it works
     * learning rate
     * how to make sure that it works properly
     * types of id119 (batch id119, stochastic
       id119, mini batch id119)
     * summary

introduction

   id119 is used while training a machine learning model. it is
   an optimization algorithm, based on a convex function, that tweaks it   s
   parameters iteratively to minimize a given function to its local
   minimum.

   it is simply used to find the values of a functions parameters
   (coefficients) that minimize a cost function as far as possible.

   you start by defining the initial parameters values and from there on
   id119 iteratively adjusts the values, using calculus, so
   that they minimize the given cost-function. but to understand it   s
   concept fully, you first need to know what a gradient is.

what is a gradient?

      a gradient measures how much the output of a function changes if you
   change the inputs a little bit.            lex fridman (mit)

   it simply measures the change in all weights with regard to the change
   in error. you can also think of a gradient as the slope of a function.
   the higher the gradient, the steeper the slope and the faster a model
   can learn. but if the slope is zero, the model stops learning. said it
   more mathematically, a gradient is a partial derivative with respect to
   its inputs.
   [1*goneafkmcjcikapc3t04-q.png]

   imagine a blindfolded man who wants to climb a hill, with the fewest
   steps possible. he just starts climbing the hill by taking really big
   steps in the steepest direction, which he can do, as long as he is not
   close to the top. as he comes further to the top, he will do smaller
   and smaller steps, since he doesn   t want to overshoot it. this process
   can be described mathematically, using the gradient.

   just take a look at the picture below. imagine it illustrates our hill
   from a top-down view, where the red arrows show the steps of our
   climber.think of a gradient in this context as a vector that contains
   the direction of the steepest step the blindfolded man can go and also
   how long this step should be.
   [0*-gapepnezoqstkux.png]

   note that the gradient ranging from x0 to x1 is much longer than the
   one reaching from x3 to x4. this is because the steepness/slope of the
   hill is less there, which determines the length of the vector. this
   perfectly represents the example of the hill, because the hill is
   getting less steep, the higher you climb it. therefore a reduced
   gradient goes along with a reduced slope and a reduced step-size for
   the hill climber.

how it works

   id119 can be thought of climbing down to the bottom of a
   valley, instead of climbing up a hill. this is because it is a
   minimization algorithm that minimizes a given function.

   the equation below describes what id119 does:    b    describes
   the next position of our climber, while    a    represents his current
   position. the minus sign refers to the minimization part of gradient
   descent. the    gamma    in the middle is a waiting factor and the gradient
   term (   f(a) ) is simply the direction of the steepest descent.
   [0*zhkyphscgi8wowpu.png]

   so this formula basically tells you the next position where you need to
   go, which is the direction of the steepest descent.

   but to be sure that you fully grasp the concept, we will go through
   another example.

   imagine you are dealing with a machine learning problem and want to
   train your algorithm with id119 to minimize your
   cost-function j(w, b) and reach its local minimum by tweaking its
   parameters (w and b).

   let   s take a look at the picture below, which is an illustration of
   id119. the horizontal axes represent the parameters (w and
   b) and the cost function j(w, b) is represented on the vertical axes.
   you can also see in the image that id119 is a convex
   function.
   [0*s0ppngce6svj8w5u.png]

   like you already know we want to find the values of w and b that
   correspond to the minimum of the cost function (marked with the red
   arrow). to start with finding the right values we initialize the values
   of w and b with some random numbers and id119 then starts at
   that point (somewhere around the top of our illustration). then it
   takes one step after another in the steepest downside direction (e.g.
   from the top to the bottom of the illustration) till it reaches the
   point where the cost function is as small as possible.

importance of the learning rate

   how big the steps are that id119 takes into the direction of
   the local minimum are determined by the so-called learning rate. it
   determines how fast or slow we will move towards the optimal weights.

   in order for id119 to reach the local minimum, we have to
   set the learning rate to an appropriate value, which is neither too low
   nor too high.

   this is because if the steps it takes are too big, it maybe will not
   reach the local minimum because it just bounces back and forth between
   the convex function of id119 like you can see on the left
   side of the image below. if you set the learning rate to a very small
   value, id119 will eventually reach the local minimum but it
   will maybe take too much time like you can see on the right side of the
   image.
   [0*qwe8m4mupsdqa3m4.png]

   this is the reason why the learning rate should be neither too high nor
   too low. you can check if you   re learning rate is doing well by
   plotting the learning rate on a graph, which we will discuss in the
   section below.

how to make sure that it works properly

   a good way to make sure that id119 runs properly is by
   plotting the cost function as id119 runs. you put the number
   of iterations on the x-axes and the value of the cost-function at the
   y-axes. this enables you to see the value of your cost function after
   each iteration of id119. this lets you easily spot how
   appropriate your learning rate is. you just try different values for it
   and plot them all together.

   you can see such a plot below on the left and the image on the right
   shows the difference between good and bad learning rates:
   [0*vrzwuzdnncfjsdig.png]

   if id119 is working properly, the cost function should
   decrease after every iteration.

   when id119 can   t decrease the cost-function anymore and
   remains more or less on the same level, we say it has converged. note
   that the number of iterations that id119 needs to converge
   can sometimes vary a lot. it can take 50 iterations, 60,000 or maybe
   even 3 million. therefore the number of iterations is hard to estimate
   in advance.

   out there are also some algorithms that can tell you automatically if
   id119 has converged but you need to define a threshold for
   the convergence beforehand which is also pretty hard to estimate. this
   is why these simple plots are the preferred convergence test.

   another advantage of monitoring id119 via plots is that you
   can easily spot if it doesn   t work properly, like for example if the
   cost function is increasing. mostly the reason for an increasing
   cost-function, while using id119, is that you   re learning
   rate is too high.

   if you see in the plot that your learning curve is just going up and
   down, without really reaching a lower point, you also should try to
   decrease the learning rate. by the way, when you   re starting out with
   id119 on a given problem, just simply try 0.001, 0.003,
   0.01, 0.03, 0.1, 0.3, 1 etc. as it   s learning rates and look which one
   performs the best.

types of id119

   out there are three popular types of id119, that mainly
   differ in the amount of data they use. we go through them one by one.

   batch id119

   batch id119, also called vanilla id119,
   calculates the error for each example within the training dataset, but
   only after all training examples have been evaluated, the model gets
   updated. this whole process is like a cycle and called a training
   epoch.

   advantages of it are that it   s computational efficient, it produces a
   stable error gradient and a stable convergence. batch id119
   has the disadvantage that the stable error gradient can sometimes
   result in a state of convergence that isn   t the best the model can
   achieve. it also requires that the entire training dataset is in memory
   and available to the algorithm.

   stochastic id119

   stochastic id119 (sgd) in contrary, does this for each
   training example within the dataset. this means that it updates the
   parameters for each training example, one by one. this can make sgd
   faster than batch id119, depending on the problem. one
   advantage is that the frequent updates allow us to have a pretty
   detailed rate of improvement.

   the thing is that the frequent updates are more computationally
   expensive as the approach of batch id119. the frequency of
   those updates can also result in noisy gradients, which may cause the
   error rate to jump around, instead of slowly decreasing.

   mini batch id119

   mini-batch id119 is the go-to method since it   s a
   combination of the concepts of sgd and batch id119. it
   simply splits the training dataset into small batches and performs an
   update for each of these batches. therefore it creates a balance
   between the robustness of stochastic id119 and the
   efficiency of batch id119.

   common mini-batch sizes range between 50 and 256, but like for any
   other machine learning techniques, there is no clear rule, because they
   can vary for different applications. note that it is the go-to
   algorithm when you are training a neural network and it is the most
   common type of id119 within deep learning.

summary

   in this post, you learned a lot about id119. you now know
   the basic terms of it and you have an understanding of how the
   algorithm works behind the scenes. furthermore, you learned why the
   learning rate is it   s most important hyperparameter and how you can
   check if your algorithm learns properly. lastly, you learned about the
   three most common types of id119 and their advantages and
   disadvantages. this knowledge enables you to train your models
   properly.

   this post was initially published at my blog
   ([18]https://machinelearning-blog.com).

     * [19]machine learning
     * [20]towards data science
     * [21]id119
     * [22]neural networks

   (button)
   (button)
   (button) 3.3k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of niklas donges

[24]niklas donges

   co-founder: markov-solutions.com     ai solutions & consulting |
   linkedin.com/in/niklas-donges | machinelearning-blog.com

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 3.3k
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/eaf8c18212f0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_bbmxbcxjq9vc---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@n.donges?source=post_header_lockup
  17. https://towardsdatascience.com/@n.donges
  18. https://machinelearning-blog.com/
  19. https://towardsdatascience.com/tagged/machine-learning?source=post
  20. https://towardsdatascience.com/tagged/towards-data-science?source=post
  21. https://towardsdatascience.com/tagged/gradient-descent?source=post
  22. https://towardsdatascience.com/tagged/neural-networks?source=post
  23. https://towardsdatascience.com/@n.donges?source=footer_card
  24. https://towardsdatascience.com/@n.donges
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/eaf8c18212f0/share/twitter
  31. https://medium.com/p/eaf8c18212f0/share/facebook
  32. https://medium.com/p/eaf8c18212f0/share/twitter
  33. https://medium.com/p/eaf8c18212f0/share/facebook
