   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

understanding and optimizing gans (going back to first principles)

   [16]go to the profile of mirantha jayathilaka
   [17]mirantha jayathilaka (button) blockedunblock (button)
   followfollowing
   mar 7, 2018
   [1*7h3i2royalm93qug8gs__g.gif]
   fig 1. improvement of fake image generation of faces over training
   using basic gan algorithm

   the hype around id3 (gans) had been growing
   ever since the architecture was first introduced by ian goodfellow and
   the numerous advancements and applications become more and more
   fascinating by the day. but for anyone who wants to get started with
   gans, it can be quite tricky to figure out where to start.    don   t
   panic.    this post shall guide you.

   as with many things, the best way to fully understand a concept is to
   hit the roots. grab hold of the first principles. for gans, this is the
   original paper here ->([18]https://arxiv.org/abs/1406.2661). now to
   understand a paper of this sort there can be two approaches,
   theoretical and practical. i often prefer the latter, but in case you
   love to dig deep into the math, [19]here is a great post by my buddy
   [20]sameera breaking down the entire algorithm theoretically. in the
   meantime this post will present a simple implementation of the
   algorithm in its purest form using keras. let   s get started.

   in the underlying setting of a gan are two models namely, the generator
   and the discriminator, where the generator is constantly competing with
   the discriminator, which is an adversary that is learning to
   distinguish between the model distribution (e.g. generated fake images)
   and the data distribution (e.g. real images). this concept is famously
   visualized by a counterfeiter vs policeman scenario, where the
   generative model is thought of as a counterfeiter generating fake cash
   and the discriminator model as a policeman trying to detect the fake
   cash. the idea is that with constant competition between each other,
   both the counterfeiter and the policeman improve in each other   s role
   but ultimately the counterfeiter achieves a stage of producing fake
   cash that is indistinguishable from the real ones. simple. now let   s
   put this into code.

   the example script provided with this post is used to generate fake
   images of faces. the ultimate result we are trying to achieve with the
   algorithm is represented in figure 1.

   building the generator model

   so the generator model is supposed to take in some noise and output a
   desirable looking image. here we use keras sequential model along with
   dense and batch id172 layers. the activation function used is
   leaky relu. refer the code snippet below. the generator model can be
   divided into several blocks. one block consisting of dense layer ->
   activation ->batch id172. three such blocks are added and the
   last block transforms the pixels into the desired shape of the image
   we   re expecting as the output. the input to the model will be a noise
   vector of shape (100,) and the model is returned at the end. note how
   the nodes in each dense layer increases as the model progresses.

   iframe: [21]/media/25485dad649a8c2a4319948b1b6eb306?postid=e5df8835ae18

   building the discriminator model

   the discriminator takes in an input of an image, flatten it and pass it
   through two blocks of dense -> activation, to finally output a scalar
   between 1 and 0. output 1 should represent that the input image is real
   and 0 otherwise. simple as that. refer the code below.

   iframe: [22]/media/7c619d7c6518e8a9e45b4917cf92af8a?postid=e5df8835ae18

   note :-
   you can modify these models later to have more blocks, more batch norm
   layers, different activations etc. as per this example, these models
   are enough to understand the concepts behind gans.

   finding loss and training

   we calculate three losses, all using binary cross id178 in this
   example in order to train the two models.

   first the discriminator. it is trained to go two ways as shown in code
   below. first to output 1 for real images (array    img   ) and then to
   output 0 for generated images (array    gen_img   ). as training progresses
   the discriminator improves at this task. but our end goal is attained
   at a theoretical point where the discriminator outputs 0.5 for both
   types of inputs (i.e. indecisive if fake or real).

   iframe: [23]/media/2aa1c8596d8c4226310287a28dc0f5dd?postid=e5df8835ae18

   next is training the generator, which is the tricky bit. to do this we
   first formulate a combined model of discriminator given the output of
   generator. remember! ideally we want this to be 1, which means the
   discriminator identifying a fake generated image as real. so we train
   the output of the combined model against 1. see the code below.

   iframe: [24]/media/fc7eadd6922cd163917e0da1f30c3983?postid=e5df8835ae18

   now let   s play!
   that is pretty much the gist of the code to simply understand the
   workings of gans.

   *** the full code can be found [25]here on my github. you can refer to
   all the additional code for importing rgb images, initializing the
   models and logging the results in the code. also note that during
   training the mini-batches are set to hi32 images in order to be able to
   run on cpu.
   also the real images used in the example are 5000 images from the
   celeba dataset. this is an open source dataset and i have uploaded it
   to my floydhub for easy downloading which you can find [26]here. **

   there are many ways you can optimize the code to obtain better results
   and also to get an idea how different components of the algorithm
   affect the efficiency of the results. observing the results while
   tweaking different components such as the optimizers, activators,
   id172s, loss calculators, hyperparameters etc, is the best way
   to enhance your understanding of the algorithm. i chose to vary the
   optimizers.

   so training for 5000 epochs with batches of 32, i tested with three
   types of optimization algorithms. with keras this process is as easy as
   importing and replacing the name of the optimizer function. all
   optimizers in-built to keras can be found [27]here.
   also the losses where plotted in each instance to understand the
   behavior of the models.
    1. using sgd ( stochastic id119 optimizer). the output and
       the loss variations are shown in figure 2 and 3 respectively.

   [1*0b-4sei9_juow0yx4062mq.gif]
   fig 2. output of the gan using sgd as the optimizer
   [1*qsfwetxcbg26usc9fvpmjg.png]
   fig 3. plot showing the variation of losses while training the gan
   using sgd

   comment         though the convergence is noisy we can see here that the
   generator loss is decreasing over epochs, which implies that the
   discriminator tends to detect fake images as real.

   2. using rmsprop optimizer. the output and the loss variations are
   shown in figure 4 and 5 respectively.
   [1*v0by6ijlfqwhhrtxy52smq.gif]
   fig 4. output of the gan using rmsprop as the optimizer

   losses:
   [1*3lbhsj_fll_jsktk6zeeka.png]
   fig 5. plot showing the variation of losses while training the gan
   using rmsprop

   comment         here also the we see that the generator loss is decreasing
   which is a good thing. surprisingly the discriminator loss on real
   images increases which is quite interesting.

   3. using adam optimizer. the output and the loss variations are shown
   in figure 6 and 7 respectively.
   [1*7h3i2royalm93qug8gs__g.gif]
   fig 6. output of the gan using adam as the optimizer
   [1*4a5bo8gvg9wmg-5wtqavog.png]
   fig 7. plot showing the variation of losses while training the gan
   using adam

   comment         the adam optimizer yields the best looking results so far.
   notice how the discriminator loss on fake images retains a larger
   value, meaning the discriminator tends to lean towards detecting fake
   images as real.

remarks

   i hope this post conveys a basic look into the inner workings of gans
   from a practical perspective to understand and see how you could
   improve on the basic models. there are numerous implementations of gans
   within the open source community on different applications and having a
   sound understanding on the first principles will help you immensely to
   understand the advancements. also gans being relatively new to deep
   learning, there are many research avenues open for anyone interested.

   so the possibilities are immense for you to explore!

   thanks to [28]chamin nalinda and [29]tds team.
     * [30]machine learning
     * [31]deep learning
     * [32]generative adversarial
     * [33]neural networks
     * [34]artificial intelligence

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [35]go to the profile of mirantha jayathilaka

[36]mirantha jayathilaka

   tech researcher. machine learning. startups. writing to share because i
   was inspired when others did.

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e5df8835ae18
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-and-optimizing-gans-going-back-to-first-principles-e5df8835ae18&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-and-optimizing-gans-going-back-to-first-principles-e5df8835ae18&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_iptqbmiwqau9---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@miranthaj?source=post_header_lockup
  17. https://towardsdatascience.com/@miranthaj
  18. https://arxiv.org/abs/1406.2661
  19. https://medium.com/@samramasinghe/generative-adversarial-networks-a-theoretical-walk-through-5889d5a8f2bb
  20. https://medium.com/@samramasinghe
  21. https://towardsdatascience.com/media/25485dad649a8c2a4319948b1b6eb306?postid=e5df8835ae18
  22. https://towardsdatascience.com/media/7c619d7c6518e8a9e45b4917cf92af8a?postid=e5df8835ae18
  23. https://towardsdatascience.com/media/2aa1c8596d8c4226310287a28dc0f5dd?postid=e5df8835ae18
  24. https://towardsdatascience.com/media/fc7eadd6922cd163917e0da1f30c3983?postid=e5df8835ae18
  25. https://github.com/miranthajayatilake/ganwkeras
  26. https://www.floydhub.com/mirantha/datasets/celeba
  27. https://keras.io/optimizers/
  28. https://medium.com/@chaminn?source=post_page
  29. https://medium.com/@tdsteam?source=post_page
  30. https://towardsdatascience.com/tagged/machine-learning?source=post
  31. https://towardsdatascience.com/tagged/deep-learning?source=post
  32. https://towardsdatascience.com/tagged/generative-adversarial?source=post
  33. https://towardsdatascience.com/tagged/neural-networks?source=post
  34. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  35. https://towardsdatascience.com/@miranthaj?source=footer_card
  36. https://towardsdatascience.com/@miranthaj
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card

   hidden links:
  40. https://medium.com/p/e5df8835ae18/share/twitter
  41. https://medium.com/p/e5df8835ae18/share/facebook
  42. https://medium.com/p/e5df8835ae18/share/twitter
  43. https://medium.com/p/e5df8835ae18/share/facebook
