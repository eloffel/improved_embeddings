   #[1]github [2]recent commits to sign-language:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]57
     * [35]star [36]524
     * [37]fork [38]239

[39]evilport2/[40]sign-language

   [41]code [42]issues 12 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   a very simple id98 project.
     * [47]64 commits
     * [48]1 branch
     * [49]1 release
     * [50]fetching contributors

   branch: master (button) new pull request
   [51]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [52]download zip

downloading...

   want to be notified of new releases in evilport2/sign-language?
   [53]sign in [54]sign up

launching github desktop...

   if nothing happens, [55]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [56]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [57]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [58]download the github extension for visual studio
   and try again.

   (button) go back
   dibakar saha dibakar saha
   dibakar saha and dibakar saha [59]opencv 4.0 compatible only
   latest commit [60]7af50aa apr 4, 2019
   [61]permalink
   type             name                 latest commit message     commit time
        failed to load latest commit information.
        [62]__pycache__             [63]critical updates           apr 3, 2019
        [64]gestures                [65]critical updates           apr 3, 2019
        [66]tmp/id98_model3
        [67].travis.yml
        [68]readme.md
        [69]id98_keras.py
        [70]id98_model_keras2.h5
        [71]id98_tf.py
        [72]confusion_matrix.png
        [73]create_gestures.py
        [74]display_all_gestures.py
        [75]flip_images.py
        [76]full_img.jpg
        [77]fun_util.py             [78]opencv 4.0 compatible only apr 3, 2019
        [79]gesture_db.db
        [80]get_model_reports.py
        [81]hist
        [82]load_images.py
        [83]model.png
        [84]recognize_gesture.py
        [85]requirements_cpu.txt
        [86]requirements_gpu.txt
        [87]set_hand_hist.py

readme.md

   [88]doi

sign-language

   a very simple id98 project.

note

   simple-opencv-calculator and this project are merged to one.
   simple-opencv-calculator will no longer be maintained.

what i did here

    1. the first thing i did was, i created 44 gesture samples using
       opencv. for each gesture i captured 1200 images which were 50x50
       pixels. all theses images were in grayscale which is stored in the
       gestures/ folder. the pictures were flipped using flip_images.py.
       this script flips every image along the vertical axis. hence each
       gesture has 2400 images.
    2. learned what a id98 is and how it works. best resources were
       [89]tensorflow's official website and
       [90]machinelearningmastery.com.
    3. created a id98 which look a lot similar to [91]this mnist
       classifying model using both tensorflow and keras. if you want to
       add more gestures you might need to add your own layers and also
       tweak some parameters, that you have to do on your own.
    4. then used the model which was trained using keras on a video
       stream.
    5. as of today, i have stored the 44 gestures for which are 26
       alphabets and 10 numbers of american sign language and some other
       gestures. and trained the model on these images.

   there are a lot of details that i left. but these are the basic and
   main steps.

outcome

   watch it [92]here.

requirements

    0. python 3.x
    1. [93]tensorflow 1.5
    2. [94]keras
    3. opencv 3.4
    4. h5py
    5. pyttsx3
    6. a good grasp over the above 5 topics along with neural networks.
       refer to the internet if you have problems with those. i myself am
       just a begineer in those.
    7. a good cpu (preferably with a gpu).
    8. patience.... a lot of it.

installing the requirements

    1. start your terminal of cmd depending on your os.
    2. if you have a nvidia gpu then make sure you have the prerequisites
       for tensorflow gpu installation (refer to official site). then use
       this commmand

pip install -r requirements_gpu.txt

    3. in case you do not have a gpu then use this command

pip install -r requirements_cpu.txt

how to use this repo

   before using this repo, let me warn about something. you will have no
   interactive interface that will tell you what to do. so you will have
   to figure out most of the stuff by yourself and also make some changes
   to the scripts if the needs arise. but here is a basic gist.

creating a gesture

    0. watch the video guide for setting the hand histogram [95]here.
    1. first set your hand histogram. you do not need to do it again if
       you have already done it. but you do need to do it if the lighting
       conditions change. to do so type the command given below and follow
       the instructions below.

python set_hand_hist.py

     * a windows "set hand histogram" will appear.
     * "set hand histogram" will have 50 squares (5x10).
     * put your hand in those squares. make sure your hand covers all the
       squares.
     * press 'c'. 1 other window will appear "thresh".
     * on pressing 'c' only white patches corresponding to the parts of
       the image which has your skin color should appear on the "thresh"
       window.
     * make sure all the squares are covered by your hand.
     * in case you are not successful then move your hand a little bit and
       press 'c' again. repeat this until you get a good histogram.
     * after you get a good histogram press 's' to save the histogram. all
       the windows close.

    2. i already have added 44 (0-43) gestures. it is on you if you want
       to add even more gestures or replace my gestures. hence this step
       is optional. to create your own gestures or replace my gestures do
       the following. it is done by the command given below. on starting
       executing this program, you will have to enter the gesture number
       and gesture name/text. then an opencv window called "capturing
       gestures" which will appear. in the webcam feed you will see a
       green window (inside which you will have to do your gesture) and a
       counter that counts the number of pictures stored.

python create_gestures.py

    3. press 'c' when you are ready with your gesture. capturing gesture
       will begin after a few seconds. move your hand a little bit here
       and there. you can pause capturing by pressing 'c' and resume it by
       pressing 'c'. capturing resumes after a few secondafter the counter
       reaches 1200 the window will close automatically.

   after capturing all the gestures you can flip the images using
python flip_images.py

    4. when you are done adding new gestures run the load_images.py file
       once. you do not need to run this file again until and unless you
       add a new gesture.

python load_images.py

displaying all gestures

    1. to see all the gestures that are stored in 'gestures/' folder run
       this command

python display_all_gestures.py

training a model

    1. so training can be done with either tensorflow or keras. if you
       want to train using tensorflow then run the id98_tf.py file. if you
       want to train using keras then use the id98_keras.py file.

python id98_tf.py
python id98_keras.py

    2. if you use tensorflow you will have the checkpoints and the
       metagraph file in the tmp/id98_model3 folder.
    3. if you use keras you will have the model in the root directory by
       the name id98_model_keras2.h5.

   you do not need to retrain your model every time. in case you added or
   removed a gesture then you need to retrain it.

get model reports

    1. to get the classification reports about the model make sure you
       have test_images and test_labels file which are generated by
       load_images.py. in case you do not have them run load_images.py
       file again. then run this file

python get_model_reports.py

    2. you will get the confusion matrix, f scores, precision and recall
       for the predictions by the model.

testing gestures

   before going into much details i would like to tell that i was not able
   to use the model trained using tensorflow. that is because i do not
   know how to use it. i tried using the predict() function of the
   estimator api but that loads the parameters into memory every time it
   is called which is a huge overhead. please help me if you can with
   this. the functions for prediction using tf is tf_predict() which you
   will find in the recognize_gesture.py file but it is never used. this
   is why i ended up using keras' model, as the loading the model into
   memory and using it for prediction is super easy.
    1. first set your hand histogram. 0. watch the video guide for setting
       the hand histogram [96]here. you do not need to do it again if you
       have already done it. but you do need to do it if the lighting
       conditions change. to do so type the command given below and follow
       the instructions below.

python set_hand_hist.py

     * a windows "set hand histogram" will appear.
     * "set hand histogram" will have 50 squares (5x10).
     * put your hand in those squares. make sure your hand covers all the
       squares.
     * press 'c'. 1 other window will appear "thresh".
     * on pressing 'c' only white patches corresponding to the parts of
       the image which has your skin color should appear on the "thresh"
       window.
     * make sure all the squares are covered by your hand.
     * in case you are not successful then move your hand a little bit and
       press 'c' again. repeat this until you get a good histogram.
     * after you get a good histogram press 's' to save the histogram. all
       the windows close.

    2. for recognition start the recognize_gesture.py file.

python recognize_gesture.py

    3. you will have a small green box inside which you need to do your
       gestures.

using fun_util.py

   here is where you will have all the fun.
    1. first set your hand histogram. you do not need to do it again if
       you have already done it. but you do need to do it if the lighting
       conditions change. to do so type the command given below and follow
       the instructions below.

python set_hand_hist.py

     * a windows "set hand histogram" will appear.
     * "set hand histogram" will have 50 squares (5x10).
     * put your hand in those squares.
     * press 'c'. 2 other windows will appear. "res" and "thresh".
     * on pressing 'c' only the parts of the image which has your skin
       color should appear on the "res" window. white patches
       corresponding to this should appear on the "thresh" window.
     * in case you are not successful then move your hand a little bit and
       press 'c' again. repeat this until you get a good histogram.
     * after you get a good histogram press 's' to save the histogram. all
       the windows close.

    2. start the file.

python fun_util.py

text mode (press 't' to go to text mode)

    1. in text mode you can create your own words using fingerspellings or
       use the predefined gestures.
    2. the text on screen will be converted to speech on removing your
       hand from the green box
    3. make sure you keep the same gesture on the green box for 15 frames
       or else the gesture will not be converted to text.

calculator mode (press 'c' to go to calculator mode)

    1. to confirm a digit make sure you keep the same gesture for 20
       frames. on successful confirmation, the number will appear in the
       vertical center of the black part of the window.
    2. to confirm a number make the "best of luck" gesture and keep in the
       green box for 25 frames. you will get used to the timing :p.
    3. you can have any number of digits for both first number and second
       number.
    4. currently there are 10 operators.
    5. during operator selection, 1 means '+', 2 means '-', 3 means '*', 4
       means '/', 5 means '%', 6 means '**', 7 means '>>' or right shift
       operator, 8 means '<<' or left shift operator, 9 means '&' or
       bitwise and and 0 means '|' or bitwise or.

got a question?

   if you have any questions that are bothering you please contact me on
   my [97]facebook profile. just do not ask me questions like where do i
   live, who do i work for etc. also no questions like what does this line
   do. if you think a line is redundant or can be removed to make the
   program better then you can obviously ask me or make a pull request.

how to cite

   saha, d.. (2018, may 9). sign-language (version 1). figshare.
   [98]https://doi.org/10.6084/m9.figshare.6241901.v1a very simple id98
   project.

     *    2019 github, inc.
     * [99]terms
     * [100]privacy
     * [101]security
     * [102]status
     * [103]help

     * [104]contact github
     * [105]pricing
     * [106]api
     * [107]training
     * [108]blog
     * [109]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [110]reload to refresh your
   session. you signed out in another tab or window. [111]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/evilport2/sign-language/commits/master.atom
   3. https://github.com/evilport2/sign-language#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/evilport2/sign-language
  32. https://github.com/join
  33. https://github.com/login?return_to=/evilport2/sign-language
  34. https://github.com/evilport2/sign-language/watchers
  35. https://github.com/login?return_to=/evilport2/sign-language
  36. https://github.com/evilport2/sign-language/stargazers
  37. https://github.com/login?return_to=/evilport2/sign-language
  38. https://github.com/evilport2/sign-language/network/members
  39. https://github.com/evilport2
  40. https://github.com/evilport2/sign-language
  41. https://github.com/evilport2/sign-language
  42. https://github.com/evilport2/sign-language/issues
  43. https://github.com/evilport2/sign-language/pulls
  44. https://github.com/evilport2/sign-language/projects
  45. https://github.com/evilport2/sign-language/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/evilport2/sign-language/commits/master
  48. https://github.com/evilport2/sign-language/branches
  49. https://github.com/evilport2/sign-language/releases
  50. https://github.com/evilport2/sign-language/graphs/contributors
  51. https://github.com/evilport2/sign-language/find/master
  52. https://github.com/evilport2/sign-language/archive/master.zip
  53. https://github.com/login?return_to=https://github.com/evilport2/sign-language
  54. https://github.com/join?return_to=/evilport2/sign-language
  55. https://desktop.github.com/
  56. https://desktop.github.com/
  57. https://developer.apple.com/xcode/
  58. https://visualstudio.github.com/
  59. https://github.com/evilport2/sign-language/commit/7af50aaf5050e116e749cf30ad2363868e1228dc
  60. https://github.com/evilport2/sign-language/commit/7af50aaf5050e116e749cf30ad2363868e1228dc
  61. https://github.com/evilport2/sign-language/tree/7af50aaf5050e116e749cf30ad2363868e1228dc
  62. https://github.com/evilport2/sign-language/tree/master/__pycache__
  63. https://github.com/evilport2/sign-language/commit/f5f9f15e55a8758d4666a330502d5a6901d78bdc
  64. https://github.com/evilport2/sign-language/tree/master/gestures
  65. https://github.com/evilport2/sign-language/commit/f5f9f15e55a8758d4666a330502d5a6901d78bdc
  66. https://github.com/evilport2/sign-language/tree/master/tmp/id98_model3
  67. https://github.com/evilport2/sign-language/blob/master/.travis.yml
  68. https://github.com/evilport2/sign-language/blob/master/readme.md
  69. https://github.com/evilport2/sign-language/blob/master/id98_keras.py
  70. https://github.com/evilport2/sign-language/blob/master/id98_model_keras2.h5
  71. https://github.com/evilport2/sign-language/blob/master/id98_tf.py
  72. https://github.com/evilport2/sign-language/blob/master/confusion_matrix.png
  73. https://github.com/evilport2/sign-language/blob/master/create_gestures.py
  74. https://github.com/evilport2/sign-language/blob/master/display_all_gestures.py
  75. https://github.com/evilport2/sign-language/blob/master/flip_images.py
  76. https://github.com/evilport2/sign-language/blob/master/full_img.jpg
  77. https://github.com/evilport2/sign-language/blob/master/fun_util.py
  78. https://github.com/evilport2/sign-language/commit/7af50aaf5050e116e749cf30ad2363868e1228dc
  79. https://github.com/evilport2/sign-language/blob/master/gesture_db.db
  80. https://github.com/evilport2/sign-language/blob/master/get_model_reports.py
  81. https://github.com/evilport2/sign-language/blob/master/hist
  82. https://github.com/evilport2/sign-language/blob/master/load_images.py
  83. https://github.com/evilport2/sign-language/blob/master/model.png
  84. https://github.com/evilport2/sign-language/blob/master/recognize_gesture.py
  85. https://github.com/evilport2/sign-language/blob/master/requirements_cpu.txt
  86. https://github.com/evilport2/sign-language/blob/master/requirements_gpu.txt
  87. https://github.com/evilport2/sign-language/blob/master/set_hand_hist.py
  88. https://zenodo.org/badge/latestdoi/122105084
  89. https://www.tensorflow.org/get_started/
  90. https://machinelearningmastery.com/
  91. https://www.tensorflow.org/tutorials/layers
  92. https://youtu.be/jnz7ofah1fg
  93. https://tensorflow.org/
  94. https://keras.io/
  95. https://youtu.be/kyfbleydmw4
  96. https://youtu.be/kyfbleydmw4
  97. http://www.facebook.com/dibakar.saha.750
  98. https://doi.org/10.6084/m9.figshare.6241901.v1a
  99. https://github.com/site/terms
 100. https://github.com/site/privacy
 101. https://github.com/security
 102. https://githubstatus.com/
 103. https://help.github.com/
 104. https://github.com/contact
 105. https://github.com/pricing
 106. https://developer.github.com/
 107. https://training.github.com/
 108. https://github.blog/
 109. https://github.com/about
 110. https://github.com/evilport2/sign-language
 111. https://github.com/evilport2/sign-language

   hidden links:
 113. https://github.com/
 114. https://github.com/evilport2/sign-language
 115. https://github.com/evilport2/sign-language
 116. https://github.com/evilport2/sign-language
 117. https://help.github.com/articles/which-remote-url-should-i-use
 118. https://github.com/evilport2/sign-language#sign-language
 119. https://github.com/evilport2/sign-language#note
 120. https://github.com/evilport2/sign-language#what-i-did-here
 121. https://github.com/evilport2/sign-language#outcome
 122. https://github.com/evilport2/sign-language#requirements
 123. https://github.com/evilport2/sign-language#installing-the-requirements
 124. https://github.com/evilport2/sign-language#how-to-use-this-repo
 125. https://github.com/evilport2/sign-language#creating-a-gesture
 126. https://github.com/evilport2/sign-language#displaying-all-gestures
 127. https://github.com/evilport2/sign-language#training-a-model
 128. https://github.com/evilport2/sign-language#get-model-reports
 129. https://github.com/evilport2/sign-language#testing-gestures
 130. https://github.com/evilport2/sign-language#using-fun_utilpy
 131. https://github.com/evilport2/sign-language#text-mode-press-t-to-go-to-text-mode
 132. https://github.com/evilport2/sign-language#calculator-mode-press-c-to-go-to-calculator-mode
 133. https://github.com/evilport2/sign-language#got-a-question
 134. https://github.com/evilport2/sign-language#how-to-cite
 135. https://github.com/
