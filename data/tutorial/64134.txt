   #[1]articles from distill

   [2]distill

   [3]about [4]prize [5]submit

                    the building blocks of interpretability

   interpretability techniques are normally studied in isolation.
   we explore the powerful interfaces that arise when you combine them         
   and the rich structure of this combinatorial space.

authors

affiliations

   [6]chris olah

   [7]google brain

   [8]arvind satyanarayan

   [9]google brain

   [10]ian johnson

   [11]google cloud

   [12]shan carter

   [13]google brain

   [14]ludwig schubert

   [15]google brain

   [16]katherine ye

   [17]cmu

   [18]alexander mordvintsev

   [19]google research

published

   march 6, 2018

doi

   [20]10.23915/distill.00010

   with the growing success of neural networks, there is a corresponding
   need to be able to explain their decisions         including building
   confidence about how they will behave in the real-world, detecting
   model bias, and for scientific curiosity. in order to do so, we need to
   both construct deep abstractions and reify (or instantiate) them in
   rich interfaces . with a few exceptions , existing work on
   interpretability fails to do these in concert.

   the machine learning community has primarily focused on developing
   powerful methods, such as [21]feature visualization , attribution , and
   id84 , for reasoning about neural networks.
   however, these techniques have been studied as isolated threads of
   research, and the corresponding work of reifying them has been
   neglected. on the other hand, the human-computer interaction community
   has begun to explore rich user interfaces for neural networks , but
   they have not yet engaged deeply with these abstractions. to the extent
   these abstractions have been used, it has been in fairly standard ways.
   as a result, we have been left with impoverished interfaces (e.g.,
   saliency maps or correlating abstract neurons) that leave a lot of
   value on the table. worse, many interpretability techniques have not
   been fully actualized into abstractions because there has not been
   pressure to make them generalizable or composable.

   in this article, we treat existing interpretability methods as
   fundamental and composable building blocks for rich user interfaces. we
   find that these disparate techniques now come together in a unified
   grammar, fulfilling complementary roles in the resulting interfaces.
   moreover, this grammar allows us to systematically explore the space of
   interpretability interfaces, enabling us to evaluate whether they meet
   particular goals. we will present interfaces that show what the network
   detects and explain how it develops its understanding, while keeping
   the amount of information human-scale. for example, we will see how a
   network looking at a labrador retriever detects floppy ears and how
   that influences its classification.
   our interfaces are speculative and one might wonder how reliable they
   are. rather than address this point piecemeal, we dedicate a section to
   it at the end of the article.

   in this article, we use googlenet, an image classification model, to
   demonstrate our interface ideas because its neurons seem unusually
   semantically meaningful.we   re actively investigating why this is, and
   hope to uncover principles for designing interpretable models. in the
   meantime, while we demonstrate our techniques on googlenet, we provide
   code for you to try them on other models. although here we   ve made a
   specific choice of task and network, the basic abstractions and
   patterns for combining them that we present can be applied to neural
   networks in other domains.

making sense of hidden layers

   much of the recent work on interpretability is concerned with a neural
   network   s input and output layers. arguably, this focus is due to the
   clear meaning these layers have: in id161, the input layer
   represents values for the red, green, and blue color channels for every
   pixel in the input image, while the output layer consists of class
   labels and their associated probabilities.

   however, the power of neural networks lies in their hidden layers         at
   every layer, the network discovers a new representation of the input.
   in id161, we use neural networks that run the same feature
   detectors at every position in the image. we can think of each layer   s
   learned representation as a three-dimensional cube. each cell in the
   cube is an activation, or the amount a neuron fires. the x- and y-axes
   correspond to positions in the image, and the z-axis is the channel (or
   detector) being run.

   the cube of activations that a neural network for id161
   develops at each hidden layer. different slices of the cube allow us to
   target the activations of individual neurons, spatial positions, or
   channels.

   to make a semantic dictionary, we pair every neuron activation with a
   visualization of that neuron and sort them by the magnitude of the
   activation. this marriage of activations and feature visualization
   changes our relationship with the underlying mathematical object.
   activations now map to iconic representations, instead of abstract
   indices, with many appearing to be similar to salient human ideas, such
   as    floppy ear,       dog snout,    or    fur.   
   we use optimization-based feature visualization to avoid spurious
   correlation, but one could use other methods.

   semantic dictionaries are powerful not just because they move away from
   meaningless indices, but because they express a neural network   s
   learned abstractions with canonical examples. with image
   classification, the neural network learns a set of visual abstractions
   and thus images are the most natural symbols to represent them. were we
   working with audio, the more natural symbols would most likely be audio
   clips. this is important because when neurons appear to correspond to
   human ideas, it is tempting to reduce them to words. doing so, however,
   is a lossy operation         even for familiar abstractions, the network may
   have learned a deeper nuance. for instance, googlenet has multiple
   floppy ear detectors that appear to detect slightly different levels of
   droopiness, length, and surrounding context to the ears. there also may
   exist abstractions which are visually familiar, yet that we lack good
   natural language descriptions for: for example, take the particular
   column of shimmering light where sun hits rippling water. moreover, the
   network may learn new abstractions that appear alien to us         here,
   natural language would fail us entirely! in general, canonical examples
   are a more natural way to represent the foreign abstractions that
   neural networks learn than native human language.

   by bringing meaning to hidden layers, semantic dictionaries set the
   stage for our existing interpretability techniques to be composable
   building blocks. as we shall see, just like their underlying vectors,
   we can apply id84 to them. in other cases, semantic
   dictionaries allow us to push these techniques further. for example,
   besides the one-way attribution that we currently perform with the
   input and output layers, semantic dictionaries allow us to attribute
   to-and-from specific hidden layers. in principle, this work could have
   been done without semantic dictionaries but it would have been unclear
   what the results meant.
   while we introduce semantic dictionaries in terms of neurons, they can
   be used with any basis of activations. we will explore this more later.

what does the network see?

   applying this technique to all the activation vectors allows us to not
   only see what the network detects at each position, but also what the
   network understands of the input image as a whole.

   and, by working across layers (eg.    mixed3a   ,    mixed4d   ), we can
   observe how the network   s understanding evolves: from detecting edges
   in earlier layers, to more sophisticated shapes and object parts in the
   latter.

   these visualizations, however, omit a crucial piece of information: the
   magnitude of the activations. by scaling the area of each cell by the
   magnitude of the activation vector, we can indicate how strongly the
   network detected features at that position:

how are concepts assembled?

   feature visualization helps us answer what the network detects, but it
   does not answer how the network assembles these individual pieces to
   arrive at later decisions, or why these decisions were made.

   attribution is a set of techniques that answers such questions by
   explaining the relationships between neurons. there are a wide variety
   of approaches to attribution but, so far, there doesn   t seem to be a
   clear right answer. in fact, there   s reason to think that all our
   present answers aren   t quite right . we think there   s a lot of
   important research to be done on attribution methods, but for the
   purposes of this article the exact approach taken to attribution
   doesn   t matter. we use a fairly simple method, linearly approximating
   the relationshipwe do attribution by linear approximation in all of our
   interfaces. that is, we estimate the effect of a neuron on the output
   is its activation times the rate at which increasing its activation
   increases the output. when we talk about a linear combination of
   activations, the attribution can be thought of as the linear
   combination of the attributions of the units, or equivalently as the
   dot product between the activation of that combination and the
   gradient.
   for spatial attribution, we do an additional trick. googlenet   s strided
   max pooling introduces a lot of noise and checkerboard patterns to it   s
   gradients. to avoid our interface demonstrations being dominated by
   this noise, we (a) do a relaxation of the gradient of max pooling,
   distributing gradient to inputs proportional to their activation
   instead of winner takes all and (b) cancel out the checkerboard
   patterns.
   the notebooks attached to diagrams provide reference implementations.,
   but could easily substitute in essentially any other technique. future
   improvements to attribution will, of course, correspondingly improve
   the interfaces built on top of them.

  spatial attribution with saliency maps

   the most common interface for attribution is called a saliency map         a
   simple heatmap that highlights pixels of the input image that most
   caused the output classification. we see two weaknesses with this
   current approach.

   first, it is not clear that individual pixels should be the primary
   unit of attribution. the meaning of each pixel is extremely entangled
   with other pixels, is not robust to simple visual transforms (e.g.,
   brightness, contrast, etc.), and is far-removed from high-level
   concepts like the output class. second, traditional saliency maps are a
   very limited type of interface         they only display the attribution for
   a single class at a time, and do not allow you to probe into individual
   points more deeply. as they do not explicitly deal with hidden layers,
   it has been difficult to fully explore their design space.

   we instead treat attribution as another user interface building block,
   and apply it to the hidden layers of a neural network. in doing so, we
   change the questions we can pose. rather than asking whether the color
   of a particular pixel was important for the    labrador retriever   
   classification, we instead ask whether the high-level idea detected at
   that position (such as    floppy ear   ) was important. this approach is
   similar to what class activation mapping (cam) methods do but, because
   they interpret their results back onto the input image, they miss the
   opportunity to communicate in terms of the rich behavior of a network   s
   hidden layers.

   the above interface affords us a more flexible relationship with
   attribution. to start, we perform attribution from each spatial
   position of each hidden layer shown to all 1,000 output classes. in
   order to visualize this thousand-dimensional vector, we use
   id84 to produce a multi-directional saliency map.
   overlaying these saliency maps on our magnitude-sized activation grids
   provides an information scent over attribution space. the activation
   grids allow us to anchor attribution to the visual vocabulary our
   semantic dictionaries first established. on hover, we update the legend
   to depict attribution to the output classes (i.e., which classes does
   this spatial position most contribute to?).

   perhaps most interestingly, this interface allows us to interactively
   perform attribution between hidden layers. on hover, additional
   saliency maps mask the hidden layers, in a sense shining a light into
   their black boxes. this type of layer-to-layer attribution is a prime
   example of how carefully considering interface design drives the
   generalization of our existing abstractions for interpretability.

   with this diagram, we have begun to think of attribution in terms of
   higher-level concepts. however, at a particular position, many concepts
   are being detected together and this interface makes it dif   cult to
   split them apart. by continuing to focus on spatial positions, these
   concepts remain entangled.

  channel attribution

   saliency maps implicitly slice our cube of activations by applying
   attribution to the spatial positions of a hidden layer. this aggregates
   over all channels and, as a result, we cannot tell which specific
   detectors at each position most contributed to the final output
   classification.

   an alternate way to slice the cube is by channels instead of spatial
   locations. doing so allows us to perform channel attribution: how much
   did each detector contribute to the final output? (this approach is
   similar to contemporaneous work by kim et al., who do attribution to
   learned combination of channels.)

   this diagram is analogous to the previous one we saw: we conduct
   layer-to-layer attribution but this time over channels rather than
   spatial positions. once again, we use the icons from our semantic
   dictionary to represent the channels that most contribute to the final
   output classification. hovering over an individual channel displays a
   heatmap of its activations overlaid on the input image. the legend also
   updates to show its attribution to the output classes (i.e., what are
   the top classes this channel supports?). clicking a channel allows us
   to drill into the layer-to-layer attributions, identifying the channels
   at lower layers that most contributed as well as the channels at higher
   layers that are most supported.

   while these diagrams focus on layer-to-layer attribution, it can still
   be valuable to focus on a single hidden layer. for example, the teaser
   figure allows us to evaluate hypotheses for why one class succeeded
   over the other.

   attribution to spatial locations and channels can reveal powerful
   things about a model, especially when we combine them together.
   unfortunately, this family of approaches is burdened by two significant
   problems. on the one hand, it is very easy to end up with an
   overwhelming amount of information: it would take hours of human
   auditing to understand the long-tail of channels that slightly impact
   the output. on the other hand, both the aggregations we have explored
   are extremely lossy and can miss important parts of the story. and,
   while we could avoid lossy aggregation by working with individual
   neurons, and not aggregating at all, this explodes the first problem
   combinatorially.

making things human-scale

   in previous sections, we   ve considered three ways of slicing the cube
   of activations: into spatial activations, channels, and individual
   neurons. each of these has major downsides. if one only uses spatial
   activations or channels, they miss out on very important parts of the
   story. for example it   s interesting that the floppy ear detector helped
   us classify an image as a labrador retriever, but it   s much more
   interesting when that   s combined with the locations that fired to do
   so. one can try to drill down to the level of neurons to tell the whole
   story, but the tens of thousands of neurons are simply too much
   information. even the hundreds of channels, before being split into
   individual neurons, can be overwhelming to show users!

   if we want to make useful interfaces into neural networks, it isn   t
   enough to make things meaningful. we need to make them human scale,
   rather than overwhelming dumps of information. the key to doing so is
   finding more meaningful ways of breaking up our activations. there is
   good reason to believe that such decompositions exist. often, many
   channels or spatial positions will work together in a highly correlated
   way and are most useful to think of as one unit. other channels or
   positions will have very little activity, and can be ignored for a
   high-level overview. so, it seems like we ought to be able to find
   better decompositions if we had the right tools.

   there is an entire field of research, called id105, that
   studies optimal strategies for breaking up matrices. by flattening our
   cube into a matrix of spatial locations and channels, we can apply
   these techniques to get more meaningful groups of neurons. these groups
   will not align as naturally with the cube as the groupings we
   previously looked at. instead, they will be combinations of spatial
   locations and channels. moreover, these groups are constructed to
   explain the behavior of a network on a particular image. it would not
   be effective to reuse the same groupings on another image; each image
   requires calculating a unique set of groups.

   in addition to naturally slicing a hidden layer   s cube of activations
   into neurons, spatial locations, or channels, we can also consider more
   arbitrary groupings of locations and channels.

   the groups that come out of this factorization will be the atoms of the
   interface a user works with. unfortunately, any grouping is inherently
   a tradeoff between reducing things to human scale and, because any
   aggregation is lossy, preserving information. id105 lets
   us pick what our groupings are optimized for, giving us a better
   tradeoff than the natural groupings we saw earlier.

   the goals of our user interface should influence what we optimize our
   id105 to prioritize. for example, if we want to
   prioritize what the network detected, we would want the factorization
   to fully describe the activations. if we instead wanted to prioritize
   what would change the network   s behavior, we would want the
   factorization to fully describe the gradient. finally, if we want to
   prioritize what caused the present behavior, we would want the
   factorization to fully describe the attributions. of course, we can
   strike a balance between these three objectives rather than optimizing
   one to the exclusion of the others.

   in the following diagram, we   ve constructed groups that prioritize the
   activations, by factorizing the activations most id105
   algorithms and libraries are set up to minimize the mean squared error
   of the reconstruction of a matrix you give them. there are ways to hack
   such libraries to achieve more general objectives through clever
   manipulations of the provided matrix, as we will see below. more
   broadly, id105 is an optimization problem, and with
   custom tools you can achieve all sorts of custom factorizations. with
   non-negative id105 as the name suggests, non-negative
   id105 (nmf) constrains its factors to be positive. this
   is fine for the activations of a relu network, which must be positive
   as well. our experience is that the groups we get from nmf seem more
   independent and semantically meaningful than those without this
   constraint. because of this constraints, groups from nmf are a less
   efficient at representing the activations than they would be without,
   but our experience is that they seem more independent and semantically
   meaningful.  . notice how the overwhelmingly large number of neurons
   has been reduced to a small set of groups, concisely summarizing the
   story of the neural network.

   this figure only focuses at a single layer but, as we saw earlier, it
   can be useful to look across multiple layers to understand how a neural
   network assembles together lower-level detectors into higher-level
   concepts.

   the groups we constructed before were optimized to understand a single
   layer independent of the others. to understand multiple layers
   together, we would like each layer   s factorization to be
      compatible            to have the groups of earlier layers naturally compose
   into the groups of later layers. this is also something we can optimize
   the factorization for we formalize this    compatibility    in a manner
   described below, although we   re not confident it   s the best
   formalization and won   t be surprised if it is superseded in future
   work.
   consider the attribution from every neuron in the layer to the set of n
   groups we want it to be compatible with. the basic idea is to split
   each entry in the activation matrix into n entries on the channel
   dimension, spreading the values proportional to the absolute value of
   its attribution to the corresponding group. any factorization of this
   matrix induces a factorization of the original matrix by collapsing the
   duplicated entries in the column factors. however, the resulting
   factorization tries to create separate factors when the activation of
   the same channel has different attributions in different places.  .

   in this section, we recognize that the way in which we break apart the
   cube of activations is an important interface decision. rather than
   resigning ourselves to the natural slices of the cube of activations,
   we construct more optimal groupings of neurons. these improved
   groupings are both more meaningful and more human-scale, making it less
   tedious for users to understand the behavior of the network.

   our visualizations have only begun to explore the potential of
   alternate bases in providing better atoms for understanding neural
   networks. for example, while we focus on creating smaller numbers of
   directions to explain individual examples, there   s recently been
   exciting work finding    globally    meaningful directions          such bases
   could be especially helpful when trying to understand multiple examples
   at a time, or in comparing models. the recent [22]nips disentangling
   workshop provides other promising directions. we   re excited to see a
   venue for this developing area of research.

the space of interpretability interfaces

   the interface ideas presented in this article combine building blocks
   such as feature visualization and attribution. composing these pieces
   is not an arbitrary process, but rather follows a structure based on
   the goals of the interface. for example, should the interface emphasize
   what the network recognizes, prioritize how its understanding develops,
   or focus on making things human-scale. to evaluate such goals, and
   understand the tradeoffs, we need to be able to systematically consider
   possible alternatives.

   we can think of an interface as a union of individual elements.

   each element displays a specific type of content (e.g., activations or
   attribution) using a particular style of presentation (e.g., feature
   visualization or traditional information visualization). this content
   lives on substrates defined by how given layers of the network are
   broken apart into atoms, and may be transformed by a series of
   operations (e.g., to filter it or project it onto another substrate).
   for example, our semantic dictionaries use feature visualization to
   display the activations of a hidden layer's neurons.

   one way to represent this way of thinking is with a formal grammar, but
   we find it helpful to think about the space visually. we can represent
   the network   s substrate (which layers we display, and how we break them
   apart) as a grid, with the content and style of presentation plotted on
   this grid as points and connections.
   [empty.svg]

   this setup gives us a framework to begin exploring the space of
   interpretability interfaces step by step. for instance, let us consider
   our teaser figure again. its goal is to help us compare two potential
   classifications for an input image.
   [teaser.svg]

   1. feature visualization [teaser-1.png]
   to understand a classification, we focus on the channels of the mixed4d
   layer. feature visualization makes these channels meaningful.
   2. filter by output attribution [teaser-2.png]
   next, we filter for specific classes by calculating the output
   attribution.
   3. drill down on hover [teaser-3.png]
   hovering over channels, we get a heatmap of spatial activations.

   in this article, we have only scratched the surface of possibilities.
   there are lots of combinations of our building blocks left to explore,
   and the design space gives us a way to do so systematically.

   moreover, each building block represents a broad class of techniques.
   our interfaces take only one approach but, as we saw in each section,
   there are a number of alternatives for feature visualization,
   attribution, and id105. an immediate next step would be
   to try using these alternate techniques, and research ways to improve
   them.

   finally, this is not the complete set of building blocks; as new ones
   are discovered, they expand the space. for example, koh & liang.
   suggest ways of understanding the influence of dataset examples on
   model behavior . we can think of dataset examples as another substrate
   in our design space, thus becoming another building block that fully
   composes with the others. in doing so, we can now imagine interfaces
   that not only allow us to inspect the influence of dataset examples on
   the final output classification (as koh & liang proposed), but also how
   examples influence the features of hidden layers, and how they
   influence the relationship between these features and the output. for
   example, if we consider our    labrador retriever    image, we can not only
   see which dataset examples most influenced the model to arrive at this
   classification, but also which dataset examples most caused the    floppy
   ear    detectors to fire, and which dataset examples most caused these
   detectors to increase the    labrador retriever    classification.
   [dataset.svg]

   a new substrate. an interface to understand how dataset examples
   influence the output classification, as presented by koh & liang an
   interface showing how examples influence the channels of hidden layers.
   an interface for identifying which dataset examples most caused
   particular detectors to increase the output classification.

   beyond interfaces for analyzing model behavior, if we add model
   parameters as a substrate, the design space now allows us to consider
   interfaces for taking action on neural networks.note that essentially
   all our interpretability techniques are differentiable, so you can
   backprop through them. while most models today are trained to optimize
   simple objective functions that one can easily describe, many of the
   things we   d like models to do in the real world are subtle, nuanced,
   and hard to describe mathematically. an extreme example of the subtle
   objective problem is something like    creating interesting art   , but
   much more mundane examples arise more or less whenever humans are
   involved. one very promising approach to training models for these
   subtle objectives is learning from human feedback . however, even with
   human feedback, it may still be hard to train models to behave the way
   we want if the problematic aspect of the model doesn   t surface strongly
   in the training regime where humans are giving feedback. there are lots
   of reasons why problematic behavior may not surface or may be hard for
   an evaluator to give feedback on. for example, discrimination and bias
   may be subtly present throughout the model   s behavior, such that it   s
   hard for a human evaluator to critique. or the model may be making a
   decision in a way that has problematic consequences, but those
   consequences never play out in the problems we   re training it on. human
   feedback on the model   s decision making process, facilitated by
   interpretability interfaces, could be a powerful solution to these
   problems. it might allow us to train models not just to make the right
   decisions, but to make them for the right reasons. (there is however a
   danger here: we are optimizing our model to look the way we want in our
   interface         if we aren   t careful, this may lead to the model fooling
   us!related ideas have occasionally been discussed under the term
      cognitive steganography.   )

   another exciting possibility is interfaces for comparing multiple
   models. for instance, we might want to see how a model evolves during
   training, or how it changes when you transfer it to a new task. or, we
   might want to understand how a whole family of models compares to each
   other. existing work has primarily focused on comparing the output
   behavior of models but more recent work is starting to explore
   comparing their internal representations as well. one of the unique
   challenges of this work is that we may want to align the atoms of each
   model; if we have completely different models, can we find the most
   analogous neurons between them? zooming out, can we develop interfaces
   that allow us to evaluate large spaces of models at once?

how trustworthy are these interfaces?

   in order for interpretability interfaces to be effective, we must trust
   the story they are telling us. we perceive two concerns with the set of
   building blocks we currently use. first, do neurons have a relatively
   consistent meaning across different input images, and is that meaning
   accurately reified by feature visualization? semantic dictionaries, and
   the interfaces that build on top of them, are premised off this
   question being true. second, does attribution make sense and do we
   trust any of the attribution methods we presently have?

   much prior research has found that directions in neural networks are
   semantically meaningful. one particularly striking example of this is
      semantic arithmetic    (eg.    king    -    man    +    woman    =    queen   ). we
   explored this question, in depth, for googlenet in our previous article
   and found that many of its neurons seem to correspond to meaningful
   ideas.we validated this in a number of ways: we visualized them without
   a generative model prior, so that the content of the visualizations was
   causally linked to the neuron firing; we inspected the spectrum of
   examples that cause the neuron to fire; and used diversity
   visualizations to try to create different inputs that cause the neuron
   to fire.
   for more details, see [23]the article   s appendix and the guided tour in
   [24]@ch402   s twitter thread. we   re actively investigating why
   googlenet   s neurons seem more meaningful. besides these neurons,
   however, we also found many neurons that do not have as clean a meaning
   including    poly-semantic    neurons that respond to a mixture of salient
   ideas (e.g.,    cat    and    car   ). there are natural ways that interfaces
   could respond to this: we could use diversity visualizations to reveal
   the variety of meanings the neuron can take, or rotate our semantic
   dictionaries so their components are more disentangled. of course, just
   like our models can be fooled, the features that make them up can be
   too         including with adversarial examples . in our view, features do
   not need to be flawless detectors for it to be useful for us to think
   about them as such. in fact, it can be interesting to identify when a
   detector misfires.

   with regards to attribution, recent work suggests that many of our
   current techniques are unreliable. one might even wonder if the idea is
   fundamentally flawed, since a function   s output could be the result of
   non-linear interactions between its inputs. one way these interactions
   can pan out is as attribution being    path-dependent   . a natural
   response to this would be for interfaces to explicitly surface this
   information: how path-dependent is the attribution? a deeper concern,
   however, would be whether this path-dependency dominates the
   attribution. clearly, this is not a concern for attribution between
   adjacent layers because of the simple (essentially linear) mapping
   between them. while there may be technicalities about correlated
   inputs, we believe that attribution is on firm grounding here. and even
   with layers further apart, our experience has been that attribution
   between high-level features at the output is much more consistent than
   attribution to the input         we believe that path-dependence is not a
   dominating concern here.

   model behavior is extremely complex, and our current building blocks
   force us to show only speci   c aspects of it. an important direction for
   future interpretability research will be developing techniques that
   achieve broader coverage of model behavior. but, even with such
   improvements, we anticipate that a key marker of trustworthiness will
   be interfaces that do not mislead. interacting with the explicit
   information displayed should not cause users to implicitly draw
   incorrect assessments about the model (we see a similar principle
   articulated by mackinlay for data visualization). undoubtedly, the
   interfaces we present in this article have room to improve in this
   regard. fundamental research, at the intersection of machine learning
   and human-computer interaction, is necessary to resolve these issues.

   trusting our interfaces is essential for many of the ways we want to
   use interpretability. this is both because the stakes can be high (as
   in safety and fairness) and also because ideas like training models
   with interpretability feedback put our interpretability techniques in
   the middle of an adversarial setting.

conclusion & future work

   there is a rich design space for interacting with enumerative
   algorithms, and we believe an equally rich space exists for interacting
   with neural networks. we have a lot of work left ahead of us to build
   powerful and trustworthy interfaces for interpretability. but, if we
   succeed, interpretability promises to be a powerful tool in enabling
   meaningful human oversight and in building fair, safe, and aligned ai
   systems.

  acknowledgments

   our article was greatly strengthened thanks to the detailed feedback by
   ben poole, emma pierson, jason yosinski, jeff heer, john backus, martin
   wattenberg, matt johnson, and tim hwang.

   we also really appreciated the conversations we   ve had with tom brown,
   catherine olsson, daniel dewey, ajeya cotra, dario amodei, paul
   christiano on the relationship of interpretability to safety; the
   thoughtful comments of michael nielsen, zak stone, zan armstrong and
   anjuli kannan; the support of wolff dobson, jack clark, charina choi,
   jason freidenfelds, christian howard, karoly zsolnai-feher in
   communicating our work to a broader audience; and the supportive
   environment fostered by greg corrado and jeff dean at google brain.

   finally, we really appreciate justin gilmer stepping in as acting
   distill editor of this article, and qiqi yan, guillaume alain, and
   anonymous reviewer c for taking the time to review our article.

  author contributions

   interface design & prototyping. this work began with a number of
   exciting interface demonstrations by alex in 2015, combining feature
   visualizations, activations, and id84. in early
   2017, chris generalized this line of research and combined it with
   attribution. katherine prototyped early interfaces for spatial
   attribution. ian built the neuron group sankey diagram interface.
   arvind created most of the final set of interfaces in the diagram,
   significantly improving them, with extensive design input and polish
   from shan.

   conceptual contributions. many of these ideas have their earliest
   provenance with alex. chris generalized and refined them, and
   integrated attribution. chris, arvind, and ian developed the building
   blocks framing. ian and chris coined the term    semantic dictionaries.   
   arvind and chris crystallized this thinking into a grammar, and
   contextualized it with respect to both the machine learning and
   human-computer interaction communities.

   writing. arvind and chris wrote the text of the article, with
   significant input from ian and shan.

   infrastructure. the core library we use to visualize neural networks,
   [25]lucid, was primarily written by chris, alex, and ludwig. chris
   wrote most of the code used for attribution and id105.
   ludwig created distributed implementations and workflows for generating
   our diagrams.

  discussion and review

   [26]review 1 - qiqi yan
   [27]review 2 - guillaume alain
   [28]review 3 - anonymous

  references

    1. thought as a technology    [29][html]
       nielsen, m., 2016.
    2. visualizing representations: deep learning and human beings
          [30][link]
       olah, c., 2015.
    3. understanding neural networks through deep visualization    [31][pdf]
       yosinski, j., clune, j., nguyen, a., fuchs, t. and lipson, h.,
       2015. arxiv preprint arxiv:1506.06579.
    4. using artificial intelligence to augment human intelligence
          [32][link]
       carter, s. and nielsen, m., 2017. distill. [33]doi:
       10.23915/distill.00009
    5. visualizing higher-layer features of a deep network    [34][pdf]
       erhan, d., bengio, y., courville, a. and vincent, p., 2009.
       university of montreal, vol 1341, pp. 3.
    6. feature visualization    [35][link]
       olah, c., mordvintsev, a. and schubert, l., 2017. distill. [36]doi:
       10.23915/distill.00007
    7. deep inside convolutional networks: visualising image
       classification models and saliency maps    [37][pdf]
       simonyan, k., vedaldi, a. and zisserman, a., 2013. arxiv preprint
       arxiv:1312.6034.
    8. deep neural networks are easily fooled: high confidence predictions
       for unrecognizable images    [38][pdf]
       nguyen, a., yosinski, j. and clune, j., 2015. proceedings of the
       ieee conference on id161 and pattern recognition, pp.
       427--436. [39]doi: 10.1109/cvpr.2015.7298640
    9. inceptionism: going deeper into neural networks    [40][html]
       mordvintsev, a., olah, c. and tyka, m., 2015. google research blog.
   10. plug & play generative networks: conditional iterative generation
       of images in latent space    [41][pdf]
       nguyen, a., clune, j., bengio, y., dosovitskiy, a. and yosinski,
       j., 2016. arxiv preprint arxiv:1612.00005.
   11. visualizing and understanding convolutional networks    [42][pdf]
       zeiler, m.d. and fergus, r., 2014. european conference on computer
       vision, pp. 818--833.
   12. striving for simplicity: the all convolutional net    [43][pdf]
       springenberg, j.t., dosovitskiy, a., brox, t. and riedmiller, m.,
       2014. arxiv preprint arxiv:1412.6806.
   13. grad-cam: why did you say that? visual explanations from deep
       networks via gradient-based localization    [44][pdf]
       selvaraju, r.r., das, a., vedantam, r., cogswell, m., parikh, d.
       and batra, d., 2016. arxiv preprint arxiv:1610.02391.
   14. interpretable explanations of black boxes by meaningful
       perturbation    [45][pdf]
       fong, r. and vedaldi, a., 2017. arxiv preprint arxiv:1704.03296.
   15. patteid56et and patternlrp--improving the interpretability of neural
       networks    [46][pdf]
       kindermans, p., schutt, k.t., alber, m., muller, k. and dahne, s.,
       2017. arxiv preprint arxiv:1705.05598. [47]doi:
       10.1007/978-3-319-10590-1_53
   16. the (un)reliability of saliency methods    [48][pdf]
       kindermans, p., hooker, s., adebayo, j., alber, m., schutt, k.t.,
       dahne, s., erhan, d. and kim, b., 2017. arxiv preprint
       arxiv:1711.00867.
   17. axiomatic attribution for deep networks    [49][pdf]
       sundararajan, m., taly, a. and yan, q., 2017. arxiv preprint
       arxiv:1703.01365.
   18. visualizing data using id167    [50][pdf]
       maaten, l.v.d. and hinton, g., 2008. journal of machine learning
       research, vol 9(nov), pp. 2579--2605.
   19. lstmvis: a tool for visual analysis of hidden state dynamics in
       recurrent neural networks    [51][pdf]
       strobelt, h., gehrmann, s., pfister, h. and rush, a.m., 2018. ieee
       transactions on visualization and computer graphics, vol 24(1), pp.
       667--676. ieee. [52]doi: 10.1109/tvcg.2017.2744158
   20. activis: visual exploration of industry-scale deep neural network
       models    [53][pdf]
       kahng, m., andrews, p.y., kalro, a. and chau, d.h.p., 2018. ieee
       transactions on visualization and computer graphics, vol 24(1), pp.
       88--97. ieee. [54]doi: 10.1109/tvcg.2017.2744718
   21. do convolutional neural networks learn class hierarchy?    [55][pdf]
       bilal, a., jourabloo, a., ye, m., liu, x. and ren, l., 2018. ieee
       transactions on visualization and computer graphics, vol 24(1), pp.
       152--162. ieee. [56]doi: 10.1109/tvcg.2017.2744683
   22. going deeper with convolutions    [57][pdf]
       szegedy, c., liu, w., jia, y., sermanet, p., reed, s., anguelov,
       d., erhan, d., vanhoucke, v., rabinovich, a. and others,, 2015.
       [58]doi: 10.1109/cvpr.2015.7298594
   23. deconvolution and checkerboard artifacts    [59][link]
       odena, a., dumoulin, v. and olah, c., 2016. distill. [60]doi:
       10.23915/distill.00003
   24. learning deep features for discriminative localization    [61][pdf]
       zhou, b., khosla, a., lapedriza, a., oliva, a. and torralba, a.,
       2016. proceedings of the ieee conference on id161 and
       pattern recognition, pp. 2921--2929. [62]doi: 10.1109/cvpr.2016.319
   25. information foraging    [63][link]
       pirolli, p. and card, s., 1999. psychological review, vol 106(4),
       pp. 643. american psychological association. [64]doi:
       10.1037//0033-295x.106.4.643
   26. tcav: relative concept importance testing with linear concept
       activation vectors    [65][pdf]
       kim, b., gilmer, j., viegas, f., erlingsson, u. and wattenberg, m.,
       2017. arxiv preprint arxiv:1711.11279.
   27. svcca: singular vector canonical correlation analysis for deep
       learning dynamics and interpretability    [66][pdf]
       raghu, m., gilmer, j., yosinski, j. and sohl-dickstein, j., 2017.
       advances in neural information processing systems 30, pp.
       6078--6087. curran associates, inc.
   28. net2vec: quantifying and explaining how concepts are encoded by
       filters in deep neural networks
       fong, r. and vedaldi, a., 2018. arxiv preprint arxiv:1801.03454.
   29. understanding black-box predictions via influence functions
          [67][pdf]
       koh, p.w. and liang, p., 2017. international conference on machine
       learning (icml).
   30. deep id23 from human preferences
       christiano, p.f., leike, j., brown, t., martic, m., legg, s. and
       amodei, d., 2017. advances in neural information processing
       systems, pp. 4302--4310.
   31. interactive optimization for steering machine classification
          [68][pdf]
       kapoor, a., lee, b., tan, d. and horvitz, e., 2010. proceedings of
       the sigchi conference on human factors in computing systems, pp.
       1343--1352. [69]doi: 10.1145/1753326.1753529
   32. interacting with predictions: visual inspection of black-box
       machine learning models    [70][pdf]
       krause, j., perer, a. and ng, k., 2016. proceedings of the 2016 chi
       conference on human factors in computing systems, pp. 5686--5697.
       [71]doi: 10.1145/2858036.2858529
   33. modeltracker: redesigning performance analysis tools for machine
       learning    [72][pdf]
       amershi, s., chickering, m., drucker, s.m., lee, b., simard, p. and
       suh, j., 2015. proceedings of the 33rd annual acm conference on
       human factors in computing systems, pp. 337--346. [73]doi:
       10.1145/2702123.2702509
   34. network dissection: quantifying interpretability of deep visual
       representations    [74][pdf]
       bau, d., zhou, b., khosla, a., oliva, a. and torralba, a., 2017.
       id161 and pattern recognition (cvpr), 2017 ieee
       conference on, pp. 3319--3327. [75]doi: 10.1109/cvpr.2017.354
   35. intriguing properties of neural networks    [76][pdf]
       szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d.,
       goodfellow, i. and fergus, r., 2013. arxiv preprint
       arxiv:1312.6199.
   36. efficient estimation of word representations in vector space
          [77][pdf]
       mikolov, t., chen, k., corrado, g. and dean, j., 2013. arxiv
       preprint arxiv:1301.3781.
   37. unsupervised representation learning with deep convolutional
       id3    [78][pdf]
       radford, a., metz, l. and chintala, s., 2015. arxiv preprint
       arxiv:1511.06434.
   38. adversarial manipulation of deep representations    [79][pdf]
       sabour, s., cao, y., faghri, f. and fleet, d.j., 2015. arxiv
       preprint arxiv:1511.05122.
   39. automating the design of graphical presentations of relational
       information    [80][pdf]
       mackinlay, j., 1986. acm transactions on graphics (tog), vol 5(2),
       pp. 110--141. acm. [81]doi: 10.1145/22949.22950

  updates and corrections

   if you see mistakes or want to suggest changes, please [82]create an
   issue on github.

  reuse

   diagrams and text are licensed under creative commons attribution
   [83]cc-by 4.0 with the [84]source available on github, unless noted
   otherwise. the figures that have been reused from other sources don   t
   fall under this license and can be recognized by a note in their
   caption:    figure from       .

  citation

   for attribution in academic contexts, please cite this work as
olah, et al., "the building blocks of interpretability", distill, 2018.

   bibtex citation
@article{olah2018the,
  author = {olah, chris and satyanarayan, arvind and johnson, ian and carter, sh
an and schubert, ludwig and ye, katherine and mordvintsev, alexander},
  title = {the building blocks of interpretability},
  journal = {distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}

   [85]distill is dedicated to clear explanations of machine learning
   [86]about [87]submit [88]prize [89]archive [90]rss [91]github
   [92]twitter      issn 2476-0757

references

   1. https://distill.pub/rss.xml
   2. https://distill.pub/
   3. https://distill.pub/about/
   4. https://distill.pub/prize/
   5. https://distill.pub/journal/
   6. https://colah.github.io/
   7. https://g.co/brain
   8. http://arvindsatya.com/
   9. https://g.co/brain
  10. https://github.com/enjalot
  11. http://cloud.google.com/
  12. http://shancarter.com/
  13. https://g.co/brain
  14. https://schubert.io/
  15. https://g.co/brain
  16. https://cs.cmu.edu/~kqy/
  17. https://cs.cmu.edu/
  18. https://znah.net/
  19. https://research.google.com/
  20. https://doi.org/10.23915/distill.00010
  21. https://distill.pub/2017/feature-visualization/
  22. https://sites.google.com/corp/view/disentanglenips2017
  23. https://distill.pub/2017/feature-visualization/appendix/
  24. https://twitter.com/ch402/status/927968700384153601
  25. https://github.com/tensorflow/lucid
  26. https://github.com/distillpub/post--interpretability-pieces/issues/10
  27. https://github.com/distillpub/post--interpretability-pieces/issues/11
  28. https://github.com/distillpub/post--interpretability-pieces/issues/12
  29. http://cognitivemedium.com/tat/index.html
  30. http://colah.github.io/posts/2015-01-visualizing-representations/
  31. http://yosinski.com/media/papers/yosinski__2015__icml_dl__understanding_neural_networks_through_deep_visualization__.pdf
  32. https://distill.pub/2017/aia/
  33. https://doi.org/10.23915/distill.00009
  34. https://www.researchgate.net/profile/aaron_courville/publication/265022827_visualizing_higher-layer_features_of_a_deep_network/links/53ff82b00cf24c81027da530.pdf
  35. https://distill.pub/2017/feature-visualization
  36. https://doi.org/10.23915/distill.00007
  37. https://arxiv.org/pdf/1312.6034.pdf
  38. https://arxiv.org/pdf/1412.1897.pdf
  39. https://doi.org/10.1109/cvpr.2015.7298640
  40. https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html
  41. https://arxiv.org/pdf/1612.00005.pdf
  42. https://arxiv.org/pdf/1311.2901.pdf
  43. https://arxiv.org/pdf/1412.6806.pdf
  44. https://arxiv.org/pdf/1610.02391.pdf
  45. https://arxiv.org/pdf/1704.03296.pdf
  46. https://arxiv.org/pdf/1705.05598.pdf
  47. https://doi.org/10.1007/978-3-319-10590-1_53
  48. https://arxiv.org/pdf/1711.00867.pdf
  49. https://arxiv.org/pdf/1703.01365.pdf
  50. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  51. https://arxiv.org/pdf/1606.07461.pdf
  52. https://doi.org/10.1109/tvcg.2017.2744158
  53. https://arxiv.org/pdf/1704.01942.pdf
  54. https://doi.org/10.1109/tvcg.2017.2744718
  55. https://arxiv.org/pdf/1710.06501.pdf
  56. https://doi.org/10.1109/tvcg.2017.2744683
  57. https://arxiv.org/pdf/1409.4842.pdf
  58. https://doi.org/10.1109/cvpr.2015.7298594
  59. http://distill.pub/2016/deconv-checkerboard
  60. https://doi.org/10.23915/distill.00003
  61. http://id98localization.csail.mit.edu/zhou_learning_deep_features_cvpr_2016_paper.pdf
  62. https://doi.org/10.1109/cvpr.2016.319
  63. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.5407&rep=rep1&type=pdf
  64. https://doi.org/10.1037//0033-295x.106.4.643
  65. https://arxiv.org/pdf/1711.11279.pdf
  66. http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf
  67. https://arxiv.org/pdf/1703.04730.pdf
  68. http://erichorvitz.com/steering_classification_2010.pdf
  69. https://doi.org/10.1145/1753326.1753529
  70. http://perer.org/papers/adamperer-prospector-chi2016.pdf
  71. https://doi.org/10.1145/2858036.2858529
  72. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/amershi.chi2015.modeltracker.pdf
  73. https://doi.org/10.1145/2702123.2702509
  74. https://arxiv.org/pdf/1704.05796.pdf
  75. https://doi.org/10.1109/cvpr.2017.354
  76. https://arxiv.org/pdf/1312.6199.pdf
  77. https://arxiv.org/pdf/1301.3781.pdf
  78. https://arxiv.org/pdf/1511.06434.pdf
  79. https://arxiv.org/pdf/1511.05122.pdf
  80. http://www2.parc.com/istl/groups/uir/publications/items/uir-1986-02-mackinlay-tog-automating.pdf
  81. https://doi.org/10.1145/22949.22950
  82. https://github.com/distillpub/post--building-blocks/issues/new
  83. https://creativecommons.org/licenses/by/4.0/
  84. https://github.com/distillpub/post--building-blocks
  85. https://distill.pub/
  86. https://distill.pub/about/
  87. https://distill.pub/journal/
  88. https://distill.pub/prize/
  89. https://distill.pub/archive/
  90. https://distill.pub/rss.xml
  91. https://github.com/distillpub
  92. https://twitter.com/distillpub
