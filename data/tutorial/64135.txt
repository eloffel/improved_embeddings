   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [16]go to the profile of nimesh sinha
   [17]nimesh sinha (button) blockedunblock (button) followfollowing
   feb 19, 2018
   [1*vonagevb1gi8dfrmr-u3ew.png]

   long short term memory networks, usually called    lstms    , were
   introduced by hochreiter and schmiduber. these have widely been used
   for id103, id38, id31 and text
   prediction. before going deep into lstm, we should first understand the
   need of lstm which can be explained by the drawback of practical use of
   recurrent neural network (id56). so, lets start with id56.

recurrent neural networks (id56)

   being human, when we watch a movie, we don   t think from scratch every
   time while understanding any event. we rely on the recent experiences
   happening in the movie and learn from them. but, a conventional neural
   network is unable to learn from the previous events because the
   information does not pass from one step to the next. on contrary, id56
   learns information from immediate previous step.

   for example, there is a scene in a movie where a person is in a
   basketball court. we will improvise the basketball activities in the
   future frames: an image of someone running and jumping probably be
   labeled as playing basketball, and an image of someone sitting and
   watching is probably a spectator watching the game.
   [1*dvlb9rtnduhwtri4e2p-bg.png]
   a typical id56 (source:
   [18]http://colah.github.io/posts/2015-08-understanding-lstms/)
   [1*xtke0g6xnmlm8iq4afdp0w.png]

   a typical id56 looks like above-where x(t) is input, h(t) is output and
   a is the neural network which gains information from the previous step
   in a loop. the output of one unit goes into the next one and the
   information is passed.

   but, sometimes we don   t need our network to learn only from immediate
   past information. suppose we want to predict the blank word in the text
       david, a 36-year old man lives in san francisco. he has a female
   friend maria. maria works as a cook in a famous restaurant in new york
   whom he met recently in a school alumni meet. maria told him that she
   always had a passion for _________ . here, we want our network to learn
   from dependency    cook    to predict    cooking. there is a gap between the
   information what we want to predict and from where we want it to get
   predicted . this is called long-term dependency. we can say that
   anything larger than trigram as a long term dependency. unfortunately,
   id56 does not work practically in this situation.

why id56 does not work practically

   during the training of id56, as the information goes in loop again and
   again which results in very large updates to neural network model
   weights. this is due to the accumulation of error gradients during an
   update and hence, results in an unstable network. at an extreme, the
   values of weights can become so large as to overflow and result in nan
   values.the explosion occurs through exponential growth by repeatedly
   multiplying gradients through the network layers that have values
   larger than 1 or vanishing occurs if the values are less than 1.

long short term memory

   the above drawback of id56 pushed the scientists to develop and invent a
   new variant of the id56 model, called long short term memory. lstm can
   solve this problem, because it uses gates to control the memorizing
   process.

   let   s understand the architecture of lstm and compare it with that of
   id56:
   [1*niu_c_fhgtluhjrstkb_4q.png]
   a lstm unit (source :
   [19]http://colah.github.io/posts/2015-08-understanding-lstms)

   the symbols used here have following meaning:

   a) x : scaling of information

   b)+ : adding information

   c)    : sigmoid layer

   d) tanh: tanh layer

   e) h(t-1) : output of last lstm unit

   f) c(t-1) : memory from last lstm unit

   g) x(t) : current input

   h) c(t) : new updated memory

   i) h(t) : current output

why tanh?

   to overcome the vanishing gradient problem, we need a function whose
   second derivative can sustain for a long range before going to zero.
   tanh is a suitable function with the above property.

why sigmoid?

   as sigmoid can output 0 or 1, it can be used to forget or remember the
   information.

   information passes through many such lstm units.there are three main
   components of an lstm unit which are labeled in the diagram:
    1. lstm has a special architecture which enables it to forget the
       unnecessary information .the sigmoid layer takes the input x(t) and
       h(t-1) and decides which parts from old output should be removed
       (by outputting a 0). in our example, when the input is    he has a
       female friend maria   , the gender of    david    can be forgotten
       because the subject has changed to    maria   . this gate is called
       forget gate f(t). the output of this gate is f(t)*c(t-1).
    2. the next step is to decide and store information from the new input
       x(t) in the cell state. a sigmoid layer decides which of the new
       information should be updated or ignored. a tanh layer creates a
       vector of all the possible values from the new input. these two are
       multiplied to update the new cell sate. this new memory is then
       added to old memory c(t-1) to give c(t). in our example, for the
       new input     he has a female friend maria   , the gender of maria will
       be updated. when the input is    maria works as a cook in a famous
       restaurant in new york whom he met recently in a school alumni
       meet   , the words like    famous   ,    school alumni meet    can be ignored
       and words like    cook,    restaurant    and    new york    will be updated.
    3. finally, we need to decide what we   re going to output. a sigmoid
       layer decides which parts of the cell state we are going to output.
       then, we put the cell state through a tanh generating all the
       possible values and multiply it by the output of the sigmoid gate,
       so that we only output the parts we decided to. in our example, we
       want to predict the blank word, our model knows that it is a noun
       related to    cook    from its memory, it can easily answer it as
          cooking   . our model does not learn this answer from the immediate
       dependency, rather it learnt it from long term dependency.

   we just saw that there is a big difference in the architecture of a
   typical id56 and a lstm. in lstm, our model learns what information to
   store in long term memory and what to get rid of.

quick implementation of lstm for sentimental analysis

   here, i used lstm on the reviews data from [20]yelp open dataset for
   id31 using keras.

   this is what my data looks like.
   [1*poz3dcc27oplbt6de-fbsg.png]
   dataset

   i used tokenizer to vectorize the text and convert it into sequence of
   integers after restricting the tokenizer to use only top most common
   2500 words. i used pad_sequences to convert the sequences into 2-d
   numpy array.

   iframe: [21]/media/4bbe47ce2c1381b9a24822d63cb6210c?postid=af410fd85b47

   then, i built my lstm network.there are a few hyper parameters:
    1. embed_dim : the embedding layer encodes the input sequence
       into a sequence of dense vectors of dimension embed_dim.
    2. lstm_out : the lstm transforms the vector sequence into a single
       vector of size lstm_out, containing information about the entire
       sequence.

   the other hyper parameters like dropout, batch_size are similar to that
   of id98.

   i used softmax as activation function.
   [1*gst2fwe_39fgmtc735924q.png]
   lstm network

   iframe: [22]/media/6c8f8fdc71929378c9e1f36eda4370ce?postid=af410fd85b47

   [1*pcpvpf7ebi07rrc0ayfqxw.png]

   now, i fit my model on training set and check the accuracy on
   validation set.

   iframe: [23]/media/5a15f23f41270d3370aeedd4bcd9b1cf?postid=af410fd85b47

   [1*xfpxsnqvb3vc5_jtrl-q3w.png]
   [1*dmzz0dp17eqlcrawftk2aa.png]

   i got a validation accuracy of 86% in just one epoch while running on a
   small dataset which includes all the businesses.

future work:

    1. we can filter the specific businesses like restaurants and then use
       lstm for id31.
    2. we can use much larger dataset with more epochs to increase the
       accuracy.
    3. more hidden dense layers can be used to improve the accuracy. we
       can tune other hyper parameters as well.

conclusion

   lstm outperforms the other models when we want our model to learn from
   long term dependencies. lstm   s ability to forget, remember and update
   the information pushes it one step ahead of id56s.

references and other useful resources:

    1. [24]my github repo
    2. [25]understanding lstm
    3. [26]beginner   s guide to id56 and lstm

   4. [27]exploring lstms

   5. [28]research paper on lstm

     * [29]machine learning
     * [30]lstm
     * [31]neural networks
     * [32]towards data science
     * [33]deep learning

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of nimesh sinha

[35]nimesh sinha

   master   s student in data scientist at university of san francisco, data
   science intern at snaplogic . linkedin:
   [36]www.linkedin.com/in/nimesh-sinha-4b6a1a31/

     (button) follow
   [37]towards data science

[38]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [39]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [40]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/af410fd85b47
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_snrjo7t5a7wn---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@nimesh280?source=post_header_lockup
  17. https://towardsdatascience.com/@nimesh280
  18. http://colah.github.io/posts/2015-08-understanding-lstms/
  19. http://colah.github.io/posts/2015-08-understanding-lstms/
  20. https://www.yelp.com/dataset
  21. https://towardsdatascience.com/media/4bbe47ce2c1381b9a24822d63cb6210c?postid=af410fd85b47
  22. https://towardsdatascience.com/media/6c8f8fdc71929378c9e1f36eda4370ce?postid=af410fd85b47
  23. https://towardsdatascience.com/media/5a15f23f41270d3370aeedd4bcd9b1cf?postid=af410fd85b47
  24. https://github.com/nsinha280/lstm
  25. http://colah.github.io/posts/2015-08-understanding-lstms/
  26. https://deeplearning4j.org/lstm.html
  27. http://blog.echen.me/2017/05/30/exploring-lstms/
  28. http://www.bioinf.jku.at/publications/older/2604.pdf
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/lstm?source=post
  31. https://towardsdatascience.com/tagged/neural-networks?source=post
  32. https://towardsdatascience.com/tagged/towards-data-science?source=post
  33. https://towardsdatascience.com/tagged/deep-learning?source=post
  34. https://towardsdatascience.com/@nimesh280?source=footer_card
  35. https://towardsdatascience.com/@nimesh280
  36. http://www.linkedin.com/in/nimesh-sinha-4b6a1a31/
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/?source=footer_card
  39. https://towardsdatascience.com/
  40. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  42. https://medium.com/p/af410fd85b47/share/twitter
  43. https://medium.com/p/af410fd85b47/share/facebook
  44. https://medium.com/p/af410fd85b47/share/twitter
  45. https://medium.com/p/af410fd85b47/share/facebook
