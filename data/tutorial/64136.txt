   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

[learning note] dropout in recurrent networks         part 1

theoretical foundations

   [14]go to the profile of ceshine lee
   [15]ceshine lee (button) blockedunblock (button) followfollowing
   sep 25, 2017

   i was puzzled by the term    variational dropout for recurrent neural
   networks    when reading through [16]this article from uber engineering
   blog[1]. (it actually turns out to be quite a simple concept.) it made
   me realize how little i know about regularizing recurrent networks, and
   i decided to spend some time figuring it out.

   the first part of this learning note will be focused on the theoretical
   aspect, and the latter one(s) will contained some empirical
   experiments.

bayesian neural network (bnn)

   this post assumes the reader to have basic understandings of [17]the
   differences of the bayesian and frequentist statistics. in short, the
   bayesian approach depends on both prior distribution and likelihood
   function, while the frequentist approach depends only on the likelihood
   function. therefore a bayesian neural network is created when we place
   a prior distribution over a neural network   s weights.

vi and mcmc

   in neural network, the likelihood function is basically a forward pass,
   and generally renders the posterior distribution intractable. there are
   two major approaches to deal with this problem         variational id136
   (vi) and id115 (mcmc)[2].

   variational id136 tries to approximate the posterior distribution
   with tractable parameterized distributions. by trying to minimize the
   id181 between the approximate distribution and
   the posterior distribution, it turns the problem into an optimization
   problem. variational id136 is irredeemably biased, unlike markov
   chain monte carlo when given enough time to converge. however,
   variational id136 requires significantly less amount of computation
   resources, therefore it is more commonly used in bnn. [3]

   in bnn, the approximate distribution is usually further simplified by
   factorizing it into independent distributions for each weight layers
   (mean field approximation). this approximation creates larger bias and
   can be problematic in the deep neural network. [4]

dropout as a bayesian approximation

   gal and ghahramani [5] showed that dropout can be interpreted as a
   variational approximation to the posterior of a bayesian neural network
   (nn). their variational approximating distribution is a mixture of two
   gaussians with small variances, with the mean of one gaussian fixed at
   zero. in other words, optimizing any neural network with dropout is
   equivalent to a form of approximate bayesian id136. [4]

   in this perspective, it is a natural idea to perform dropout at test
   time as a way to sample from the posterior distribution. it is called
   monte carlo dropout (mc dropout).[1, 6] the traditional way of taking
   the expectations of the weights of each layer is called standard
   dropout approximation. the former can provides uncertainty measures.
   the latter can   t.

   empirically, mc dropout outperforms black box variational
   id136(bbvi) of [18]edward[4]:
   [1*4blbrwgf3idnljxonrqqzg.png]
   experiment results by andrew rowan [4]

   rowan [4] explains the mean field vi used by edward cannot capture
   posterior dependencies between network weight layers, while mc dropout
   represents a joint distribution over each layer   s weights. it was shown
   that including correlations between layers improves results for bnn.
     __________________________________________________________________

dropout in recurrent networks

   before gal and ghahramani [6], new dropout masks are created for each
   time step. empirical results have led many to believe that noise added
   to recurrent layers (connections between id56 units) will be amplified
   for long sequences, and drown the signal [7]. consequently, it was
   concluded that dropout should be used with only the inputs and outputs
   of the id56. (see the left part of figure below)
   [1*g4q37g7mlizety7j1b64uw.png]
   left: previous technique right: bayesian technique [6]

   (colored connections represent dropped-out inputs, with different
   colors corresponding to different dropout masks. dashed lines
   correspond to standard connections with no dropout)

   the variant gal and ghahramani [6] proposed is to use the same dropout
   mask at each time step for both inputs, outputs, and recurrent layers.
   and it is equivalent to a form of variational id136 in recurrent
   neural networks, which will be derived in the following sections.

theoretical derivation of variational id56

   (bnn) given weight matrices wi and bias vectors bi for layer i, we
   often place standard matrix gaussian prior distributions over the
   weight matrices, p(wi) = n(0, i) and often assume a point estimate for
   the bias vectors for simplicity.

   the posterior over the weights given our observables x, y is p(  |x, y),
   and is generally intractable. for vi, we need to minimize the kl
   divergence between the approximating distribution q(  ) and the full
   posterior:
   [1*blzc4us21yiq25qxzmfebg.png]

   the above formulation is called evidence lower bound (elbo). the first
   term(expected log likelihood) measures how well samples from
   approximate posterior q(  ) explain data x (reconstruction cost). the
   second term(prior kl) ensures explanation of doesn   t deviate too far
   from our prior beliefs (penalizes complexity). [4]

   now we turn to simple id56 models. derivations for lstm and gru follows
   similarly. given input sequence x = [x_1,    , x_t] of length t, a simple
   id56 is formed by repeated application of a function f_h. this generates
   a hidden state h_t for time step t:
   [1*3btlb6teapcglajuronnka.png]

   this id56 can be viewed as a probabilistic model by regarding    =
   {w_h,u_h,b_h,w_y,b_y} as random variables (following normal prior
   distributions)

   evaluating the expected log likelihood in elbo yields:
   [1*uvg2pcosj0rq0ockhyluig.png]
   recursive function expanded. (h_0 = 0)

   approximate it with monte carlo integration with a single sample:
   [1*fxikazc-n3j3-m9ebty4rw.png]
   an unbiased estimator of each log likelihood

   plug it back to elbo and we get:
   [1*u_klqq5e0kxwrycmd8ekag.png]

   note we sample a new    for each sequence, but each symbol in the
   sequence is passed through the same function with the same   .

   we factorize the approximation distribution over the weight matrices
   and their rows in   . for every weight matrix row w_k the approximating
   distribution is:
   [1*jqzxb7erglm6yqd8pdzc6a.png]
   a mixture of two gaussians with small variances, with the mean of one
   gaussian fixed at zero.

   with m_k variational parameter (row vector), p given in advance (the
   dropout id203), and small     . optimizing the id168 using
   id119 will update m_k, which correspond to a row of the
   weight matrix in the traditional settings.

   the prior kl (q(  )||p(  )) can be approximated as l2 id173 over
   variational parameter m_k.

   predictions can be made via mc dropout:
   [1*-kc_nldsfeac-eltlpflzq.png]

implementation of variational dropout in lstm

   lstm is defined using four gates:    input   ,    forget   ,    output       and
      input modulation   :
   [1*lazerozikny0zlwhblg7eq.png]
   untied-weights lstm

   this parameterization is call untied-weights lstm. the alternative
   parameterization is called tied-weights lstm:
   [1*efetbbgrlvuidg7p3kclfa.png]
   tied-weights lstm

   the approximation distributions q(  ) are different for these two
   parameterization. for variational untied-weights lstm one could use
   different dropout masks for different gates, because q(  ) is placed
   over matrices instead of inputs. this leads to slower forward-pass but
   slight better results than variational tied-weights lstm:
   [1*jejvypuudzfiq4dialvn9w.png]
   variational tied-weights lstm. the dropout mask is shared for all
   four gates.

   in comparison, zaremba et al.[7] use different dropout masks in every
   time step, and does not use dropout on hidden states:
   [1*bcf_tt6xruoi1vt3kzewew.png]
   the previous technique to apply dropout on lstm

id27s dropout

   gal and ghahramani [6] also propose a new way to regularize word
   embedding, in addition to apply dropout on inputs. they suggest dropout
   be used on word type, instead of individual words. that is, randomly
   setting rows of the embedding matrix to zero.

   so, for the sequence    the dog and the cat   , the dropout might yield
               dog and         cat    or    the         and the cat   , but never             dog and the
   cat   .

   this technique can be interpreted as encouraging the model not to
   depend on single words for its output.

weight-dropped lstm

   recently proposed [8], weight-dropped lstm apply dropout to recurrent
   hidden-to-hidden weight matrices (u_i, u_f, u_g, u_o), in hope to
   prevent over-fitting on the recurrent connection.

   from my understanding, weight-dropped lstm is essentially further
   factorizing the approximation distribution over the the elements of
   each row (untied-weight lstm). in addition,    was sampled for each
   mini-batch instead of for each sequence. it also comes with some
   performance bonus. as the dropout operation is applied once to the
   weight matrices, before the forward and backward pass, the impact on
   training speed and any standard (and optimized) black box id56
   implementation can be used.

implementation in keras and pytorch

   for variational dropout, keras has already implemented it in [19]its
   lstm layer use parameter dropout for input dropout (w matrices). use
   parameter recurrent_dropout for hidden state dropout (u matrices). the
   dropout seems to be in untied-weights settings. pytorch does not
   natively support variational dropout, but you can implement it yourself
   by manually iterating through time steps, or borrow code from
   [20]awd-lstm language model (weightdrop with variational=true). it
   seems    was sampled for each mini-batch in these implementations,
   probably for simplicity.

   for id27s dropout, keras seems to once have dropout parameter
   in its embedding layer, but it has been removed for some reason. i   m
   not aware other native way to do it in keras so far. for pytorch, once
   again you can borrow code from [21]awd-lstm language model (see
   [22]embed_regularize.py).

   (2017/09/30 correction: it appears my memory about the keras
   implementation was problematic. please refer to the upcoming part 2 for
   more precise explanations.)

to be continued

   that   s it for the theoretical part. i   ve also done some preliminary
   empirical experiments to evaluate the effect of different dropout
   variants, but i   ll need to find some time to organize them to be more
   coherent and create some visualization.
   [23][learning note] dropout in recurrent networks         part 2
   recurrent dropout implementation in keras and pytorchmedium.com
   [24][learning note] dropout in recurrent networks         part 3
   some empirical result comparisonmedium.com
     __________________________________________________________________

references:

    1. zhu, l., & laptev, n. [25]engineering uncertainty estimation in
       neural networks for time series prediction at uber. uber
       engineering blog.
    2. salimans, t., diederik, a., kingma, p., & welling, m. (2015).
       [26]id115 and variational id136: bridging
       the gap.
    3. [27]when should i prefer variational id136 over mcmc for
       bayesian analysis? quora.
    4. rowan, a. [28]bayesian deep learning with edward (and a trick using
       dropout). pydata london 2017. ([29]slides)
    5. gal, y., & ghahramani, z. (2016). [30]dropout as a bayesian
       approximation: representing model uncertainty in deep learning.
    6. gal, y., & ghahramani, z. (2015). [31]a theoretically grounded
       application of dropout in recurrent neural networks.
    7. zaremba, w., sutskever, i., & vinyals, o. (2014). [32]recurrent
       neural network id173.
    8. merity, s., keskar, n. s., & socher, r. (2017). [33]regularizing
       and optimizing lstm language models.

   iframe: [34]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=57a9c19a2307

   [35][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [36][1*v-ppfkswhbvlwwamsvhhwg.png]
   [37][1*wt2auqisieaozxj-i7brdq.png]

     * [38]machine learning
     * [39]deep learning
     * [40]recurrent neural network
     * [41]artificial intelligence
     * [42]ai

   (button)
   (button)
   (button) 191 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [43]go to the profile of ceshine lee

[44]ceshine lee

   humanist. data geek.

     (button) follow
   [45]becoming human: artificial intelligence magazine

[46]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 191
     * (button)
     *
     *

   [47]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [48]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/57a9c19a2307
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_nvjxre7l5ouk---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@ceshine?source=post_header_lockup
  15. https://becominghuman.ai/@ceshine
  16. https://eng.uber.com/neural-networks-uncertainty-estimation/
  17. https://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english
  18. http://edwardlib.org/
  19. https://keras.io/layers/recurrent/#lstm
  20. https://github.com/salesforce/awd-lstm-lm
  21. https://github.com/salesforce/awd-lstm-lm
  22. https://github.com/salesforce/awd-lstm-lm/blob/master/embed_regularize.py
  23. https://medium.com/towards-data-science/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8
  24. https://medium.com/@ceshine/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4
  25. https://eng.uber.com/neural-networks-uncertainty-estimation/
  26. https://arxiv.org/pdf/1410.6460.pdf
  27. https://www.quora.com/when-should-i-prefer-variational-id136-over-mcmc-for-bayesian-analysis
  28. https://www.youtube.com/watch?v=i09qvnrus3q
  29. http://rpubs.com/arowan/bayesian_deep_learning
  30. https://arxiv.org/pdf/1506.02142.pdf
  31. http://arxiv.org/abs/1512.05287
  32. https://arxiv.org/abs/1409.2329
  33. http://arxiv.org/abs/1708.02182
  34. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=57a9c19a2307
  35. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  36. https://upscri.be/8f5f8b
  37. https://becominghuman.ai/write-for-us-48270209de63
  38. https://becominghuman.ai/tagged/machine-learning?source=post
  39. https://becominghuman.ai/tagged/deep-learning?source=post
  40. https://becominghuman.ai/tagged/recurrent-neural-network?source=post
  41. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  42. https://becominghuman.ai/tagged/ai?source=post
  43. https://becominghuman.ai/@ceshine?source=footer_card
  44. https://becominghuman.ai/@ceshine
  45. https://becominghuman.ai/?source=footer_card
  46. https://becominghuman.ai/?source=footer_card
  47. https://becominghuman.ai/
  48. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  50. https://medium.com/towards-data-science/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8
  51. https://medium.com/@ceshine/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4
  52. https://medium.com/p/57a9c19a2307/share/twitter
  53. https://medium.com/p/57a9c19a2307/share/facebook
  54. https://medium.com/p/57a9c19a2307/share/twitter
  55. https://medium.com/p/57a9c19a2307/share/facebook
