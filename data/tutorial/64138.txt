   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

[learning note] dropout in recurrent networks         part 3

some empirical result comparison

   [16]go to the profile of ceshine lee
   [17]ceshine lee (button) blockedunblock (button) followfollowing
   oct 2, 2017

   in the first part we went through the theoretical foundations of
   variational dropout in recurrent networks. and in the second part the
   implementation details in keras and pytorch were examined. it   s time
   for the final part where we compare different models on common
   datasets.

mc dropout implementation

   keras doesn   t have a parameter or function to enable dropout in test
   time. [18]though you can use a lambda layer which is always active,
   it   s not easy for us to compare the results from standard dropout
   approximation and monte carlo dropout.

   yarin gal, who wrote the centerpiece paper [1], [19]provided an example
   that implemented mc dropout on github in february 2017. it is also the
   time when keras started to provide built-in support to recurrent
   dropout. it quickly became out of date just after a few months. i dug a
   bit and[20] seems to have made it working with the latest version of
   keras that comes with tensorflow 1.3.0.

   for pytorch, things are much easier. just use model.eval() before
   feed-forward to enable standard dropout approximation and use
   model.train() to enable monte carlo dropout.

   please note that some hyper-parameters are adjusted empirically
   framework-wise. for example, in keras the learning rate is set to 1e-3,
   but in pytorch it is set to 1e-4. the purpose is to avoid the model
   taking forever to converge. therefore it is advised to focus on the
   comparison between models implemented in the same framework.
     __________________________________________________________________

[21]cornell film review dataset

   this dataset is used in the paper [1] to evaluate different models.
   there are 10,620 training samples, and 2,655 test/validation samples.
   it is a regression problem. given a sequence of words, the model need
   to predict what is the score the user gave to the film. the [22]scores
   are normalized to make them zero-centered with unit variance. in the
   following discussion,    raw mse    will correspond to the mean squared
   error to the normalized scores,    rmse    will be the root mean squared
   error to the original scores (the paper use rmse).

   batch size is set to 128 with sequence length 200 (padded), the same as
   in the paper. the number of hidden units remained the same in all
   model. the only difference is how the dropout is applied.
   [1*uwddceaqdwczbm7fulw7gw.png]
   fig 3(b) in [1]: naive dropout lstm over-fits eventually

   the dropout id203 used in paper appears mostly to be 0.5. it
   might have something to do with th slow convergence speed as you can
   see in the figure above. the variational lstm requires hundreds of
   epochs to outperform the other two model. i did   nt have much time to
   train so many epochs, so the dropout id203 was tuned down to
   boost speed while allowing some amounts of over-fitting. for the exact
   probabilities, please refer to the spreadsheet at the end of this
   section.

keras

   three models were evaluated:
    1. variational: use input dropout, recurrent dropout, and output
       dropout.
    2. no dropout: only have a very low weight decay.
    3. naive dropout: use time-step independent input dropout, and output
       dropout.

   no embedding dropout for all three models.
   [1*qzs1gi-uswt0aoznf_25pg.png]

   (correction to the axis labels of the chart to the right: x axis:
      differences in raw mse   , y axis:    count   )

   the chart    (mc         approx) histogram    is a histogram of the raw mse of mc
   dropout minus the one of standard dropout approximation for each
   sample. note in both variational and naive dropout lstm models, mc
   dropout generally produces lower raw mse.
   [1*newruhtmveje7dglv3tpaa.png]

   naive dropout seems to be the best performer, and does not tend to
   over-fit over time.

pytorch

   five models were tests:
    1. weight dropped [2]: use input dropout, weight dropout, and output
       dropout, embedding dropout.
    2. no dropout: vanilla single layer lstm with no weight decay.
    3. naive dropout: use time-step independent input dropout, and output
       dropout.
    4. variational weight dropped: same as weight dropped, but with
       variational parameter set to true.
    5. variational without recurrent dropout (variaional-2 , v w/o
       r-drop): same as weight dropped, but with weight dropout
       id203 set to zero. (no recurrent dropout)

   [1*xmbe4ezb4hwcps3dpl9ppg.png]

   for weight dropped lstm, mc dropout under-performs standard dropout
   approximation. it   s also the case for variational lstm without
   recurrent dropout.
   [1*gsp34mlh0glyq-ry14hc9q.png]

   in this case,    no dropout    model over-fitted severely. comparing with
   the results from keras, we might be able to attribute the steep climb
   in rmse to the absence of weight decay. however,    naive dropout    model
   in pytorch also had a slow upward trend in rmse.    variational weight
   dropped    model seems to have some under-fitting issues.

summary

   below is the detail records of the experiments and their results:
   [1*5c9mlh9vv28uncrstgkdew.png]
   [23]spreadsheet link

   (there seems to be no way to embed google spreadsheet inside a medium
   post. please use the link below the table for a web page version of the
   table.)

   (although mc dropout has poorer performance with weight-dropped models,
   i sticked to mc dropout for simplicity: all predictions (except for the
      no-dropout    model) are created using mc dropout with 10 rounds. )
     __________________________________________________________________

[24]imdb movie review dataset

   this dataset is a binary classification problem. there are 25,000 train
   samples and 25,000 test samples. the goal is to predict whether the
   user like the movie or not from the given word sequence.

   the batch size is 128, with sequence length 80 (padded).

   the model structures are almost the same as before, so i won   t repeat
   here.

keras

   [1*jkkrpn9gwxc1xiwascp2qg.png]

   as before, mc dropout usually generates a lower log loss.
   [1*z_5_ps_i1mkrorpzoaz4ww.png]

   (there are some numerical problems after epoch 17, which generates nans
   as log loss. those data points are omitted in the chart.)

   in this dataset, all models over-fitted over time.

pytorch

   [1*uwjyuo51kl43jytujcnqzw.png]

   as before, weight-dropped model performs worse with mc dropout, as with
   variational model with recurrent dropout (not in the histogram).
   [1*zv03ixb0jracdlr_bh4zqg.png]

   the    naive dropout    model over-fitted significantly in this case. the
   other three dropout models all got similar performances.

summary

   [1*dv5jhzbj5ucxyk0cf9nyzg.png]
   [25]spreadsheet link

      variational weight dropped    performs surprising well in this dataset,
   with decent validation losses and rather resistant to over-fitting.
     __________________________________________________________________

overall remarks

    1. we can get some decent performances from weight-dropped models.
       however, the dropout id203 needs to be carefully tuned.
    2. i suspect the drop in mc dropout performance related to the use of
       embedding dropout. i   d try using more sampling round or just
       disable embedding dropout in test time.
    3. from the results of imdb dataset, it seem like embedding dropout
       contribute moderately in preventing over-fitting.
    4. performing experiments in jupyter notebook (link to the code below)
       requires a lot of manual labor, and is only manageable to a point.
       i   d use something like [26]sacred to automate the process, if a
       more comprehensive evaluation is required.

   [27]ceshine/recurrent-dropout-experiments
   contribute to recurrent-dropout-experiments development by creating an
   account on github.github.com
     __________________________________________________________________

references:

    1. gal, y., & ghahramani, z. (2016). [28]dropout as a bayesian
       approximation: representing model uncertainty in deep learning.
    2. merity, s., keskar, n. s., & socher, r. (2017). [29]regularizing
       and optimizing lstm language models.

previous parts

   [30][learning note] dropout in recurrent networks         part 1
   theoretical foundationsbecominghuman.ai
   [31][learning note] dropout in recurrent networks         part 2
   recurrent dropout implementation in keras and pytorchmedium.com

     * [32]machine learning
     * [33]deep learning

   (button)
   (button)
   (button) 106 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of ceshine lee

[35]ceshine lee

   humanist. data geek.

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 106
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1b161d030cd4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_xla1swcbdgug---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ceshine?source=post_header_lockup
  17. https://towardsdatascience.com/@ceshine
  18. https://github.com/fchollet/keras/issues/1606
  19. https://medium.com/p/1b161d030cd4/edit
  20. https://github.com/ceshine/recurrent-dropout-experiments/blob/master/yaringal_callbacks.py
  21. http://www.cs.cornell.edu/people/pabo/movie-review-data/
  22. https://github.com/ceshine/recurrent-dropout-experiments/blob/master/yaringal_dataset.py#l51
  23. https://docs.google.com/spreadsheets/d/e/2pacx-1vq_3nx8kee3hoygxac4qaueo5hdat9__ffhtiqc_sgwg0howbellgyzxefdfonpctb9n2dbkvmixcnq/pubhtml?gid=0&single=true
  24. http://ai.stanford.edu/~amaas/data/sentiment/
  25. https://docs.google.com/spreadsheets/d/e/2pacx-1vq_3nx8kee3hoygxac4qaueo5hdat9__ffhtiqc_sgwg0howbellgyzxefdfonpctb9n2dbkvmixcnq/pubhtml?gid=490942750&single=true
  26. http://sacred.readthedocs.io/en/latest/quickstart.html
  27. https://github.com/ceshine/recurrent-dropout-experiments
  28. https://arxiv.org/pdf/1506.02142.pdf
  29. http://arxiv.org/abs/1708.02182
  30. https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307
  31. https://medium.com/towards-data-science/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8
  32. https://towardsdatascience.com/tagged/machine-learning?source=post
  33. https://towardsdatascience.com/tagged/deep-learning?source=post
  34. https://towardsdatascience.com/@ceshine?source=footer_card
  35. https://towardsdatascience.com/@ceshine
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://github.com/ceshine/recurrent-dropout-experiments
  42. https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307
  43. https://medium.com/towards-data-science/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8
  44. https://medium.com/p/1b161d030cd4/share/twitter
  45. https://medium.com/p/1b161d030cd4/share/facebook
  46. https://medium.com/p/1b161d030cd4/share/twitter
  47. https://medium.com/p/1b161d030cd4/share/facebook
