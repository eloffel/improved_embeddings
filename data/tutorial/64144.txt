   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    dynamic id27s for evolving semantic
   discovery comments feed [4]can you trust the trend? discovering
   simpson   s paradoxes in social data [5]as we may think [6]alternate
   [7]alternate [8]the morning paper [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

dynamic id27s for evolving semantic discovery

   february 22, 2018
   tags: [17]data science, [18]machine learning

   [19]dynamic id27s for evolving semantic discovery yao et al.,
   wsdm   18

   one of the most popular posts on this blog is my introduction to word
   embeddings with id97 (   [20]the amazing power of word vectors   ). in
   today   s paper choice yao et al. introduce a lovely extension that
   enables you to track how the meaning of words changes over time. it
   would be a superb tool for analysing brands for example.

     human language is an evolving construct, with word semantic
     associations changing over time. for example, apple which was
     traditionally only associated with fruits, is now also associated
     with a technology company. similarly the association of names of
     famous personalities (e.g., trump) changes with a change in their
     roles. for this reason, understanding and tracking word evolution is
     useful for time-aware knowledge extraction tasks   .

   let   s go straight to some examples of what we   re talking about here:

   ([21]enlarge)

   consider the trajectory of    apple   : in 1994 it   s most closely
   associated with fruits, and by 2000 changing dietary associations can
   be seen, and apple is associated with the less healthy    cake,       tart,   
   and    cream.    from 2005 through 2016 though, the word is strongly
   associated with apple the company, and moreover you can see the
   changing associations with apple over time, from    itunes    to google,
   microsoft, samsung et al..

   likewise    amazon    moves from a river to the company amazon, and    obama   
   moves from his pre-presidential roles to president, as does    trump.   

   these embeddings are learned from articles in the new york times
   between 1990 and 2016. the results are really interesting (we   ll see
   more fun things you can do with them shortly), but you might be
   wondering why this is hard to do. why not simply divide up the articles
   in the corpus (e.g., by year), learn id27s for each partition
   (which we know how to do), and then compare them?

   what makes this complicated is that when you learn an embedding for a
   word in one time window (e.g.,    bank   ), there   s no guarantee that the
   embedding will match that in another time window, even if there is no
   semantic change in the meaning of the word across the two. so the
   meaning of    bank    in 1990 and 1995 could be substantially the same, and
   yet the learned embeddings might not be. this is known as the alignment
   problem.

     a key practical issue of learning different id27s for
     different time periods is alignment. specifically, most cost
     functions for training are invariant to rotations, as a byproduct,
     the learned embeddings across time may not be placed in the same
     latent space.

   prior approaches to solving this problem first use independent learning
   as per our straw man, and then post process the embeddings in an
   alignment phase to try and match them up. but yao et al. have found a
   way to learn temporal embeddings in all time slices concurrently, doing
   away with the need for a separate alignment phase. the experimental
   results suggests that this yields better outcomes that the prior
   two-step methods, and the approach is also robust against data sparsity
   (it will tolerate time slices where some words are rarely present or
   even missing).

temporal id27s

   recall that underpinning id27s is the idea of a co-occurrence
   matrix (see    [22]glove: global vectors for word representation   )
   capturing the pointwise mutual information (pmi) between any two words
   in the vocabulary. given a corpus \mathcal{d} , we can compute a pmi
   matrix using windows of size l (around the word in question), where the
   entry at (w,c) for words w and_c_ is given by:
   \displaystyle pmi(\mathcal{d},l)_{w,c} = \log \left( \frac{\#(w,c)
   \cdot |\mathcal{d}|}{\#(w) \cdot \#(c)} \right)

   where \#(w,c) counts the number of times that words w and c co-occur
   within a window of size l in corpus \mathcal{d} and \#(w) and \#(c)
   count the number of occurences of w and c in the corpus respectively.

   the learned embedding vectors for w and c, u_w and u_c , are such that
   u_{w}^{t}u_c \approx pmi(\mathcal{d},l)_{w,c} .

   adding a temporal dimension to this, for each time slice t the positive
   pmi matrix ppmi(t,l) is defined as :
   \displaystyle ppmi(t,l)_{w,c} = \max\{ pmi(\mathcal{d}_t,l)_{w,c}, 0 \}
   := y(t)

   and the temporal id27s u(t) must satisfy u(t)u(t)^t \approx
   ppmi(t,l) .

   this still doesn   t solve the alignment problem though. to encourage
   alignment, the authors cast finding temporal id27s as the
   solution to the following joint optimisation problem:

   where y(t) = ppmi(t,l) and \lambda, \tau are configurable parameters
   greater than zero.
     * the penalty term ||u(t)||^{2}_{f} enforces low-rank data fidelity
       as has been widely adopted in previous work
     * the smoothing term || u(t-1) - u(t) ||^{2}_{f} encourages the word
       embeddings to align. the parameter \tau controls how fast the
       embeddings can change over time.

   this is decomposed to solve the objective function across time for each
   u(t) , using block coordinate descent (bcd) which minimises with
   respect to a single block ($u(t)$) at a time. in theory bcd lacks
   convergence guarantees, but in practice it seems to work well. you
   could always swap bcd for e.g. sgd if you wanted to (but it would make
   slower progress).

finding equivalent terms over time

   let   s return to the fun things you can do with the resulting
   embeddings. here   s an example of finding conceptually equivalent items
   or people over time. for example, the closest equivalent to the
      iphone    as of 2012 was    pc    in 2003, and by 2013-16 it was
      smartphone.    likewise back in 1990-94 the equivalents of twitter were
      broadcast   ,    id98   ,    radio    etc.. in the    mp3    column you can clearly
   see associations with the dominant form of music consumption in the
   given time periods.

   we can do a similar thing asking who played a certain political role in
   a given year:

   even more impressive, is this search for equivalence in sport by
   looking for the atp no. 1 ranked male tennis player in a given year.
   here we   re asking, who played the same role that nadal did in the year
   2010?

tracking popularity over time

   the learned word vector norms across times grow with word frequency,
   and can be viewed as a time series for detecting trending concepts with
   more robustness than word frequency.

     generally, comparing to frequencies which are more sporadic and
     noisy, we note that the norm of our embeddings encourages smoothness
     and id172 while being indicative of the periods when the
     corresponding words were making news rounds.

   here   s a comparison of norms (top chart) and word frequency (bottom
   chart) for the names of us presidents over time.

the last word

     our proposed method simultaneously learns the embeddings and aligns
     them across time, and has several benefits: higher interpretability
     for embeddings, better quality with less data, and more reliable
     alignment for across-time querying.

share this:

     * [23]twitter
     * [24]linkedin
     * [25]email
     * [26]print
     *

like this:

   like loading...

related

   from     [27]uncategorized
   [28]    can you trust the trend? discovering simpson   s paradoxes in
   social data
   [29]as we may think    
   5 comments [30]leave one    
    1. zteve [31]permalink
       february 22, 2018 10:00 am
       adrian, the wordpress typesetting of maths is getting annoying.
       [32]reply
          + [33]adriancolyer [34]permalink*
            february 22, 2018 10:04 am
            yes, it   s really annoying! i   d consider moving platform if it
            wasn   t so much effort, still hoping wp.com will fix it   
            [35]https://twitter.com/adriancolyer/status/966584058275860480
            [36]reply
    2. [37]theo-m [38]permalink
       february 22, 2018 1:22 pm
       you could try github pages with jenkins, you can serve any type of
       static website with it. might be a pain for the comments and i
       don   t know how they   d handle your traffic, but it   s an option i
       guess.
       anyway, thanks for your amazing work! congrats from france     
       [39]reply
    3. [40]auggy (@mmmpork) [41]permalink
       february 22, 2018 5:30 pm
       thanks for sharing this! i   m planning to explore this approach with
       very specific structured data sets and technology terms used within
       those domains (less ambiguity). however, i   m concerned that the
       authors are all cs/engineering and there isn   t a single linguist or
       other social scientist involved in this research. i   m also
       concerned that the paper may be misusing the idea of    meaning    with
       regards to id97. when i   ve shared this i   ve rephrased it as
          the evolving context of terms used in the ny times   . ultimately
       id97 only provides insights into what words are used together,
       but whether that is sufficient for semantic meaning is debatable.
       [42]reply
    4. [43]christopher phipps [44]permalink
       february 26, 2018 6:48 pm
       this is great, but i wasn   t sure how you asked questions of the
       id27s. for example, you asked    who played the same role
       that nadal did in the year 2010?   . how was that query implemented?
       did you use a natural language id53 system?
       [45]reply

leave a reply [46]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [47]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [48]log out /
   [49]change )
   google photo

   you are commenting using your google account. ( [50]log out /
   [51]change )
   twitter picture

   you are commenting using your twitter account. ( [52]log out /
   [53]change )
   facebook photo

   you are commenting using your facebook account. ( [54]log out /
   [55]change )
   [56]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [57]learn how your comment data
   is processed.
     * subscribe

                        [58][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [59]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [60]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [61]establishing software root of trust unconditionally
          + [62]amazon aurora: on avoiding distributed consensus for i/os,
            commits, and membership changes
          + [63]the amazing power of word vectors
          + [64]calvin: fast distributed transactions for partitioned
            database systems
          + [65]on the criteria to be used in decomposing systems into
            modules
          + [66]neural ordinary differential equations
          + [67]programming paradigms for dummies: what every programmer
            should know
          + [68]applying the universal scalability law to organisations
     * rss
          + [69]rss - posts
          + [70]rss - comments
     * live on twitter[71]my tweets

   [72]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [73]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [74]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/feed/
   4. https://blog.acolyer.org/2018/02/21/can-you-trust-the-trend-discovering-simpsons-paradoxes-in-social-data/
   5. https://blog.acolyer.org/2018/02/23/as-we-may-think/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/data-science/
  18. https://blog.acolyer.org/tag/machine-learning/
  19. https://arxiv.org/abs/1703.00607
  20. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  21. https://adriancolyer.files.wordpress.com/2018/02/evolving-word-embeddings-fig-1.jpeg
  22. https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/
  23. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?share=twitter
  24. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?share=linkedin
  25. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?share=email
  26. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#print
  27. https://blog.acolyer.org/category/uncategorized/
  28. https://blog.acolyer.org/2018/02/21/can-you-trust-the-trend-discovering-simpsons-paradoxes-in-social-data/
  29. https://blog.acolyer.org/2018/02/23/as-we-may-think/
  30. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#respond
  31. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-21507
  32. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?replytocom=21507#respond
  33. https://adriancolyer.wordpress.com/
  34. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-21508
  35. https://twitter.com/adriancolyer/status/966584058275860480
  36. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?replytocom=21508#respond
  37. http://gravatar.com/tmatussiere
  38. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-21515
  39. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?replytocom=21515#respond
  40. http://twitter.com/mmmpork
  41. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-21521
  42. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?replytocom=21521#respond
  43. http://thelousylinguist.blogspot.com/
  44. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-21635
  45. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/?replytocom=21635#respond
  46. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#respond
  47. https://gravatar.com/site/signup/
  48. javascript:highlandercomments.doexternallogout( 'wordpress' );
  49. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/
  50. javascript:highlandercomments.doexternallogout( 'googleplus' );
  51. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/
  52. javascript:highlandercomments.doexternallogout( 'twitter' );
  53. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/
  54. javascript:highlandercomments.doexternallogout( 'facebook' );
  55. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/
  56. javascript:highlandercomments.cancelexternalwindow();
  57. https://akismet.com/privacy/
  58. http://eepurl.com/bbzpm9
  59. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
  60. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
  61. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
  62. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
  63. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  64. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
  65. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
  66. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
  67. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
  68. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
  69. https://blog.acolyer.org/feed/
  70. https://blog.acolyer.org/comments/feed/
  71. https://twitter.com/519408925733425153
  72. https://wordpress.com/?ref=footer_blog
  73. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#cancel
  74. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  76. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-form-guest
  77. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-form-load-service:wordpress.com
  78. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-form-load-service:twitter
  79. https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/#comment-form-load-service:facebook
