   #[1]deep ideas    feed [2]deep ideas    comments feed [3]deep ideas   
   deep learning from scratch i: computational graphs comments feed
   [4]alternate [5]alternate

   [6]facebook[7]twitter[8]email[9]rss[10]linkedin[11]tumblr

   [12]deep ideas logo deep ideas retina logo

a blog on artificial intelligence, deep learning and cognitive science

     * [13]home
     * [14]list of articles
     * [15]about me
     * ____________________
          

   [16]previous [17]next

     * [18]view larger image

deep learning from scratch i: computational graphs

   this is part 1 of a series of tutorials, in which we develop the
   mathematical and algorithmic underpinnings of deep neural networks from
   scratch and implement our own neural network library in python,
   mimicing the [19]tensorflow api.
     * [20]part i: computational graphs
     * [21]part ii: id88s
     * [22]part iii: training criterion
     * [23]part iv: id119 and id26
     * [24]part v: multi-layer id88s
     * [25]part vi: tensorflow

   i do not assume that you have any preknowledge about machine learning
   or neural networks. however, you should have some preknowledge of
   calculus, id202, fundamental algorithms and id203 theory
   on an undergraduate level. if you get stuck at some point, please leave
   a comment.

   by the end of this text, you will have a deep understanding of the math
   behind neural networks and how deep learning libraries work under the
   hood.

   i have tried to keep the code as simple and concise as possible,
   favoring conceptual clarity over efficiency. since our api mimics the
   tensorflow api, you will know how to use tensorflow once you have
   finished this text, and you will know how tensorflow works under the
   hood conceptually (without all the overhead that comes with an
   omnipotent, maximally efficient machine learning api).

   the full source code of the api can be found at
   [26]https://github.com/danielsabinasz/tensorslow. you also find a
   jupyter notebook there, which is equivalent to this blog post but
   allows you to fiddle with the code.

computational graphs

   we shall start by defining the concept of a computational graph, since
   neural networks are a special form thereof. a computational graph is a
   directed graph where the nodes correspond to operations or variables.
   variables can feed their value into operations, and operations can feed
   their output into other operations. this way, every node in the graph
   defines a function of the variables.

   the values that are fed into the nodes and come out of the nodes are
   called tensors, which is just a fancy word for a multi-dimensional
   array. hence, it subsumes scalars, vectors and matrices as well as
   tensors of a higher rank.

   let   s look at an example. the following computational graph computes
   the sum $z$ of two inputs $x$ and $y$.
   here, $x$ and $y$ are input nodes to $z$ and $z$ is a consumer of $x$
   and $y$. $z$ therefore defines a function $z : \mathbb{r^2} \rightarrow
   \mathbb{r}$ where $z(x, y) = x + y$.

   [addition.png]

   the concept of a computational graph becomes more useful once the
   computations become more complex. for example, the following
   computational graph defines an affine transformation $z(a, x, b) = ax +
   b$.

   [affine_transformation.png]

operations

   every operation is characterized by three things:
     * a compute function that computes the operation   s output given
       values for the operation   s inputs
     * a list of input_nodes which can be variables or other operations
     * a list of consumers that use the operation   s output as their input

   let   s put this into code:

   iframe:
   [27]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132293/operation


some elementary operations

   let   s implement some elementary operations in order to become familiar
   with the operation class (and because we will need them later). in both
   of these operations, we assume that the tensors are [28]numpy arrays,
   in which the element-wise addition and id127 (.dot) are
   already implemented for us.

addition

   iframe:
   [29]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132294/addition

id127

   iframe:
   [30]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132295/matrix%20multiplication

placeholders

   not all the nodes in a computational graph are operations. for example,
   in the affine transformation graph, $a$, $x$ and $b$ are not
   operations. rather, they are inputs to the graph that have to be
   supplied with a value once we want to compute the output of the graph.
   to provide such values, we introduce placeholders.

   iframe:
   [31]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132296/placeholder

variables

   in the affine transformation graph, there is a qualitative difference
   between $x$ on the one hand and $a$ and $b$ on the other hand. while
   $x$ is an input to the operation, $a$ and $b$ are parameters of the
   operation, i.e. they are intrinsic to the graph. we will refer to such
   parameters as variables.

   iframe:
   [32]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132297/variable

the graph class

   finally, we   ll need a class that bundles all the operations,
   placeholders and variables together. when creating a new graph, we can
   call its as_default method to set the _default_graph to this graph.
   this way, we can create operations, placeholders and variables without
   having to pass in a reference to the graph everytime.

   iframe:
   [33]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132298/graph

example

   let   s now use the classes we have built to create a computational graph
   for the following affine transformation:

   $$
   z = \begin{pmatrix}
   1 & 0 \\
   0 & -1
   \end{pmatrix}
   \cdot
   x
   +
   \begin{pmatrix}
   1 \\
   1
   \end{pmatrix}
   $$

   iframe:
   [34]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132299/example


computing the output of an operation

   now that we are confident creating computational graphs, we can start
   to think about how to compute the output of an operation.

   let   s create a session class that encapsulates an execution of an
   operation. we would like to be able to create a session instance and
   call a run method on this instance, passing the operation that we want
   to compute and a dictionary containing values for the placeholders:
session = session()
output = session.run(z, {
    x: [1, 2]
})


   this should compute the following value:

   $$
   z = \begin{pmatrix}
   1 & 0 \\
   0 & -1
   \end{pmatrix}
   \cdot
   \begin{pmatrix}
   1 \\
   2
   \end{pmatrix}
   +
   \begin{pmatrix}
   1 \\
   1
   \end{pmatrix}
   =
   \begin{pmatrix}
   2 \\
   -1
   \end{pmatrix}
   $$

   in order to compute the function represented by an operation, we need
   to apply the computations in the right order. for example, we cannot
   compute $z$ before we have computed $y$ as an intermediate result.
   therefore, we have to make sure that the operations are carried out in
   the right order, such that the values of every node that is an input to
   an operation $o$ has been computed before $o$ is computed. this can be
   achieved via [35]post-order traversal.

   iframe:
   [36]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132300/session

   let   s test our class on the example from above:

   iframe:
   [37]https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57
   849/computational-graphs/132301/example

   looks good.

   if you have any questions, feel free to leave a comment. otherwise,
   continue with the next part: [38]ii: id88s.

share this:

     * [39]click to share on twitter (opens in new window)
     * [40]click to share on facebook (opens in new window)
     * [41]click to share on google+ (opens in new window)
     * [42]click to share on skype (opens in new window)
     * [43]click to share on linkedin (opens in new window)
     * [44]click to print (opens in new window)
     * [45]click to share on pocket (opens in new window)
     * [46]click to share on tumblr (opens in new window)
     * [47]click to share on reddit (opens in new window)
     * [48]click to share on telegram (opens in new window)
     * [49]click to share on pinterest (opens in new window)
     * [50]click to share on whatsapp (opens in new window)
     *

related

   by [51]daniel| 2018-02-26t11:57:07+00:00 august 26th,
   2017|[52]artificial intelligence, [53]deep learning, [54]machine
   learning, [55]python, [56]tensorflow|[57]9 comments

stay updated

   subscribe to the mailing list and get updated about new blog posts by
   email.
   ____________________
   [ ] i consent to my submitted data being collected via this form*
   sign up now

   thank you for subscribing.

   something went wrong.

   i respect your privacy and take protecting it seriously

follow me on twitter

   [58]my tweets

   copyright 2017 daniel sabinasz
   [59]facebook[60]twitter[61]email[62]rss[63]linkedin[64]tumblr

references

   visible links
   1. http://www.deepideas.net/feed/
   2. http://www.deepideas.net/comments/feed/
   3. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/feed/
   4. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
   5. http://www.deepideas.net/wp-json/oembed/1.0/embed?url=http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/&format=xml
   6. http://fb.me/deepideas.net
   7. https://twitter.com/deepideas_net
   8. mailto:daniel@sabinasz.net
   9. http://www.deepideas.net/feed/
  10. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  11. https://deepideas.tumblr.com/
  12. http://www.deepideas.net/
  13. http://www.deepideas.net/
  14. http://www.deepideas.net/list-of-articles/
  15. http://www.deepideas.net/about-me/
  16. http://www.deepideas.net/deep-learning-from-scratch-theory-and-implementation/
  17. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  18. https://i2.wp.com/www.deepideas.net/wp-content/uploads/2017/08/neuron.jpg?fit=900,300
  19. http://www.tensorflow.org/
  20. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  21. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  22. http://www.deepideas.net/deep-learning-from-scratch-iii-training-criterion/
  23. http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-id26/
  24. http://www.deepideas.net/deep-learning-from-scratch-v-multi-layer-id88s/
  25. http://www.deepideas.net/deep-learning-from-scratch-vi-tensorflow/
  26. https://github.com/danielsabinasz/tensorslow
  27. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132293/operation
  28. http://www.numpy.org/
  29. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132294/addition
  30. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132295/id127
  31. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132296/placeholder
  32. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132297/variable
  33. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132298/graph
  34. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132299/example
  35. https://en.wikipedia.org/wiki/tree_traversal#post-order
  36. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132300/session
  37. https://tech.io/playground-widget/3ba0b9380fa5326b2ef084cc28b57ec57849/computational-graphs/132301/example
  38. http://www.deepideas.net/deep-learning-from-scratch-ii-id88s/
  39. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=twitter
  40. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=facebook
  41. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=google-plus-1
  42. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=skype
  43. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=linkedin
  44. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/#print
  45. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=pocket
  46. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=tumblr
  47. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=reddit
  48. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=telegram
  49. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/?share=pinterest
  50. https://api.whatsapp.com/send?text=deep learning from scratch i: computational graphs http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  51. http://www.deepideas.net/author/daniel/
  52. http://www.deepideas.net/category/artificial-intelligence/
  53. http://www.deepideas.net/category/artificial-intelligence/machine-learning/deep-learning/
  54. http://www.deepideas.net/category/artificial-intelligence/machine-learning/
  55. http://www.deepideas.net/category/python/
  56. http://www.deepideas.net/category/artificial-intelligence/machine-learning/tensorflow/
  57. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/#comments
  58. https://twitter.com/deepideas_net
  59. http://fb.me/deepideas.net
  60. https://twitter.com/deepideas_net
  61. mailto:daniel@sabinasz.net
  62. http://www.deepideas.net/feed/
  63. https://www.linkedin.com/in/daniel-sabinasz-7b220b100/
  64. https://deepideas.tumblr.com/

   hidden links:
  66. http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/
  67. https://www.facebook.com/deepideas.net/
