   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]data science
   [7]become a member[8]sign in[9]get started
     __________________________________________________________________

get started with nlp (part ii): overview of an nlp workflow

   [10]go to the profile of sigmoider
   [11]sigmoider (button) blockedunblock (button) followfollowing
   mar 4, 2018
   [1*ogd8_fmqfbj_ckst_h3www.jpeg]
   image by the author.

   this is the second part of a series of natural language processing
   tutorials for beginners. for an introduction to nlp, please check out
   the [12]first part.

introduction:

   let me tell you a story.

     tim is a little guy who likes to build things with his varied lego  
     pieces. however, tim is a bit messy, so he keeps all the components
     of his different puzzles in a big drawer. he now wants to build a
     castle with some lego   pieces but, in order to do that (since he is
     very meticulous with his artworks), he first selects the pieces that
     could be useful from the entire set by selecting only lego   pieces.
     then, he removes the damaged pieces and the ones that he does not
     find useful for his purpose, and unifies some small ones that could
     fit together in a block. later, he groups pieces by their colour,
     shape or purpose. shortly after, tim brings all the pieces together
     in a specific order and builds his mighty castle. finally, he can
     play with it.

   [0*9vpvypgnh-ieasl9.png]
   lego   meme (s[13]ource)

   in this post we will see that, when we try to approach an nlp task (and
   also most of machine learning ones), we should behave like tim.
     __________________________________________________________________

1. preprocessing:

   just as tim has his drawer, we have different data sources with
   different types of data. we as humans are drawers in a way, with all of
   our biometrics, phone calls, contact agenda, youtube views, films,
   series, songs, text messages, tweets, books, activities, commutes, and
   so on.

   that said, we can refer to two main types of data:
   [0*4c5pvma1catatqkc.png]
   figure 1: example of relational data (sql table). [14]source
    1. structured data: information that has a pre-defined structure,
       which is typically represented in a numerical way but can also
       include text (to denote classes for example). a good example is an
       sql table. sql is a standard language for storing, manipulating and
       retrieving data in databases. this means that data is structured
       and related through different columns. in figure 1 we can see 2
       columns that can take numerical values (employeeid and
       departmentid) and other two for textual information (lastname and
       coutry).
    2. unstructured data: this is a large chunk of the total amount of
       data that we consume and produce every day: information that does
       not have a well defined systematic structure. when we deal with
       natural language, we are dealing with unstructured data: we can   t
       specify a universal structure or an invariable range of values that
       a sentence can have (in contrast with the example above).

   i like to think of this type of information as information that we
   don   t currently know how we process and manipulate (cognitively). we
   can conceive nlp as the different set of tools that can be applied in
   order to structure natural language for different purposes. as it was
   mentioned in the [15]first part of the series, our dataset will be
   called corpus, since it is composed by a set of textual information
   (the plural is corpora). we can think of this corpus as the set of
   lego   pieces that tim selects initially, by discarding pieces from
   other puzzles.
   [1*xhm9c9qdfxa3zcqjiovm_w.jpeg]
   preprocessing ([16]source)

   when tim removes the damaged or unuseful pieces from his set, this is
   called the preprocessing step, in which he tries to select useful
   pieces for the building process. when we preprocess the data, in nlp we
   name it text id172 or data preparation, since we are trying to
      normalize    in some way the elements of our corpus.

   the following posts will cover each of the different tools that can be
   applied in each step, but, for the sake of argument, if we have both
   cat and cats in our corpus, we would be interested in normalizing them
   by unifying both terms into cat (this, as we will see, is called
   id30). other examples would be splitting the corpus into different
   sentences or removing urls and other elements that could be present in
   our corpus and may not be of our interest for the task.
     __________________________________________________________________

2. structuring:

   just as tim groups pieces in different ways, depending on our objetive
   we may want to organise our corpus in different ways. this is the
   structuring step, in which we choose which are the elements that we
   want to detect. depending on the objetive we could be interested in the
   extraction of several things, including, but not limited to:
     * the entities that are present in the text. for example, suppose
       that our corpus includes several countries, names of people,
       trademarks or animals that we want to detect as different elements.
     * the intents or actions written by users. imagine that our corpus is
       comprised of online flight bookings from different users, then the
       intents would be the actions or options that the user could have
       available. in this context bookrequest (with sentences such as:    i
       want to book a flight, please   ), setbookingdate (e.g.    is there any
       booking available for tomorrow?   ) or savebookingprocess (e.g.    i
       want to continue later. please, save it   ) would be example intents.

   [0*dax5sk_au451vciq.png]
   entities and intents ([17]source)
     * the grammatical function of the words in the corpus, which is
       called part of speech (pos) tagging. we may want to extract only
       the verbs and nouns from the corpus to analyse them.
     * the syntactic role of each element in the corpus through dependency
       parsing, maybe to extract only the subjects from the corpus or
       other dependencies.
     * the different topics that are present in the corpus, grouped by
       terms through id96 techniques.

   [0*3yigp_yuwcwpnfm8.png]
   id96 ([18]source)
     * the feelings that are present in the text, quantifying them through
       id31. if our corpus is made up of tweets related to a
       contentious matter in today   s society we would want to measure how
       is people feeling about it: angry, happy, sad, ...

   [0*wxczba71wzppnlk5.jpg]
   id31 ([19]source)

   as it was already mentioned, the structuring tools applied will depend
   on the objective that we have.
     __________________________________________________________________

3. analysis:

   once we have applied the structuring techniques we can proceed to look
   at the structured information. this is the analysis step, in which we
   extract different features in order to perform the task that we had in
   mind. we now want to bring all the pieces together in a specific order
   like tim did to build his castle.

   sometimes this step just consists in counting down the different
   elements (e.g. count the number of instances for each sentiment class
   to get an insight of the overall feeling), but in other cases we may
   want to use the features in a more complex model, such as a machine
   learning algorithm (e.g. train a neural network that transforms
   entities into vectors or translates from one language to another).
   [0*luahia6fdazmodux.gif]
   machine translation ([20]source)

   a good way to identify what you need to do in this step would be to ask
   yourself: what do i need to do with the structured data to achieve my
   goal?
     __________________________________________________________________

4. transformation:

   now that we have our nlp castle, we can use it for our particular
   purpose just as tim did with his construction. this is the
   transformation step, the last one in the workflow. in this step the
   purpose is to translate the obtained information into an interpretable
   source in order to make our decision, observe it or analyse it.
   [0*kgqhisegg9et2ste.png]
   word representation through vectors ([21]source)

   as with the analysis, this can vary from problem to problem. sometimes
   this step simply involves outputting the conclusions visually (e.g.
   representing the obtained vectors as points in 2 or 3 dimensions), a
   proportion (e.g. the percentage for each sentiment class) or a
   predominant detected class (e.g. the sentiment class that obtained a
   higher count). it can also involve more complex tasks such as language
   generation (e.g. generating a novel or personalised response for a user
   request) or id133 (e.g. producing a response with a human
   voice).
     __________________________________________________________________

conclusions:

   i hope that this post gives a good initial overview of the complexity
   and structure of nlp projects. it is a good exercise to ask yourself
   questions like:
     * what would i do with this corpus?
     * and what about my own textual data?
     * how would i design an nlp workflow for a system like amazon   alexa?
     __________________________________________________________________

what comes next:

   thank you very much for reading this! i   m open to and will thank any
   (respectful) form of suggestion and/or question. the next post will
   cover the different possibilities available in a preprocessing step
   coding examples with [22]spacy.

   sigmoider

   ([23]twitter)
     __________________________________________________________________

resources:

     * [24]some interesting datasets
     * [25]some interesting corpora
     * [26]awesome nlp repository
     * [27]nlp data
     * [28]nlp tasks and selected references

     * [29]machine learning
     * [30]nlp
     * [31]natural language process
     * [32]artificial intelligence
     * [33]workflow

   (button)
   (button)
   (button) 660 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of sigmoider

[35]sigmoider

   nlp research, ai and music.

     * (button)
       (button) 660
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7ba1f5948b24
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/topic/data-science
   7. https://medium.com/membership?source=upgrade_membership---nav_full
   8. https://medium.com/m/signin?redirect=https://medium.com/@gon.esbuyo/get-started-with-nlp-part-ii-overview-of-an-nlp-workflow-7ba1f5948b24&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/@gon.esbuyo/get-started-with-nlp-part-ii-overview-of-an-nlp-workflow-7ba1f5948b24&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@gon.esbuyo?source=post_header_lockup
  11. https://medium.com/@gon.esbuyo
  12. https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828
  13. http://connectedcomedy.com/wp-content/uploads/2011/01/legos-the-oatmeal.png
  14. http://www.sqlatoms.com/atomsnetwork/files/1576/table output.png
  15. https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828
  16. https://cdn-images-1.medium.com/max/1600/1*xhm9c9qdfxa3zcqjiovm_w.jpeg
  17. https://media.licdn.com/mpr/mpr/aaeaaqaaaaaaaamwaaaajguyy2fizju5lta4mmetndjiys1hmwiwlty4mte2zmjlodm5yg.png
  18. https://cdn-images-1.medium.com/max/637/0*3yigp_yuwcwpnfm8.png
  19. https://i.imgur.com/ltpvdfa.jpg
  20. https://3.bp.blogspot.com/-3pbj_dvt0vo/v-qe-nl6p5i/aaaaaaaabqc/z0_6wtvwtvartmk0i9_atleyygyv6ai4wclcb/s1600/id4-model-fast.gif
  21. http://www.samyzaf.com/ml/nlp/id972.png
  22. https://spacy.io/
  23. https://twitter.com/sigmoider
  24. https://github.com/awesomedata/awesome-public-datasets#natural-language
  25. http://nlpforhackers.io/corpora/
  26. https://github.com/keon/awesome-nlp
  27. https://github.com/niderhoff/nlp-datasets
  28. https://github.com/kyubyong/nlp_tasks
  29. https://medium.com/tag/machine-learning?source=post
  30. https://medium.com/tag/nlp?source=post
  31. https://medium.com/tag/natural-language-process?source=post
  32. https://medium.com/tag/artificial-intelligence?source=post
  33. https://medium.com/tag/workflow?source=post
  34. https://medium.com/@gon.esbuyo?source=footer_card
  35. https://medium.com/@gon.esbuyo

   hidden links:
  37. https://medium.com/p/7ba1f5948b24/share/twitter
  38. https://medium.com/p/7ba1f5948b24/share/facebook
  39. https://medium.com/p/7ba1f5948b24/share/twitter
  40. https://medium.com/p/7ba1f5948b24/share/facebook
