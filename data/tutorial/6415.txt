   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]jungle book
   [7]jungle book
   [8]sign in[9]get started
     __________________________________________________________________

towards data set augmentation with gans

   [10]go to the profile of pedro ferreira
   [11]pedro ferreira (button) blockedunblock (button) followfollowing
   oct 5, 2017

   id3 (gans) have taken over the machine
   learning community by storm. their elegant theoretical foundations and
   the great results that are continuously improved upon in the computer
   vision domain make them one of the most active topics of research in
   machine learning in recent years. in fact, yann lecun, director of
   facebook ai research said in 2016 that [12]gans,    and the variations
   that are now being proposed is the most interesting idea in the last 10
   years in ml, in my opinion   . to get an idea of how much this topic is
   being explored right now, visit [13]this great blog post.

   although they have been shown to work wonderfully as generative models
   for images, including pictures of faces and bedrooms, gans haven   t
   really been tested extensively on data sets such as the ones a factory
   would provide you, containing a large amount of measurements from
   sensors in a production line, for example. such data sets may even
   contain time series information that our machine learning models must
   leverage to make predictions on future events         something which doesn   t
   happen for static data such as pictures. applying a generative model to
   these kinds of data can be useful, for example, if our predictive
   models need an even larger number of samples to train on to improve its
   generalization. also, if we come up with a model that can generate
   high-quality synthetic data, then it surely must have learned the
   original data   s underlying structure. and if it has, we can use that
   representation as a new feature set for our predictive models to
   exploit!

   in this post i will describe some of the gan architectures that may be
   useful for data set augmentation, both sample- and feature-wise. let   s
   start with the basic gan.

id3

   a gan is a model made up of two entities: a generator and a
   discriminator. here we will consider them both to always be
   parametrized neural networks: g and d, respectively. the
   discriminator   s parameters are optimized to maximize the id203 of
   correctly distinguishing real from fake data (from the generator) while
   the generator   s goal is to maximize the id203 of the
   discriminator failing to classify its fake samples as fake.
   [1*hr4k8zzxnvchdfx3zicx5a.png]
   figure 1: id3

   the generator network produces samples by taking an input vector z,
   sampled from what is called a latent distribution, and transforming it
   by applying the g function defined by the network, yielding g(z). the
   discriminator network receives alternating g(z) and x, a real data
   sample and outputs a id203 of that input being real.

   with proper hyperparameter tuning and enough training iterations, the
   generator and the discriminator will converge jointly (performing
   parameter updates via a id119 method) to a point where the
   distribution which described the fake data is the same as the
   distribution from which real data is sampled.

   in the rest of this post the gans workings will be illustrated on the
   mnist dataset for generating new digits, or encoding the original ones
   on a latent space. we will also look at how they can be used on
   categorical and time series data.

   to start off, here   s a bunch of samples generated by a simple gan whose
   neural networks are multilayer id88s (mlps), trained on the mnist
   dataset:
   [1*oqo5zhptkhdqcrscbd1nsa.png]
   figure 2: generating new digits.

not everything is fine with gans

   although the gan as we   ve seen it works, in practice it has some
   drawbacks whose solutions have been object of intensive research since
   the [14]original 2014 paper by ian goodfellow et al. the major
   drawbacks have to do with the training of the gan, which has become
   quite infamous for being extremely difficult: first, training a gan is
   highly hyperparameter-dependent. second, and most importantly, the loss
   functions (both the generator   s and discriminator   s) are not
   informative: while the generated samples may start to closely resemble
   the true data         approximating significantly its distribution         this
   behavior can   t be indexed to a trend of the losses in general. this
   means that we can   t just run a hyperparameter optimizer such as
   [15]skopt using the losses and must instead iteratively tune them
   manually, which is a shame.

   the other drawbacks of this gan architecture have to do with
   functionality. the way it   s shown in figure 1 and using the original
   cross-id178 loss, we can   t:
    1. control what data to generate.
    2. generate categorical data.
    3. access the latent space to use it as a feature.

   generating categorical data is a particularly difficult problem for
   gans. ian goodfellow explained it in a very intuitive way in [16]this
   reddit post:

     you can make slight changes to the synthetic data only if it is
     based on continuous numbers. if it is based on discrete numbers,
     there is no way to make a slight change.

     for example, if you output an image with a pixel value of 1.0, you
     can change that pixel value to 1.0001 on the next step.

     if you output the word    penguin   , you can   t change that to    penguin
     + .001    on the next step, because there is no such word as    penguin
     + .001   . you have to go all the way from    penguin    to    ostrich   .

   the key idea is that of the impossibility of the generator to go    all
   the way    from one entity (eg,    penguin   ) to another (eg,    ostrich   ).
   because the space in between has 0 id203 of occurring, the
   discriminator can easily tell that samples in that space are not real,
   and hence it can not be fooled by the generator.

alternative gans

   to solve the issues associated with the original gan, several other
   training approaches and architectures have been developed. in the
   following paragraphs, a brief description of each is presented. the
   goal of these descriptions is to get a sense of how these methods could
   be applied to structured data such as the one you would find in a
   [17]kaggle competition.

conditional gan

   previously, we   ve set up a gan to generate random digits that look like
   the ones from the mnist data set. but what if we want to generate
   specific digits? to be able to tell our generator to generate any digit
   we want by command, only a very small change in training is needed. for
   each iteration, the generator takes as input not only z but also a
   one-hot encoded vector indicating the digit. the discriminator input
   consists then of not only the real or fake sample but also the same
   label vector.
   [1*uzmm2ayvia_53sme7ftp_g.png]
   figure 3: conditional gan

   proceeding the same way as before but with this slight change of
   inputs, the conditional gan (cgan) learns to generate samples
   conditioned on the label it takes as input.

   let   s then generate one sample of each digit! when sampling from the
   latent space, we also input a one-hot encoded vector indicating the
   class we want. doing this for all 10 classes of digits yields the
   results in figure 4:
   [1*nq9c7bqkuchvyrrbsbenia.png]
   figure 4: conditionally generated digit samples

wasserstein gan

   the wasserstein gan (wgan) is one of the most popular gans and consists
   of an objective change which results in training stability,
   interpretability (correlation of the losses with sample quality) and
   the ability of generating categorical data. the key aspect is that the
   generator   s objective is to approximate the true data distribution, and
   for that the choice of distance measure between distributions is
   important, as that is the objective to minimize. the wgan chooses the
   wasserstein (or earth-mover) distance         or rather, an approximation of
   it         as it can be shown that it converges for sets of distributions for
   which the kullback-leibler and jensen-shannon divergences don   t. if you
   are interested in the theory, read the [18]original paper or [19]this
   excellent summary.

   implementation-wise, the implications of approximating the wasserstein
   distance can be summarized as follows:
    1. the output of the discriminator is no longer a id203, which
       motivates the renaming of the discriminator to critic.
    2. the discriminator   s parameters are clipped to some threshold (or
       [20]gradient penalization can be performed)
    3. for each training iteration, the discriminator   s parameters are
       updated more times than the generator   s.

wasserstein gan on categorical data

   the authors of the wgan paper show that a gan trained in this way
   exhibits training stability and interpretability, but only later
   [21]was it proven that using the wasserstein distance also provides the
   gan with the ability of generating categorical data (i.e., not
   continuous-valued data like images or even integer-coded data like 1
   for sunday, 2 for monday and so on). while if the original gan was
   trained on this kind of data, the discriminator   s loss would remain low
   throughout iterations while the generator   s wouldn   t stop increasing,
   training a wgan on categorical data is done the same way as on
   continuous-valued data.

   all one needs to do is this (see figure 5 for an example): for each
   categorical variable in a data set, have a corresponding softmax output
   in the generator network of dimensionality equal to the number of
   possible discrete values. instead of one-hot encoding the softmax
   output and using that as input to the discriminator, use the raw
   softmax output as if it was a set of continuous-valued variables. in
   this way, training will converge! at test time, to generate fake
   categorical data, just one-hot encode the generator   s discrete outputs
   and there you go!
   [1*poxtpvuemvigp6ic3gqhnq.png]
   figure 5: example of a generator of mixed categorical and continuous
   variables. here, categorical variable 1 takes 1 of 3 possible values,
   categorical variable 2 takes 1 of 2 possible values and there   s one
   continuous variable.

   as an example of training a wgan with gradient penalty on a data set
   with categorical values, in figure 6 you can see the beautiful stable,
   converging id168s you get when training it on the [22]sberbank
   russian housing market data set from a kaggle competition, which
   contains both continuous and categorical variables.
   [1*lbhf7j2gdcco2gf8j_rt9g.png]
   figure 6: wgan-gp training on the sberbank russian housing market
   data set

   of course, we may also combine the wgan with the cgan to train the wgan
   in supervised fashion to generate samples conditioned on class labels!

   note: a further improvement on the wasserstein gan is the [23]cramer
   gan, which aims at providing even better-quality samples and improved
   training stability. inspecting its possibility of generating
   categorical data is a topic for future research.

bidirectional gan

   although the wgan seems to solve quite a lot of our problems, it
   doesn   t allow the access to latent space representations of the data.
   finding these representations may be useful not only for controlling
   what data to generate by moving continuously in the latent space, but
   also for feature extraction.
   [1*gd1wutu2gc05ihgy2iezmg.png]
   figure 7: bidirectional gan

   the [24]bidirectional gan (bigan) is an attempt at solving this issue.
   it works by learning not only a generative network but also, at the
   same time, an encoder network e which maps the data to the generator   s
   latent space. this is done also in an adversarial setting using only
   one discriminator for both the generating and encoding tasks. the
   authors of the bigan show that, in the limit, the pair g and e yield an
   autoencoder: encoding a data sample via e and decoding it via g yields
   the original sample.

infogan

   we saw earlier that the cgan allows the conditioning of the generator
   to generate samples according to their labels. but would it be possible
   to learn to distinguish digits in a fully unsupervised manner by simply
   forcing a categorical structure in the gan   s latent space? what about
   setting also a continuous code space that we can access to describe
   continuous semantic variations in data samples (in the case of mnist,
   things like digit width or tilt)?

   the answer to both questions is yes. better than that: we can do both
   things simultaneously! truth is, we can impose any set of code space
   distributions that we find useful and train the gan to encode
   meaningful traits in those distributions. each code would learn to
   contain a different semantic trait of the data, resulting in effective
   information disentanglement.
   [1*by1euz9h2n4bozjkn9_gjq.png]
   figure 8: infogan

   the gan that allows such a thing is the [25]infogan. simply put, the
   infogan tries to maximize the mutual information between the
   generator   s input code space and an id136 net   s output. the
   id136 net can be set to simply be an output layer on the
   discriminator network, sharing all the other parameters, meaning it   s
   computationally free. once trained, the infogan   s discriminator
   id136 output layer can be used for feature extraction or, if the
   code space contains label information, for classification!

   creating an infogan with two code spaces         one continuous of dimension
   2 and one discrete of dimension 10         we can generate data conditioned
   on the discrete code to generate specific digits, and on the continuous
   code to generate specifically-styled digits, as seen in figure 9. note
   that no labels were harmed in this entirely unsupervised learning
   scheme         imposing a categorical distribution in the latent space
   suffices to make the model learn to encode label information in that
   distribution!
   [1*vwfmyvf8tvow5q0jplvh-w.png]
   figure 9: fixing the discrete code and varying the continuous code at
   the generator   s input.

adversarial autoencoder

   [1*u7grtibbnumglxdzaidgsq.png]
   figure 10: adversarial autoencoder

   the [26]adversarial autoencoder (aae) is where autoencoders meet gans.
   in this model, two objectives are optimized: the first, the
   minimization of the reconstruction error of the data x through the
   encoder and decoder networks, p and q, respectively. the second
   training criterion is the enforcement of a prior distribution on the
   code p(x), via adversarial training where the generator corresponds to
   p. so while p and q are optimized to minimize the distance between x
   and q(z), where z is the code space vector of the autoencoder, p and d
   are optimized as a gan to force the code space p(x) to match a
   pre-defined structure. this can be seen as a id173 on the
   autoencoder, forcing it to learn a meaningful, structured and cohesive
   code space (as opposed to fractured         see page 76 of [27]these lecture
   notes by geoffrey hinton) that allows for effective feature extraction
   or id84. also, because a known prior is imposed on
   the code vector, sampling from such prior and passing the samples
   through q, the decoder network, configures a generative modelling
   scheme!

   let us then impose a 2d gaussian distribution with standard deviation 5
   on the code space via adversarial training on the autoencoder. sampling
   from neighboring points in this space, a continuous variation of the
   generated digits is observed!
   [1*rkcl-6gbvql7wp86yjuclq.png]
   figure 11: left: validation data projection in the 2d code space.
   right: sampling from neighboring points in the code space and decoding
   the samples to generate digits.

   another thing we can do is train the aae with labels to force the
   disentanglement of label and digit style information. this way, by
   fixing the desired label, variations in the imposed continuous latent
   space will result in different styles of the same digit. for digit
   number eight for example:
   [1*4d11t2qwqd4azg1iaupfyg.png]
   figure 12: fixing the label and sampling from neighboring points in the
   latent aae space

   clearly, there is a meaningful relationship between neighboring points!
   this property may come in handy when generating samples for our data
   set augmentation problem.

time series data?

   often, real-world structured data consists of time series. this is data
   in which each sample has some dependence on the previous sample. for
   this type of data, using recurrent neural network (id56)-based models
   are often chosen for their intrinsic ability of modelling it.
   leveraging these neural networks in our gan models could, in principle,
   result in higher-quality samples and features!

recurrent gan

   let us then replace the mlps we used before in our gans by id56s as
   proposed [28]here. in particular, let   s make those id56s long short-term
   memory (lstm) units (we really are dealing with the buzziest of the
   buzz words of deep learning         oops, i did it again) and try what i very
   originally called the waves dataset. this dataset contains 1-d
   sinusoidal and sawtooth signals of different offsets, frequencies and
   amplitudes, all with the same number of time steps. from the id56   s
   point of view, each sample consists of one wave with 30 time steps.

   let us then run our cgan with both generator and discriminator networks
   as lstm-based neural networks, turning it into an rcgan that will be
   trained to learn to generate sinusoidal and sawtooth waves on demand:
   [1*qvgvidx2cehjpse_zkhy_w.png]
   figure 13: left: generated sinusoidal waves. right: generated
   sawtooth waves.

   after training, we may also inspect how variations in the latent space
   yield a continuous variation in the characteristics of the generated
   samples. in particular, if we impose a 2d normally distributed latent
   space and fix the class label to sinusoidal waves we get the samples
   shown in figure 14. there, a clear continuous variation between low and
   high frequencies and amplitudes is observed, meaning that the rcgan
   learned a meaningful latent space!
   [1*ijoy4eiitsz1x2djos4aiw.png]
   figure 14: fixing the label and sampling from neighboring points in the
   latent rcgan space

   while using id56s in gans is useful for real-valued sequential data
   generation, it still doesn   t work for discrete sequences, and using the
   wasserstein distance using id56s is not yet a clear option (enforcing
   the lipschitz constraint on an id56 is a topic for further research).
   some ideas to note that aim at solving this issue are [29]seqgan and
   more recently the [30]arae .

conclusion

   we have seen that aside from all the fuss being generated (get it?)
   around gans    ability to generate really cool pictures, some
   architectures may also be useful for more general machine learning
   problems containing continuous and discrete-valued data. this post
   served as an introduction to that idea and was not intended to be a
   hard comparison between multi-purpose generative models, but it does
   prove that such a study involving gans is bound to be done.

   note: the work shown here was developed in its entirety during a summer
   internship i took at [31]jungle.ai.

   originally published at [32]towardsdatascience.com on october 5, 2017.

   thanks to [33]alexander helmer and [34]silvio rodrigues.
     * [35]machine learning
     * [36]deep learning
     * [37]unsupervised learning
     * [38]data science
     * [39]towards data science

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of pedro ferreira

[41]pedro ferreira

   interested in machine learning.

     (button) follow
   [42]jungle book

[43]jungle book

   insights from applied research in ai and refreshing perspectives on
   business, people and design. by and for explorers of unknown territory.
   see [44]https://jungle.ai

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [45]jungle book
   never miss a story from jungle book, when you sign up for medium.
   [46]learn more
   never miss a story from jungle book
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9dd64e9628e6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/jungle-book?source=avatar-lo_xsgudl486ttt-d3d11ca83b1
   7. https://medium.com/jungle-book?source=logo-lo_xsgudl486ttt---d3d11ca83b1
   8. https://medium.com/m/signin?redirect=https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@pfferreira?source=post_header_lockup
  11. https://medium.com/@pfferreira
  12. https://www.quora.com/what-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning
  13. https://deephunt.in/the-gan-zoo-79597dc8c347
  14. https://arxiv.org/abs/1406.2661
  15. https://scikit-optimize.github.io/
  16. https://www.reddit.com/r/machinelearning/comments/40ldq6/generative_adversarial_networks_for_text/
  17. http://www.kaggle.com/
  18. https://arxiv.org/abs/1701.07875
  19. http://www.alexirpan.com/2017/02/22/wasserstein-gan.html
  20. https://arxiv.org/abs/1704.00028
  21. https://arxiv.org/abs/1704.00028
  22. https://www.kaggle.com/c/sberbank-russian-housing-market
  23. https://arxiv.org/abs/1705.10743
  24. https://arxiv.org/abs/1605.09782
  25. https://arxiv.org/abs/1606.03657
  26. https://arxiv.org/abs/1511.05644
  27. https://www.cs.toronto.edu/~hinton/csc2535/notes/lec11new.pdf
  28. https://arxiv.org/abs/1706.02633
  29. https://arxiv.org/abs/1609.05473
  30. https://arxiv.org/abs/1706.04223
  31. http://jungle.ai/
  32. https://towardsdatascience.com/towards-data-set-augmentation-with-gans-9dd64e9628e6
  33. https://medium.com/@amchelmer?source=post_page
  34. https://medium.com/@sfrodrigues?source=post_page
  35. https://medium.com/tag/machine-learning?source=post
  36. https://medium.com/tag/deep-learning?source=post
  37. https://medium.com/tag/unsupervised-learning?source=post
  38. https://medium.com/tag/data-science?source=post
  39. https://medium.com/tag/towards-data-science?source=post
  40. https://medium.com/@pfferreira?source=footer_card
  41. https://medium.com/@pfferreira
  42. https://medium.com/jungle-book?source=footer_card
  43. https://medium.com/jungle-book?source=footer_card
  44. https://jungle.ai/
  45. https://medium.com/jungle-book
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/p/9dd64e9628e6/share/twitter
  49. https://medium.com/p/9dd64e9628e6/share/facebook
  50. https://medium.com/p/9dd64e9628e6/share/twitter
  51. https://medium.com/p/9dd64e9628e6/share/facebook
