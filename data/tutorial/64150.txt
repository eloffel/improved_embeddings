   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

get started with nlp (part i)

   [9]go to the profile of sigmoider
   [10]sigmoider (button) blockedunblock (button) followfollowing
   nov 29, 2017
   [1*abwp7flwflwdchlgv4cgig.jpeg]
   image from [11]this source

   this is the first part of a series of natural language processing
   tutorials for beginners. next post gives an [12]introduction to nlp
   workflows.

introduction:

   nowadays, language is getting a lot of importance due to the recent
   boom of the so-called chatbots or conversational agents in several
   industries. but, as it happens with other fields of human knowledge,
   the study of natural language has a long past and these agents are not
   the first important application. having a deep understanding of
   language is very important since we use it everyday in different
   scenarios and with different behaviors.

   this series of tutorials have the purpose of serving as an introduction
   to the amazing field of study that constitutes natural language
   processing.
     __________________________________________________________________

what is nlp / ml / dl?

   appart from an approach to communication, personal development,
   and psychotherapy (neuro linguistic programming), nlp stands for
   natural language processing, which is defined as the application of
   computational techniques to the analysis and synthesis of natural
   language and speech. in other words: the use of different techniques
   from computer science (algorithms) to understand and manipulate human
   language and speech.

   sometimes nlp is confused with machine learning (ml), but this is just
   because some tools from ml are applied to nlp and improve the field. we
   can think of ml as a set of tools such that some of this tools are
   useful to solve nlp tasks.

   also, to make this clear, deep learning (dl) is a branch of ml that
   makes use of a specific type of architectures or models named neural
   networks to solve learning tasks. we can think of dl models as a subset
   of tools within the set of ml models. it is not the only existing
   branch (there are others, such as id107), but it is
   obtaining a lot of importance in the ml community due to two important
   reasons:
    1. the performance of this models when you have a great amount of data
       is noticeable and nowadays we have such data in several scenarios.
    2. recent advances in computing capability with the release of better
       hardware (graphic processing units, gpus) and sometimes also
       dedicated (tensor processing units, tpus) are making this models
       more feasible.

   this is important, since most of the current state of the art of nlp is
   being obtained through applying dl models. we will explain key concepts
   of dl for nlp in a later post, but now it is better to focus on the nlp
   basics.

   finally, in machine learning the source is known as dataset, but in nlp
   and in general when our dataset is a large collection of texts, we
   usually talk about the corpus.
     __________________________________________________________________

nlp vs. computational linguistics:

   there   s an area which is closely related to nlp and sometimes confused
   with it, that is computational linguistics. as jason eisner points out,
   the difference is the following:
     * computational linguistics is a more theoretical field that develops
       computational methods to answer the scientific questions from the
       point of view of linguists.
     * natural language processing is dedicated to give solutions to
       engineering problems related to natural language, focusing on the
       people.

   both fields make use of computer science, linguistics, and machine
   learning.
     __________________________________________________________________

main nlp challenges:

   the development of nlp has its meaning because of some specific
   problems and phenomena that arrive when we study natural language. most
   of the times, these problems are unique in comparison to the problems
   that emerge in other fields of computer science or engineering, and
   that is in part what makes nlp such an interesting and different area.
   [1*gbcgmit0r4hktr-92pizqw.jpeg]
   ambiguity
     * ambiguity: the main challenge of nlp is the understanding and
       modelling of elements within a variable context. in language, words
       are unique but can have different meanings depending on the context
       in which they are being evaluated. this results in the most
       important and sounded linguistic phenomena: ambiguity. we can have
       words (or even sentences) with different meanings in the same
       sentence depending on the way we interpret these words. this
       happens because of the difference between signifier (the way we
       represent the information, word) and signified (the meaning of that
       information, concept).

   [0*hgxuvokeocs1qklq.png]
   synonymy
     * synonymy: other key phenomenon of natural language is that we can
       express the same idea with different terms. this occurs because of
       synonymy, which is also dependent of the specific context: fine is
       synonym of correct in the context of performance, but it is synonym
       of thin in the context of density or profundity or a synonym of
       penalty fee in the context of punishments.

   [0*6lfk29lt7wm8exvk.jpeg]
   syntax
     * syntax: other peculiarity of natural language is its structure,
       which takes into account several rules but also some irregularities
       in different cases. we can also reorder a sentence in different
       ways but not every ordering of the terms is valid.

   [0*d6nfdznyb5tfzki-.png]
   coreference
     * coreference: other challenge that we face everyday although we
       perfectly cope with it in our conversations is the reference to
       specific concepts that were mentioned in an earlier sentence or
       directly omitted because they are deduced from the context. this is
       called coreference.

   [0*wcq96jvrkllc4czn.jpg]
   id172 vs. information
     * id172 vs. information: when we process natural language, in
       order to be able to manage it in a more general way, we need to
       normalize it. this means that depending on the task we would want
       all the words to be lowercased or to convert plural terms into
       singular ones if we don   t want to consider dog and dogs as two
       different entities. other times we might encounter different forms
       of the same verb in a document and we would want to consider just
       that verb instead of making a distinction between each form. all
       these processes normalize natural language in some way, and we will
       learn the techniques that are used to achieve it, but the key idea
       here is that when we normalize we are losing part of the
       information in exchange of being able to generalize better. this
       id172/information trade-off is common in the study of data,
       but also very important in the study of natural language.

   [0*axauq_m_zp4nnug1.png]
   word representations
     * representation: language is composed of characters which we say
       that are discrete values because they can only take certain values:
          a   ,     b   ,    c   , etc. in stead of continuous ones, that can take any
       value within a range (like 0, 0.1, 0.025 or 0.5 in the range
       between 0 and 1), since there is no possible value between    a    and
           b    in the range between    a    and    z   . it is easier to process data
       when it has continues features since, in terms of learning, we can
       obtain a number that is close to the value that we want with a
       certain error (say for example 0.99 to 1 with error of 0.01), but
       we can   t approximate the word    tree    with a certain error. we will
       learn that one way of coping with this problem and relating
       different terms is to transform words into vectors (called word
       embeddings).

   [0*vhvwghijirxytayh.jpg]
   irony and sarcasm
     * personality, intention and style: the are also different styles to
       express the same idea depending on the personality or the intention
       in a specific scenario. some of them (such as irony or sarcasm) may
       have an opposite idea from the one that can be initially thought
       due to the context. we can state for example    oh, great    referring
       to a feeling of joy but also to the completely opposite feeling if
       we are being sarcastic.
     __________________________________________________________________

some basic nlp techniques:

   now that we know what is and is not nlp and what problems does it face,
   we can start to learn which are the most basic nlp tools. in the next
   post we will apply these techniques using a python nlp library called
   [13]spacy. this post focuses on the concepts.

   a. id30 and lemmatizing: this tasks consist of reducing different
   forms of a word to a common base form. for example:
     * in the sentence    i am a student    the process would result in    i be
       a student   .

   [0*uddqoomij-njh-bh.png]
     * in the sentence    my dog   s fur is dark    the process would result in
          my dog fur be dark   .

   [0*mcvjaxjifypkikxr.png]

   id30 usually refers to a crude process that chops off the ends of
   words in the hope of achieving this goal correctly most of the time,
   and often includes the removal of derivational units (the obtained
   element is known as the stem).

   on the other hand, lemmatization consists in doing things properly with
   the use of a vocabulary and morphological analysis of words, to return
   the base or dictionary form of a word, which is known as the lemma.

   if we stem the sentence    i saw an amazing thing    we would obtain    s   
   instead of    saw   , but if we lemmatize it we would obtain    see   , which
   is the lemma.
   [0*ytnh6cyfok9ol-0d.png]

   as it was already mentioned, both techniques could remove important
   information but also help us to normalize our corpus (although
   lemmatization is the one that is usually applied).

   b. coreference resolution: consists of solving the coreferences that
   are present in our corpus. this can also be thought as a normalizing
   or preprocessing task.

   c. part-of-speech (pos) tagging: a pos tagger marks each word in a
   corpus by assigning a syntactic category such as:
     * open class categories or types (those with relatively fixed
       membership): noun, verb, adjective, adverb.
     * closed class types: preposition, determiner, pronoun, conjunction,
       auxiliary verb, particle, numeral.

   for example, given the sentence    i want to play the piano    a pos tagger
   should return:
i (preposition)
want (verb)
play (verb)
piano (noun)

   d. id33: sometimes instead of the category (pos tag) of a
   word we want to know the role of that word in a specific sentence of
   our corpus, this is the task of dependency parsers. the objective is to
   obtain the dependencies or relations of words in the format of a
   dependency tree.

   the considered dependencies are in general terms subject, object,
   complement and modifier relations.

   as an example, given the sentence    i want to play the piano    a
   dependency parser would produce the following tree:
   [1*y7fvjfgbyog6p2je84fvxg.png]
   spacy   s dependency tree visualized with [14]displacy

   here we can see that the dependency parser that i use (spacy   s
   dependency parser) also outputs the pos tags. if you think about it, it
   makes sense because we first need to know the category of each word to
   extract dependencies.

   we will see in detail the types of dependencies, but in this case we
   have:
want     i: nominal subject.
want     play: open clausal complement.
play     to: auxiliary verb.
play     the piano: direct object

   where a         b: r means    b is r of a   . for example,    the piano is direct
   object of play    (which is play         the piano: direct object from above).

   e. id39 (ner):

   in the real world, in our daily conversations we don   t work directly
   with the categories of words. instead, for example, if we want to build
   a netflix chatbot we want it to recognize both    batman    and    avatar    as
   instances of the same group which we call    films    , but    steven
   spielberg    as a    director   . this concept of semantic field dependent of
   a context is what we define as entity. the role of a named entity
   recognizer is to detect relevant entities in our corpus.

   for example, if our ner knows the entities    film   ,    location    and
      director   , given the sentence    james cameron filmed part of avatar in
   new zealand   , it will output:
james cameron: director
avatar: film
new zealand: location

   note that in the example instances of entities can be just a single
   word (   avatar   ) or several ones (   new zealand    or    james cameron   ).
     __________________________________________________________________

what comes next:

   congratulations! now you know the very basics of nlp so we can move to
   action and start learning to use this tools. the next post gives an
   [15]introduction to nlp workflows. this was my first [16]medium article
   so i hope that it was as useful and clear as possible to you. thank you
   very much for reading it, i   m open to and will thank any (respectful)
   form of suggestions and questions.

   sigmoider

   ([17]twitter)
     __________________________________________________________________

resources:

     * [18]computational linguistics and nlp
     * [19]id30 and lemmatizing
     * [20]id52
     * [21]id33
     * [22]geometry and meaning: a very recommended book.
     * [23]spacy
     * [24]nlpforhackers: an interesting resource.

     * [25]machine learning
     * [26]nlp
     * [27]natural language
     * [28]artificial intelligence
     * [29]ai

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of sigmoider

[31]sigmoider

   nlp research, ai and music.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d67ca26cc828
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@gon.esbuyo?source=post_header_lockup
  10. https://medium.com/@gon.esbuyo
  11. http://pictures-and-images.net/single/70_circuit-minimalistic-hd-wallpapers_14.html
  12. https://medium.com/@11113a8761d7/7ba1f5948b24
  13. http://spacy.io/
  14. https://demos.explosion.ai/displacy/?text=i want to play the piano&model=en_core_web_sm&cpu=1&cph=1
  15. https://medium.com/@11113a8761d7/7ba1f5948b24
  16. https://medium.com/@medium
  17. https://twitter.com/sigmoider
  18. https://www.quora.com/how-is-computational-linguistics-different-from-natural-language-processing
  19. https://nlp.stanford.edu/ir-book/html/htid113dition/id30-and-lemmatization-1.html
  20. https://web.stanford.edu/~jurafsky/slp3/10.pdf
  21. https://web.stanford.edu/~jurafsky/slp3/14.pdf
  22. https://www.amazon.co.uk/geometry-meaning-dominic-widdows/dp/1575864487
  23. https://spacy.io/
  24. http://nlpforhackers.io/start/
  25. https://medium.com/tag/machine-learning?source=post
  26. https://medium.com/tag/nlp?source=post
  27. https://medium.com/tag/natural-language?source=post
  28. https://medium.com/tag/artificial-intelligence?source=post
  29. https://medium.com/tag/ai?source=post
  30. https://medium.com/@gon.esbuyo?source=footer_card
  31. https://medium.com/@gon.esbuyo

   hidden links:
  33. https://medium.com/p/d67ca26cc828/share/twitter
  34. https://medium.com/p/d67ca26cc828/share/facebook
  35. https://medium.com/p/d67ca26cc828/share/twitter
  36. https://medium.com/p/d67ca26cc828/share/facebook
