   #[1]deep learning course wiki (en) [2]deep learning course wiki atom
   feed

calculus for deep learning

   from deep learning course wiki
   jump to: [3]navigation, [4]search

   work in progress!

contents

     * [5]1 derivatives
          + [6]1.1 definition
          + [7]1.2 calculation
          + [8]1.3 walk through example
               o [9]1.3.1 steps
               o [10]1.3.2 result
          + [11]1.4 formula
          + [12]1.5 code
          + [13]1.6 derivatives in deep learning
          + [14]1.7 tutorials
     * [15]2 gradients
          + [16]2.1 partial derivatives
          + [17]2.2 walk-through example
               o [18]2.2.1 steps
               o [19]2.2.2 result
          + [20]2.3 directional derivatives
               o [21]2.3.1 calculation
          + [22]2.4 properties of gradients
               o [23]2.4.1 why direction of steepest ascent?
          + [24]2.5 code
          + [25]2.6 gradients in deep learning
               o [26]2.6.1 id119
               o [27]2.6.2 adjusting weights
     * [28]3 chain rule
          + [29]3.1 how it works
          + [30]3.2 walk-through example
               o [31]3.2.1 steps
          + [32]3.3 higher dimensions
               o [33]3.3.1 example
               o [34]3.3.2 sources
          + [35]3.4 chain rule in deep learning
     * [36]4 resources
     * [37]5 sources

derivatives

   a derivative can be defined in two ways:
    1. instantaneous rate of change (physics)
    2. slope of a line at a specific point (geometry)

   both represent the same principle, but for our purposes it   s easier to
   explain using the geometric definition.

definition

   in geometry slope represents the steepness of a line. it answers the
   question: how much does y or f(x) change given a specific change in x ?

   [38]calculus slope basic.png

   using this definition we can easily calculate the slope between two
   points:

   [39]calculus slope example.png

   but what if i asked you, instead of the slope between two points, what
   is the slope at a single point on the line? in this case there isn   t
   any obvious    rise over run    to calculate. derivatives help us answer
   this question.

   a derivative outputs an expression we can use to calculate the
      instantaneous rate of change    or slope at a single point on a line.
   after solving for the derivative you can use it to calculate the slope
   at every other point on the line.

calculation

   consider the graph below, where : f(x) = x^2+3. .

   [40]calculus slope calculation example secant.png

   the slope between (1,4) and (3,12) would be:

          \frac{y2-y1}{x2-x1} = \frac{12-4}{3-1} = 4

   but how do we calculate the slope at point (1,4) to reveal the change
   in slope at that specific point?
     * option 1: find the two nearest points, calculate their slopes
       relative to x and take the average.
     * option 2: calculate the slope from x to another point an
       infinitesimally small distance away from x
     * option 3: calculate the derivative

   option 2 and 3 are essentially the same. what happens to the output
   f(x) when we make a very very tiny increase to x? in this way,
   derivatives estimate the slope between two points that are an
   infinitesimally small distance away from each other. a very, very, very
   small distance, but large enough to calculate the slope.

   in math language we represent this infinitesimally small increase using
   a limit. a limit is defined as the output value a function "approaches"
   as the input value approaches another value. in our case the target
   value is the specific point at which we want to calculate slope.

walk through example

   calculating the derivative is the same as calculating normal slope,
   however in this case we calculate the slope between our point and a
   point infinitesimally close to it. we use the variable h to represent
   this infinitesimally distance.

steps

   1. given the function

          f(x) = x^2

   2. increment x by a very small value h (h =   x)

          f(x + h) = (x + h)^2

   3. apply the slope formula

          \frac{f(x + h) - f(x)}{h}

   4. simplify the equation

          \frac{x^2+2xh+h^2-x^2}{h} = \frac{2xh+h^2}{h} = 2x+h

   5. set h to 0 (the limit as h heads toward 0)

          {2x + 0} = {2x}

result

   so what does this mean? it means for f(x) = x^2 , the slope at any
   point can be calculated using the function 2x .

formula

          \frac{df}{dx} = \lim_{h\to0}\frac{f(x+h) - f(x)}{h}

code

   let's write code to calculate the derivative for f(x) = x^2 . we know
   the derivative should be 2x .
def xsquared(x):
    return x**2

def getderiv(func, x):
  h = 0.0001
  return (func(x+h) - func(x)) / h

x = 3
derivative = getderiv(xsquared, x)
actual = 2*x

derivative, actual = 6.0001, 6

derivatives in deep learning

   machine learning uses derivatives to find optimal solutions to
   problems. it   s useful in optimization functions like id119
   because it helps us decide whether to increase or decrease our weights
   in order to maximize or minimize some metrics (e.g. loss)

   it also helps us model nonlinear functions as linear functions (tangent
   lines), which have constant slopes. with a constant slope we can decide
   whether to move up or down the slope (increase or decrease our weights)
   to get closer to the target value (class label).

tutorials

     * [41]excellent explanation starting at 1:05

gradients

   a gradient is a vector that stores the partial derivatives of
   multivariable functions. it helps us calculate the slope at a specific
   point on a curve for functions with multiple independent variables. in
   order to calculate this more complex slope, we need to isolate each
   variable to determine how it impacts the output on its own. to do this
   we iterate through each of the variables and calculate the derivative
   of the function after holding all other variables constant. each
   iteration produces a partial derivative which we store in the gradient.

partial derivatives

   in functions with 2 or more variables, the partial derivative is the
   derivative of one variable with respect to the others. if we change x ,
   but hold all other variables constant, how does f(x,z) change? that   s
   one partial derivative. the next variable is z . if we change z but
   hold x constant, how does f(x,z) change? we store partial derivatives
   in a gradient, which represents the full derivative of the
   multivariable function.

walk-through example

   here is an example of calculating the gradient for a multivariable
   function.

steps

   1. given a multivariable function

          f(x,z) = 2z^3x^2

   2. calculate the derivative with respect to x

          \frac{df}{dx}(x,z) =  ??

   3. swap 2z^3 with a constant value b

          f(x,z) = bx^2

   4. calculate the derivative with b constant

          \lim_{h\to0}\frac{f(x+h) - f(x)}{h}
          \lim_{h\to0}\frac{b(x+h)^2 - b(x^2)}{h}
          \lim_{h\to0}\frac{b((x+h)(x+h)) - bx^2}{h}
          \lim_{h\to0}\frac{b((x^2 + xh + hx + h^2)) - bx^2}{h}
          \lim_{h\to0}\frac{bx^2 + 2bxh + bh^2 - bx^2}{h}
          \lim_{h\to0}\frac{2bxh + bh^2}{h}
          \lim_{h\to0}\frac{2bxh + bh^2}{h}
          \lim_{h\to0} 2bx + bh

   as h    > 0

          2bx + 0

   5. swap 2z^3 back into the equation

          2(2z^3)x
          4z^3x

   6. arrive at the partial derivative with respect to x

          \frac{df}{dx}(x,z) = 4z^3x

   7. repeat steps above to calculate the derivative with respect to z

          \frac{df}{dz}(x,z) = 6x^2z^2

   8. store the partial derivatives in a gradient

          \nabla f(x,z)=\begin{bmatrix} \frac{df}{dx} \\ \frac{df}{dz} \\
          \end{bmatrix} =\begin{bmatrix} 4z^3x \\ 6x^2z^2 \\ \end{bmatrix}

result

   how does this gradient relate to the original function? how can it be
   used?

directional derivatives

   another important concept is directional derivatives. when calculating
   the partial derivatives of multivariable functions we use our old
   technique of analyzing the impact of infinitesimally small increases to
   each of our independent variables. by increasing each variable we alter
   the function output in the direction of the slope.

   but what if we want to change directions? for example, imagine we   re
   traveling north through mountainous terrain on a 3-dimensional plane.
   the gradient we calculated above tells us we   re traveling north at our
   current location. but what if we wanted to travel southwest? how can we
   determine the steepness of the hills in the southwest direction?
   directional derivatives help us find the slope if we move in a
   direction different from the one specified by the gradient.

calculation

   the directional derivative is computed by taking the [42]dot product of
   the gradient of f and a unit vector \vec{v} of "tiny nudges"
   representing the direction. the unit vector describes the proportions
   we want to move in each direction. the output of this calculation is a
   scalar number representing how much f will change if the current input
   moves with vector \vec{v} .

   [43]kahn academy explains this great. i'll replay it here:

   let's say you have the function f(x,y,z) and you want to compute its
   directional derivative along the following vector:

          \vec{v}=\begin{bmatrix} 2 \\ 3 \\ -1 \\ \end{bmatrix}

   as described above, we take the dot product of the gradient and the
   directional vector:

          \begin{bmatrix} \frac{df}{dx} \\ \frac{df}{dx} \\ \frac{df}{dx}
          \\ \end{bmatrix} \cdot \begin{bmatrix} 2 \\ 3 \\ -1 \\
          \end{bmatrix}

   we can rewrite the dot product as:

          \nabla_\vec{v} f = 2 \frac{df}{dx} + 3 \frac{df}{dy} - 1
          \frac{df}{dz}

   this should make sense because a tiny nudge along \vec{v} can be broken
   down into two tiny nudges in the x-direction, three tiny nudges in the
   y-direction, and a tiny nudge backwards, by    1 in the z-direction.

properties of gradients

   there are two additional properties of gradients that are especially
   useful in deep learning. a gradient:
    1. always points in the direction of greatest increase of a function
       ([44]explained here)
    2. is zero at a local maximum or local minimum

why direction of steepest ascent?

   let's start with a simple linear function like f(x) = 3x . remember the
   derivative of a linear function equals the function's slope. in this
   case the derivative equals 3. since linear functions only have one
   independent variable, we only have two levers to manipulate f(x) :
   increase or decrease x . if the derivative (slope) of our function is
   positive it means we should increase x to increase the output of our
   function. moving x in a negative direction would decrease the output. a
   positive slope requires a positive change to increase f(x) .

   for negative slopes the logic is reversed. to increase the function
   output we need to decrease x . a negative slope requires a negative
   change to increase f(x) . from this we can see how, for linear
   functions, the direction of steepest ascent equals the direction of the
   derivative.

   [graph] and [graph of derivative]

   the same logic extends to higher-order functions. let's consider a
   multivariable function f(x,y) = 2x^2 - y^2 in a 3-dimensional space.
   the gradient of this function would be:

          \begin{bmatrix} \frac{df}{dx} \\ \frac{df}{dy} \\ \end{bmatrix}
          = \begin{bmatrix} 4x \\ -2y \\ \end{bmatrix}

   [insert explanation here]

code

   here is some code for calculating the gradient in python.
     * [45]numpy code for gradient

gradients in deep learning

   gradients are used in neural networks to adjust weights and optimize
   cost functions.

id119

   the relevant concept in deep learning is called id119. in
   id119, neural networks use property #1 above to determine
   how to adjust their weights for each variable (feature) in the model.
   rather than moving in the direction of greatest increase, as specified
   by the gradient, neural networks move in the opposite direction to
   minimize a id168, like error percent or log loss. after
   adjusting their weights, neural networks compute the gradient again and
   move in the direction opposite to the one specified by the gradient.

adjusting weights

   neural networks use the concept of directional derivatives to adjust
   their weights. after computing the gradient of the current weights
   (features), neural networks identify the direction of greatest decrease
   to the id168, and then multiply the current weights by a vector
   (or matrix) containing the direction and magnitude that minimizes the
   id168. the networks can change their weights with varying
   magnitudes, but the changes must be proportional to maintain the proper
   direction of greatest decrease.

chain rule

   the chain rule is a formula for calculating the derivatives of
   composite functions. composite functions are functions composed of
   functions inside other function(s).

how it works

   given a composite function f(x) = a(b(x)) , the derivative of f(x)
   equals the product of the derivative of a with respect to b(x) and the
   derivative of b with respect to x .

          \mbox{composite function derivative} = \mbox{outer function
          derivative} * \mbox{inner function derivative}

   for example, given a composite function f(x) , where:

          f(x) = h(g(x))

   the chain rule tells us that the derivative of f(x) equals:

          \frac{df}{dx} = \frac{dh}{dg} \cdot \frac{dg}{dx}

walk-through example

   say f(x) is composed of two functions h(x) = x^3 and g(x) = x^2 . and
   that:

          f(x) = h(g(x))
          f(x) = (x^2)^3

   the derivative of f(x) would equal:

          \frac{df}{dx} = \frac{dh}{dg} \frac{dg}{dx} = \frac{dh}{d(x^2)}
          \frac{dg}{dx}

steps

   1. solve for the inner derivative of g(x) = x^2

          \frac{dg}{dx} = 2x

   2. solve for the outer derivative of h(x) = x^3 , using a placeholder b
   to represent the inner function x^2

          \frac{dh}{db} = 3b^2

   3. swap out the placeholder variable for the inner function

          3x^4

   4. return the product of the two derivatives

          3x^4 \cdot 2x = 6x^5

higher dimensions

   in the above example we assumed a composite function containing a
   single inner function. but the chain rule can also be applied to
   higher-order functions like:

          f(x) = a(b(c(x)))

   the chain rule tells us that the derivative of this function equals:

          \frac{df}{dx} = \frac{da}{db} \frac{db}{dc} \frac{dc}{dx}

   we can also write this derivative equation f' notation:

          f' = a'(b(c(x)) \cdot b'(c(x)) \cdot c'(x)

example

   given the function f(x) = a(b(c(x))) , lets assume:

          a(x) = sin(x)
          b(x) = x^2
          c(x) = 4x

   the derivatives of these functions would be:

          a'(x) = cos(x)
          b'(x) = 2x
          c'(x) = 4

   we can calculate the derivative of f(x) using the following formula:

          f'(x) = a'( (4x)^2) \cdot b'(4x) \cdot c'(x)

   we then input the derivatives and simplify the expression:

          f'(x) = cos((4x)^2) \cdot 2(4x) \cdot 4
          f'(x) = cos(16x^2) \cdot 8x \cdot 4
          f'(x) = cos(16x^2)32x

sources

     * [46]chain rule khan academy
     * [47]chain rule wikipedia

chain rule in deep learning

resources

     * [48]basic differentiation
     * [49]the chain rule

sources

     * [50]wikipedia derivatives
     * [51]wikipedia partial derivatives
     * [52]wikipedia gradients
     * [53]understanding the gradient
     * [54]understanding the gradient
     * [55]khan academy partial derivative and gradient
     * [56]khan academy directional derivative
     * [57]derivatives basic intro
     * [58]definition of derivative
     * [59]algebra review
     * [60]intro to derivatives
     * [61]latex symbols
     * [62]chain rule wikipedia
     * [63]khan academy chain rule
     * [64]chain rule lamar.edu

   retrieved from
   "[65]http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&old
   id=1483"

navigation menu

personal tools

     * [66]log in

namespaces

     * [67]page
     * [68]discussion

variants

views

     * [69]read
     * [70]view source
     * [71]view history

more

search

   ____________________ search go

navigation

     * [72]main page
     * [73]recent changes
     * [74]random page
     * [75]help

tools

     * [76]what links here
     * [77]related changes
     * [78]special pages
     * [79]printable version
     * [80]permanent link
     * [81]page information

     * this page was last modified on 28 january 2017, at 18:37.

     * [82]privacy policy
     * [83]about deep learning course wiki
     * [84]disclaimers

     * [85]powered by mediawiki

references

   visible links
   1. http://wiki.fast.ai/opensearch_desc.php
   2. http://wiki.fast.ai/index.php?title=special:recentchanges&feed=atom
   3. http://wiki.fast.ai/index.php/calculus_for_deep_learning#mw-head
   4. http://wiki.fast.ai/index.php/calculus_for_deep_learning#p-search
   5. http://wiki.fast.ai/index.php/calculus_for_deep_learning#derivatives
   6. http://wiki.fast.ai/index.php/calculus_for_deep_learning#definition
   7. http://wiki.fast.ai/index.php/calculus_for_deep_learning#calculation
   8. http://wiki.fast.ai/index.php/calculus_for_deep_learning#walk_through_example
   9. http://wiki.fast.ai/index.php/calculus_for_deep_learning#steps
  10. http://wiki.fast.ai/index.php/calculus_for_deep_learning#result
  11. http://wiki.fast.ai/index.php/calculus_for_deep_learning#formula
  12. http://wiki.fast.ai/index.php/calculus_for_deep_learning#code
  13. http://wiki.fast.ai/index.php/calculus_for_deep_learning#derivatives_in_deep_learning
  14. http://wiki.fast.ai/index.php/calculus_for_deep_learning#tutorials
  15. http://wiki.fast.ai/index.php/calculus_for_deep_learning#gradients
  16. http://wiki.fast.ai/index.php/calculus_for_deep_learning#partial_derivatives
  17. http://wiki.fast.ai/index.php/calculus_for_deep_learning#walk-through_example
  18. http://wiki.fast.ai/index.php/calculus_for_deep_learning#steps_2
  19. http://wiki.fast.ai/index.php/calculus_for_deep_learning#result_2
  20. http://wiki.fast.ai/index.php/calculus_for_deep_learning#directional_derivatives
  21. http://wiki.fast.ai/index.php/calculus_for_deep_learning#calculation_2
  22. http://wiki.fast.ai/index.php/calculus_for_deep_learning#properties_of_gradients
  23. http://wiki.fast.ai/index.php/calculus_for_deep_learning#why_direction_of_steepest_ascent.3f
  24. http://wiki.fast.ai/index.php/calculus_for_deep_learning#code_2
  25. http://wiki.fast.ai/index.php/calculus_for_deep_learning#gradients_in_deep_learning
  26. http://wiki.fast.ai/index.php/calculus_for_deep_learning#gradient_descent
  27. http://wiki.fast.ai/index.php/calculus_for_deep_learning#adjusting_weights
  28. http://wiki.fast.ai/index.php/calculus_for_deep_learning#chain_rule
  29. http://wiki.fast.ai/index.php/calculus_for_deep_learning#how_it_works
  30. http://wiki.fast.ai/index.php/calculus_for_deep_learning#walk-through_example_2
  31. http://wiki.fast.ai/index.php/calculus_for_deep_learning#steps_3
  32. http://wiki.fast.ai/index.php/calculus_for_deep_learning#higher_dimensions
  33. http://wiki.fast.ai/index.php/calculus_for_deep_learning#example
  34. http://wiki.fast.ai/index.php/calculus_for_deep_learning#sources
  35. http://wiki.fast.ai/index.php/calculus_for_deep_learning#chain_rule_in_deep_learning
  36. http://wiki.fast.ai/index.php/calculus_for_deep_learning#resources
  37. http://wiki.fast.ai/index.php/calculus_for_deep_learning#sources_2
  38. http://wiki.fast.ai/index.php/file:calculus_slope_basic.png
  39. http://wiki.fast.ai/index.php/file:calculus_slope_example.png
  40. http://wiki.fast.ai/index.php/file:calculus_slope_calculation_example_secant.png
  41. https://youtu.be/phmznw8agq4?t=1m5s
  42. https://en.wikipedia.org/wiki/dot_product
  43. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction
  44. https://betterexplained.com/articles/understanding-pythagorean-distance-and-the-gradient
  45. https://docs.scipy.org/doc/numpy/reference/generated/numpy.gradient.html
  46. https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/a/chain-rule-overview
  47. https://en.wikipedia.org/wiki/chain_rule#higher_dimensions
  48. http://wiki.fast.ai/index.php/basic_differentiation
  49. http://wiki.fast.ai/index.php/the_chain_rule
  50. https://en.wikipedia.org/wiki/derivative
  51. https://en.wikipedia.org/wiki/partial_derivative
  52. https://en.wikipedia.org/wiki/gradient
  53. https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/
  54. https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/
  55. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient
  56. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient-and-contour-maps
  57. https://www.mathsisfun.com/calculus/derivatives-introduction.html
  58. http://tutorial.math.lamar.edu/classes/calci/defnofderivative.aspx
  59. http://www.algebrahelp.com/lessons/simplifying/foilmethod/pg2.htm
  60. http://www.sosmath.com/calculus/diff/der00/der00.html
  61. http://csrgxtu.github.io/2015/03/20/writing-mathematic-fomulars-in-markdown/
  62. https://en.wikipedia.org/wiki/chain_rule#higher_dimensions
  63. https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction
  64. http://tutorial.math.lamar.edu/classes/calci/chainrule.aspx
  65. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&oldid=1483
  66. http://wiki.fast.ai/index.php?title=special:userlogin&returnto=calculus+for+deep+learning
  67. http://wiki.fast.ai/index.php/calculus_for_deep_learning
  68. http://wiki.fast.ai/index.php?title=talk:calculus_for_deep_learning&action=edit&redlink=1
  69. http://wiki.fast.ai/index.php/calculus_for_deep_learning
  70. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&action=edit
  71. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&action=history
  72. http://wiki.fast.ai/index.php/main_page
  73. http://wiki.fast.ai/index.php/special:recentchanges
  74. http://wiki.fast.ai/index.php/special:random
  75. https://www.mediawiki.org/wiki/special:mylanguage/help:contents
  76. http://wiki.fast.ai/index.php/special:whatlinkshere/calculus_for_deep_learning
  77. http://wiki.fast.ai/index.php/special:recentchangeslinked/calculus_for_deep_learning
  78. http://wiki.fast.ai/index.php/special:specialpages
  79. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&printable=yes
  80. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&oldid=1483
  81. http://wiki.fast.ai/index.php?title=calculus_for_deep_learning&action=info
  82. http://wiki.fast.ai/index.php/deep_learning_course_wiki:privacy_policy
  83. http://wiki.fast.ai/index.php/deep_learning_course_wiki:about
  84. http://wiki.fast.ai/index.php/deep_learning_course_wiki:general_disclaimer
  85. http://www.mediawiki.org/

   hidden links:
  87. http://wiki.fast.ai/index.php/calculus_for_deep_learning
  88. http://wiki.fast.ai/index.php/calculus_for_deep_learning
  89. http://wiki.fast.ai/index.php/main_page
