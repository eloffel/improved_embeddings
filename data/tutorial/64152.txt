   [1][qs-logo-trans-trimmed.png] (button)

     * [2]quantcademy
     * [3]books
     * [4]articles
     * [5]reading list
     * [6]qstrader

   [7]log in [8]join the quantcademy

matrix algebra - id202 for deep learning (part 2)

   matrix algebra - id202 for deep learning (part 2)

   by quantstart team

   last week [9]i posted an article, which formed the first part in a
   series on id202 for deep learning. the response to the article
   was extremely positive, both in terms of feedback, article views and
   also more broadly on social media.

   many of you commented that there was "an appetite" for introductory
   mathematical content and this only confirms the results of the
   [10]quantstart 2017 content survey. hence i've decided to write more
   introductory articles, not only continuing with id202, but
   also on the topics of calculus and id203, which are fundamental
   topics for machine learning   and quantitative finance more broadly.

   in the [11]previous article we introduced the three basic entities that
   will be used in id202, namely the scalar, vector and the
   matrix. we saw that they were all really specific versions of a more
   general entity known as a tensor.

   in this article we are going to look at how to form operations between
   these entities. such operations include addition and multiplication.
   while you will be very familiar with scalar addition and
   multiplication, the rules differ somewhat when dealing with more
   general tensor entities. this article will precisely define those
   operations and provide numeric examples to give you some intuition.

   at this stage it is not likely to be clear why these operations will be
   useful in the context of deep learning. however, in the previous
   article i stated that id202 was the 'language in which machine
   learning was written'. if we can understand the basics of the language,
   we'll be in a much better position to grasp the more complex ideas that
   form the backbone of neural network models in later articles.

   we will begin by looking at matrix addition and then consider matrix
   multiplication. these operations will eventually allow us to discuss a
   topic known as matrix inversion, which will form the basis of the next
   article.

matrix addition

   what does it mean to add two matrices together? in this section we will
   explore such an operation and hopefully see that it is actually quite
   intuitive.

   matrices can be added to scalars, vectors and other matrices. each of
   these operations has a precise definition. these techniques are used
   frequently in machine learning and deep learning so it is worth
   familiarising yourself with them.

matrix-matrix addition

   given two matrices of size $m \times n$, $\boldsymbol{a}=[a_{ij}]$ and
   $\boldsymbol{b}=[b_{ij}]$, it is possible to define the matrix
   $\boldsymbol{c}=[c_{ij}]$ as the matrix sum $\boldsymbol{c} =
   \boldsymbol{a} + \boldsymbol{b}$ where $c_{ij} = a_{ij} + b_{ij}$.

   that is, $\boldsymbol{c}$ is constructed by element-wise summing the
   respective elements of $\boldsymbol{a}$ and $\boldsymbol{b}$. this
   operation is only defined where the two matrices have equal size,
   except in the case of broadcasting below. the definition implies that
   $\boldsymbol{c}$ also has size $m \times n$.

   matrix addition is commutative. this means that it doesn't matter which
   way around the matrices are added:
   \begin{equation} \boldsymbol{a} + \boldsymbol{b} = \boldsymbol{b} +
   \boldsymbol{a} \end{equation}

   it is also associative. this means that you get the same result if you
   add two matrices together first, and then another, as if you add
   another two together first and then the other:
   \begin{equation} \boldsymbol{a} + (\boldsymbol{b} + \boldsymbol{c}) =
   (\boldsymbol{a} + \boldsymbol{b}) + \boldsymbol{c} \end{equation}

   both of these results follow from the fact that normal scalar addition
   is itself commutative and associative, because we're just adding the
   elements together.

   i'm stressing commutivity and associativity for matrix addition because
   id127 (defined below) is certainly not commutative.
   we'll see why later.

example

   consider two matrices $\boldsymbol{a}$ and $\boldsymbol{b}$. we can
   create a new matrix $\boldsymbol{c}$ through addition:
   \[ \boldsymbol{a} + \boldsymbol{b} = \left[ \begin{array}{ccc} 1 & 4 &
   17 \\ 18 & 3 & 2 \\ 5 & 19 & 1 \\ \end{array} \right] + \left[
   \begin{array}{ccc} 12 & 18 & 6 \\ 4 & 3 & 33 \\ 23 & 5 & 8 \\
   \end{array} \right] = \left[ \begin{array}{ccc} 13 & 22 & 23 \\ 22 & 6
   & 35 \\ 28 & 24 & 9 \\ \end{array} \right] = \boldsymbol{c} \]

   it is clear to see that the elements of $\boldsymbol{c}$ are simply the
   elements added in the corresponding positions from $\boldsymbol{a}$ and
   $\boldsymbol{b}$.

matrix-scalar addition

   it is possible to add a scalar value $x$ to a matrix
   $\boldsymbol{a}=[a_{ij}]$ to produce a new matrix
   $\boldsymbol{b}=[b_{ij}]$ where $b_{ij} = x + a_{ij}$. this simply
   means that we're adding the same scalar value to every element of the
   matrix. it is written as $\boldsymbol{b} = x + \boldsymbol{a}$.

   scalar-matrix addition is once again commutative and associative,
   because normal scalar addition is both commutative and associative.

broadcasting

   for certain applications in machine learning it is possible to define a
   shorthand notation known as broadcasting. consider $\boldsymbol{a} \in
   \mathbb{r}^{m \times n}$ an $m \times n$-dimensional real-valued matrix
   and $\boldsymbol{x} \in \mathbb{r}^m$ an $m$-dimensional vector.

   it is possible to define $\boldsymbol{b} = \boldsymbol{a} +
   \boldsymbol{x}$, despite the fact that the matrices $\boldsymbol{a}$
   and $\boldsymbol{x}$ are unequal in size. the definition of the sum
   takes $b_{ij} = a_{ij} + x_j$.

   that is, the elements of $\boldsymbol{x}$ are copied into each row of
   the matrix $\boldsymbol{a}$. since the value of $\boldsymbol{x}$ is
   `broadcast' into each row the process is known as broadcasting.

id127

   the rules for matrix addition are relatively simple and intuitive.
   however when it comes to multiplication of matrices the rules become
   more complex.

matrix transpose

   in order to define certain matrix-id127 operations such
   as the dot-product (discussed below) it is necessary to define the
   transpose of a matrix. the transpose of a matrix
   $\boldsymbol{a}=[a_{ij}]_{m \times n}$ of size $m \times n$ is denoted
   by $\boldsymbol{a}^{t}$ of size $n \times m$ and is given element-wise
   by the following expression:
   \begin{equation} \boldsymbol{a}^{t} = [a_{ji}]_{n \times m}
   \end{equation}

   that is, the indices $i$ and $j$ are swapped. this has the effect of
   reflecting the matrix along the line of diagonal elements $a_{ii}$. the
   operation is defined for non-square matrices, as well as vectors and
   scalars (which are simply $1 \times 1$ matrices). note that a scalar is
   its own transpose: $x = x^t$. in addition the transpose of the
   transpose of a matrix is simply itself: $\boldsymbol{a}^{tt} =
   \boldsymbol{a}$.

examples

   \[ \boldsymbol{a} = \left[ \begin{array}{ccc} a_{11} & a_{12} & a_{13}
   \\ a_{21} & a_{22} & a_{23} \end{array} \right], \quad \boldsymbol{a}^t
   = \left[ \begin{array}{cc} a_{11} & a_{21} \\ a_{12} & a_{22} \\ a_{13}
   & a_{23} \end{array} \right] \] \[ \boldsymbol{x} = \left[
   \begin{array}{c} x_{1} \\ x_{2} \\ x_{3} \end{array} \right], \quad
   \boldsymbol{x}^t = \left[ \begin{array}{ccc} x_{1} & x_{2} & x_3
   \end{array} \right] \]

   note here that $\boldsymbol{a}^{t}$ does not represent $\boldsymbol{a}$
   multiplied by itself $t$ times. this is an entirely different
   operation. however, it will usually be clear from the context whether
   we mean the transpose of a matrix or repeated multiplication by itself.

matrix-id127

   we are now going to consider matrix-id127. this is a
   more complex operation than matrix addition because it does not simply
   involve multiplying the matrices element-wise. instead a more complex
   procedure is utilised, for each element, involving an entire row of one
   matrix and an entire column of the other.

   the operation is only defined for matrices of specific sizes. the first
   matrix must have as many columns as the second matrix has rows,
   otherwise the operation is not defined.

   the definition below can be a bit tricky to understand initially, so
   have a look at it first and then try working through the examples to
   see how specific numeric instances match up to the general formula.

   given a matrix $\boldsymbol{a}=[a_{ij}]_{m \times n}$ and a matrix
   $\boldsymbol{b}=[b_{ij}]_{n \times p}$ the matrix product
   $\boldsymbol{c}=\boldsymbol{a}\boldsymbol{b}=[c_{ij}]_{m \times p}$ is
   defined element-wise by:
   \begin{equation} c_{ij} = \sum^n_{k=1} a_{ik} b_{kj} \end{equation}

   that is the elements $c_{ij}$ of the matrix
   $\boldsymbol{c}=\boldsymbol{a}\boldsymbol{b}$ are given by summing the
   products of the elements of the $i$-th row of $\boldsymbol{a}$ with the
   elements of the $j$-th column of $\boldsymbol{b}$.

   note that matrix-id127 is not commutative. that is:
   \begin{eqnarray} \boldsymbol{a}\boldsymbol{b} \neq
   \boldsymbol{b}\boldsymbol{a} \end{eqnarray}

examples

   given the following two matrices:
   \[ \boldsymbol{a} = \left[ \begin{array}{ccc} 2 & 5 & 1 \\ 7 & 3 & 6
   \end{array} \right], \quad \boldsymbol{b} = \left[ \begin{array}{cc} 1
   & 8 \\ 9 & 4 \\ 3 & 5 \end{array} \right] \]

   it is possible to construct the product $\boldsymbol{a}\boldsymbol{b}$
   of size $2 \times 2$:
   \[ \boldsymbol{ab} = \left[ \begin{array}{cc} 2 \cdot 1 + 5 \cdot 9 + 1
   \cdot 3 & 2 \cdot 8 + 5 \cdot 4 + 1 \cdot 5 \\ 7 \cdot 1 + 3 \cdot 9 +
   6 \cdot 3 & 7 \cdot 8 + 3 \cdot 4 + 6 \cdot 5 \end{array} \right] =
   \left[ \begin{array}{cc} 50 & 41 \\ 52 & 98 \end{array} \right] \]

   it is also possible to construct the product
   $\boldsymbol{b}\boldsymbol{a}$ of size $3 \times 3$:
   \[ \boldsymbol{ba} = \left[ \begin{array}{ccc} 1 \cdot 2 + 8 \cdot 7 &
   1 \cdot 5 + 8 \cdot 3 & 1 \cdot 1 + 8 \cdot 6 \\ 9 \cdot 2 + 4 \cdot 7
   & 9 \cdot 5 + 4 \cdot 3 & 9 \cdot 1 + 4 \cdot 6 \\ 3 \cdot 2 + 5 \cdot
   7 & 3 \cdot 5 + 5 \cdot 3 & 3 \cdot 1 + 5 \cdot 6 \\ \end{array}
   \right] = \left[ \begin{array}{ccc} 58 & 29 & 49 \\ 46 & 57 & 33 \\ 41
   & 30 & 33 \end{array} \right] \]

   the above definition does not initially seem to be motivated in a
   simple way. why is matrix-id127 defined like this? it
   has to do with a deeper result involving matrices representing
   [12]linear map functions between two vector spaces. we need not worry
   about these certain deeper aspects of linear maps as they are beyond
   the scope of this article series.

   however, we can briefly provide some intuition through the idea of
   composing functions together. it is common in mathematics to compose
   two functions together to produce a new function. that is the function
   $h$ can be defined as $h = f \circ g$, with the $\circ$ symbol
   representing function composition. this notation means that $h$ is
   equivalent to $g$ being carried out first, and then $f$.

   if, for example $f=sin(x)$ and $g=x^2$ then $h = sin(x^2)$. function
   composition is not commutative and as such $f \circ g \neq g \circ f$.
   instead $g \circ f = sin^2 (x)$, which is a completely different
   function. this is why matrix-id127 is not commutative
   and also why it is defined as above.

   note also that this definition does not imply that the elements of the
   matrix $\boldsymbol{c}$ are defined as the element-wise multiplication
   of those from $\boldsymbol{a}$ and $\boldsymbol{b}$. that is, the
   elements in each location cannot simply be multiplied together to get
   the new product matrix. that is an entirely different operation known
   as the hadamard product and will be discussed below.

   since a column vector is in fact a $n \times 1$ matrix it is possible
   to carry out matrix-vector multiplication. if the left-multiplying
   matrix has size $m \times n$ then this is a valid operation that will
   produce another $m \times 1$ matrix (column vector) as output.

   matrix-matrix and matrix-vector multiplication are extremely common
   operation in the physical sciences, computational graphics and machine
   learning fields. as such highly optimised software libraries such as
   [13]blas and [14]lapack have been developed to allow efficient
   scientific computation--particularly on [15]gpus.

scalar-id127

   scalar-id127 is simpler than matrix-matrix
   multiplication. given a matrix $\boldsymbol{a}=[a_{ij}]_{m \times n}$
   and a scalar $\lambda \in \mathbb{r}$ the scalar-matrix product
   $\lambda \boldsymbol{a}$ is calculated by multiplying every element of
   $\boldsymbol{a}$ by $\lambda$ such that $\lambda \boldsymbol{a} =
   [\lambda a_{ij}]_{m \times n}$.

   if we take two real-valued scalars $\lambda, \mu \in \mathbb{r}$ the
   subsequent useful relationships follow from the definition above:
   \begin{eqnarray} \lambda (\boldsymbol{a} + \boldsymbol{b}) &=& \lambda
   \boldsymbol{a} + \lambda \boldsymbol{b} \\ (\lambda + \mu)
   \boldsymbol{a} &=& \lambda \boldsymbol{a} + \mu \boldsymbol{a} \\
   \lambda (\mu \boldsymbol{a}) &=& (\lambda \mu) \boldsymbol{a}
   \end{eqnarray}

   the first result states that you will get the same answer if you add
   two matrices together, and then multiply them by a scalar, as if you
   individually multiplied each matrix by the scalar and then added them
   together.

   the second result states that if you add two scalars together and then
   multiply the result by a matrix it gives the same answer as if you
   individually multiplied the matrix separately by each scalar and added
   the result.

   the third result states that the order of scalar multiplication does
   not matter. if you multiply the matrix by one scalar, and then multiply
   the result by another scalar it gives the same answer as if you first
   multiplied both scalars together and then by the matrix.

   all of these results follow from the simple rules of scalar
   multiplication and addition.

hadamard product

   it is possible to define element-wise multiplication of matrices, which
   differs from the definition of matrix-id127 above. the
   hadamard product of two matrices $\boldsymbol{a}=[a_{ij}]_{m \times n}$
   and $\boldsymbol{b}=[b_{ij}]_{m \times n}$ is denoted by
   $\boldsymbol{a} \odot \boldsymbol{b}$ and calculated by the following
   expression:
   \begin{equation} \boldsymbol{a} \odot \boldsymbol{b} = [a_{ij}
   b_{ij}]_{m \times n} \end{equation}

   that is, the elements of the new matrix are simply given as the scalar
   multiples of each of the elements from the other matrices. note that
   since scalar multiplication is commutative so is the hadamard product,
   unlike normal matrix-id127.

   when might it be necessary to use the hadamard product? in fact such a
   process has wide applications, including correcting codes in satellite
   transmissions and cryptography, signal processing as well as lossy
   compression algorithms for images in jpeg format.

dot product

   an important special case of matrix-id127 occurs
   between two vectors and is known as the dot product. it has a deep
   relationship with geometry and is useful in all areas of the physical
   and computational sciences.

   we have to be extremely careful here in regards to notation. i am being
   particularly precise about this definition, which may be unusual to
   some of you who have it seen it written before. in particular the dot
   product is really only defined as a true matrix-id127,
   where one of the vectors is a row "matrix" and the other a column
   "matrix". however, you will often see a slight "notational abuse" where
   it is defined for any two vectors (row or column).

   the usual notation for a dot product between two $n$-dimensional
   vectors $\boldsymbol{x}, \boldsymbol{y} \in \mathbb{r}^n$ is
   $\boldsymbol{x} \cdot \boldsymbol{y}$, which is where the name of the
   operation comes from. however in more advanced textbooks (particularly
   the popular graduate level statistics, machine learning and deep
   learning texts) you will see it written as $\boldsymbol{x}^t
   \boldsymbol{y}$, where $t$ represents the transpose of
   $\boldsymbol{x}$.

   this is because $\boldsymbol{x}$ and $\boldsymbol{y}$ are usually
   considered to both be column vectors. a matrix-id127
   between two column vectors is not defined. hence, one of the vectors
   needs to be transposed into a row vector such that the matrix-matrix
   multiplication is properly defined. hence you will see the
   $\boldsymbol{x}^t \boldsymbol{y}$ notation frequently in more advanced
   textbooks and papers. now we can proceed with the definition!

   given two column vectors $\boldsymbol{x}, \boldsymbol{y} \in
   \mathbb{r}^n$ it is possible to define the dot product (or scalar
   product) between them by taking the transpose of one to form a product
   that is defined within the rules of matrix-id127. such
   a product produces a scalar value and is commutative:
   \begin{equation} \boldsymbol{x}^t \boldsymbol{y} = \sum^n_{i=1} x_i y_i
   = \boldsymbol{y}^t \boldsymbol{x} \end{equation}

   the dot product has an important geometric meaning. it is the product
   of the [16]euclidean lengths of the two vectors and the cosine of the
   angle between them. in subsequent articles the concept of norms will be
   introduced, at which point this definition will be formalised.

   another way to think about a dot product is that if we take the dot
   product of a vector with itself it is the square of the length of the
   vector:
   \begin{equation} \boldsymbol{x}^t \boldsymbol{x} = \sum^n_{i=1} x_i x_i
   = \sum^n_{i=1} x_i^2 \end{equation}

   hence to find the (euclidean) length of a vector you can simply take
   the square root of the dot product of the vector,
   $\sqrt{\boldsymbol{x}^t \boldsymbol{x}}$.

   the dot product is a special case of a more general mathematical entity
   known as an inner product. in more abstract vector spaces the inner
   product allows intuitive concepts such as length and angle of a vector
   to be made rigourous. however such spaces are beyond the scope of this
   article series and will not be discussed further.

next steps

   this article has all been about operations applied to one or more
   matrices. we can now add and multiply matrices together. but what does
   this give us? how can we use it?

   in the next article we are going to look at one of the most fundamental
   topics in id202   inverting a matrix. matrix inversion allows us
   to solve matrix equations, in exactly the same way that scalar algebra
   allows us to solve scalar equations.

   this is a widespread operation in the physical and computational
   sciences and will be indispensible in our studies of deep learning.

article series

     * [17]scalars, vectors, matrices and tensors - id202 for
       deep learning (part 1)
     * [18]matrix algebra - id202 for deep learning (part 2)

references

     * [1] blyth, t.s. and robertson, e.f. (2002) basic id202,
       2nd ed., springer
     * [2] strang, g. (2016) introduction to id202, 5th ed.,
       wellesley-cambridge press
     * [19][3] goodfellow, i.j., bengio, y., courville, a. (2016) deep
       learning, mit press

   [20]quantcademy

[21]the quantcademy

   join the quantcademy membership portal that caters to the
   rapidly-growing retail quant trader community and learn how to increase
   your strategy profitability.
   [22]find out more
   [23]successful algorithmic trading

[24]successful algorithmic trading

   how to find new trading strategy ideas and objectively assess them for
   your portfolio using a custom-built backtesting engine in python.
   [25]find out more
   [26]advanced algorithmic trading

[27]advanced algorithmic trading

   how to implement advanced trading strategies using time series
   analysis, machine learning and bayesian statistics with r and python.
   [28]find out more

   [29]privacy policy | [30]back to top

     2012-2019 quarkgluon ltd. all rights reserved.

references

   1. https://www.quantstart.com/
   2. https://www.quantstart.com/quantcademy?ref=hdjq
   3. https://www.quantstart.com/ebooks
   4. https://www.quantstart.com/articles
   5. https://www.quantstart.com/articles/quantitative-finance-reading-list
   6. https://www.quantstart.com/qstrader
   7. https://www.quantstart.com/members/login
   8. https://www.quantstart.com/quantcademy?ref=hdjq
   9. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
  10. https://www.quantstart.com/articles/quantstart-upcoming-content-survey-2017
  11. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
  12. https://en.wikipedia.org/wiki/linear_map
  13. https://en.wikipedia.org/wiki/basic_linear_algebra_subprograms
  14. https://en.wikipedia.org/wiki/lapack
  15. https://en.wikipedia.org/wiki/graphics_processing_unit
  16. https://en.wikipedia.org/wiki/euclidean_distance
  17. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
  18. https://www.quantstart.com/articles/matrix-algebra-linear-algebra-for-deep-learning-part-2
  19. http://www.deeplearningbook.org/
  20. https://www.quantstart.com/quantcademy?ref=art
  21. https://www.quantstart.com/quantcademy?ref=art
  22. https://www.quantstart.com/quantcademy?ref=art
  23. https://www.quantstart.com/successful-algorithmic-trading-ebook
  24. https://www.quantstart.com/successful-algorithmic-trading-ebook
  25. https://www.quantstart.com/successful-algorithmic-trading-ebook
  26. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  27. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  28. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  29. https://www.quantstart.com/privacy-policy
  30. https://www.quantstart.com/articles/matrix-algebra-linear-algebra-for-deep-learning-part-2
