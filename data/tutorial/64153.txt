   [1][qs-logo-trans-trimmed.png] (button)

     * [2]quantcademy
     * [3]books
     * [4]articles
     * [5]reading list
     * [6]qstrader

   [7]log in [8]join the quantcademy

scalars, vectors, matrices and tensors - id202 for deep learning
(part 1)

   scalars, vectors, matrices and tensors - id202 for deep
   learning (part 1)

   by quantstart team

   back in march we [9]ran a content survey and found that many of you
   were interested in a refresher course for the key mathematical topics
   needed to understand deep learning and quant finance in general.

   since [10]deep learning is going to be a big part of this year's
   content we thought it would be worthwhile to write some beginner
   tutorials on the key mathematical topics   id202, calculus and
   id203   that are necessary to really understand deep learning for
   quant trading.

   this article is the first in the series of posts on the topic of linear
   algebra for deep learning. it is intended to get you up to scratch in
   some of the basic ideas and notation that will be found in the more
   advanced deep learning textbooks and research papers. reading these
   papers is absolutely crucial to find the best quantitative trading
   methods and as such it helps to speak the language!

   id202 is a fundamental topic in the subject of mathematics and
   is extremely pervasive in the physical sciences. it also forms the
   backbone of many machine learning algorithms. hence it is crucial for
   the deep learning practitioner to understand the core ideas.

   id202 is a branch of [11]continuous, rather than [12]discrete
   mathematics. the mathematician, physicist, engineer and quant will
   likely be familiar with continuous mathematics through the study of
   [13]differential equations, which are used to model many physical and
   financial phenomena.

   the computer scientist, software developer or retail discretionary
   trader however may only have gained exposure to mathematics through
   subjects such as [14]id207 or [15]combinatorics   topics found
   within discrete mathematics. hence the set and function notation
   presented here may be initially unfamiliar.

   for this reason the discussion presented in this article series will
   omit the usual "theorem and proof" approach of an undergraduate
   mathematics textbook. instead the focus will be on selected topics that
   are relevant to deep learning practitioners from diverse backgrounds.

   please note that the outline of id202 presented in this
   article series closely follows the notation and excellent treatments of
   goodfellow et al (2016)^[16][3], blyth and robertson (2002)^[17][1] and
   strang (2016)^[18][2].

motivation

   id202, id203 and calculus are the 'languages' in which
   machine learning is written. learning these topics will provide a
   deeper understanding of the underlying algorithmic mechanics and allow
   development of new algorithms, which can ultimately be deployed as more
   sophisticated quantitative trading strategies.

   many [19]supervised machine learning and [20]deep learning algorithms
   largely entail optimising a [21]id168 by adjusting model
   parameters. to carry this out requires some notion of how the loss
   function changes as the parameters of the model are varied.

   this immediately motivates calculus   the elementary topic in mathematics
   which describes changes of quantities with respect to another. in
   particular it requires the concept of a [22]partial derivative, which
   specifies how the id168 is altered through individual changes
   in each parameter.

   these partial derivatives are often grouped together   in matrices   to
   allow more straightforward calculation. even the most elementary
   machine learning models such as [23]id75 are optimised
   with these id202 techniques.

   a key topic in id202 is that of vector and matrix notation.
   being able to 'read the language' of id202 will open up the
   ability to understand textbooks, web posts and research papers that
   contain more complex model descriptions. this will not only allow
   reproduction and verification of existing models, but will allow
   extensions and new developments that can subsequently be deployed in
   trading strategies.

   id202 provides the first steps into vectorisation, presenting
   a deeper way of thinking about parallelisation of certain operations.
   algorithms written in standard 'for-loop' notation can be reformulated
   as matrix equations providing significant gains in computational
   efficiency.

   such methods are used in the major python libraries such as numpy,
   scipy, scikit-learn, pandas and tensorflow. gpus have been designed to
   carry out optimised id202 operations. the explosive growth in
   deep learning can partially be attributed to the highly parallelised
   nature of the underlying algorithms on commodity gpu hardware.

   id202 is a continuous mathematics subject but ultimately the
   entities discussed below are implemented in a discrete computational
   environment. these discrete representations of id202 entities
   can lead to issues of [24]overflow and [25]underflow, which represent
   the limits of effectively representing extremely large and small
   numbers computationally.

   one mechanism for mitigating the effects of limited numerical
   presentation is to make use of [26]matrix factorisation techniques.
   such techniques allow certain matrices to be represented in terms of
   simpler, structured matrices that have useful computational properties.

   matrix decomposition techniques include [27]lower upper (lu)
   decomposition, [28]qr decomposition and [29]singular value
   decomposition (svd). they are an intrinsic component of certain machine
   learning algorithms including [30]linear least squares and [31]pricipal
   components analysis (pca). matrix decomposition will be discussed at
   length later in this series.

   it can not be overemphasised how fundamental id202 is to deep
   learning. for those that are aiming to deploy the most sophisticated
   quant models based on deep learning techniques   or are seeking
   employment at firms that are   it will be necessary to learn linear
   algebra extremely well.

   the material in this article series will cover the bare minimum, but to
   understand the research frontier it will be necessary to go much
   further than this. please see the references at the end of the article
   for a brief list on where to continue studying id202.

vectors and matrices

   the two primary mathematical entities that are of interest in linear
   algebra are the vector and the matrix. they are examples of a more
   general entity known as a tensor. tensors possess an order (or rank),
   which determines the number of dimensions in an array required to
   represent it.

scalars

   scalars are single numbers and are an example of a 0th-order tensor. in
   mathematics it is necessary to describe the set of values to which a
   scalar belongs. the notation $x \in \mathbb{r}$ states that the
   (lowercase) scalar value $x$ is an element of (or member of) the set of
   real-valued numbers, $\mathbb{r}$.

   there are various sets of numbers of interest within machine learning.
   $\mathbb{n}$ represents the set of positive integers ($1, 2,
   3,\ldots$). $\mathbb{z}$ represents the integers, which include
   positive, negative and zero values. $\mathbb{q}$ represents the set of
   rational numbers that may be expressed as a fraction of two integers.

vectors

   vectors are ordered arrays of single numbers and are an example of
   1st-order tensor. vectors are members of objects known as vector
   spaces. a vector space can be thought of as the entire collection of
   all possible vectors of a particular length (or dimension). the
   three-dimensional real-valued vector space, denoted by $\mathbb{r}^3$
   is often used to represent our real-world notion of three-dimensional
   space mathematically.

   more formally a vector space is an $n$-dimensional [32]cartesian
   product of a set with itself, along with proper definitions on how to
   add vectors and multiply them with scalar values. if all of the scalars
   in a vector are real-valued then the notation $\boldsymbol{x} \in
   \mathbb{r}^n$ states that the (boldface lowercase) vector value
   $\boldsymbol{x}$ is a member of the $n$-dimensional vector space of
   real numbers, $\mathbb{r}^n$.

   sometimes it is necessary to identify the components of a vector
   explicitly. the $i$th scalar element of a vector is written as $x_i$.
   notice that this is non-bold lowercase since the element is a scalar.
   an $n$-dimensional vector itself can be explicitly written using the
   following notation:
   \begin{equation} \boldsymbol{x}=\begin{bmatrix} \kern4pt x_1 \kern4pt
   \\ \kern4pt x_2 \kern4pt \\ \kern4pt \vdots \kern4pt \\ \kern4pt x_n
   \kern4pt \end{bmatrix} \end{equation}

   given that scalars exist to represent values why are vectors necessary?
   one of the primary use cases for vectors is to represent physical
   quantities that have both a magnitude and a direction. scalars are only
   capable of representing magnitudes.

   for instance scalars and vectors encode the difference between the
   speed of a car and its velocity. the velocity contains not only its
   speed but also its direction of travel. it is not difficult to imagine
   many more physical quantities that possess similar characteristics such
   as gravitational and electromagnetic forces or wind velocity.

   in machine learning vectors often represent feature vectors, with their
   individual components specifying how important a particular feature is.
   such features could include relative importance of words in a text
   document, the intensity of a set of pixels in a two-dimensional image
   or historical price values for a cross-section of financial
   instruments.

matrices

   matrices are rectangular arrays consisting of numbers and are an
   example of 2nd-order tensors. if $m$ and $n$ are positive integers,
   that is $m,n \in \mathbb{n}$ then the $m \times n$ matrix contains $mn$
   numbers, with $m$ rows and $n$ columns.

   if all of the scalars in a matrix are real-valued then a matrix is
   denoted with uppercase boldface letters, such as $\boldsymbol{a} \in
   \mathbb{r}^{m \times n}$. that is the matrix lives in a $m \times
   n$-dimensional real-valued vector space. hence matrices are really
   vectors that are just written in a two-dimensional table-like manner.

   its components are now identified by two indices $i$ and $j$. $i$
   represents the index to the matrix row, while $j$ represents the index
   to the matrix column. each component of $\boldsymbol{a}$ is identified
   by $a_{ij}$.

   the full $m \times n$ matrix can be written as:
   \begin{equation} \boldsymbol{a}=\begin{bmatrix} \kern4pt a_{11} &
   a_{12} & a_{13} & \ldots & a_{1n} \kern4pt \\ \kern4pt a_{21} & a_{22}
   & a_{23} & \ldots & a_{2n} \kern4pt \\ \kern4pt a_{31} & a_{32} &
   a_{33} & \ldots & a_{3n} \kern4pt \\ \kern4pt \vdots & \vdots & \vdots
   & \ddots & \vdots \kern4pt \\ \kern4pt a_{m1} & a_{m2} & a_{m3} &
   \ldots & a_{mn} \kern4pt \\ \end{bmatrix} \end{equation}

   it is often useful to abbreviate the full matrix component display into
   the following expression:
   \begin{equation} \boldsymbol{a} = [a_{ij}]_{m \times n} \end{equation}

   where $a_{ij}$ is referred to as the $(i,j)$-element of the matrix
   $\boldsymbol{a}$. the subscript of $m \times n$ can be dropped if the
   dimension of the matrix is clear from the context.

   note that a column vector is a size $m \times 1$ matrix, since it has
   $m$ rows and 1 column. unless otherwise specified all vectors will be
   considered to be column vectors.

   matrices represent a type of function known as a [33]linear map. based
   on rules that will be outlined in subsequent articles, it is possible
   to define multiplication operations between matrices or between
   matrices and vectors. such operations are immensely important across
   the physical sciences, quantitative finance, computer science and
   machine learning.

   matrices can encode geometric operations such as rotation, reflection
   and transformation. thus if a collection of vectors represents the
   vertices of a three-dimensional geometric model in [34]computer aided
   design software then multiplying these vectors individually by a
   pre-defined [35]rotation matrix will output new vectors that represent
   the locations of the rotated vertices. this is the basis of modern 3d
   computer graphics.

   in deep learning neural network weights are stored as matrices, while
   feature inputs are stored as vectors. formulating the problem in terms
   of id202 allows compact handling of these computations. by
   casting the problem in terms of tensors and utilising the machinery of
   id202, rapid training times on modern gpu hardware can be
   obtained.

tensors

   the more general entity of a tensor encapsulates the scalar, vector and
   the matrix. it is sometimes necessary   both in the physical sciences and
   machine learning   to make use of tensors with order that exceeds two.

   in theoretical physics, and general relativity in particular, the
   [36]riemann curvature tensor is a 4th-order tensor that describes the
   local curvature of [37]spacetime. in machine learning, and deep
   learning in particular, a 3rd-order tensor can be used to describe the
   intensity values of multiple channels (red, green and blue) from a
   two-dimensional image.

   tensors will be identified in this series of posts via the boldface
   sans-serif notation, $\textsf{a}$. for a 3rd-order tensor elements will
   be given by $a_{ijk}$, whereas for a 4th-order tensor elements will be
   given by $a_{ijkl}$.

next steps

   in the next article the basic operations of matrix-vector and
   matrix-id127 will be outlined. this topic is
   collectively known as matrix algebra.

article series

     * [38]scalars, vectors, matrices and tensors - id202 for
       deep learning (part 1)
     * [39]matrix algebra - id202 for deep learning (part 2)

references

     * [1] blyth, t.s. and robertson, e.f. (2002) basic id202,
       2nd ed., springer
     * [2] strang, g. (2016) introduction to id202, 5th ed.,
       wellesley-cambridge press
     * [40][3] goodfellow, i.j., bengio, y., courville, a. (2016) deep
       learning, mit press

   [41]quantcademy

[42]the quantcademy

   join the quantcademy membership portal that caters to the
   rapidly-growing retail quant trader community and learn how to increase
   your strategy profitability.
   [43]find out more
   [44]successful algorithmic trading

[45]successful algorithmic trading

   how to find new trading strategy ideas and objectively assess them for
   your portfolio using a custom-built backtesting engine in python.
   [46]find out more
   [47]advanced algorithmic trading

[48]advanced algorithmic trading

   how to implement advanced trading strategies using time series
   analysis, machine learning and bayesian statistics with r and python.
   [49]find out more

   [50]privacy policy | [51]back to top

     2012-2019 quarkgluon ltd. all rights reserved.

references

   1. https://www.quantstart.com/
   2. https://www.quantstart.com/quantcademy?ref=hdjq
   3. https://www.quantstart.com/ebooks
   4. https://www.quantstart.com/articles
   5. https://www.quantstart.com/articles/quantitative-finance-reading-list
   6. https://www.quantstart.com/qstrader
   7. https://www.quantstart.com/members/login
   8. https://www.quantstart.com/quantcademy?ref=hdjq
   9. https://www.quantstart.com/articles/quantstart-upcoming-content-survey-2017
  10. https://www.quantstart.com/articles/what-is-deep-learning
  11. https://en.wikipedia.org/wiki/continuous_function
  12. https://en.wikipedia.org/wiki/discrete_mathematics
  13. https://en.wikipedia.org/wiki/differential_equation
  14. https://en.wikipedia.org/wiki/graph_theory
  15. https://en.wikipedia.org/wiki/combinatorics
  16. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1#ref-goodfellow2016
  17. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1#ref-blyth2002
  18. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1#ref-strang2016
  19. https://www.quantstart.com/articles/beginners-guide-to-statistical-machine-learning-part-i
  20. https://www.quantstart.com/articles/what-is-deep-learning
  21. https://en.wikipedia.org/wiki/loss_function
  22. https://en.wikipedia.org/wiki/partial_derivative
  23. https://www.quantstart.com/articles/maximum-likelihood-estimation-for-linear-regression
  24. https://en.wikipedia.org/wiki/integer_overflow
  25. https://en.wikipedia.org/wiki/arithmetic_underflow
  26. https://en.wikipedia.org/wiki/matrix_decomposition
  27. https://www.quantstart.com/articles/lu-decomposition-in-python-and-numpy
  28. https://www.quantstart.com/articles/qr-decomposition-with-python-and-numpy
  29. https://en.wikipedia.org/wiki/singular_value_decomposition
  30. https://en.wikipedia.org/wiki/linear_least_squares_(mathematics)
  31. https://en.wikipedia.org/wiki/principal_component_analysis
  32. https://en.wikipedia.org/wiki/cartesian_product
  33. https://en.wikipedia.org/wiki/linear_map
  34. https://en.wikipedia.org/wiki/computer-aided_design
  35. https://en.wikipedia.org/wiki/rotation_matrix
  36. https://en.wikipedia.org/wiki/riemann_curvature_tensor
  37. https://en.wikipedia.org/wiki/spacetime
  38. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
  39. https://www.quantstart.com/articles/matrix-algebra-linear-algebra-for-deep-learning-part-2
  40. http://www.deeplearningbook.org/
  41. https://www.quantstart.com/quantcademy?ref=art
  42. https://www.quantstart.com/quantcademy?ref=art
  43. https://www.quantstart.com/quantcademy?ref=art
  44. https://www.quantstart.com/successful-algorithmic-trading-ebook
  45. https://www.quantstart.com/successful-algorithmic-trading-ebook
  46. https://www.quantstart.com/successful-algorithmic-trading-ebook
  47. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  48. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  49. https://www.quantstart.com/advanced-algorithmic-trading-ebook
  50. https://www.quantstart.com/privacy-policy
  51. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
