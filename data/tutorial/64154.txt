   [1]tryolabs blog
     * tryolabs

     * [2]blog
     * [3]subscribe
     * [4]get in touch
     * [5]search

     * [6]all
     * [7]backend
     * [8]frontend
     * [9]machine-learning
     * [10]mobile
     * [11]news
     * [12]others
     * [13]resources

   read time: 16 minutes

introduction to visual id53: datasets, approaches and
evaluation

   [14]javier thu, mar 1, 2018 in [15]machine learning
     * [16]deep learning
     * [17]vqa
     * [18]id161
     * [19]nlp

   historically, building a system that can answer natural language
   questions about any image has been considered a very ambitious goal.
   imagine a system that, given the image below, could answer these
   questions:
     * what is in the image?
     * are there any humans?
     * what sport is being played?
     * who has the ball?
     * how many players are in the image?
     * who are the teams?
     * is it raining?

   [visual-question-answering.aa6ecaa1.jpg]

    argentina facing england in 1986

   [20]image source

   so, how many players are in the image? well, we can count them and see
   that there are eleven players, since we are smart enough not to count
   the referee, right?

   although as humans we can normally perform this task without major
   inconveniences, the development of a system with these capabilities has
   always seemed closer to science fiction than to the current
   possibilities of artificial intelligence (ai). however, with the advent
   of deep learning (dl), we have witnessed enormous research progress in
   visual id53 (vqa), in such a way that systems capable of
   answering these questions are emerging with promising results.

   in this article i will briefly go through some of the current datasets,
   approaches and id74 in vqa, and on how this challenging
   task can be applied to real life use cases.

                           a multi-discipline problem

   in a general way we can define a vqa system as an algorithm that takes
   as input an image and a natural language question about the image and
   generates a natural language answer as the output. this is by nature a
   multi-discipline research problem. let   s take, for example, the
   questions about the previous image. we need nlp for at least two
   reasons: to understand the question and to generate the answer. these
   are common problems in text-based q&a, a well studied problem in nlp.
   given the following sentence:

     how many bridges are there in paris?

   a nlp q&a system is typically going to:
     * classify or type the question: this is a    how many    question, so
       the response must be a number.
     * extract the object to count: bridges.
     * extract the context where the count must be performed: in this
       case, the city of paris.

   after the question has been analyzed, the system builds some kind of
   query and relies on a knowledge base to get the answer. this is far
   from trivial (e.g. there are at least 22 cities called paris in the
   united states), but a lot of work has been done since the 1970s.

   the main difference in vqa is that the search and the reasoning part
   must be performed over the content of an image. so, to answer if there
   are any humans, the system must be able to detect objects. to say if it
   is raining, it needs to classify a scene. to answer who are the teams,
   the system needs world knowledge. finally, to say which player has the
   ball, commonsense reasoning and, very likely, knowledge reasoning are
   necessary. many of these tasks (object recognition, id164,
   scene classification, etc.) have been addressed in the field of
   id161 (cv), with impressive results in the last few years.

   so, as we can see, a good vqa system must be capable of solving a broad
   spectrum of typical nlp and cv tasks, as well as reasoning about image
   content. it is clearly a multi-discipline ai research problem,
   involving cv, nlp and id99 & reasoning (kr).

                               available datasets

   as with many problems in nlp or cv, such as machine translation, image
   captioning or named entities recognition, the availability of datasets
   is a key issue. they allow, in combination with well defined metrics
   (see the    id74    section below), to fairly compare
   different approaches, to compare them with human decisions and to
   measure how they perform in an absolute way, that is, to determine the
   empirical limitations of the state-of-the-art.

   the vqa field is so complex that a good dataset should be large enough
   to capture the long range of possibilities within questions and image
   content in real world scenarios. many of the datasets contain images
   from the [21]microsoft common objects in context (coco), a dataset with
   328,000 images having 91 objects types that would be easily
   recognizable by a 4 year old, with a total of 2.5 million labeled
   instances.
   [dataset-coco.f15ae758.jpg]

    examples of annotated images from the coco dataset

   [22]image source

   the coco dataset can simplify and accelerate the process of building a
   vqa dataset. however, it is not an easy task. for example, collecting
   varied, convenient and non ambiguous questions is a great challenge.
   besides the variety and precision problem, a good dataset also has to
   avoid biases that could be exploited. for example, given a dataset with
   only yes/no answers, where 90% of the answers are yes, a trivial most
   frequent class strategy will obtain an accuracy of 90%, without solving
   anything in terms of vqa.

   to get deeper into the vqa complexity, let   s see some of the most
   important datasets released.

the daquar dataset

   the first significant vqa dataset was the [23]dataset for question
   answering on real-world images (daquar). it contains 6794 training and
   5674 test question-answer pairs, based on images from the [24]nyu-depth
   v2 dataset. that means about 9 pairs per image on average.

   although it is a great initiative, the nyu dataset contains only indoor
   scenes with, sometimes, lightning conditions that make it difficult to
   answer the questions. in fact, evaluation on humans shows an accuracy
   of 50.2%.
   [dataset-daquar.a55d3711.jpg]

    examples of human generated question-answer pairs for an image

   [25]image source

   the other drawback of the daquar dataset is that its size makes it
   unsuitable for training and evaluating complex models.

the coco-qa dataset

   the [26]coco-qa dataset is significantly larger than daquar. it
   contains 123,287 images coming from the coco dataset, 78,736 training
   and 38,948 testing qa pairs. to create such a large amount of qa pairs,
   the [27]authors used a nlp algorithm to automatically generate them
   from the coco image captions. for example, for a given caption such as
      two chairs in a room   , they would generate a question like    how many
   chairs are there?   . it must be noted that all the answers are a single
   word.

   although this is very clever, the obvious issue here is that the
   questions suffer from the inherent nlp limitations, so they are
   sometimes bizarrely formulated or have grammatical errors. in some
   cases, they are simply incomprehensible:
   [dataset-coco-qa.fc71ab77.jpg]

    examples of grammatical errors in the question

   [28]image source

   another inconvenience is that the dataset only has four kinds of
   questions, not equally distributed: object (69.84%), color (16.59%),
   counting (7.47%) and location (6.10%).

the vqa dataset

   compared to other datasets, the [29]vqa dataset is relatively larger.
   in addition to 204,721 images from the coco dataset, it includes 50,000
   abstract cartoon images. there are three questions per image and ten
   answers per question, that is over 760k questions with around 10m
   answers. to achieve this, a team of amazon mechanical turk workers
   generated the questions and another team wrote the answers.

   one interesting point is that for testing, they propose two kind of
   answer modes: open-ended and multiple-choice. for the first mode, they
   propose the following metric:

   \mathit{accuracy} = \mathit{min}\left(\dfrac{\mathit{\#}\text{ humans
   that provided that answer}}{3}, 1\right)

   that means that an answer is considered 100% accurate if at least 3
   workers provided that exact answer.

   for the multiple-choice mode, they create 18 candidate answers (correct
   and incorrect) per question:
     * the correct answer: the most common answer given by the ten
       annotators.
     * plausible answers: three answers collected from annotators without
       looking at the image.
     * popular answers: the top ten most popular answers in the dataset
       (e.g.    yes   ,    no   ,    2   ,    1   ,    white   ,    3   ,    red   ,    blue   ,    4   ,
          green    for real images)
     * random answers: randomly selected correct answers for other
       questions.

   despite the precautions taken in the design of the dataset (e.g. the
   inclusion of popular answers makes it more difficult to infer the type
   of question from the set of answers), we can observe some issues. maybe
   the most striking one is that some questions are too subjective to have
   a single right answer. sometimes, like in the case of the image below,
   a most likely answer can be given.
   [dataset-vqa.30ac344a.jpg]

    example of a tricky question. in green, the answers given when looking at
    the image. in blue, those given when not looking at the image.

   [30]image source

   we can not say that this man has children, but it is without doubt the
   most likely answer. however, there are questions such as    would you
   like to fly in that?    over the image of an airplane, that do not have a
   correct answer.

   finally, a major drawback regards what we could call trivial questions,
   that is questions that generally don   t need an image to have a good (or
   likely) answer. for example:    how many legs does the dog have?    or
      what color are the trees?   , where, although not always correct, four
   and green are the most frequent and obvious answers.

                               current approaches

   in general, we can outline the approaches in vqa as follows:
    1. extract features from the question.
    2. extract features from the image.
    3. combine the features to generate an answer.

   for text features, techniques such as bag-of-words (bow) or long short
   term memory (lstm) encoders can be used. in the case of image features,
   id98s pre-trained on id163 is the most frequent choice. regarding the
   generation of the answer, the approaches usually model the problem as a
   classification task.
   [visual-question-answering-approach.49eaa652.jpg]

    simplified scheme of vqa approaches

   [31]image source

   thus, the main difference between several approaches is how they
   combine the textual and image features. for example, they can simply
   combine them using concatenation and then feed a linear classifier. or
   they can use bayesian models to infer the underlying relationships
   between the feature distributions of the question, the image and the
   answer. since the large number of algorithms proposed in the last years
   exceeds the purpose of this post, i will only mention some of them. for
   a more detailed comparative analysis, i recommend you to read this
   [32]excellent survey on vqa, by kafle and kanan.

baselines

   as for many classification problems, a trivial baseline consists on
   always giving the most frequent answer to any question. another trivial
   baseline is to pick up a random answer. for instance, the work by
   [33]antol et al. (2016) shows that always selecting the most popular
   answer from the top 1k answers on the vqa dataset (the answer is    yes   )
   leads to an accuracy of 29.72%. leaving aside that this probably has to
   do with an undesired dataset bias, such a result illustrates the
   importance of having good baselines: they determine the minimal level
   of performance acceptable, and can also give a hint about the inherent
   complexity of the task and/or the dataset.

   a more sophisticated baseline, widely used in vqa, consists on training
   a linear classifier or a multilayer id88 using vectors
   representing a combination of the features as input. this combination
   can be a simple concatenation or a element wise sum or product of the
   features.

   for example, the previous cited work experiments with two models:
    1. a multi-layer id88 (mlp) neural network classifier with two
       hidden layers and 1000 hidden units (dropout 0.5) in each layer
       with tanh non-linearity.
    2. an lstm model followed by a softmax layer to generate the answer.

   in the first case, for the textual features they use a bow approach,
   using the top 1,000 words in the questions and the 1,000 most popular
   words in the captions to compute them. for the image features they use
   the last hidden layer of [34]vggnet. as for the lstm model, they use a
   one-hot encoding for the questions, and the same image features as
   above followed by a linear transformation to transform the image
   features to 1024 dimensions to match the lstm encoding of the question.
   question and image encodings are combined via element wise
   multiplication.

   the performances achieved by these baselines are very interesting. for
   example, if the models are trained only on textual features, the
   accuracy is 48.09%, whereas if they are trained only on visual features
   it goes down to 28.13%. their best model, a lstm trained on both kind
   of features, has an accuracy of 53.74%. the authors confirm that
   results on multiple-choice are better than open-answer and, as
   expected, all methods are significantly worse than human performance.

   many variations in this framework can be implemented to obtain
   different baselines.

approaches based on attention

   the goal of the attention-based approaches is to set the focus of the
   algorithm on the most relevant parts of the input. for example, if the
   question is    what color is the ball?   , the image region containing the
   ball is more relevant than the others. in the same way,    color    and
      ball    are more informative that the rest of the words.

   the most common choice in vqa is to use spatial attention to generate
   region specific features to train id98s. there are two common methods to
   obtain the spatial regions of an image. first, by projecting a grid
   over the image.
   [approach-attention.041719cb.jpg]

    using a grid to incorporate attention

   [35]image source

   after the grid has been applied, the relevance of each region is
   determined by the specific question.

   the other way is by proposing automatically generated bounding boxes.
   [approach-attention-bounding-box.5952d81d.jpg]

    example of proposed regions using edge boxes

   [36]image source

   given the proposed regions, we can use the question to determine the
   relevance of the features for each one, and to pick only those that are
   necessary to answer the question.

   these are only two techniques to incorporate attention to vqa systems,
   and many more can be found in the literature.

bayesian approaches

   the idea behind bayesian approaches is to model co-occurrence
   statistics of both the question and the image features, as a way of
   inferring relationships between questions and images.

   in [37]kafle and kanan (2016), for example, the authors model the
   id203 of image features given the question features and the type
   of the answer. they do so because they observe that given a question,
   the type of answer can be frequently predicted. for example,    how many
   players are in the image?    is a    how many    question, that needs a
   number as an answer. to model the probabilities they combine a bayesian
   model with a discriminative model. regarding the features, they use
   [38]resnet for the images and skip-thought vectors for text.

                               id74

   it may seem obvious, but it is good to remember: the performance of any
   system (and consequently the state-of-the-art in a given domain)
   depends on the metrics with which it is evaluated. in vqa, the most
   straightforward metric we can use is classic accuracy. while this is a
   reasonable option for multiple-choice answer systems, when it comes to
   open-ended answers it tends to be very penalizing. if the ground truth
   answer is    oak tree   , does that mean that the answer    tree    is
   absolutely incorrect? if the question is    what animals appear in the
   image?   , and the image shows dogs, cats and rabbits, how incorrect the
   answer    cats and dogs    should be?

   these are very complex issues that have to be addressed in order to
   evaluate the different approaches as precisely as possible.

wups

   the wups measure, proposed by [39]malinowski and fritz in 2014, and
   based on the [40]wup measure by wu and palmer back in 1994, estimates
   the semantic distance between an answer and the ground truth, that is a
   value between 0 and 1. they rely on id138 to compute similarity using
   the distance, in the semantic tree, of the terms contained both in the
   answer and the ground truth. that way, for single terms, we could get
   results such as:

   \mathit{wups} (``\text{oak tree}", ``\text{tree}") = 0.94

   \mathit{wups} (``\text{dogs, cats and rabbits}", ``\text{cats and
   rabbits}") = 0.8

   as with almost all semantic measures, wups assigns relatively
   significant values to terms that are absolutely disconnected. to
   address this issue, the authors proposes to scale down by a factor of
   0.1 the scores below 0.9.

   it is undeniable that for many cases wups fits better than classic
   accuracy. however, since it relies on semantic similarity, the answer
      red    will have a very high score if the ground truth is    black    or
      green    or another color. the other problem is that it only works in
   small terms, and only if they have a id138 meaning.

multiple independent ground truth answers

   instead of relying on semantic measures, we could have multiple ground
   truth answers per question, as we saw for the vqa dataset. then we can,
   for example, state that a given answer is correct if it matches the
   more frequent answer, or if at least it matches one of the possible
   ground truth answers. the latter has to be applied carefully, because
   of the yes/no questions that do not have a consensual answer: in that
   case, any answer would be correct.

manual evaluation

   finally, another way of addressing the evaluation phase is to use human
   judges to assess the answers. this is, of course, an extremely
   expensive approach. moreover, guidelines with clear criteria had to be
   established so that judges can correctly evaluate the answers. some
   type of training should be foreseen, aiming at the good quality of the
   evaluations and a good agreement between judges.

                             real life applications

   there are many potential applications for vqa. probably the most direct
   application is to help blind and visually-impaired users. a vqa system
   could provide information about an image on the web or any social
   media. another obvious application is to integrate vqa into image
   retrieval systems. this could have a huge impact on social media or
   e-commerce. vqa can also be used with educational or recreational
   purposes.

   the [41]vqa consortium has a very complete and useful website with
   information, resources and software on vqa. they hold the vqa challenge
   as well as the vqa challenge workshop, and it is worthwhile to take a
   look at the tasks, talks and papers because they give a good hint about
   the future directions of the field. you can also try [42]cloudcv, a
   very interesting online demo of a vqa system.

     thinking of implementing a machine learning project in your
     organization? [43]here are 11 essential questions to ask before
     kicking off an ml initiative.

                                 final thoughts

   vqa is a recent field that requires the understanding of both text and
   vision. since deep learning techniques are significantly improving nlp
   and cv results, we can reasonably expect that vqa is going to be more
   and more accurate in the next years.

   as with many other tasks in ia, the built datasets and the defined
   metrics have somehow shaped the research work that has been done so
   far. the best way to evaluate a vqa system is still an open question,
   and it is very likely that new datasets and metrics will allow to
   deepen and refine the notion of quality.
   [visual-question-answering-phrasing.2ccf95ef.jpg]

    how the phrasing of a question can bias a vqa system.

   [44]image source

   there are still several remaining issues and ongoing discussions. for
   example, is it good, appropriate or fair to use multiple-choice
   datasets, given that a vqa system can be right by chance? is a system
   with excellent performance in multiple-choice datasets a good system in
   real life, where these are not available? what is the impact of the way
   a question is formulated in the response that is obtained? (see image
   above)

   although current performances are far from human decisions, the results
   are already exploitable and, in fact, very promising. as vqa is adopted
   by large public platforms, devices and tools, it is very likely that it
   will change the way we search and interact with data.
   [45]twitter [46]linkedin [47]facebook [48]reddit
     __________________________________________________________________

like what you read?

   subscribe to our newsletter and get updates on deep learning, nlp,
   id161 & python.
   ____________________ subscribe
   error
   success!
   sending...
   ____________________
   no spam, ever. we'll never share your email address and you can opt out
   at any time.

   please enable javascript to view the [49]comments powered by disqus.
   [50]comments powered by disqus

what to read next

     *

5 actionable steps to get your data ready for price optimization with ml
     *

11 questions to ask before starting a successful machine learning project
     *

the major advancements in deep learning in 2018

get in touch

do you have a project in mind?
we'd love to e-meet you!

   ____________________
   ____________________
   ____________________
   ____________________

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   [ ] subscribe to receive news and blog updates
   contact us
   thanks for getting in touch! we will reply shortly.
   there was an error :(

   tryolabs

   about
     * [51]about us
     * [52]what we do
     * [53]ml consulting

   our work
     * [54]clients
     * [55]products
     * [56]brochure

   community
     * [57]blog
     * [58]open source
     * [59]careers

   contact
     * uy phone: (598) 2716 8997
     * [60]hello@tryolabs.com

   offices
     * us: 156 2nd street, sf.
     * uy: rambla gandhi 655/701, mvd.

   social
   twitter
   linkedin
   github
   facebook
   instagram
   tryolabs    2019. all rights reserved.

references

   visible links
   1. https://tryolabs.com/
   2. https://tryolabs.com/blog/
   3. https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/#subscribe
   4. https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/#contact-form
   5. https://tryolabs.com/blog/search/
   6. https://tryolabs.com/blog/
   7. https://tryolabs.com/blog/categories/backend/
   8. https://tryolabs.com/blog/categories/frontend/
   9. https://tryolabs.com/blog/categories/machine-learning/
  10. https://tryolabs.com/blog/categories/mobile/
  11. https://tryolabs.com/blog/categories/news/
  12. https://tryolabs.com/blog/categories/others/
  13. https://tryolabs.com/blog/categories/resources/
  14. https://tryolabs.com/blog/authors/javier-couto/
  15. https://tryolabs.com/blog/categories/machine-learning/
  16. https://tryolabs.com/blog/tags/deep-learning/
  17. https://tryolabs.com/blog/tags/vqa/
  18. https://tryolabs.com/blog/tags/computer-vision/
  19. https://tryolabs.com/blog/tags/nlp/
  20. http://www.telediariodigital.net/wp-content/uploads/2015/06/maradona-contra-inglaterra-widescreen-2.jpg
  21. http://cocodataset.org/#home
  22. https://arxiv.org/abs/1405.0312
  23. https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/
  24. https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html
  25. https://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf
  26. http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/
  27. https://arxiv.org/abs/1505.02074
  28. https://arxiv.org/abs/1610.01465
  29. http://www.visualqa.org/download.html
  30. https://arxiv.org/abs/1505.00468
  31. https://arxiv.org/abs/1610.01465
  32. https://arxiv.org/abs/1610.01465
  33. https://arxiv.org/abs/1505.00468
  34. http://www.robots.ox.ac.uk/~vgg/research/very_deep/
  35. https://arxiv.org/abs/1610.01465
  36. https://pdollar.github.io/files/papers/zitnickdollareccv14edgeboxes.pdf
  37. http://www.chriskanan.com/wp-content/uploads/kafle2016.pdf
  38. https://arxiv.org/abs/1512.03385
  39. https://arxiv.org/abs/1410.0210
  40. https://dl.acm.org/citation.cfm?id=981751
  41. http://www.visualqa.org/
  42. http://vqa.cloudcv.org/
  43. https://tryolabs.com/blog/2019/02/13/11-questions-to-ask-before-starting-a-successful-machine-learning-project/
  44. https://arxiv.org/abs/1610.01465
  45. http://twitter.com/share?url=https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/&text=introduction to visual id53: datasets, approaches and evaluation
  46. https://www.linkedin.com/sharearticle?mini=true&url=https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/&title=introduction to visual id53: datasets, approaches and evaluation&source=tryolabs&summary=historically, building a system that can answer natural language questions about any image has been considered a very ambitious goal. imagine a system that, given the image below, could answer these questions:
 what is in the image? are there any humans? what sport is being played? who has the ball? how many players are in the image? who are the teams? is it raining?  argentina facing england in 1986image source
  47. https://www.facebook.com/sharer.php?u=https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/&title=introduction to visual id53: datasets, approaches and evaluation
  48. http://www.reddit.com/submit?url=https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/&title=introduction to visual id53: datasets, approaches and evaluation
  49. http://disqus.com/?ref_noscript
  50. http://disqus.com/
  51. https://tryolabs.com/about/
  52. https://tryolabs.com/what-we-do/
  53. https://tryolabs.com/what-we-do/machine-learning-consulting/
  54. https://tryolabs.com/work/
  55. https://tryolabs.com/work/#products
  56. https://tryolabs.com/downloads/tryolabs-brochure-2017-11-01.pdf
  57. https://tryolabs.com/blog/
  58. https://github.com/tryolabs
  59. https://tryolabs.com/careers/
  60. mailto:hello@tryolabs.com

   hidden links:
  62. https://tryolabs.com/blog/2019/03/27/data-preparation-price-optimization-machine-learning/
  63. https://tryolabs.com/blog/2019/02/13/11-questions-to-ask-before-starting-a-successful-machine-learning-project/
  64. https://tryolabs.com/blog/2018/12/19/major-advancements-deep-learning-2018/
  65. https://twitter.com/tryolabs
  66. https://www.linkedin.com/company/tryolabs
  67. https://github.com/tryolabs
  68. https://www.facebook.com/tryolabs
  69. https://www.instagram.com/tryolabsteam
