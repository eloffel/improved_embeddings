   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]technology@nineleaps
   [7]technology@nineleaps
   [8]sign in[9]get started
     __________________________________________________________________

unsupervised learning

   [10]go to the profile of joydeep bhattacharjee
   [11]joydeep bhattacharjee (button) blockedunblock (button)
   followfollowing
   feb 26, 2018

   emergence
   [1*utvm-03fyuirmxnhoore0g.jpeg]
   eagle nebula

   till now we have mostly concerned ourselves with supervised machine
   learning and this is the field with the most practical applications
   right now.
   [12]id28
   the mother of all classification techniquesmedium.com

   having said that, it is unsupervised machine learning that holds the
   promise for the future. why? because getting data is cheap; it is
   getting labelled data that is relatively hard.

   in this post, we will take a view of the common unsupervised machine
   learning algorithms and techniques.

   iframe: [13]/media/a4d84afcdf60ab26e1528840afe9de57?postid=3dee8e6c8477

   unsupervised learning: another look

id116 id91

   id116 id91 aims to cluster or group n observations into k
   clusters such that the centers correspond to the respective means of
   the respective groups. to find the mean euclidean distance is used but
   you can use any valid measure for computing the distance. the algorithm
   for learning using id116 id91 is below.
    1. guess cluster centers at random.
    2. assign each data point to the cluster centers. even though they are
       randomly chosen, assign the most likely corresponding data points.
       here, euclidean distance can be used.

   now that we have a correspondence between the data points and the
   cluster centers, find the optimal cluster center that corresponds to
   the points associated with the current groups of data points. you will
   now have a new set of cluster centers.

   3. with this new set of cluster centers jump to step 2 and iterate
   through the rest of the processes either by defining the number of
   iterations or by defining if the step jump of the cluster centers is
   below a small threshold.
   [0*mvgxv1au_f5ueu3x.gif]
   [14]source

expectation maximization

   id116 algorithms are a simple and scalable machine learning
   algorithm. but it does not have the satisfaction and beauty of other
   machine learning algorithms as there is sound mathematical background
   involved in the understanding of why id116 works. we see the result
   of this in practice where id116 is quite prone to converging to local
   minima. this is where expectation minimization algorithm comes into the
   picture.
   [15]expectation maximisation - joydeep bhattacharjee - medium
   a peek into generative algorithms id116 id91 is one of the most
   simple algorithms out there and has been   medium.com

   in expectation maximization all the cluster points have mapping to all
   the data points, the difference is that the correspondence is more
   loosely based and probabilistic. on the flip side, due to the
   correspondence being a id203 distribution, the computation can be
   very slow.
   [1*wyhqzwmiylxtpxx13vwcrq.png]
   cluster and distribution side by side

   for example, for a two clusters, the id203 distribution of the
   two clusters may look like the figure on the right. thus in em, the aim
   is to compute the id203 distributions and arrive at the final
   cumulative distribution.

id84

   id84 techniques can be used for finding the
   underlying relationships between data. in the real world the data that
   we have correlates to each other a lot.
   [16]dimensional reduction and principal component analysis         i
   normally when we are applying any of the machine learning concepts, we
   need to deal with a lot of matrices. each matrix   medium.com

   the steps in principal component analysis are shown below.
    1. take the whole data set with d dimensions and n samples.
    2. compute the d dimensional mean vector (the mean for every
       dimension).
    3. compute the covariance matrix of the whole data set.
    4. compute the eigen vectors and the eigen values.
    5. sort the eigen vectors based on decreasing eigen values.
    6. choose only the first k eigen vectors based on how many dimensions
       you want to keep, where k     d.
    7. use this kxn matrix to transform the samples into the new space.

   through the application of id84, clusters in the
   data may emerge and hence, we will be able to bucket them. pca helps us
   in understanding latent interaction between the variables in an
   unsupervised setting.

spectral id91

   till now all the techniques for unsupervised learning that we have
   discussed are based on linear transformations. these linear algorithms
   fail when the relationships are non-linear. non-linearity in case of
   supervised algorithms have been handled using [17]kernel methods but we
   don   t have that luxury in case of unsupervised learning.

   for example although the below curve is two dimension we can express it
   in one dimension as it is a sine curve. we can do so because we know
   the relationship between the x axis and the y axis. but if the y axis
   relationship is not known as is the case with unsupervised learning,
   then reducing the dimension will be a tough job.
   [0*uo4yxgirmz7dmtq1.png]

   hence, to tackle such issues spectral id91 is used. the
   fundamental idea of spectral id91 is to cluster by affinity. so,
   you will create an affinity matrix which will be a sparse matrix and
   will show the distance of all the points relative to all the other
   points. affinity matrix can be created in various ways, either by
   defining an adjacency matrix, or by using a gaussian kernel.
   scikit-learn uses the following formula for finding the similarity.
similarity = np.exp(-beta * distance / distance.std())

   [18]spectral id91 for image segmentation - scikit-learn 0.19.1
   documentation
   in addition, as there is no useful information in the intensity of the
   image, or its gradient, we choose to perform the   scikit-learn.org

   once done, we can run id116 as the final step.

   in case you found this post useful then please click on the claps
   button. also, follow me [19]here or on [20]@alt227joydeep for more of
   my articles. you can also tweet me in case you want to discuss. i would
   be more than happy to help.

   references:
     * [21]expectation maximisation algorithm
     * [22]gaussian mixture models
     * [23]multivariate normal distributions
     * [24]principal component analysis
     * [25]spectral id91
     * [26]scikitlearn: spectral id91
     * [27]http://www.statisticshowto.com/em-algorithm-expectation-maximiz
       ation/

   thanks to [28]silvy peter.
     * [29]unsupervised learning
     * [30]machine learning
     * [31]id116
     * [32]data
     * [33]algorithms

   (button)
   (button)
   (button) 106 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of joydeep bhattacharjee

[35]joydeep bhattacharjee

   machine learning engineer and avid reader

     (button) follow
   [36]technology@nineleaps

[37]technology@nineleaps

   technology insights, selection & implementation

     * (button)
       (button) 106
     * (button)
     *
     *

   [38]technology@nineleaps
   never miss a story from technology@nineleaps, when you sign up for
   medium. [39]learn more
   never miss a story from technology@nineleaps
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3dee8e6c8477
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/technology-nineleaps?source=avatar-lo_rxzfhcc8i2zx-479df42c7b8e
   7. https://medium.com/technology-nineleaps?source=logo-lo_rxzfhcc8i2zx---479df42c7b8e
   8. https://medium.com/m/signin?redirect=https://medium.com/technology-nineleaps/unsupervised-learning-3dee8e6c8477&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/technology-nineleaps/unsupervised-learning-3dee8e6c8477&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@joydeepubuntu?source=post_header_lockup
  11. https://medium.com/@joydeepubuntu
  12. https://medium.com/technology-nineleaps/logistic-regression-bac1db38cb8c
  13. https://medium.com/media/a4d84afcdf60ab26e1528840afe9de57?postid=3dee8e6c8477
  14. http://www.turingfinance.com/id91-countries-real-gdp-growth-part2/
  15. https://medium.com/@joydeepubuntu/expectation-maximisation-4bb203841757
  16. https://medium.com/technology-nineleaps/dimensional-reduction-and-principal-component-analysis-i-8ce60a5ed2c2
  17. https://www.youtube.com/watch?v=9ift8kxx_9c
  18. http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html
  19. https://medium.com/@joydeepubuntu/latest
  20. https://twitter.com/alt227joydeep
  21. https://www.youtube.com/watch?v=anbinavp3eq
  22. https://www.youtube.com/watch?v=qmtuma86nzu
  23. https://brilliant.org/wiki/multivariate-normal-distribution/
  24. http://sebastianraschka.com/articles/2014_pca_step_by_step.html
  25. https://calculatedcontent.com/2012/10/09/spectral-id91/
  26. http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html
  27. http://www.statisticshowto.com/em-algorithm-expectation-maximization/
  28. https://medium.com/@silvy.peter?source=post_page
  29. https://medium.com/tag/unsupervised-learning?source=post
  30. https://medium.com/tag/machine-learning?source=post
  31. https://medium.com/tag/id116?source=post
  32. https://medium.com/tag/data?source=post
  33. https://medium.com/tag/algorithms?source=post
  34. https://medium.com/@joydeepubuntu?source=footer_card
  35. https://medium.com/@joydeepubuntu
  36. https://medium.com/technology-nineleaps?source=footer_card
  37. https://medium.com/technology-nineleaps?source=footer_card
  38. https://medium.com/technology-nineleaps
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/technology-nineleaps/logistic-regression-bac1db38cb8c
  42. https://medium.com/@joydeepubuntu/expectation-maximisation-4bb203841757
  43. https://medium.com/technology-nineleaps/dimensional-reduction-and-principal-component-analysis-i-8ce60a5ed2c2
  44. http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html
  45. https://medium.com/p/3dee8e6c8477/share/twitter
  46. https://medium.com/p/3dee8e6c8477/share/facebook
  47. https://medium.com/p/3dee8e6c8477/share/twitter
  48. https://medium.com/p/3dee8e6c8477/share/facebook
