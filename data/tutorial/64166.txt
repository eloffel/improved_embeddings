   [1]sebastianraschka

   [2]about [3]blog [4]books [5]elsewhere [6]resources [7]publications
   [8]teaching [9]software

     [10]
   [rss]

implementing a principal component analysis (pca)

    in python, step by step

   apr 13, 2014
   by sebastian raschka

sections

     * [11]sections
     * [12]introduction
          + [13]principal component analysis (pca) vs. multiple
            discriminant analysis (mda)
               o [14]what is a    good    subspace?
          + [15]summarizing the pca approach
     * [16]generating some 3-dimensional sample data
          + [17]why are we chosing a 3-dimensional sample?
     * [18]1. taking the whole dataset ignoring the class labels
     * [19]2. computing the d-dimensional mean vector
     * [20]3. a) computing the scatter matrix
     * [21]3. b) computing the covariance matrix (alternatively to the
       scatter matrix)
     * [22]4. computing eigenvectors and corresponding eigenvalues
          + [23]checking the eigenvector-eigenvalue calculation
          + [24]visualizing the eigenvectors
     * [25]5.1. sorting the eigenvectors by decreasing eigenvalues
     * [26]5.2. choosing k eigenvectors with the largest eigenvalues
     * [27]6. transforming the samples onto the new subspace
     * [28]using the pca() class from the matplotlib.mlab library
          + [29]class attributes of pca()
     * [30]differences between the step by step approach and
       matplotlib.mlab.pca()
     * [31]using the pca() class from the sklearn.decomposition library to
       confirm our results

introduction

   the main purposes of a principal component analysis are the analysis of
   data to identify patterns and finding patterns to reduce the dimensions
   of the dataset with minimal loss of information.

   here, our desired outcome of the principal component analysis is to
   project a feature space (our dataset consisting of -dimensional
   samples) onto a smaller subspace that represents our data    well   . a
   possible application would be a pattern classification task, where we
   want to reduce the computational costs and the error of parameter
   estimation by reducing the number of dimensions of our feature space by
   extracting a subspace that describes our data    best   .

principal component analysis (pca) vs. multiple discriminant analysis (mda)

   both multiple discriminant analysis (mda) and principal component
   analysis (pca) are linear transformation methods and closely related to
   each other. in pca, we are interested to find the directions
   (components) that maximize the variance in our dataset, where in mda,
   we are additionally interested to find the directions that maximize the
   separation (or discrimination) between different classes (for example,
   in pattern classification problems where our dataset consists of
   multiple classes. in contrast two pca, which ignores the class labels).

   in other words, via pca, we are projecting the entire set of data
   (without class labels) onto a different subspace, and in mda, we are
   trying to determine a suitable subspace to distinguish between patterns
   that belong to different classes. or, roughly speaking in pca we are
   trying to find the axes with maximum variances where the data is most
   spread (within a class, since pca treats the whole data set as one
   class), and in mda we are additionally maximizing the spread between
   classes.

   in typical pattern recognition problems, a pca is often followed by an
   mda.

what is a    good    subspace?

   let   s assume that our goal is to reduce the dimensions of a
   -dimensional dataset by projecting it onto a -dimensional subspace
   (where ). so, how do we know what size we should choose for , and how
   do we know if we have a feature space that represents our data    well   ?
   later, we will compute eigenvectors (the components) from our data set
   and collect them in a so-called scatter-matrix (or alternatively
   calculate them from the covariance matrix). each of those eigenvectors
   is associated with an eigenvalue, which tell us about the    length    or
      magnitude    of the eigenvectors. if we observe that all the eigenvalues
   are of very similar magnitude, this is a good indicator that our data
   is already in a    good    subspace. or if some of the eigenvalues are much
   much higher than others, we might be interested in keeping only those
   eigenvectors with the much larger eigenvalues, since they contain more
   information about our data distribution. vice versa, eigenvalues that
   are close to 0 are less informative and we might consider in dropping
   those when we construct the new feature subspace.

summarizing the pca approach

   listed below are the 6 general steps for performing a principal
   component analysis, which we will investigate in the following
   sections.
    1. [32]take the whole dataset consisting of -dimensional samples
       ignoring the class labels
    2. [33]compute the -dimensional mean vector (i.e., the means for every
       dimension of the whole dataset)
    3. [34]compute the scatter matrix (alternatively, the covariance
       matrix) of the whole data set
    4. [35]compute eigenvectors () and corresponding eigenvalues ()
    5. [36]sort the eigenvectors by decreasing eigenvalues and choose
       eigenvectors with the largest eigenvalues to form a dimensional
       matrix (where every column represents an eigenvector)
    6. [37]use this eigenvector matrix to transform the samples onto the
       new subspace. this can be summarized by the mathematical equation:
       (where is a -dimensional vector representing one sample, and is the
       transformed -dimensional sample in the new subspace.)

generating some 3-dimensional sample data

   for the following example, we will generate 40 3-dimensional samples
   randomly drawn from a multivariate gaussian distribution.
   here, we will assume that the samples stem from two different classes,
   where one half (i.e., 20) samples of our data set are labeled (class 1)
   and the other half (class 2).

   (sample means)

   (covariance matrices)

why are we chosing a 3-dimensional sample?

   the problem of multi-dimensional data is its visualization, which would
   make it quite tough to follow our example principal component analysis
   (at least visually). we could also choose a 2-dimensional sample data
   set for the following examples, but since the goal of the pca in an
      diminsionality reduction    application is to drop at least one of the
   dimensions, i find it more intuitive and visually appealing to start
   with a 3-dimensional dataset that we reduce to an 2-dimensional dataset
   by dropping 1 dimension.
import numpy as np

np.random.seed(234234782384239784) # random seed for consistency

# a reader pointed out that python 2.7 would raise a
# "valueerror: object of too small depth for desired array".
# this can be avoided by choosing a smaller random seed, e.g. 1
# or by completely omitting this line, since i just used the random seed for
# consistency.

mu_vec1 = np.array([0,0,0])
cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]])
class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20).t
assert class1_sample.shape == (3,20), "the matrix has not the dimensions 3x20"

mu_vec2 = np.array([1,1,1])
cov_mat2 = np.array([[1,0,0],[0,1,0],[0,0,1]])
class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20).t
assert class2_sample.shape == (3,20), "the matrix has not the dimensions 3x20"

   using the code above, we created two datasets - one dataset for each
   class and -
   where each column can be pictured as a 3-dimensional vector so that our
   dataset will have the form

   just to get a rough idea how the samples of our two classes and are
   distributed, let us plot them in a 3d scatter plot.
%pylab inline
from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from mpl_toolkits.mplot3d import proj3d

fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(111, projection='3d')
plt.rcparams['legend.fontsize'] = 10
ax.plot(class1_sample[0,:], class1_sample[1,:], class1_sample[2,:], 'o', markers
ize=8, color='blue', alpha=0.5, label='class1')
ax.plot(class2_sample[0,:], class2_sample[1,:], class2_sample[2,:], '^', markers
ize=8, alpha=0.5, color='red', label='class2')

plt.title('samples for class 1 and class 2')
ax.legend(loc='upper right')

plt.show()

   png

1. taking the whole dataset ignoring the class labels

   because we don   t need class labels for the pca analysis, let us merge
   the samples for our 2 classes into one -dimensional array.
all_samples = np.concatenate((class1_sample, class2_sample), axis=1)
assert all_samples.shape == (3,40), "the matrix has not the dimensions 3x40"

2. computing the d-dimensional mean vector

mean_x = np.mean(all_samples[0,:])
mean_y = np.mean(all_samples[1,:])
mean_z = np.mean(all_samples[2,:])

mean_vector = np.array([[mean_x],[mean_y],[mean_z]])

print('mean vector:\n', mean_vector)

    mean vector:
     [[ 0.50576644]
     [ 0.30186591]
     [ 0.76459177]]

3. a) computing the scatter matrix

   the scatter matrix is computed by the following equation:
   where is the mean vector
scatter_matrix = np.zeros((3,3))
for i in range(all_samples.shape[1]):
    scatter_matrix += (all_samples[:,i].reshape(3,1) - mean_vector).dot((all_sam
ples[:,i].reshape(3,1) - mean_vector).t)
print('scatter matrix:\n', scatter_matrix)

    scatter matrix:
     [[ 48.91593255   7.11744916   7.20810281]
     [  7.11744916  37.92902984   2.7370493 ]
     [  7.20810281   2.7370493   35.6363759 ]]

3. b) computing the covariance matrix (alternatively to the scatter matrix)

   alternatively, instead of calculating the scatter matrix, we could also
   calculate the covariance matrix using the in-built numpy.cov()
   function. the equations for the covariance matrix and scatter matrix
   are very similar, the only difference is, that we use the scaling
   factor (here: ) for the covariance matrix. thus, their eigenspaces will
   be identical (identical eigenvectors, only the eigenvalues are scaled
   differently by a constant factor).
cov_mat = np.cov([all_samples[0,:],all_samples[1,:],all_samples[2,:]])
print('covariance matrix:\n', cov_mat)

    covariance matrix:
     [[ 1.25425468  0.1824987   0.18482315]
     [ 0.1824987   0.97253923  0.07018075]
     [ 0.18482315  0.07018075  0.91375323]]

4. computing eigenvectors and corresponding eigenvalues

   to show that the eigenvectors are indeed identical whether we derived
   them from the scatter or the covariance matrix, let us put an assert
   statement into the code. also, we will see that the eigenvalues were
   indeed scaled by the factor 39 when we derived it from the scatter
   matrix.
# eigenvectors and eigenvalues for the from the scatter matrix
eig_val_sc, eig_vec_sc = np.linalg.eig(scatter_matrix)

# eigenvectors and eigenvalues for the from the covariance matrix
eig_val_cov, eig_vec_cov = np.linalg.eig(cov_mat)

for i in range(len(eig_val_sc)):
    eigvec_sc = eig_vec_sc[:,i].reshape(1,3).t
    eigvec_cov = eig_vec_cov[:,i].reshape(1,3).t
    assert eigvec_sc.all() == eigvec_cov.all(), 'eigenvectors are not identical'

    print('eigenvector {}: \n{}'.format(i+1, eigvec_sc))
    print('eigenvalue {} from scatter matrix: {}'.format(i+1, eig_val_sc[i]))
    print('eigenvalue {} from covariance matrix: {}'.format(i+1, eig_val_cov[i])
)
    print('scaling factor: ', eig_val_sc[i]/eig_val_cov[i])
    print(40 * '-')

    eigenvector 1:
    [[-0.84190486]
     [-0.39978877]
     [-0.36244329]]
    eigenvalue 1 from scatter matrix: 55.398855957302445
    eigenvalue 1 from covariance matrix: 1.4204834860846791
    scaling factor:  39.0
    ----------------------------------------
    eigenvector 2:
    [[-0.44565232]
     [ 0.13637858]
     [ 0.88475697]]
    eigenvalue 2 from scatter matrix: 32.42754801292286
    eigenvalue 2 from covariance matrix: 0.8314755900749456
    scaling factor:  39.0
    ----------------------------------------
    eigenvector 3:
    [[ 0.30428639]
     [-0.90640489]
     [ 0.29298458]]
    eigenvalue 3 from scatter matrix: 34.65493432806495
    eigenvalue 3 from covariance matrix: 0.8885880596939733
    scaling factor:  39.0
    ----------------------------------------

checking the eigenvector-eigenvalue calculation

   let us quickly check that the eigenvector-eigenvalue calculation is
   correct and satisfy the equation

   where
for i in range(len(eig_val_sc)):
    eigv = eig_vec_sc[:,i].reshape(1,3).t
    np.testing.assert_array_almost_equal(scatter_matrix.dot(eigv), eig_val_sc[i]
 * eigv,
                                         decimal=6, err_msg='', verbose=true)

visualizing the eigenvectors

   and before we move on to the next step, just to satisfy our own
   curiosity, we plot the eigenvectors centered at the sample mean.
%pylab inline

from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from mpl_toolkits.mplot3d import proj3d
from matplotlib.patches import fancyarrowpatch


class arrow3d(fancyarrowpatch):
    def __init__(self, xs, ys, zs, *args, **kwargs):
        fancyarrowpatch.__init__(self, (0,0), (0,0), *args, **kwargs)
        self._verts3d = xs, ys, zs

    def draw(self, renderer):
        xs3d, ys3d, zs3d = self._verts3d
        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.m)
        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))
        fancyarrowpatch.draw(self, renderer)

fig = plt.figure(figsize=(7,7))
ax = fig.add_subplot(111, projection='3d')

ax.plot(all_samples[0,:], all_samples[1,:], all_samples[2,:], 'o', markersize=8,
 color='green', alpha=0.2)
ax.plot([mean_x], [mean_y], [mean_z], 'o', markersize=10, color='red', alpha=0.5
)
for v in eig_vec_sc.t:
    a = arrow3d([mean_x, v[0]], [mean_y, v[1]], [mean_z, v[2]], mutation_scale=2
0, lw=3, arrowstyle="-|>", color="r")
    ax.add_artist(a)
ax.set_xlabel('x_values')
ax.set_ylabel('y_values')
ax.set_zlabel('z_values')

plt.title('eigenvectors')

plt.show()

   png

5.1. sorting the eigenvectors by decreasing eigenvalues

   we started with the goal to reduce the dimensionality of our feature
   space, i.e., projecting the feature space via pca onto a smaller
   subspace, where the eigenvectors will form the axes of this new feature
   subspace. however, the eigenvectors only define the directions of the
   new axis, since they have all the same unit length 1, which we can
   confirm by the following code:
for ev in eig_vec_sc:
    numpy.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))
    # instead of 'assert' because of rounding errors

   so, in order to decide which eigenvector(s) we want to drop for our
   lower-dimensional subspace, we have to take a look at the corresponding
   eigenvalues of the eigenvectors. roughly speaking, the eigenvectors
   with the lowest eigenvalues bear the least information about the
   distribution of the data, and those are the ones we want to drop.
   the common approach is to rank the eigenvectors from highest to lowest
   corresponding eigenvalue and choose the top eigenvectors.
# make a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_val_sc[i]), eig_vec_sc[:,i]) for i in range(len(eig_val
_sc))]

# sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs.sort(key=lambda x: x[0], reverse=true)

# visually confirm that the list is correctly sorted by decreasing eigenvalues
for i in eig_pairs:
    print(i[0])

    55.3988559573
    34.6549343281
    32.4275480129

5.2. choosing k eigenvectors with the largest eigenvalues

   for our simple example, where we are reducing a 3-dimensional feature
   space to a 2-dimensional feature subspace, we are combining the two
   eigenvectors with the highest eigenvalues to construct our -dimensional
   eigenvector matrix .
matrix_w = np.hstack((eig_pairs[0][1].reshape(3,1), eig_pairs[1][1].reshape(3,1)
))
print('matrix w:\n', matrix_w)

    matrix w:
     [[-0.84190486  0.30428639]
     [-0.39978877 -0.90640489]
     [-0.36244329  0.29298458]]

6. transforming the samples onto the new subspace

   in the last step, we use the -dimensional matrix that we just computed
   to transform our samples onto the new subspace via the equation .
transformed = matrix_w.t.dot(all_samples)
assert transformed.shape == (2,40), "the matrix is not 2x40 dimensional."

plt.plot(transformed[0,0:20], transformed[1,0:20], 'o', markersize=7, color='blu
e', alpha=0.5, label='class1')
plt.plot(transformed[0,20:40], transformed[1,20:40], '^', markersize=7, color='r
ed', alpha=0.5, label='class2')
plt.xlim([-4,4])
plt.ylim([-4,4])
plt.xlabel('x_values')
plt.ylabel('y_values')
plt.legend()
plt.title('transformed samples with class labels')

plt.show()

   png

using the pca() class from the matplotlib.mlab library

   now, that we have seen how a principal component analysis works, we can
   use the in-built pca() class from the matplotlib library for our
   convenience in future applications. unfortunately, the original
   documentation
   ([38]http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mla
   b.pca) is very sparse;
   a better documentation can be found here:
   [39]https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml.

   and the original code implementation of the pca() class can be viewed
   at:
   [40]https://sourcegraph.com/github.com/matplotlib/matplotlib/symbols/py
   thon/lib/matplotlib/mlab/pca

class attributes of pca()

    attrs:

    a : a centered unit sigma version of input a

    numrows, numcols: the dimensions of a

    mu : a numdims array of means of a

    sigma : a numdims array of atandard deviation of a

    fracs : the proportion of variance of each of the principal components

    wt : the weight vector for projecting a numdims point or array into pca spac
e

    y : a projected into pca space

   also, it has to be mentioned that the pca() class expects a np.array()
   as input where: 'we assume data in a is organized with
   numrows>numcols'), so that we have to transpose our dataset.

   matplotlib.mlab.pca() keeps all -dimensions of the input dataset after
   the transformation (stored in the class attribute pca.y), and assuming
   that they are already ordered (   since the pca analysis orders the pc
   axes by descending importance in terms of describing the id91, we
   see that fracs is a list of monotonically decreasing values.   ,
   [41]https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml) we
   just need to plot the first 2 columns if we are interested in
   projecting our 3-dimensional input dataset onto a 2-dimensional
   subspace.
from matplotlib.mlab import pca as mlabpca

mlab_pca = mlabpca(all_samples.t)

print('pc axes in terms of the measurement axes scaled by the standard deviation
s:\n', mlab_pca.wt)

plt.plot(mlab_pca.y[0:20,0],mlab_pca.y[0:20,1], 'o', markersize=7, color='blue',
 alpha=0.5, label='class1')
plt.plot(mlab_pca.y[20:40,0], mlab_pca.y[20:40,1], '^', markersize=7, color='red
', alpha=0.5, label='class2')

plt.xlabel('x_values')
plt.ylabel('y_values')
plt.xlim([-4,4])
plt.ylim([-4,4])
plt.legend()
plt.title('transformed samples with class labels from matplotlib.mlab.pca()')

plt.show()

    pc axes in terms of the measurement axes scaled by the standard deviations:
     [[ 0.65043619  0.53023618  0.54385876]
     [-0.01692055  0.72595458 -0.68753447]
     [ 0.75937241 -0.43799491 -0.48115902]]

   png

differences between the step by step approach and matplotlib.mlab.pca()

   when we plot the transformed dataset onto the new 2-dimensional
   subspace, we observe that the scatter plots from our step by step
   approach and the matplotlib.mlab.pca() class do not look identical.
   this is due to the fact that matplotlib.mlab.pca() class scales the
   variables to unit variance prior to calculating the covariance
   matrices. this will/could eventually lead to different variances along
   the axes and affect the contribution of the variable to principal
   components.

   one example where a scaling would make sense would be if one variable
   was measured in the unit inches where the other variable was measured
   in cm.
   however, for our hypothetical example, we assume that both variables
   have the same (arbitrary) unit, so that we skipped the step of scaling
   the input data.

using the pca() class from the sklearn.decomposition library to confirm our
results

   in order to make sure that we have not made a mistake in our step by
   step approach, we will use another library that doesn   t rescale the
   input data by default.
   here, we will use the pca class from the scikit-learn machine-learning
   library. the documentation can be found here:
   [42]http://scikit-learn.org/stable/modules/generated/sklearn.decomposit
   ion.pca.html.

   for our convenience, we can directly specify to how many components we
   want to reduce our input dataset via the n_components parameter.
    n_components : int, none or string

    number of components to keep. if n_components is not set all components are
kept:
        n_components == min(n_samples, n_features)
        if n_components ==    id113   , minka   s id113 is used to guess the dimension if
0 < n_components < 1,
        select the number of components such that the amount of variance that ne
eds to be explained
        is greater than the percentage specified by n_components

   next, we just need to use the .fit_transform() in order to perform the
   id84.
from sklearn.decomposition import pca as sklearnpca

sklearn_pca = sklearnpca(n_components=2)
sklearn_transf = sklearn_pca.fit_transform(all_samples.t)

plt.plot(sklearn_transf[0:20,0],sklearn_transf[0:20,1], 'o', markersize=7, color
='blue', alpha=0.5, label='class1')
plt.plot(sklearn_transf[20:40,0], sklearn_transf[20:40,1], '^', markersize=7, co
lor='red', alpha=0.5, label='class2')

plt.xlabel('x_values')
plt.ylabel('y_values')
plt.xlim([-4,4])
plt.ylim([-4,4])
plt.legend()
plt.title('transformed samples with class labels from matplotlib.mlab.pca()')

plt.show()

   png

   the plot above seems to be the exact mirror image of the plot from out
   step by step approach. this is due to the fact that the signs of the
   eigenvectors can be either positive or negative, since the eigenvectors
   are scaled to the unit length 1, both we can simply multiply the
   transformed data by to revert the mirror image.
sklearn_transf = sklearn_transf * (-1)

# sklearn.decomposition.pca
plt.plot(sklearn_transf[0:20,0],sklearn_transf[0:20,1], 'o', markersize=7, color
='blue', alpha=0.5, label='class1')
plt.plot(sklearn_transf[20:40,0], sklearn_transf[20:40,1], '^', markersize=7, co
lor='red', alpha=0.5, label='class2')
plt.xlabel('x_values')
plt.ylabel('y_values')
plt.xlim([-4,4])
plt.ylim([-4,4])
plt.legend()
plt.title('transformed samples via sklearn.decomposition.pca')
plt.show()

# step by step pca
plt.plot(transformed[0,0:20], transformed[1,0:20], 'o', markersize=7, color='blu
e', alpha=0.5, label='class1')
plt.plot(transformed[0,20:40], transformed[1,20:40], '^', markersize=7, color='r
ed', alpha=0.5, label='class2')
plt.xlim([-4,4])
plt.ylim([-4,4])
plt.xlabel('x_values')
plt.ylabel('y_values')
plt.legend()
plt.title('transformed samples step by step approach')
plt.show()

   png

   png

   looking at the 2 plots above, the distributions along the component
   axes look identical, only the center of the data is slightly different.
   if we want to mimic the results produced by scikit-learn   s pca class,
   we can subtract the mean vectors from the samples x to center the data
   at the coordinate system   s origin (thanks to a suggestion by alexander
   guth)     that is, replacing the transformation transformed =
   matrix_w.t.dot(all_samples) by transformed = matrix_w.t.dot(all_samples
   - mean_vector).

   [43]q

      2013-2019 sebastian raschka

references

   visible links
   1. http://sebastianraschka.com/
   2. https://sebastianraschka.com/about.html
   3. http://sebastianraschka.com/blog/index.html
   4. https://sebastianraschka.com/books.html
   5. https://sebastianraschka.com/elsewhere.html
   6. https://sebastianraschka.com/resources.html
   7. http://stat.wisc.edu/~sraschka/publications/
   8. http://stat.wisc.edu/~sraschka/teaching
   9. http://stat.wisc.edu/~sraschka/software
  10. http://sebastianraschka.com/rss_feed.xml
  11. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#sections
  12. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#introduction
  13. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#principal-component-analysis-pca-vs-multiple-discriminant-analysis-mda
  14. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#what-is-a-good-subspace
  15. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#summarizing-the-pca-approach
  16. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#generating-some-3-dimensional-sample-data
  17. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#why-are-we-chosing-a-3-dimensional-sample
  18. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#1-taking-the-whole-dataset-ignoring-the-class-labels
  19. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#2-computing-the-d-dimensional-mean-vector
  20. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#3-a-computing-the-scatter-matrix
  21. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#3-b-computing-the-covariance-matrix-alternatively-to-the-scatter-matrix
  22. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#4-computing-eigenvectors-and-corresponding-eigenvalues
  23. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#checking-the-eigenvector-eigenvalue-calculation
  24. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#visualizing-the-eigenvectors
  25. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#51-sorting-the-eigenvectors-by-decreasing-eigenvalues
  26. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#52-choosing-k-eigenvectors-with-the-largest-eigenvalues
  27. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#6-transforming-the-samples-onto-the-new-subspace
  28. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#using-the-pca-class-from-the-matplotlibmlab-library
  29. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#class-attributes-of-pca
  30. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#differences-between-the-step-by-step-approach-and-matplotlibmlabpca
  31. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#using-the-pca-class-from-the-sklearndecomposition-library-to-confirm-our-results
  32. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#drop_labels
  33. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#mean_vec
  34. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#sc_matrix
  35. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#eig_vec
  36. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#sort_eig
  37. http://sebastianraschka.com/articles/2014_pca_step_by_step.html#transform
  38. http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mlab.pca
  39. https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml
  40. https://sourcegraph.com/github.com/matplotlib/matplotlib/symbols/python/lib/matplotlib/mlab/pca
  41. https://www.clear.rice.edu/comp130/12spring/pca/pca_docs.shtml
  42. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.html
  43. https://www.quora.com/profile/sebastian-raschka-1

   hidden links:
  45. http://sebastianraschka.com/articles/2014_pca_step_by_step.html
  46. http://sebastianraschka.com/email.html
  47. https://twitter.com/rasbt
  48. https://github.com/rasbt
  49. https://scholar.google.com/citations?user=x4rcc0iaaaaj&hl=enrasbt
  50. https://linkedin.com/in/sebastianraschka
