   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   4 march 2018 / [12]id21

requests for research

   requests for research

   this post aims to provide inspiration and ideas for research directions
   to junior researchers and those trying to get into research.

   table of contents:
     * [13]task-independent data augmentation for nlp
     * [14]few-shot learning for nlp
     * [15]id21 for nlp
     * [16]id72
     * [17]cross-lingual learning
     * [18]task-independent architecture improvements

   it can be hard to find compelling topics to work on and know what
   questions are interesting to ask when you are just starting as a
   researcher in a new field. machine learning research in particular
   moves so fast these days that it is difficult to find an opening.

   this post aims to provide inspiration and ideas for research directions
   to junior researchers and those trying to get into research. it gathers
   a collection of research topics that are interesting to me, with a
   focus on nlp and id21. as such, they might obviously not
   be of interest to everyone. if you are interested in reinforcement
   learning, openai provides a [19]selection of interesting rl-focused
   research topics. in case you'd like to collaborate with others or are
   interested in a broader range of topics, have a look at the
   [20]artificial intelligence open network.

   most of these topics are not thoroughly thought out yet; in many cases,
   the general description is quite vague and subjective and many
   directions are possible. in addition, most of these are not low-hanging
   fruit, so serious effort is necessary to come up with a solution. i am
   happy to provide feedback with regard to any of these, but will not
   have time to provide more detailed guidance unless you have a working
   proof-of-concept. i will update this post periodically with new
   research directions and advances in already listed ones. note that this
   collection does not attempt to review the extensive literature but only
   aims to give a glimpse of a topic; consequently, the references won't
   be comprehensive.

   i hope that this collection will pique your interest and serve as
   inspiration for your own research agenda.

task-independent data augmentation for nlp

   data augmentation aims to create additional training data by producing
   variations of existing training examples through transformations, which
   can mirror those encountered in the real world. in id161
   (cv), common augmentation techniques are [21]mirroring, random
   cropping, shearing, etc. data augmentation is super useful in cv. for
   instance, it has been used to great effect in alexnet (krizhevsky et
   al., 2012) ^[22][1] to combat overfitting and in most state-of-the-art
   models since. in addition, data augmentation makes intuitive sense as
   it makes the training data more diverse and should thus increase a
   model's generalization ability.

   however, in nlp, data augmentation is not widely used. in my mind, this
   is for two reasons:
    1. data in nlp is discrete. this prevents us from applying simple
       transformations directly to the input data. most recently proposed
       augmentation methods in cv focus on such transformations, e.g.
       domain randomization (tobin et al., 2017) ^[23][2].
    2. small perturbations may change the meaning. deleting a negation may
       change a sentence's sentiment, while modifying a word in a
       paragraph might inadvertently change the answer to a question about
       that paragraph. this is not the case in cv where perturbing
       individual pixels does not change whether an image is a cat or dog
       and even stark changes such as interpolation of different images
       can be useful (zhang et al., 2017) ^[24][3].

   existing approaches that i am aware of are either rule-based (li et
   al., 2017) ^[25][4] or task-specific, e.g. for parsing (wang and
   eisner, 2016) ^[26][5] or zero-pronoun resolution (liu et al., 2017)
   ^[27][6]. xie et al. (2017) ^[28][7] replace words with samples from
   different distributions for language modelling and machine translation.
   recent work focuses on creating adversarial examples either by
   replacing words or characters (samanta and mehta, 2017; ebrahimi et
   al., 2017) ^[29][8] ^[30][9], concatenation (jia and liang, 2017)
   ^[31][10], or adding adversarial perturbations (yasunaga et al., 2017)
   ^[32][11]. an adversarial setup is also used by li et al. (2017)
   ^[33][12] who train a system to produce sequences that are
   indistinguishable from human-generated dialogue utterances.

   back-translation (sennrich et al., 2015; sennrich et al., 2016)
   ^[34][13] ^[35][14] is a common data augmentation method in machine
   translation (mt) that allows us to incorporate monolingual training
   data. for instance, when training a en\(\rightarrow\)fr system,
   monolingual french text is translated to english using an
   fr\(\rightarrow\)en system; the synthetic parallel data can then be
   used for training. back-translation can also be used for id141
   (mallinson et al., 2017) ^[36][15]. id141 has been used for data
   augmentation for qa (dong et al., 2017) ^[37][16], but i am not aware
   of its use for other tasks.

   another method that is close to id141 is generating sentences
   from a continuous space using a variational autoencoder (bowman et al.,
   2016; guu et al., 2017) ^[38][17] ^[39][18]. if the representations are
   disentangled as in (hu et al., 2017) ^[40][19], then we are also not
   too far from style transfer (shen et al., 2017) ^[41][20].

   there are a few research directions that would be interesting to
   pursue:
    1. evaluation study: evaluate a range of existing data augmentation
       methods as well as techniques that have not been widely used for
       augmentation such as id141 and style transfer on a diverse
       range of tasks including text classification and sequence
       labelling. identify what types of data augmentation are robust
       across task and which are task-specific. this could be packaged as
       a software library to make future benchmarking easier (think
       [42]cleverhans for nlp).
    2. data augmentation with style transfer: investigate if style
       transfer can be used to modify various attributes of training
       examples for more robust learning.
    3. learn the augmentation: similar to dong et al. (2017) we could
       learn either to paraphrase or to generate transformations for a
       particular task.
    4. learn a id27 space for data augmentation: a typical word
       embedding space clusters synonyms and antonyms together; using
       nearest neighbours in this space for replacement is thus
       infeasible. inspired by recent work (mrk  i   et al., 2017)
       ^[43][21], we could specialize the id27 space to make it
       more suitable for data augmentation.
    5. adversarial data augmentation: related to recent work in
       interpretability (ribeiro et al., 2016) ^[44][22], we could change
       the most salient words in an example, i.e. those that a model
       depends on for a prediction. this still requires a
       semantics-preserving replacement method, however.

few-shot learning for nlp

   zero-shot, one-shot and few-shot learning are one of the most
   interesting recent research directions imo. following the key insight
   from vinyals et al. (2016) ^[45][23] that a few-shot learning model
   should be explicitly trained to perform few-shot learning, we have seen
   several recent advances (ravi and larochelle, 2017; snell et al., 2017)
   ^[46][24] ^[47][25].

   learning from few labeled samples is one of the hardest problems imo
   and one of the core capabilities that separates the current generation
   of ml models from more generally applicable systems. zero-shot learning
   has only been investigated in the context of [48]learning word
   embeddings for unknown words afaik. dataless classification (song and
   roth, 2014; song et al., 2016) ^[49][26] ^[50][27] is an interesting
   related direction that embeds labels and documents in a joint space,
   but requires interpretable labels with good descriptions.

   potential research directions are the following:
    1. standardized benchmarks: create standardized benchmarks for
       few-shot learning for nlp. vinyals et al. (2016) introduce a
       one-shot language modelling task for the id32. the task,
       while useful, is dwarfed by the extensive evaluation on cv
       benchmarks and has not seen much use afaik. a few-shot learning
       benchmark for nlp should contain a large number of classes and
       provide a standardized split for reproducibility. good candidate
       tasks would be topic classification or fine-grained entity
       recognition.
    2. evaluation study: after creating such a benchmark, the next step
       would be to evaluate how well existing few-shot learning models
       from cv perform for nlp.
    3. novel methods for nlp: given a dataset for benchmarking and an
       empirical evaluation study, we could then start developing novel
       methods that can perform few-shot learning for nlp.

id21 for nlp

   id21 has had a large impact on id161 (cv) and
   has greatly lowered the entry threshold for people wanting to apply cv
   algorithms to their own problems. cv practicioners are no longer
   required to perform extensive feature-engineering for every new task,
   but can simply fine-tune a model pretrained on a large dataset with a
   small number of examples.

   in nlp, however, we have so far only been pretraining the first layer
   of our models via pretrained embeddings. recent approaches (peters et
   al., 2017, 2018) ^[51][28] ^[52][29] add pretrained language model
   embedddings, but these still require custom architectures for every
   task. in my opinion, in order to unlock the true potential of transfer
   learning for nlp, we need to pretrain the entire model and fine-tune it
   on the target task, akin to fine-tuning id163 models. language
   modelling, for instance, is a great task for pretraining and could be
   to nlp what id163 classification is to cv (howard and ruder, 2018)
   ^[53][30].

   here are some potential research directions in this context:
    1. identify useful pretraining tasks: the choice of the pretraining
       task is very important as even fine-tuning a model on a related
       task might only provide limited success (mou et al., 2016)
       ^[54][31]. other tasks such as those explored in recent work on
       learning general-purpose sentence embeddings (conneau et al., 2017;
       subramanian et al., 2018; nie et al., 2017) ^[55][32] ^[56][33]
       ^[57][34] might be complementary to language model pretraining or
       suitable for other target tasks.
    2. fine-tuning of complex architectures: pretraining is most useful
       when a model can be applied to many target tasks. however, it is
       still unclear how to pretrain more complex architectures, such as
       those used for pairwise classification tasks (augenstein et al.,
       2018) or reasoning tasks such as qa or reading comprehension.

id72

   id72 (mtl) has become more commonly used in nlp. see
   [58]here for a general overview of id72 and [59]here for
   mtl objectives for nlp. however, there is still much we don't
   understand about id72 in general.

   the main questions regarding mtl give rise to many interesting research
   directions:
    1. identify effective auxiliary tasks: one of the main questions is
       which tasks are useful for id72. label id178 has
       been shown to be a predictor of mtl success (alonso and plank,
       2017) ^[60][35], but this does not tell the whole story. in recent
       work (augenstein et al., 2018) ^[61][36], we have found that
       auxiliary tasks with more data and more fine-grained labels are
       more useful. it would be useful if future mtl papers would not only
       propose a new model or auxiliary task, but also try to understand
       why a certain auxiliary task might be better than another closely
       related one.
    2. alternatives to hard parameter sharing: hard parameter sharing is
       still the default modus operandi for mtl, but places a strong
       constraint on the model to compress knowledge pertaining to
       different tasks with the same parameters, which often makes
       learning difficult. we need better ways of doing mtl that are easy
       to use and work reliably across many tasks. recently proposed
       methods such as cross-stitch units (misra et al., 2017; ruder et
       al., 2017) ^[62][37] ^[63][38] and a label embedding layer
       (augenstein et al., 2018) are promising steps in this direction.
    3. artificial auxiliary tasks: the best auxiliary tasks are those,
       which are tailored to the target task and do not require any
       additional data. i have outlined a list of potential artificial
       auxiliary tasks [64]here. however, it is not clear which of these
       work reliably across a number of diverse tasks or what variations
       or task-specific modifications are useful.

cross-lingual learning

   creating models that perform well across languages and that can
   transfer knowledge from resource-rich to resource-poor languages is one
   of the most important research directions imo. there has been much
   progress in learning cross-lingual representations that project
   different languages into a shared embedding space. refer to ruder et
   al. (2017) ^[65][39] for a survey.

   cross-lingual representations are commonly evaluated either
   intrinsically on similarity benchmarks or extrinsically on downstream
   tasks, such as text classification. while recent methods have advanced
   the state-of-the-art for many of these settings, we do not have a good
   understanding of the tasks or languages for which these methods fail
   and how to mitigate these failures in a task-independent manner, e.g.
   by injecting task-specific constraints (mrk  i   et al., 2017).

task-independent architecture improvements

   novel architectures that outperform the current state-of-the-art and
   are tailored to specific tasks are regularly introduced, superseding
   the previous architecture. i have outlined [66]best practices for
   different nlp tasks before, but without comparing such architectures on
   different tasks, it is often hard to gain insights from specialized
   architectures and tell which components would also be useful in other
   settings.

   a particularly promising recent model is the transformer (vaswani et
   al., 2017) ^[67][40]. while the complete model might not be appropriate
   for every task, components such as multi-head attention or
   position-based encoding could be building blocks that are generally
   useful for many nlp tasks.

conclusion

   i hope you've found this collection of research directions useful. if
   you have suggestions on how to tackle some of these problems or ideas
   for related research topics, feel free to comment below.

   cover image is from tobin et al. (2017).
     __________________________________________________________________

    1. krizhevsky, a., sutskever, i., & hinton, g. e. (2012). id163
       classification with deep convolutional neural networks. in advances
       in neural information processing systems (pp. 1097-1105). [68]      
    2. tobin, j., fong, r., ray, a., schneider, j., zaremba, w., & abbeel,
       p. (2017). domain randomization for transferring deep neural
       networks from simulation to the real world. arxiv preprint
       arxiv:1703.06907. retrieved from
       [69]http://arxiv.org/abs/1703.06907 [70]      
    3. zhang, h., cisse, m., dauphin, y. n., & lopez-paz, d. (2017).
       mixup: beyond empirical risk minimization, 1   11. retrieved from
       [71]http://arxiv.org/abs/1710.09412 [72]      
    4. li, y., cohn, t., & baldwin, t. (2017). robust training under
       linguistic adversity. in proceedings of the 15th conference of the
       european chapter of the association for computational linguistics
       (vol. 2, pp. 21   27). [73]      
    5. wang, d., & eisner, j. (2016). the galactic dependencies treebanks:
       getting more data by synthesizing new languages. tacl, 4, 491   505.
       retrieved from
       [74]https://www.transacl.org/ojs/index.php/tacl/article/viewfile/91
       7/212 https://transacl.org/ojs/index.php/tacl/article/view/917
       [75]      
    6. liu, t., cui, y., yin, q., zhang, w., wang, s., & hu, g. (2017).
       generating and exploiting large-scale pseudo training data for zero
       pronoun resolution. in proceedings of the 55th annual meeting of
       the association for computational linguistics (pp. 102   111). [76]      
    7. xie, z., wang, s. i., li, j., levy, d., nie, a., jurafsky, d., &
       ng, a. y. (2017). data noising as smoothing in neural network
       language models. in proceedings of iclr 2017. [77]      
    8. samanta, s., & mehta, s. (2017). towards crafting text adversarial
       samples. arxiv preprint arxiv:1707.02812. [78]      
    9. ebrahimi, j., rao, a., lowd, d., & dou, d. (2017). hotflip:
       white-box adversarial examples for nlp. retrieved from
       [79]http://arxiv.org/abs/1712.06751 [80]      
   10. jia, r., & liang, p. (2017). adversarial examples for evaluating
       reading comprehension systems. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [81]      
   11. yasunaga, m., kasai, j., & radev, d. (2017). robust multilingual
       part-of-speech tagging via adversarial training. in proceedings of
       naacl 2018. retrieved from [82]http://arxiv.org/abs/1711.04903
       [83]      
   12. li, j., monroe, w., shi, t., ritter, a., & jurafsky, d. (2017).
       adversarial learning for neural dialogue generation. in proceedings
       of the 2017 conference on empirical methods in natural language
       processing. retrieved from [84]http://arxiv.org/abs/1701.06547
       [85]      
   13. sennrich, r., haddow, b., & birch, a. (2015). improving neural
       machine translation models with monolingual data. arxiv preprint
       arxiv:1511.06709. [86]      
   14. sennrich, r., haddow, b., & birch, a. (2016). edinburgh neural
       machine translation systems for wmt 16. arxiv preprint
       arxiv:1606.02891. [87]      
   15. mallinson, j., sennrich, r., & lapata, m. (2017). id141
       revisited with id4. in proceedings of the
       15th conference of the european chapter of the association for
       computational linguistics: volume 1, long papers (vol. 1, pp.
       881-893). [88]      
   16. dong, l., mallinson, j., reddy, s., & lapata, m. (2017). learning
       to paraphrase for id53. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [89]      
   17. bowman, s. r., vilnis, l., vinyals, o., dai, a. m., jozefowicz, r.,
       & bengio, s. (2016). generating sentences from a continuous space.
       in proceedings of the 20th signll conference on computational
       natural language learning (conll). retrieved from
       [90]http://arxiv.org/abs/1511.06349 [91]      
   18. guu, k., hashimoto, t. b., oren, y., & liang, p. (2017). generating
       sentences by editing prototypes. [92]      
   19. hu, z., yang, z., liang, x., salakhutdinov, r., & xing, e. p.
       (2017). toward controlled generation of text. in proceedings of the
       34th international conference on machine learning. [93]      
   20. shen, t., lei, t., barzilay, r., & jaakkola, t. (2017). style
       transfer from non-parallel text by cross-alignment. in advances in
       neural information processing systems. retrieved from
       [94]http://arxiv.org/abs/1705.09655 [95]      
   21. mrk  i  , n., vuli  , i., s  aghdha, d.   ., leviant, i., reichart, r.,
       ga  i  , m.,     young, s. (2017). semantic specialisation of
       distributional word vector spaces using monolingual and
       cross-lingual constraints. tacl. retrieved from
       [96]http://arxiv.org/abs/1706.00374 [97]      
   22. ribeiro, m. t., singh, s., & guestrin, c. (2016, august). why
       should i trust you?: explaining the predictions of any classifier.
       in proceedings of the 22nd acm sigkdd international conference on
       knowledge discovery and data mining (pp. 1135-1144). acm. [98]      
   23. vinyals, o., blundell, c., lillicrap, t., kavukcuoglu, k., &
       wierstra, d. (2016). matching networks for one shot learning. nips
       2016. retrieved from [99]http://arxiv.org/abs/1606.04080 [100]      
   24. ravi, s., & larochelle, h. (2017). optimization as a model for
       few-shot learning. in iclr 2017. [101]      
   25. snell, j., swersky, k., & zemel, r. s. (2017). prototypical
       networks for few-shot learning. in advances in neural information
       processing systems. [102]      
   26. song, y., & roth, d. (2014). on dataless hierarchical text
       classification. proceedings of aaai, 1579   1585. retrieved from
       [103]http://cogcomp.cs.illinois.edu/papers/songsoro14.pdf [104]      
   27. song, y., upadhyay, s., peng, h., & roth, d. (2016). cross-lingual
       dataless classification for many languages. ijcai, 2901   2907.
       [105]      
   28. peters, m. e., ammar, w., bhagavatula, c., & power, r. (2017).
       semi-supervised sequence tagging with bidirectional language
       models. in proceedings of the 55th annual meeting of the
       association for computational linguistics (acl 2017). [106]      
   29. peters, m. e., neumann, m., iyyer, m., gardner, m., clark, c., lee,
       k., & zettlemoyer, l. (2018). deep contextualized word
       representations. proceedings of naacl. [107]      
   30. howard, j., & ruder, s. (2018). fine-tuned language models for text
       classification. arxiv preprint arxiv:1801.06146. [108]      
   31. mou, l., meng, z., yan, r., li, g., xu, y., zhang, l., & jin, z.
       (2016). how transferable are neural networks in nlp applications?
       proceedings of 2016 conference on empirical methods in natural
       language processing. [109]      
   32. conneau, a., kiela, d., schwenk, h., barrault, l., & bordes, a.
       (2017). supervised learning of universal sentence representations
       from natural language id136 data. in proceedings of the 2017
       conference on empirical methods in natural language processing.
       [110]      
   33. subramanian, s., trischler, a., bengio, y., & pal, c. j. (2018).
       learning general purpose distributed sentence representations via
       large scale id72. in proceedings of iclr 2018.
       [111]      
   34. nie, a., bennett, e. d., & goodman, n. d. (2017). dissent: sentence
       representation learning from explicit discourse relations. arxiv
       preprint arxiv:1710.04334. retrieved from
       [112]http://arxiv.org/abs/1710.04334 [113]      
   35. alonso, h. m., & plank, b. (2017). when is multitask learning
       effective? multitask learning for semantic sequence prediction
       under varying data conditions. in eacl. retrieved from
       [114]http://arxiv.org/abs/1612.02251 [115]      
   36. augenstein, i., ruder, s., & s  gaard, a. (2018). multi-task
       learning of pairwise sequence classification tasks over disparate
       label spaces. in proceedings of naacl 2018. [116]      
   37. misra, i., shrivastava, a., gupta, a., & hebert, m. (2016).
       cross-stitch networks for id72. in proceedings of
       the ieee conference on id161 and pattern recognition.
       [117]http://doi.org/10.1109/cvpr.2016.433 [118]      
   38. ruder, s., bingel, j., augenstein, i., & s  gaard, a. (2017). sluice
       networks: learning what to share between loosely related tasks.
       arxiv preprint arxiv:1705.08142. [119]      
   39. ruder, s., vuli  , i., & s  gaard, a. (2017). a survey of
       cross-lingual id27 models. arxiv preprint
       arxiv:1706.04902. retrieved from
       [120]http://arxiv.org/abs/1706.04902 [121]      
   40. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l.,
       gomez, a. n.,     polosukhin, i. (2017). attention is all you need.
       in advances in neural information processing systems. [122]      

   sebastian ruder

[123]sebastian ruder

   read [124]more posts by this author.
   [125]read more

       sebastian ruder    

[126]id21

     * [127]neural id21 for natural language processing (phd
       thesis)
     * [128]aaai 2019 highlights: dialogue, reproducibility, and more
     * [129]10 exciting ideas of 2018 in nlp

   [130]see all 12 posts    

   [131]text classification with tensorflow estimators

   tensorflow

text classification with tensorflow estimators

   this post is a tutorial that shows how to use tensorflow estimators for
   text classification. it covers loading data using datasets, using
   pre-canned estimators as baselines, id27s, and building
   custom estimators, among others.

     * sebastian ruder
       [132]sebastian ruder

   [133]optimization for deep learning highlights in 2017

   optimization

optimization for deep learning highlights in 2017

   different id119 optimization algorithms have been proposed
   in recent years but adam is still most commonly used. this post
   discusses the most exciting highlights and most promising recent
   approaches that may shape the way we will optimize our models in the
   future.

     * sebastian ruder
       [134]sebastian ruder

   [135]sebastian ruder
      
   requests for research
   share this
   please enable javascript to view the [136]comments powered by disqus.

   [137]sebastian ruder    2019

   [138]latest posts [139]twitter [140]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/transfer-learning/index.html
  13. http://ruder.io/requests-for-research/index.html#taskindependentdataaugmentationfornlp
  14. http://ruder.io/requests-for-research/index.html#fewshotlearningfornlp
  15. http://ruder.io/requests-for-research/index.html#transferlearningfornlp
  16. http://ruder.io/requests-for-research/index.html#multitasklearning
  17. http://ruder.io/requests-for-research/index.html#crosslinguallearning
  18. http://ruder.io/requests-for-research/index.html#taskindependentarchitectureimprovements
  19. https://blog.openai.com/requests-for-research-2/
  20. https://ai-on.org/
  21. https://www.coursera.org/learn/convolutional-neural-networks/lecture/ayzbx/data-augmentation
  22. http://ruder.io/requests-for-research/index.html#fn1
  23. http://ruder.io/requests-for-research/index.html#fn2
  24. http://ruder.io/requests-for-research/index.html#fn3
  25. http://ruder.io/requests-for-research/index.html#fn4
  26. http://ruder.io/requests-for-research/index.html#fn5
  27. http://ruder.io/requests-for-research/index.html#fn6
  28. http://ruder.io/requests-for-research/index.html#fn7
  29. http://ruder.io/requests-for-research/index.html#fn8
  30. http://ruder.io/requests-for-research/index.html#fn9
  31. http://ruder.io/requests-for-research/index.html#fn10
  32. http://ruder.io/requests-for-research/index.html#fn11
  33. http://ruder.io/requests-for-research/index.html#fn12
  34. http://ruder.io/requests-for-research/index.html#fn13
  35. http://ruder.io/requests-for-research/index.html#fn14
  36. http://ruder.io/requests-for-research/index.html#fn15
  37. http://ruder.io/requests-for-research/index.html#fn16
  38. http://ruder.io/requests-for-research/index.html#fn17
  39. http://ruder.io/requests-for-research/index.html#fn18
  40. http://ruder.io/requests-for-research/index.html#fn19
  41. http://ruder.io/requests-for-research/index.html#fn20
  42. https://github.com/tensorflow/cleverhans
  43. http://ruder.io/requests-for-research/index.html#fn21
  44. http://ruder.io/requests-for-research/index.html#fn22
  45. http://ruder.io/requests-for-research/index.html#fn23
  46. http://ruder.io/requests-for-research/index.html#fn24
  47. http://ruder.io/requests-for-research/index.html#fn25
  48. http://ruder.io/word-embeddings-2017/index.html#oovhandling
  49. http://ruder.io/requests-for-research/index.html#fn26
  50. http://ruder.io/requests-for-research/index.html#fn27
  51. http://ruder.io/requests-for-research/index.html#fn28
  52. http://ruder.io/requests-for-research/index.html#fn29
  53. http://ruder.io/requests-for-research/index.html#fn30
  54. http://ruder.io/requests-for-research/index.html#fn31
  55. http://ruder.io/requests-for-research/index.html#fn32
  56. http://ruder.io/requests-for-research/index.html#fn33
  57. http://ruder.io/requests-for-research/index.html#fn34
  58. http://ruder.io/multi-task/
  59. http://ruder.io/multi-task-learning-nlp/
  60. http://ruder.io/requests-for-research/index.html#fn35
  61. http://ruder.io/requests-for-research/index.html#fn36
  62. http://ruder.io/requests-for-research/index.html#fn37
  63. http://ruder.io/requests-for-research/index.html#fn38
  64. http://ruder.io/multi-task-learning-nlp/
  65. http://ruder.io/requests-for-research/index.html#fn39
  66. http://ruder.io/deep-learning-nlp-best-practices/
  67. http://ruder.io/requests-for-research/index.html#fn40
  68. http://ruder.io/requests-for-research/index.html#fnref1
  69. http://arxiv.org/abs/1703.06907
  70. http://ruder.io/requests-for-research/index.html#fnref2
  71. http://arxiv.org/abs/1710.09412
  72. http://ruder.io/requests-for-research/index.html#fnref3
  73. http://ruder.io/requests-for-research/index.html#fnref4
  74. https://www.transacl.org/ojs/index.php/tacl/article/viewfile/917/212
https://transacl.org/ojs/index.php/tacl/article/view/917
  75. http://ruder.io/requests-for-research/index.html#fnref5
  76. http://ruder.io/requests-for-research/index.html#fnref6
  77. http://ruder.io/requests-for-research/index.html#fnref7
  78. http://ruder.io/requests-for-research/index.html#fnref8
  79. http://arxiv.org/abs/1712.06751
  80. http://ruder.io/requests-for-research/index.html#fnref9
  81. http://ruder.io/requests-for-research/index.html#fnref10
  82. http://arxiv.org/abs/1711.04903
  83. http://ruder.io/requests-for-research/index.html#fnref11
  84. http://arxiv.org/abs/1701.06547
  85. http://ruder.io/requests-for-research/index.html#fnref12
  86. http://ruder.io/requests-for-research/index.html#fnref13
  87. http://ruder.io/requests-for-research/index.html#fnref14
  88. http://ruder.io/requests-for-research/index.html#fnref15
  89. http://ruder.io/requests-for-research/index.html#fnref16
  90. http://arxiv.org/abs/1511.06349
  91. http://ruder.io/requests-for-research/index.html#fnref17
  92. http://ruder.io/requests-for-research/index.html#fnref18
  93. http://ruder.io/requests-for-research/index.html#fnref19
  94. http://arxiv.org/abs/1705.09655
  95. http://ruder.io/requests-for-research/index.html#fnref20
  96. http://arxiv.org/abs/1706.00374
  97. http://ruder.io/requests-for-research/index.html#fnref21
  98. http://ruder.io/requests-for-research/index.html#fnref22
  99. http://arxiv.org/abs/1606.04080
 100. http://ruder.io/requests-for-research/index.html#fnref23
 101. http://ruder.io/requests-for-research/index.html#fnref24
 102. http://ruder.io/requests-for-research/index.html#fnref25
 103. http://cogcomp.cs.illinois.edu/papers/songsoro14.pdf
 104. http://ruder.io/requests-for-research/index.html#fnref26
 105. http://ruder.io/requests-for-research/index.html#fnref27
 106. http://ruder.io/requests-for-research/index.html#fnref28
 107. http://ruder.io/requests-for-research/index.html#fnref29
 108. http://ruder.io/requests-for-research/index.html#fnref30
 109. http://ruder.io/requests-for-research/index.html#fnref31
 110. http://ruder.io/requests-for-research/index.html#fnref32
 111. http://ruder.io/requests-for-research/index.html#fnref33
 112. http://arxiv.org/abs/1710.04334
 113. http://ruder.io/requests-for-research/index.html#fnref34
 114. http://arxiv.org/abs/1612.02251
 115. http://ruder.io/requests-for-research/index.html#fnref35
 116. http://ruder.io/requests-for-research/index.html#fnref36
 117. http://doi.org/10.1109/cvpr.2016.433
 118. http://ruder.io/requests-for-research/index.html#fnref37
 119. http://ruder.io/requests-for-research/index.html#fnref38
 120. http://arxiv.org/abs/1706.04902
 121. http://ruder.io/requests-for-research/index.html#fnref39
 122. http://ruder.io/requests-for-research/index.html#fnref40
 123. http://ruder.io/author/sebastian/index.html
 124. http://ruder.io/author/sebastian/index.html
 125. http://ruder.io/author/sebastian/index.html
 126. http://ruder.io/tag/transfer-learning/index.html
 127. http://ruder.io/thesis/index.html
 128. http://ruder.io/aaai-2019-highlights/index.html
 129. http://ruder.io/10-exciting-ideas-of-2018-in-nlp/index.html
 130. http://ruder.io/tag/transfer-learning/index.html
 131. http://ruder.io/index.html
 132. http://ruder.io/author/sebastian/index.html
 133. http://ruder.io/index.html
 134. http://ruder.io/author/sebastian/index.html
 135. http://ruder.io/
 136. https://disqus.com/?ref_noscript
 137. http://ruder.io/
 138. http://ruder.io/
 139. https://twitter.com/seb_ruder
 140. https://ghost.org/

   hidden links:
 142. https://twitter.com/seb_ruder
 143. http://ruder.io/rss/index.rss
 144. http://ruder.io/index.html
 145. http://ruder.io/index.html
 146. https://twitter.com/share?text=requests%20for%20research&url=http://ruder.io/requests-for-research/
 147. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/requests-for-research/
