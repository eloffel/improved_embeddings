   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]machine learnings
     * [9]all articles
     * [10]non-technical guide to ai
     * [11]newsletter     
     __________________________________________________________________

text classification using neural networks

   [12]go to the profile of gk_
   [13]gk_ (button) blockedunblock (button) followfollowing
   jan 25, 2017

   understanding [14]how chatbots work is important. a fundamental piece
   of machinery inside a chat-bot is the text classifier. let   s look at
   the inner workings of an id158 (ann) for text
   classification.
   [1*dpmau1p85zsgamwydkzl-a.png]
   multi-layer ann

   we   ll use 2 layers of neurons (1 hidden layer) and a    bag of words   
   approach to organizing our training data. [15]text classification comes
   in 3 flavors: pattern matching, algorithms, neural nets. while the
   [16]algorithmic approach using multinomial naive bayes is surprisingly
   effective, it suffers from 3 fundamental flaws:
     * the algorithm produces a score rather than a id203. we want a
       id203 to ignore predictions below some threshold. this is
       akin to a    squelch    dial on a vhf radio.
     * the algorithm    learns    from examples of what is in a class, but not
       what isn   t. this learning of patterns of what does not belong to a
       class is often very important.
     * classes with disproportionately large training sets can create
       distorted classification scores, forcing the algorithm to adjust
       scores relative to class size. this is not ideal.

     join 30,000+ people who read the weekly [17]    machine
     learnings    newsletter to understand how ai will impact the way they
     work and live.

   as with its    naive    counterpart, this classifier isn   t attempting to
   understand the meaning of a sentence, it   s trying to classify it. in
   fact so called    ai chat-bots    do not understand language, but that   s
   [18]another story.

if you are new to id158s, here is [19]how they work.

to understand an algorithm approach to classification, see [20]here.

   let   s examine our text classifier one section at a time. we will take
   the following steps:
    1. refer to libraries we need
    2. provide training data
    3. organize our data
    4. iterate: code + test the results + tune the model
    5. abstract

   the code is [21]here, we   re using [22]ipython notebook which is a super
   productive way of working on data science projects. the code syntax is
   python.

   we begin by importing our natural language toolkit. we need a way to
   reliably tokenize sentences into words and a way to stem words.

   iframe: [23]/media/c9a956d7aaa2653d9f83466495751ba2?postid=f5cd7b8765c6

   and our training data, 12 sentences belonging to 3 classes (   intents   ).

   iframe: [24]/media/7790b6dc35aba68266929eb7851f8b6d?postid=f5cd7b8765c6

12 sentences in training data

   we can now organize our data structures for documents, classes and
   words.

   iframe: [25]/media/0b68ecdd857bc11dda550dc1116d92a9?postid=f5cd7b8765c6

12 documents
3 classes ['greeting', 'goodbye', 'sandwich']
26 unique stemmed words ['sandwich', 'hav', 'a', 'how', 'for', 'ar', 'good', 'ma
k', 'me', 'it', 'day', 'soon', 'nic', 'lat', 'going', 'you', 'today', 'can', 'lu
nch', 'is', "'s", 'see', 'to', 'talk', 'yo', 'what']

   notice that each word is stemmed and lower-cased. id30 helps the
   machine equate words like    have    and    having   . we don   t care about
   case.
   [1*euedufal7_si_qwseistzg.png]

   our training data is transformed into    bag of words    for each sentence.

   iframe: [26]/media/65dd76f2599f437225bb983b5751a2f8?postid=f5cd7b8765c6

['how', 'ar', 'you', '?']
[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0]

   the above step is a classic in text classification: each training
   sentence is reduced to an array of 0   s and 1   s against the array of
   unique words in the corpus.
['how', 'are', 'you', '?']

   is stemmed:
['how', 'ar', 'you', '?']

   then transformed to input: a 1 for each word in the bag (the ? is
   ignored)
[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

   and output: the first class
[1, 0, 0]

   note that a sentence could be given multiple classes, or none.

   make sure the above makes sense and play with the code until you grok
   it.

your first step in [27]machine learning is to have clean data.

   [1*ccqpggeblgej32mvf2lalg.png]

   next we have our core functions for our 2-layer neural network.

   if you are new to id158s, here is [28]how they
   work.

   we use [29]numpy because we want our id127 to be fast.
   [1*8sjcwjxz8j7yty6k-dwxkw.png]

   we use a sigmoid function to normalize values and its derivative to
   measure the error rate. iterating and adjusting until our error rate is
   acceptably low.

   also below we implement our bag-of-words function, transforming an
   input sentence into an array of 0   s and 1   s. this matches precisely
   with our transform for training data, always crucial to get this right.

   iframe: [30]/media/bac1bf2bbcc7a0194f622ed446b93ce1?postid=f5cd7b8765c6

   and now we code our neural network training function to create synaptic
   weights. don   t get too excited, this is mostly matrix
   multiplication         from middle-school math class.

   iframe: [31]/media/d4188aa4f7e31330e0af33a0ecea4388?postid=f5cd7b8765c6

   credit andrew trask
   [32]https://iamtrask.github.io//2015/07/12/basic-python-network/

   we are now ready to build our neural network model, we will save this
   as a json structure to represent our synaptic weights.

   you should experiment with different    alpha    (id119
   parameter) and see how it affects the error rate. this parameter helps
   our error adjustment find the lowest error rate:

   synapse_0 += alpha * synapse_0_weight_update
   [1*hz-yqpdbm4hdbh4q5fcsma.png]

   we use 20 neurons in our hidden layer, you can adjust this easily.
   these parameters will vary depending on the dimensions and shape of
   your training data, tune them down to ~10^-3 as a reasonable error
   rate.

   iframe: [33]/media/560b05767eb05998a8b42951e9f9ccc7?postid=f5cd7b8765c6

training with 20 neurons, alpha:0.1, dropout:false
input matrix: 12x26    output matrix: 1x3
delta after 10000 iterations:0.0062613597435
delta after 20000 iterations:0.00428296074919
delta after 30000 iterations:0.00343930779307
delta after 40000 iterations:0.00294648034566
delta after 50000 iterations:0.00261467859609
delta after 60000 iterations:0.00237219554105
delta after 70000 iterations:0.00218521899378
delta after 80000 iterations:0.00203547284581
delta after 90000 iterations:0.00191211022401
delta after 100000 iterations:0.00180823798397
saved synapses to: synapses.json
processing time: 6.501226902008057 seconds

   the synapse.json file contains all of our synaptic weights, this is our
   model.
   [1*qykcgpe3dd26vd-qdwsica.jpeg]

   this classify() function is all that   s needed for the classification
   once synapse weights have been calculated: ~15 lines of code.

   the catch: if there   s a change to the training data our model will need
   to be re-calculated. for a very large dataset this could take a
   non-insignificant amount of time.

   we can now generate the id203 of a sentence belonging to one (or
   more) of our classes. this is super fast because it   s dot-product
   calculation in our previously defined think() function.

   iframe: [34]/media/a239137b797b8d9ca5491473369d1258?postid=f5cd7b8765c6

sudo make me a sandwich
 [['sandwich', 0.99917711814437993]]
how are you today?
 [['greeting', 0.99864563257858363]]
talk to you tomorrow
 [['goodbye', 0.95647479275905511]]
who are you?
 [['greeting', 0.8964283843977312]]
make me some lunch
 [['sandwich', 0.95371924052636048]]
how was your lunch today?
 [['greeting', 0.99120883810944971], ['sandwich', 0.31626066870883057]]

   experiment with other sentences and different probabilities, you can
   then add training data and improve/expand the model. notice the solid
   predictions with scant training data.

   some sentences will produce multiple predictions (above a threshold).
   you will need to establish the right threshold level for your
   application. not all text classification scenarios are the same: some
   predictive situations require more confidence than others.

   the last classification shows some internal details:
found in bag: good
found in bag: day
sentence: good day
 bow: [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
good day
 [['greeting', 0.99664077655648697]]

   notice the bag-of-words (bow) for the sentence, 2 words matched our
   corpus. the neural-net also learns from the 0   s, the non-matching
   words.

   a low-id203 classification is easily shown by providing a
   sentence where    a    (common word) is the only match, for example:
found in bag: a
sentence: a burrito!
 bow: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
a burrito!
 [['sandwich', 0.61776860634647834]]

   here you have a fundamental piece of machinery for building a chat-bot,
   capable of handling a large # of classes (   intents   ) and suitable for
   classes with limited or extensive training data (   patterns   ). adding
   one or more responses to an intent is trivial.
     __________________________________________________________________

   [35][1*xkdpcjflfiqalxzrf0iuna.png]

   thanks to [36]alexander pinto.
     * [37]machine learning
     * [38]neural networks
     * [39]artificial intelligence
     * [40]chatbots
     * [41]python

   (button)
   (button)
   (button) 2.8k claps
   (button) (button) (button) 32 (button) (button)

     (button) blockedunblock (button) followfollowing
   [42]go to the profile of gk_

[43]gk_

   philosopher, entrepreneur, investor

     (button) follow
   [44]machine learnings

[45]machine learnings

   understand how machine learning and artificial intelligence will change
   your work & life.

     * (button)
       (button) 2.8k
     * (button)
     *
     *

   [46]machine learnings
   never miss a story from machine learnings, when you sign up for medium.
   [47]learn more
   never miss a story from machine learnings
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://machinelearnings.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f5cd7b8765c6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6&source=--------------------------nav_reg&operation=register
   8. https://machinelearnings.co/?source=logo-lo_es868h7mosx8---6b021343882e
   9. https://machinelearnings.co/tagged/machine-learning
  10. https://machinelearnings.co/a-humans-guide-to-machine-learning-e179f43b67a0
  11. http://subscribe.machinelearnings.co/?utm_source=machine_learnings&utm_medium=blog&utm_campaign=publication_homepage&utm_content=navigation_cta
  12. https://machinelearnings.co/@gk_?source=post_header_lockup
  13. https://machinelearnings.co/@gk_
  14. https://medium.com/p/how-chat-bots-work-dfff656a35e2
  15. https://medium.com/@gk_/how-chat-bots-work-dfff656a35e2#.3zb2b9g2v
  16. https://medium.com/@gk_/text-classification-using-algorithms-e4d50dcba45#.mho4fx7e5
  17. http://subscribe.machinelearnings.co/?utm_source=machine_learnings&utm_medium=blog&utm_campaign=guest_post&utm_content=george_kassabgi
  18. https://medium.com/@gk_/the-ai-label-is-bullshit-559b171867ff#.cqbwy3eb7
  19. https://medium.com/@gk_/how-neural-networks-work-ff4c7ad371f7
  20. https://chatbotslife.com/text-classification-using-algorithms-e4d50dcba45
  21. https://github.com/ugik/notebooks/blob/master/neural_network_classifier.ipynb
  22. https://ipython.org/notebook.html
  23. https://machinelearnings.co/media/c9a956d7aaa2653d9f83466495751ba2?postid=f5cd7b8765c6
  24. https://machinelearnings.co/media/7790b6dc35aba68266929eb7851f8b6d?postid=f5cd7b8765c6
  25. https://machinelearnings.co/media/0b68ecdd857bc11dda550dc1116d92a9?postid=f5cd7b8765c6
  26. https://machinelearnings.co/media/65dd76f2599f437225bb983b5751a2f8?postid=f5cd7b8765c6
  27. https://machinelearnings.co/a-humans-guide-to-machine-learning-e179f43b67a0
  28. https://medium.com/@gk_/how-neural-networks-work-ff4c7ad371f7
  29. http://www.numpy.org/
  30. https://machinelearnings.co/media/bac1bf2bbcc7a0194f622ed446b93ce1?postid=f5cd7b8765c6
  31. https://machinelearnings.co/media/d4188aa4f7e31330e0af33a0ecea4388?postid=f5cd7b8765c6
  32. https://iamtrask.github.io//2015/07/12/basic-python-network/
  33. https://machinelearnings.co/media/560b05767eb05998a8b42951e9f9ccc7?postid=f5cd7b8765c6
  34. https://machinelearnings.co/media/a239137b797b8d9ca5491473369d1258?postid=f5cd7b8765c6
  35. http://subscribe.machinelearnings.co/?utm_source=machine_learnings&utm_medium=blog&utm_campaign=guest_post&utm_content=george_kassabgi
  36. https://medium.com/@alexanderpinto_36207?source=post_page
  37. https://machinelearnings.co/tagged/machine-learning?source=post
  38. https://machinelearnings.co/tagged/neural-networks?source=post
  39. https://machinelearnings.co/tagged/artificial-intelligence?source=post
  40. https://machinelearnings.co/tagged/chatbots?source=post
  41. https://machinelearnings.co/tagged/python?source=post
  42. https://machinelearnings.co/@gk_?source=footer_card
  43. https://machinelearnings.co/@gk_
  44. https://machinelearnings.co/?source=footer_card
  45. https://machinelearnings.co/?source=footer_card
  46. https://machinelearnings.co/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://medium.com/p/f5cd7b8765c6/share/twitter
  50. https://medium.com/p/f5cd7b8765c6/share/facebook
  51. https://medium.com/p/f5cd7b8765c6/share/twitter
  52. https://medium.com/p/f5cd7b8765c6/share/facebook
