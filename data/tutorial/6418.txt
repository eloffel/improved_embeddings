   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

soul of the machine: how chatbots work

   [9]go to the profile of gk_
   [10]gk_ (button) blockedunblock (button) followfollowing
   jan 5, 2017
   [1*hrgcopw8vspqm-gxkohhww.jpeg]

   since the early industrial age, we   ve been fascinated by self-operating
   devices. they represent the humanization of technology.

   today, it is software that that   s becoming more human         most obviously
      chatbots.   

   but how do these machines work? first, wind back time and explore an
   earlier         yet similar         technology.

how a music box works

   [1*pveiqddv2zsog9ryjtuz-q.png]
   an early example of automation         the mechanical music box.

   a set of tuned metallic teeth aligned on a comb-like structure are
   positioned next to a cylinder with pins. each pin corresponds to a note
   at a specific interval.

   as the mechanism turns, it creates a tune by striking one or multiple
   pins at the predefined time. to play a different song, you can swap in
   a different cylindrical drum (assuming the set of unique notes is
   equivalent).

   in addition to striking a note, the movement of the drum can cause
   other automation, such as a moving figurine. either way, the
   fundamental machinery of the music box remains the same.

how a chatbot works

   text input is processed by a software function called a    classifier   ,
   this classification associates an input sentence with an    intent    (a
   conversational intent) which produces a response.
   [1*asgri9nom3j5vt2fmlo5ig.png]
   [11]a chatbot example (http://lauragelston.ghost.io/speakeasy/)

   think of a classifier as a way of categorizing a piece of data (a
   sentence) into one of several categories (an intent). the input    how
   are you?    is classified as an intent, which is associated with a
   response such as    i   m good    or (better)    i am well.   

   we learned about classification early in elementary science: a
   chimpanzee is in the class    mammals   , a blue jay is in the class
      birds   , the earth is in the class    planets    and so on. simple.

   generally speaking, there are 3 different kinds of text classifiers.
   think of these as software machinery, built for a specific purpose,
   like the drum of a music box.

chatbot text classification approaches

     * pattern matchers
     * algorithms
     * neural networks

   regardless of which type of classifier is used, the end-result is a
   response. like a music box, there can be additional    movements   
   associated with the machinery. a response can make use of external
   information (like weather, a sports score, a web lookup, etc.) but this
   isn   t specific to chatbots, it   s just additional code. a response may
   reference specific    parts of speech    in the sentence, for example: a
   proper noun. also the response (for an intent) can use conditional
   logic to provide different responses depending on the    state    of the
   conversation, this can be a random selection (to insert some    natural   
   feeling).

pattern matchers

   early chatbots used pattern matching to classify text and produce a
   response. this is often referred to as    brute force    as the author of
   the system needs to describe every pattern for which there is a
   response.

   a standard structure for these patterns is    aiml    (artificial
   intelligence markup language). its use of the term    artificial
   intelligence    is quite an embellishment, but [12]that   s another story.

   a simple pattern matching definition:
<aiml version = "1.0.1" encoding = "utf-8"?>
   <category>
      <pattern> who is albert einstein </pattern>
      <template>albert einstein was a german physicist.</template>
   </category>

   <category>
      <pattern> who is isaac newton </pattern>
      <template>isaac newton was a english physicist and mathematician.</templat
e>
   </category>

   <category>
      <pattern>do you know who * is</pattern>
      <template>
         <srai>who is <star/></srai>
      </template>
   </category>
</aiml>

   the machine then produces:
human: do you know who albert einstein is
robot: albert einstein was a german physicist.

   it knows who a physicist is only because his or her name has an
   associated pattern. likewise it responds to anything solely because of
   an authored pattern. given hundreds or thousands of patterns you might
   see a chatbot    persona    emerge.

   in 2000 a chatbot built using this approach was [13]in the news for
   passing the    turing test   , built by john denning and colleagues. it was
   built to emulate the replies of a 13 year old boy from ukraine (broken
   english and all). i met with john in 2015 and he made no false
   pretenses about the internal workings of this automaton. it may have
   been    brute force    but it proved a point: parts of a conversation can
   be made to appear    natural    using a sufficiently large definition of
   patterns. it proved alan turing   s assertion, that this question of a
   machine fooling humans was    meaningless   .

   an example of this approach used in building chatbots is
   [14]pandorabots, they claim over 285k chatbots have been constructed
   using their framework.

algorithms

   the brute-force mechanism is daunting: for each unique input a pattern
   must be available to specify a response. this creates a hierarchical
   structure of patterns, the inspiration for the idiom    rats nest   .

   to reduce the classifier to a more manageable machine, we can approach
   the work algorithmically, that is to say: we can build an equation for
   it. this is what computer scientists call a    reductionist    approach:
   the problem is reduced so that the solution is simplified.

   a classic text classification algorithm is called    multinomial naive
   bayes   , [15]taught in courses at stanford and elsewhere. here is the
   equation:
   [1*sj0tmp9mh6gee9z3xajyya.png]

   this is a lot less complicated than it appears. given a set of
   sentences, each belonging to a class, and a new input sentence, we can
   count the occurrence of each word in each class, account for its
   commonality and assign each class a score. factoring for commonality is
   important: matching the word    it    is considerably less meaningful than
   a match for the word    cheese   . the class with the highest score is the
   one most likely to belong to the input sentence. this is a slight
   oversimplification as words need to be reduced to their [16]stems, but
   you get the basic idea.

   a sample training set:
class: weather
    "is it nice outside?"
    "how is it outside?"
    "is the weather nice?"
class: greeting
    "how are you?"
    "hello there"
    "how is it going?"

   let   s classify a few sample input sentences:
input: "hi there"
 term: "hi" (no matches)
 term: "there" (class: greeting)
 classification: greeting (score=1)
input: "what   s it like outside?"
 term: "it" (class: weather (2), greeting)
 term: "outside (class: weather (2) )
 classification: weather (score=4)

   notice that the classification for    what   s it like outside    found a
   term in another class but the term similarities to the desired class
   produced a higher score. by using an equation we are looking for word
   matches given some sample sentences for each class, and we avoid having
   to identify every pattern.

   the classification score produced identifies the class with the highest
   term matches (accounting for commonality of words) but this has
   limitations. a score is not the same as a id203, a score tells us
   which intent is most like the sentence but not the likelihood of it
   being a match. thus it is difficult to apply a threshold for which
   classification scores to accept or not. having the highest score from
   this type of algorithm only provides a relative basis, it may still be
   an inherently weak classification. also the algorithm doesn   t account
   for what a sentence is not, it only counts what it is like. you might
   say this approach doesn   t consider what makes a sentence not a given
   class.

   many chatbot frameworks use [17]algorithms such as this to classify
   intent. most of what   s taking place is word counting against training
   datasets, it   s    naive    but surprisingly effective.

neural networks

   id158s, invented in the 1940   s, are a way of
   calculating an output from an input (a classification) using weighted
   connections (   synapses   ) that are calculated from repeated iterations
   through training data. each pass through the training data alters the
   weights such that the neural network produces the output with greater
      accuracy    (lower error rate).
   [1*hulatc7wx7ctzybtixgbvq.png]
   a neural network structure: nodes (circles) and synapses (lines)

   there   s not much new about these structures, except today   s software is
   using much faster processors and can work with a lot more memory. the
   combination of working memory and speed is crucial when you   re doing
   hundreds of thousands of id127s (the neural network   s
   essential math operation).

   as in the prior method, each class is given with some number of example
   sentences. once again each sentence is broken down by word (stemmed)
   and each word becomes an input for the neural network. the synaptic
   weights are then calculated by iterating through the training data
   thousands of times, each time adjusting the weights slightly to greater
   accuracy. by recalculating back across multiple layers
   (   back-propagation   ) the weights of all synapses are calibrated while
   the results are compared to the training data output. these weights are
   like a    strength    measure, in a neuron the synaptic weight is what
   causes something to be more memorable than not. you remember a thing
   more because you   ve seen it more times: each time the    weight   
   increases slightly.

   at some point the adjustment reaches a point of diminishing returns,
   this is called    over-fitting    and going beyond this is
   counter-productive.
   [1*qckgibgj74bhmaqinqwsdw.png]

   the trained neural network is less code than an comparable algorithm
   but it requires a potentially large matrix of    weights   . in a
   relatively small sample, where the training sentences have 150 unique
   words and 30 classes this would be a matrix of 150x30. imagine
   multiplying a matrix of this size 100,000 times to establish a
   sufficiently low error rate. this is where processing speed comes in.

   if the neural network sounds magnificently sophisticated, relax, it
   boils down to [18]id127 and a [19]formula for reducing
   values between -1 and 1 or some other minimal range. a middle-school
   math student could learn this in a few hours. the hard work is
   achieving clean training data.

   just as there are variations in pattern matching code and in
   algorithms, there are variations in neural networks, some more complex
   than others. the basic machinery is the same. the essential work is
   that of classification.
   [1*_lder2wurmqnq6pgp5j24w.jpeg]

   the mechanical music box knows nothing about music theory, likewise
   chatbot machinery knows nothing about language.

   chatbot machinery is looking for patterns in collections of terms, each
   term is reduced to a token. in this machine words have no meaning
   except for their patterned existence within training data. the label
      artificial intelligence    applied to such machinery [20]is mostly bs.

   the chatbot is like the mechanical music box: it   s a machine that
   produces output according to patterns. rather than using pins and
   cylindrical drums the chatbot uses software code and mathematics.

   thanks to [21]alexander pinto.
     * [22]machine learning
     * [23]artificial intelligence
     * [24]chatbots
     * [25]neural networks
     * [26]messaging

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of gk_

[28]gk_

   philosopher, entrepreneur, investor

     * (button)
       (button) 1k
     * (button)
     *
     *

   [29]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [30]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/dfff656a35e2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@gk_/how-chat-bots-work-dfff656a35e2&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@gk_/how-chat-bots-work-dfff656a35e2&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@gk_?source=post_header_lockup
  10. https://medium.com/@gk_
  11. http://lauragelston.ghost.io/speakeasy/
  12. https://medium.com/@gk_/the-ai-label-is-bullshit-559b171867ff
  13. http://mashable.com/2014/06/12/eugene-goostman-turing-test/
  14. http://www.pandorabots.com/
  15. http://nlp.stanford.edu/ir-book/pdf/13bayes.pdf
  16. https://en.wikipedia.org/wiki/id30
  17. https://medium.com/@gk_/text-classification-using-algorithms-e4d50dcba45#.ewnhttxa4
  18. https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro
  19. https://en.wikipedia.org/wiki/sigmoid_function
  20. https://medium.com/@gk_/the-ai-label-is-bullshit-559b171867ff#.3tlhftemt
  21. https://medium.com/@alexanderpinto_36207?source=post_page
  22. https://medium.com/tag/machine-learning?source=post
  23. https://medium.com/tag/artificial-intelligence?source=post
  24. https://medium.com/tag/chatbots?source=post
  25. https://medium.com/tag/neural-networks?source=post
  26. https://medium.com/tag/messaging?source=post
  27. https://medium.com/@gk_?source=footer_card
  28. https://medium.com/@gk_
  29. https://medium.freecodecamp.org/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/dfff656a35e2/share/twitter
  33. https://medium.com/p/dfff656a35e2/share/facebook
  34. https://medium.com/p/dfff656a35e2/share/twitter
  35. https://medium.com/p/dfff656a35e2/share/facebook
