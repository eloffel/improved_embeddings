   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]data science

comparing production-grade nlp libraries: training spark-nlp and spacy pipelines

   a step-by-step guide to initialize the libraries, load the data, and
   train a tokenizer model using spark-nlp and spacy.

   by [27]saif addin ellafi

   february 28, 2018

   golden pipes golden pipes (source: [28]publicdomainpictures.net)

   [29]check out the "natural language understanding at scale with spacy
   and spark nlp" tutorial session at the strata data conference in
   london, may 21-24, 2018.

   the goal of [30]this blog series is to run a realistic natural language
   processing (nlp) scenario by utilizing and comparing the leading
   production-grade linguistic programming libraries: [31]john snow labs   
   nlp for apache spark and [32]explosion ai   s spacy. both libraries are
   open source with commercially permissive licenses (apache 2.0 and mit,
   respectively). both are under active development with frequent releases
   and a growing community.

   the intention is to analyze and identify the strengths of each library,
   how they compare for data scientists and developers, and into which
   situations it may be more convenient to use one or the other. this
   analysis aims to be an objective run-through and (as in every natural
   language understanding application, by definition) involves a good
   amount of subjective decision-making in several stages.

   as simple as it may sound, it is tremendously challenging to compare
   two different libraries and make comparable benchmarking. remember that
   your application will have a different use case, data pipeline, text
   characteristics, hardware setup, and non-functional requirements than
   what   s done here.

   i'll be assuming the reader is familiar with nlp concepts and
   programming. even without knowledge of the involved tools, i aim to
   make the code as self-explanatory as possible in order to make it
   readable without bogging into too much detail. both libraries have
   public documentation and are completely open source, so consider
   reading through [33]spacy 101 and the [34]spark-nlp quick start
   documentation first.

the libraries

   spark-nlp was open sourced in october 2017. it is a native extension of
   apache spark as a spark library. it brings a suite of spark ml pipeline
   stages, in the shape of estimators and transformers, to process
   distributed data sets. [35]spark nlp annotators go from fundamentals
   like id121, id172, and id52, to
   advanced id31, spell checking, assertion status, and
   others. these are put to work within the spark ml framework. the
   library is written in scala, runs within the jvm, and takes advantage
   of spark optimizations and execution planning. the library currently
   has api   s in scala and in python.

   spacy is a popular and easy-to-use natural language processing library
   in python. it recently [36]released version 2.0, which incorporates
   neural network models, entity recognition models, and much more. it
   provides current state-of-the-art accuracy and speed levels, and has an
   active open source community. spacy been here for at least three years,
   with its first releases on github tracking back to early 2015.

   spark-nlp does not yet come with a set of pretrained models. spacy
   offers pre-trained models in seven (european) languages, so the user
   can quickly inject target sentences and get results back without having
   to train models. this includes tokens, lemmas, part-of-speech (pos),
   similarity, entity recognition, and more.

   both libraries offer customization through parameters in some level or
   another, allow the saving of trained pipelines in disk, and require the
   developer to wrap around a program that makes use of the library in a
   certain use case. spark nlp makes it easier to [37]embed an nlp
   pipeline as part of a spark ml machine learning pipeline, which also
   enables faster execution since spark can optimize the entire
   execution   from data load, nlp, feature engineering, model training,
   hyper-parameter optimization, and measurement   together at once.

the benchmark application

   the programs i am writing here, will predict part-of-speech tags in raw
   .txt files. a lot of data cleaning and preparation are in order. both
   applications will train on the same data and predict on the same data,
   to achieve the maximum possible common ground.

   my intention here is to verify two pillars of any statistical program:
    1. accuracy, which measures how good a program can predict linguistic
       features
    2. performance, which means how long i'll have to wait to achieve such
       accuracy, and how much input data i can throw at the program before
       it either collapses or my grandkids grow old.

   in order to compare these metrics, i need to make sure both libraries
   share a common ground. i have the following at my disposal:
    1. a desktop pc, running linux mint with 16gb of ram on an ssd
       storage, and an intel core i5-6600k processor running 4 cores at
       3.5ghz
    2. training, target, and correct results data, which follow nltk pos
       format (see below)
    3. jupyter python 3 notebook with spacy 2.0.5 installed
    4. apache zeppelin 0.7.3 notebook with spark-nlp 1.3.0 and apache
       spark 2.1.1 installed

the data

   data for training, testing, and measuring has been taken from the
   [38]national american corpus, utilizing their masc 3.0.2 written
   corpora from the newspaper section.

   data is wrangled with one of their tools (anctool) and, though i could
   have worked with conll data format, which contains a lot of tagged
   information such as lemma, indexes, and entity recognition, i preferred
   to utilize an nltk data format with penn pos tags, which in this
   article serves my purposes enough. it looks like this:

   neither|dt davison|nnp nor|cc most|rbs other|jj rxp|nnp opponents|nns
   doubt|vbp the|dt efficacy|nn of|in medications|nns .|.

   as you can see, the content in the training data is:
     * sentence boundary detected (new line, new sentence)
     * tokenized (space separated)
     * pos detected (pipe delimited)

   whereas in the raw text files, everything comes mixed up, dirty, and
   without any standard bounds

   here are key metrics about the benchmarks we   ll run:

the benchmark data sets

   we   ll use two benchmark data sets throughout this article. the first is
   a very small one, enabling interactive debugging and experimentation:
     * training data: 36 .txt files, totaling 77 kb
     * testing data: 14 .txt files, totaling 114 kb
     * 21,362 words to predict

   the second data set is still not    big data    by any means, but is a
   larger data set and intended to evaluate a typical single-machine use
   case:
     * training data: 72 .txt files, totaling 150 kb
     * two testing data sets: 9225 .txt files, totaling 75 mb; and 1,125,
       totaling 15 mb
     * 13+ million words

   note that we have not evaluated    big data    data sets here. this is
   because while spacy can take advantage of multicore cpu   s, it cannot
   take advantage of a cluster in the way spark nlp natively does.
   therefore, spark nlp is orders of magnitude faster on terabyte-size
   data sets using a cluster   in the same way a large-scale mpp database
   will greatly outperform a locally installed mysql server. our goal here
   is to evaluate these libraries on a single machine, using the multicore
   functionality of both libraries. this is a common scenario for systems
   under development, and also for applications that do not need to
   process large data sets.

getting started

   let's get our hands dirty, then. first things first, we've got to bring
   the necessary imports and start them up.

spacy

import os
import io
import time

import re
import random

import pandas as pd
import spacy

nlp_model = spacy.load('en', disable=['parser', 'ner'])
nlp_blank = spacy.blank('en', disable=['parser', 'ner'])

   i've disabled some pipelines in spacy in order to not bloat it with
   unnecessary parsers. i have also kept an nlp_model for reference, which
   is a pre-trained nlp model provided by spacy, but i am going to use
   nlp_blank, which will be more representative, as it will be the one
   i   ll be training myself.

spark-nlp

import org.apache.spark.sql.expressions.window
import org.apache.spark.ml.pipeline
import com.johnsnowlabs.nlp._
import com.johnsnowlabs.nlp.annotators._
import com.johnsnowlabs.nlp.annotators.pos.id88._
import com.johnsnowlabs.nlp.annotators.sbd.pragmatic._
import com.johnsnowlabs.nlp.util.io.resourcehelper
import com.johnsnowlabs.util.benchmark

   the first challenge i face is that i am dealing with three types of
   id121 results that are completely different, and will make it
   difficult to identify whether a word matched both the token and the pos
   tag:
    1. spacy's tokenizer, which works on a rule-based approach with an
       included vocabulary that saves many common abbreviations from
       breaking up
    2. sparknlp tokenizer, which also has its own rules for id121
    3. my training and testing data, which is tokenized by anc's standard
       and, in many cases, it will be splitting the words quite
       differently than our tokenizers

   so, to overcome this, i need to decide how i am going to compare pos
   tags that refer to a completely different set of tags. for spark-nlp, i
   am leaving as it is, which matches somewhat the anc open standard
   id121 format with its default rules. for spacy, i need to relax
   the infix rule so i can increase token accuracy matching by not
   breaking words by a dash "-".

spacy

class dummytokenmatch:
    def __init__(self, content):
        self.start = lambda : 0
        self.end = lambda : len(content)
def do_nothing(content):
    return [dummytokenmatch(content)]

model_tokenizer = nlp_model.tokenizer

nlp_blank.tokenizer = spacy.tokenizer.tokenizer(nlp_blank.vocab, prefix_search=m
odel_tokenizer.prefix_search,
                                suffix_search=model_tokenizer.suffix_search,
                                infix_finditer=do_nothing,
                                token_match=model_tokenizer.token_match)

   note: i am passing vocab from nlp_blank, which is not really blank.
   this vocab object has english language rules and strategies that help
   our blank model tag pos and tokenize english words   so, spacy begins
   with a slight advantage. spark-nlp doesn   t know anything about the
   english language beforehand.

training pipelines

   proceeding with the training, in spacy i need to provide a specific
   training data format, which follows this shape:
train_data = [
("i like green eggs", {'tags': ['n', 'v', 'j', 'n']}),
("eat blue ham", {'tags': ['v', 'j', 'n']})
]

   whereas in spark-nlp, i have to provide a folder of .txt files
   containing delimited word|tag data, which looks just like anc training
   data. so, i am just passing the path to the pos tagger, which is called
   id88approach.

   let   s load the training data for spacy. bear with me, as i have to add
   a few manual exceptions and rules with some characters since spacy   s
   training set is expecting clean content.

spacy

start = time.time()
train_path = "./target/training/"
train_files = sorted([train_path + f for f in os.listdir(train_path) if os.path.
isfile(os.path.join(train_path, f))])
train_data = []
for file in train_files:
    fo = io.open(file, mode='r', encoding='utf-8')
    for line in fo.readlines():
        line = line.strip()
        if line == '':
            continue
        line_words = []
        line_tags = []
        for pair in re.split("\\s+", line):
            tag = pair.strip().split("|")
            line_words.append(re.sub('(\w+)\.', '\1', tag[0].replace('$', '').re
place('-', '').replace('\'', '')))
            line_tags.append(tag[-1])
        train_data.append((' '.join(line_words), {'tags': line_tags}))
    fo.close()
train_data[240] = ('the company said the one  time provision would substantially
 eliminate all future losses at the unit .', {'tags': ['dt', 'nn', 'vbd', 'dt',
'jj', '-', 'nn', 'nn', 'md', 'rb', 'vb', 'dt', 'jj', 'nns', 'in', 'dt', 'nn', '.
']})

n_iter=5
tagger = nlp_blank.create_pipe('tagger')
tagger.add_label('-')
tagger.add_label('(')
tagger.add_label(')')
tagger.add_label('#')
tagger.add_label('...')
tagger.add_label("one-time")
nlp_blank.add_pipe(tagger)

optimizer = nlp_blank.begin_training()
for i in range(n_iter):
    random.shuffle(train_data)
    losses = {}
    for text, annotations in train_data:
        nlp_blank.update([text], [annotations], sgd=optimizer, losses=losses)
    print(losses)
print (time.time() - start)

runtime

{'tagger': 5.773235303101046}
{'tagger': 1.138113870966123}
{'tagger': 0.46656132966405683}
{'tagger': 0.5513760568314119}
{'tagger': 0.2541630900934435}
time to run: 122.11359786987305 seconds

   i had to do some field work in order to bypass a few hurdles. the
   training wouldn   t let me pass my tokenizer words, which contain some
   ugly characters within (e.g., it won   t let you train a sentence with a
   token "large-screen" or "no." unless it exists in vocab labels. then, i
   had to add those characters to the list of labels for it to work once
   found during the training.

   let see how it is to construct a pipeline in spark-nlp.

spark-nlp

val documentassembler = new documentassembler()
    .setinputcol("text")
    .setoutputcol("document")

val tokenizer = new tokenizer()
    .setinputcols("document")
    .setoutputcol("token")
    .setprefixpattern("\\a([^\\s\\p{l}\\d\\$\\.#]*)")
    .addinfixpattern("(\\$?\\d+(?:[^\\s\\d]{1}\\d+)*)")

val postagger = new id88approach()
    .setinputcols("document", "token")
    .setoutputcol("pos")
    .setcorpuspath("/home/saif/nlp/comparison/target/training")
    .setniterations(5)

val finisher = new finisher()
    .setinputcols("token", "pos")
    .setoutputasarray(true)

val pipeline = new pipeline()
    .setstages(array(
        documentassembler,
        tokenizer,
        postagger,
        finisher
    ))

val model = benchmark.time("time to train model") {
    pipeline.fit(data)
}

   as you can see, constructing a pipeline is a quite linear process: you
   set the document assembling, which makes the target text column a
   target for the next annotator, which is the tokenizer; then, the
   id88approach is the pos model, which will take as inputs both the
   document text and the tokenized form.

   i had to update the prefix pattern and add a new infix pattern to match
   dates and numbers the same way anc does (this will probably be made
   default in the next release). as you can see, every component of the
   pipeline is under control of the user; there is no implicit vocab or
   english knowledge, as opposed to spacy.

   the corpuspath from id88approach is passed to the folder
   containing the pipe-separated text files, and the finisher annotator
   wraps up the results of the pos and tokens for it to be useful next.
   setoutputasarray() will return, as it says, an array instead of a
   concatenated string, although that has some cost in processing.

   the data passed to fit() does not really matter since the only nlp
   annotator being trained is the id88approach, and this one is
   trained with external pos corpora.

runtime

time to train model: 3.167619593sec

   as a side note, it would be possible to inject in the pipeline a
   sentencedetector or a spellchecker, which in some scenarios might help
   the accuracy of the pos by letting the model know where a sentence
   ends.

what   s next?

   so far, we have initialized the libraries, loaded the data, and trained
   a tokenizer model using each one. note that spacy comes with pretrained
   tokenizers, so this step may not be necessary if your text data is from
   a language (i.e., english) and domain (i.e., news articles) that it was
   trained on, though the id121 infix alteration is significant in
   order to more likely match tokens to our anc corpus. training was more
   than 38 times faster on spark-nlp for about five iterations.

   in the [39]next installment in the blog series, we will walk through
   the code, accuracy, and performance for running this nlp pipeline using
   the models we   ve just trained.

   [40]check out the "natural language understanding at scale with spacy
   and spark nlp" tutorial session at the strata data conference in
   london, may 21-24, 2018.
   article image: golden pipes (source: [41]publicdomainpictures.net).
   tags: [42]nlp libraries

   share
    1. [43]tweet
    2.
    3.
     __________________________________________________________________

   [44]saif addin ellafi

[45]saif addin ellafi

   saif addin ellafi is a software developer, analyst, data scientist, and
   forever a student while being an extreme sports and gaming enthusiast.
   having wide experience in problem solving and quality assurance in the
   data fields in banking and finance industry, he is now the main
   contributor in spark-nlp at john snow labs.
   [46]more
     __________________________________________________________________

   video
   [47]logstash ui

   [48]data science

[49]revealing the uncommonly common with elasticsearch

   by [50]mark harwood

   this webcast looks at how elasticsearch is taking search engine
   technology and branching it out to provide insightful analysis of large
   datasets.

   [51]marina bay and the skyline of the central business district of
   singapore at dusk, by william cho.

   [52]data science

[53]how intelligent data platforms are powering smart cities

   by [54]ben lorica

   smart cities and smart nations run on data.

   [55]a woman posed with a hollerith pantograph: the keyboard is for the
   1920 population card, and a 1940 census form.

   [56]data science

[57]beyond algorithms: optimizing the search experience

   by [58]daniel tunkelang

   making search smarter through better human-computer interaction.

   runnable code
   [59]pandas table

   [60]data science

[61]introducing pandas objects

   by [62]jake vanderplas

   python data science handbook: early release

about us

     * [63]our company
     * [64]teach/speak/write
     * [65]careers
     * [66]customer service
     * [67]contact us

site map

     * [68]ideas
     * [69]learning
     * [70]topics
     * [71]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [72]terms of service     [73]privacy policy     [74]editorial independence

   animal

   iframe: [75]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/data-science
  27. https://www.oreilly.com/people/saif-addin-ellafi
  28. https://www.publicdomainpictures.net/view-image.php?image=207614&picture=golden-pipes
  29. https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/64514?intcmp=il-data-confreg-lp-steu18_new_site_saif_ellafi_blog_series_part_one_training_pipelines_top_cta
  30. https://www.oreilly.com/tags/nlp-libraries
  31. http://nlp.johnsnowlabs.com/
  32. https://spacy.io/
  33. https://spacy.io/usage/spacy-101
  34. http://nlp.johnsnowlabs.com/quickstart.html
  35. http://nlp.johnsnowlabs.com/components.html
  36. https://github.com/explosion/spacy/releases/tag/v2.0.0
  37. https://databricks.com/blog/2017/10/19/introducing-natural-language-processing-library-apache-spark.html
  38. http://www.anc.org/
  39. https://www.oreilly.com/ideas/comparing-production-grade-nlp-libraries-running-spark-nlp-and-spacy-pipelines
  40. https://conferences.oreilly.com/strata/strata-eu/public/schedule/detail/64514?intcmp=il-data-confreg-lp-steu18_new_site_saif_ellafi_blog_series_part_one_training_pipelines_end_cta
  41. https://www.publicdomainpictures.net/view-image.php?image=207614&picture=golden-pipes
  42. https://www.oreilly.com/tags/nlp-libraries
  43. https://twitter.com/share
  44. https://www.oreilly.com/people/saif-addin-ellafi
  45. https://www.oreilly.com/people/saif-addin-ellafi
  46. https://www.oreilly.com/people/saif-addin-ellafi
  47. https://www.oreilly.com/learning/revealing-the-uncommonly-common-with-elasticsearch
  48. https://www.oreilly.com/topics/data-science
  49. https://www.oreilly.com/learning/revealing-the-uncommonly-common-with-elasticsearch
  50. https://www.oreilly.com/people/d9f55-mark-harwood
  51. https://www.oreilly.com/ideas/how-intelligent-data-platforms-are-powering-smart-cities
  52. https://www.oreilly.com/topics/data-science
  53. https://www.oreilly.com/ideas/how-intelligent-data-platforms-are-powering-smart-cities
  54. https://www.oreilly.com/people/4e7ad-ben-lorica
  55. https://www.oreilly.com/ideas/beyond-algorithms-optimizing-the-search-experience
  56. https://www.oreilly.com/topics/data-science
  57. https://www.oreilly.com/ideas/beyond-algorithms-optimizing-the-search-experience
  58. https://www.oreilly.com/people/e9a1c-daniel-tunkelang
  59. https://www.oreilly.com/learning/introducing-pandas-objects
  60. https://www.oreilly.com/topics/data-science
  61. https://www.oreilly.com/learning/introducing-pandas-objects
  62. https://www.oreilly.com/people/89c9c-jake-vanderplas
  63. http://oreilly.com/about/
  64. http://oreilly.com/work-with-us.html
  65. http://oreilly.com/careers/
  66. http://shop.oreilly.com/category/customer-service.do
  67. http://shop.oreilly.com/category/customer-service.do
  68. https://www.oreilly.com/ideas
  69. https://www.oreilly.com/topics/oreilly-learning
  70. https://www.oreilly.com/topics
  71. https://www.oreilly.com/all
  72. http://oreilly.com/terms/
  73. http://oreilly.com/privacy.html
  74. http://www.oreilly.com/about/editorial_independence.html
  75. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  77. https://www.oreilly.com/
  78. https://www.facebook.com/oreilly/
  79. https://twitter.com/oreillymedia
  80. https://www.youtube.com/user/oreillymedia
  81. https://plus.google.com/+oreillymedia
  82. https://www.linkedin.com/company/oreilly-media
  83. https://www.oreilly.com/
