   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

uncovering the intuition behind id22 and inverse graphics

   [14]go to the profile of tanay kothari
   [15]tanay kothari (button) blockedunblock (button) followfollowing
   nov 23, 2017

     does the brain do inverse graphics?         geoffrey hinton

1. introduction

      id22    and    inverse graphics    seem like intimidating and
   somewhat vague terms when heard for the first time. these terms weren   t
   prevalent in mainstream media until recently, after the godfather of
   deep learning, geoffrey hinton, came out with two papers on dynamic
   routing between capsules and on matrix capsules with em routing [this
   is currently a blind submission under review for iclr 2018 but let   s be
   honest, we know it   s going to be hinton et al.].

   in this article, i will try to distill these ideas and explain the
   intuition behind them and how these are bringing machine learning
   models in id161 one step closer to emulating human vision.
   starting with the intuition behind id98s, i   ll dive into how they arise
   from our hypotheses about the neuroscience behind human sight and how
   inverse graphics is the way to create the next generation of computer
   vision systems and finally give a brief overview of how all of this
   connects to id22.

2. the first breakthrough idea: hierarchy

   research about the neuroscience and human sight led us to realize the
   fact that humans learn and analyze visual information hierarchically.
   babies first learn to recognize boundaries and colors. they take this
   information to recognize more complex entities like shapes and figure.
   slowly they learn to go from circles to eyes and mouths to entire
   faces.

   when we look at an image of a person, our brain recognizes two eyes,
   one nose, and one mouth: it recognizes all these entities which are
   present in a face and you think    this looks like a person   .

   this was the initial intuition for the origin of deep neural networks
   when they were first architected in the 1970s. these networks were
   architected to recognize low-level features and build complex entities
   from them one layer at a time.

3. the second breakthrough idea: positional equivariance

   [0*bcx3oqid868jwol8o.]
   figure 1: recognizing a cat should be the same learning process
   irrelevant of its position

invariance vs. equivariance

   what does this mean? if we have translational invariance (which is true
   for the id98s that we use right now), then these two images will be both
   predicted as cats. that is, the position of the image does not (and
   should not) affect what we classify the image as.

   the concept of equivariance is similar to invariance, except that, in
   addition to having the classification irrelevant to the position, we
   also want to predict where the object is: i.e in addition to detecting
   that it is a cat, we want the network to be able to detect if it is a
   cat on the left side, or a cat on the right side.

   let   s say we have this hierarchical network that can detect cats. you
   wouldn   t want to have one set of nodes try to learn to recognize a cat
   at one specific area in the image and another set trying to learn the
   same cat but somewhere else.

convolutions

   enter convolutional neural networks ([16]you can read more here if this
   is new to you)! these have small kernels that analyze local regions of
   an image to try to recognize features. the revolutionary idea was to
   use the same kernel all over the image to detect the occurrence of the
   same feature in multiple locations. this made the systems perform
   better and also faster due to the reduction in parameters through
   sharing them over all locations in the image.

   in 2012, hinton, with ilya sutskever and alex krizhevsky created
   alexnet: a deep convolutional neural network, which performed
   phenomenally on id163.

   id98s soon became synonymous with id161 and became applied to
   all major tasks: from id164 and image classification to
   segmentation, generative models and much more.

4. the first fall: maxpool did not work for modern day problems

        the pooling operation used in convolutional neural networks is a
     big mistake and the fact that it works so well is a
     disaster.            hinton

   [1*famfou8vs5kro3 dml7w.png]
   figure 2: max pooling with a 2x2 kernel and 2 stride

     if maxpool was a messenger between the two layers, what it tells the
     second layer is that    we saw a high-6 somewhere in the top left
     corner and a high-8 somewhere in the top right corner.

   [1*r5sjf4xtv9fapc7n39mpew.png]
   figure 3: a id98 with maxpool would classify both images as human faces
   because it would detect all the required features. because of maxpool,
   it never learnt any spatial relation between these elements.
   [[17]source]

   the first implementation of convolutional networks was in the 1980s by
   kunihiko fukushima         who architected a deep neural network called the
   [18]neocognitron with convolutional layers (which embodied
   translational equivariance) with a pooling layer after each
   convolutional layer (to allow for translational invariance). fukushima
   used maxpool at that time and eloquently explained the intuition behind
   it in his paper. the initial idea of a pooling layer made sense back
   then because the task they were trying to solve was recognizing
   handwritten digits. and we still kept on with the remnants of a distant
   past. it was about time we did something about it.

     not only do we detect all parts that make up the whole, as humans,
     we also need for all these elements to be spatially related to each
     other. however, maxpool slowly strips off this information to create
     translational invariance.

   sure, you can detect basic features in the first layer and attempt to
   send that to the next layer of nodes to detect more complex objects.
   but how do you decide how to transmit this information between these
   two sets of nodes?

   and here lies the most important piece of the puzzle: routing. what is
   routing?

     routing: the strategy or protocol used to send information from
     nodes in one layer to nodes in the next layer in a hierarchical
     learning system (aka deep neural network)

   maxpool seemed like a hack which solved the problem, performed well and
   was used as the standard. but the ghosts of our past came back to haunt
   us. by representing each 2x2 block with one number, it allowed for some
   translational invariance in where the feature could be detected and
   lead to the same output. what if the face was slightly off-center? the
   eyes could be a bit more towards the left edge, but these minute
   changes shouldn   t affect our predictions. right?

   but maxpool was far too lenient with how much translational invariance
   it allowed.
   [0*a1cbhlfyzuibzzrp.]
   figure 4: messing around with the original image actually improved the
   confidence. seems fishy. [[19]source]

   well, maybe it is a bit too lenient, but we do generalize to different
   orientations right?
   [0*6uepc8ierhzwbivn.]
   figure 5: clearly our network isn   t generalizing to different
   orientations.

   not really. the real issue wasn   t that maxpool wasn   t doing its task
   well: it was great at translational invariance.

     maxpooling (or subsampling) allows our models to be invariant to
     small changes in viewpoint.

   today, our tasks span a number of areas, where a majority of the time
   we are training our models on real life 3d images. just translational
   invariance is not going to suffice anymore. what we are looking for now
   is    viewpoint invariance   .
   [1*dmxjlnd7r5iry7-1x7vylq.png]
   figure 6: an excerpt from hinton   s paper demonstrating the same object
   observed from different viewpoints

   and here is where we dive into the basics of the math behind
   viewpoints: inverse graphics.

5. distilling viewpoints and inverse graphics

   while trying to produce a 2d image from a 3d scene from the viewpoint
   of a camera (similar to taking a picture         this is called rendering:
   which is used to create computer games and movies like avatar), the
   rendering engine needs to know where all objects are w.r.t. the camera
   (aka the viewpoint). however, we wouldn   t want to define all objects
   relative to the camera. we   d rather define them in our coordinate
   frame, and then render them from any camera viewpoint we want.

   in addition, while creating these graphics, you might want to, for
   example, define the location of the eye relative to the face, but not
   necessarily relative the entire person. essentially, you would have a
   hierarchy of parts creating a whole object.

   pose matrices help us define camera viewpoints for all objects and also
   represent the relation between the parts and the whole.

pose matrix: the essence of graphics

   a pose matrix (also called a transformation matrix) is a 4x4 matrix
   which represents the properties of an object in a coordinate frame.
   this matrix represents the 3-dimensional translation (x-y-z coordinates
   with respect to the origin), scale, and rotation.
   [1*f20n00pnarqulhvsgz_jbg.png]
   figure 7: a pose matrix. the t-values represent the translation, while
   the r-values represent the rotation and scale of the image.

   those of you who have experience in 3d modeling or image editing, you
   know exactly what i   m talking about. these concepts existed in standard
   computer graphics for decades, but somehow had escaped the grasp of
   machine learning.

   note: it is not essential to understand what the elements of the pose
   matrix mean. if you want, you can read more about the math behind pose
   matrices [20]here.
   [1*lirjju8nwseydhdrgedlqw.png]
   figure 8: pose matrices representing hierarchical relationships. m
   represents where the face is on the person, while n represents where
   the mouth is on the face.

   a whole is made of its parts, and each part is related to the main
   object via a pose matrix. the pose matrix represents the smaller part
   in the coordinates of the whole object. and here comes the most
   essential part of pose matrices: if m is the pose matrix for a face
   w.r.t a person, and n is the pose matrix for the mouth w.r.t to the
   face, we can get the coordinates of the mouth w.r.t the person (i.e.
   its pose matrix w.r.t the person) as n   = mn.

     aside: think about a pose matrix as relative velocity. if a is 5 m/s
     faster than b, and b is 5 m/s faster than c, then we can say that a
     is 5 + 5 = 10 m/s faster than c. just as we can add the two numbers
     to calculate the relative speed, we can multiply the two pose
     matrices to get the pose matrix of the mouth relative to the person.

   now if we have a camera, and we know that in the frame of the camera,
   the person   s pose matrix is p, we can extract the pose matrix and thus
   all essential properties of every part of the person by multiplying
   pose matrices. in the above example, the pose matrix for the face in
   the frame of the camera would be given by m    = pm. this is how all
   rendering engines used for games and movies function under the hood.
   [1*fnrlvnqzflt6lqh6r9-ivg.png]
   figure 9: yellow highlighted text is the pose of the objects w.r.t the
   camera viewpoint

   this pose matrix represents the different viewpoints we can look at the
   object from. all features of a face are the same, all that differs is
   the pose of the face from your viewpoint. all viewpoints of all other
   objects can be derived from only knowing p.
   [1*mijaajgo6i82p2grvdfr-a.png]
   figure 10: example of a coordinate conversion from one frame to
   another, described by the 4x4 matrix. here xw is a coordinate in the
   person   s frame, and the 4x4 matrix is p. xc is the same coordinate in
   the camera   s frame.

distilling viewpoints and inverse graphics

   inverse graphics is going in the opposite direction to what we talked
   about above. hinton believes that the brain works in this sort of way.
   looking at a 2d image, it tries to estimate the viewpoint through which
   we are looking at a virtual 3d object.
   [1*mj5nhm5ayw9pfy-5gwtxoa.png]
   figure 11: when you look at these images, do you imagine a 3d chair
   like that in front of you? can you rotate the image in your head and
   visualize how the chair would look from a top view? this is what hinton
   was getting at with inverse graphics representing the human function
   of sight.

   now, we can combine hierarchical recognition and viewpoint invariance
   to dive into how this system actually works.

estimating the inverse pose matrix

   [1*3jsyfulo947l_kjedoyzbg.png]
   figure 12: estimating the pose of the face from the pose of the
   left eye

   given a pose for the mouth, you can estimate the pose for the face (or
   in other words, if i tell you where the left eye is, you can imagine
   where the rest of the face would be, right?). similarly, we can
   estimate the pose for the face from the pose for the mouth. if you
   remember the images of kim we had earlier on, if we have a normal
   straight image, both the estimates for the face pose from the mouth and
   the left eye are similar: we can confidently say that they belong to
   the same face and thus are related. similarly, even in the upside down
   image, both the upside-down mouth and upside-down left eye hint that
   the face should be upside down. thus we assign both features to be part
   of the same whole.
   [1*gnatkvftntltbcezwaluwg.png]
   figure 13: if we can estimate ev and mv from the image (through ml
   models), we can multiply them with the inverse pose matrices (e and m)
   to get an approximate pose matrix for the face
     __________________________________________________________________

   [1*nr1dkbmqeleoh4ahfvbitw.png]
   figure 14: non-agreement on the position (pose) of the face. left: the
   actual image. right: pose estimates for the face based on the mouth and
   the left eye.

   for the distorted image of kim above, the mouth gives a hint of the
   face in the top corner while the left eye says the face should be at
   the bottom of the image. that doesn   t seem to match. so we would have
   lesser agreement between these features, and they shouldn   t be
   considered to be parts of the same whole. because this agreement only
   happens when they are placed in the correct locations w.r.t each other,
   this theory leads the network to learn relative spatial positioning. in
   this case, it would learn that a mouth should be below and between the
   two eyes for it to be part of a face, instead of recognizing a face
   just by the existence of a mouth and an eye.

     pixels might change drastically upon changing a viewpoint but the
     pose matrix changes linearly.

   this allows us to model spatial relations using linear transformations,
   enabling us to generalize to multiple viewpoints and represent
   information hierarchically by design.

   and this is what the two papers dynamic routing in capsules and matrix
   capsules with em routing explore.

   aside: there has been some work done earlier in using inverse graphics
   in machine learning: dc-ign (deep convolutional inverse graphics
   network) by kulkarni et al.
     __________________________________________________________________

6. a peek into dynamic routing with capsules

   while current networks have nodes that output scalar values (activation
   for the feature), id22 replace them with capsules which
   output the activation in addition to a vector/matrix which encapsulates
   information about the feature as well. this could be the position,
   rotation, scale, thickness of stroke, or anything else that you can
   think of.

     i know you   re thinking    why don   t we just use 8 convolutional layers
     instead of this 8d output from a capsule?    the next few sections
     would make it clearer.

   [1*gfxhh3kldywccfj5agx6pw.png]
   figure 15: the network finds the vertical and horizontal lines, and a
   hint of the diagonal line. it found all features of the 7 and some of
   the features of the 4. the text highlighted in yellow are the
   capsule outputs

   in a id98, both of the horizontal and vertical lines have a high weight
   towards both the numbers: it realizes that they can be part of the 7
   and and the 4. even though this image clearly does not have a 4, a id98
   would give the 4 a decently high id203.

   now let   s see how a capsule network would work:

   a capsule network has each capsule outputting a vector with the
   information about the position, scale, and rotation about the feature.

step 1:

     * when the capsule for 4 receives its inputs: a vertical line, a
       misplaced horizontal line, and a barely detected diagonal line, the
       capsule for the 4 outputs a low activation.
     * when the capsule for 7 receives its inputs: an ideally-placed
       vertical line, an ideally-placed horizontal line, and a barely
       detected diagonal line, the capsule for the 7 outputs a high
       activation.

   [1*db-xj6mvc67jb8qzcclyja.png]
   figure 16: pose estimates for the 4 from the poses of the features
   detected in the image. all estimates are different from each other.

dynamic routing

   the power of a capsule network comes from dynamic routing. over
   multiple epochs, our network learns to detect different features
   through its many nodes and develops a general idea of how they are
   related (for eg, a vertical line can belong to a 4 or a 7 or a 1 or
   even a 9). what dynamic routing controls is whether the vertical line
   should send information to the 4 or the 7         which one is it a part of
   in this context? this is a calculation that happens during each
   iteration to route information from capsules in a layer to capsules in
   the next layer. each capsule in layer l has a coupling strength c with
   each capsule in layer l+1, which represents the likelihood of that part
   belonging to the particular whole.

     while normal forward propagation has standard weights to pass
     information:
     z = w * a, with the coupling strength, it becomes z = c * w * a with
     (c < 1)

step 2:

     * now, the features (initial feature capsules) look at the capsules
       for the next layer.
     * the vertical line looks at the 7 and the 4, and goes like    woah,
       the 4 barely has any activation, and its approximate position is
       different from what i predicted, but the 7 looks like its exactly
       where i predicted it to be if the image was a 7 and because it has
       a high activation, the other features agree with me on that too!   .
       so it increases its coupling strength towards 7 and reduces it
       towards 4. the same thing happens for the horizontal line.
     * now that we have new coupling strengths, we recalculate our
       estimates for the pose and activation of 7 and 4. all capsules have
       a lower coupling strength to 4, so the activation for 4 would be
       even lower this time.

   hinton in his paper repeats step 1 and step 2 three times for each pair
   of capsule layers.

     through dynamic routing, lower-layer capsules get feedback from
     higher-layer capsules about what to pay attention to.

     note: if you   ve heard about the em or expectation-maximization
     algorithm, can you see how step 1 and step 2 represent the m and e
     step? ;)

   in general training of the capsule network, each capsule is not
   enforced to record specifically the pose and translational properties.
   it is free to use the vector output to encapsulate whatever it wants.
   so how do we ensure that it uses the vector representations to capture
   the properties of the object (like scale and position) as well?

reconstruction

   here   s where we use the ability to reconstruct. let   s look at the
   images of the cat again.
   [0*bcx3oqid868jwol8o.]
   figure 17: two cats: what do you get from this image?

   if i asked you to look at both these images and try to draw them again:
   if all you got from the image was that it was a cat, you might just
   draw a cat in the middle of the canvas for both the images. however, if
   you wanted your drawing to be close to the original one, when you look
   at the image, you would also try to remember where the cat was and how
   big it was to be able to draw an accurate picture.

   hinton enforced the same principle with reconstruction.
   [1*yaardfj2m2dxpnydincl7w.png]
   figure 18: training the capsule network. the encoder is the capsule
   network while the decoder is a set of fully connected layers which take
   the output of the capsule network (aka the compressed representation)

   the output of a capsule network is the activation for each capsule
   along with a vector. while training, in addition to the cross-id178
   loss for correct classification, hinton also added a reconstruction
   loss: on passing the vector-output of the correct class through a
   decoder, how off was the recreated image from the correct one?

   this automatically forced the capsule layers to be able to learn and
   represent the positions and properties of the images in addition to
   just their activations.
   [1*wjhnme3wy5adyhvx87yrhq.png]
   figure 19: each of these rows represent the perturbations along one of
   the dimensions of the vector output for different digits.

wrapping it all together

   with this, we can see that the lower layer features need to have
   similar estimates for more complex entities (like digits). the
   distorted face that we saw earlier would not be detected as a face at
   all! more so, if we pass the image of the cat on the left and the cat
   on the right, the activations and every other component of the vector
   output would be similar, except for the x-position vector, and this
   could be used to reconstruct both the images of the cat from just their
   respective encodings, without having to go through any hacks. this has
   also been shown to generalize very well to 3d objects viewed from
   different angles, and 2d objects with affine transformations. each of
   the techniques seems decent on its own, but when put together, they
   become something with a potential to revolutionize machine learning for
   the years to come.

bonus extra: coordinate addition

   as we discussed, the network can learn whatever features it seems are
   most suitable. we can tune it to recognize particular features through
   a technique called coordinate addition on top of the reconstruction
   process.

   to test if you have a perfect idea of the (x, y) coordinate of the cat
   if you can reconstruct the initial image properly, i can ask you to
   draw a cat at (x + 10, y + 5). if your output matches the same image of
   the cat shifted by that amount, then i know that you learnt to store
   the position as an (x, y) pair.

   similarly with the reconstruction, let   s say we have (in case of the
   dynamic routing paper), a 16-dimensional vector representing the
   correct output class. it reconstructs to produce the original image.
   now, we can add a small dx and dy to the first two coordinates, and
   test the network   s ability to reconstruct the image shifted by dx, dy
   pixels. the only way for the network to be consistent now is to use the
   first two elements to represent the position of the entity that was
   detected, without affecting what it was and where it was detected.

   similarly, coordinate addition can be used in all capsule layers to
   tune all capsules to keep track of spatial information. in hinton   s
   second paper on matrix capsules with em routing, they implemented
   coordinate addition by adding the position of the center of the
   kernel   s receptive field to the first two elements of the output pose
   matrix.

   this was shown to perform much better than networks without any
   coordinate addition (with a 1.8% test error rate compared to 2.6%
   without coordinate addition while using capsules with matrix outputs).

7. conclusion

   inverse graphics seems to be a pretty accurate model of human sight
   based on our current knowledge about this space. while hierarchical
   learning and parameter sharing have been around for a few years, the
   proof-of-concept of inverse graphics in id161 opens many new
   avenues for development.

   these have been shown to have state of the art performance on standard
   datasets like mnist (with a 99.75% test accuracy) and smallnorb (with a
   45% reduction in error from the previous state of the art). however,
   the applications and performance of these networks on real and more
   complex data has not been verified. but one very important benefit that
   id22 provide is taking a step to move from black-box neural
   networks to those that represent more concrete features that can help
   us analyze and understand what these are doing under the hood. (if
   you   ve seen the black mirror episode on neural networks, you would know
   how crucial it is to be able to understand this black box
   interpretation. it freaked me out.)

   this concludes the first part of this series on inverse graphics and
   dynamic routing. i hope this article helped demystify the concepts
   around inverse graphics and how they tie together human and computer
   vision. in part ii, which would be out soon, i will talk more about how
   this is implemented in id22 through dynamic routing and em
   routing.
     __________________________________________________________________

   if you enjoyed reading this article, please be sure to give me a clap
   (or more if you   d like) and follow me to know when i post the second
   part to this!

acknowledgements and further readings

     * the paper on [21]   dynamic routing between capsules   , sabour,
       hinton, frosst. the first and most well-known paper demonstrating
       the usability of id22.
     * the paper on [22]   matrix capsules with em routing   , which is
       currently an anonymous submission for the double blind review at
       iclr 2018
     * [23]   does the brain do inverse graphics   : hinton et al., university
       of toronto
     * the deep convolutional inverse graphics network ([24]dc-ign on
       arxiv). one of the first papers which trains a network to perform
       inverse graphics id136s.
     * a great article giving another perspective at id22:
       [25]kendrick tan: id22 explained
     * inspired from [26]max pechyonkin.
     * the origins of id98s and pooling: [27]the neocognitron, kunihiko
       fukushima
     * thanks to rishab mehra, shagun goel, and vikul gupta for crucial
       feedback.

     * [28]machine learning
     * [29]id22
     * [30]artificial intelligence
     * [31]graphics
     * [32]towards data science

   (button)
   (button)
   (button) 2.7k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of tanay kothari

[34]tanay kothari

   life is like stochastic id119: a little momentum can always
   help, stanford    20

     (button) follow
   [35]hacker noon

[36]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 2.7k
     * (button)
     *
     *

   [37]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [38]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7412d121798d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/uncovering-the-intuition-behind-capsule-networks-and-inverse-graphics-part-i-7412d121798d&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/uncovering-the-intuition-behind-capsule-networks-and-inverse-graphics-part-i-7412d121798d&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_zz7ipbzxudsm---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@tanaykothari?source=post_header_lockup
  15. https://hackernoon.com/@tanaykothari
  16. http://cs231n.github.io/convolutional-networks/
  17. https://medium.com/ai  -theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
  18. https://en.wikipedia.org/wiki/neocognitron
  19. https://hackernoon.com/capsule-networks-are-shaking-up-ai-heres-how-to-use-them-c233a0971952
  20. http://web.cse.ohio-state.edu/~wang.3602/courses/cse5542-2013-spring/6-transformation_ii.pdf
  21. https://arxiv.org/abs/1710.09829
  22. https://openreview.net/forum?id=hjwlfgwrb
  23. http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10754.pdf
  24. https://arxiv.org/abs/1503.03167
  25. https://kndrck.co/posts/capsule_networks_explained/
  26. https://medium.com/@pechyonkin?source=post_header_lockup
  27. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.569.5982&rep=rep1&type=pdf
  28. https://hackernoon.com/tagged/machine-learning?source=post
  29. https://hackernoon.com/tagged/capsule-networks?source=post
  30. https://hackernoon.com/tagged/artificial-intelligence?source=post
  31. https://hackernoon.com/tagged/graphics?source=post
  32. https://hackernoon.com/tagged/towards-data-science?source=post
  33. https://hackernoon.com/@tanaykothari?source=footer_card
  34. https://hackernoon.com/@tanaykothari
  35. https://hackernoon.com/?source=footer_card
  36. https://hackernoon.com/?source=footer_card
  37. https://hackernoon.com/
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/p/7412d121798d/share/twitter
  41. https://medium.com/p/7412d121798d/share/facebook
  42. https://medium.com/p/7412d121798d/share/twitter
  43. https://medium.com/p/7412d121798d/share/facebook
