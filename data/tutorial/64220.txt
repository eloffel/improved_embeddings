   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

game of thrones id27s, does r+l = j ?         part 1

   [16]go to the profile of jc testud
   [17]jc testud (button) blockedunblock (button) followfollowing
   oct 11, 2017

     can we learn something from the word vector representations from the
     5 game of thrones books?

   [1*j-zdkqdkmkh7xvke0mckuq.png]
   i know this is stupid, please read on :)

   spoiler warning: this story contains spoilers for game of thrones
   season 7

   after my first story on [18]text generation, i decided to learn and
   write about id27s. id27s were the [19]the hot new
   thing in 2013 (which is now prehistoric times). now, let   s apply this
   data science breakthrough to something stupid!
     __________________________________________________________________

     this is a 2-part story:
     - part 1: id27s? what? how and why?
     - part 2: [20]fun with game of thrones embeddings
     (code available [21]here)

     in this first part, we will start with a quick refresher on
     character and word encoding for machine learning    if you are
     familiar with nlp, you can go directly to [22]part 2.
     __________________________________________________________________

character encoding

   machine learning algorithms (and math in general) works with numbers.
   in natural language processing (nlp), we have to represent characters
   and words with numbers, and more precisely vectors (specific
   coordinates in space).

   when working at the character-level (let   s say on the lowercase
   alphabet), we have to encode 26 characters. in this scenario, the
      vocabulary    is a list of 26 different    tokens    (   a    to    z   ). since 26
   is small and manageable, one simple way is to give each character its
   own dimension in space. one character becomes a vector with 1 in the
   corresponding character coordinate and zeros elsewhere:
   [1*kyvp4jvwmznon9s7i5od7g.png]
   lowercase alphabet one-hot encoding

   in this space, from a geometrical point of view, all characters are at
   the same distance from each other. we are letting our algorithm know
   that an    a    has nothing to do with a    b   , and more generally that all
   characters are equally different. this way of representing a vocabulary
   (and categorical variables in general) is often called    one-hot
   encoding    (only one coordinate in space being    hot   ).

   we could also encode everything in one dimension with, for example a =
   (1), b = (2), all the way to z = (26). but in this space    z    would be
   very far from    a   , and    z    being 26 times    a   , the algorithm could
   think that a    z    is just an    a    on steroids, which is not true. it
   could work but it is not a very wise representation.
     __________________________________________________________________

word encoding

   back to our main task, in our case, we want to encode words. the game
   of thrones books contain 12 000 unique words. each of these words must
   be given a vector representation in space. if we    one-hot encode    this
   vocabulary, we will get vectors in a 12 000-dimensional space. you can
   see that it quickly gets out of hand. as a consequence, words are often
   encoded in a fixed lower-dimensional subspace.

   in addition, this time, our tokens (words) are not 100% independent,
   some words are semantically very similar. for example,    stupid    and
      dumb    mean more or less the same thing. by one-hot encoding them, we
   would lose that information. since many words in english share semantic
   similarities, one interesting way to represent the vocabulary is to
   create similar vectors for similar words.

   when words are represented in a dense vector space with a task in mind
   (like keeping semantically-closed word close to each other), the
   process (and the resulting vectors) are often called    id27s   .
     __________________________________________________________________

how to build id27s

   there are two well known algorithms to build    universal   
   (multi-purpose) embeddings, google   s [23]id97 (2013) and stanford   s
   [24]glove (2014).

   both approaches quantify similarity between two words by how much they
   co-occur (the intellectual term is the [25]distributional hypothesis).
   even though the process by which the embedding vector space is created
   is different, they produce similar vectors. here are the simplified
   steps for each algorithm :
     * id97 (skip-gram mode) uses a shallow feed-forward neural
       network (1 hidden layer). for a given word in input, the network is
       trained to predict the id203 for each word of the vocabulary
       to appear alongside our input word. to get better at this task, the
       network has only one weight matrix to adjust, the one from a dense
       intermediate hidden layer. once trained, this hidden layer is the
       vector representation of our words.
     * glove is more traditional (no neural network involved) and uses
       id105. it starts by going through the text and
       counting the number of times word couples are seen close to each
       other (in a given window, like 10 words). this information is
       stored in a structure called a    co-occurrence matrix   . words
       vectors are built and adjusted iteratively, to minimize the
       (cosine) distance between words having a high id203 of
       co-occurrence.
     __________________________________________________________________

id27s properties

   one particular aspect of id27s that blew everyone   s mind in
   2013, is the fact that the resulting vector space has a lot of
   unexpected properties. it accidentally encodes several high-level
   semantic concepts. for example, pronouns like    he    and    she    are very
   closed to each other (as expected), but at the same time, the path from
      he    to    she    (another vector) has amazing properties.

   by applying the same    he-to-she    transformation to something else, we
   are actually swapping gender.    man    becomes    woman    and    actor    becomes
      actress   . to illustrate this, here is a glimpse at [26]part 2. below,
   a visualization of word vectors built from game of thrones books
   (only):
   [1*x5bibl-0bhqiymr5_tqciq.png]
   illustration of the    gender-swap    relationship (pca projection of
   selected game of thrones word vectors)

   in a completely unsupervised fashion, an algorithm has encoded the
   concept of    gender    (without naming it), how is this possible?

   think of it like a space where words are attracted to each other for
   different reasons. let   s take this short text as an example:

        ned, catelyn, bran, and sansa are from house stark.
     robert, cersei, joffrey, and myrcella are from house baratheon.
     catelyn, sansa, cersei and myrcella are women.
     ned, bran, robert and joffrey are men.   

   with this text, we expect people from the same family to be attracted
   to each other, and also people from the same sex.
   [1*eapthymy22iw4ojnuq84jw.png]
   truncated word co-occurrence matrix

   on the left, a truncated co-occurrence matrix with only some specific
   target words (rows) and context words (columns).
   [1*kmku91bvrdazhk3u-txdsq.png]
   2-d representation of our words

   with a direct 2-d approximation (pca factorization) of the
   co-occurrence matrix, we get the following word representations (this
   operation is obviously different and more complex in glove). in this
   2-d space, four    forces    are at work. men are pulling other men towards
   them, and, at the same time, women, starks, and baratheons do the same.

   it reaches an equilibrium where baratheons end up at the bottom, starks
   on top, women on the left and men on the right, all of it without any
   guidance.

   embeddings capture a lot of these semantic relationships (or
      linguistic regularities    like some are calling them). they can be used
   to solve word analogy tasks like:

        a    is similar to    b    as    c    is similar to    d   

   example with one of the four terms missing:

        he    is to    she    as    man    is to <?>

   this is called    analogical reasoning   , the task was introduced by
   id97 authors tomas mikolov et al. who built a complete [27]dataset
   containing a lot of these questions.

     in vector arithmetic, the analogy    a is to b as c is to d   
     translates to:
     b - a ~ d - c
     b - a + c ~ d
        she    -    he    +    man    ~ woman

   our 4-sentence word space can already solve some things like:

        robert    is to    baratheons    what    ned    is to <?>
     equivalent to   
     what is    baratheons    minus    robert    plus    ned   ?

   in our space, it gives us a vector that is more or less    starks   
   (tada!).

   you can see that a lot is already going on in our short text example,
   imagine now in 5 books, or in wikipedia, and also with more dimensions
   (hundreds) to let the algorithm some space to express itself.

   ideally, to    learn    the english language, you have to use a massive
   corpus. the stanford   s group (as well as google) provide pre-trained
   id27s where the corpus contained more than 10 billion words
   (usually from wikipedia). in our case, we only have 10 million words,
   and they come from a medieval fantasy book (so please be indulgent).

   you are now ready for [28]part 2!
     __________________________________________________________________

bonus: is this unsupervised learning?

   id97 is often considered a highly successful    unsupervised
   learning    algorithm. one could rightfully say that, since neural
   networks are trained with input/output pairs to minimize a loss
   function, training them is always a supervised task in a way.

   however, there is definitely a distinction between    manually-labelled   
   and    automatically-labelled    datasets. the way the network gets its
   expected output (also called label/target) is completely different:
     *    manually-labelled    example: a network is trained to classify input
       images as containing a    dog    or a    cat   . to train such a
       classifier, one has to manually annotate images of cats and dogs to
       build a dataset.
     *    automatically-labelled    example: in id97, there is a clever
       autonomous way to generate the labels for any text. as a
       consequence, you can use any dataset you want, and no human
       interaction is needed (some would call this    self-supervised   ). it
       is relatively similar for time series forecasting or auto-encoders.
     __________________________________________________________________

   that   s it for part 1. if you liked it, you can follow me on [29]medium
   or [30]twitter to get the latest news. also, if you see something wrong
   or if you have any question, feel free to ask in the comment section.

     * [31]id97
     * [32]embedding
     * [33]deep learning
     * [34]game of thrones
     * [35]towards data science

   (button)
   (button)
   (button) 169 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of jc testud

[37]jc testud

   [38]https://twitter.com/jctestud

     (button) follow
   [39]towards data science

[40]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 169
     * (button)
     *
     *

   [41]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [42]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8ca70a8f1fad
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ohqufhkhacr5---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@jctestud?source=post_header_lockup
  17. https://towardsdatascience.com/@jctestud
  18. https://medium.com/@jctestud/yet-another-text-generation-project-5cfb59b26255
  19. https://arxiv.org/pdf/1301.3781.pdf
  20. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b
  21. https://github.com/jctestud/got-word-embeddings
  22. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b
  23. https://arxiv.org/abs/1301.3781
  24. https://nlp.stanford.edu/pubs/glove.pdf
  25. https://en.wikipedia.org/wiki/distributional_semantics#distributional_hypothesis
  26. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b
  27. http://download.tensorflow.org/data/questions-words.txt
  28. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b
  29. https://medium.com/@jctestud
  30. https://twitter.com/jctestud
  31. https://towardsdatascience.com/tagged/id97?source=post
  32. https://towardsdatascience.com/tagged/embedding?source=post
  33. https://towardsdatascience.com/tagged/deep-learning?source=post
  34. https://towardsdatascience.com/tagged/game-of-thrones?source=post
  35. https://towardsdatascience.com/tagged/towards-data-science?source=post
  36. https://towardsdatascience.com/@jctestud?source=footer_card
  37. https://towardsdatascience.com/@jctestud
  38. https://twitter.com/jctestud
  39. https://towardsdatascience.com/?source=footer_card
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/8ca70a8f1fad/share/twitter
  45. https://medium.com/p/8ca70a8f1fad/share/facebook
  46. https://medium.com/p/8ca70a8f1fad/share/twitter
  47. https://medium.com/p/8ca70a8f1fad/share/facebook
