   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

game of thrones id27s, does r+l = j ?         part 2

   [16]go to the profile of jc testud
   [17]jc testud (button) blockedunblock (button) followfollowing
   oct 11, 2017

     can we learn something from the word vector representations from the
     5 game of thrones books?

   [1*vw5vwfnqwphnx9iyrusbvg.png]

   now, it gets serious. let   s build our vectors and question this space   

     reminder, this is a 2-part story:
     part 1: [18]id27s? what? how and why?
     part 2: fun with game of thrones embeddings
     __________________________________________________________________

building game of throne embeddings

     note: everything i tried can be found [19]in this repository along
     with the trained vectors. it is really easy to jump in and try new
     things (just some pandas, numpy, and matplotlib in notebooks).
     please tell me if you find something cool!

   everything will be based on a 100-d glove word vector representation
   from the first 5 books of game of thrones. this corpus being extremely
   small, it took glove just one hour to stabilize. we now have 12000
   words represented by 100-d dense vectors:
   [1*ek301fupyh3yvwpd9cml9g.png]
   our 100-d got vectors in a dataframe
     __________________________________________________________________

quick evaluation of our vectors

   glove is built so that the vectors for similar words get a similar
   direction in space (it tries to maximize their dot product). one way to
   measure a direction similarity is through cosine similarity (~angle
   between the vectors). if vectors have the same amplitude, dot product
   and cosine similarity are equivalent. so to make everything a dot
   product, the first step is to normalize the word vectors.

   we can now find similar words to a known word, or any vector we may
   produce, like the ones that answer analogy questions. to check whether
   or not, creating id27s on this unusual dataset is completely
   useless, let   s see how our vectors fare on the google word analogy
   dataset we mentioned earlier ([20]part 1). it is a set of 19000+ word
   analogies. here are the results:
questions: 4688 filtered out of 19558 (missing words)
accuracy = 9.94% (466/4688)
second-guess accuracy = 14.04% (658/4688)

   a lot of analogies were not testable since some words are not in the
   vocabulary (there is no mention of paris or berlin in the books, go
   figure). for the rest, our vectors achieve almost 10% accuracy. it is
   obviously very bad but it shows also that this vector space is not
   useless. below, some of the    solved    analogies:
'man' is to 'woman' as 'husband' is to 'wife'
'boy' is to 'girl' as 'uncle' is to 'aunt'
'falling' is to 'fell' as 'going' is to 'went'
     __________________________________________________________________

     general note: i am going to cherry-pick a lot. the examples below
     (that are already not particularly good) are among the best i have
     found while exploring the word vectors.

visualizing game of thrones word vectors

   let   s start by something very visual. we can select some words and
   project the corresponding 100-d vectors to a friendlier 2-d space for
   plotting. let   s do it on the main characters of the show.

     note: i did not considered full names as words, from the algorithm
     point of view, jon snow is a 2-word sequence (   jon    and    snow   ). one
     way to change this behavior is to replace each occurrence of    jon
     snow    by    jon_snow    in our text. in the plot above, family names
     were added manually. so jon snow   s vector has probably drifted in
     space because of jon arryn   

   [1*30tdjpd_dcynlc089as5fg.png]
   2-d id167 projection of the main characters (colored by house)

   the vectors are clearly not random. we can see clusters for characters
   of he same family. we also see couples close to each other and heroes
   on top (yes there are heroes in got).
     __________________________________________________________________

   we can also visualize 2-d projection of words that should form pairs
   (like we did in [21]part 1). below, an illustration of the relation
   between a got house and its seat (main castle usually):
   [1*4osuxr7c7rfcs9pmrr-urq.png]
   game of thrones house-to-seat regularities
   [1*jewl68lnhbrwdtfbvb5aig.png]
   extract from my notebook
   [1*q6qipue7atkjqgt9x3uvaw.png]
   bonus: adjective         comparative         superlative regularities
     __________________________________________________________________

   there is also another cool 2-d visualization of word vectors we can
   try. i have only seen it in the [22]word2viz project, i don   t know if
   it has a name. let   s call it the    word2viz    plot. it works this way:
    1. you create 2 interesting word vector differences like the path from
          woman-to-man    and the one from    evil-to-good   
    2. you select some words (let   s take the main characters of got)
    3. for each word, you compute the dot product between the word and
       these 2 special vectors
    4. the two values you get per word become the two coordinates in a 2-d
       plot

     mathematically, for a given word vector w:
     w.(man-woman) = w.man - w.woman
     ~ similarity to    man    - similarity to    woman   

     this scalar value is intuitively    how to place a word on a scale of
     woman to man   . this gives us our x coordinate. if we use our second
        concept    for y, we will have a custom 2-d space where words are
     spread out along two differentiation axis.

   this gives us:
   [1*njgknrbu9k5fain4-xsjbq.png]
   the evil-good axis is not working very well (cersei and joffrey #?!)
     __________________________________________________________________

   [1*s7hf0d2iyvk9cxckt53isg.png]
   westeros places through a 2-d projection on (east-west) and
   (north-south)

   another example   

   i took some important places from the world of game of thrones and
   projected the corresponding vectors on a north-south, and a east-west
      word vector scale   .

   and surprisingly, the results are not completely incoherent.

   image credits to reddit user selvag (complete map [23]available here)
     __________________________________________________________________

finding jon   s parents

   [1*nidq9kvyqmgju6njnddk9q.png]
   last known sighting of jon snow

spoiler recap:

   [24]r + l = j was for several years the non-spoilery way to discuss the
   fact that rhaegar targaryen (daenerys    brother) and lyanna stark (ned   s
   sister) could be jon snow   s parents. this theory was confirmed during
   season 6 and 7 of the tv show. we even know his real name now! he is
   aegon targaryen, rightful heir to the iron throne, and future first
   cousin of his own child!#mindblown

   it is quite obvious that the actual r+l=j vector equation won   t hold.
   for a start, because it is not a good way to ask a question to word
   embeddings. it does not leverage any    regularities   .

   we may choose a more clever way to ask the question, like through a
      father-to-son    regularity, and try to go from rhaegar to jon by using,
   for example, the path from ned to robb .
   [1*a1nda5x75q10qx0ssvuyjw.png]
   more clever ways to find family relations

   but, even then, it would not work either because the basis material
   (~word sequences that would put jon in the vicinity of his father while
   talking about parenthood) is not there in the books. if millions of
   reader could not find it, an algorithm with less deductive capability
   will not do better.
   [1*qjenuce0lw3rxnelo7lakg.png]

   the only way for it to work would be if george r. r. martin had built
   its 5 books (and the corresponding co-occurrence matrix) knowing
   someone will ask this stupid question on a 100d-trained glove
   embeddings. i tried and i can confirm he did not.
     __________________________________________________________________

   that   s it for part 2. if you liked it, you can follow me on [25]medium
   or [26]twitter to get the latest news. also, if you see something
   wrong, or if you have any question, feel free to comment below.
     __________________________________________________________________

bibliography and recommended reading

     * [27]   deep learning, nlp, and representations   , christopher olah
     * [28]   on id27s   , sebastian ruder
     * [29]   id97 tutorial   , chris mccormick
     * [30]   glove: global vectors for word representation   , jeffrey
       pennington, richard socher, christopher d. manning
     * [31]   word2viz   , julia bazi  ska and piotr migda  

     * [32]game of thrones
     * [33]machine learning
     * [34]id97
     * [35]embedding
     * [36]data science

   (button)
   (button)
   (button) 75 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [37]go to the profile of jc testud

[38]jc testud

   [39]https://twitter.com/jctestud

     (button) follow
   [40]towards data science

[41]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 75
     * (button)
     *
     *

   [42]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [43]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/30290b1c0b4b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_qgcwm9htgrxc---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@jctestud?source=post_header_lockup
  17. https://towardsdatascience.com/@jctestud
  18. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad
  19. https://github.com/jctestud/got-word-embeddings
  20. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad
  21. https://medium.com/@jctestud/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad
  22. https://github.com/lamyiowce/word2viz
  23. https://www.reddit.com/r/gameofthrones/comments/34tkek/no_spoilers_i_made_a_google_maps_version_of/
  24. http://knowyourmeme.com/memes/rlj
  25. https://medium.com/@jctestud
  26. https://twitter.com/jctestud
  27. http://colah.github.io/posts/2014-07-nlp-id56s-representations/
  28. http://ruder.io/word-embeddings-1/index.html
  29. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  30. https://nlp.stanford.edu/projects/glove/
  31. https://github.com/lamyiowce/word2viz
  32. https://towardsdatascience.com/tagged/game-of-thrones?source=post
  33. https://towardsdatascience.com/tagged/machine-learning?source=post
  34. https://towardsdatascience.com/tagged/id97?source=post
  35. https://towardsdatascience.com/tagged/embedding?source=post
  36. https://towardsdatascience.com/tagged/data-science?source=post
  37. https://towardsdatascience.com/@jctestud?source=footer_card
  38. https://towardsdatascience.com/@jctestud
  39. https://twitter.com/jctestud
  40. https://towardsdatascience.com/?source=footer_card
  41. https://towardsdatascience.com/?source=footer_card
  42. https://towardsdatascience.com/
  43. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  45. https://medium.com/p/30290b1c0b4b/share/twitter
  46. https://medium.com/p/30290b1c0b4b/share/facebook
  47. https://medium.com/p/30290b1c0b4b/share/twitter
  48. https://medium.com/p/30290b1c0b4b/share/facebook
