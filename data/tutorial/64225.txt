   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

character-level language model

   [16]go to the profile of imad dabbura
   [17]imad dabbura (button) blockedunblock (button) followfollowing
   feb 21, 2018
   [1*sjnbnoox7sx1cl5bewljgq.jpeg]
   iphone   s text suggestion

introduction

   have you ever wondered how gmail automatic reply works? or how your
   phone suggests next word when texting? or even how a neural network can
   generate musical notes? the general way of generating a sequence of
   text is to train a model to predict the next word/character given all
   previous words/characters. such model is called a statistical language
   model. what is a statistical language model? a statistical language
   model tries to capture the statistical structure (latent space) of
   training text it   s trained on. usually recurrent neural network (id56)
   models family are used to train the model due to the fact that they are
   very powerful and expressive in which they remember and process past
   information through their high dimensional hidden state units. the main
   goal of any language model is to learn the joint id203
   distribution of sequences of characters/words in a training text, i.e.
   trying to learn the joint id203 function. for example, if we   re
   trying to predict a sequence of t words, we try to get the joint
   id203 p(w_1,w_2,    ,w_t) as big as we can which is equal to the
   product of all conditional probabilities     p(w_t/w_{t-1}) at all time
   steps (t) .

   in this post, we   ll cover the character-level language model where
   almost all the concepts hold for any other language models such as
   word-language models. the main task of the character-level language
   model is to predict the next character given all previous characters in
   a sequence of data, i.e. generates text character by character. more
   formally, given a training sequence (x  ,     , x^t), the id56 uses the
   sequence of its output vectors (o  ,     , o^t) to obtain a sequence of
   predictive distributions p(x^t/x^{t-1}) = softmax(o^t).

   let   s illustrate how the character-level language model works using my
   first name (   imad   ) as an example (see figure 1 for all the details of
   this example).
    1. we first build a vocabulary dictionary using all the unique letters
       of the names in the corpus as keys and the index of each letter
       starting from zero (since python is a zero-indexed language) in an
       ascending order. for our example, the vocabulary dictionary would
       be: {   a   : 0,    d   : 1,    i   : 2,    m   : 3}. therefore,    imad    would
       become a list of the following integers: [2, 3, 0, 1].
    2. convert the input and the output characters to lists of integers
       using the vocabulary dictionary. in this post, we   ll assume that x  
       = \vec{0} for all examples. therefore, y =    imad    and x = \vec{0}\
       +    ima   . in other words, x^{t + 1} = y^t which gives us: y = [2, 3,
       0, 1] and x = [\vec{0}, 2, 3, 0].
    3. for each character in the input:
    4. convert the input characters into one-hot vectors. notice how the
       first character is: zero vector
    5. compute the hidden state layer.
    6. compute the output layer and then pass it through softmax to get
       the results as probabilities.
    7. feed the target character at time step (t) as the input character
       at time step (t + 1).
    8. go back to step a and repeat until we finish all the letters in the
       name.

   the objective is to make the green numbers as big as we can and the red
   numbers as small as we can in the id203 distribution layer. the
   reason is that the true index should have the highest id203 by
   making it as close as we can to 1. the way to do that is to measure the
   loss using cross-id178 and then compute the gradients of the loss
   w.r.t. all parameters to update them in the opposite of the gradient
   direction. repeating the process over many times where each time we
   adjust the parameters based on the gradient direction    > model will be
   able to correctly predict next characters given all previous ones using
   all names in the training text. notice that hidden state $h   $ has all
   past information about all characters.
   [0*9lxr_80gisfc-a9n.png]
   figure 1: illustrative example of character-level language model
   using id56

   note: to shorten the length of the post, i deleted all the docstrings
   of python functions and i didn   t include some functions that i didn   t
   think are necessary to understand the main concepts. the notebook and
   the script that created this post can be found [18]here and [19]here.

training

   the [20]dataset we   ll be using has 5,163 names: 4,275 male names, 1,219
   female names, and 331 names that can be both female and male names. the
   id56 architecture we   ll be using to train the character-level language
   model is called many to many where time steps of the input t_x= time
   steps of the output t_y. in other words, the sequence of the input and
   output are synced (see figure 2).
   [0*ddoz-c1havpz4uqw.png]
   figure 2: id56 architecture: many to many

   the character-level language model will be trained on names; which
   means after we   re done with training the model, we   ll be able to
   generate some interesting names :).

   in this section, we   ll go over four main parts:
    1. forward propagation.
    2. id26.
    3. sampling.
    4. fitting the model.

forward propagation

   we   ll be using stochastic id119 (sgd) where each batch
   consists of only one example. in other words, the id56 model will learn
   from each example (name) separately, i.e. run both forward and backward
   passes on each example and update parameters accordingly. below are all
   the steps needed for a forward pass:
     * create a vocabulary dictionary using the unique lower case letters.
     * create a character to index dictionary that maps each character to
       its corresponding index in an ascending order. for example,    a   
       would have index 1 (since python is a zero index language and we   ll
       reserve 0 index to eos    \n   ) and    z    would have index 26. we will
       use this dictionary in converting names into lists of integers
       where each letter will be represented as one-hot vector.
     * create an index to character dictionary that maps indices to
       characters. this dictionary will be used to convert the output of
       the id56 model into characters which will be translated into names.
     * initialize parameters: weights will be initialized to small random
       numbers from standard normal distribution to break symmetry and
       make sure different hidden units learn different things. on the
       other hand, biases will be initialized to zeros.
     * w_hh: weight matrix connecting previous hidden state h^{t         1} to
       current hidden state h^t.
     * w_xh: weight matrix connecting input x^t to hidden state h^t.
     * b: hidden state bias vector.
     * w_hy: weight matrix connecting hidden state h^t to output o^t.
     * c: output bias vector.
     * convert input x^t and output y^t into one-hot vector each. the
       dimension of the one-hot vector is vocab_size x 1. everything will
       be zero except for the index of the letter at (t) would be 1. in
       our case, x^t would be the same as y^t shifted to the left where x  
       = \vec{0}; however, starting from t = 2, x^{t + 1} = y^t. for
       example, if we use    imad    as the input, then y = [3, 4, 1, 2, 0]
       while x = [\vec{0}, 3, 4, 1, 2]. notice that x   = \vec{0} and not
       the index 0. moreover, we   re using    \n    as eos (end of
       sentence/name) for each name so that the id56 learns    \n    as any
       other character. this will help the network learn when to to stop
       generating characters. therefore, the last target character for all
       names will be    \n    that represents the end of the name.
     * compute the hidden state using the following formula:

   [1*vgxivhxoelp5mtikq21htg.png]

   notice that we use hyperbolic tangent as the non-linear function. one
   of the main advantages of the hyperbolic tangent function is that it
   resembles the identity function.
     * compute the output layer using the following formula:

   [1*4yjcpoagiscnxjer5perqq.png]
     * pass the output through softmax layer to normalize the output that
       allows us to express it as a id203, i.e. all output will be
       between 0 and 1 and sum up to 1. below is the softmax formula:

   [1*3jnhr9bqykkwfpe8cp-xfa.png]

   the softmax layer has the same dimension as the output layer which is
   vocab_size x 1. as a result, y^t[i] is the id203 of index i being
   the next character at time step (t).
     * as mentioned before, the objective of a character-level language
       model is to minimize the negative log-likelihood of the training
       sequence. therefore, the id168 at time step (t) and the
       total loss across all time steps are:

   [1*3r0ryi6vopgrtaqyjs7r1g.png]

   since we   ll be using sgd, the loss will be noisy and have many
   oscillations, so it   s a good practice to smooth out the loss using
   exponential weighted average.
     * pass the target character y^t as the next input x^{t + 1} until we
       finish the sequence.

   iframe: [21]/media/7342e20dec97f04a0d5d28c11cc468ed?postid=1439f5dd87fe

id26

   with id56 based models, the gradient-based technique that will be used
   is called id26 through time (bptt). we start at last time
   step $t$ and backpropagate id168 w.r.t. all parameters across
   all time steps and sum them up (see figure 3).
   [0*np68e5zskzkrii4r.png]
   figure 3: id26 through time (bptt)

   in addition, since id56s are known to have steep cliffs (sudden steep
   decrease in l, gradients may overshoot the minimum and undo a lot of
   the work that was done even if we are using adaptive learning methods
   such as rmsprop. the reason is because gradient is a linear
   approximation of the id168 and may not capture information
   further than the point it was evaluated on such as the curvature of
   loss curve. therefore, it   s a common practice to clip the gradients to
   be in the interval [-maxvalue, maxvalue]. for this exercise, we   ll clip
   the gradients to be in the interval [-5, 5]. that means if the gradient
   is > 5 or < -5, it would be clipped to 5 and -5 respectively. below are
   all the formulas needed to compute the gradients w.r.t. all parameters
   at all time steps.
   [1*z7okk9gaq4gpvzeyepipba.png]

   note that at last time step t, we   ll initialize dh_next to zeros since
   we can   t get values from future. to stabilize the update at each time
   step since sgd may have so many oscillations, we   ll be using one of the
   adaptive learning method optimizers. more specifically, we   ll use
   [22]root mean squared propagation (rmsprop) which tends to have
   acceptable performance.

   iframe: [23]/media/48b0d05cc8a4385760bdc7e1b596caf8?postid=1439f5dd87fe

sampling

   sampling is what makes the text generated by the id56 at each time step
   an interesting/creative text. on each time step (t), the id56 output the
   id155 distribution of the next character given all
   the previous characters, i.e. p(c_t/c_1, c_2,    , c_{t-1}). let   s assume
   that we are at time step t = 3 and we   re trying to predict the third
   character, the id155 distribution is: p(c_3/c_1, c_2)
   = (0.2, 0.3, 0.4, 0.1). we   ll have two extremes:
    1. maximum id178: the character will be picked randomly using
       uniform id203 distribution; which means that all characters
       in the vocabulary dictionary are equally likely. therefore, we   ll
       end up with maximum randomness in picking the next character and
       the generated text will not be either meaningful or sound real.
    2. minimum id178: the character with the highest conditional
       id203 will be picked on each time step. that means next
       character will be what the model estimates to be the right one
       based on the training text and learned parameters. as a result, the
       names generated will be both meaningful and sound real. however, it
       will also be repetitive and not as interesting since all the
       parameters were optimized to learn joint id203 distribution
       in predicting the next character.

   as we increase randomness, text will lose local structure; however, as
   we decrease randomness, the generated text will sound more real and
   start to preserve its local structure. for this exercise, we will
   sample from the distribution that   s generated by the model which can be
   seen as an intermediate level of randomness between maximum and minimum
   id178 (see figure 4). using this sampling strategy on the above
   distribution, the index 0 has 20% id203 of being picked, while
   index 2 has 40% id203 to be picked.
   [0*tpdyjf-hs8awad6j.png]
   figure 4: sampling: an example of predicting next character using
   character-level language model

   therefore, sampling will be used at test time to generate names
   character by character.

   iframe: [24]/media/9fdacc8a5f83c79fb7b13c49389772ac?postid=1439f5dd87fe

fitting the model

   after covering all the concepts/intuitions behind character-level
   language model, now we   re ready to fit the model. we   ll use the default
   settings for rmsprop   s hyperparameters and run the model for 100
   iterations. on each iteration, we   ll print out one sampled name and
   smoothed loss to see how the names generated start to get more
   interesting with more iterations as well as the loss will start
   decreasing. when done with fitting the model, we   ll plot the loss
   function and generate some names.

   iframe: [25]/media/b5c83f3e31c6246feec40bf8229f5a03?postid=1439f5dd87fe

   iframe: [26]/media/39043304df065787587db22e77a172e4?postid=1439f5dd87fe

   below is few of output generated during training:
there are 36121 characters and 27 unique characters.
epoch 0
=======
sampled name: nijqikkgzst
smoothed loss: 23.0709
epoch 10
=======
sampled name: milton
smoothed loss: 14.7446
epoch 30
=======
sampled name: dangelyn
smoothed loss: 13.8179
epoch 70
=======
sampled name: lacira
smoothed loss: 13.3782
epoch 99
=======
sampled name: cathranda
smoothed loss: 13.3380

   [1*j42rzod6mo5d4zdv5lxitw.png]
   figure 6: smoothed loss

   the names that were generated started to get more interesting after 15
   epochs. i didn   t include the results of all epochs to shorten the post;
   however, you can check the results in the [27]notebook associated with
   this post. one of the interesting names is    yasira    which is an arabic
   name :).

conclusion

   statistical language models are very crucial in natural language
   processing (nlp) such as id103 and machine translation. we
   demonstrated in this post the main concepts behind statistical language
   models using character-level language model. the task of this model is
   to generate names character by character using names obtained from
   census data that were consisted of 5,163 names. below are the main key
   takeaways:
     * if we have more data, a bigger model, and train longer, we may get
       more interesting results. however, to get a very interesting
       results, we should instead use long short-term memory (lstm) model
       with more than one layer deep. people have used 3 layers deep lstm
       model with dropout and were able to generate very interesting
       results when applied on cookbooks and shakespeare poems. lstm
       models outperform simple id56 due to its ability in capturing longer
       time dependencies.
     * with the sampling technique we   re using, don   t expect the id56 to
       generate meaningful sequence of characters (names).
     * we used in this post each name as its own sequence; however, we may
       be able to speed up learning and get better results if we increase
       the batch size; let   s say from one name to a sequence of 50
       characters.
     * we can control the level of randomness using the sampling strategy.
       here, we balanced between what the model thinks it   s the right
       character and the level of randomness.
     __________________________________________________________________

   originally published at [28]imaddabbura.github.io on february 22, 2018.

     * [29]machine learning
     * [30]artificial intelligence
     * [31]deep learning

   (button)
   (button)
   (button) 105 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of imad dabbura

[33]imad dabbura

   data scientist

     (button) follow
   [34]towards data science

[35]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 105
     * (button)
     *
     *

   [36]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [37]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1439f5dd87fe
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/character-level-language-model-1439f5dd87fe&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/character-level-language-model-1439f5dd87fe&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_qowegbxqhytk---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@imadphd?source=post_header_lockup
  17. https://towardsdatascience.com/@imadphd
  18. https://nbviewer.jupyter.org/github/imaddabbura/blog-posts/blob/master/notebooks/character-level-language-model.ipynb
  19. https://github.com/imaddabbura/blog-posts/blob/master/scripts/character_level_language_model.py
  20. http://deron.meranda.us/data/census-derived-all-first.txt
  21. https://towardsdatascience.com/media/7342e20dec97f04a0d5d28c11cc468ed?postid=1439f5dd87fe
  22. https://nbviewer.jupyter.org/github/imaddabbura/deep-learning/blob/master/notebooks/optimization-algorithms.ipynb
  23. https://towardsdatascience.com/media/48b0d05cc8a4385760bdc7e1b596caf8?postid=1439f5dd87fe
  24. https://towardsdatascience.com/media/9fdacc8a5f83c79fb7b13c49389772ac?postid=1439f5dd87fe
  25. https://towardsdatascience.com/media/b5c83f3e31c6246feec40bf8229f5a03?postid=1439f5dd87fe
  26. https://towardsdatascience.com/media/39043304df065787587db22e77a172e4?postid=1439f5dd87fe
  27. https://nbviewer.jupyter.org/github/imaddabbura/blog-posts/blob/master/notebooks/character-level-language-model.ipynb
  28. https://imaddabbura.github.io/post/character_level_language_model/
  29. https://towardsdatascience.com/tagged/machine-learning?source=post
  30. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  31. https://towardsdatascience.com/tagged/deep-learning?source=post
  32. https://towardsdatascience.com/@imadphd?source=footer_card
  33. https://towardsdatascience.com/@imadphd
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/1439f5dd87fe/share/twitter
  40. https://medium.com/p/1439f5dd87fe/share/facebook
  41. https://medium.com/p/1439f5dd87fe/share/twitter
  42. https://medium.com/p/1439f5dd87fe/share/facebook
