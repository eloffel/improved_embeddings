   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning from scratch: part 4

functions and classification

   [16]go to the profile of sebastian kwiatkowski
   [17]sebastian kwiatkowski (button) blockedunblock (button)
   followfollowing
   mar 15, 2018

table of contents

     * [18]part 1: attributes and patterns
     * [19]part 2: collections and data
     * [20]part 3: arrays and representations
     * part 4: functions and classification
     __________________________________________________________________

   changelog: i have added a few sentences to the discussion of sets in
   part 2.

   the revised text mentions that sets are denoted with capital letters,
   especially s, while lowercase letters are used for set members. for
   example, we can refer to the collection of basic ice cream flavors, {
   vanilla, strawberry and chocolate } as s.

   additionally, the updated section introduces notation for membership in
   a set and the size of a set. the fact that an element x belongs to a
   set s is written as x     s. for example, we have: vanilla    s. vertical
   bars around the name of a set denote its size. for the set of basic
   flavors, we have |s| = 3.
     __________________________________________________________________

functions

   functions are at the core of machine learning.

   at the beginning of a project, the business objectives are specified in
   terms of functions.

   during the project, functions are used at just about every step to
   process the data, to discover patterns and to evaluate the performance
   of the system.

   at the end of the project, what is ultimately delivered to the client,
   is another function. the deliverable may be coded in a programming
   language or take the form of a composition of several lower-level
   functions, but it is a function nonetheless.

functions map inputs to outputs

   many readers will be familiar with functions from mathematics and/or
   programming.

   mathematically, a function specifies how elements in one set are
   related to elements of another set.

   programmatically, a function processes an input to generate an output.

   the following graphic combines aspects from both definitions:
   [0*da2cwryrbl8njotg.]
   fig. 4.1: edited version of a graphic released into the public domain
   by [21]wvbailey

   these two definitions are consistent with each other.[1] the acceptable
   input to a function belongs to one set and the output that the function
   generates belongs to another another set. functions define how we get
   from one to the other.

   and that   s all there is to know about the essence of functions.

   as a preview of what is to come, i will mention that the input to the
   functions at the center of machine learning are the representations of
   objects that were discussed last time (e.g., pixel intensities and the
   presence or absence of certain words). the output of functions are the
   predicted values for the targets that we are interested in.

notation

   to define a function mathematically, we need to specify two things:
    1. the sets that are involved
    2. the rule by which the elements from one set are mapped on to
       elements of the other set.

sets in functions

   the fact that a function f maps elements from set a to elements in set
   b is written as f: a     b.

   the two sets, a and b, are called domain and co-domain, respectively.
   they can and, in many cases, do refer to the same set.

   the letters used in this notation (f, a and b) can be understood as
   placeholders. we will often use different names.

mapping

   the input to a function f is usually denoted with the letter x. in the
   context of machine learning, x usually refers to the array that
   represents an object.

   the letter y designates the output of a function. we will use the
   letter y to denote predictions.

   combining these building blocks, the following equation simply states
   that y is the output of the application of function f to the input x:
   f(x) = y.

function names

   in many cases, we will need to refer to more than one function. to keep
   them apart, one of the following naming strategies can be employed:
     * other letters in the alphabet, especially g and h
     * subscripts (or superscripts), such as f_1, f_2, etc.
     * words or abbreviations of words: e.g., increment or inc

a simple example

   consider the self-descriptive example of a function named
   fahrenheit_to_celsius.

   given that temperatures of trillions of degrees of fahrenheit have been
   achieved in the laboratory[2], i think it   s fair to use the set of real
   numbers for this function. at the level of sets, we can write:
   [0*8wsl5hnham28hgan.png]

   at the level of individual elements, we have:
   [0*nvvxbsnldcupb6li.png]

   using a style that emphasizes the equivalence between mathematics and
   programming, this function corresponds to the following python code[3]:

   iframe: [22]/media/e1261a3fc86d80fbf9def06d627d7c6f?postid=10117c005a28
     __________________________________________________________________

real-valued functions

   for our purposes, a scalar is a real number[4] or, put differently, a
   member of the set of real numbers    . alternatively, we can think of a
   scalar as a vector with one entry or a matrix with one row and one
   column.

   a real-valued vector consists of d real numbers and belongs to the set
      ^d. analogously, a real-valued matrix has m x n real entries and
   belongs to    ^(m x n). and, finally, a real-valued 3d array belongs to
   r^(m x n x o)[5].
   [0*wexerw1dzgg9_l30.]
   fig. 4.2

   all of the arrays and functions used in this series are real-valued.
   this includes the binary vectors mentioned in part 3. of course, 0 and
   1 are real numbers, too. to emphasize that they are real numbers, we
   can write them as 0.0 and 1.0.
     __________________________________________________________________

supervised learning problems

   now that we have introduced real-valued functions, we can start to
   discuss the problems that we can solve using supervised machine
   learning.

   at a high level, there is a distinction between classification and
   regression.
   [0*ohjwisdavaimzwxv.]
   fig. 4.3

classification

   in a classification task, the target we are trying to predict is
   discrete. it can take on one of n values or, phrased differently,
   belongs to one of n classes.

   when the objects of interest are thought of as members of categories
   rather than classes, the term categorization is used instead of
   classification. for example, some authors prefer the term document
   categorization to document classification.

   for n = 2, the task of predicting the correct class is called binary
   classification. the two classes are often referred to as the positive
   class and the negative class.

   the term used for n > 2 is multi-class classification or n-class
   classification.
   [0*9ykjoyogvpaqzhvw.]
   fig. 4

   classifying email as worthy or unworthy of a place in your inbox is a
   binary classification task. assigning one of several possible labels to
   emails from friends, colleagues, services, advertisers and other groups
   involves multiple classes.

   distinguishing between benign and malignant skin lesions is a binary
   classification task. staging a cancer, on the other hand, is a
   multi-class classification problem.

probabilities

   an interval [a, b] is a set of real numbers that includes every number
   between a and b, including the endpoints. the interval [0, 1] contains
   all real number from 0 to 1.

   a id203 p is a member of the interval [0, 1].

   the term counter-id203 refers to 1-p. the id203 and the
   counter-id203, obviously, add up to 1.

   a id203 distribution is a vector whose d entries are
   probabilities that sum up to 1. i will write id203 distributions
   as [0, 1]^d.

   the vector [ 0.2, 0.4, 0.3, 0.1 ] is an example of a id203
   distribution.
   [0*l27gc690uakwkbcs.]
   fig. 5

probabilistic output

   classification output is probabilistic.

   in binary classification, the output is the id203 p that the
   object belongs to the positive class. the counter-id203 1-p is
   the id203 that the object is a member of the negative class.

   high probabilities indicate that the object is likely an instance of
   the positive class.

   an output of zero implies that membership in the positive class is
   impossible and membership in the negative class is certain. conversely,
   an object necessarily belongs to the positive class if the id203
   is one.

   suppose, for example, that the output is p = 0.85. this means that
   there is an 85 percent id203 that the object belongs to the
   positive class and a 15 percent id203 (1   0.85=0.15) that the
   object is a member of the negative class.

   in multi-class classification, the output is a id203
   distribution.

classifiers are functions

   a classifier is a function designed to perform classification.

   the input to a classifier is a representation of an object. the
   generated output is a prediction.

   if the input is a vector, the domain and co-domain of the classifier
   can be stated as follows:
   [0*v1my2ihyoj2cziqk.png]
   a function that maps a vector representation to a id203

   analogously, if the object is represented by a matrix, we write:
   [0*yf1gvmgllzwwycup.png]
   a function that maps a matrix to a id203

   assuming that |c| is the number of classes, the fact that a classifier
   maps a matrix to a id203 distribution of appropriate length is
   written as:
   [0*jjllmyvejgqxqr_j.png]
   a function that maps a matrix to a id203 distribution

mnist

   no tutorial on the essentials of machine learning would be complete
   without mentioning digit recognition and the mnist data set[6].

   recognizing digits is a historically important task in machine
   learning. the classifier receives an representation of an image and
   returns a id203 distribution over the ten possible digits from 0
   to 9. the system then selects the digit with the highest class
   id203.

   mnist is, arguably, the most famous data set in all of machine learning
   and contains 70,000 grayscale images with 28x28 pixels each. the most
   widely used form of this labeled image collection has been published
   twenty years ago.[7]
   [0*pqknrqm2hk3tv4j3.png]
   fig. 6: a few samples from the mnist data set (image by [23]josef
   steppan / cc by-sa 4.0)

   digit recognizers applied to the mnist data are functions that map
   matrices with 28 rows and 28 columns to id203 distributions over
   10 classes:
   [0*opxqlatjmqcpz1re.png]

   originally, this task had immediate and obvious applications, such as
   efficiency gains for postal service and financial services during an
   era of hand-written letters and cheques.

   over time, researchers achieved near-human performance on digit
   recognition tasks and moved on to more ambitious goals.

   meanwhile, the mnist dataset has been repurposed as a test for new
   ideas in machine learning. if a classification algorithms fails to
   deliver excellent results on mnist, chances are that it won   t be of
   much use for more challenging problems.

   thanks to the disciplined use of the mnist data set over many years,
   experimental results can be easily compared to hundreds of
   peer-reviewed studies. this is an important benefit of standardized
   testing.
     __________________________________________________________________

regression

   another supervised learning problem is regression. i would like to
   provide a brief overview in this article and then cover the topic in
   more detail in part 5.

   in a regression task, the target is continuous. in other words, it can
   take on any value within a range.

   depending on the context, the term regressor either refers to the
   independent variable (the input) or a function designed to perform
   regression (the analogue to a classifier).

   a classic application of regression is the prediction of prices. in
   this case, the range would be the set of positive real numbers (or an
   interval between 0 and a maximum price).

   other applications of regression include the prediction of biomarkers,
   demand, environmental measurements, performance, popularity, ratings,
   risk, scores and yield.
     __________________________________________________________________

baselines

   to conclude this article, i would like to discuss the use of constant
   function as baselines.

   constant functions return the same output for every input. they are
   written as f(x) = c (where c is a constant value) and can serve as
   baselines.

   in machine learning, a baseline is a simple rule to generate a
   prediction: something that can be described in a few words and is
   devoid of any sophistication.

   arguably, the most important baseline is to always predict the most
   frequently occurring target value. (an alternative is to generate
   predictions randomly. the latter can be the right choice if you don   t
   have reliable information about the distribution of the target values.)

   the following questions should be among the first that come to mind
   when you are evaluating a machine learning system:
    1. what is a baseline for the problem i am trying to solve?
    2. does the machine learning system significantly outperform the
       baseline?

a baseline for id31

   suppose you commission a machine learning specialist to develop a
   customized id31 solution. the requirements are to collect
   mentions of your company on relevant websites and to send notifications
   when publicly expressed opinions are detected.

   fast forward to the first iteration, and you receive a report about the
   initial test results: the system achieves an accuracy rate of 90%. is
   that good or bad? well, it depends.

   consider two cases: in the first case, a representative sample
   indicates that more than 90% of all publicly expressed opinions about
   your company are positive. the good news is that customers love you.
   the bad news is that the machine learning project is failing, because
   the system does not outperform the baseline of always predicting the
   most frequent target. if positive reviews are represented by the target
   value 1, that baseline would be the constant function f(x) = 1.

   in the second case, your company is controversial and public opinion is
   evenly split between fans and critics. an accuracy rate of 90% in light
   of these circumstances is a promising first sign that the project is a
   success.

   baselines, thus, provide a lower bound on what can be expected from the
   performance of the system.
     __________________________________________________________________

   so far, we have treated classifiers and regressors as a black box.

   starting with the next article, we will open up this box and describe
   the most commonly used functions in applied machine learning.

thank you for reading! if you   ve enjoyed this article, hit the clap button
and follow me to get the next part in the series.
     __________________________________________________________________

notes

   [1] this assumes that the programmed function always returns the same
   output for a given input and does not cause any side-effects.

   [2]
   [24]http://blogs.nature.com/news/2012/08/hot-stuff-cern-physicists-crea
   te-record-breaking-subatomic-soup.html

   [3] of course, the floating-point numbers used by computers do not
   perfectly correspond to set of real numbers. they have finite range and
   are less precise.

   [4] to be more precise, a scalar is an element of a field. a field is a
   set of numbers for which certain rules are defined. the real numbers
   are a field.

   [5] different authors user different symbols for the dimensions of a 3d
   array, especially for the last one. one mathematician i talked to
   prefers the use of the letter p, because o is close in appearance to 0.
   ultimately, i have decided to go with the letter o. it   s intuitive to
   use three consecutive letters and the letter p is reserved for
   probabilities.

   [6] the abbreviation mnist data set stands for modified national
   institute of standards and technology data set.

   [7] lecun, y., bottou, l., bengio, y. and haffner, p., 1998.
   gradient-based learning applied to document recognition. proceedings of
   the ieee, 86(11), pp.2278   2324.

     * [25]artificial intelligence
     * [26]machine learning
     * [27]business
     * [28]data science
     * [29]towards data science

   (button)
   (button)
   (button) 529 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of sebastian kwiatkowski

[31]sebastian kwiatkowski

     (button) follow
   [32]towards data science

[33]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 529
     * (button)
     *
     *

   [34]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [35]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/10117c005a28
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-4-10117c005a28&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-from-scratch-part-4-10117c005a28&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_rgir5cikof8n---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@sekwiatkowski?source=post_header_lockup
  17. https://towardsdatascience.com/@sekwiatkowski
  18. https://towardsdatascience.com/machine-learning-from-scratch-part-1-76603dececa6
  19. https://towardsdatascience.com/machine-learning-from-scratch-part-2-99ce4c78a3cc
  20. https://towardsdatascience.com/machine-learning-from-scratch-part-3-ed572330367d
  21. https://commons.wikimedia.org/wiki/file:function_machine2.svg
  22. https://towardsdatascience.com/media/e1261a3fc86d80fbf9def06d627d7c6f?postid=10117c005a28
  23. https://commons.wikimedia.org/wiki/file:mnistexamples.png
  24. http://blogs.nature.com/news/2012/08/hot-stuff-cern-physicists-create-record-breaking-subatomic-soup.html
  25. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/business?source=post
  28. https://towardsdatascience.com/tagged/data-science?source=post
  29. https://towardsdatascience.com/tagged/towards-data-science?source=post
  30. https://towardsdatascience.com/@sekwiatkowski?source=footer_card
  31. https://towardsdatascience.com/@sekwiatkowski
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/10117c005a28/share/twitter
  38. https://medium.com/p/10117c005a28/share/facebook
  39. https://medium.com/p/10117c005a28/share/twitter
  40. https://medium.com/p/10117c005a28/share/facebook
