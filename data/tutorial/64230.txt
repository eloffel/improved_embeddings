   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id75         detailed view

   [16]go to the profile of saishruthi swaminathan
   [17]saishruthi swaminathan (button) blockedunblock (button)
   followfollowing
   feb 26, 2018
   [1*dtoo8pnrhbmyfwmplp6wrq.png]
   figure 1 : id75 graph ( source:
   [18]http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/r/r5_correlation-regr
   ession/r5_correlation-regression_print.html)

   id75 is used for finding linear relationship between
   target and one or more predictors. there are two types of linear
   regression- simple and multiple.

simple id75

   simple id75 is useful for finding relationship between two
   continuous variables. one is predictor or independent variable and
   other is response or dependent variable. it looks for statistical
   relationship but not deterministic relationship. relationship between
   two variables is said to be deterministic if one variable can be
   accurately expressed by the other. for example, using temperature in
   degree celsius it is possible to accurately predict fahrenheit.
   statistical relationship is not accurate in determining relationship
   between two variables. for example, relationship between height and
   weight.

   the core idea is to obtain a line that best fits the data. the best fit
   line is the one for which total prediction error (all data points) are
   as small as possible. error is the distance between the point to the
   regression line.

   (full
   code         [19]https://github.com/ssaishruthi/linear_regression_detailed_im
   plementation)

   real-time example

   we have a dataset which contains information about relationship between
      number of hours studied    and    marks obtained   . many students have been
   observed and their hours of study and grade are recorded. this will be
   our training data. goal is to design a model that can predict marks if
   given the number of hours studied. using the training data, a
   regression line is obtained which will give minimum error. this linear
   equation is then used for any new data. that is, if we give number of
   hours studied by a student as an input, our model should predict their
   mark with minimum error.

   y(pred) = b0 + b1*x

   the values b0 and b1 must be chosen so that they minimize the error. if
   sum of squared error is taken as a metric to evaluate the model, then
   goal to obtain a line that best reduces the error.
   [1*utp8sgylk7h39qoqy9pf1a.png]
   figure 2: error calculation

   if we don   t square the error, then positive and negative point will
   cancel out each other.

   for model with one predictor,
   [1*1evy0pucuencpdp_qrplig.png]
   figure 3: intercept calculation
   [1*cx1yej9zlvi1o16i3modqa.png]
   figure 4: co-efficient formula

   exploring    b1   
     * if b1 > 0, then x(predictor) and y(target) have a positive
       relationship. that is increase in x will increase y.
     * if b1 < 0, then x(predictor) and y(target) have a negative
       relationship. that is increase in x will decrease y.

   exploring    b0   
     * if the model does not include x=0, then the prediction will become
       meaningless with only b0. for example, we have a dataset that
       relates height(x) and weight(y). taking x=0(that is height as 0),
       will make equation have only b0 value which is completely
       meaningless as in real-time height and weight can never be zero.
       this resulted due to considering the model values beyond its scope.
     * if the model includes value 0, then    b0    will be the average of all
       predicted values when x=0. but, setting zero for all the predictor
       variables is often impossible.
     * the value of b0 guarantee that residual have mean zero. if there is
       no    b0    term, then regression will be forced to pass over the
       origin. both the regression co-efficient and prediction will be
       biased.

   co-efficient from normal equations

   apart from above equation co-efficient of the model can also be
   calculated from normal equation.
   [1*rwgc2rwbjagqr4ysgxcuow.png]
   figure 5: co-efficient calculation using normal equation

   theta contains co-efficient of all predictors including constant term
      b0   . normal equation performs computation by taking inverse of input
   matrix. complexity of the computation will increase as the number of
   features increase. it gets very slow when number of features grow
   large.

   below is the python implementation of the equation.
   [1*gemwflfdvjom-5sthgqfya.png]
   figure 6: python implementation of normal equation

   optimizing using id119

   complexity of the normal equation makes it difficult to use, this is
   where id119 method comes into picture. partial derivative of
   the cost function with respect to the parameter can give optimal
   co-efficient value.

   (complete details of id119 is in
   [20]https://medium.com/@saishruthi.tn/math-behind-gradient-descent-4d66
   eb96d68d)

   python code for id119
   [1*55by_gllylvwrkmnv9nkcg.png]
   figure 7: python implementation of id119

   residual analysis

   randomness and unpredictability are the two main components of a
   regression model.

   prediction = deterministic + statistic

   deterministic part is covered by the predictor variable in the model.
   stochastic part reveals the fact that the expected and observed value
   is unpredictable. there will always be some information that are missed
   to cover. this information can be obtained from the residual
   information.

   let   s explain the concept of residue through an example. consider, we
   have a dataset which predicts sales of juice when given a temperature
   of place. value predicted from regression equation will always have
   some difference with the actual value. sales will not match exactly
   with the true output value. this difference is called as residue.

   residual plot helps in analyzing the model using the values of
   residues. it is plotted between predicted values and residue. their
   values are standardized. the distance of the point from 0 specifies how
   bad the prediction was for that value. if the value is positive, then
   the prediction is low. if the value is negative, then the prediction is
   high. 0 value indicates prefect prediction. detecting residual pattern
   can improve the model.

   non-random pattern of the residual plot indicates that the model is,
     * missing a variable which has significant contribution to the model
       target
     * missing to capture non-linearity (using polynomial term)
     * no interaction between terms in model

   characteristics of a residue
     * residuals do not exhibit any pattern
     * adjacent residuals should not be same as they indicate that there
       is some information missed by system.

   residual implementation and plot
   [1*32nhbubx5tqgucpq6vg1cq.png]
   figure 8: residual plot

   [21](some other reference materials         1) &([22]reference material         2)

   metrics for model evaluation

   r-squared value

   this value ranges from 0 to 1. value    1    indicates predictor perfectly
   accounts for all the variation in y. value    0    indicates that predictor
      x    accounts for no variation in    y   .

   1. regression sum of squares (ssr)

   this gives information about how far estimated regression line is from
   the horizontal    no relationship    line (average of actual output).
   [1*exrb9isttlftrpksfbwehg.png]
   figure 9: regression error formula

   2. sum of squared error (sse)

   how much the target value varies around the regression line (predicted
   value).
   [1*m7ukjzntvpd6tqqnxxggzq.png]
   figure 10: sum of square formula

   3. total sum of squares (ssto)

   this tells how much the data point move around the mean.
   [1*lxac7fplogb1l3iqsukl5a.png]
   figure 11: total error formula

   python implementation
   [1*t7maoprfvikjaxdnajtt8w.png]
   figure 12: python implementation of r-square

   is the range of r-square always between 0 to 1?

   value of r2 may end up being negative if the regression line is made to
   pass through a point forcefully. this will lead to forcefully making
   regression line to pass through the origin (no intercept) giving an
   error higher than the error produced by the horizontal line. this will
   happen if the data is far away from the origin.

   (for mode
   details         [23]https://medium.com/@saishruthi.tn/is-r-sqaure-value-alway
   s-between-0-to-1-36a8d17807d1)

   correlation co-efficient (r)

   this is related to value of    r-squared    which can be observed from the
   notation itself. it ranges from -1 to 1.

   r = (+/-) sqrt(r  )

   if the value of b1 is negative, then    r    is negative whereas if the
   value of    b1    is positive then,    r    is positive. it is unitless.

   null-hypothesis and p-value

   null hypothesis is the initial claim that researcher specify using
   previous research or knowledge.

   low p-value: rejects null hypothesis indicating that the predictor
   value is related to the response

   high p-value: changes in predictor are not associated with change in
   target

   obtained regression line
   [1*xckzmc9xgm0ulj4koiompg.png]
   figure 13: final regression line over test data

   full code:
   https://github.com/ssaishruthi/linear_regression_detailed_implementatio
   n

   this is an educational post made by complilation of materials (like
   prof.andrew ng course, siraj raval   s videos, etc. ) that helped me in
   my journey. other references are stated near the content.

               to be continued

     * [24]machine learning
     * [25]data science
     * [26]data visualization
     * [27]data mining
     * [28]statistical analysis

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of saishruthi swaminathan

[30]saishruthi swaminathan

   passionate about transforming data into useful products. happy sharing
   my knowledge in data science to all!!

     (button) follow
   [31]towards data science

[32]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [33]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [34]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ea73175f6e86
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_iyjohdsasncg---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@saishruthi.tn?source=post_header_lockup
  17. https://towardsdatascience.com/@saishruthi.tn
  18. http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/r/r5_correlation-regression/r5_correlation-regression_print.html
  19. https://github.com/ssaishruthi/linear_regression_detailed_implementation
  20. https://medium.com/@saishruthi.tn/math-behind-gradient-descent-4d66eb96d68d
  21. https://newonlinecourses.science.psu.edu/stat501/node/250/
  22. http://blog.minitab.com/blog/adventures-in-statistics-2/why-you-need-to-check-your-residual-plots-for-regression-analysis
  23. https://medium.com/@saishruthi.tn/is-r-sqaure-value-always-between-0-to-1-36a8d17807d1
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/tagged/data-visualization?source=post
  27. https://towardsdatascience.com/tagged/data-mining?source=post
  28. https://towardsdatascience.com/tagged/statistical-analysis?source=post
  29. https://towardsdatascience.com/@saishruthi.tn?source=footer_card
  30. https://towardsdatascience.com/@saishruthi.tn
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/ea73175f6e86/share/twitter
  37. https://medium.com/p/ea73175f6e86/share/facebook
  38. https://medium.com/p/ea73175f6e86/share/twitter
  39. https://medium.com/p/ea73175f6e86/share/facebook
