   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning basics         part 1         concept of regression

   [16]go to the profile of daniel deutsch
   [17]daniel deutsch (button) blockedunblock (button) followfollowing
   feb 23, 2018
   [0*owad7v5jylcutpof.]
   photo by andre benz on
   unsplash         [18]https://unsplash.com/photos/cxu6tnxhub0

   in this article i revisit the learned material from the amazing
   [19]machine learning course by andre ng on coursera and create an
   overview about the concepts. all quotes refer to the material from the
   course if not explicitly stated otherwise.

table of contents

     * [20]definition
     * [21]id75 with one variable
     * [22]model representation
     * [23]cost function
     * [24]id119
     * [25]id75 with multiple variables
     * [26]feature scaling and mean id172
     * [27]learning rate
     * [28]polynomial regression
     * [29]normal equation (for analytical computing)
     * [30]id28
     * [31]classification
     * [32]adapted cost function and id119
     * [33]alternatives to id119
     * [34]multiclass classification
     * [35]problem of overfitting and the use of id173
     * [36]id173

definition

     a computer program is said to learn from experience e with respect
     to some class of tasks t and performance measure p, if its
     performance at tasks in t, as measured by p, improves with
     experience e.         tom mitchell

id75 with one variable

model representation

   id75 tries to fit points to a line generated by an
   algorithm. this optimized line (the model) is capable of predicting
   values for certain input values and can be plotted.

cost function

   we want to set the parameters in order to achieve a minimal difference
   between the predicted and the real values.

     we can measure the accuracy of our hypothesis function by using a
     cost function. this takes an average difference (actually a fancier
     version of an average) of all the results of the hypothesis with
     inputs from x   s and the actual output y   s.

   [0*n8wrdrbmoqxxjwod.png]

id119

   id119 keeps changing the parameters to reduce the cost
   function gradually. with each iteration we shall come closer to a
   minimum. with each iteration the parameters must be adapted
   simultaneously! the size of a    step   /iteration is determined by the
   parameter alpha (the learning rate).

     the way we do this is by taking the derivative (the tangential line
     to a function) of our cost function. the slope of the tangent is the
     derivative at that point and it will give us a direction to move
     towards. we make steps down the cost function in the direction with
     the steepest descent.

   [0*mv3azt83ny38xkqv.png]

   choosing the value of alpha is crucial. if it is too small the
   algorithm will be slow, if it is too large it will fail to converge.

   when specifically applied to the case of id75, a new form
   of the id119 equation can be derived, where m is the size of
   the training set. again both parameters must be updated simultaneously.
   [0*y9v-x2rhnntfrstl.png]

     note that, while id119 can be susceptible to local minima
     in general, the optimization problem we have posed here for linear
     regression has only one global, and no other local, optima; thus
     id119 always converges (assuming the learning rate    is
     not too large) to the global minimum.

id75 with multiple variables

   now, instead of one feature/variable that is responsible for a certain
   outcome we have multiple ones.

   therefore the hypothesis changes accordingly and takes multiple
   parameters into account. the same applies for the id119. it
   simply is an extension by the additional parameters, which must be
   updated.
   [0*fshi_g_k5cbic90v.png]

feature scaling and mean id172

   to make sure that all values of features are on a same scale and have
   the same mean it   s necessary to use feature scaling and mean
   id172.

     feature scaling involves dividing the input values by the range
     (i.e. the maximum value minus the minimum value) of the input
     variable, resulting in a new range of just 1. mean id172
     involves subtracting the average value for an input variable from
     the values for that input variable resulting in a new average value
     for the input variable of just zero.

learning rate

   to choose a suitable learning rate, id119 has to be plotted
   and    debugged   .

     make a plot with number of iterations on the x-axis. now plot the
     cost function, j(  ) over the number of iterations of gradient
     descent. if j(  ) ever increases, then you probably need to decrease
       .

   if j(0) stops to decrease significantly in an iteration step
   convergence can be declared.

polynomial regression

   features can be improved by re-defining the hypothesis function into a
   quadratic, cubic or square root function.

   in this case, extra emphasize must be applied to feature scaling!

normal equation (for analytical computing)

   instead of using id119 for gradually minimizing the cost
   function, the normal equation sets the derivatives to zero.
   [0*r_3qqxhz-rpeztkj.png]

   the normal equation doesn   t need a learning rate alpha and no iteration
   at all, but requires the transpose of the design matrix. when you have
   a large number of features (eg 10000) the calculation will take longer
   than the iterative process with id119. to improve the
   quality of the normal equation algorithm features should be regularized
   and redundant features deleted.

id28

classification

   to classify data the result shall eiter be 0 or 1 (binary
   classification). from a regression point of view this can mean to
   classify output, that is >= 0.5 as 1 and output that is < 0.5 as 0
   (whereas 0,5 is the decision boundary).

   the adapted hypothesis, using the logistic/sigmoid function, would now
   be:
   [0*ohgxiwlhppr_tdbn.png]

   it returns the id203 for the output being 1!

adapted cost function and id119

   due to the use of the sigmoid function, the cost function has to be
   adapted accordingly by using the logarithm. since the goal is now not
   to minimize the distance from a predicted value, but rather to minimize
   the distance between the output by the hypothesis and y (0 or 1).
   [0*c3kzyvyd0wsd_gnk.png]

   or for a vectorized implementation:
   [0*7olt5axyivdffwwg.png]

   however, id119 stays the same because the formula uses the
   derivative part of the hypothesis!
   [0*m29ou-ab98hctw2i.png]

   or for a vectorized implementation:
   [0*2qb97n8pmceqll_o.png]

alternatives to id119

   more complex optimization algorithms like
     * conjugate gradient,
     * bfgs or
     * l-bfgs

   often allow faster computation with no need for picking a learning rate
   alpha.

multiclass classification

   the previously described classification problem solving only works for
   binary classification. having more possible outcome than n=2 is called
   multiclass classification. to apply the concept on multiple classes the
      one-vs-all    method is used, which is essentially applying the binary
   classification on each class (one class is positive, all the rest is
   negative). instead of setting y to either 0 or 1, y is set to i, which
   itself is tested against all the other classes. basically the process
   is twofold:
    1. setting the logistic classifier to y. (if y is 3, we create 3
       classifiers)
    2. new input is tested against all classifiers and choose the one with
       the highest id203.

problem of overfitting and the use of id173

   in the case of overfitting, the model captures the data structure
   perfectly, whereas in underfitting the model captures not enough of the
   data structure (ie. the graph of the model barely touches all of the
   data points).

   to solve the problem of overfitting either the features can be reduced
   or the magnitude of their values can be regularized.

id173

   for regularizing a model, a parameter (lambda) has to be added to the
   cost function. it de- or inflates the parameter theta.
   [0*_7wgyykbjpfq9fiv.png]

   consequently applying it to the id28 looks like this:
   [0*uqf_amgvewnuzi_h.png]

   note how the regularizing parameter starts at 1         not regularizing the
   [37]bias term theta 0.
   [0*66oyiyognh-i_o00.png]
     __________________________________________________________________

   this wraps up the first part. in the next one, neural networks will be
   described. stay tuned!
     __________________________________________________________________

   thanks for reading my article! feel free to leave any feedback!
     __________________________________________________________________

   daniel is a ll.m. student in business law, working as a software
   engineer and organizer of tech-related events in vienna. his current
   personal learning efforts focus on machine learning.

   connect on:
     * [38]linkedin
     * [39]github
     * [40]medium
     * [41]twitter
     * [42]steemit
     * [43]hashnode

     * [44]machine learning
     * [45]coursera
     * [46]regression
     * [47]math
     * [48]statistics

   (button)
   (button)
   (button) 178 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [49]go to the profile of daniel deutsch

[50]daniel deutsch

   aspiring web developer with business law background. pushing the limits
   to make the world a better place. open for projects of any kind.

     (button) follow
   [51]towards data science

[52]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 178
     * (button)
     *
     *

   [53]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [54]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/31982e8d8ced
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-1-concept-of-regression-31982e8d8ced&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-1-concept-of-regression-31982e8d8ced&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_nowqdayhdc5z---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ddcreationstudi?source=post_header_lockup
  17. https://towardsdatascience.com/@ddcreationstudi
  18. https://unsplash.com/photos/cxu6tnxhub0
  19. https://www.coursera.org/learn/machine-learning
  20. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#definition
  21. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#linear-regression-with-one-variable
  22. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#model-representation
  23. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#cost-function
  24. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#gradient-descent
  25. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#linear-regression-with-multiple-variables
  26. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#feature-scaling-and-mean-id172
  27. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#learning-rate
  28. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#polynomial-regression
  29. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#normal-equation-for-analytical-computing
  30. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#logistic-regression
  31. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#classification
  32. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#adapted-cost-function-and-gradient-descent
  33. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#alternatives-to-gradient-descent
  34. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#multiclass-classification
  35. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#problem-of-overfitting-and-the-use-of-id173
  36. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop1.md#id173
  37. https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks
  38. https://www.linkedin.com/in/createdd
  39. https://github.com/createdd
  40. https://medium.com/@ddcreationstudi
  41. https://twitter.com/ddcreationstudi
  42. https://steemit.com/@createdd
  43. https://hashnode.com/@ddcreationstudio
  44. https://towardsdatascience.com/tagged/machine-learning?source=post
  45. https://towardsdatascience.com/tagged/coursera?source=post
  46. https://towardsdatascience.com/tagged/regression?source=post
  47. https://towardsdatascience.com/tagged/math?source=post
  48. https://towardsdatascience.com/tagged/statistics?source=post
  49. https://towardsdatascience.com/@ddcreationstudi?source=footer_card
  50. https://towardsdatascience.com/@ddcreationstudi
  51. https://towardsdatascience.com/?source=footer_card
  52. https://towardsdatascience.com/?source=footer_card
  53. https://towardsdatascience.com/
  54. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  56. https://medium.com/p/31982e8d8ced/share/twitter
  57. https://medium.com/p/31982e8d8ced/share/facebook
  58. https://medium.com/p/31982e8d8ced/share/twitter
  59. https://medium.com/p/31982e8d8ced/share/facebook
