   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning basics         part 3         vector machines, unsupervised learning and
principal component analysis

   [16]go to the profile of daniel deutsch
   [17]daniel deutsch (button) blockedunblock (button) followfollowing
   mar 9, 2018
   [0*coja3wy7bw9jrwib.]
   photo by anders jild  n on
   unsplash         [18]https://unsplash.com/photos/cyrmqa7a3wc

   in this article i revisit the learned material from the amazing
   [19]machine learning course by andre ng on coursera and create an
   overview about the concepts. the article is not designed as a tutorial
   but rather to fresh up on the basic ideas.

   all quotes refer to the material from the course if not explicitly
   stated otherwise.

table of contents

     * [20]support vector machines
     * [21]-mathematical definition
     * [22]-large margin classifier
     * [23]-kernels
     * [24]-how to choose the landmarks
     * [25]-id166 parameters
     * [26]-tips for practice and how to choose the right system
     * [27]id91 and unsupervised learning
     * [28]-id116 algorithm
     * [29]-optimization objective
     * [30]id84 and principal component analysis (pca)
     * [31]-pca
     * [32]-practical tips

support vector machines

mathematical definition

   instead of regularizing the second term with lambda like we did in the
   original cost function:
   [0*k4kmh522qqplvl2y.png]

   now we want to regularize the first term with the parameter c and
   adding a new cost function (cost1 and cost0) for transpose theta:
   [0*tbrw8jw0za2lw8zu.png]

   plotting the cost1 and cost0 function looks something like this:
   [0*2hib8f9hcdufkpym.png]

   hence, if we want a result y = 1 theta transpose x must be greater than
   1 and if y = 0 theta transpose x must be smaller than -1.

   in essence, we just have simplified the cost function in order to use
   geometry for further steps.

large margin classifier

   in case of linearly separable data, the id166 algorithm chooses the line
   that separates the classes with the largest margin.

   using calculus the length of a parameter can easily be retrieved from
   the initial formula.
   [0*27p1bm6azqrywzao.png]
   by martin thoma         own work, cc by 3.0,
   [33]https://commons.wikimedia.org/w/index.php?curid=20159892

   basically, the projection of vector x is multiplied by the length of
   parameter theta and optimized to be a maximum/minimum. this results in
   always returning a line that seems to separate 2 classes evenly.

   note, that in order to neglect outliners it helps to decrease the value
   of c (regularizing).

kernels

   since polynomial features can be computational expensive, common
   practice introduces kernels. for this, new features which depend on
   similarity of features and examples, are computed. it is like putting
   landmarks on the plot and calculating the similarity with the gaussian
   kernel formula. if the similarity is 1, the training example is close
   to the picked landmark.
   [0*ipazorbtkpebg_0m.png]

   the choose for the value of the parameter sigma determines the boundary
   for similarity.

   introducing the landmark-similarity system allows to classify data that
   is not linear.

how to choose the landmarks

   to compute the landmarks we adapt the cost function to the following:
   [0*c7fhb70wckrnfrh6.png]

   keep in mind that for the regularizing part, instead of n (number of
   features) m (training examples) should be used. this makes sense, since
   we want to calculate the landmarks which are related to the examples.

   also note, that you can implement the concept of kernels on logistic
   regression as well but the mathematical benefits of id166 cannot be
   utilized properly and the implementation will likely be slower.

   another term for kernel is    similarity-function   .

id166 parameters

   to address over- and underfitting, the parameters lambda (in c) and
   sigma can be used.

   increasing c (essentially minimizing lambda) or decreasing sigma
   squared improves underfitting (high c leads to higher variance).

tips for practice and how to choose the right system

     * use id166 packages instead of trying to write your own id166
       calculation
     * the most common id81s are the linear kernel (using no
       kernel) or the gaussian kernel
     * there are other kernels as well, but be check if they are able to
       satisfy the    mercer   s theorem   
     * if n is much larger than m, use id28 or id166 with a
       linear kernel
     * if n is in a adequate range of m, use a gaussian kernel
     * if n is smaller than m, use id28 or id166 with a
       linear kernel or add more features
     * neural networks work well for all of those settings but might be
       slower to train

id91 and unsupervised learning

   in a supervised learning problem a set of labels is given to fit a
   hypothesis to it. in contrast, in the unsupervised learning problem
   we   re given data that does not have any labels associated with it.

   the goal of the algorithm is to find structure (clusters) within the
   data set.

id116 algorithm

   to put this algorithm in simple terms:
     * initialize randomly the    centroids    (marks in the middle of the
       data)
     * assign the data points the each centroid, which are the closest to
       the data point
     * move the centroid to the center (mean) of the data points
     * repeat the 2 previous steps until there is no change in the
       clusters

optimization objective

   the cost function, which tries to minimize the mean of the squared
   distance between an example point and the location of the corresponding
   cluster centroid, looks like the following:
   [0*xf1lftmsc0pht3jn.png]

   in order to avoid local optima, the following steps should be
   implemented multiple times:
    1. randomly initialize id116
    2. run the id116 algorithm to get the index of a cluster and the
       cluster centroids
    3. compute the cost function

   to choose the number of clusters k the    elbow method    can be used,
   which plots the cost function to the number of clusters and uses the
   number where the curve shows an    elbow   . however, since this method can
   be difficult to use on certain graphs, another way would be to simply
   select the number according to a later/downstream purpose (like desired
   product sizes         small, medium, large).

id84 and principal component analysis (pca)

   reducing data from multiple dimensions to 2 or 3 dimensions through
   data compression allows to plot data and give valuable additional
   insight. simply reducing data can speed up the running time of a
   learning algorithm and reduces the space needed for storage.

pca

   the most common algorithm is principal component analysis. the idea
   behind it is to reduce a dimension by finding a direction (vector) onto
   which to project the data to minimize the projection error. when
   plotted, the algorithm might look similar to the id75
   model. however, it is important to note that in id75 the
   variable y is predicted by the variable x, whereas in pca the different
   variables x are treated equally.

   to implement a pca algorithm, you normally
    1. perform mean id172 and feature scaling
    2. calculate a covariance matrix (sigma) with the following formula
    3. use singular value decomposition (svd) on sigma
    4. multiply the transpose of the first k columns of the resulting u
       matrix with the feature vector x that shall be reduced and return
       the resulting z feature vector

   [0*zy2_q0zy8s7mcftc.png]

   to decompress the data and harness the real power of this concept, it
   is possible to reconstruct the (approximated) original by simply
   multiplying the u matrix with the z vector again.

   to choose the variable k (number of principal components) the following
   formula can be used:
   [0*1cghcsy6fxzb7isd.png]

   the idea is to divide the average squared projection error (which we
   try to minimize) by the total variation in the data.

   the practical implementation would be to try the pca algorithm with k =
   1 and test if the condition for the retained variance is fulfilled, and
   if not the procedere with an increased k should be continued. or to
   take the s matrix, which resulted from using the singular value
   decomposition on sigma and testing it like the following:
   [0*i9hclu4v25_aii7l.png]

   (is essentially the equivalent to the previous formula)

practical tips

   to speed up a supervised learning case, you should
    1. extract only the inputs (to have an unlabeled training set if you
       are provided a labeled one)
    2. perform the pca algorithm
    3. create a new training set by substituting your previous x with the
       new input z
    4. train your algorithm with the new data set

   note that, pca should only be used on the training set and not the
   cross validation or testing set. afterwards the resulted mapping from x
   to z can be applied to the cross validation and testing set as well.

   be careful to not use pca when your model has a problem of overfitting.
   although reducing features helps in addressing the problem, the concept
   of pca throws away some amount of information without knowing the
   values of y. this can lead to bad results. it is better to use
   id173 instead.

   lastly, always try to train your algorithm with original data. pca
   should only be applied if the normal machine learning architecture is
   not sufficient!
     __________________________________________________________________

   this wraps up the third part. in the next one, anomaly detection,
   recommender systems and scaling issues will be described. stay tuned!
     __________________________________________________________________

   thanks for reading my article! feel free to leave any feedback!
     __________________________________________________________________

   daniel is a ll.m. student in business law, working as a software
   engineer and organizer of tech-related events in vienna. his current
   personal learning efforts focus on machine learning.

   connect on:
     * [34]linkedin
     * [35]github
     * [36]medium
     * [37]twitter
     * [38]steemit
     * [39]hashnode

     * [40]machine learning
     * [41]unsupervised learning
     * [42]support vector machine
     * [43]data science
     * [44]andrew ng

   (button)
   (button)
   (button) 146 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [45]go to the profile of daniel deutsch

[46]daniel deutsch

   aspiring web developer with business law background. pushing the limits
   to make the world a better place. open for projects of any kind.

     (button) follow
   [47]towards data science

[48]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 146
     * (button)
     *
     *

   [49]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [50]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5b51aac6dd0c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-3-vector-machines-unsupervised-learning-and-principal-component-5b51aac6dd0c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-3-vector-machines-unsupervised-learning-and-principal-component-5b51aac6dd0c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_vnfx5vr1ayfe---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ddcreationstudi?source=post_header_lockup
  17. https://towardsdatascience.com/@ddcreationstudi
  18. https://unsplash.com/photos/cyrmqa7a3wc
  19. https://www.coursera.org/learn/machine-learning
  20. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#support-vector-machines
  21. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#mathematical-definition
  22. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#large-margin-classifier
  23. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#kernels
  24. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#how-to-choose-the-landmarks
  25. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#id166-parameters
  26. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#tips-for-practice-and-how-to-choose-the-right-system
  27. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#id91-and-unsupervised-learning
  28. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#id116-algorithm
  29. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#optimization-objective
  30. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#dimensionality-reduction-and-principal-component-analysis-pca
  31. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#pca
  32. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop3.md#practical-tips
  33. https://commons.wikimedia.org/w/index.php?curid=20159892
  34. https://www.linkedin.com/in/createdd
  35. https://github.com/createdd
  36. https://medium.com/@ddcreationstudi
  37. https://twitter.com/ddcreationstudi
  38. https://steemit.com/@createdd
  39. https://hashnode.com/@ddcreationstudio
  40. https://towardsdatascience.com/tagged/machine-learning?source=post
  41. https://towardsdatascience.com/tagged/unsupervised-learning?source=post
  42. https://towardsdatascience.com/tagged/support-vector-machine?source=post
  43. https://towardsdatascience.com/tagged/data-science?source=post
  44. https://towardsdatascience.com/tagged/andrew-ng?source=post
  45. https://towardsdatascience.com/@ddcreationstudi?source=footer_card
  46. https://towardsdatascience.com/@ddcreationstudi
  47. https://towardsdatascience.com/?source=footer_card
  48. https://towardsdatascience.com/?source=footer_card
  49. https://towardsdatascience.com/
  50. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  52. https://medium.com/p/5b51aac6dd0c/share/twitter
  53. https://medium.com/p/5b51aac6dd0c/share/facebook
  54. https://medium.com/p/5b51aac6dd0c/share/twitter
  55. https://medium.com/p/5b51aac6dd0c/share/facebook
