   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

machine learning basics         part 4         anomaly detection, recommender systems
and scaling

   [16]go to the profile of daniel deutsch
   [17]daniel deutsch (button) blockedunblock (button) followfollowing
   mar 16, 2018
   [0*7rytz6rg3mywst0w.]
   photo by fahrul azmi on
   unsplash         [18]https://unsplash.com/photos/vr-nb0bncoy

   in this article i revisit the learned material from the amazing
   [19]machine learning course by andre ng on coursera and create an
   overview about the concepts. the article is not designed as a tutorial
   but rather to fresh up on the basic ideas.

   all quotes refer to the material from the course if not explicitly
   stated otherwise.

table of contents

     * [20]anomaly detection
     * [21]-develop a anomaly detection system
     * [22]-practical tips and difference to a supervised learning system
     * [23]-multivariat gaussian distribution
     * [24]recommender systems
     * [25]-id171 with id185
     * [26]-further usage
     * [27]scaling machine learning systems
     * [28]-stochastic id119
     * [29]-mini-batch id119
     * [30]-test for convergence
     * [31]-online learning
     * [32]-map-reduce and data parallelism
     * [33]tricks for use on applications
     * [34]-create a pipeline for your problem
     * [35]-getting more data
     * [36]-ceiling analysis

anomaly detection

   anomaly detection tests a new example against the behavior of other
   examples in that range. this idea is often used in fraud detection,
   manufacturing or monitoring of machines. it is always useful if the
   goal is to detect certain outliners.

   using a gaussian distribution algorithm implies that the example x is
   distributed with a mean mu and variance sigma squared.

   the formula for mu and sigma squared are:
   [0*0slckvectmtyiihg.png]

   the formula for calculating the id203 is:
   [0*5ya1bhhxxnqfmxxo.png]

   the steps to build the algorithm are
    1. choose the features x that might be indicative of anomalous
       examples
    2. calculate the parameters mu and sigma
    3. compute the id203 p of x
    4. test against your set id203 boundary epsilon

develop a anomaly detection system

   when the algorithm is implemented it is important to introduce a
   real-number evaluation metric.

   as always, it is advisable to split the data set into a training,
   cross-validation and testing set (60   20   20).

   the steps to build the system would be:
    1. fit the model p(x) on the training set
    2. predict y on the resulting probabilities of your cross-validation
       and testing sets
    3. evaluate the result using a contingency table (true positives,
       false positives,    ), precision/recall methods or the f1-score
    4. change values of epsilon (if necessary)

practical tips and difference to a supervised learning system

   an anomaly detection system should be used if
     * a large number of negative examples but a small number of positives
       examples are available
     * the anomalies themselves cannot be classified and may vary in
       future examples
     * eg. fraud detection, monitoring machines, etc.

   if a classification can easily be done, ie having large numbers of
   positive and negative examples and future examples will be similar, it
   is advisable to use a supervised learning algorithm. (eg spam, cancer
   classification)

   to analyse errors it makes sense to plot the features and see if they
   behave gaussian. if not, constants (like log(x)) can be added, to try
   to make it look as gaussian as possible.

   the basic assumption for using anomaly detection system is to have few
   anomalous examples and many normal ones. if this is not met, the
   misclassified example should be inspected for behavior that allows to
   come up with a new feature.

multivariat gaussian distribution

   in certain cases the normal gaussian distribution is not enough the
   accurately flag anomalies. a multivariat gaussian distribution
   calculates the id203 model of x at once, instead of modelling the
   probabilities for each feature alone. it uses a covariance matrix
   instead of sigma squared.

   the formula looks like:
   [0*nn1dw3gmnd0gy2dw.png]

   whereas:
   [0*bjfs1xwwihqgqwri.png]

   the multivariate gaussian model is worth to be considered when the
   number of examples is much larger than the number of features. it
   captures correlations between features but is computational expensive.
   when it is obvious what feature combinations can capture the anomalies,
   it is advisable to first implement those with the original gaussian
   model.

recommender systems

   a id126 is one of the most common and most successful
   practical examples for applying a machine learning algorithm in real
   life.

   assuming you have a content-based recommender system. first, a problem
   has to be formulated. this can be something like predicting the rating
   of a certain product of a certain user.

   given the ratings of a movie, to learn the parameter theta for a
   certain user, the optimization algorithm can look like this:
   [0*hdam9qk3dmphuxqh.png]
     * the parameters theta denote a vector for a certain user
     * the feature x denotes a vector for a movie
     * y denotes the rating by a certain user on a certain movie
     * n denotes the number of users

   this is the basic cost function of a squared error with id173
   summed up for different users (theta j).

   and using id119 (multiplying the learning rate alpha with
   the partial derivative with respect to your parameter of the
   optimization objective) to gradually minimize the result. note, that
   theta 0 for k = 0 should not be regularized (as explained in linear
   regression).

id171 with id185

   given the parameters theta of each user for a certain movie, the
   feature vector of a movie can be estimated with the optimization
   algorithm:

   one way to address the problem of what vector to calculate first
   (feature vector of a movie or the parameter vector fo a user), is to
   guess the parameter vector for a user and then use the estimation to
   define a (better) feature vector for a movie.

   this implementation is called id185 because with each
   rating of a user the algorithm is able to define better movie feature
   vectors and improves the output for all users.

   to use id185 simultaneously (updating theta and x at
   the same time), the following formula can be used:
   [0*gify9v1a7iprurqv.png]

   this leads to the following id119 implementation:
   [0*4yy8dphosj-3bsav.png]

   to implement this formula, you have to
    1. initialize all thetas and xs with small random values
    2. minimize the cost function with the provided id119
       formula
    3. predict the movie rating of a user with the parameter theta with
       the learned feature x.

further usage

   after implementing the id185 system another step can
   be to suggest related movies/products.

   this is easily done since we have already calculated a feature vector
   x. now to find related movies/products, we simply have to find the ones
   with the smallest distance, like:
   [0*rw1old5bidq_05vx.png]

   note, that if you have a user or movie/product with no rating at all,
   you should perform mean id172 before implementing the learning
   algorithm. to accomplish this, first the mean should be subtracted from
   the result matrix and re-added when predicting the rating. but you
   should always ask yourself if it makes sense to recommend something to
   a completely undefined unit anyways.

scaling machine learning systems

   when having a case with very large numbers of examples (~100 mio)
   always ask yourself if it is possible to reduce the dataset with
   keeping the results.

   one way is to plot a learning curve for a range of values of m and
   verify that the algorithm has high variance when m is small. when th
   algorithm already has a high bias, increasing the dataset does not
   help.

stochastic id119

   on large training sets id119 becomes very computational
   expensive. a way to address this problem is to use stochastic gradient
   descent.

   instead of iterating through all trainings examples at once, you
   shuffle your dataset randomly and perform id119 on a single
   example as follows:
   [0*7lc8-mu82kaul6xk.png]

   this allows to improve the parameters on every single example and
   therefore takes much less time than improving them on all examples at
   once. (on the cost that it might not converge at all         but ends up
   close enough for most practical use cases).

mini-batch id119

   as a middle way between going through all examples or just 1 example in
   each id119 iteration, mini-batch allows to set a certain
   number b of examples per iteration. the adapted loop could look like
   this:
   [0*qxxbkqpeor6iyypb.png]

test for convergence

   to test if either mini-batch or stochastic id119 are
   converging the cost function can be plotted and checked.

   whereas for mini-batch id119 the cost function of the number
   of iterations can just be plotted, for stochastic id119 the
   cost function (on a certain example) has to be plotted on the average
   of multiple examples.

   if the algorithm fails to converge try to slowly decrease the learning
   rate alpha.

online learning

   in the idea of online learning data is considered to be endless and
   free. for example getting a stream of user data on a website. in this
   case, id119 can be performed on one example at each time for
   endless times. with each incoming example the algorithm is improved and
   this way the algorithm can also adapt according to changes in user
   preference.

map-reduce and data parallelism

   another way to address huge data sets is to use batch id119
   but splitting it to different sub sets, allowing multiple machines to
   work on their own set of data. afterwards the results can simply be
   added together to fit the original formula (basically using the sums of
   functions).

tricks for use on applications

create a pipeline for your problem

   for example
     * detect text
     * segment characters
     * classify characters

getting more data

     * try to create additional data by adding distortions to your already
       available data set (artificial data synthesis)
     * try collecting/labelling data yourself
     * crowd source data

ceiling analysis

   analyse what part of your pipeline is worth spending time for
   improvements by comparing the accuracy improvements.
     __________________________________________________________________

   this wraps up the fourth part. what an incredible course! :)
     __________________________________________________________________

   thanks for reading my article! feel free to leave any feedback!
     __________________________________________________________________

   daniel is a ll.m. student in business law, working as a software
   engineer and organizer of tech-related events in vienna. his current
   personal learning efforts focus on machine learning.

   connect on:
     * [37]linkedin
     * [38]github
     * [39]medium
     * [40]twitter
     * [41]steemit
     * [42]hashnode

     * [43]machine learning
     * [44]anomaly detection
     * [45]data science
     * [46]statistics
     * [47]andrew ng

   (button)
   (button)
   (button) 185 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [48]go to the profile of daniel deutsch

[49]daniel deutsch

   aspiring web developer with business law background. pushing the limits
   to make the world a better place. open for projects of any kind.

     (button) follow
   [50]towards data science

[51]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 185
     * (button)
     *
     *

   [52]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [53]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b8bbf0413aa9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-4-anomaly-detection-recommender-systems-and-scaling-b8bbf0413aa9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/machine-learning-basics-part-4-anomaly-detection-recommender-systems-and-scaling-b8bbf0413aa9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_0ievtdz74mhc---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ddcreationstudi?source=post_header_lockup
  17. https://towardsdatascience.com/@ddcreationstudi
  18. https://unsplash.com/photos/vr-nb0bncoy
  19. https://www.coursera.org/learn/machine-learning
  20. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#anomaly-detection
  21. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#develop-a-anomaly-detection-system
  22. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#practical-tips-and-difference-to-a-supervised-learning-system
  23. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#multivariat-gaussian-distribution
  24. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#recommender-systems
  25. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#feature-learning-with-collaborative-filtering
  26. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#further-usage
  27. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#scaling-machine-learning-systems
  28. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#stochastic-gradient-descent
  29. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#mini-batch-gradient-descent
  30. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#test-for-convergence
  31. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#online-learning
  32. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#map-reduce-and-data-parallelism
  33. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#tricks-for-use-on-applications
  34. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#create-a-pipeline-for-your-problem
  35. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#getting-more-data
  36. https://github.com/ddcreationstudios/writing/blob/master/2018/articles/mlintrop4.md#ceiling-analysis
  37. https://www.linkedin.com/in/createdd
  38. https://github.com/createdd
  39. https://medium.com/@ddcreationstudi
  40. https://twitter.com/ddcreationstudi
  41. https://steemit.com/@createdd
  42. https://hashnode.com/@ddcreationstudio
  43. https://towardsdatascience.com/tagged/machine-learning?source=post
  44. https://towardsdatascience.com/tagged/anomaly-detection?source=post
  45. https://towardsdatascience.com/tagged/data-science?source=post
  46. https://towardsdatascience.com/tagged/statistics?source=post
  47. https://towardsdatascience.com/tagged/andrew-ng?source=post
  48. https://towardsdatascience.com/@ddcreationstudi?source=footer_card
  49. https://towardsdatascience.com/@ddcreationstudi
  50. https://towardsdatascience.com/?source=footer_card
  51. https://towardsdatascience.com/?source=footer_card
  52. https://towardsdatascience.com/
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://medium.com/p/b8bbf0413aa9/share/twitter
  56. https://medium.com/p/b8bbf0413aa9/share/facebook
  57. https://medium.com/p/b8bbf0413aa9/share/twitter
  58. https://medium.com/p/b8bbf0413aa9/share/facebook
