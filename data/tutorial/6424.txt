   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

how did we build book recommender systems in an hour part 2         k nearest
neighbors and id105

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   sep 19, 2017
   [1*fds5bvzpsedtotix3ntb1w.png]

   in the [18]last post, we saw how we could use simple correlational
   techniques to create a measure of similarity between books    users based
   on their rating records. in this post, we will show how you can use
   these same sorts of similarity metrics, to make recommendations to
   readers.

id185 using k-nearest neighbors (knn)

   knn is a machine learning algorithm to find clusters of similar users
   based on common book ratings, and make predictions using the average
   rating of top-k nearest neighbors. for example, we first present
   ratings in a matrix, with the matrix having one row for each item
   (book) and one column for each user, like so:
   [1*pyl_ivlsasnvcdoyfijppg.png]

   we then find the k item that have the most similar user engagement
   vectors. in this case, nearest neighbors of item id 5= [7, 4, 8,    ].
   now let   s implement knn into our book recommender system.

data

   we are using the same book data we used the last time: it consists of
   three tables: ratings, books info, and users info. i downloaded these
   three tables from [19]here.
   [1*gwncpylt3qtnfs9ckywddq.png]

   rating info
   [1*z4q2w0h96gqpqhazrywrtq.png]

   user info
   [1*xox0hrlal1zrrp0hvqx0oa.png]

   book info
   [1*vjvuf--yfid98z_olkgfmzq.png]

to ensure statistical significance, we will be only looking at the
popular books

   in order to find out which books are popular, we need to combine book
   data with rating data.
   [1*q_k23-ha2ex-4uwk4_3_9g.png]

   we then group by book titles and create a new column for total rating
   count.
   [1*fz3fkh_mw3eu06byi9visg.png]

   we now combine the rating data with the total rating count data, this
   gives us exactly what we need to filter out the lesser known books.
   [1*z_uulmtutdivurawrbjtbg.png]

   let   s look at the statistics of total rating count:
   [1*92gxvrtvnaheblqk1bp84g.png]

   the median book has been rated only once. let   s look at the top of the
   distribution:
   [1*pbppsg4abteftvklibbniw.png]

   about 1% of the books received 50 or more ratings. because we have so
   many books in our data, we will limit it to the top 1%, and this will
   give us 2713 unique books.
   [1*tt8dugi8t0flk6jrpbycaw.png]

filter to users in us and canada only

   in order to improve computing speed, and not run into the    memoryerror   
   issue, i will limit our user data to those in the us and canada. and
   then combine the user data with rating data and total rating count
   data.
   [1*bk5pqlmgzpieifk9kxe1ag.png]

implementing knn

   we convert our table to a 2d matrix, and fill the missing values with
   zeros (since we will calculate distances between rating vectors). we
   then transform the values(ratings) of the matrix dataframe into a scipy
   sparse matrix for more efficient calculations.

finding the nearest neighbors

   we use unsupervised algorithms with sklearn.neighbors. the algorithm we
   use to compute the nearest neighbors is    brute   , and we specify
      metric=cosine    so that the algorithm will calculate the cosine
   similarity between rating vectors. finally, we fit the model.
   [1*au0ibptnpcpiwi__vduila.png]

test our model and make some recommendations:

   in this step, the knn algorithm measures distance to determine the
      closeness    of instances. it then classifies an instance by finding its
   nearest neighbors, and picks the most popular class among the
   neighbors.
   [1*ve1c64oh6j7lbimkbwd-xw.png]

   perfect! green mile series books definitely should be recommended, one
   after another.

id185 using id105

   [1*igyfuxfvim7yblpmgmwzoa.png]
   source: [20]ohai

   id105 is simply a mathematical tool for playing around
   with matrices. the id105 techniques are usually more
   effective, because they allow users to discover the latent
   (hidden)features underlying the interactions between users and items
   (books).

   we use singular value decomposition (svd)         one of the matrix
   factorization models for identifying latent factors.

   similar with knn, we convert our usa canada user rating table into a 2d
   matrix (called a utility matrix here) and fill the missing values with
   zeros.
   [1*23hbi849cdvhqvexzhvmgw.png]

   we then transpose this utility matrix, so that the booktitles become
   rows and userids become columns. after using truncatedsvd to decompose
   it, we fit it into the model for id84. this
   compression happened on the dataframe   s columns since we must preserve
   the book titles. we choose n_components = 12 for just 12 latent
   variables, and you can see, our data   s dimensions have been reduced
   significantly from 40017 x 2442 to 2442 x 12.
   [1*gh85zirveucpetorkhwjyw.png]

   we calculate the pearson   s r correlation coefficient for every book
   pair in our final matrix. to compare this with the results from knn, we
   pick the same book    the green mile: coffey   s hands (green mile series)   
   to find the books that have high correlation coefficients (between 0.9
   and 1.0) with it.
   [1*pfaeevf8d3suain8bjackq.png]

   there you have it!
   [1*agtoyhi2wf04nj-tebse5w.png]

   not too shabby! our system can beat amazon   s, don   t you think? look at
   the title screen shot!

   reference: [21]music recommendations

     * [22]machine learning
     * [23]deep learning
     * [24]python
     * [25]data science
     * [26]id126

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of susan li

[28]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [29]https://www.linkedin.com/in/susanli/

     (button) follow
   [30]towards data science

[31]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [32]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [33]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c04b3c2ef55c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-part-2-k-nearest-neighbors-and-matrix-c04b3c2ef55c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/how-did-we-build-book-recommender-systems-in-an-hour-part-2-k-nearest-neighbors-and-matrix-c04b3c2ef55c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_uchdtlqvapj5---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. https://medium.com/@actsusanli/how-did-we-build-book-recommender-systems-in-an-hour-the-fundamentals-dfee054f978e
  19. http://www2.informatik.uni-freiburg.de/~cziegler/bx/
  20. http://katbailey.github.io/page/2/
  21. https://beckernick.github.io/music_recommender/
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/deep-learning?source=post
  24. https://towardsdatascience.com/tagged/python?source=post
  25. https://towardsdatascience.com/tagged/data-science?source=post
  26. https://towardsdatascience.com/tagged/recommendation-system?source=post
  27. https://towardsdatascience.com/@actsusanli?source=footer_card
  28. https://towardsdatascience.com/@actsusanli
  29. https://www.linkedin.com/in/susanli/
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/?source=footer_card
  32. https://towardsdatascience.com/
  33. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  35. https://medium.com/p/c04b3c2ef55c/share/twitter
  36. https://medium.com/p/c04b3c2ef55c/share/facebook
  37. https://medium.com/p/c04b3c2ef55c/share/twitter
  38. https://medium.com/p/c04b3c2ef55c/share/facebook
