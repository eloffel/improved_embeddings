   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

deep learning book notes, chapter 2: id202 for deep learning

   [14]go to the profile of adrien lucas ecoffet
   [15]adrien lucas ecoffet (button) blockedunblock (button)
   followfollowing
   mar 16, 2018
   [1*vq74dmngsvrixuswuonwjw.jpeg]

   these are my notes for chapter 2 of the deep learning book. they can
   also serve as a quick intro to id202 for deep learning.

   my notes for chapter 1 can be found below:
   [16]deep learning book notes, chapter 1
   these are my notes on the deep learning book. there are many like them
   but these ones are mine.becominghuman.ai

   for this section i decided to make things a bit more intuitive using
   code, which should be appealing to the many of us who are coders first
   and math people second (or eighth). this means that my    notes    are
   probably on the same order of length as the chapter itself, but
   hopefully they are extra useful for people out there (and for myself,
   frankly).
   [17][1*s_e42x8_h-yhrgnfjlosoq.png]

   the code i am including will be in the form of images because uploading
   chunks of notebooks to medium is quite hard. to see the full notebook
   itself, go [18]here.

book recommendations

recommendations from the deep learning book

     * [19]the matrix cookbook by petersen and pedersen, for people who
       need a refresher.
       my comment: i haven   t (yet) read it, but the pdf is free (linked
       above), so probably good to check out.
     * [20]id202 by shilov, for a full course.
       my comment: i haven   t (yet?) read it. seems to be getting good
       reviews and very cheap (only 10 bucks). apparently pretty dense.

top 3 most popular ai articles:

     [21]1. making a simple neural network

     [22]2. google will beat apple at its own game with superior ai

     [23]3. the ai job wars: episode i

my recommendations

     * [24]essence of id202 by 3blue1brown, for building
       intuition.
       my comment: this is an amazing youtube playlist about linear
       algebra. i highly recommend you watch it. a much easier option than
       all of the rest since it is based on videos, but won   t give you as
       much practice.
     * [25]id202 and its applications by strang, for a full
       course.
       my comment: this was/is my main book for id202. engaging
       presentation and lots of applications.
       an alternative would be [26]introduction to id202, by the
       same author. my understanding is that this book is at the same time
       more purely mathematical but also doesn   t go as far as    and its
       applications   .
       strang   s mit video lectures are also available [27]here.
     * [28]id202 done right by axler, for a full course.
       my comment: i haven   t yet read it, but i am planning to start soon.
       axler supposedly takes a pretty different approach to teaching
       id202 that is more focused on pure math than on
       applications (so perhaps less applicable for deep learning) but
       also gives a different perspective on the field, which is why i   m
       interested in this book as a second look at id202. it   s
       supposed to be one of the best (albeit a bit controversial) books
       on the subject.
     * the end of [29]calculus, vol 1 by apostol and much of [30]calculus,
       vol 2, for a full course including multivariable calculus.
       my comment: currently reading. this is particularly useful because
       it puts more emphasis on the multivariable calculus aspect while at
       the same time teaching id202. the one think that makes me
       sad is that the style is not nearly as engaging as in spivak   s
       [31]calculus (considered to be apostol   s main    competitor   ), but
       sadly spivak doesn   t cover multivariable calculus nor linear
       algebra in any non-super-advanced book of his.

basic objects

     * scalar: a single number, written in italics. a scalar is a vector
       with a single entry.
       a scalar in python: x = 0.0.
     * vector: a 1d array of numbers, written in bold. you can    access   
       elements by using a subscript, eg x_1 is the first element of x.
       can be thought of as a point in nd space where n is the number of
       entries of the vector. a vector is a matrix with a single column.
       a vector of 10 zeros in python: v = np.zeros(10)
     * matrix: a 2d array of numbers. written in bold and uppercase.
       accessing elements is the same as with a vector but using two
       subscripts, the first for the row and the second for the column, eg
       a1,2a1,2 is the second element of the first row. a1,:a1,: is the
       entire first row. matrices can be transposed: atat is simply a with
       the columns made into rows and the rows made into columns. a matrix
       is a tensor that happens to have 2 dimensions.
       a 10x7 matrix of all ones in python: m = np.ones((10, 7))
     * tensor: a nd array of numbers. indexing is the same as with a
       matrix or vector.
       a 2x3x7x4 tensor of all -1 in python: t = -np.ones((2, 3, 7, 4)).

   all of these can be added to other objects of the same size (this is
   done elementwise) and multiplied by scalars (each element is multiplied
   by the scalar). the book permits    broadcasting   , in which an object of
   lower dimension can be added to an object of greater dimension (eg a
   matrix + a vector), in this case, for example, the vector is added to
   each row of the matrix.
   [1*7blkevb8nuwkbzbtpkcyiq.png]

multiplication

dot product

   unlike to the authors, i like to think of the dot product first and of
   id127 second, so let   s define the dot product first:

   the dot product of two vectors that we can call a and b can be obtained
   by first multiplying the each element of a by the corresponding element
   of b and then by taking the sum of the result. in mathematical
   notation, it becomes (if nn is the number of elements in both a and b):
   [1*ihhs4gce6ky5qmpeca4bfw.png]

   in python, we can either implement it directly by using multiplication
   and sum, or simply use the .dot function.
   [1*8lin4_y5wzmdwu4xbm1mmw.png]

id127

   to multiply matrix a and b into a matrix c, we simply make it so that
   each entry ci,j is the dot product of row i of a and column j of b, so
   we have, for all possible i and j:
   [1*4eniyop_nclshlef4bvxsw.png]

   this means that the length of every row of a must be the same as the
   length of every column of b, in other words that the number of columns
   of a is equal to the number of rows in b. c will have the same number
   of rows as a and the same number of columns as b. in other words, if a
   is of size m x n and b is of size n x k the multiplication is valid and
   c is of size m x k.

   often, a and b will both be    square    (meaning they have the same number
   of rows and columns), in which case c will have the same size as both
   of them.

   because id127 is so similar to the dot product, it is
   implemented using the .dotfunction in numpy as well.

   in fact, you could even say that the vector dot product is the same as
   the multiplication of a matrix with only one row with a matrix with
   only one column, and indeed this is why the notation used by the book
   for the vector dot product is not a   b but atb.
   [1*-twox4jjvpwwz8sqzl_--w.png]

   id127 is not commutative, ie ba   ab

   proof:
   [1*baepkb7i-8dpktsnsy9vsq.png]

   this seems to tell us everything we need to know about matrix
   multiplication, but there are actually a couple other important things
   to note before we move on.

   first, we viewed id127 on an entry-by-entry basis, and
   this is how the deep learning book (and most everybody else) presents
   it. however, it is sometimes useful to see id127 as
   involving either entire columns or entire rows! i will not go into
   details here, but gilbert strang explains this concept in his mit ocw
   course, you can see his full explanation of id127
   including the    entire columns    and    entire rows    aspects here:

   iframe: [32]/media/75e5e06d747137c57db66e900b5e743b?postid=af776cf52506

   secondly, since we   ve already talked about transposes, it is
   interesting to ask what the transpose of a multiplication is. as it
   turns out, transposing the result of a id127 is the
   same as multiplying the original transposed matrices in reverse order,
   or, in math:
   [1*n67_vl6wvuxokrs7ckjqpq.png]

   in code:
   [1*edejgzxftnvfzbuwf5msow.png]

   finally, note that there is a huge difference between matrix
   multiplication and element-wisemultiplication of two matrices. in the
   element-wise product, we simply multiply each element in one matrix by
   the corresponding element in the other, whereas in matrix
   multiplication, we use the dot product of a row and a vector.

   in numpy, id127 is implemented using the .dot function
   whereas you can perform element-wise multiplication using the *
   operator. in the book's math notation, element-wise multiplication is
   represented by    , though other operators are used by others. matrix
   multiplication can just be represented by stringing the matrices
   together.
   [1*kogy4l6updmkdodqcfrstg.png]

inverses

   the matrix i has a very important property: multiplying it with any
   other matrix does nothing at all! it is simply a matrix with 0s
   everywhere and 1s on the diagonal going from the upper left to the
   lower right. in python, it is always possible to create an identity
   matrix of a given size using the np.eyefunction (i -> eye, get it?).

   i is basically the equivalent of 1 in the matrix world (since
   multiplying by 1 does nothing to ordinary numbers).
   [1*5al9jln9cgzyntxxifbugw.png]

   most (but not all) square matrices have a special matrix called an
      inverse   , represented with a -1 superscript like so: a   1a   1. the
   inverse of a matrix functions very much like the inverse of a number,
   in that multiplying a matrix by its inverse gives the identity function
   (just like multiplying a number by its inverse gives 1). basically,
   multiplying by the inverse of a matrix is very similar to dividing by
   that matrix. so:
   [1*ojoueotim2dqugzb95qg6a.png]

   you can get the inverse of a matrix in numpy using np.linalg.inv.

   inverses can help us solve equations using matrices! suppose we have a
   the following equation:
   [1*drahoozpjvxnhistatm2pa.png]

   so a matrix a multiplies an unknown vector x to get a known vector b,
   what is the x that makes this possible?

   using the inverse we get:
   [1*dehvtswioyuy0qypt7jeog.png]

   and we can now compute x!
   [1*d4jzg7hcwqv2mcb7bbb8ag.png]
   [1*j984sgrt0hytm9o8lzv0ua.png]

   not all matrices have inverses! next we will see why.
   [1*ncwjf_wmrhvx4zf2khovpq.png]

spans

   let   s explore why our bad matrix couldn   t be inverted before. for this,
   the easiest way is to introduce a geometrical way of thinking about
   matrices and vectors. i highly recommend watching 3blue1brown   s videos
   (see resources above) to gain full intuition on this.

   you can think of any vector as a point in space. with a 2d vector this
   is simple, the first component is its location on the x axis and the
   second on the y axis. with 3d things are still relatively familiar,
   just add a z axis. with more dimensions, the vector is still a point in
   space, even though you can   t imagine it.

     to deal with a 14-dimensional space, visualize a 3-d space and say
        fourteen    to yourself very loudly. everyone does it.

     geoffrey hinton

   a id127 takes a point in space and moves it to another
   point in space, that   s all it does.

   viewing this this way, we can say that a matrix inverse takes a point
   that (conceptually at least) has been moved by the original matrix and
   finds the original location of that point (before it was moved).

   the problem is: do all matrices map every point to a single other
   point? what if a matrix map several source points to the same
   destination point? what if it never maps any point to a given
   destination? in either case, we can   t have an inverse!

   in our non-invertible matrix, both of these problems occur! there are
   at least two different vectors x that map to the point [0, 0], as we
   show in code below, and yet there is no way to get to the point [1, 1]
   (try it!)
   [1*dj66dcy0apylotkwdzv77g.png]

   to understand this, let   s visualize the way the two matrices transform
   different points. we   ll represent the points using a grid:
   [1*fwnxwpm8leo2zk-fuwvcdw.png]
   [1*wva3ipupqe8a65gc7tf6bq.png]

   so it turns out that our bad matrix projects the entire grid onto a
   line, whereas a just tilts things around a little bit. this means that
   any point outside of the line cannot have been created by bad matrix,
   and thus an inverse cannot exist! (and as we saw before, several points
   in 2d can give us the same point on the line, which is also a problem).

   this line is the span of our bad matrix (the set of all values it can
   map to). by contrast, the span of a is the entire plane, as can be
   guessed from the fact that it   s only slightly shearing the grid.

   as it turns out, matrices can only ever span linear spaces such as
   points, lines, planes and hyperplanes (a plane in more than 2
   dimensions). further, all these spaces always have to contain the point
   at the origin, since multiplying any matrix by the 0 vector always
   gives the 0 vector. only matrices that span the entire space they are
   in have an inverse.

   if we look more closely at our bad matrix, we notice something strange
   about its columns: the second column ([2, 4]) is exactly twice the
   first column ([1, 2])! as it turns out, this is exactly why our matrix
   doesn   t span the whole space!

   multiplying a matrix and a vector can be thought of combining the
   columns of the matrix based on the elements of the vector, so if i
   multiply a matrix m by the vector [1, 2, 3, 4], the final vector is 1
   times the first column of m plus 2 times the second column of m and so
   on. so whenever we multiply our bad matrix with a vector, the result
   can only ever be a multiple of the vector [1, 2], which indeed forms a
   line!

   a set of vectors is called    dependent    if it is possible to generate
   one of the vectors by multiplying and adding some of the other vectors
   (in our case, just multiplying). if the columns of a matrix are
   dependent, the matrix doesn   t span the whole space and can   t be
   inverted.

   also note that if the columns of a matrix are dependent, its rows are
   also dependent, and vice-versa. we won   t prove this here.

norms

   norms are a measure of the length of a vector. the most common types of
   norms are called the lp norms, and they are of the form:
   [1*etdvbuj5hkqq-rx3y0vdoa.png]

   the most common lp norms are the l1, l2 and l    norms, which you might
   already know under the names of mahnattan distance (the distance to go
   from the origin to the tip of the vector, if you can only move along an
   axis), euclidian distance (the distance to go from the origin to the
   tip if you can go in any direction you want) and maximum (of the
   absolute values), respectively.

   l    might seem like a weird name, but it is actually simply what happens
   as p reaches infinity.

   you can access the norms using np.linalg.norm.
   [1*klmbwfsdsk_4t-zkbx5s-q.png]

   finally note that we can measure a matrix using the same norms, but
   that sometimes people call norms on matrices differently! in
   particular, the frobenius norm is simply the l2 norm applied to a
   matrix. remember to use that word if you want to sound smart.

special matrices

     * diagonal matrix: only has non-zero entries on its upper left to
       lower right diagonal (the other diagonal doesn   t count!)
       numpy can create a diagonal from a vector using np.diag.

   [1*gdfq_8gbdkizjnyxwsj7vg.png]
     * symmetric matrix: equal to its own transpose (the entries are
       symmetric across the up-left to down-right diagonal).
       a matrix times its transpose is always symmetric.

   [1*p8xrclhh3v2bp4fsjvnnzw.png]
     * unit vector: a vector whose l2 norm is 1.
       you can make a vector into a unit vector by dividing by its l2
       norm.

   [1*8jcl3nmgzsftklngemp90a.png]
     * orthogonal vectors: two vectors whose dot product is 0.
       [0, 0] is orthogonal to every vector. non-zero orthogonal vectors
       are perpendicular.

   [1*aiue8oead1c59uqp9oijyg.png]
     * orthonormal vectors: two unit vectors who are also orthogonal.

   [1*7ufyvswtn9n4bfosev6ncg.png]
     * orthogonal matrix: a matrix whose columns (and rows) are mutually
       orthonormal (yes, it is called orthogonal but it is made up of
       orthonormal vectors). amazing property: the transpose of an
       orthogonal matrix is its own inverse!!!

   [1*a-9mtcl9pzf2ma9hzlpexa.png]

eigen-stuff

   the word    eigen    can seem scary, so i like to mentally replace it with
      special    (that   s not what the word actually means in german, but it   s
   good enough for our purposes). so whenever i write    eigenvector    or
      eigenvalue   , replace it with    special vector    and    special value   .

   so how are these vectors and values special? the eigenvectors are
   special because they only get stretched when they are multiplied by the
   matrix (ie their direction doesn   t change, only their length, and they
   might also be going    backwards    if they are stretched by a negative
   amount). the eigenvalues is the amount by which the vectors are
   stretched.

   eigen-stuff is accessible in numpy through np.linalg.eig. let's look at
   what they do. below the black vector is always the original one and the
   red vector is the transformed one.
   [1*9_qkiflfmklhnv4e4b7obq.png]
   [1*egvr6cflratsqemrtalhxg.png]

   here we see that the first (arbitrary) vector is not just stretched,
   but also slightly rotated by the matrix, so it clearly isn   t an
   eigenvector. we compute the eigenvectors and show how they get
   transformed and indeed they do not change direction! we also see that
   in the first case the transformed vector is much longer than the
   original vector, and in the second case it is much shorter, so the
   amount of stretching is per-vector.

   now the two important questions are:
     * does every matrix have eigenvectors and eigenvalues, and if so how
       many?
     * what is the point of all of this?

   the answer for the first question is    sorta. given the definition we
   have given so far, the answer should be an emphatic    no   , because there
   exist rotation matrices, which always rotate a given vector. since we
   have defined eigenvectors as    vectors that don   t get rotated when they
   are multiplied with the matrix    and since rotation matrices rotate
   every vector, they should not have any eigenvectors. however, if you
   call np.linalg.eig on a rotation matrix such as [[0, -1], [1, 0]]
   (which rotates vectors by 90 degrees), you will find that it won't
   error out! the reason is that if you use complex eigenvalues and
   eigenvectors, you can find eigen-stuff for every possible matrix.

   having to deal with complex numbers is not very convenient, of course.
   thankfully, many of the matrices we   ll encounter in practice will have
   real eigenvalues and eigenvectors.

   now as to what the point of all this is. it so happens that we can
   decompose a matrix using its eigenvectors. specifically, if we have a
   matrix a, we can put all its eigenvectors in the columns of a matrix v
   and all its eigenvalues along the diagonal of a diagonal matrix    and
   we find that:
   [1*mcatriwziqmdnskocjgz0q.png]

   an important way in which this is useful is that it allows us to
   multiply a matrix with itself repeatedly very efficiently: when we
   multiply a with itself, the v^{   1} of the leftmost decomposition
   cancels out the v of the rightmost decomposition, and we end up with:
   [1*wvb6w5anwxo99pdajvsa1w.png]

   because    is a diagonal matrix, taking it to a power simply involves
   taking each of its elements to that power, which is much faster than
   doing a lot of id127s.
   [1*uzd-irzrm55p3ziusxveqw.png]

   finally we should note that for all symmetric matrices, the eigenvector
   matrix is orthogonal, which means that its transpose is its own
   inverse, so we get:
   [1*db9hudya8dks6clvvv82ig.png]

   if a is symmetric, which is particularly useful.

svd and pseudoinverse

   in the previous section we found that eigenvalues were convenient but
   sadly didn   t apply to all matrices: specifically, for some real square
   matrices they required the use of complex numbers, and they of course
   don   t work at all for non-square matrices.

   singular value decomposition (svd) tries to solve this problem by
   providing a decomposition with two orthogonal matrices (u and v) and
   one diagonal matrix (d), such that:
   [1*pdcxpy1z6h2snze6scky1q.png]

   the book does not go into many details about the uses of svd (though
   there are many) except for finding the moore-penrose pseudoinverse.

   svd can be accessed using np.linalg.svd.
   [1*kxoz4drof4kmpch7gay1ca.png]

   the moore-penrose pseudoinverse can be computed using svd. how doesn   t
   matter too much here, the point is what it can do: basically it can
   find the an    inverse    for non-invertible matrices. of course, because
   they are not invertible, the    inverse    will lack some properties, but
   it will still be quite useful.

   as you might recal, the inverse was useful for solving equations like:
   [1*18ewrlefs2e8ostapdzm1a.png]

   in which case you could find x using:
   [1*vhzxhae0jpco0zpdvmyfaw.png]

   with the non-invertible, there might not be an x that satisfies ax=b,
   but the pseudoinverse can find the x that comes closest (by minimizing
   the l2 distance between ax and b).

   the moore-penrose pseudoinverse is accessible by using np.linalg.pinv.
   [1*o3ap7g5wpifo9d_97zktjw.png]

trace and determinant

   the trace is simply an operator that sums the entries along the
   diagonals of a matrix. it has some interesting properties (eg the trace
   of a transpose is the same as the original trace and the order of
   id127 doesn   t matter within the trace operator), and
   its main use is to simplify some math by getting rid of explicit
   summing in equations in some cases.

   it is accessible as np.trace
   [1*pfkgjcvgwqr10epneamofw.png]

   the determinant is explained very quickly in the book, although it has
   many interesting properties. suffice it to say that it is a single
   number that describes a matrix, it has the following properties:
     * if det(a) is 0, then a is singular
     * it is the product of the eigenvalues
     * it can be thought of as the amount to which multiplying by the
       matrix streches space: if the determinant is 2, then the matrix can
       be thought of as doubling the volume of space. if it is one, it
       doesn   t stretch space at all.

   finally, the chapter ends on a derivation of pca with a single
   component. it is a cool example of derivation but i won   t go through it
   here since it doesn   t really introduce new material, just shows how to
   use what we saw above.

   that   s it for this week   s notes. i think i   ll keep translating the
   book   s concepts into python code, it enlivens things a bit and makes
   them more concrete, but maybe next time   s notes will be shorter: this
   was a lot of work!

   iframe: [33]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=af776cf52506

   [34][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [35][1*v-ppfkswhbvlwwamsvhhwg.png]
   [36][1*wt2auqisieaozxj-i7brdq.png]

     * [37]machine learning
     * [38]ai
     * [39]artificial intelligence
     * [40]deep learning
     * [41]neural networks

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [42]go to the profile of adrien lucas ecoffet

[43]adrien lucas ecoffet

     (button) follow
   [44]becoming human: artificial intelligence magazine

[45]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [46]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [47]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/af776cf52506
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/deep-learning-book-notes-chapter-2-linear-algebra-for-deep-learning-af776cf52506&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/deep-learning-book-notes-chapter-2-linear-algebra-for-deep-learning-af776cf52506&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_gthxl264yopy---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@adrienle?source=post_header_lockup
  15. https://becominghuman.ai/@adrienle
  16. https://becominghuman.ai/deep-learning-book-notes-chapter-1-b310837c76cf
  17. http://bit.ly/businessofbots
  18. https://github.com/adrienle/dl_book_notes/blob/master/02.ipynb
  19. http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf
  20. https://www.amazon.com/linear-algebra-dover-books-mathematics-ebook/dp/b00a73ixrc
  21. https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20
  22. https://becominghuman.ai/google-will-beat-apple-at-its-own-game-with-superior-ai-534ab3ada949
  23. https://becominghuman.ai/the-ai-job-wars-episode-i-c18e932ff225
  24. https://www.youtube.com/watch?v=kjboeszcoqc&list=plzhqobowtqdpd3mizzm2xvfitgf8he_ab
  25. https://www.amazon.com/linear-algebra-its-applications-4th/dp/0030105676
  26. https://www.amazon.com/introduction-linear-algebra-gilbert-strang/dp/0980232775/ref=pd_lpo_sbs_14_t_0?_encoding=utf8&psc=1&refrid=g2317mhw3ey2zxnmbkn0
  27. https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/
  28. https://www.amazon.com/linear-algebra-right-undergraduate-mathematics/dp/0387982582
  29. https://www.amazon.com/calculus-vol-1-tom-m-apostol/dp/8126515198/ref=sr_1_1?s=books&ie=utf8&qid=1520987006&sr=1-1&keywords=apostol+calculus
  30. https://www.amazon.com/calculus-ii-2nd-tom-apostol/dp/8126515201/ref=sr_1_3?s=books&ie=utf8&qid=1520987006&sr=1-3&keywords=apostol+calculus
  31. https://www.amazon.com/calculus-4th-michael-spivak/dp/0914098918/ref=sr_1_1?s=books&ie=utf8&qid=1520987248&sr=1-1&keywords=spivak+calculus
  32. https://becominghuman.ai/media/75e5e06d747137c57db66e900b5e743b?postid=af776cf52506
  33. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=af776cf52506
  34. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  35. https://upscri.be/8f5f8b
  36. https://becominghuman.ai/write-for-us-48270209de63
  37. https://becominghuman.ai/tagged/machine-learning?source=post
  38. https://becominghuman.ai/tagged/ai?source=post
  39. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  40. https://becominghuman.ai/tagged/deep-learning?source=post
  41. https://becominghuman.ai/tagged/neural-networks?source=post
  42. https://becominghuman.ai/@adrienle?source=footer_card
  43. https://becominghuman.ai/@adrienle
  44. https://becominghuman.ai/?source=footer_card
  45. https://becominghuman.ai/?source=footer_card
  46. https://becominghuman.ai/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://becominghuman.ai/deep-learning-book-notes-chapter-1-b310837c76cf
  50. https://medium.com/p/af776cf52506/share/twitter
  51. https://medium.com/p/af776cf52506/share/facebook
  52. https://medium.com/p/af776cf52506/share/twitter
  53. https://medium.com/p/af776cf52506/share/facebook
