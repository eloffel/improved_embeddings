   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

deep learning book notes, chapter 1

   [14]go to the profile of adrien lucas ecoffet
   [15]adrien lucas ecoffet (button) blockedunblock (button)
   followfollowing
   mar 9, 2018
   [1*kd4kdnrxolztjbxfhw2dpa.jpeg]

   these are my notes on the [16]deep learning book. there are many like
   them but these ones are mine.

   they are all based on my second reading of the various chapters, and
   the hope is that they will help me solidify and review the material
   easily. if they can help someone out there too, that   s great.

   the notes are also available on [17]github.

chapter 1: introduction

   ai was initially based on finding solutions to reasoning problems
   (symbolic ai), which are usually difficult for humans. however, it
   quickly turned out that problems that seem easy for humans (such as
   vision) are actually much harder.

top 3 most popular ai articles:

     [18]1. how i used machine learning as inspiration for physical
     paintings

     [19]2. ms or startup job         which way to go to build a career in deep
     learning?

     [20]3. top 100 medium articles related with artificial intelligence
     / machine learning    / deep learning (until jan 2017)

   you need a lot of knowledge about the world to solve these problems,
   but attempts to hard code such knowledge has consistently failed so
   far. instead, machine learning usually does better because it can
   figure out the useful knowledge for itself.

representations

   good representations are important: if your representation of the data
   is appropriate for the problem, it can become easy.

   for example, see the figure below: in cartesian coordinates, the
   problem isn   t linearly separable, but in polar coordinates it is. the
   polar representation is more useful for this problem.
   [0*adngpayzctxski4v.png]

   unfortunately, good representations are hard to create: eg if we are
   building a car detector, it would be good to have a representation for
   a wheel, but wheels themselves can be hard to detect, due to
   perspective distortions, shadows etc.!

   the solution is to learn the representations as well. this is one of
   the great benefits of deep learning, and in fact historically some of
   the representations learned by deep learning algorithms in minutes have
   permitted better algorithms than those that researchers had spent years
   to fine-tune!

   good representations are related to the factors of variation: these are
   underlying facts about the world that account for the observed data.
   for instance, factors of variation to explain a sample of speech could
   include the age, sex and accent of the speaker, as well as what words
   they are saying.

   unfortunately, there are a lot of factors of variation for any small
   piece of data. how do you disentangle them? how do you figure out what
   they are in the first place?

   the deep learning solution is to express representations in terms of
   simpler representations: eg a face is made up of contours and corners,
   which themselves are made up of edges etc.. it   s representations all
   the way down! (well, not really).

   below is an example of the increasingly complex representations
   discovered by a convolutional neural network.
   [0*xrvdg4jkd5mgmxo4.png]

   there is another way of thinking about deep network than as a sequence
   of increasingly complex representations: instead, we can simply think
   of it as a form of computation: each layer does some computation and
   stores its output in memory for the next layer to use. in this
   interpretation, the outputs of each layer don   t need to be factors of
   variation, instead they can be anything computationally useful for
   getting the final result.

meaning of    deep   

   how deep a network is depends on your definition of depth. there is no
   universal definition of depth although in practice many people count
      layers    as defined by a id127 followed by an
   activation function and maybe some id172 etc.. you could also
   count elementary operations in which case the id127,
   activation, id172 etc. would all add to the depth individually
   etc.. some networks such as resnet (not mentioned in the book) even
   have a notion of    block    (a resnet block is made up of two layers), and
   you could count those instead as well.

   the book also mentioned that yet another definition of depth is the
   depth of the graph by which concepts are related to each other. in this
   case, you could move back from complex representations to simpler
   representations, thus implicitly increasing the depth. their example is
   that you can infer a face from, say, a left eye, and from the face
   infer the existence of the right eye. to be honest i don   t fully
   understand this definition at this point. according to the book it is
   related to deep probabilistic models.

history

   deep learning is not a new technology: it has just gone through many
   cycles of rebranding!

   it was called    cybernetics    from the 40s to the 60s,    connectionism   
   from the 80s to the 90s and now deep learning from 2006 to the present.
   the networks themselves have been called id88s, adaline
   (id88 was for classification and adaline for regression),
   multilayer id88 (mlp) and id158s. the most
   common names nowadays are neural networks and mlps.

   a quick history of neural networks, pieced together from the book and
   other things that i   m aware of:
     * 1940s to 1960s: neural networks (cybernetics) are popular under the
       form of id88s and adaline. they typically use only a single
       layer though people are aware of the possibility of multilayer
       id88s (they just don   t know how to train them). in 1969,
       marvin minsky and seymour papert publish    id88s    and prove
       that single-layer id88s can   t learn even simple functions
       like xor. neural networks fall out of fashion.
     * 1980s to mid-1990s: id26 is first applied to neural
       networks, making it possible to train good multilayer id88s.
       in the 1990s, significant progress is made with recurrent neural
       networks, including the invention of lstms. by the mid-1990s
       however, neural networks start falling out of fashion due to their
       failure to meet exceedingly high expectations and the fact that
       id166s and id114 start gaining success: unlike neural
       networks, many of their properties are much more provable, and they
       were thus seen as more rigorous. this led to what jeremy howard
       calls the    [21]id166 winter   .
     * 2006 to 2012: geoffrey hinton manages to train id50
       efficiently. later groups show that many similar networks can be
       trained in a similar way. many neural networks start outperforming
       other systems. much of the focus is still on unsupervised learning
       on small dataset.
     * 2012 to today: neural networks become dominant in machine learning
       due to major performance breakthroughs. the focus shifts to
       supervised learning on large datasets. breakthroughs include:
     * in 2012, a deep neural net brought down the error rate on image net
       from 26.1% to 15.3%. current error rate: 3.6%.
     * cutting id103 error in half in many situations.
     * superhuman performance in traffic sign classification.
     * neural nets label an entire sequence instead of each element in the
       sequence (for street numbers).
     * revolutionized machine translation.
     * id63s can read and write from memory cells. can
       learn simple programs (eg sorting).
     * id23: can play atari games with human level
       performance. improve robotics.
     * can help design new drugs, search for subatomic particles, parse
       microscope images to construct 3d map of human brain etc..

factors

   here are some factors which, according to the book, helped deep
   learning become a dominant form of machine learning today:
     * bigger datasets: deep learning is a lot easier when you can provide
       it with a lot of data, and as the information age progresses, it
       becomes easier to collect large datasets.
     * rule of thumb: good performance with around 5,000 examples, human
       performance with around 10 million examples.

   [0*_pwwacr2kr8l1-mf.png]
     * bigger models: more computation = bigger network. we know from
       observing the brain that having lots of neurons is a good thing.
     * two factors: number of neurons and connections per neuron.
     * because deep learning typically uses dense networks, the number of
       connections per neuron is actually not too far from humans.

   [0*m_4ydo0dhatjabjn.png]
     * number of neurons is still way behind.

   [0*hzvjbgcsrguonsye.png]
     * won   t have as many neurons as human brains until 2050 unless major
       computational progress is made. and we might need more than that
       because each human neuron is more complex than a deep learning
       neuron.
     * better performance = better real world impact: current networks are
       more accurate and do not need, say, pictures to be cropped near the
       object to classify anymore. can recognize thousands of different
       classes.
     * see all the breakthroughs above

connection to neuroscience

   deep learning models are usually not designed to be realistic brain
   models. deep learning is based a more general principle of learning
   multiple levels of composition.

   why are we not trying to be more realistic? because we can   t know
   enough about the brain right now! but we do know that whatever the
   brain is doing, it   s very generic: experiments have shown that it is
   possible for animals to learn to    see    using their auditory cortex:
   this gives us hope that a generic learning algorithm is possible.

   some aspects of neuroscience that influenced deep learning:
     * the concept that many simple computations is what makes animals
       intelligent.
     * the neocognitron model of the mamalian visual system inspired
       convolutional neural networks
     * similarly, relu is a simplified version of the function in
       cognitron, which is based on brain function knowledge
     * although it is simplified, so far greater realism generally doesn   t
       improve performance.

   so far brain knowledge has mostly influenced architectures, not
   learning algorithms. on a personal level, this is why i   m interested in
   metalearning, which promises to make learning more biologically
   plausible.

   neuroscience is certainly not the only important field for deep
   learning, arguably more important are applied math (id202,
   id203, id205 and numerical optimization in
   particular). some deep learning researchers don   t care about
   neuroscience at all.

   actual brain simulation and models for which biological plausibility is
   the most important thing is more the domain of computational
   neuroscience.

   iframe: [22]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=b310837c76cf

   [23][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [24][1*v-ppfkswhbvlwwamsvhhwg.png]
   [25][1*wt2auqisieaozxj-i7brdq.png]

     * [26]artificial intelligence
     * [27]deep learning
     * [28]ai
     * [29]machine learning
     * [30]neural networks

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of adrien lucas ecoffet

[32]adrien lucas ecoffet

     (button) follow
   [33]becoming human: artificial intelligence magazine

[34]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 1k
     * (button)
     *
     *

   [35]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [36]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b310837c76cf
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/deep-learning-book-notes-chapter-1-b310837c76cf&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/deep-learning-book-notes-chapter-1-b310837c76cf&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_dxouuyqpv07k---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@adrienle?source=post_header_lockup
  15. https://becominghuman.ai/@adrienle
  16. http://www.deeplearningbook.org/
  17. https://github.com/adrienle/dl_book_notes
  18. https://becominghuman.ai/digital-processes-inspiring-analog-paintings-a358eb7801a0
  19. https://becominghuman.ai/ms-or-startup-job-which-way-to-go-to-build-a-career-in-deep-learning-2d62a5b10f9e
  20. https://becominghuman.ai/top-100-medium-com-c2695ab3270c
  21. https://twitter.com/jeremyphoward/status/961398047555993600
  22. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=b310837c76cf
  23. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  24. https://upscri.be/8f5f8b
  25. https://becominghuman.ai/write-for-us-48270209de63
  26. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  27. https://becominghuman.ai/tagged/deep-learning?source=post
  28. https://becominghuman.ai/tagged/ai?source=post
  29. https://becominghuman.ai/tagged/machine-learning?source=post
  30. https://becominghuman.ai/tagged/neural-networks?source=post
  31. https://becominghuman.ai/@adrienle?source=footer_card
  32. https://becominghuman.ai/@adrienle
  33. https://becominghuman.ai/?source=footer_card
  34. https://becominghuman.ai/?source=footer_card
  35. https://becominghuman.ai/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/b310837c76cf/share/twitter
  39. https://medium.com/p/b310837c76cf/share/facebook
  40. https://medium.com/p/b310837c76cf/share/twitter
  41. https://medium.com/p/b310837c76cf/share/facebook
