   #[1]jeremy jordan

   [2]jeremy jordan
     * [3]home
     * [4]about
     * [5]data science
     * [6]reading list
     * [7]quotes
     * [8]life
     * [9]favorite talks
     * [10]materials science

   [11]subscribe

   18 july 2017 / [12]data science

neural networks: training with id26.

   in my first post on neural networks, i discussed a [13]model
   representation for neural networks and how we can feed in inputs and
   calculate an output. we calculated this output, layer by layer, by
   combining the inputs from the previous layer with weights for each
   neuron-neuron connection. i mentioned that we'd talk about how to find
   the proper weights to connect neurons together in a future post - this
   is that post!

overview

   in the previous post i had just assumed that we had magic prior
   knowledge of the proper weights for each neural network. in this post,
   we'll actually figure out how to get our neural network to "learn" the
   proper weights. however, for the sake of having somewhere to start,
   let's just initialize each of the weights with random values as an
   initial guess. we'll come back and revisit this random initialization
   step later on in the post.

   given our randomly initialized weights connecting each of the neurons,
   we can now feed in our matrix of observations and calculate the outputs
   of our neural network. this is called forward propagation. given that
   we chose our weights at random, our output is probably not going to be
   very good with respect to our expected output for the dataset.

   so where do we go from here?

   well, for starters, let's define what a "good" output looks like.
   namely, we'll develop a cost function which penalizes outputs far from
   the expected value.

   next, we need to figure out a way to change the weights so that the
   cost function improves. any given path from an input neuron to an
   output neuron is essentially just a composition of functions; as such,
   we can use partial derivatives and the chain rule to define the
   relationship between any given weight and the cost function. we can use
   this knowledge to then leverage [14]id119 in updating each
   of the weights.

pre-requisites

   when i was first learning id26, many people tried to
   abstract away the underlying math (derivative chains) and i was never
   really able to grasp what the heck was going on until watching
   [15]professor winston's lecture at mit. i hope that this post gives a
   better understanding of id26 than simply "this is the step
   where we send the error backwards to update the weights".

   in order to fully grasp the concepts discussed in this post, you should
   be familiar with the following:

   partial derivatives
   this post is going to be a bit dense with a lot of partial derivatives.
   however, it is my hope that even a reader without prior knowledge of
   multivariate calculus can still follow the logic behind
   id26.

   if you're not familiar with calculus, $\frac{{\partial f\left( x
   \right)}}{{\partial x}}$ will probably look pretty foreign. you can
   interpret this expression as "how does $f\left( x \right)$ change as i
   change $x$?" this will be useful because we can ask questions like "how
   does the cost function change when i change this parameter? does it
   increase or decrease the cost function?" in search of the optimal
   parameters.

   if you'd like to brush up on multivariate calculus, check out [16]khan
   academy's lessons on the subject.

   id119
   to prevent this post from getting too long, i've separated the topic of
   id119 into another post. if you're not familiar with the
   method, be sure to read about it [17]here and understand it before
   continuing this post.

   id127
   [18]here is a quick refresher from khan academy.

starting simple

   to figure out how to use id119 in training a neural network,
   let's start with the simplest neural network: one input neuron, one
   hidden layer neuron, and one output neuron.

   to show a more complete picture of what's going on, i've expanded each
   neuron to show 1) the linear combination of inputs and weights and 2)
   the activation of this linear combination. it's easy to see that the
   forward propagation step is simply a series of functions where the
   output of one feeds as the input to the next.

defining "good" performance in a neural network

   let's define our cost function to simply be the squared error.

   [j\left( \theta \right) = {\raise0.5ex\hbox{$\scriptstyle 1$}
   \kern-0.1em/\kern-0.15em
   \lower0.25ex\hbox{$\scriptstyle 2$}}{\left( {y - {{\rm{a}}^{(3)}}}
   \right)^2}]

   there's a myriad of cost functions we could use, but for this neural
   network squared error will work just fine.

   remember, we want to evaluate our model's output with respect to the
   target output in an attempt to minimize the difference between the two.

relating the weights to the cost function

   in order to minimize the difference between our neural network's output
   and the target output, we need to know how the model performance
   changes with respect to each parameter in our model. in other words, we
   need to define the relationship (read: partial derivative) between our
   cost function and each weight. we can then update these weights in an
   iterative process using id119.

   [\frac{{\partial j\left( \theta \right)}}{{\partial {\theta _1}}} = ?]

   [\frac{{\partial j\left( \theta \right)}}{{\partial {\theta _2}}} = ?]

   let's look at $\frac{{\partial j\left( \theta \right)}}{{\partial
   {\theta _2}}}$ first. keep the following figure in mind as we progress.

   let's take a moment to examine how we could express the relationship
   between $j\left( \theta \right)$ and $\theta _2$. note how $\theta _2$
   is an input to ${z^{(3)}}$, which is an input to ${{\rm{a}}^{(3)}}$,
   which is an input to $j\left( \theta \right)$. when we're trying to
   compute a derivative of this sort, we can use the chain rule to solve.

   as a reminder, the chain rule states:
   [\frac{\partial }{{\partial z}}p\left( {q\left( z \right)} \right) =
   \frac{{\partial p}}{{\partial q}}\frac{{\partial q}}{{\partial z}}]

   let's apply the chain rule to solve for $\frac{{\partial j\left( \theta
   \right)}}{{\partial {\theta _2}}}$.

   [\frac{{\partial j\left( \theta \right)}}{{\partial {\theta _2}}} =
   \left( {\frac{{\partial j\left( \theta \right)}}{{\partial
   {{\rm{a}}^{(3)}}}}} \right)\left( {\frac{{\partial
   {{\rm{a}}^{(3)}}}}{{\partial z}}} \right)\left( {\frac{{\partial
   z}}{{\partial {\theta _2}}}} \right)]

   by similar logic, we can find $\frac{{\partial j\left( \theta
   \right)}}{{\partial {\theta _1}}}$.

   [\frac{{\partial j\left( \theta \right)}}{{\partial {\theta _1}}} =
   \left( {\frac{{\partial j\left( \theta \right)}}{{\partial
   {{\rm{a}}^{(3)}}}}} \right)\left( {\frac{{\partial
   {{\rm{a}}^{(3)}}}}{{\partial {z^{(3)}}}}} \right)\left(
   {\frac{{\partial {z^{(3)}}}}{{\partial {{\rm{a}}^{(2)}}}}}
   \right)\left( {\frac{{\partial {{\rm{a}}^{(2)}}}}{{\partial
   {z^{(2)}}}}} \right)\left( {\frac{{\partial {z^{(2)}}}}{{\partial
   {\theta _1}}}} \right)]

   for the sake of clarity, i've updated our neural network diagram to
   visualize these chains. make sure you are comfortable with this process
   before proceeding.

adding complexity

   now let's try this same approach on a slightly more complicated
   example. now, we'll look at a neural network with two neurons in our
   input layer, two neurons in one hidden layer, and two neurons in our
   output layer. for now, we'll disregard the bias neurons that are
   missing from the input and hidden layers.

     let's take a second to go over the notation i'll be using so you can
     follow along with these diagrams. the superscript ^(1) denotes what
     layer the object is in and the subscript denotes which neuron we're
     referring to in a given layer. for example $a_1^{(2)}$ is the
     activation of the first neuron in the second layer. for the
     parameter values $\theta$, i like to read them like a mailing label
     - the first value denotes what neuron the input is being sent to in
     the next layer, and the second value denotes from what neuron the
     information is being sent. for example, ${\theta _{21}^{(2)}}$ is
     used to send input to the 2^nd neuron, from the 1^st neuron in layer
     2. the superscript denoting the layer corresponds with where the
     input is coming from. this notation is consistent with the matrix
     representation we discussed in my post on neural networks
     representation.

   let's expand this network to expose all of the math that's going on.

   yikes! things got a little more complicated. i'll walk through the
   process for finding one of the partial derivatives of the cost function
   with respect to one of the parameter values; i'll leave the rest of the
   calculations as an exercise for the reader (and post the end results
   below).

   foremost, we'll need to revisit our cost function now that we're
   dealing with a neural network with more than one output. let's now use
   the mean squared error as our cost function.

   [j\left( \theta \right) = \frac{1}{{2m}}\sum{{{\left( {{y _i} -
   {\rm{a}} _i^{(2)}} \right)}^2}} ]

   note: if you're training on multiple observations (which you always
   will in practice), we'll also need to perform a summation of the cost
   function over all training examples. for this cost function, it's
   common to normalize by $\frac{1}{{m}}$ where $m$ is the number of
   examples in your training dataset.

   now that we've corrected our cost function, we can look at how changing
   a parameter affects the cost function. specifically, i'll calculate
   $\frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}}$ in this example. looking at the diagram, $\theta
   _{11}^{(1)}$ affects the output for both $a _1^{(3)}$ and $a _2^{(3)}$.
   because our cost function is a summation of individual costs for each
   output, we can calculate the derivative chain for each path and simply
   add them together.

   the derivative chain for the blue path is:
   $$ \left( {\frac{{\partial j\left( \theta \right)}}{{\partial {\rm{a}}
   _1^{(3)}}}} \right)\left( {\frac{{\partial {\rm{a}}
   _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left( {\frac{{\partial z
   _1^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}} \right)\left(
   {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z _1^{(2)}}}}
   \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right) $$

   the derivative chain for the orange path is:
   $$ \left( {\frac{{\partial j\left( \theta \right)}}{{\partial {\rm{a}}
   _2^{(3)}}}} \right)\left( {\frac{{\partial {\rm{a}}
   _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left( {\frac{{\partial z
   _2^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}} \right)\left(
   {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z _1^{(2)}}}}
   \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right)$$

   combining these, we get the total expression for $\frac{{\partial
   j\left( \theta \right)}}{{\partial \theta _{11}^{(1)}}}$.

   i've provided the remainder of the partial derivatives below. remember,
   we need these partial derivatives because they describe how changing
   each parameter affects the cost function. thus, we can use this
   knowledge to change all of the parameter values in a way that continues
   to decrease the cost function until we converge on some minimum value.

layer 2 parameters

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial \theta _{11}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{12}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial \theta _{12}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{21}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial \theta _{21}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{22}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial \theta _{22}^{(2)}}}} \right)$$

layer 1 parameters

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z
   _1^{(2)}}}} \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial
   \theta _{11}^{(1)}}}} \right) + \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z
   _1^{(2)}}}} \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial
   \theta _{11}^{(1)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{12}^{(1)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z
   _1^{(2)}}}} \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial
   \theta _{12}^{(1)}}}} \right) + \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z
   _1^{(2)}}}} \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial
   \theta _{12}^{(1)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{21}^{(1)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial {\rm{a}} _2^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _2^{(2)}}}{{\partial z
   _2^{(2)}}}} \right)\left( {\frac{{\partial z _2^{(2)}}}{{\partial
   \theta _{21}^{(1)}}}} \right) + \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial {\rm{a}} _2^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _2^{(2)}}}{{\partial z
   _2^{(2)}}}} \right)\left( {\frac{{\partial z _2^{(2)}}}{{\partial
   \theta _{21}^{(1)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{22}^{(1)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial {\rm{a}} _2^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _2^{(2)}}}{{\partial z
   _2^{(2)}}}} \right)\left( {\frac{{\partial z _2^{(2)}}}{{\partial
   \theta _{22}^{(1)}}}} \right) + \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial {\rm{a}} _2^{(2)}}}}
   \right)\left( {\frac{{\partial {\rm{a}} _2^{(2)}}}{{\partial z
   _2^{(2)}}}} \right)\left( {\frac{{\partial z _2^{(2)}}}{{\partial
   \theta _{22}^{(1)}}}} \right) $$

   woah there. we just went from a neural network with 2 parameters that
   needed 8 partial derivative terms in the previous example to a neural
   network with 8 parameters that needed 52 partial derivative terms. this
   is going to quickly get out of hand, especially considering many neural
   networks that are used in practice are much larger than these examples.

   fortunately, upon closer inspection many of these partial derivatives
   are repeated. if we're smart about how we approach this problem, we can
   dramatically reduce the computational cost of training. further, it'd
   really be a pain in the ass if we had to manually calculate the
   derivative chains for every parameter. let's look at what we've done so
   far and see if we can generalize a method to this madness.

generalizing a method

   let's examine the partial derivatives above and make a few
   observations. we'll start with looking at the partial derivatives with
   respect to the parameters for layer 2. remember, the parameters in for
   layer 2 are combined with the activations in layer 2 to feed as inputs
   into layer 3.

layer 2 parameters

   let's analyze the following expressions; i encourage you to solve the
   partial derivatives as we go along to convince yourself of my logic.
   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial \theta _{11}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{12}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _1^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _1^{(3)}}}{{\partial z _1^{(3)}}}} \right)\left(
   {\frac{{\partial z _1^{(3)}}}{{\partial \theta _{12}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{21}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial \theta _{21}^{(2)}}}} \right)
   $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{22}^{(2)}}} = \left( {\frac{{\partial j\left( \theta
   \right)}}{{\partial {\rm{a}} _2^{(3)}}}} \right)\left( {\frac{{\partial
   {\rm{a}} _2^{(3)}}}{{\partial z _2^{(3)}}}} \right)\left(
   {\frac{{\partial z _2^{(3)}}}{{\partial \theta _{22}^{(2)}}}} \right)$$

   foremost, it seems as if the columns contain very similar values. for
   example, the first column contains partial derivatives of the cost
   function with respect to the neural network outputs. in practice, this
   is the difference between the expected output and the actual output
   (and then scaled by $m$) for each of the output neurons.

   the second column represents the derivative of the activation function
   used in the output layer. note that for each layer, neurons will use
   the same activation function. a homogenous activation function within a
   layer is necessary in order to be able to leverage matrix operations in
   calculating the neural network output. thus, the value for the second
   column will be the same for all four terms.

   the first and second columns can be combined as $\delta _i^{(3)}$ for
   the sake of convenience later on down the road. it is not immediately
   evident why this would be helpful, but you'll see as we go backwards
   another layer why this is useful. people will often refer to this
   expression as the "error" term which we use to "send back error from
   the output throughout the network". we'll see why this is the case
   soon. each neuron in the network will have a corresponding $\delta$
   term that we'll solve for.

   [\delta _i^{(3)} = \frac{1}{m}\left( {{y _i} - a _i^{(3)}}
   \right)f'\left( {{a^{(3)}}} \right)]

   the third column represents how the parameter of interest changes with
   respect to the weighted inputs for the current layer; when you
   calculate the derivative this corresponds with the activation from the
   previous layer.

   i'd also like to note that the first two partial derivative terms seem
   concerned with the first output neuron (neuron 1 in layer 3) while the
   last two partial derivative terms seem concerned with the second output
   neuron (neuron 2 in layer 3). this is evident in the $\frac{{\partial
   j\left( \theta \right)}}{{\partial {\rm{a}}_i^{(3)}}}$ term. let's use
   this knowledge to rewrite the partial derivatives using the $\delta$
   expression we defined above.
   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(2)}}} = \delta _1^{(3)}\left( {\frac{{\partial z
   _1^{(3)}}}{{\partial \theta _{11}^{(2)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{12}^{(2)}}} = \delta _1^{(3)}\left( {\frac{{\partial z
   _1^{(3)}}}{{\partial \theta _{12}^{(2)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{21}^{(2)}}} = \delta _2^{(3)}\left( {\frac{{\partial z
   _2^{(3)}}}{{\partial \theta _{21}^{(2)}}}} \right) $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{22}^{(2)}}} = \delta _2^{(3)}\left( {\frac{{\partial z
   _2^{(3)}}}{{\partial \theta _{22}^{(2)}}}} \right) $$

   next, let's go ahead and calculate the last partial derivative term. as
   we noted, this partial derivative ends up representing activations from
   the previous layer.

   note: i find it helpful to use the expanded neural network graphic in
   the previous section when calculating the partial derivatives.
   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(2)}}} = \delta _1^{(3)}{\rm{a}} _1^{(2)} $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{12}^{(2)}}} = \delta _1^{(3)}{\rm{a}} _2^{(2)} $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{21}^{(2)}}} = \delta _2^{(3)}{\rm{a}} _1^{(2)} $$

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{22}^{(2)}}} = \delta _2^{(3)}{\rm{a}} _2^{(2)} $$

   it appears that we're combining the "error" terms with activations from
   the previous layer to calculate each partial derivative. it's also
   interesting to note that the indices $j$ and $k$ for $\theta _{jk}$
   match the combined indices of $\delta _j^{(3)}$ and ${\rm{a}}
   _k^{(2)}$.

   [\frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{jk}^{(2)}}} = \delta _j^{(3)}{\rm{a}} _k^{(2)}]

   let's see if we can figure out a matrix operation to compute all of the
   partial derivatives in one expression.

   ${\delta ^{(3)}}$ is a vector of length $j$ where $j$ is equal to the
   number of output neurons.
   [math: <msup> <mi>  </mi> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> <mo>=</mo><mrow><mo>[</mo> <mrow> <mtable> <mtr> <mtd> <mrow>
   <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>   </mo><msubsup> <mi>a</mi>
   <mn>1</mn> <mrow> <mo stretchy='false'>(</mo><mn>3</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> </mtr> <mtr>
   <mtd> <mrow> <msub> <mi>y</mi> <mn>2</mn> </msub> <mo>   </mo><msubsup>
   <mi>a</mi> <mn>2</mn> <mrow> <mo stretchy='false'>(</mo><mn>3</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> </mtr> <mtr>
   <mtd> <mo>   </mo> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>y</mi>
   <mi>j</mi> </msub> <mo>   </mo><msubsup> <mi>a</mi> <mi>j</mi> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> </mtr> </mtable> </mrow>
   <mo>]</mo></mrow><msup> <mi>f</mi> <mo>   </mo> </msup> <mrow><mo>(</mo>
   <mrow> <msup> <mi>a</mi> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> </mrow> <mo>)</mo></mrow> :math]

   ${\rm{a}}^{(2)}$ is a vector of length $k$ where $k$ is the number of
   neurons in the previous layer. the values of this vector represent
   activations of the previous layer calculated during forward
   propagation; in other words, it's the vector of inputs to the output
   layer.

   $$ {a^{(2)}} = \left[ {\begin{array}{* {20}{c}}
   {a _1^{(2)}}&{a _2^{(2)}}& \cdots &{a _k^{(2)}}
   \end{array}} \right] $$

   multiplying these vectors together, we can calculate all of the partial
   derivative terms in one expression.
   [math: <mfrac> <mrow> <mo>   </mo><mi>j</mi><mrow><mo>(</mo> <mi>  </mi>
   <mo>)</mo></mrow> </mrow> <mrow> <mo>   </mo><msubsup> <mi>  </mi> <mrow>
   <mi>i</mi><mi>j</mi> </mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mfrac> <mo>=</mo><mrow><mo>[</mo> <mrow> <mtable>
   <mtr> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup>
   <mi>  </mi> <mn>2</mn> <mrow> <mo stretchy='false'>(</mo><mn>3</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> </mtr>
   </mtable> </mrow> <mo>]</mo></mrow><mrow><mo>[</mo> <mrow> <mtable>
   <mtr> <mtd> <mrow> <msubsup> <mi>a</mi> <mn>1</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> <mtd> <mrow> <msubsup> <mi>a</mi> <mn>2</mn>
   <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo>
   </mrow> </msubsup> </mrow> </mtd> </mtr> </mtable> </mrow>
   <mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo> <mrow> <mtable> <mtr> <mtd>
   <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> <msubsup> <mi>a</mi> <mn>1</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>1</mn>
   <mrow> <mo stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo>
   </mrow> </msubsup> <msubsup> <mi>a</mi> <mn>2</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup>
   <mi>  </mi> <mn>2</mn> <mrow> <mo stretchy='false'>(</mo><mn>3</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> <msubsup> <mi>a</mi>
   <mn>1</mn> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> <mtd> <mrow>
   <msubsup> <mi>  </mi> <mn>2</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> <msubsup> <mi>a</mi> <mn>2</mn> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> </mtr> </mtable> </mrow> <mo>]</mo></mrow>
   :math]

   to summarize, see the graphic below.

   note: technically, the first column should also be scaled by $m$ to be
   an accurate derivative. however, my focus was to point out that this is
   the column where we're concerned with the difference between the
   expected and actual outputs. the $\delta$ term on the right has the
   full derivative expression.

layer 1 parameters

   now let's take a look and see what's going on in layer 1.

   whereas the weights in layer 2 only directly affected one output, the
   weights in layer 1 affect all of the outputs. recall the following
   graphic.

   this results in a partial derivative of the cost function with respect
   to a parameter now becoming a summation of different chains.
   specifically, we'll have a derivative chain for every $\delta$ we
   calculated in the next layer forward. remember, we started at the end
   of the network and are working our way backwards through the network.
   thus, the next layer forward represents the $\delta$ values that we
   previously calculated.

   as we did for layer 2, let's make a few observations.

   the first two columns (of each summed term) corresponds with a $\delta
   _j^{(3)}$ calculated in the next layer forward (remember, we started at
   the end of the network and are working our way back).

   the third column corresponds with some parameter that connects layer 2
   to layer 3. if we considered $\delta _j^{(3)}$ to be some "error" term
   for layer 3, and each derivative chain is now a summation of these
   errors, then this third column allows us to weight each respective
   error. thus, the first three terms combined represent some measure of
   the proportional error.

   we'll also redefine $\delta$ for all layers excluding the output layer
   to include this combination of weighted errors.

   $$ \delta _j^{(l)} = f'\left( {{a^{(l)}}} \right)\sum\limits _{i = 1}^n
   {\delta _i^{(l + 1)}\theta _{ij}^{(l)}} $$

   we could write this more succinctly using matrix expressions.

   $$ {\delta ^{(l)}} = {\delta ^{(l + 1)}}{\theta ^{(l)}}f'\left(
   {{a^{(l)}}} \right) $$

   note: it's standard notation to denote vectors with lowercase letters
   and matrices as uppercase letters. thus, $\theta$ represents a vector
   while $\theta$ represents a matrix.

   the fourth column represents the derivative of the activation function
   used in the current layer. remember that for each layer, neurons will
   use the same activation function.

   lastly, the fifth column represents various inputs from the previous
   layer. in this case, it is the actual inputs to the neural network.

   let's take a closer look at one of the terms, $\frac{{\partial j\left(
   \theta \right)}}{{\partial \theta _{11}^{(1)}}}$.

   foremost, we established that the first two columns of each derivative
   chains were previously calculated as $\delta _j^{(3)}$.

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \delta _1^{(3)}\left( {\frac{{\partial z
   _1^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}} \right)\left(
   {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z _1^{(2)}}}}
   \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right) + \delta _2^{(3)}\left( {\frac{{\partial z
   _2^{(3)}}}{{\partial {\rm{a}} _1^{(2)}}}} \right)\left(
   {\frac{{\partial {\rm{a}} _1^{(2)}}}{{\partial z _1^{(2)}}}}
   \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right) $$

   further, we established the the third columns of each derivative chain
   was a parameter that acted to weight each of the respective $\delta$
   terms. we also established that the fourth column was the derivative of
   the activation function.

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \delta _1^{(3)}\theta _{11}^{(2)}f'\left( {{a^{(2)}}}
   \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right) + \delta _2^{(3)}\theta _{21}^{(2)}f'\left(
   {{a^{(2)}}} \right)\left( {\frac{{\partial z _1^{(2)}}}{{\partial
   \theta _{11}^{(1)}}}} \right) $$

   factoring out $\left( {\frac{{\partial z_1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right)$,

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right)\left( {\delta _1^{(3)}\theta _{11}^{(2)}f'\left(
   {{a^{(2)}}} \right) + \delta _2^{(3)}\theta _{21}^{(2)}f'\left(
   {{a^{(2)}}} \right)} \right) $$

   we're left with our new definition of $\delta_j^{(l)}$. let's go ahead
   and substitute that in.

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \left( {\frac{{\partial z _1^{(2)}}}{{\partial \theta
   _{11}^{(1)}}}} \right)\left( {\delta _1^{(2)}} \right) $$

   lastly, we established the fifth column (${\frac{{\partial
   z_1^{(2)}}}{{\partial \theta_{11}^{(1)}}}}$) corresponded with an input
   from the previous layer. in this case, the derivative calculates to be
   $x_1$.

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{11}^{(1)}}} = \delta _1^{(2)}{x _1} $$

   again let's figure out a matrix operation to compute all of the partial
   derivatives in one expression.

   $\delta ^{(2)}$ is a vector of length $j$ where $j$ is the number of
   neurons in the current layer (layer 2). we can calculate $\delta
   ^{(2)}$ as the weighted combination of errors from layer 3, multiplied
   by the derivative of the activation function used in layer 2.
   [math: <msup> <mi>  </mi> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> <mo>=</mo><mrow><mo>[</mo> <mrow> <mtable> <mtr> <mtd> <mrow>
   <msubsup> <mi>  </mi> <mn>1</mn> <mrow> <msup> <mrow></mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> </mrow> </msubsup> </mrow> </mtd> <mtd> <mrow> <msubsup>
   <mi>  </mi> <mn>2</mn> <mrow> <msup> <mrow></mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>3</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> </mrow> </msubsup> </mrow> </mtd> </mtr> </mtable> </mrow>
   <mo>]</mo></mrow><mrow><mo>[</mo> <mrow> <mtable> <mtr> <mtd> <mrow>
   <msubsup> <mi>  </mi> <mrow> <mn>11</mn> </mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mrow>
   <mn>12</mn> </mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> </mtr> <mtr>
   <mtd> <mrow> <msubsup> <mi>  </mi> <mrow> <mn>21</mn> </mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mrow>
   <mn>22</mn> </mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msubsup> </mrow> </mtd> </mtr>
   </mtable> </mrow> <mo>]</mo></mrow><msup> <mi>f</mi> <mo>   </mo> </msup>
   <mrow><mo>(</mo> <mrow> <msup> <mi>a</mi> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> </mrow> <mo>)</mo></mrow><mo>=</mo><mrow><mo>[</mo> <mrow>
   <mtable> <mtr> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow>
   <msup> <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> </mrow>
   </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>2</mn> <mrow> <msup>
   <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> </mrow>
   </mtd> </mtr> </mtable> </mrow> <mo>]</mo></mrow> :math]

   $x$ is a vector of input values.

   multiplying these vectors together, we can calculate all of the partial
   derivative terms in one expression.
   [math: <mfrac> <mrow> <mtext>   </mtext><mi>j</mi><mrow><mo>(</mo>
   <mi>  </mi> <mo>)</mo></mrow> </mrow> <mrow> <mtext>   </mtext><msubsup>
   <mi>  </mi> <mrow> <mi>i</mi><mi>j</mi> </mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>1</mn><mo stretchy='false'>)</mo> </mrow>
   </msubsup> </mrow> </mfrac> <mo>=</mo><mrow><mo>[</mo> <mrow> <mtable>
   <mtr> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow> <msup>
   <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> </mrow>
   </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>2</mn> <mrow>
   <msup> <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> </mrow>
   </mtd> </mtr> </mtable> </mrow> <mo>]</mo></mrow><mrow><mo>[</mo>
   <mrow> <mtable> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub>
   </mrow> </mtd> <mtd> <mrow> <msub> <mi>x</mi> <mn>2</mn> </msub>
   </mrow> </mtd> </mtr> </mtable> </mrow>
   <mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo> <mrow> <mtable> <mtr> <mtd>
   <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow> <msup> <mrow></mrow>
   <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo>
   </mrow> </msup> </mrow> </msubsup> <msub> <mi>x</mi> <mn>1</mn> </msub>
   </mrow> </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>1</mn> <mrow>
   <msup> <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> <msub>
   <mi>x</mi> <mn>2</mn> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow>
   <msubsup> <mi>  </mi> <mn>2</mn> <mrow> <msup> <mrow></mrow> <mrow> <mo
   stretchy='false'>(</mo><mn>2</mn><mo stretchy='false'>)</mo> </mrow>
   </msup> </mrow> </msubsup> <msub> <mi>x</mi> <mn>1</mn> </msub> </mrow>
   </mtd> <mtd> <mrow> <msubsup> <mi>  </mi> <mn>2</mn> <mrow> <msup>
   <mrow></mrow> <mrow> <mo stretchy='false'>(</mo><mn>2</mn><mo
   stretchy='false'>)</mo> </mrow> </msup> </mrow> </msubsup> <msub>
   <mi>x</mi> <mn>2</mn> </msub> </mrow> </mtd> </mtr> </mtable> </mrow>
   <mo>]</mo></mrow> :math]

   if you've made it this far, congrats! we just calculated all of the
   partial derivatives necessary to use id119 and optimize our
   parameter values. in the next section, i'll introduce a way to
   visualize the process we've just developed in addition to presenting an
   end-to-end method for implementing id26. if you understand
   everything up to this point, it should be smooth sailing from here on
   out.

id26

   in the last section, we developed a way to calculate all of the partial
   derivatives necessary for id119 (partial derivative of the
   cost function with respect to all model parameters) using matrix
   expressions. in calculating the partial derivatives, we started at the
   end of the network and, layer by layer, worked our way back to the
   beginning. we also developed a new term, $\delta$, which essentially
   serves to represent all of the partial derivative terms that we would
   need to reuse later and we progress, layer by layer, backwards through
   the network.

   note: id26 is simply a method for calculating the partial
   derivative of the cost function with respect to all of the parameters.
   the actual optimization of parameters (training) is done by gradient
   descent or another more advanced optimization technique.

   generally, we established that you can calculate the partial
   derivatives for layer $l$ by combining $\delta$ terms of the next layer
   forward with the activations of the current layer.

   $$ \frac{{\partial j\left( \theta \right)}}{{\partial \theta
   _{ij}^{(l)}}} = {\left( {{\delta ^{(l + 1)}}} \right)^t}{a^{(l)}} $$

id26 visualized

   before defining the formal method for id26, i'd like to
   provide a visualization of the process.

   first, we have to compute the output of a neural network via forward
   propagation.

   next, we compute the ${\delta ^{(3)}}$ terms for the last layer in the
   network. remember, these $\delta$ terms consist of all of the partial
   derivatives that will be used again in calculating parameters for
   layers further back. in practice, we typically refer to $\delta$ as the
   "error" term.

   ${\theta ^{(2)}}$ is the matrix of parameters that connects layer 2 to
   3. we multiply the error from the third layer by the inputs in the
   second layer to calculate our partial derivatives for this set of
   parameters.

   next, we "send back" the "error" terms in the exact same manner that we
   "send forward" the inputs to a neural network. the only difference is
   that time, we're starting from the back and we're feeding an error
   term, layer by layer, backwards through the network. hence the name:
   id26. the act of "sending back our error" is accomplished
   via the expression ${\delta ^{(3)}}{\theta ^{(2)}}$.

   ${\theta ^{(1)}}$ is the matrix of parameters that connects layer 1 to
   2. we multiply the error from the second layer by the inputs in the
   first layer to calculate our partial derivatives for this set of
   parameters.

   for every layer except for the last, the "error" term is a linear
   combination of parameters connecting to the next layer (moving forward
   through the network) and the "error" terms of that next layer. this is
   true for all of the hidden layers, since we don't compute an "error"
   term for the inputs.

   the last layer is a special case because we calculate the $\delta$
   values by directly comparing each output neuron to its expected output.

a formalized method for implementing id26

   here, i'll present a practical method for implementing id26
   through a network of layers $l=1,2,...,l$.
    1. perform forward propagation.
    2. compute the $\delta$ term for the output layer.
       [{\delta ^{(l)}} = \frac{1}{m}\left( {y - {a^{(l)}}}
       \right)f'\left( {{a^{(l)}}} \right)]
    3. compute the partial derivatives of the cost function with respect
       to all of the parameters that feed into the output layer, ${\theta
       ^{(l - 1)}}$.
       [\frac{{\partial j\left( \theta \right)}}{{\partial \theta
       _{ij}^{(l-1)}}} = {\left( {{\delta ^{(l)}}} \right)^t}{a^{(l-1)}}]
    4. go back one layer.
       $l = l - 1$
    5. compute the $\delta$ term for the current hidden layer.
       [{\delta ^{(l)}} = {\delta ^{(l + 1)}}{\theta ^{(l)}}f'\left(
       {{a^{(l)}}} \right)]
    6. compute the partial derivatives of the cost function with respect
       to all of the parameters that feed into the current layer.
       [\frac{{\partial j\left( \theta \right)}}{{\partial \theta
       _{ij}^{(l)}}} = {\left( {{\delta ^{(l + 1)}}} \right)^t}{a^{(l)}}]
    7. repeat 4 through 6 until you reach the input layer.

revisiting the weights initialization

   when we started, i proposed that we just randomly initialize our
   weights in order to have a place to start. this allowed us to perform
   forward propagation, compare the outputs to the expected values, and
   compute the cost of our model.

   it's actually very important that we initialize our weights to random
   values so that we can break the symmetry in our model. if we had
   initialized all of our weights to be equal to be the same, every neuron
   in the next layer forward would be equal to the same linear combination
   of values.

   by this same logic, the $\delta$ values would also be the same for
   every neuron in a given layer.

   further, because we calculate the partial derivatives in any given
   layer by combining $\delta$ values and activations, all of the partial
   derivatives in any given layer would be identical. as such, the weights
   would update symmetrically in id119 and multiple neurons in
   any layer would be useless. this obviously would not be a very helpful
   neural network.

   randomly initializing the network's weights allows us to break this
   symmetry and update each weight individually according to its
   relationship with the cost function. typically we'll assign each
   parameter to a random value in $\left[ { - \varepsilon ,\varepsilon }
   \right]$ where $\varepsilon$ is some value close to zero.

putting it all together

   after we've calculated all of the partial derivatives for the neural
   network parameters, we can use id119 to update the weights.

   in general, we defined id119 as
   [{\theta _i}: = {\theta _i} + \delta {\theta _i}]

   where $\delta {\theta _i}$ is the "step" we take walking along the
   gradient, scaled by a learning rate, $\eta$.

   [\delta {\theta _i} = - \eta \frac{{\partial j\left( \theta
   \right)}}{{\partial {\theta _i}}}]

   we'll use this formula to update each of the weights, recompute forward
   propagation with the new weights, backpropagate the error, and
   calculate the next weight update. this process continues until we've
   converged on an optimal value for our parameters.

   during each iteration we perform forward propagation to compute the
   outputs and backward propagation to compute the errors; one complete
   iteration is known as an epoch. it is common to report evaluation
   metrics after each epoch so that we can watch the evolution of our
   neural network as it trains.

further reading

     * [19]the matrix calculus you need for deep learning

   iframe: [20]https://www.youtube.com/embed/uxt8qf2zzfo

   iframe: [21]https://www.youtube.com/embed/59hbtz7xgjm

   iframe: [22]https://www.youtube.com/embed/ih5mr93e-2c

     * [23]stanford cs231n: id26, intuitions
     * [24]lectures on deep learning
     * [25]yes you should understand backprop
     * [26]building blocks of neural networks
     * and in case you just gave up on id26... [27]deep
       learning without id26

subscribe to jeremy jordan

   get the latest posts delivered right to your inbox
   ____________________
   (button) subscribe

   jeremy jordan

[28]jeremy jordan

   machine learning engineer. broadly curious.
   [29]read more

       jeremy jordan    

[30]data science

     * [31]scaling nearest neighbors search with approximate methods.
     * [32]organizing machine learning projects: project management
       guidelines.
     * [33]an overview of id164: one-stage methods.

   [34]see all 41 posts    

   data science

evaluating a machine learning model.

   so you've built a machine learning model and trained it on some data...
   now what? in this post, i'll discuss how to evaluate your model, and
   practical advice for improving the model based

   jeremy jordan [35]jeremy jordan

   data science

id119.

   id119 is an optimization technique commonly used in training
   machine learning algorithms. often when we're building a machine
   learning model, we'll develop a cost function which is capable of
   measuring how well

   jeremy jordan [36]jeremy jordan
   [37]jeremy jordan icon jeremy jordan
      
   neural networks: training with id26.
   share this

   [38]jeremy jordan    2019

   [39]latest posts [40]twitter [41]ghost

subscribe to jeremy jordan

   stay up to date! get all the latest & greatest posts delivered straight
   to your inbox
   ____________________
   (button) subscribe

references

   visible links
   1. https://www.jeremyjordan.me/rss/
   2. https://www.jeremyjordan.me/
   3. https://www.jeremyjordan.me/
   4. https://www.jeremyjordan.me/about/
   5. https://www.jeremyjordan.me/data-science/
   6. https://www.jeremyjordan.me/books/
   7. https://www.jeremyjordan.me/quotes/
   8. https://www.jeremyjordan.me/tag/life/
   9. https://www.jeremyjordan.me/my-favorite-talks/
  10. https://www.jeremyjordan.me/tag/materials-science/
  11. https://www.jeremyjordan.me/neural-networks-training/#subscribe
  12. https://www.jeremyjordan.me/tag/data-science/
  13. http://www.jeremyjordan.me/neural-networks-representation/
  14. http://www.jeremyjordan.me/gradient-descent/
  15. https://www.youtube.com/watch?v=uxt8qf2zzfo
  16. https://www.khanacademy.org/math/multivariable-calculus
  17. http://www.jeremyjordan.me/gradient-descent/
  18. https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro
  19. http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html
  20. https://www.youtube.com/embed/uxt8qf2zzfo
  21. https://www.youtube.com/embed/59hbtz7xgjm
  22. https://www.youtube.com/embed/ih5mr93e-2c
  23. http://cs231n.github.io/optimization-2/
  24. https://github.com/ndrplz/deep_learning_lectures
  25. https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b
  26. https://www.coursera.org/learn/neural-networks-deep-learning/lecture/ugcun/building-blocks-of-deep-neural-networks
  27. https://iamtrask.github.io/2017/03/21/synthetic-gradients/
  28. https://www.jeremyjordan.me/author/jeremy/
  29. https://www.jeremyjordan.me/author/jeremy/
  30. https://www.jeremyjordan.me/tag/data-science/
  31. https://www.jeremyjordan.me/scaling-nearest-neighbors-search-with-approximate-methods/
  32. https://www.jeremyjordan.me/ml-projects-guide/
  33. https://www.jeremyjordan.me/object-detection-one-stage/
  34. https://www.jeremyjordan.me/tag/data-science/
  35. https://www.jeremyjordan.me/author/jeremy/
  36. https://www.jeremyjordan.me/author/jeremy/
  37. https://www.jeremyjordan.me/
  38. https://www.jeremyjordan.me/
  39. https://www.jeremyjordan.me/
  40. https://twitter.com/jeremyjordan
  41. https://ghost.org/

   hidden links:
  43. https://twitter.com/jeremyjordan
  44. https://www.jeremyjordan.me/evaluating-a-machine-learning-model/
  45. https://www.jeremyjordan.me/gradient-descent/
  46. https://twitter.com/share?text=neural%20networks%3a%20training%20with%20id26.&url=https://www.jeremyjordan.me/neural-networks-training/
  47. https://www.facebook.com/sharer/sharer.php?u=https://www.jeremyjordan.me/neural-networks-training/
  48. https://www.jeremyjordan.me/neural-networks-training/
