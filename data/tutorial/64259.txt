   #[1]adventures in machine learning    tensorflow dataset api tutorial    
   build high performance data pipelines comments feed [2]alternate
   [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

tensorflow dataset api tutorial     build high performance data pipelines

   by [9]admin | [10]tensorflow

     * you are here:
     * [11]home
     * [12]tensorflow
     * [13]tensorflow dataset api tutorial     build high performance data
       pipelines

   mar 17
   [14]5
   tensorflow dataset tutorial - mnist example output

   consuming data efficiently becomes really paramount to training
   performance in deep learning. in a previous post i discussed the
   [15]tensorflow data queuing framework. however, tensorflow development
   is always on the move and they have now created a more streamlined and
   efficient way of setting up data input pipelines. this tensorflow
   dataset tutorial will show you how to use this dataset framework to
   enable you to produce highly efficient input data pipelines. this is an
   important topic which isn   t covered very well in most tensorflow
   tutorials     rather, these tutorials will often use the feed_dict and
   placeholder method of feeding data into the model. this method of
   feeding data into your network in tensorflow is inefficient and will
   likely slow down your training for large, realistic datasets     see a
   discussion about this on the [16]tensorflow website. why is this
   framework better than the feed_dict method that is so commonly used?
   simply, all of the operations to transform data and feed it into the
   model which can be performed with the dataset api i.e. reading the data
   from arrays and files, transforming it, shuffling it etc. can all be
   automatically optimized and paralleled to provide efficient consumption
   of data.

   in this tensorflow dataset tutorial, i will show you how to use the
   framework with some simple examples, and finally show you how to
   consume the scikit-learn mnist dataset to create an mnist classifier.
   as always, the code for this tutorial can be found on this site   s
   [17]github repository.

     __________________________________________________________________

eager to learn more? get the book [18]here
     __________________________________________________________________

the tensorflow dataset framework     main components

   the tensorflow dataset framework has two main components:
     * the dataset
     * an associated iterator

   the dataset is basically where the data resides. this data can be
   loaded in from a number of sources     existing tensors, numpy arrays and
   numpy files, the tfrecord format and direct from text files. once
   you   ve loaded the data into the dataset object, you can string together
   various operations to apply to the data, these include operations such
   as:
     * batch()     this allows you to consume the data from your tensorflow
       dataset in batches
     * map()     this allows you to transform the data using lambda
       statements applied to each element
     * zip()     this allows you to zip together different dataset objects
       into a new dataset, in a similar way to the python zip function
     * filter()     this allows you to remove problematic data-points in
       your data-set, again based on some lambda function
     * repeat()     this operation restricts the number of times data is
       consumed from the dataset before a tf.errors.outofrangeerror error
       is thrown
     * shuffle()     this operation shuffles the data in the dataset

   there are many other methods that the dataset api includes     see
   [19]here for more details.  the next component in the tensorflow
   dataset framework is the iterator. this creates operations which can be
   called during the training, validation and/or testing of your model in
   tensorflow. i   ll introduce more of both components in some examples
   below.

simple tensorflow dataset examples

   in the first simple example, we   ll create a dataset out of numpy
   ranges:
x = np.arange(0, 10)

   we can create a tensorflow dataset object straight from a numpy array
   using from_tensor_slices():
# create dataset object from numpy array
dx = tf.data.dataset.from_tensor_slices(x)

   the object dx is now a tensorflow dataset object. the next step is to
   create an iterator that will extract data from this dataset. in the
   code below, the iterator is created using the
   method make_one_shot_iterator().  the iterator arising from this method
   can only be initialized and run once     it can   t be re-initialized. the
   importance of being able to re-initialize an iterator will be explained
   more later.
# create a one-shot iterator
iterator = dx.make_one_shot_iterator()
# extract an element
next_element = iterator.get_next()

   after the iterator is created, the next step is to setup a tensorflow
   operation which can be called from the training code to extract the
   next element from the dataset. finally, the dataset operation can be
   examined by running the following code:
with tf.session() as sess:
    for i in range(11):
        val = sess.run(next_element)
        print(val)

   this code will print out integers from 0 to 9 but then throw an
   outofrangeerror. this is because the code extracted all the data slices
   from the dataset and it is now out of range or    empty   .

   if we want to repeatedly extract data from a dataset, one way we can do
   it is to make the dataset re-initializable. we can do that by first
   adjusting the make_one_shot_iterator() line to the following:
iterator = dx.make_initializable_iterator()

   then, within the tensorflow session, the code looks like this:
with tf.session() as sess:
    sess.run(iterator.initializer)
    for i in range(15):
        val = sess.run(next_element)
        print(val)
        if i % 9 == 0 and i > 0:
            sess.run(iterator.initializer)

   note that the first operation run is the iterator.initializer
   operation. this is required to get your iterator ready for action and
   if you don   t do this before running the next_element operation it will
   throw an error. the final change is the last two lines:
   this if statement ensures that when we know that the iterator has run
   out of data (i.e. i == 9), the iterator is re-initialized by the
   iterator.initializer operation. running this new code will produce: 0 1
   2 3 4 5 6 7 8 9 0 1 2 3 4. no error this time!

   there are also other things that can be done to manipulate the dataset
   and how it can be used. first, the batch function:
dx = tf.data.dataset.from_tensor_slices(x).batch(3)

   after this change, when the next_element operation is run, a batch of
   length 3 will be extracted from the data. running the code below:
with tf.session() as sess:
    sess.run(iterator.initializer)
    for i in range(15):
        val = sess.run(next_element)
        print(val)
        if (i + 1) % (10 // 3) == 0 and i > 0:
            sess.run(iterator.initializer)

   will produce an output like:
   [0 1 2] [3 4 5] [6 7 8] [0 1 2] [3 4 5] [6 7 8]

   and so on.

   next, we can zip together datasets. this is useful when pairing up
   input-output training/validation pairs of data (i.e. input images and
   matching labels for each image). the code below does this:
def simple_zip_example():
    x = np.arange(0, 10)
    y = np.arange(1, 11)
    # create dataset objects from the arrays
    dx = tf.data.dataset.from_tensor_slices(x)
    dy = tf.data.dataset.from_tensor_slices(y)
    # zip the two datasets together
    dcomb = tf.data.dataset.zip((dx, dy)).batch(3)
    iterator = dcomb.make_initializable_iterator()
    # extract an element
    next_element = iterator.get_next()
    with tf.session() as sess:
        sess.run(iterator.initializer)
        for i in range(15):
            val = sess.run(next_element)
            print(val)
            if (i + 1) % (10 // 3) == 0 and i > 0:
                sess.run(iterator.initializer)

   the zip combination of the two datasets (dx, dy) can be seen in the
   line where dcomb is created. note the chaining together of multiple
   operations     first the zip method, then the batching operation. the
   rest of the code is the same. this code will produce an output like the
   following:

   (array([0, 1, 2]), array([1, 2, 3]))
   (array([3, 4, 5]), array([4, 5, 6]))
   (array([6, 7, 8]), array([7, 8, 9]))
   (array([0, 1, 2]), array([1, 2, 3]))

   and so on. as you can observe, the batching takes place appropriately
   within the zipped together datasets i.e. 3 items from dx, 3 items from
   dy. as stated above, this is handy for combining input data and
   matching labels.

   note, the re-initialization if statement on the last two lines is a bit
   unwieldy, we can actually get rid of it by replacing the dcomb dataset
   creation line with the following:
dcomb = tf.data.dataset.zip((dx, dy)).repeat().batch(3)

   note the addition of the repeat() method to the operation list. when
   this method is applied to the dataset with no argument, it means that
   the dataset can be repeated indefinitely without throwing an
   outofrangeerror. this will be shown in the next more detailed example    
   using the sci-kit learn mnist dataset to create a hand-written digits
   classifier.

tensorflow dataset mnist example

   in this section, i   ll show how to create an mnist hand-written digit
   classifier which will consume the mnist image and label data from the
   simplified mnist dataset supplied from the python [20]scikit-learn
   package (a must-have package for practical machine learning
   enthusiasts). i   ll step through the code slowly below.

   first, we have to load the data from the package and split it into
   train and validation datasets. this can be performed with the following
   code:
# load the data
digits = load_digits(return_x_y=true)
# split into train and validation sets
train_images = digits[0][:int(len(digits[0]) * 0.8)]
train_labels = digits[1][:int(len(digits[0]) * 0.8)]
valid_images = digits[0][int(len(digits[0]) * 0.8):]
valid_labels = digits[1][int(len(digits[0]) * 0.8):]

   the load_digits method will extract the data from the relevant location
   in the scikit-learn package, and the code above splits the first 80% of
   the data into the training arrays, and the remaining 20% into the
   validation arrays.

   next, the tensorflow datasets of the training data are created:
# create the training datasets
dx_train = tf.data.dataset.from_tensor_slices(train_images)
# apply a one-hot transformation to each label for use in the neural network
dy_train = tf.data.dataset.from_tensor_slices(train_labels).map(lambda z: tf.one
_hot(z, 10))
# zip the x and y training data together and shuffle, batch etc.
train_dataset = tf.data.dataset.zip((dx_train, dy_train)).shuffle(500).repeat().
batch(30)

   the dx_train statement is straightforward, however there is an extra
   element that has been added in the dy_train statement. note the use of
   the map() method. the labels in the mnist dataset are integers between
   0 and 9 corresponding to the hand-written digit in the image. this
   integer data must be transformed into one-hot format, i.e. the integer
   label 4 transformed into the vector [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. to
   do this, the lambda statement is used, where every row (expressed as z
   in the above) in the label dataset is transformed into one-hot data
   format using the tensorflow one_hot function. if you   d like to learn
   more about one hot data structures and neural networks, see [21]my
   neural network tutorial.

   finally, the training x and y data is zipped together in the
   full train_dataset. chained along together with this zip method is
   first the shuffle() dataset method. this method randomly shuffles the
   data, using a buffer of data specified in the argument     500 in this
   case. next, the repeat() method is used, to allow the iterator to
   continuously extract data from this dataset, finally the data is
   batched with a batch size of 30.

   the same steps are used to create the validation dataset:
# do the same operations for the validation set
dx_valid = tf.data.dataset.from_tensor_slices(valid_images)
dy_valid = tf.data.dataset.from_tensor_slices(valid_labels).map(lambda z: tf.one
_hot(z, 10))
valid_dataset = tf.data.dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().
batch(30)

   now, we want to be able to extract data from either the train_dataset
   or the valid_dataset seaid113ssly. this is important, as we don   t want to
   have to change how data flows through the neural network structure when
   all we want to do is just change the dataset the model is consuming. to
   do this, we can use another way of creating the iterator object    
   the from_structure() method. this method creates a generic iterator
   object     all it needs is the data types of the data it will be
   outputting and the output data size/shape in order to be created. the
   code below uses this methodology:
# create general iterator
iterator = tf.data.iterator.from_structure(train_dataset.output_types,
                                               train_dataset.output_shapes)
next_element = iterator.get_next()

   the second line of the above creates a standard get_next() iterator
   operation which can be called to extract data from this generic
   iterator structure. next, we need some operations which can be called
   during training or validating to initialize this generic iterator and
      point it    to the desired dataset. these are as follows:
# make datasets that we can initialize separately, but using the same structure
via the common iterator
training_init_op = iterator.make_initializer(train_dataset)
validation_init_op = iterator.make_initializer(valid_dataset)

   these operations can be run to    switch over    the iterator from one
   dataset to another. this    switching over    will be demonstrated in code
   below.

   next, the neural network model is created     this is standard tensorflow
   usage and in this case i will be utilizing the tensorflow layers api to
   create a simple fully connected or dense neural network, with dropout
   and a first layer of batch id172 to effectively scale the input
   data. if you   d like to learn some of the basics of tensorflow, check
   out my [22]python tensorflow tutorial. the tensorflow model is defined
   as follows:
def nn_model(in_data):
    bn = tf.layers.batch_id172(in_data)
    fc1 = tf.layers.dense(bn, 50)
    fc2 = tf.layers.dense(fc1, 50)
    fc2 = tf.layers.dropout(fc2)
    fc3 = tf.layers.dense(fc2, 10)
    return fc3

   to call this model creation function, the code below can be used:
# create the neural network model
logits = nn_model(next_element[0])

   note that the next_element operation is handled directly in the model    
   in other words, it doesn   t need to be called explicitly during the
   training loop as will be seen below. rather, whenever any of the
   operations following this point in the graph are called (i.e. the loss
   operation, the optimization operation etc.) the tensorflow graph
   structure will know to run the next_element operation and extract the
   data from whichever dataset has been initialized into the iterator. the
   next_element operation, because it is operating on the generic iterator
   which is defined by the shape of the train_dataset, is a tuple     the
   first element ([0]) will contain the mnist images, while the second
   element ([1]) will contain the corresponding labels. therefore,
   next_element[0] will extract the image data batch and send it into the
   neural network model (nn_model) as the input data.

   next are some standard tensorflow operations to calculate the loss
   function, the optimization step and prediction accuracy (again, for
   more details see [23]this tutorial or [24]this one):
# add the optimizer and loss
loss = tf.reduce_sum(tf.nn.softmax_cross_id178_with_logits_v2(labels=next_elem
ent[1], logits=logits))
optimizer = tf.train.adamoptimizer().minimize(loss)
# get accuracy
prediction = tf.argmax(logits, 1)
equality = tf.equal(prediction, tf.argmax(next_element[1], 1))
accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))
init_op = tf.global_variables_initializer()

   now we can run the training loop:
# run the training
epochs = 600
with tf.session() as sess:
    sess.run(init_op)
    sess.run(training_init_op)
    for i in range(epochs):
        l, _, acc = sess.run([loss, optimizer, accuracy])
        if i % 50 == 0:
            print("epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%".format(i
, l, acc * 100))
    # now setup the validation run
    valid_iters = 100
    # re-initialize the iterator, but this time with validation data
    sess.run(validation_init_op)
    avg_acc = 0
    for i in range(valid_iters):
        acc = sess.run([accuracy])
        avg_acc += acc[0]
    print("average validation set accuracy over {} iterations is {:.2f}%".format
(valid_iters,
           (avg_acc / valid_iters) * 100))

   as can be observed, before the main training loop is entered into, the
   session executes the training_init_op operation, which initializes the
   generic iterator to extract data from train_dataset.
   after running epochs iterations to train the model, we then want to
   check how the trained model performs on the validation dataset
   (valid_dataset). to do this, we can simply run the validation_init_op
   operation in the session to point the generic iterator to
   valid_dataset. then we run the accuracy operation as per normal,
   knowing that the operation will be calculating the model accuracy based
   on the validation data, rather than the training data. running this
   code will produce an output that will look something like:
   tensorflow dataset tutorial - mnist example output

   tensorflow dataset tutorial     mnist example output

   obviously not a create validation set accuracy for mnist     but this is
   just an example model to demonstrate how to use the tensorflow dataset
   framework. for more accurate ways of performing image classification,
   check out my [25]convolutional neural network tutorial in tensorflow.

   so there you have it     hopefully you are now in a position to use this
   new, streamlined data input pipeline api in tensorflow. enjoy your
   newly optimized tensorflow code.
     __________________________________________________________________

eager to learn more? get the book [26]here
     __________________________________________________________________


about the author

     jl says:
   [27]may 2, 2018 at 10:41 am

   great guide! i   m wondering if i can use this feature with tensorflow
   1.2.1 with contrib.data.dataset api. i   m currently working with large
   datasets and this feature would be a great improvement. thanks!

     ah says:
   [28]may 28, 2018 at 9:01 am

   good post

     shravankumar says:
   [29]may 30, 2018 at 6:33 pm

   very clear , thanks for sharing.
     * andy says:
       [30]may 30, 2018 at 9:04 pm
       glad you found it useful, thanks

     hanlin says:
   [31]may 30, 2018 at 11:47 pm

   nice work

   ____________________ (button)

   recent posts
     * [32]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [33]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [34]keras, eager and tensorflow 2.0     a new tf paradigm
     * [35]introduction to tensorboard and tensorflow visualization
     * [36]tensorflow eager tutorial

   recent comments
     * andry on [37]neural networks tutorial     a pathway to deep learning
     * sandipan on [38]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [39]neural networks tutorial     a pathway to deep learning
     * martin on [40]neural networks tutorial     a pathway to deep learning
     * uri on [41]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [42]march 2019
     * [43]january 2019
     * [44]october 2018
     * [45]september 2018
     * [46]august 2018
     * [47]july 2018
     * [48]june 2018
     * [49]may 2018
     * [50]april 2018
     * [51]march 2018
     * [52]february 2018
     * [53]november 2017
     * [54]october 2017
     * [55]september 2017
     * [56]august 2017
     * [57]july 2017
     * [58]may 2017
     * [59]april 2017
     * [60]march 2017

   categories
     * [61]amazon aws
     * [62]cntk
     * [63]convolutional neural networks
     * [64]cross id178
     * [65]deep learning
     * [66]gensim
     * [67]gpus
     * [68]keras
     * [69]id168s
     * [70]lstms
     * [71]neural networks
     * [72]nlp
     * [73]optimisation
     * [74]pytorch
     * [75]recurrent neural networks
     * [76]id23
     * [77]tensorboard
     * [78]tensorflow
     * [79]tensorflow 2.0
     * [80]weight initialization
     * [81]id97

   meta
     * [82]log in
     * [83]entries rss
     * [84]comments rss
     * [85]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [86]thrive themes | powered by [87]wordpress

   (button) close dialog

   session expired

   [88]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[89]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  13. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/
  14. http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments
  15. https://adventuresinmachinelearning.com/introduction-tensorflow-queuing/
  16. https://www.tensorflow.org/performance/performance_guide
  17. https://github.com/adventuresinml/adventures-in-ml-code
  18. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  19. https://www.tensorflow.org/api_docs/python/tf/data/dataset#repeat
  20. http://scikit-learn.org/stable/index.html
  21. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  22. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  23. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  24. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  25. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  26. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  27. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments/5210
  28. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments/5211
  29. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments/5212
  30. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments/5213
  31. https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/#comments/5214
  32. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  33. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  34. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  35. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  36. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  37. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  38. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  39. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  40. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  41. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  42. https://adventuresinmachinelearning.com/2019/03/
  43. https://adventuresinmachinelearning.com/2019/01/
  44. https://adventuresinmachinelearning.com/2018/10/
  45. https://adventuresinmachinelearning.com/2018/09/
  46. https://adventuresinmachinelearning.com/2018/08/
  47. https://adventuresinmachinelearning.com/2018/07/
  48. https://adventuresinmachinelearning.com/2018/06/
  49. https://adventuresinmachinelearning.com/2018/05/
  50. https://adventuresinmachinelearning.com/2018/04/
  51. https://adventuresinmachinelearning.com/2018/03/
  52. https://adventuresinmachinelearning.com/2018/02/
  53. https://adventuresinmachinelearning.com/2017/11/
  54. https://adventuresinmachinelearning.com/2017/10/
  55. https://adventuresinmachinelearning.com/2017/09/
  56. https://adventuresinmachinelearning.com/2017/08/
  57. https://adventuresinmachinelearning.com/2017/07/
  58. https://adventuresinmachinelearning.com/2017/05/
  59. https://adventuresinmachinelearning.com/2017/04/
  60. https://adventuresinmachinelearning.com/2017/03/
  61. https://adventuresinmachinelearning.com/category/amazon-aws/
  62. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  63. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  64. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  65. https://adventuresinmachinelearning.com/category/deep-learning/
  66. https://adventuresinmachinelearning.com/category/nlp/gensim/
  67. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  68. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  69. https://adventuresinmachinelearning.com/category/loss-functions/
  70. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  71. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  72. https://adventuresinmachinelearning.com/category/nlp/
  73. https://adventuresinmachinelearning.com/category/optimisation/
  74. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  75. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  76. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  77. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  78. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  79. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  80. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  81. https://adventuresinmachinelearning.com/category/nlp/id97/
  82. https://adventuresinmachinelearning.com/wp-login.php
  83. https://adventuresinmachinelearning.com/feed/
  84. https://adventuresinmachinelearning.com/comments/feed/
  85. https://wordpress.org/
  86. https://www.thrivethemes.com/
  87. http://www.wordpress.org/
  88. https://adventuresinmachinelearning.com/wp-login.php
  89. http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/

   hidden links:
  91. https://adventuresinmachinelearning.com/author/admin/
