   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

building a id28 in python, step by step

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   sep 28, 2017
   [1*lra_hrggxm_bdfippytvxq.png]
   photo credit: scikit-learn

   [18]id28 is a machine learning classification algorithm
   that is used to predict the id203 of a categorical dependent
   variable. in id28, the dependent variable is a binary
   variable that contains data coded as 1 (yes, success, etc.) or 0 (no,
   failure, etc.). in other words, the id28 model predicts
   p(y=1) as a function of x.

id28 assumptions

     * binary id28 requires the dependent variable to be
       binary.
     * for a binary regression, the factor level 1 of the dependent
       variable should represent the desired outcome.
     * only the meaningful variables should be included.
     * the independent variables should be independent of each other. that
       is, the model should have little or no multicollinearity.
     * the independent variables are linearly related to the log odds.
     * id28 requires quite large sample sizes.

   keeping the above assumptions in mind, let   s look at our dataset.

data

   the dataset comes from the [19]uci machine learning repository, and it
   is related to direct marketing campaigns (phone calls) of a portuguese
   banking institution. the classification goal is to predict whether the
   client will subscribe (1/0) to a term deposit (variable y). the dataset
   can be downloaded from [20]here.
import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt
plt.rc("font", size=14)
from sklearn.linear_model import logisticregression
from sklearn.model_selection import train_test_split
import seaborn as sns
sns.set(style="white")
sns.set(style="whitegrid", color_codes=true)

   the dataset provides the bank customers    information. it includes
   41,188 records and 21 fields.
   [1*44iy8wnw4bisvdii5agj7q.png]
   figure 1

   input variables
    1. age (numeric)
    2. job : type of job (categorical:    admin   ,    blue-collar   ,
          entrepreneur   ,    housemaid   ,    management   ,    retired   ,
          self-employed   ,    services   ,    student   ,    technician   ,    unemployed   ,
          unknown   )
    3. marital : marital status (categorical:    divorced   ,    married   ,
          single   ,    unknown   )
    4. education (categorical:    basic.4y   ,    basic.6y   ,    basic.9y   ,
          high.school   ,    illiterate   ,    professional.course   ,
          university.degree   ,    unknown   )
    5. default: has credit in default? (categorical:    no   ,    yes   ,
          unknown   )
    6. housing: has housing loan? (categorical:    no   ,    yes   ,    unknown   )
    7. loan: has personal loan? (categorical:    no   ,    yes   ,    unknown   )
    8. contact: contact communication type (categorical:    cellular   ,
          telephone   )
    9. month: last contact month of year (categorical:    jan   ,    feb   ,
          mar   ,    ,    nov   ,    dec   )
   10. day_of_week: last contact day of the week (categorical:    mon   ,
          tue   ,    wed   ,    thu   ,    fri   )
   11. duration: last contact duration, in seconds (numeric). important
       note: this attribute highly affects the output target (e.g., if
       duration=0 then y=   no   ). the duration is not known before a call is
       performed, also, after the end of the call, y is obviously known.
       thus, this input should only be included for benchmark purposes and
       should be discarded if the intention is to have a realistic
       predictive model
   12. campaign: number of contacts performed during this campaign and for
       this client (numeric, includes last contact)
   13. pdays: number of days that passed by after the client was last
       contacted from a previous campaign (numeric; 999 means client was
       not previously contacted)
   14. previous: number of contacts performed before this campaign and for
       this client (numeric)
   15. poutcome: outcome of the previous marketing campaign (categorical:
          failure   ,    nonexistent   ,    success   )
   16. emp.var.rate: employment variation rate         (numeric)
   17. cons.price.idx: consumer price index         (numeric)
   18. cons.conf.idx: consumer confidence index         (numeric)
   19. euribor3m: euribor 3 month rate         (numeric)
   20. nr.employed: number of employees         (numeric)

   predict variable (desired target):

   y         has the client subscribed a term deposit? (binary:    1   , means
      yes   ,    0    means    no   )

   the education column of the dataset has many categories and we need to
   reduce the categories for a better modelling. the education column has
   the following categories:
   [1*0eyxmgz7r_omqzc4p_pbsw.png]
   figure 2

   let us group    basic.4y   ,    basic.9y    and    basic.6y    together and call
   them    basic   .
data['education']=np.where(data['education'] =='basic.9y', 'basic', data['educat
ion'])
data['education']=np.where(data['education'] =='basic.6y', 'basic', data['educat
ion'])
data['education']=np.where(data['education'] =='basic.4y', 'basic', data['educat
ion'])

   after grouping, this is the columns:
   [1*pv6se8hwuvh3n7w8mpknya.png]
   figure 3

data exploration

   [1*4qhcg1fijicolelr3ef_kw.png]
   figure 4
count_no_sub = len(data[data['y']==0])
count_sub = len(data[data['y']==1])
pct_of_no_sub = count_no_sub/(count_no_sub+count_sub)
print("percentage of no subscription is", pct_of_no_sub*100)
pct_of_sub = count_sub/(count_no_sub+count_sub)
print("percentage of subscription", pct_of_sub*100)

   percentage of no subscription is 88.73458288821988

   percentage of subscription 11.265417111780131

   our classes are imbalanced, and the ratio of no-subscription to
   subscription instances is 89:11. before we go ahead to balance the
   classes, let   s do some more exploration.
   [1*c1nf9dyzum768_jqpa5ava.png]
   figure 5

   observations:
     * the average age of customers who bought the term deposit is higher
       than that of the customers who didn   t.
     * the pdays (days since the customer was last contacted) is
       understandably lower for the customers who bought it. the lower the
       pdays, the better the memory of the last call and hence the better
       chances of a sale.
     * surprisingly, campaigns (number of contacts or calls made during
       the current campaign) are lower for customers who bought the term
       deposit.

   we can calculate categorical means for other categorical variables such
   as education and marital status to get a more detailed sense of our
   data.
   [1*hjgealt6dn_zua6pzxitfq.png]
   figure 6
   [1*fj_bzzxgbyahrrcgwkl6aw.png]
   figure 7

visualizations

%matplotlib inline
pd.crosstab(data.job,data.y).plot(kind='bar')
plt.title('purchase frequency for job title')
plt.xlabel('job')
plt.ylabel('frequency of purchase')
plt.savefig('purchase_fre_job')

   [1*sjewyzhtlr4m7rs54q7bfa.png]
   figure 8

   the frequency of purchase of the deposit depends a great deal on the
   job title. thus, the job title can be a good predictor of the outcome
   variable.
table=pd.crosstab(data.marital,data.y)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=true)
plt.title('stacked bar chart of marital status vs purchase')
plt.xlabel('marital status')
plt.ylabel('proportion of customers')
plt.savefig('mariral_vs_pur_stack')

   [1*vsrmnyoqpsjiqzb35f5wdw.png]
   figure 9

   the marital status does not seem a strong predictor for the outcome
   variable.
table=pd.crosstab(data.education,data.y)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=true)
plt.title('stacked bar chart of education vs purchase')
plt.xlabel('education')
plt.ylabel('proportion of customers')
plt.savefig('edu_vs_pur_stack')

   [1*5btk1pr0lerxwaemoyvhog.png]
   figure 10

   education seems a good predictor of the outcome variable.
pd.crosstab(data.day_of_week,data.y).plot(kind='bar')
plt.title('purchase frequency for day of week')
plt.xlabel('day of week')
plt.ylabel('frequency of purchase')
plt.savefig('pur_dayofweek_bar')

   [1*dx3lon5sfqfvumnxpre0_g.png]
   figure 11

   day of week may not be a good predictor of the outcome.
pd.crosstab(data.month,data.y).plot(kind='bar')
plt.title('purchase frequency for month')
plt.xlabel('month')
plt.ylabel('frequency of purchase')
plt.savefig('pur_fre_month_bar')

   [1*vr11kd8p1ipaaj3k6sireq.png]
   figure 12

   month might be a good predictor of the outcome variable.
data.age.hist()
plt.title('histogram of age')
plt.xlabel('age')
plt.ylabel('frequency')
plt.savefig('hist_age')

   [1*ttfhhfm6qv1gfdjad6oa8a.png]
   figure 13

   most of the customers of the bank in this dataset are in the age range
   of 30   40.
pd.crosstab(data.poutcome,data.y).plot(kind='bar')
plt.title('purchase frequency for poutcome')
plt.xlabel('poutcome')
plt.ylabel('frequency of purchase')
plt.savefig('pur_fre_pout_bar')

   [1*9k90mt_6rwkjtpr3sb_sxq.png]
   figure 14

   poutcome seems to be a good predictor of the outcome variable.

create dummy variables

   that is variables with only two values, zero and one.
cat_vars=['job','marital','education','default','housing','loan','contact','mont
h','day_of_week','poutcome']
for var in cat_vars:
    cat_list='var'+'_'+var
    cat_list = pd.get_dummies(data[var], prefix=var)
    data1=data.join(cat_list)
    data=data1
cat_vars=['job','marital','education','default','housing','loan','contact','mont
h','day_of_week','poutcome']
data_vars=data.columns.values.tolist()
to_keep=[i for i in data_vars if i not in cat_vars]

   our final data columns will be:
data_final=data[to_keep]
data_final.columns.values

   [1*2nvijswq0pir0n0ixd8bag.png]
   figure 15

over-sampling using smote

   with our training data created, i   ll up-sample the no-subscription
   using the [21]smote algorithm(synthetic minority oversampling
   technique). at a high level, smote:
    1. works by creating synthetic samples from the minor class
       (no-subscription) instead of creating copies.
    2. randomly choosing one of the k-nearest-neighbors and using it to
       create a similar, but randomly tweaked, new observations.

   we are going to implement [22]smote in python.
x = data_final.loc[:, data_final.columns != 'y']
y = data_final.loc[:, data_final.columns == 'y']
from imblearn.over_sampling import smote
os = smote(random_state=0)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_
state=0)
columns = x_train.columns
os_data_x,os_data_y=os.fit_sample(x_train, y_train)
os_data_x = pd.dataframe(data=os_data_x,columns=columns )
os_data_y= pd.dataframe(data=os_data_y,columns=['y'])
# we can check the numbers of our data
print("length of oversampled data is ",len(os_data_x))
print("number of no subscription in oversampled data",len(os_data_y[os_data_y['y
']==0]))
print("number of subscription",len(os_data_y[os_data_y['y']==1]))
print("proportion of no subscription data in oversampled data is ",len(os_data_y
[os_data_y['y']==0])/len(os_data_x))
print("proportion of subscription data in oversampled data is ",len(os_data_y[os
_data_y['y']==1])/len(os_data_x))

   [1*lybujnfrbr8dyovj664rzq.png]
   figure 16

   now we have a perfect balanced data! you may have noticed that i
   over-sampled only on the training data, because by oversampling only on
   the training data, none of the information in the test data is being
   used to create synthetic observations, therefore, no information will
   bleed from test data into the model training.

recursive feature elimination

   [23]recursive feature elimination (rfe) is based on the idea to
   repeatedly construct a model and choose either the best or worst
   performing feature, setting the feature aside and then repeating the
   process with the rest of the features. this process is applied until
   all features in the dataset are exhausted. the goal of rfe is to select
   features by recursively considering smaller and smaller sets of
   features.
data_final_vars=data_final.columns.values.tolist()
y=['y']
x=[i for i in data_final_vars if i not in y]
from sklearn.feature_selection import rfe
from sklearn.linear_model import logisticregression
logreg = logisticregression()
rfe = rfe(logreg, 20)
rfe = rfe.fit(os_data_x, os_data_y.values.ravel())
print(rfe.support_)
print(rfe.ranking_)

   [1*s-aan0cuvihiugqrdcxq7a.png]
   figure 16

   the rfe has helped us select the following features:    euribor3m   ,
      job_blue-collar   ,    job_housemaid   ,    marital_unknown   ,
      education_illiterate   ,    default_no   ,    default_unknown   ,
      contact_cellular   ,    contact_telephone   ,    month_apr   ,    month_aug   ,
      month_dec   ,    month_jul   ,    month_jun   ,    month_mar   ,    month_may   ,
      month_nov   ,    month_oct   ,    poutcome_failure   ,    poutcome_success   .
cols=['euribor3m', 'job_blue-collar', 'job_housemaid', 'marital_unknown', 'educa
tion_illiterate', 'default_no', 'default_unknown',
      'contact_cellular', 'contact_telephone', 'month_apr', 'month_aug', 'month_
dec', 'month_jul', 'month_jun', 'month_mar',
      'month_may', 'month_nov', 'month_oct', "poutcome_failure", "poutcome_succe
ss"]
x=os_data_x[cols]
y=os_data_y['y']

implementing the model

import statsmodels.api as sm
logit_model=sm.logit(y,x)
result=logit_model.fit()
print(result.summary2())

   [1*r8inm4hfqfpe6kmnjk15fw.png]
   figure 17

   the p-values for most of the variables are smaller than 0.05, except
   four variables, therefore, we will remove them.
cols=['euribor3m', 'job_blue-collar', 'job_housemaid', 'marital_unknown', 'educa
tion_illiterate',
      'month_apr', 'month_aug', 'month_dec', 'month_jul', 'month_jun', 'month_ma
r',
      'month_may', 'month_nov', 'month_oct', "poutcome_failure", "poutcome_succe
ss"]
x=os_data_x[cols]
y=os_data_y['y']
logit_model=sm.logit(y,x)
result=logit_model.fit()
print(result.summary2())

   [1*pzmf6y9sxuzazju83yemdq.png]
   figure 18

id28 model fitting

from sklearn.linear_model import logisticregression
from sklearn import metrics
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_
state=0)
logreg = logisticregression()
logreg.fit(x_train, y_train)

   [1*qmy92xrvtepsf_rfvzddvq.png]
   figure 19

   predicting the test set results and calculating the accuracy
y_pred = logreg.predict(x_test)
print('accuracy of id28 classifier on test set: {:.2f}'.format(lo
greg.score(x_test, y_test)))

   accuracy of id28 classifier on test set: 0.74

confusion matrix

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

   [[6124 1542]

   [2505 5170]]

   the result is telling us that we have 6124+5170 correct predictions and
   2505+1542 incorrect predictions.

compute precision, recall, f-measure and support

   to quote from [24]scikit learn:

   the precision is the ratio tp / (tp + fp) where tp is the number of
   true positives and fp the number of false positives. the precision is
   intuitively the ability of the classifier to not label a sample as
   positive if it is negative.

   the recall is the ratio tp / (tp + fn) where tp is the number of true
   positives and fn the number of false negatives. the recall is
   intuitively the ability of the classifier to find all the positive
   samples.

   the f-beta score can be interpreted as a weighted harmonic mean of the
   precision and recall, where an f-beta score reaches its best value at 1
   and worst score at 0.

   the f-beta score weights the recall more than the precision by a factor
   of beta. beta = 1.0 means recall and precision are equally important.

   the support is the number of occurrences of each class in y_test.
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

   [1*-id18i6fw4h_ab9euexlesa.png]
   figure 20

   interpretation: of the entire test set, 74% of the promoted term
   deposit were the term deposit that the customers liked. of the entire
   test set, 74% of the customer   s preferred term deposits that were
   promoted.

roc curve

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='id28 (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('false positive rate')
plt.ylabel('true positive rate')
plt.title('receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('log_roc')
plt.show()

   [1*uvlp3wn0ln60e2gk2trvpq.png]
   figure 21

   [25]the receiver operating characteristic (roc) curve is another common
   tool used with binary classifiers. the dotted line represents the roc
   curve of a purely random classifier; a good classifier stays as far
   away from that line as possible (toward the top-left corner).

   the jupyter notebook used to make this post is available [26]here. i
   would be pleased to receive feedback or questions on any of the above.

   reference: [27]learning predictive analytics with python book

     * [28]machine learning
     * [29]data science
     * [30]python
     * [31]id28
     * [32]classification

   (button)
   (button)
   (button) 6.3k claps
   (button) (button) (button) 72 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of susan li

[34]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [35]https://www.linkedin.com/in/susanli/

     (button) follow
   [36]towards data science

[37]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 6.3k
     * (button)
     *
     *

   [38]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [39]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/becd4d56c9c8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_dawcxo8gvhuw---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. https://en.wikipedia.org/wiki/logistic_regression
  19. http://archive.ics.uci.edu/ml/index.php
  20. https://raw.githubusercontent.com/madmashup/targeted-marketing-predictive-engine/master/banking.csv
  21. https://arxiv.org/pdf/1106.1813.pdf
  22. http://imbalanced-learn.org/en/stable/over_sampling.html#smote-variants
  23. http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.rfe.html
  24. http://scikit-learn.org/stable/index.html
  25. https://en.wikipedia.org/wiki/receiver_operating_characteristic
  26. https://github.com/susanli2016/machine-learning-with-python/blob/master/id28 balanced.ipynb
  27. https://books.google.com/books/about/learning_predictive_analytics_with_pytho.html?id=ia5kdaaaqbaj&printsec=frontcover&source=kp_read_button#v=onepage&q&f=false
  28. https://towardsdatascience.com/tagged/machine-learning?source=post
  29. https://towardsdatascience.com/tagged/data-science?source=post
  30. https://towardsdatascience.com/tagged/python?source=post
  31. https://towardsdatascience.com/tagged/logistic-regression?source=post
  32. https://towardsdatascience.com/tagged/classification?source=post
  33. https://towardsdatascience.com/@actsusanli?source=footer_card
  34. https://towardsdatascience.com/@actsusanli
  35. https://www.linkedin.com/in/susanli/
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/?source=footer_card
  38. https://towardsdatascience.com/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/becd4d56c9c8/share/twitter
  42. https://medium.com/p/becd4d56c9c8/share/facebook
  43. https://medium.com/p/becd4d56c9c8/share/twitter
  44. https://medium.com/p/becd4d56c9c8/share/facebook
