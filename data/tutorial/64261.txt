     * search
       ____________________

     * [1]news
     * [2]jobs
     [3]register/login
       [4]log out

     * [5]news
     * [6]jobs

   [7]register/login
   [8]log out

   (button)
     * [9]news
     * [10]jobs

     * [11]latest news
     * [12]startup
     * [13]funding
     * [14]artificial intelligence
     * [15]submit an article

   [16]nand kishor contributor

   nand kishor is the product manager of house of bots. after finishing
   his studies in computer science, he ideated & re-launched real estate
   business intelligence tool, where he created one of the leading
   business intelligence tool for property price analysis in 2012. he also
   writes, research and sharing knowledge about artificial intelligence
   (ai), machine learning (ml), data science, big data, python language
   etc... ...
   [17]full bio
   follow on
   (*) full bio ( ) recent posts ( ) popular posts

   nand kishor is the product manager of house of bots. after finishing
   his studies in computer science, he ideated & re-launched real estate
   business intelligence tool, where he created one of the leading
   business intelligence tool for property price analysis in 2012. he also
   writes, research and sharing knowledge about artificial intelligence
   (ai), machine learning (ml), data science, big data, python language
   etc...

   [18]3 best programming languages for internet of things development in
   2018
   176 days ago

   [19]data science is the big draw in business schools
   349 days ago

   [20]7 effective methods for fitting a liner
   359 days ago

   [21]3 thoughts on why deep learning works so well
   359 days ago

   [22]3 million at risk from the rise of robots
   359 days ago

   [23]top 10 hot artificial intelligence (ai) technologies
   290766 views

   [24]here's why so many data scientists are leaving their jobs
   78285 views

   [25]want to be a millionaire before you turn 25? study artificial
   intelligence or machine learning
   71850 views

   [26]2018 data science interview questions for top tech companies
   68901 views

   [27]google announces scholarship program to train 1.3 lakh indian
   developers in emerging technologies
   59307 views
   [28]architecture [29]deep learning [30]machine learning [31]neural
   networks

the 8 neural network architectures machine learning researchers need to learn

   share this on
   by [32]nand kishor |[33]email | mar 22, 2018 | 36183 views

   in this blog post, i want to share the 8 neural network architectures
   from the course that i believe any machine learning researchers should
   be familiar with to advance their work.
   [0img.png]
   why do we need machine learning?
   machine learning is needed for tasks that are too complex for humans to
   code directly. some tasks are so complex that it is impractical, if not
   impossible, for humans to work out all of the nuances and code for them
   explicitly. so instead, we provide a large amount of data to a machine
   learning algorithm and let the algorithm work it out by exploring that
   data and searching for a model that will achieve what the programmers
   have set it out to achieve.
   let's look at these 2 examples:
     * it is very hard to write programs that solve problems like
       recognizing a 3-dimensional object from a novel viewpoint in new
       lighting conditions in a cluttered scene. we don't know what
       program to write because we don't know how it's done in our brain.
       even if we had a good idea about how to do it, the program might be
       horrendously complicated.
     * it is hard to write a program to compute the id203 that a
       credit card transaction is fraudulent. there may not be any rules
       that are both simple and reliable. we need to combine a very large
       number of weak rules. fraud is a moving target but the program
       needs to keep changing.

   then comes the machine learning approach: instead of writing a program
   by hand for each specific task, we collect lots of examples that
   specify the correct output for a given input. a machine learning
   algorithm then takes these examples and produces a program that does
   the job. the program produced by the learning algorithm may look very
   different from a typical hand-written program. it may contain millions
   of numbers. if we do it right, the program works for new cases as well
   as the ones we trained it on. if the data changes the program can
   change too by training on the new data. you should note that massive
   amounts of computation are now cheaper than paying someone to write a
   task-specific program.
   given that, some examples of tasks best solved by machine learning
   include:
     * recognizing patterns: objects in real scenes, facial identities or
       facial expressions, spoken words
     * recognizing anomalies: unusual sequences of credit card
       transactions, unusual patterns of sensor readings in a nuclear
       power plant
     * prediction: future stock prices or currency exchange rates, which
       movies will a person like

   what are neural networks?
   neural networks are a class of models within the general machine
   learning literature. so for example, if you took a coursera course on
   machine learning, neural networks will likely be covered. neural
   networks are a specific set of algorithms that has revolutionized the
   field of machine learning. they are inspired by biological neural
   networks and the current so called deep neural networks have proven to
   work quite very well. neural networks are themselves general function
   approximations, that is why they can be applied to literally almost any
   machine learning problem where the problem is about learning a complex
   mapping from the input to the output space.
   here are the 3 reasons to convince you to study neural computation:
     * to understand how the brain actually works: it's very big and very
       complicated and made of stuff that dies when you poke it around. so
       we need to use computer simulations.
     * to understand a style of parallel computation inspired by neurons
       and their adaptive connections: it's a very different style from a
       sequential computation.
     * to solve practical problems by using novel learning algorithms
       inspired by the brain: learning algorithms can be very useful even
       if they are not how the brain actually works.

   after finishing the famous [34]andrew ng's machine learning coursera
   course, i started developing interest towards neural networks and deep
   learning. thus, i started looking at the best online resources to learn
   about the topics and found [35]geoffrey hinton's neural networks for
   machine learning course. if you are a deep learning practitioner or
   someone who want to get into the deep learning/machine learning world,
   you should really take this course. geoffrey hinton is without a doubt
   a godfather of the deep learning world. and he actually provided
   something extraordinary in this course. in this blog post, i want to
   share the 8 neural network architectures from the course that i believe
   any machine learning researchers should be familiar with to advance
   their work.
   [1img.png]
   generally, these architectures can be put into 3 specific categories:
   1  ??-  ??feed-forward neural networks
   these are the commonest type of neural network in practical
   applications. the first layer is the input and the last layer is the
   output. if there is more than one hidden layer, we call them   ??deep  ??
   neural networks. they compute a series of transformations that change
   the similarities between cases. the activities of the neurons in each
   layer are a non-linear function of the activities in the layer below.
   2  ??-  ??recurrent networks
   these have directed cycles in their connection graph. that means you
   can sometimes get back to where you started by following the arrows.
   they can have complicated dynamics and this can make them very
   difficult to train. they are more biologically realistic.
   there is a lot of interest at present in finding efficient ways of
   training recurrent nets. recurrent neural networks are a very natural
   way to model sequential data. they are equivalent to very deep nets
   with one hidden layer per time slice; except that they use the same
   weights at every time slice and they get input at every time slice.
   they have the ability to remember information in their hidden state for
   a long time but is very hard to train them to use this potential.
   3  ??-  ??symmetrically connected networks
   these are like recurrent networks, but the connections between units
   are symmetrical (they have the same weight in both directions).
   symmetric networks are much easier to analyze than recurrent networks.
   they are also more restricted in what they can do because they obey an
   energy function. symmetrically connected nets without hidden units are
   called   ??hopfield nets.  ?? symmetrically connected network with hidden
   units are called   ??id82s.  ??
   1  ??-  ??id88s
    considered the first generation of neural networks, id88s are
   simply computational models of a single neuron. they were popularized
   by [36]frank rosenblatt in the early 1960s. they appeared to have a
   very powerful learning algorithm and lots of grand claims were made for
   what they could learn to do. in 1969, minsky and papers published a
   book called "[37]id88s" that analyzed what they could do and
   showed their limitations. many people thought these limitations applied
   to all neural network models. however, the id88 learning
   procedure is still widely used today for tasks with enormous feature
   vectors that contain many millions of features.
   [2img.png]
   in the standard paradigm for statistical pattern recognition, we first
   convert the raw input vector into a vector of feature activations. we
   then use hand-written programs based on common-sense to define the
   features. next, we learn how to weight each of the feature activations
   to get a single scalar quantity. if this quantity is above some
   threshold, we decide that the input vector is a positive example of the
   target class.
   the standard id88 architecture follows the feed-forward model,
   meaning inputs are sent into the neuron, are processed, and result in
   an output. in the diagram below, this means the network reads
   bottom-up: input comes in from the bottom and output goes out from the
   top.
   [3img.png]
   however, id88s do have limitations: if you are followed to choose
   the features by hand and if you use enough features, you can do almost
   anything. for binary input vectors, we can have a separate feature unit
   for each of the exponentially many binary vectors and so we can make
   any possible discrimination on binary input vectors. but once the
   hand-coded features have been determined, there are very strong
   limitations on what a id88 can learn.
   this result is devastating for id88s because the whole point of
   pattern recognition is to recognize patterns despite transformations
   like translation. minsky and papert's   ??group invariance theorem  ??
   says that the part of a id88 that learns cannot learn to do this
   if the transformations form a group. to deal with such transformations,
   a id88 needs to use multiple feature units to recognize
   transformations of informative sub-patterns. so the tricky part of
   pattern recognition must be solved by the hand-coded feature detectors,
   not the learning procedure.
   networks without hidden units are very limited in the input-output
   mappings they can learn to model. more layers of linear units do not
   help. it's still linear. fixed output non-linearities are not enough.
   thus, we need multiple layers of adaptive, non-linear hidden units. but
   how we train such nets? we need an efficient way of adapting all the
   weights, not just the last layer. this is hard. learning the weights
   going into hidden units is equivalent to learning features. this is
   difficult because nobody is telling us directly what the hidden units
   should do.
   2  ??-  ??convolutional neural networks
    machine learning research has focused extensively on id164
   problems over the time. there are various things that make it hard to
   recognize objects:
     * segmentation: real scenes are cluttered with other objects. it's
       hard to tell which pieces go together as parts of the same object.
       parts of an object can be hidden behind other objects.
     * lighting: the intensities of the pixels are determined as much by
       the lighting as by the objects.
     * deformation: objects can deform in a variety of non-affine ways.
       e.g., a handwritten too can have a large loop or just a cusp.
     * affordances: object classes are often defined by how they are used.
       e.g., chairs are things designed for sitting on so they have a wide
       variety of physical shapes.
     * viewpoint: changes in viewpoint cause changes in images that
       standard learning methods cannot cope with. information hops
       between input dimensions (i.e. pixels)
     * imagine a medical database in which the age of a patient sometimes
       hopes to the input dimension that normally codes for weight! to
       apply machine learning we would first want to eliminate this
       dimension-hopping.

   [4img.png]
   the replicated feature approach is currently the dominant approach for
   neural networks to solve id164 problem. it uses many
   different copies of the same feature detector with different positions.
   it could also replicate across scale and orientation, which is tricky
   and expensive. replication greatly reduces the number of free
   parameters to be learned. it uses several different feature types, each
   with its own map of replicated detectors. it also allows each patch of
   image to be represented in several ways.
   so what does replicating the feature detectors achieve?
     * equivalent activities: replicated features do not make the neural
       activities invariant to translation. the activities of are
       equivariant.
     * invariant knowledge: if a feature is useful in some locations
       during training, detectors for that feature will be available in
       all locations during testing.

   in 1998, yann lecun and his collaborators developed a really good
   recognizer for handwritten digits called [38]lenet. it used back
   propagation in a feedforward net with many hidden layers, many maps of
   replicated units in each layer, pooling of the outputs of nearby
   replicated units, a wide net that can cope with several characters at
   once even if they overlap, and a clever way of training a complete
   system, not just a recognizer. later it is formalized under the name
   convolutional neural networks. fun fact: this net was used for reading
   ~10% of the checks in north america.
   [5img.png]
   convolutional neural networks can be used for all work related to
   object recognition from hand-written digits to 3d objects. however,
   recognizing real objects in color photographs downloaded from the web
   is much more complicated than recognizing hand-written digits. there
   are hundred times as many classes (1000 vs 10), hundred times as many
   pixels (256 x 256 color vs 28 x 28 gray), two-dimensional images of
   three-dimensional scenes, cluttered scenes requiring segmentation, and
   multiple objects in each image. will the same type of convolutional
   neural network work?
   then came [39]the ilsvrc-2012 competition on id163, a dataset with
   approximately 1.2 million high-resolution training images. test images
   will be presented with no initial annotation (no segmentation or
   labels) and algorithms will have to produce labelings specifying what
   objects are present in the images. some of the best existing computer
   vision methods were tried on this dataset by leading id161
   groups from oxford, inria, xrce  ?   typically, id161 systems
   use complicated multi-stage systems and the early stages are typically
   hand-tuned by optimizing a few parameters.
   the winner of the competition, alex krizhevsky (nips 2012), developed a
   very deep convolutional neural net of the type pioneered by yann lecun.
   its architecture includes 7 hidden layers not counting some max-pooling
   layers. the early layers were convolutional, while the last 2 layers
   were globally connected. the id180 were rectified linear
   units in every hidden layer. these train much faster and are more
   expressive than logistic units. in addition to that, it also uses
   competitive id172 to suppress hidden activities when nearby
   units have stronger activities. this helps with variations in
   intensity.
   there are a couple of technical tricks that significantly improve
   generalization for the neural net:
     * training on random 224 x 224 patches from the 256 x 256 images to
       get more data and using left-right reflections of the images. at
       test time, combining the opinions from 10 different patches: the
       four 224 x 224 corner patches plus the central 224 x 224 patch plus
       the reflections of those 5 patches.
     * using   ??dropout  ?? to regularize the weights in the globally
       connected layers (which contain most of the parameters). dropout
       means that half of the hidden units in a layer are randomly removed
       for each training example. this stops hidden units from relying too
       much on other hidden units.

   [6img.png]
   in terms of hardware requirement, alex uses a very efficient
   implementation of convolutional nets on 2 nvidia gtx 580 gpus (over
   1000 fast little cores). the gpus are very good for matrix-matrix
   multiplies and also have very high bandwidth to memory. this allows him
   to train the network in a week and makes it quick to combine results
   from 10 patches at test time. we can spread a network over many cores
   if we can communicate the states fast enough. as cores get cheaper and
   datasets get bigger, big neural nets will improve faster than
   old-fashioned id161 systems.

   3  ??-  ??recurrent neural network
   [7img.png]
   to understand id56s, we need to have a brief overview of sequence
   modeling. when applying machine learning to sequences, we often want to
   turn an input sequence into an output sequence that lives in a
   different domain; for example, turn a sequence of sound pressures into
   a sequence of word identities. when there is no separate target
   sequence, we can get a teaching signal by trying to predict the next
   term in the input sequence. the target output sequence is the input
   sequence with an advance of 1 step. this seems much more natural than
   trying to predict one pixel in an image from the other pixels, or one
   patch of an image from the rest of the image. predicting the next term
   in a sequence blurs the distinction between supervised and unsupervised
   learning. it uses methods designed for supervised learning, but it
   doesn't require a separate teaching signal.
   memoryless models are the standard approach to this task. in
   particular, autoregressive models can predict the next term in a
   sequence from a fixed number of previous terms using   ??delay taps; and
   feed-forward neural nets are generalized autoregressive models that use
   one or more layers of non-linear hidden units. however, if we give our
   generative model some hidden state, and if we give this hidden state
   its own internal dynamics, we get a much more interesting kind of
   model: it can store information in its hidden state for a long time. if
   the dynamics are noisy and the way they generate outputs from their
   hidden state is noisy, we can never know its exact hidden state. the
   best we can do is to infer a id203 distribution over the space of
   hidden state vectors. this id136 is only tractable for 2 types of
   hidden state model.
   recurrent neural networks are very powerful, because they combine 2
   properties: 1) distributed hidden state that allows them to store a lot
   of information about the past efficiently, and 2) non-linear dynamics
   that allow them to update their hidden state in complicated ways. with
   enough neurons and time, id56s can compute anything that can be computed
   by your computer. so what kinds of behavior can id56s exhibit? they can
   oscillate, they can settle to point attractors, they can behave
   chaotically. and they could potentially learn to implement lots of
   small programs that each capture a nugget of knowledge and run in
   parallel, interacting to produce very complicated effects.
   [8img.png]
   however, the computational power of id56s makes them very hard to train.
   it is quite difficult to train a id56 because of the exploding or
   vanishing gradients problem. as we backpropagate through many layers,
   what happens to the magnitude of the gradients? if the weights are
   small, the gradients shrink exponentially. if the weights are big, the
   gradients grow exponentially. typical feed-forward neural nets can cope
   with these exponential effects because they only have a few hidden
   layers. on the other hand, in a id56 trained on long sequences, the
   gradients can easily explode or vanish. even with good initial weights,
   it's very hard to detect that the current target output depends on an
   input from many time-steps ago, so id56s have difficulty dealing with
   long-range dependencies.
   there are essentially 4 effective ways to learn a id56:
     * long short term memory: make the id56 out of little modules that are
       designed to remember values for a long time.
     * hessian free optimization: deal with the vanishing gradients
       problem by using a fancy optimizer that can detect directions with
       a tiny gradient but even smaller curvature.
     * echo state networks: initialize the input -> hidden and hidden ->
       hidden and output -> hidden connections very carefully so that the
       hidden state has a huge reservoir of weakly coupled oscillators
       which can be selectively driven by the input.
     * good initialization with momentum: initialize like in echo state
       networks, but then learn all of the connections using momentum.


   4  ??-  ??long/short term memory network
   [9img.png]
   [40]hochreiter & schmidhuber (1997) solved the problem of getting a id56
   to remember things for a long time (like hundreds of time steps) by
   building what known as long-short term memory network. they designed a
   memory cell using logistic and linear units with multiplicative
   interactions. information gets into the cell whenever its   ??write  ??
   gate is on. the information stays in the cell so long as its   ??keep  ??
   gate is on. information can be read from the cell by turning on its
     ??read  ?? gate.
   reading cursive handwriting is a natural task for an id56. the input is
   a sequence of (x, y, p) coordinates of the tip of the pen, where p
   indicates whether the pen is up or down. the output is a sequence of
   characters. [41]graves & schmidhuber (2009) showed that id56s with lstm
   are currently the best systems for reading cursive writing. in brief,
   they used a sequence of small images as input rather than pen
   coordinates.
   5  ??-  ??hopfield networks

   recurrent networks of non-linear units are generally very hard to
   analyze. they can behave in many different ways: settle to a stable
   state, oscillate, or follow chaotic trajectories that cannot be
   predicted far into the future. a hopfield net is composed of binary
   threshold units with recurrent connections between them. in 1982,
   [42]john hopfield realized that if the connections are symmetric, there
   is a global energy function. each binary   ??configuration  ?? of the
   whole network has an energy; while the binary threshold decision rule
   causes the network to settle for a minimum of this energy function. a
   neat way to make use of this type of computation is to use memories as
   energy minima for the neural net. using energy minima to represent
   memories gives a content-addressable memory. an item can be accessed by
   just knowing part of its content. it is robust against hardware damage.
   [10img.png]
   each time we memorize a configuration, we hope to create a new energy
   minimum. but what if two nearby minima at an intermediate location?
   this limits the capacity of a hopfield net. so how do we increase the
   capacity of a hopfield net? physicists love the idea that the math they
   already know might explain how the brain works. many papers were
   published in physics journals about hopfield nets and their storage
   capacity. eventually, elizabeth gardnerfigured out that there was a
   much better storage rule that uses the full capacity of the weights.
   instead of trying to store vectors in one shot, she cycled through the
   training set many times and used the id88 convergence procedure
   to train each unit to have the correct state given the states of all
   the other units in that vector. statisticians call this technique
     ??pseudo-likelihood.  ??
   there is another computational role for hopfield nets. instead of using
   the net to store memories, we use it to construct interpretations of
   sensory input. the input is represented by the visible units, the
   interpretation is represented by the states of the hidden units, and
   the badness of the interpretation is represented by the energy.
   6  ??-  ??id82 network
    a id82 is a type of stochastic recurrent neural network.
   it can be seen as the stochastic, generative counterpart of hopfield
   nets. it was one of the first neural networks capable of learning
   internal representations and is able to represent and solve difficult
   combinatoric problems.
   [11img.png]
   the goal of learning for id82 learning algorithm is to
   maximize the product of the probabilities that the id82
   assigns to the binary vectors in the training set. this is equivalent
   to maximizing the sum of the log probabilities that the boltzmann
   machine assigns to the training vectors. it is also equivalent to
   maximizing the id203 that we would obtain exactly the n training
   cases if we did the following: 1) let the network settle to its
   stationary distribution n different time with no external input; and 2)
   sample the visible vector once each time.
   an efficient mini-batch learning procedure was proposed for boltzmann
   machines by salakhutdinov and hinton in 2012.
     * for the positive phase, first initialize the hidden probabilities
       at 0.5, then clamp a data vector on the visible units, then update
       all the hidden units in parallel until convergence using mean field
       updates. after the net has converged, record pipj for every
       connected pair of units and average this over all data in the
       mini-batch.
     * for the negative phase: first keep a set of   ??fantasy
       particles.  ?? each particle has a value that is a global
       configuration. then sequentially update all the units in each
       fantasy particle a few times. for every connected pair of units,
       average sisj over all the fantasy particles.

   in a general id82, the stochastic updates of units need to
   be sequential. there is a special architecture that allows alternating
   parallel updates which are much more efficient (no connections within a
   layer, no skip-layer connections). this mini-batch procedure makes the
   updates of the id82 more parallel. this is called a deep
   id82 (dbm), a general id82 with a lot of
   missing connections.
   [12img.png]
   in 2014, salakhutdinov and hinton came up with another update for their
   model, calling it restricted id82s. they restrict the
   connectivity to make id136 and learning easier (only one layer of
   hidden units and no connections between hidden units). in an rbm it
   only takes one step to reach thermal equilibrium when the visible units
   are clamped.
   another efficient mini-batch learning procedure for rbm goes like this:
     * for the positive phase, first clamp a data vector on the visible
       units. then compute the exact value of <vihj> for all pairs of a
       visible and a hidden unit. for every connected pair of units,
       average <vihj> over all data in the mini-batch.
     * for the negative phase, also keep a set of   ??fantasy particles.  ??
       then update each fantasy particle a few times using alternating
       parallel updates. for every connected pair of units, average vihj
       over all the fantasy particles.


   7  ??-  ??deep belief network
   [13img.png]
   back-propagation is considered the standard method in artificial neural
   networks to calculate the error contribution of each neuron after a
   batch of data is processed. however, there are some major problems
   using back-propagation. firstly, it requires labeled training data;
   while almost all data is unlabeled. secondly, the learning time does
   not scale well, which means it is very slow in networks with multiple
   hidden layers. thirdly, it can get stuck in poor local optima, so for
   deep nets they are far from optimal.
   to overcome the limitations of back-propagation, researchers have
   considered using unsupervised learning approaches. this helps keep the
   efficiency and simplicity of using a gradient method for adjusting the
   weights, but also use it for modeling the structure of the sensory
   input. in particular, they adjust the weights to maximize the
   id203 that a generative model would have generated the sensory
   input. the question is what kind of generative model should we learn?
   can it be an energy-based model like a id82? or a causal
   model made of idealized neurons? or a hybrid of the two?
   [14img.png]
   a belief net is a directed acyclic graph composed of stochastic
   variables. using belief net, we get to observe some of the variables
   and we would like to solve 2 problems: 1) the id136 problem: infer
   the states of the unobserved variables, and 2) the learning problem:
   adjust the interactions between variables to make the network more
   likely to generate the training data.
   early id114 used experts to define the graph structure and
   the conditional probabilities. by then, the graphs were sparsely
   connected; so researchers initially focused on doing correct id136,
   not on learning. for neural nets, learning was central and hand-writing
   the knowledge was not cool, because knowledge came from learning the
   training data. neural networks did not aim for interpretability or
   sparse connectivity to make id136 easy. nevertheless, there are
   neural network versions of belief nets.
   there are two types of generative neural network composed of stochastic
   binary neurons: 1) energy-based, in which we connect binary stochastic
   neurons using symmetric connections to get a id82; and 2)
   causal, in which we connect binary stochastic neurons in a directed
   acyclic graph to get a sigmoid belief net. the descriptions of these
   two types go beyond the scope of this article.
   8  ??-  ??deep auto-encoders
   [15img.png]
   finally, let's discuss deep auto-encoders. they always looked like a
   really nice way to do non-linear id84 because of a
   few reasons: they provide flexible mappings both ways. the learning
   time is linear (or better) in the number of training cases. and the
   final encoding model is fairly compact and fast. however, it turned out
   to be very difficult to optimize deep auto encoders using back
   propagation. with small initial weights, the back propagated gradient
   dies. we now have a much better ways to optimize them; either use
   unsupervised layer-by-layer pre-training or just initialize the weights
   carefully as in echo-state nets.
   for pre-training task, there are actually 3 different types of shallow
   auto-encoders:
    1. rbm's as auto-encoders: when we train an rbm with one-step
       contrastive divergence, it tries to make the reconstructions look
       like data. it's like an auto encoder, but it's strongly regularized
       by using binary activities in the hidden layer. when trained with
       maximum likelihood, rbms are not like auto encoders. we can replace
       the stack of rbm's used for pre-training by a stack of shallow auto
       encoders; however pre-training is not as effective (for subsequent
       discrimination) if the shallow auto encoders are regularized by
       penalizing the squared weights.
    2. denoising auto encoders: these add noise to the input vector by
       setting many of its components to 0 (like dropout, but for inputs).
       they are still required to reconstructing these components so they
       must extract features that capture correlations between inputs.
       pre-training is very effective if we use a stack of denoting auto
       encoders. it's as good as or better than pre-training with rbms.
       it's also simpler to evaluate the pre-training because we can
       easily compute the value of the objective function. it lacks the
       nice variational bound we get with rbms, but this is only of
       theoretical interest.
    3. contractive auto encoders: another way to regularize an auto
       encoder is to try to make the activities of the hidden units as
       insensitive as possible to the inputs; but they cannot just ignore
       the inputs because they must reconstruct them. we achieve this by
       penalizing the squared gradient of each hidden activity with
       respect to the inputs. contractive auto encoders work very well for
       pre-training. the codes tend to have the property that only a small
       subset of the hidden units are sensitive to changes in the input.

   [16img.png]
   in brief, there are now many different ways to do layer-by-layer
   pre-training of features. for datasets that do not have huge numbers of
   labeled cases, pre-training helps subsequent discriminative learning.
   for very large, labeled datasets, initializing the weights used in
   supervised learning by using unsupervised pre-training is not
   necessary, even for deep nets. pre-training was the first good way to
   initialize the weights for deep nets, but now there are other ways. but
   if we make the nets much larger, we will need pre-training again!
   last takeaway
   neural networks are one of the most beautiful programming paradigms
   ever invented. in the conventional approach to programming, we tell the
   computer what to do, breaking big problems up into many small,
   precisely defined tasks that the computer can easily perform. by
   contrast, in a neural network we don't tell the computer how to solve
   our problem. instead, it learns from observational data, figuring out
   its own solution to the problem at hand.
   today, deep neural networks and deep learning achieve outstanding
   performance on many important problems in id161, speech
   recognition, and natural language processing. they're being deployed on
   a large scale by companies such as google, microsoft, and facebook.
   i hope that this post helps you learn the core concepts of neural
   networks, including modern techniques for deep learning. you can get
   all the lecture slides, research papers and programming assignments i
   have done for dr. hinton's coursera course from my [43]github repo
   here. good luck studying!
   source: kdnugget
   (button) subscribe to updates (button) unsubscribe from updates

   share this on

   [44]previous story [45]next story

most read news

   [46]highest paying programming language, skills: here are the top
   earners
   [47]every programmer should strive for reading these 5 books
   [48]which programming languages in demand & earn the highest salaries?
   [49]top 10 best countries for software engineers to work & high
   in-demand programming languages
   [50]top 10 hot artificial intelligence (ai) technologies

job openings

   [51]java microservices developer @ tcs
   posted today | 201 views
   [52]automation engineer @ verifone
   posted today | 225 views
   [53]data scientist @ airbnb
   posted today | 303 views
   [54]senior sql dba @ barclays
   posted today | 288 views
   [55]data scientist-machine learning @ boeing
   posted today | 909 views

latest news [56]view all

   [57]stay tuned with data science updates on top linkedin groups
   apr 5, 2019
   [58]facial recognition system used in schools of china to analyze
   students' emotions
   apr 5, 2019
   [59]equating the workings between iot and ai
   apr 5, 2019
   [60]a beginner must avoid these mistakes in data science
   apr 5, 2019
   [61]being an intern for data science will take you to heights
   apr 5, 2019

recommended for you

     *

being an intern for data science will take you to heights
       so if you are willing to go for an internship in data science, just
       don't hold your hands and go, be prepared as it is not that easy,
       you just have to put efforts to make your future and face
       challenges too.
       hob
       894
     *

nlp is enhancing our knowledge and giving a way to innovative ideas
       natural language processing is an interdisciplinary field that
       includes both computer science and linguistics. as humans, we have
       an innate ability to understand other people who speak the same
       language.
       hob
       933
     *

here is the elementary study of deep learning algorithms
       data is run by deep learning algorithms through multiple layers of
       the algorithms of the neural network each of which passes a
       simplified representation of the data to the next layer.
       hob
       8535
     *

learn at home and master machine learning through these books
       books highlighting you with some machine learning content for
       fresher graduates as well as professionals.
       hob
       2058

   valuefirst digital media pvt. ltd.
   b-18, sector-34, infocity, gurugram,
   122001, india.
   [62]about us [63]contact us [64]privacy policy
   [65]twitter [66]facebook [67]skype
   house of bots @ 2019, valuefirst

references

   visible links
   1. http://houseofbots.com/bot-latest-news
   2. http://houseofbots.com/all-jobs
   3. http://houseofbots.com/1/overlaylogin.jsp
   4. http://houseofbots.com/3/logout
   5. http://houseofbots.com/bot-latest-news
   6. http://houseofbots.com/all-jobs
   7. http://houseofbots.com/1/overlaylogin.jsp
   8. http://houseofbots.com/3/logout
   9. http://houseofbots.com/bot-latest-news
  10. http://houseofbots.com/all-jobs
  11. http://houseofbots.com/bot-latest-news
  12. http://houseofbots.com/bot-startup
  13. http://houseofbots.com/bot-funding
  14. http://houseofbots.com/artificial-intelligence
  15. http://houseofbots.com/1/submit-news
  16. http://houseofbots.com/profile/nandkishor1
  17. http://houseofbots.com/profile/1
  18. http://houseofbots.com/news-detail/3722-4-3-best-programming-languages-for-internet-of-things-development-in-2018
  19. http://houseofbots.com/news-detail/2761-4-data-science-is-the-big-draw-in-business-schools
  20. http://houseofbots.com/news-detail/2659-4-7-effective-methods-for-fitting-a-liner
  21. http://houseofbots.com/news-detail/2658-4-3-thoughts-on-why-deep-learning-works-so-well
  22. http://houseofbots.com/news-detail/2656-4-3-million-at-risk-from-the-rise-of-robots
  23. http://houseofbots.com/news-detail/1606-1-top-10-hot-artificial-intelligence-ai-technologies
  24. http://houseofbots.com/news-detail/2547-4-here's-why-so-many-data-scientists-are-leaving-their-jobs
  25. http://houseofbots.com/news-detail/1335-1-want-to-be-a-millionaire-before-you-turn-25-study-artificial-intelligence-or-machine-learning
  26. http://houseofbots.com/news-detail/2464-4-2018-data-science-interview-questions-for-top-tech-companies
  27. http://houseofbots.com/news-detail/1584-4-google-announces-scholarship-program-to-train-1point3-lakh-indian-developers-in-emerging-technologies
  28. http://houseofbots.com/tagged-news?tag=architecture
  29. http://houseofbots.com/tagged-news?tag= deep learning
  30. http://houseofbots.com/tagged-news?tag= machine learning
  31. http://houseofbots.com/tagged-news?tag= neural networks
  32. http://houseofbots.com/profile/nandkishor1
  33. mailto:nand.kishor@vfirst.com
  34. https://github.com/khanhnaid1131994/machine-learning
  35. https://github.com/khanhnaid1131994/neural-nets
  36. https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/
  37. https://mitpress.mit.edu/books/id88s
  38. http://yann.lecun.com/exdb/lenet/
  39. http://www.image-net.org/challenges/lsvrc/2012/
  40. http://www.bioinf.jku.at/publications/older/2604.pdf
  41. http://people.idsia.ch/~juergen/nips2009.pdf
  42. http://www.pnas.org/content/79/8/2554.full.pdf
  43. https://github.com/khanhnaid1131994/neural-nets
  44. http://houseofbots.com/news-detail/2426-sterling-paper-acquires-philippines-ai-powered-chatbot-startup-chatbotph
  45. http://houseofbots.com/news-detail/2428-dsfaf
  46. http://houseofbots.com/news-detail/11594-4-highest-paying-programming-language-skills-here-are-the-top-earners
  47. http://houseofbots.com/news-detail/4630-1-every-programmer-should-strive-for-reading-these-5-books
  48. http://houseofbots.com/news-detail/3692-4-which-programming-languages-in-demand-&-earn-the-highest-salaries
  49. http://houseofbots.com/news-detail/3951-1-top-10-best-countries-for-software-engineers-to-work-&-high-in-demand-programming-languages
  50. http://houseofbots.com/news-detail/1606-1-top-10-hot-artificial-intelligence-ai-technologies
  51. http://houseofbots.com/job-detail/839-java microservices developer
  52. http://houseofbots.com/job-detail/838-automation engineer
  53. http://houseofbots.com/job-detail/837-data scientist
  54. http://houseofbots.com/job-detail/836-senior sql dba
  55. http://houseofbots.com/job-detail/835-data scientist-machine learning
  56. http://houseofbots.com/bot-latest-news
  57. http://houseofbots.com/news-detail/11758-1-stay-tuned-with-data-science-updates-on-top-linkedin-groups
  58. http://houseofbots.com/news-detail/11757-1-facial-recognition-system-used-in-schools-of-china-to-analyze-students-emotions
  59. http://houseofbots.com/news-detail/11756-1-equating-the-workings-between-iot-and-ai
  60. http://houseofbots.com/news-detail/11755-1-a-beginner-must-avoid-these-mistakes-in-data-science
  61. http://houseofbots.com/news-detail/11754-1-being-an-intern-for-data-science-will-take-you-to-heights
  62. http://houseofbots.com/aboutus
  63. http://houseofbots.com/contactus
  64. http://houseofbots.com/privacy
  65. http://houseofbots.com/news-detail/2427-4-the-8-neural-network-architectures-machine-learning-researchers-need-to-learn
  66. http://houseofbots.com/news-detail/2427-4-the-8-neural-network-architectures-machine-learning-researchers-need-to-learn
  67. http://houseofbots.com/news-detail/2427-4-the-8-neural-network-architectures-machine-learning-researchers-need-to-learn

   hidden links:
  69. http://houseofbots.com/
  70. http://houseofbots.com/
  71. http://houseofbots.com/
  72. https://linkedin.com/in/nkishorp
  73. https://www.pinterest.com/pin/create/button/?url=https://www.houseofbots.com/news-detail/2427-the-8-neural-network-architectures-machine-learning-researchers-need-to-learn
  74. http://houseofbots.com/news-detail/11754-1-being-an-intern-for-data-science-will-take-you-to-heights
  75. http://houseofbots.com/news-detail/11753-1-nlp-is-enhancing-our-knowledge-and-giving-a-way-to-innovative-ideas
  76. http://houseofbots.com/news-detail/11747-1-here-is-the-elementary-study-of-deep-learning-algorithms
  77. http://houseofbots.com/news-detail/11744-1-learn-at-home-and-master-machine-learning-through-these-books
