   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

introduction to k-nearest-neighbors

what is k-nearest-neighbors (knn), some useful applications, and how it works

   [16]go to the profile of devin soni
   [17]devin soni (button) blockedunblock (button) followfollowing
   mar 12, 2018
   [1*ndgsyok0o4ir5dq99v3jvg.gif]

   the k-nearest-neighbors (knn) method of classification is one of the
   simplest methods in machine learning, and is a great way to introduce
   yourself to machine learning and classification in general. at its most
   basic level, it is essentially classification by finding the most
   similar data points in the training data, and making an educated guess
   based on their classifications. although very simple to understand and
   implement, this method has seen wide application in many domains, such
   as in id126s, semantic searching, and anomaly
   detection.
     __________________________________________________________________

   as we would need to in any machine learning problem, we must first find
   a way to represent data points as feature vectors. a feature vector is
   our mathematical representation of data, and since the desired
   characteristics of our data may not be inherently numerical,
   preprocessing and feature-engineering may be required in order to
   create these vectors. given data with n unique features, the feature
   vector would be a vector of length n, where entry i of the vector
   represents that data point   s value for feature i. each feature vector
   can thus be thought of as a point in r^n.

   now, unlike most other methods of classification, knn falls under lazy
   learning, which means that there is no explicit training phase before
   classification. instead, any attempts to generalize or abstract the
   data is made upon classification. while this does mean that we can
   immediately begin classifying once we have our data, there are some
   inherent problems with this type of algorithm. we must be able to keep
   the entire training set in memory unless we apply some type of
   reduction to the data-set, and performing classifications can be
   computationally expensive as the algorithm parse through all data
   points for each classification. for these reasons, knn tends to work
   best on smaller data-sets that do not have many features.
     __________________________________________________________________

   once we have formed our training data-set, which is represented as an m
   x n matrix where m is the number of data points and n is the number of
   features, we can now begin classifying. the gist of the knn method is,
   for each classification query, to:
1. compute a distance value between the item to be classified and every item in
the training data-set
2. pick the k closest data points (the items with the k lowest distances)
3. conduct a    majority vote    among those data points     the dominating classifica
tion in that pool is decided as the final classification

   there are two important decisions that must be made before making
   classifications. one is the value of k that will be used; this can
   either be decided arbitrarily, or you can try cross-validation to find
   an optimal value. the next, and the most complex, is the distance
   metric that will be used.

   there are many different ways to compute distance, as it is a fairly
   ambiguous notion, and the proper metric to use is always going to be
   determined by the data-set and the classification task. two popular
   ones, however, are euclidean distance and cosine similarity.

   euclidean distance is probably the one that you are most familiar with;
   it is essentially the magnitude of the vector obtained by subtracting
   the training data point from the point to be classified.
   [1*6baymtk2wqzcsyzf4z1xwg.gif]
   general formula for euclidean distance

   another common metric is cosine similarity. rather than calculating a
   magnitude, cosine similarity instead uses the difference in direction
   between two vectors.
   [1*ohgbb709_a9s4wuam1cspa.png]
   general formula for cosine similarity

   choosing a metric can often be tricky, and it may be best to just use
   cross-validation to decide, unless you have some prior insight that
   clearly leads to using one over the other. for example, for something
   like word vectors, you may want to use cosine similarity because the
   direction of a word is more meaningful than the sizes of the component
   values. generally, both of these methods will run in roughly the same
   time, and will suffer from highly-dimensional data.

   after doing all of the above and deciding on a metric, the result of
   the knn algorithm is a decision boundary that partitions r^n into
   sections. each section (colored distinctly below) represents a class in
   the classification problem. the boundaries need not be formed with
   actual training examples         they are instead calculated using the
   distance metric and the available training points. by taking r^n in
   (small) chunks, we can calculate the most likely class for a
   hypothetical data-point in that region, and we thus color that chunk as
   being in the region for that class.
   [1*fzfxe1hvgpst5lxjo5jgpq.gif]

   this information is all that is needed to begin implementing the
   algorithm, and doing so should be relatively simple. there are, of
   course, many ways to improve upon this base algorithm. common
   modifications include weighting, and specific preprocessing to reduce
   computation and reduce noise, such as various algorithms for feature
   extraction and dimension reduction. additionally, the knn method has
   also been used, although less-commonly, for regression tasks, and
   operates in a manner very similar to that of the classifier through
   averaging.
     __________________________________________________________________

   make sure you give this post 50 claps and my blog a follow if you
   enjoyed this post and want to see more!

     * [18]machine learning
     * [19]data science
     * [20]artificial intelligence
     * [21]big data
     * [22]towards data science

   (button)
   (button)
   (button) 4.1k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of devin soni

[24]devin soni

   all opinions are my own     crypto markets, data science     twitter
   [25]@devin_soni     website [26]https://100.github.io/

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 4.1k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3b534bb11d26
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_jbdya3sq3fle---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@devins?source=post_header_lockup
  17. https://towardsdatascience.com/@devins
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/data-science?source=post
  20. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  21. https://towardsdatascience.com/tagged/big-data?source=post
  22. https://towardsdatascience.com/tagged/towards-data-science?source=post
  23. https://towardsdatascience.com/@devins?source=footer_card
  24. https://towardsdatascience.com/@devins
  25. http://twitter.com/devin_soni
  26. https://100.github.io/
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/3b534bb11d26/share/twitter
  33. https://medium.com/p/3b534bb11d26/share/facebook
  34. https://medium.com/p/3b534bb11d26/share/twitter
  35. https://medium.com/p/3b534bb11d26/share/facebook
