   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

beat atari with deep id23! (part 0: intro to rl)

   [14]go to the profile of adrien lucas ecoffet
   [15]adrien lucas ecoffet (button) blockedunblock (button)
   followfollowing
   aug 21, 2017
   [1*mvxnrbvskyoscengsi9cow.png]
   [16]http://www.arcadepunks.com/wp-content/uploads/2016/03/atari2600.png

   in late 2013, a then little-known company called deepmind achieved a
   breakthrough in the world of id23: using deep
   id23, they implemented a system that could learn to
   play many classic atari games with human (and sometimes superhuman)
   performance.

   notably, in a famous video they showed the impressive progress that
   their algorithm achieved on atari breakout:

   iframe: [17]/media/35d5a15ba98d196b0eeeea3e052bd966?postid=9b2c5336e0ec

   deepmind   s algorithm beating breakout

   while their achievement was certainly quite impressive and required
   massive amounts of insights to discover, it also turns out that deep
   id23 is also quite straightforward to understand.
   further, recent libraries such as openai gym and keras have made it
   much more straightforward to implement the code behind deepmind   s
   algorithm. in this series, you will learn to implement it and many of
   the improvements that came after.
   [18]the chatbot conference
   the chatbot conference on september 12, chatbot's life, will host our
   first chatbot conference in san francisco. the   www.eventbrite.com

prereqs

   the prerequisites for this series of posts are quite simple and typical
   of any deep learning tutorial, namely:
     * familiarity with convolutional neural networks, and ideally some
       familiarity with keras.
     * access to a machine with a recent nvidia gpu and relatively large
       amounts of ram (i would say at least 16gb, and even then you will
       probably struggle a little with memory optimizations). an aws p2
       instance should work fine for this. i personally used a desktop
       computer with 16gb of ram and a gtx1070 gpu.

   note that you don   t need any familiarity with id23: i
   will explain all you need to know about it to play atari in due time.

alternatives

   this blog post series isn   t the first deep id23
   tutorial out there, in particular, i would highlight two other
   multi-part tutorials that i think are particularly good:
     * [19]simple id23 with tensorflow covers a lot of
       material about id23, more than i will have time
       to cover here. it should be a great read if you want to learn about
       different areas in id23, but it doesn   t cover the
       specific areas i will cover here (deep q-networks) in as much
       depth.
     * [20]let   s make a id25 is an excellent tutorial, and is one of the
       primary sources i used when i was learning about id25s. unlike this
       series, however, it primarily focuses on toy examples and leaves it
       up to you to implement more complex systems.

   thus the primary differences between this series and previous tutorials
   are:
     * this series will focus on paper reproduction: in each post (except
       this first one where i am laying out the background), we will
       reproduce the results of one or two papers.
     * as such, instead of looking at toy examples, we will focus on atari
       games (at least for the foreseeable future), as they were a focus
       of much research.

   that said, in a way the primary value of this series of posts is that
   it presents the material in a slightly different way which hopefully
   will be useful for some people.

mdps

   for our purposes in this series of posts, id23 is
   about solving id100 (mdps). an mdp is simply a
   formal way of describing a game using the concepts of states, actions
   and rewards. let   s explain what these are using atari as an example:

states

   [1*sorgfswm9xu-6slzbtczhq.png]
   an inadequate representation of state

   the state is the current situation that the agent (your program) is in.
   the simplest approximation of a state is simply the current frame in
   your atari game. unfortunately, this is not always sufficient: given
   the image on the left, you are probably unable to tell whether the ball
   is going up or going down!

   though this fact might seem innocuous, it actually matters a lot
   because such a state representation would break the markov property of
   the mdp, namely that history doesn   t matter: there mustn   t be any
   useful information in previous states for the markov property to be
   satisfied. in the case of using a single image as our state, we are
   breaking the markov property because previous frames could be used to
   infer the speed and acceleration of the ball and paddle.
   [1*me-kirwuc1b5_gzszn2uaq.gif]
   a better state representation

   a simple trick to deal with this is simply to bring some of the
   previous history into your state (that is perfectly acceptable under
   the markov property). deepmind chose to use the past 4 frames, so we
   will do the same. 2 frames is necessary for our algorithm to learn
   about the speed of objects, 3 frames is necessary to infer
   acceleration. it is unclear to me how necessary the 4th frame is (to
   infer the 3rd derivative of position? to gain better precision?),
   perhaps this is something you can experiment with.

actions

   [1*npkpvstfe9ahsag9w518-g.png]
   the atari joystick, through which human players send actions

   an action is a command that you can give in the game in the hope of
   reaching a certain state and reward (more on those later). in the case
   of atari games, actions are all sent via the joystick.

   a total of 18 actions can be performed with the joystick: doing
   nothing, pressing the action button, going in one of 8 directions (up,
   down, left and right as well as the 4 diagonals) and going in any of
   these directions while pressing the button. of course, only a subset of
   these make sense in any given game (eg in breakout, only 4 actions
   apply: doing nothing,    asking for a ball    at the beginning of the game
   by pressing the button and going either left or right).

   it is worth noting that with atari games, the number of possible states
   is much larger than the number of possible actions. this is quite
   fortunate because dealing with a large state space turns out to be much
   easier than dealing with a large action space.

   note also that actions do not have to work reliably in our mdp world.
   in other words, it is perfectly possible that taking action 1 in state
   a will take you to state b 50% of the time and state c another 50% of
   the time. as it turns out this does not complicate the problem very
   much.

rewards

   the last component of our mdps are the rewards. rewards are given after
   performing an action, and are normally a function of your starting
   state, the action you performed, and your end state. the goal of your
   id23 program is to maximize long term rewards.
   [1*mqkxokef4rruf_hmzhdydq.gif]
   score increases from 2 to 3, so we get +1 reward

   in the case of atari, rewards simply correspond to changes in score, ie
   every time your score increases, you get a positive rewards of the size
   of the increase, and vice versa if your score ever decreases (which
   should be very rare).

   discounting
   in practice, our id23 algorithms will never optimize
   for total rewards per se, instead, they will optimize for total
   discounted rewards. in other words, we will choose some number   
   (gamma) where 0 <    < 1, and at each step in the future, we optimize
   for r0 +    r1 +      r2 +      r3    (where r0 is the immediate reward, r1
   the reward one step from now etc.).

   many people who first hear of discounting find it strange or even
   crazy. lots of justifications have been given in the rl literature
   (analogies with interest rates, the fact that we have a finite lifetime
   etc.), but perhaps the simplest way to see how this is useful is to
   think about all the things that could go wrong without discounting:
   with discounting, your sum of rewards is guaranteed to be finite,
   whereas without discounting it might be infinite. infinite total
   rewards can create a bunch of weird issues: for example, how do you
   choose between an algorithm that gets +1 at every step and one that
   gets +1 every 2 steps? the answer might seem obvious, but without
   discounting, both have a total reward of infinity and are thus
   equivalent!

   the right discount rate is often difficult to choose: too low, and our
   agent will put itself in long term difficulty for the sake of cheap
   immediate rewards. too high, and it will be difficult for our algorithm
   to converge because so much of the future needs to be taken into
   account. for atari, we will mostly be using 0.99 as our discount rate.

id23

   in most of this series we will be considering an algorithm called
   id24. id24 is perhaps the most important and well known
   id23 algorithm, and it is surprisingly simple to
   explain. we will be doing exactly that in this section, but first, we
   must quickly explain the concept of policies:

policies

   policies are the output of any id23 algorithm.
   policies simply indicate what action to take for any given state (ie a
   policy could be described as a set of rules of the type    if i am in
   state a, take action 1, if in state b, take action 2, etc.   ).

   a policy is called    deterministic    if it never involves    flipping a
   coin    for deciding the action at any state. it is called    optimal    if
   following it gives the highest expected discounted reward of any
   policy.

   in mdps, there is always an optimal deterministic policy. in other
   words, you can always find a deterministic policy that is better than
   any other policy (and this even if the mdp itself is nondeterministic).

the q function

   at the heart of id24 is the function q(s, a). this function gives
   the discounted total value of taking action a in state s. how is that
   determined you say? well, q(s, a) is simply equal to the reward you get
   for taking a in state s, plus the discounted value of the state s   
   where you end up. further, the value of a state is simply the value of
   taking the optimal action at that state, ie max   (q(s, a)), so we have:

     q(s, a) = r +    max      (q(s   , a   ))

   in practice, with a non-deterministic environment, you might actually
   end up getting a different reward and a different next state each time
   you perform action a in state s. this is not a problem however, simply
   use the average (aka expected value) of the above equation as your q
   function.

   crucially for our purposes, knowing the optimal q function
   automatically gives us the optimal policy! specifically, the best
   policy consists in, at every state, choosing the optimal action, in
   other words:

       (s) = argmax   (q(s,a))

   where   (s) is the policy at state s.
     __________________________________________________________________

   now all we need to do is find a good way to estimate the q function.
   that   s what the next lesson is all about!

   now that you   re done with part 0, you can make your way to [21]beat
   atari with deep id23! (part 1: id25)!

   ps: i   m all about feedback. if anything was unclear or even incorrect
   in this tutorial, please leave a comment so i can keep improving these
   posts.

   iframe: [22]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=9b2c5336e0ec

   [23][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [24][1*v-ppfkswhbvlwwamsvhhwg.png]
   [25][1*wt2auqisieaozxj-i7brdq.png]

     * [26]machine learning
     * [27]deep learning
     * [28]id23
     * [29]artificial intelligence

   (button)
   (button)
   (button) 452 claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of adrien lucas ecoffet

[31]adrien lucas ecoffet

     (button) follow
   [32]becoming human: artificial intelligence magazine

[33]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 452
     * (button)
     *
     *

   [34]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [35]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9b2c5336e0ec
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/lets-build-an-atari-ai-part-0-intro-to-rl-9b2c5336e0ec&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/lets-build-an-atari-ai-part-0-intro-to-rl-9b2c5336e0ec&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_bsoj793qdors---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@adrienle?source=post_header_lockup
  15. https://becominghuman.ai/@adrienle
  16. http://www.arcadepunks.com/wp-content/uploads/2016/03/atari2600.png
  17. https://becominghuman.ai/media/35d5a15ba98d196b0eeeea3e052bd966?postid=9b2c5336e0ec
  18. https://www.eventbrite.com/e/the-chatbot-conference-tickets-34868758395?aff=cbl
  19. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  20. https://jaromiru.com/2016/09/27/lets-make-a-id25-theory/
  21. https://medium.com/@adrienle/lets-build-an-atari-ai-part-1-id25-df57e8ff3b26
  22. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=9b2c5336e0ec
  23. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  24. https://upscri.be/8f5f8b
  25. https://becominghuman.ai/write-for-us-48270209de63
  26. https://becominghuman.ai/tagged/machine-learning?source=post
  27. https://becominghuman.ai/tagged/deep-learning?source=post
  28. https://becominghuman.ai/tagged/reinforcement-learning?source=post
  29. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  30. https://becominghuman.ai/@adrienle?source=footer_card
  31. https://becominghuman.ai/@adrienle
  32. https://becominghuman.ai/?source=footer_card
  33. https://becominghuman.ai/?source=footer_card
  34. https://becominghuman.ai/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://www.eventbrite.com/e/the-chatbot-conference-tickets-34868758395?aff=cbl
  38. https://medium.com/p/9b2c5336e0ec/share/twitter
  39. https://medium.com/p/9b2c5336e0ec/share/facebook
  40. https://medium.com/p/9b2c5336e0ec/share/twitter
  41. https://medium.com/p/9b2c5336e0ec/share/facebook
