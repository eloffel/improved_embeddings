   #[1]rss

   (button) toggle navigation [2]the cohort
     *
     * [3]about
     * [4]people
     * [5]demos
     * [6]datasets & code

canonical correlation id136 for mapping abstract scenes to text

description

   this project focuses on the use of canonical correlation analysis to
   map images and text to a shared space, and then use this shared space
   to map unseen images to corresponding captions. the dataset we use is
   [7]abstract scene dataset, developed at microsoft, for which we show a
   couple of pictures below.

   alt text

paper

   click [8]here for the following paper.
@inproceedings{papasarantopoulos-18,
  title={canonical correlation id136 for mapping abstract scenes to text},
  author={nikos papasarantopoulos and helen jiang and and shay b. cohen},
  booktitle={proceedings of {aaai}},
  year={2018}
}

some notes

   click [9]here to download the ranked captions.

   the format of the file is as follows. there are 300 lines, a line per
   ranked image. each line has fields separated by ^. the fields are as
   follows:
     * the image name in the <a
       href=https://vision.ece.vt.edu/clipart/>abstract scene dataset</a>
       (in the renderedscenes/ directory)
     * gold-standard caption 1
     * gold-standard caption 2
     * gold-standard caption 3
     * gold-standard caption 4
     * gold-standard caption 5
     * gold-standard caption 6
     * gold-standard caption 7
     * gold-standard caption 8
     * gold-standard caption that was rated
     * the caption from ortiz et al. (id151
       system) that was rated
     * the cca caption that was rated
     * average rating (by 2-3 subjects) for the gold caption (a number
       between 1 - least relevant and 5 - most relevant)
     * average rating for the smt caption
     * average rating for the cca caption

   not all images have 8 gold-standard captions, so some can be empty.

   click [10]here to download the splits that we used for the
   training/development/tuning/test sets. these are the same splits as
   were used by ortiz et al. each file in this gzipped tarball contains a
   list of pointers to the scenes that were used for the relevant set. the
   human-ranked images were taken from the test set (first 300 images).

references

   visible links
   1. http://cohort.inf.ed.ac.uk/feed.xml
   2. http://cohort.inf.ed.ac.uk/#page-top
   3. http://cohort.inf.ed.ac.uk/#about
   4. http://cohort.inf.ed.ac.uk/#people
   5. http://cohort.inf.ed.ac.uk/#demos
   6. http://cohort.inf.ed.ac.uk/#downloads
   7. https://vision.ece.vt.edu/clipart/
   8. https://arxiv.org/abs/1608.02784
   9. http://kinloch.inf.ed.ac.uk/scenes/ranking-data.txt.gz
  10. http://kinloch.inf.ed.ac.uk/scenes/splits.tgz

   hidden links:
  12. http://cohort.inf.ed.ac.uk/canonical-correlation-id136.html#page-top
  13. http://cohort.inf.ed.ac.uk/canonical-correlation-id136.html#page-top
