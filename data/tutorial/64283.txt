   #[1]search [2]classification of text documents using sparse features
   [3]featurehasher and dictvectorizer comparison

   [4]logo
     * [5]home
     * [6]installation
     * [7]documentation
          + scikit-learn
          + [8]tutorials
          + [9]user guide
          + [10]api
          + [11]glossary
          + [12]faq
          + [13]development
          + [14]roadmap
          +
     * [15]all available versions
     * [16]pdf documentation

     [17]examples

   [18]fork me on github

   [19]previous
   featurehasher... featurehasher and dictvectorizer comparison

   [20]next
   classificatio... classification of text documents using sparse features

   [21]up
   examples examples

   scikit-learn v0.21.dev0
   [22]other versions

   please [23]cite us if you use the software.
     * [24]id91 text documents using id116

   [x]

   note

   click [25]here to download the full example code

id91 text documents using id116[26]  

   this is an example showing how the scikit-learn can be used to cluster
   documents by topics using a bag-of-words approach. this example uses a
   scipy.sparse matrix to store the features instead of standard numpy
   arrays.

   two feature extraction methods can be used in this example:

     * tfidfvectorizer uses a in-memory vocabulary (a python dict) to map
       the most frequent words to features indices and hence compute a
       word occurrence frequency (sparse) matrix. the word frequencies are
       then reweighted using the inverse document frequency (idf) vector
       collected feature-wise over the corpus.
     * hashingvectorizer hashes word occurrences to a fixed dimensional
       space, possibly with collisions. the word count vectors are then
       normalized to each have l2-norm equal to one (projected to the
       euclidean unit-ball) which seems to be important for id116 to
       work in high dimensional space.
       hashingvectorizer does not provide idf weighting as this is a
       stateless model (the fit method does nothing). when idf weighting
       is needed it can be added by pipelining its output to a
       tfidftransformer instance.

   two algorithms are demoed: ordinary id116 and its more scalable
   cousin minibatch id116.

   additionally, latent semantic analysis can also be used to reduce
   dimensionality and discover latent patterns in the data.

   it can be noted that id116 (and minibatch id116) are very sensitive
   to feature scaling and that in this case the idf weighting helps
   improve the quality of the id91 by quite a lot as measured
   against the    ground truth    provided by the class label assignments of
   the 20 newsgroups dataset.

   this improvement is not visible in the silhouette coefficient which is
   small for both as this measure seem to suffer from the phenomenon
   called    concentration of measure    or    curse of dimensionality    for high
   dimensional datasets such as text data. other measures such as
   v-measure and adjusted rand index are information theoretic based
   evaluation scores: as they are only based on cluster assignments rather
   than distances, hence not affected by the curse of dimensionality.

   note: as id116 is optimizing a non-convex objective function, it will
   likely end up in a local optimum. several runs with independent random
   init might be necessary to get a good convergence.

   out:
usage: plot_document_id91.py [options]

options:
  -h, --help            show this help message and exit
  --lsa=n_components    preprocess documents with latent semantic analysis.
  --no-minibatch        use ordinary id116 algorithm (in batch mode).
  --no-idf              disable inverse document frequency feature weighting.
  --use-hashing         use a hashing feature vectorizer
  --n-features=n_features
                        maximum number of features (dimensions) to extract
                        from text.
  --verbose             print progress reports inside id116 algorithm.
loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
3387 documents
4 categories

extracting features from the training dataset using a sparse vectorizer
done in 0.715012s
n_samples: 3387, n_features: 10000

id91 sparse data with minibatchkmeans(batch_size=1000, init_size=1000, n_c
lusters=4, n_init=1)
done in 0.083s

homogeneity: 0.359
completeness: 0.440
v-measure: 0.396
adjusted rand-index: 0.253
silhouette coefficient: 0.007

top terms per cluster:
cluster 0: henry alaska toronto moon zoo spencer aurora space nsmca zoology
cluster 1: com graphics university posting host nntp know uk article cs
cluster 2: god com sandvik people keith morality sgi kent livesey jesus
cluster 3: space nasa access gov digex pat shuttle hst orbit net

# author: peter prettenhofer <peter.prettenhofer@gmail.com>
#         lars buitinck
# license: bsd 3 clause
from sklearn.datasets import [27]fetch_20newsgroups
from sklearn.decomposition import [28]truncatedsvd
from sklearn.feature_extraction.text import [29]tfidfvectorizer
from sklearn.feature_extraction.text import [30]hashingvectorizer
from sklearn.feature_extraction.text import [31]tfidftransformer
from sklearn.pipeline import [32]make_pipeline
from sklearn.preprocessing import [33]normalizer
from sklearn import metrics

from sklearn.cluster import [34]kmeans, [35]minibatchkmeans

import logging
from optparse import optionparser
import sys
from time import time

import numpy as np


# display progress logs on stdout
logging.basicconfig(level=logging.info,
                    format='%(asctime)s %(levelname)s %(message)s')

# parse commandline arguments
op = optionparser()
op.add_option("--lsa",
              dest="n_components", type="int",
              help="preprocess documents with latent semantic analysis.")
op.add_option("--no-minibatch",
              action="store_false", dest="minibatch", default=true,
              help="use ordinary id116 algorithm (in batch mode).")
op.add_option("--no-idf",
              action="store_false", dest="use_idf", default=true,
              help="disable inverse document frequency feature weighting.")
op.add_option("--use-hashing",
              action="store_true", default=false,
              help="use a hashing feature vectorizer")
op.add_option("--n-features", type=int, default=10000,
              help="maximum number of features (dimensions)"
                   " to extract from text.")
op.add_option("--verbose",
              action="store_true", dest="verbose", default=false,
              help="print progress reports inside id116 algorithm.")

print(__doc__)
op.print_help()


def is_interactive():
    return not hasattr(sys.modules['__main__'], '__file__')


# work-around for jupyter notebook and ipython console
argv = [] if is_interactive() else sys.argv[1:]
(opts, args) = op.parse_args(argv)
if len(args) > 0:
    op.error("this script takes no arguments.")
    sys.exit(1)


# #############################################################################
# load some categories from the training set
categories = [
    'alt.atheism',
    'talk.religion.misc',
    'comp.graphics',
    'sci.space',
]
# uncomment the following to do the analysis on all the categories
# categories = none

print("loading 20 newsgroups dataset for categories:")
print(categories)

dataset = [36]fetch_20newsgroups(subset='all', categories=categories,
                             shuffle=true, random_state=42)

print("%d documents" % len(dataset.data))
print("%d categories" % len(dataset.target_names))
print()

labels = dataset.target
true_k = [37]np.unique(labels).shape[0]

print("extracting features from the training dataset "
      "using a sparse vectorizer")
t0 = time()
if opts.use_hashing:
    if opts.use_idf:
        # perform an idf id172 on the output of hashingvectorizer
        hasher = [38]hashingvectorizer(n_features=opts.n_features,
                                   stop_words='english', alternate_sign=false,
                                   norm=none, binary=false)
        vectorizer = [39]make_pipeline(hasher, [40]tfidftransformer())
    else:
        vectorizer = [41]hashingvectorizer(n_features=opts.n_features,
                                       stop_words='english',
                                       alternate_sign=false, norm='l2',
                                       binary=false)
else:
    vectorizer = [42]tfidfvectorizer(max_df=0.5, max_features=opts.n_features,
                                 min_df=2, stop_words='english',
                                 use_idf=opts.use_idf)
x = vectorizer.fit_transform(dataset.data)

print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % x.shape)
print()

if opts.n_components:
    print("performing id84 using lsa")
    t0 = time()
    # vectorizer results are normalized, which makes kmeans behave as
    # spherical id116 for better results. since lsa/svd results are
    # not normalized, we have to redo the id172.
    svd = [43]truncatedsvd(opts.n_components)
    normalizer = [44]normalizer(copy=false)
    lsa = [45]make_pipeline(svd, normalizer)

    x = lsa.fit_transform(x)

    print("done in %fs" % (time() - t0))

    explained_variance = svd.explained_variance_ratio_.sum()
    print("explained variance of the svd step: {}%".format(
        int(explained_variance * 100)))

    print()


# #############################################################################
# do the actual id91

if opts.minibatch:
    km = [46]minibatchkmeans(n_clusters=true_k, init='id116++', n_init=1,
                         init_size=1000, batch_size=1000, verbose=opts.verbose)
else:
    km = [47]kmeans(n_clusters=true_k, init='id116++', max_iter=100, n_init=1,
                verbose=opts.verbose)

print("id91 sparse data with %s" % km)
t0 = time()
km.fit(x)
print("done in %0.3fs" % (time() - t0))
print()

print("homogeneity: %0.3f" % [48]metrics.homogeneity_score(labels, km.labels_))
print("completeness: %0.3f" % [49]metrics.completeness_score(labels, km.labels_)
)
print("v-measure: %0.3f" % [50]metrics.v_measure_score(labels, km.labels_))
print("adjusted rand-index: %.3f"
      % [51]metrics.adjusted_rand_score(labels, km.labels_))
print("silhouette coefficient: %0.3f"
      % [52]metrics.silhouette_score(x, km.labels_, sample_size=1000))

print()


if not opts.use_hashing:
    print("top terms per cluster:")

    if opts.n_components:
        original_space_centroids = svd.inverse_transform(km.cluster_centers_)
        order_centroids = original_space_centroids.argsort()[:, ::-1]
    else:
        order_centroids = km.cluster_centers_.argsort()[:, ::-1]

    terms = vectorizer.get_feature_names()
    for i in range(true_k):
        print("cluster %d:" % i, end='')
        for ind in order_centroids[i, :10]:
            print(' %s' % terms[ind], end='')
        print()

   total running time of the script: ( 0 minutes 1.161 seconds)
   [53]download python source code: plot_document_id91.py
   [54]download jupyter notebook: plot_document_id91.ipynb

   [55]gallery generated by sphinx-gallery

      2007 - 2019, scikit-learn developers (bsd license). [56]show this
   page source

   [57]previous
   [58]next

references

   visible links
   1. https://scikit-learn.org/dev/search.html
   2. https://scikit-learn.org/dev/auto_examples/text/plot_document_classification_20newsgroups.html
   3. https://scikit-learn.org/dev/auto_examples/text/plot_hashing_vs_dict_vectorizer.html
   4. https://scikit-learn.org/dev/index.html
   5. https://scikit-learn.org/dev/index.html
   6. https://scikit-learn.org/dev/install.html
   7. https://scikit-learn.org/dev/documentation.html
   8. https://scikit-learn.org/dev/tutorial/index.html
   9. https://scikit-learn.org/dev/user_guide.html
  10. https://scikit-learn.org/dev/modules/classes.html
  11. https://scikit-learn.org/dev/glossary.html
  12. https://scikit-learn.org/dev/faq.html
  13. https://scikit-learn.org/dev/developers/index.html
  14. https://scikit-learn.org/dev/roadmap.html
  15. http://scikit-learn.org/dev/versions.html
  16. https://scikit-learn.org/dev/_downloads/scikit-learn-docs.pdf
  17. https://scikit-learn.org/dev/auto_examples/index.html
  18. https://github.com/scikit-learn/scikit-learn
  19. https://scikit-learn.org/dev/auto_examples/text/plot_hashing_vs_dict_vectorizer.html
  20. https://scikit-learn.org/dev/auto_examples/text/plot_document_classification_20newsgroups.html
  21. https://scikit-learn.org/dev/auto_examples/index.html
  22. http://scikit-learn.org/dev/versions.html
  23. https://scikit-learn.org/dev/about.html#citing-scikit-learn
  24. https://scikit-learn.org/dev/auto_examples/text/plot_document_id91.html
  25. https://scikit-learn.org/dev/auto_examples/text/plot_document_id91.html#sphx-glr-download-auto-examples-text-plot-document-id91-py
  26. https://scikit-learn.org/dev/auto_examples/text/plot_document_id91.html#id91-text-documents-using-id116
  27. https://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups
  28. https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.truncatedsvd.html#sklearn.decomposition.truncatedsvd
  29. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.tfidfvectorizer
  30. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.hashingvectorizer
  31. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.tfidftransformer
  32. https://scikit-learn.org/dev/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline
  33. https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.normalizer.html#sklearn.preprocessing.normalizer
  34. https://scikit-learn.org/dev/modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.kmeans
  35. https://scikit-learn.org/dev/modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.minibatchkmeans
  36. https://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups
  37. https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html#numpy.unique
  38. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.hashingvectorizer
  39. https://scikit-learn.org/dev/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline
  40. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.tfidftransformer.html#sklearn.feature_extraction.text.tfidftransformer
  41. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.hashingvectorizer.html#sklearn.feature_extraction.text.hashingvectorizer
  42. https://scikit-learn.org/dev/modules/generated/sklearn.feature_extraction.text.tfidfvectorizer.html#sklearn.feature_extraction.text.tfidfvectorizer
  43. https://scikit-learn.org/dev/modules/generated/sklearn.decomposition.truncatedsvd.html#sklearn.decomposition.truncatedsvd
  44. https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.normalizer.html#sklearn.preprocessing.normalizer
  45. https://scikit-learn.org/dev/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline
  46. https://scikit-learn.org/dev/modules/generated/sklearn.cluster.minibatchkmeans.html#sklearn.cluster.minibatchkmeans
  47. https://scikit-learn.org/dev/modules/generated/sklearn.cluster.kmeans.html#sklearn.cluster.kmeans
  48. https://scikit-learn.org/dev/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score
  49. https://scikit-learn.org/dev/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score
  50. https://scikit-learn.org/dev/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score
  51. https://scikit-learn.org/dev/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score
  52. https://scikit-learn.org/dev/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score
  53. https://scikit-learn.org/dev/_downloads/plot_document_id91.py
  54. https://scikit-learn.org/dev/_downloads/plot_document_id91.ipynb
  55. https://sphinx-gallery.readthedocs.io/
  56. https://scikit-learn.org/dev/_sources/auto_examples/text/plot_document_id91.rst.txt
  57. https://scikit-learn.org/dev/auto_examples/text/plot_hashing_vs_dict_vectorizer.html
  58. https://scikit-learn.org/dev/auto_examples/text/plot_document_classification_20newsgroups.html

   hidden links:
  60. javascript:void(0);
