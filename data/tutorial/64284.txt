   [1][cover_header_combined_small.png]

[2]interested in building a crypto trading bot using machine learning and big
data? check out my book!

   (button)   

[3]hey! i've got some news!

   [4][cover_small.png]
   i just published a book:
   [5]real-time crypto!
   learn how to use bitcoin data to train an algorithm to execute crypto
   trades in real-time. gain insight into blockchain and big data
   architectures. walk away with the know how to build a quantitative
   trading pipeline on your own.
   if you like my blog i think you are going to love my book! i would love
   for you to [6]check it out and let me know what you think.
   best,
   brandon
   (button) close

   [7]home

document id91 with python

   [header_short.jpg]

   in this guide, i will explain how to cluster a set of documents using
   python. my motivating example is to identify the latent structures
   within the synopses of the top 100 films of all time (per an imdb
   list). see [8]the original post for a more detailed discussion on the
   example. this guide covers:
     * tokenizing and id30 each synopsis
     * transforming the corpus into vector space using [9]tf-idf
     * calculating cosine distance between each document as a measure of
       similarity
     * id91 the documents using the [10]id116 algorithm
     * using [11]multidimensional scaling to reduce dimensionality within
       the corpus
     * plotting the id91 output using [12]matplotlib and [13]mpld3
     * conducting a hierarchical id91 on the corpus using [14]ward
       id91
     * plotting a ward dendrogram
     * id96 using [15]id44 (lda)

   note that my [16]github repo for the whole project is available. the
   'cluster_analysis' workbook is fully functional; the
   'cluster_analysis_web' workbook has been trimmed down for the purpose
   of creating this walkthrough. feel free to download the repo and use
   'cluster_analysis' to step through the guide yourself.

   if you have any questions for me, feel free to reach out on twitter to
   [17]@brandonmrose

contents[18]  

     * [19]stopwords, id30, and id121
     * [20]tf-idf and document similarity
     * [21]id116 id91
     * [22]multidimensional scaling
     * [23]visualizing document clusters
     * [24]hierarchical document id91
     * [25]id44 (lda)

   but first, i import everything i am going to need up front
   in [4]:
import numpy as np
import pandas as pd
import nltk
import re
import os
import codecs
from sklearn import feature_extraction
import mpld3

   for the purposes of this walkthrough, imagine that i have 2 primary
   lists:
     * 'titles': the titles of the films in their rank order
     * 'synopses': the synopses of the films matched to the 'titles' order

   in the full workbook that i posted to github you can walk through the
   import of these lists, but for brevity just keep in mind that for the
   rest of this walk-through i will focus on using these two lists. of
   primary importance is the 'synopses' list; 'titles' is mostly used for
   labeling purposes.
   in [17]:
print titles[:10] #first 10 titles

[  the godfather  ,   the shawshank redemption  , "schindler  s list",   raging bull  ,
   casablanca  , "one flew over the cuckoo  s nest",   gone with the wind  ,   citizen
 kane  ,   the wizard of oz  ,   titanic  ]


   in [21]:
print synopses[0][:200] #first 200 characters in first synopses (for 'the godfat
her')
print
print

plot [edit] [ [ edit edit ] ]
 on the day of his only daughter  s wedding, vito corleone hears requests in his
role as the godfather, the don of a new york crime family. vito  s youngest son,
michael,




stopwords, id30, and tokenizing[26]  

   this section is focused on defining some functions to manipulate the
   synopses. first, i load [27]nltk's list of english stop words. [28]stop
   words are words like "a", "the", or "in" which don't convey significant
   meaning. i'm sure there are much better explanations of this out there.
   in [23]:
# load nltk's english stopwords as variable called 'stopwords'
stopwords = nltk.corpus.stopwords.words('english')

   in [51]:
print stopwords[:10]

[  i  ,   me  ,   my  ,   myself  ,   we  ,   our  ,   ours  ,   ourselves  ,   you  ,   your  ]


   next i import the [29]snowball stemmer which is actually part of nltk.
   [30]id30 is just the process of breaking a word down into its root.
   in [32]:
# load nltk's snowballstemmer as variabled 'stemmer'
from nltk.stem.snowball import snowballstemmer
stemmer = snowballstemmer("english")

   below i define two functions:
     * tokenize_and_stem: tokenizes (splits the synopsis into a list of
       its respective words (or tokens) and also stems each token
     * tokenize_only: tokenizes the synopsis only

   i use both these functions to create a dictionary which becomes
   important in case i want to use stems for an algorithm, but later
   convert stems back to their full words for presentation purposes. guess
   what, i do want to do that!
   in [33]:
# here i define a tokenizer and stemmer which returns the set of stems in the te
xt that it is passed

def tokenize_and_stem(text):
    # first tokenize by sentence, then by word to ensure that punctuation is cau
ght as it's own token
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_to
kenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw pu
nctuation)
    for token in tokens:
        if re.search('[a-za-z]', token):
            filtered_tokens.append(token)
    stems = [stemmer.stem(t) for t in filtered_tokens]
    return stems


def tokenize_only(text):
    # first tokenize by sentence, then by word to ensure that punctuation is cau
ght as it's own token
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk
.word_tokenize(sent)]
    filtered_tokens = []
    # filter out any tokens not containing letters (e.g., numeric tokens, raw pu
nctuation)
    for token in tokens:
        if re.search('[a-za-z]', token):
            filtered_tokens.append(token)
    return filtered_tokens

   below i use my id30/tokenizing and tokenizing functions to iterate
   over the list of synopses to create two vocabularies: one stemmed and
   one only tokenized.
   in [49]:
#not super pythonic, no, not at all.
#use extend so it's a big flat list of vocab
totalvocab_stemmed = []
totalvocab_tokenized = []
for i in synopses:
    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokeni
ze/stem
    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed'
 list

    allwords_tokenized = tokenize_only(i)
    totalvocab_tokenized.extend(allwords_tokenized)

   using these two lists, i create a pandas dataframe with the stemmed
   vocabulary as the index and the tokenized words as the column. the
   benefit of this is it provides an efficient way to look up a stem and
   return a full token. the downside here is that stems to tokens are one
   to many: the stem 'run' could be associated with 'ran', 'runs',
   'running', etc. for my purposes this is fine--i'm perfectly happy
   returning the first token associated with the stem i need to look up.
   in [48]:
vocab_frame = pd.dataframe({'words': totalvocab_tokenized}, index = totalvocab_s
temmed)
print 'there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame'

there are 312209 items in vocab_frame


   you'll notice there is clearly some repetition here. i could clean it
   up, but there are only 312209 items in the dataframe which isn't huge
   overhead in looking up a stemmed word based on the stem-index.
   in [50]:
print vocab_frame.head()
print
print
print
print

     words
plot  plot
edit  edit
edit  edit
edit  edit
on      on






tf-idf and document similarity[31]  

   [2402]

   here, i define term frequency-inverse document frequency (tf-idf)
   vectorizer parameters and then convert the synopses list into a tf-idf
   matrix.

   to get a tf-idf matrix, first count word occurrences by document. this
   is transformed into a document-term matrix (dtm). this is also just
   called a term frequency matrix. an example of a dtm is here at right.

   then apply the term frequency-inverse document frequency weighting:
   words that occur frequently within a document but not frequently within
   the corpus receive a higher weighting as these words are assumed to
   contain more meaning in relation to the document.

   a couple things to note about the parameters i define below:
     * max_df: this is the maximum frequency within the documents a given
       feature can have to be used in the tfi-idf matrix. if the term is
       in greater than 80% of the documents it probably cares little
       meanining (in the context of film synopses)
     * min_idf: this could be an integer (e.g. 5) and the term would have
       to be in at least 5 of the documents to be considered. here i pass
       0.2; the term must be in at least 20% of the document. i found that
       if i allowed a lower min_df i ended up basing id91 on
       names--for example "michael" or "tom" are names found in several of
       the movies and the synopses use these names frequently, but the
       names carry no real meaning.
     * ngram_range: this just means i'll look at unigrams, bigrams and
       trigrams. see [32]id165s

   in [54]:
from sklearn.feature_extraction.text import tfidfvectorizer

#define vectorizer parameters
tfidf_vectorizer = tfidfvectorizer(max_df=0.8, max_features=200000,
                                 min_df=0.2, stop_words='english',
                                 use_idf=true, tokenizer=tokenize_and_stem, ngra
m_range=(1,3))

%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorize
r to synopses

print(tfidf_matrix.shape)

cpu times: user 29.1 s, sys: 468 ms, total: 29.6 s
wall time: 37.8 s
(100, 563)


   terms is just a list of the features used in the tf-idf matrix. this is
   a vocabulary
   in [55]:
terms = tfidf_vectorizer.get_feature_names()

   dist is defined as 1 - the cosine similarity of each document. cosine
   similarity is measured against the tf-idf matrix and can be used to
   generate a measure of similarity between each document and the other
   documents in the corpus (each synopsis among the synopses). subtracting
   it from 1 provides cosine distance which i will use for plotting on a
   euclidean (2-dimensional) plane.

   note that with dist it is possible to evaluate the similarity of any
   two or more synopses.
   in [57]:
from sklearn.metrics.pairwise import cosine_similarity
dist = 1 - cosine_similarity(tfidf_matrix)
print
print





id116 id91[33]  

   now onto the fun part. using the tf-idf matrix, you can run a slew of
   id91 algorithms to better understand the hidden structure within
   the synopses. i first chose [34]id116. id116 initializes with a
   pre-determined number of clusters (i chose 5). each observation is
   assigned to a cluster (cluster assignment) so as to minimize the within
   cluster sum of squares. next, the mean of the clustered observations is
   calculated and used as the new cluster centroid. then, observations are
   reassigned to clusters and centroids recalculated in an iterative
   process until the algorithm reaches convergence.

   i found it took several runs for the algorithm to converge a global
   optimum as id116 is susceptible to reaching local optima.
   in [66]:
from sklearn.cluster import kmeans

num_clusters = 5

km = kmeans(n_clusters=num_clusters)

%time km.fit(tfidf_matrix)

clusters = km.labels_.tolist()

cpu times: user 232 ms, sys: 6.64 ms, total: 239 ms
wall time: 305 ms


   i use joblib.dump to pickle the model, once it has converged and to
   reload the model/reassign the labels as the clusters.
   in [67]:
from sklearn.externals import joblib

#uncomment the below to save your model
#since i've already run my model i am loading from the pickle

#joblib.dump(km,  'doc_cluster.pkl')

km = joblib.load('doc_cluster.pkl')
clusters = km.labels_.tolist()

   here, i create a dictionary of titles, ranks, the synopsis, the cluster
   assignment, and the genre [rank and genre were scraped from imdb].

   i convert this dictionary to a pandas dataframe for easy access. i'm a
   huge fan of [35]pandas and recommend taking a look at some of its
   awesome functionality which i'll use below, but not describe in a ton
   of detail.
   in [62]:
films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clust
ers, 'genre': genres }

frame = pd.dataframe(films, index = [clusters] , columns = ['rank', 'title', 'cl
uster', 'genre'])

   in [63]:
frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to
 4)

   out[63]:
4    26
0    25
2    21
1    16
3    12
dtype: int64

   in [64]:
grouped = frame['rank'].groupby(frame['cluster']) #groupby cluster for aggregati
on purposes

grouped.mean() #average rank (1 to 100) per cluster

   out[64]:
cluster
0          47.200000
1          58.875000
2          49.380952
3          54.500000
4          43.730769
dtype: float64

   note that clusters 4 and 0 have the lowest rank, which indicates that
   they, on average, contain films that were ranked as "better" on the top
   100 list.

   here is some fancy indexing and sorting on each cluster to identify
   which are the top n (i chose n=6) words that are nearest to the cluster
   centroid. this gives a good sense of the main topic of the cluster.
   in [69]:
from __future__ import print_function

print("top terms per cluster:")
print()
#sort cluster centers by proximity to centroid
order_centroids = km.cluster_centers_.argsort()[:, ::-1]

for i in range(num_clusters):
    print("cluster %d words:" % i, end='')

    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster
        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0
].encode('utf-8', 'ignore'), end=',')
    print() #add whitespace
    print() #add whitespace

    print("cluster %d titles:" % i, end='')
    for title in frame.ix[i]['title'].values.tolist():
        print(' %s,' % title, end='')
    print() #add whitespace
    print() #add whitespace

print()
print()

top terms per cluster:

cluster 0 words: family, home, mother, war, house, dies,

cluster 0 titles: schindler  s list, one flew over the cuckoo  s nest, gone with t
he wind, the wizard of oz, titanic, forrest gump, e.t. the extra-terrestrial, th
e silence of the lambs, gandhi, a streetcar named desire, the best years of our
lives, my fair lady, ben-hur, doctor zhivago, the pianist, the exorcist, out of
africa, good will hunting, terms of endearment, giant, the grapes of wrath, clos
e encounters of the third kind, the graduate, stagecoach, wuthering heights,

cluster 1 words: police, car, killed, murders, driving, house,

cluster 1 titles: casablanca, psycho, sunset blvd., vertigo, chinatown, amadeus,
 high noon, the french connection, fargo, pulp fiction, the maltese falcon, a cl
ockwork orange, double indemnity, rebel without a cause, the third man, north by
 northwest,

cluster 2 words: father, new, york, new, brothers, apartments,

cluster 2 titles: the godfather, raging bull, citizen kane, the godfather: part
ii, on the waterfront, 12 angry men, rocky, to kill a mockingbird, braveheart, t
he good, the bad and the ugly, the apartment, goodfellas, city lights, it happen
ed one night, midnight cowboy, mr. smith goes to washington, rain man, annie hal
l, network, taxi driver, rear window,

cluster 3 words: george, dance, singing, john, love, perform,

cluster 3 titles: west side story, singin   in the rain, it  s a wonderful life, s
ome like it hot, the philadelphia story, an american in paris, the king  s speech
, a place in the sun, tootsie, nashville, american graffiti, yankee doodle dandy
,

cluster 4 words: killed, soldiers, captain, men, army, command,

cluster 4 titles: the shawshank redemption, lawrence of arabia, the sound of mus
ic, star wars, 2001: a space odyssey, the bridge on the river kwai, dr. strangel
ove or: how i learned to stop worrying and love the bomb, apocalypse now, the lo
rd of the rings: the return of the king, gladiator, from here to eternity, savin
g private ryan, unforgiven, raiders of the lost ark, patton, jaws, butch cassidy
 and the sundance kid, the treasure of the sierra madre, platoon, dances with wo
lves, the deer hunter, all quiet on the western front, shane, the green mile, th
e african queen, mutiny on the bounty,





multidimensional scaling[36]  

   here is some code to convert the dist matrix into a 2-dimensional array
   using [37]multidimensional scaling. i won't pretend i know a ton about
   mds, but it was useful for this purpose. another option would be to use
   [38]principal component analysis.
   in [74]:
import os  # for os.path.basename

import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.manifold import mds

mds()

# convert two components as we're plotting points in a two-dimensional plane
# "precomputed" because we provide a distance matrix
# we will also specify `random_state` so the plot is reproducible.
mds = mds(n_components=2, dissimilarity="precomputed", random_state=1)

pos = mds.fit_transform(dist)  # shape (n_components, n_samples)

xs, ys = pos[:, 0], pos[:, 1]
print()
print()





visualizing document clusters[39]  

   in this section, i demonstrate how you can visualize the document
   id91 output using matplotlib and mpld3 (a matplotlib wrapper for
   d3.js).

   first i define some dictionaries for going from cluster number to color
   and to cluster name. i based the cluster names off the words that were
   closest to each cluster centroid.
   in [75]:
#set up colors per clusters using a dict
cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#6
6a61e'}

#set up cluster names using a dict
cluster_names = {0: 'family, home, war',
                 1: 'police, killed, murders',
                 2: 'father, new york, brothers',
                 3: 'dance, singing, love',
                 4: 'killed, soldiers, captain'}

   next, i plot the labeled observations (films, film titles) colored by
   cluster using matplotlib. i won't get into too much detail about the
   matplotlib plot, but i tried to provide some helpful commenting.
   in [51]:
#some ipython magic to show the matplotlib plots inline
%matplotlib inline

#create data frame that has the result of the mds plus the cluster numbers and t
itles
df = pd.dataframe(dict(x=xs, y=ys, label=clusters, title=titles))

#group by cluster
groups = df.groupby('label')


# set up plot
fig, ax = plt.subplots(figsize=(17, 9)) # set size
ax.margins(0.05) # optional, just adds 5% padding to the autoscaling

#iterate through groups to layer the plot
#note that i use the cluster_name and cluster_color dicts with the 'name' lookup
 to return the appropriate color/label
for name, group in groups:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,
            label=cluster_names[name], color=cluster_colors[name],
            mec='none')
    ax.set_aspect('auto')
    ax.tick_params(\
        axis= 'x',          # changes apply to the x-axis
        which='both',      # both major and minor ticks are affected
        bottom='off',      # ticks along the bottom edge are off
        top='off',         # ticks along the top edge are off
        labelbottom='off')
    ax.tick_params(\
        axis= 'y',         # changes apply to the y-axis
        which='both',      # both major and minor ticks are affected
        left='off',      # ticks along the bottom edge are off
        top='off',         # ticks along the top edge are off
        labelleft='off')

ax.legend(numpoints=1)  #show legend with only 1 point

#add label in x,y position with the label as the film title
for i in range(len(df)):
    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)



plt.show() #show the plot

#uncomment the below to save the plot if need be
#plt.savefig('clusters_small_noaxes.png', dpi=200)

   [iiiiiiiisdun4f9erereresknvpwlyiiiiiiitlokfgxererereraec6xux3
   myzhbn2qmxererererh5cwx+3id35iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
   iiiii iiiiiiiiiiiiiijt1f8bbdm1ufp3iu4aaaaasuvork5cyii= ]
   in [52]:
plt.close()

   the id91 plot looks great, but it pains my eyes to see
   overlapping labels. having some experience with [40]d3.js i knew one
   solution would be to use a browser based/javascript interactive.
   fortunately, i recently stumbled upon [41]mpld3 a matplotlib wrapper
   for d3. mpld3 basically let's you use matplotlib syntax to create web
   interactives. it has a really easy, high-level api for adding tooltips
   on mouse hover, which is what i am interested in.

   it also has some nice functionality for zooming and panning. the below
   javascript snippet basicaly defines a custom location for where the
   zoom/pan toggle resides. don't worry about it too much and you actually
   don't need to use it, but it helped for formatting purposes when
   exporting to the web later. the only thing you might want to change is
   the x and y attr for the position of the toolbar.
   in [77]:
#define custom toolbar location
class toptoolbar(mpld3.plugins.pluginbase):
    """plugin for moving toolbar to top of figure"""

    javascript = """
    mpld3.register_plugin("toptoolbar", toptoolbar);
    toptoolbar.prototype = object.create(mpld3.plugin.prototype);
    toptoolbar.prototype.constructor = toptoolbar;
    function toptoolbar(fig, props){
        mpld3.plugin.call(this, fig, props);
    };

    toptoolbar.prototype.draw = function(){
      // the toolbar svg doesn't exist
      // yet, so first draw it
      this.fig.toolbar.draw();

      // then change the y position to be
      // at the top of the figure
      this.fig.toolbar.toolbar.attr("x", 150);
      this.fig.toolbar.toolbar.attr("y", 400);

      // then remove the draw function,
      // so that it is not called again
      this.fig.toolbar.draw = function() {}
    }
    """
    def __init__(self):
        self.dict_ = {"type": "toptoolbar"}

   here is the actual creation of the interactive scatterplot. i won't go
   into much more detail about it since it's pretty much a straightforward
   copy of one of the mpld3 examples, though i use a pandas groupby to
   group by cluster, then iterate through the groups as i layer the
   scatterplot. note that relative to doing this with raw d3, mpld3 is
   much simpler to integrate into your python workflow. if you click
   around the rest of my website you'll see that i do love d3, but for
   basic interactives i will probably use mpld3 a lot going forward.

   note that mpld3 lets you define some custom css, which i use to style
   the font, the axes, and the left margin on the figure.
   in [78]:
#create data frame that has the result of the mds plus the cluster numbers and t
itles
df = pd.dataframe(dict(x=xs, y=ys, label=clusters, title=titles))

#group by cluster
groups = df.groupby('label')

#define custom css to format the font and to remove the axis labeling
css = """
text.mpld3-text, div.mpld3-tooltip {
  font-family:arial, helvetica, sans-serif;
}

g.mpld3-xaxis, g.mpld3-yaxis {
display: none; }

svg.mpld3-figure {
margin-left: -200px;}
"""

# plot
fig, ax = plt.subplots(figsize=(14,6)) #set plot size
ax.margins(0.03) # optional, just adds 5% padding to the autoscaling

#iterate through groups to layer the plot
#note that i use the cluster_name and cluster_color dicts with the 'name' lookup
 to return the appropriate color/label
for name, group in groups:
    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18,
                     label=cluster_names[name], mec='none',
                     color=cluster_colors[name])
    ax.set_aspect('auto')
    labels = [i for i in group.title]

    #set tooltip using points, labels and the already defined 'css'
    tooltip = mpld3.plugins.pointhtmltooltip(points[0], labels,
                                       voffset=10, hoffset=10, css=css)
    #connect tooltip to fig
    mpld3.plugins.connect(fig, tooltip, toptoolbar())

    #set tick marks as blank
    ax.axes.get_xaxis().set_ticks([])
    ax.axes.get_yaxis().set_ticks([])

    #set axis as blank
    ax.axes.get_xaxis().set_visible(false)
    ax.axes.get_yaxis().set_visible(false)


ax.legend(numpoints=1) #show legend with only one dot

mpld3.display() #show the plot

#uncomment the below to export to html
#html = mpld3.fig_to_html(fig)
#print(html)

   out[78]:

hierarchical document id91[42]  

   now that i was successfuly able to cluster and plot the documents using
   id116, i wanted to try another id91 algorithm. i chose the
   [43]ward id91 algorithm because it offers hierarchical
   id91. ward id91 is an agglomerative id91 method,
   meaning that at each stage, the pair of clusters with minimum
   between-cluster distance are merged. i used the precomputed cosine
   distance matrix (dist) to calclate a linkage_matrix, which i then plot
   as a dendrogram.

   note that this method returned 3 primary clusters, with the largest
   cluster being split into about 4 major subclusters. note that the
   cluster in red contains many of the "killed, soldiers, captain" films.
   braveheart and gladiator are within the same low-level cluster which is
   interesting as these are probably my two favorite movies.
   in [83]:
from scipy.cluster.hierarchy import ward, dendrogram

linkage_matrix = ward(dist) #define the linkage_matrix using ward id91 pre
-computed distances

fig, ax = plt.subplots(figsize=(15, 20)) # set size
ax = dendrogram(linkage_matrix, orientation="right", labels=titles);

plt.tick_params(\
    axis= 'x',          # changes apply to the x-axis
    which='both',      # both major and minor ticks are affected
    bottom='off',      # ticks along the bottom edge are off
    top='off',         # ticks along the top edge are off
    labelbottom='off')

plt.tight_layout() #show plot with tight layout

#uncomment below to save figure
plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters

   [wdc ze77qj1osqaaaabjru5erkjggg== ]
   in [60]:
plt.close()

id44[44]  

   this section focuses on using [45]id44 (lda) to
   learn yet more about the hidden structure within the top 100 film
   synopses. lda is a probabilistic topic model that assumes documents are
   a mixture of topics and that each word in the document is attributable
   to the document's topics. there is quite a good high-level overview of
   probabilistic topic models by one of the big names in the field, david
   blei, available in the [46]communications of the acm here.
   incidentally, blei was one of the authors of the seminal paper on lda.

   for my implementaiton of lda, i use the [47]gensim pacakage. i'm going
   to preprocess the synopses a bit differently here, and first i define a
   function to remove any proper noun.
   in [82]:
#strip any proper names from a text...unfortunately right now this is yanking th
e first word from a sentence too.
import string
def strip_proppers(text):
    # first tokenize by sentence, then by word to ensure that punctuation is cau
ght as it's own token
    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_to
kenize(sent) if word.islower()]
    return "".join([" "+i if not i.startswith("'") and i not in string.punctuati
on else i for i in tokens]).strip()

   since the above function is just based on capitalization, it is prone
   to remove words at the beginning of sentences. so, i wrote the below
   function using nltk's part of speech tagger. however, it took way too
   long to run across all synopses, so i stuck with the above.
   in [80]:
#strip any proper nouns (nnp) or plural proper nouns (nnps) from a text
from nltk.tag import pos_tag

def strip_proppers_pos(text):
    tagged = pos_tag(text.split()) #use nltk's part of speech tagger
    non_propernouns = [word for word,pos in tagged if pos != 'nnp' and pos != 'n
nps']
    return non_propernouns

   here i run the actual text processing (removing of proper nouns,
   id121, removal of stop words)
   in [83]:
from gensim import corpora, models, similarities

#remove proper names
%time preprocess = [strip_proppers(doc) for doc in synopses]

#tokenize
%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]

#remove stop words
%time texts = [[word for word in text if word not in stopwords] for text in toke
nized_text]

cpu times: user 12.9 s, sys: 148 ms, total: 13 s
wall time: 15.9 s
cpu times: user 15.1 s, sys: 172 ms, total: 15.3 s
wall time: 19.3 s
cpu times: user 4.56 s, sys: 39.2 ms, total: 4.6 s
wall time: 5.95 s


   below are some gensim specific conversions; i also filter out extreme
   words (see inline comment)
   in [84]:
#create a gensim dictionary from the texts
dictionary = corpora.dictionary(texts)

#remove extremes (similar to the min/max df step used when creating the tf-idf m
atrix)
dictionary.filter_extremes(no_below=1, no_above=0.8)

#convert the dictionary to a bag of words corpus for reference
corpus = [dictionary.doc2bow(text) for text in texts]

   the actual model runs below. i took 100 passes to ensure convergence,
   but you can see that it took my machine 13 minutes to run. my chunksize
   is larger than the corpus so basically all synopses are used per pass.
   i should optimize this, and gensim has the capacity to run in parallel.
   i'll likely explore this further as i use the implementation on larger
   corpora.
   in [85]:
%time lda = models.ldamodel(corpus, num_topics=5,
                             word=dictionary,
                            update_every=5,
                            chunksize=10000,
                            passes=100)

cpu times: user 9min 53s, sys: 5.87 s, total: 9min 59s
wall time: 13min 1s


   each topic has a set of words that defines it, along with a certain
   id203.
   in [87]:
lda.show_topics()

   out[87]:
[u  0.006*men + 0.005*kill + 0.004*soldier + 0.004*order + 0.004*patient + 0.004*
night + 0.003*priest + 0.003*becom + 0.003*new + 0.003*speech  ,
 u"0.006*n  t + 0.005*go + 0.005*fight + 0.004*doe + 0.004*home + 0.004*famili +
0.004*car + 0.004*night + 0.004*say + 0.004*next",
 u"0.005*ask + 0.005*meet + 0.005*kill + 0.004*say + 0.004*friend + 0.004*car +
0.004*love + 0.004*famili + 0.004*arriv + 0.004*n  t",
 u  0.009*kill + 0.006*soldier + 0.005*order + 0.005*men + 0.005*shark + 0.004*at
tempt + 0.004*offic + 0.004*son + 0.004*command + 0.004*attack  ,
 u  0.004*kill + 0.004*water + 0.004*two + 0.003*plan + 0.003*away + 0.003*set +
0.003*boat + 0.003*vote + 0.003*way + 0.003*home  ]

   here, i convert the topics into just a list of the top 20 words in each
   topic. you can see a similar breakdown of topics as i identified using
   id116 including a war/family topic and a more clearly war/epic topic.
   in [86]:
topics_matrix = lda.show_topics(formatted=false, num_words=20)
topics_matrix = np.array(topics_matrix)

topic_words = topics_matrix[:,:,1]
for i in topic_words:
    print([str(word) for word in i])
    print()

[  men  ,   kill  ,   soldier  ,   order  ,   patient  ,   night  ,   priest  ,   becom  ,   new
  ,   speech  ,   friend  ,   decid  ,   young  ,   ward  ,   state  ,   front  ,   would  ,   hom
e  ,   two  ,   father  ]

["n  t",   go  ,   fight  ,   doe  ,   home  ,   famili  ,   car  ,   night  ,   say  ,   next  ,
  ask  ,   day  ,   want  ,   show  ,   goe  ,   friend  ,   two  ,   polic  ,   name  ,   meet  ]

[  ask  ,   meet  ,   kill  ,   say  ,   friend  ,   car  ,   love  ,   famili  ,   arriv  , "n  t"
,   home  ,   two  ,   go  ,   father  ,   money  ,   call  ,   polic  ,   apart  ,   night  ,   ho
us  ]

[  kill  ,   soldier  ,   order  ,   men  ,   shark  ,   attempt  ,   offic  ,   son  ,   command
  ,   attack  ,   water  ,   friend  ,   ask  ,   fire  ,   arriv  ,   wound  ,   die  ,   battl  ,
   death  ,   fight  ]

[  kill  ,   water  ,   two  ,   plan  ,   away  ,   set  ,   boat  ,   vote  ,   way  ,   home  ,
  run  ,   ship  ,   would  ,   destroy  ,   guilti  ,   first  ,   attack  ,   go  ,   use  ,   fo
rc  ]

references

   1. https://realtimecrypto.io/
   2. https://realtimecrypto.io/
   3. https://realtimecrypto.io/
   4. https://realtimecrypto.io/
   5. https://realtimecrypto.io/
   6. https://realtimecrypto.io/
   7. http://www.brandonrose.org/
   8. http://www.brandonrose.org/top100
   9. http://en.wikipedia.org/wiki/tf   idf
  10. http://en.wikipedia.org/wiki/id116_id91
  11. http://en.wikipedia.org/wiki/multidimensional_scaling
  12. http://matplotlib.org/
  13. http://mpld3.github.io/
  14. http://en.wikipedia.org/wiki/ward's_method
  15. http://en.wikipedia.org/wiki/latent_dirichlet_allocation
  16. https://github.com/brandomr/document_cluster
  17. https://twitter.com/brandonmrose
  18. http://brandonrose.org/id91#contents
  19. http://brandonrose.org/id91#stopwords,-id30,-and-tokenizing
  20. http://brandonrose.org/id91#tf-idf-and-document-similarity
  21. http://brandonrose.org/id91#id116-id91
  22. http://brandonrose.org/id91#multidimensional-scaling
  23. http://brandonrose.org/id91#visualizing-document-clusters
  24. http://brandonrose.org/id91#hierarchical-document-id91
  25. http://brandonrose.org/id91#latent-dirichlet-allocation
  26. http://brandonrose.org/id91#stopwords,-id30,-and-tokenizing
  27. http://www.nltk.org/
  28. http://en.wikipedia.org/wiki/stop_words
  29. http://snowball.tartarus.org/
  30. http://en.wikipedia.org/wiki/id30
  31. http://brandonrose.org/id91#tf-idf-and-document-similarity
  32. http://en.wikipedia.org/wiki/id165
  33. http://brandonrose.org/id91#id116-id91
  34. http://en.wikipedia.org/wiki/id116_id91
  35. http://pandas.pydata.org/
  36. http://brandonrose.org/id91#multidimensional-scaling
  37. http://en.wikipedia.org/wiki/multidimensional_scaling
  38. http://en.wikipedia.org/wiki/principal_component_analysis
  39. http://brandonrose.org/id91#visualizing-document-clusters
  40. http://d3js.org/
  41. https://mpld3.github.io/
  42. http://brandonrose.org/id91#hierarchical-document-id91
  43. http://en.wikipedia.org/wiki/ward's_method
  44. http://brandonrose.org/id91#latent-dirichlet-allocation
  45. http://en.wikipedia.org/wiki/latent_dirichlet_allocation
  46. http://delivery.acm.org/10.1145/2140000/2133826/p77-blei.pdf?ip=68.48.185.120&id=2133826&acc=open&key=4d4702b0c3e38b35.4d4702b0c3e38b35.4d4702b0c3e38b35.6d218144511f3437&cfid=612398453&cftoken=48760790&__acm__=1419436704_2d47aefe0700e44f81eb822df659a341
  47. https://radimrehurek.com/gensim/
