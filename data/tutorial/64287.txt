    #[1]index [2]search [3]benchmarking performance and scaling of python
   id91 algorithms [4]how hdbscan works

   [5]hdbscan
   latest
   ____________________
     * [6]basic usage of hdbscan* for id91
     * [7]getting more information about a id91
     * [8]parameter selection for hdbscan*
     * [9]outlier detection
     * [10]predicting clusters for new points
     * [11]soft id91 for hdbscan*
     * [12]frequently asked questions

     * [13]how hdbscan works
     * [14]comparing python id91 algorithms
          + [15]some rules for eda id91
          + [16]getting set up
          + [17]testing id91 algorithms
          + [18]id116
          + [19]affinity propagation
          + [20]mean shift
          + [21]spectral id91
          + [22]agglomerative id91
          + [23]dbscan
          + [24]hdbscan
     * [25]benchmarking performance and scaling of python id91
       algorithms
     * [26]how soft id91 for hdbscan works

     * [27]api reference

   [28]hdbscan
     * [29]docs   
     * comparing python id91 algorithms
     * [30]edit on github
     __________________________________________________________________

comparing python id91 algorithms[31]  

   there are a lot of id91 algorithms to choose from. the standard
   sklearn id91 suite has thirteen different id91 classes
   alone. so what id91 algorithms should you be using? as with every
   question in data science and machine learning it depends on your data.
   a number of those thirteen classes in sklearn are specialised for
   certain tasks (such as co-id91 and bi-id91, or id91
   features instead data points). obviously an algorithm specializing in
   text id91 is going to be the right choice for id91 text
   data, and other algorithms specialize in other specific kinds of data.
   thus, if you know enough about your data, you can narrow down on the
   id91 algorithm that best suits that kind of data, or the sorts of
   important properties your data has, or the sorts of id91 you need
   done. all well and good, but what if you don   t know much about your
   data? if, for example, you are    just looking    and doing some
   exploratory data analysis (eda) it is not so easy to choose a
   specialized algorithm. so, what algorithm is good for exploratory data
   analysis?

some rules for eda id91[32]  

   to start, lets    lay down some ground rules of what we need a good eda
   id91 algorithm to do, then we can set about seeing how the
   algorithms available stack up.
     * don   t be wrong!: if you are doing eda you are trying to learn and
       gain intuitions about your data. in that case it is far better to
       get no result at all than a result that is wrong. bad results lead
       to false intuitions which in turn send you down completely the
       wrong path. not only do you not understand your data, you
       misunderstand your data. this means a good eda id91 algorithm
       needs to conservative in int   s id91; it should be willing to
       not assign points to clusters; it should not group points together
       unless they really are in a cluster; this is true of far fewer
       algorithms than you might think.
     * intuitive parameters: all id91 algorithms have parameters;
       you need some knobs to turn to adjust things. the question is: how
       do you pick settings for those parameters? if you know little about
       your data it can be hard to determine what value or setting a
       parameter should have. this means parameters need to be intuitive
       enough that you can hopefully set them without having to know a lot
       about your data.
     * stable clusters: if you run the algorithm twice with a different
       random initialization, you should expect to get roughly the same
       clusters back. if you are sampling your data, taking a different
       random sample shouldn   t radically change the resulting cluster
       structure (unless your sampling has problems). if you vary the
       id91 algorithm parameters you want the id91 to change
       in a somewhat stable predictable fashion.
     * performance: data sets are only getting bigger. you can sub-sample
       (but see stability), but ultimately you need a id91 algorithm
       that can scale to large data sizes. a id91 algorithm isn   t
       much use if you can only use it if you take such a small sub-sample
       that it is no longer representative of the data at large!

   there are other nice to have features like soft clusters, or
   overlapping clusters, but the above desiderata is enough to get started
   with because, oddly enough, very few id91 algorithms can satisfy
   them all!

getting set up[33]  

   if we are going to compare id91 algorithms we   ll need a few
   things; first some libraries to load and cluster the data, and second
   some visualisation tools so we can look at the results of id91.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.cluster as cluster
import time
%matplotlib inline
sns.set_context('poster')
sns.set_color_codes()
plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}

   next we need some data. in order to make this more interesting i   ve
   constructed an artificial dataset that will give id91 algorithms
   a challenge     some non-globular clusters, some noise etc.; the sorts of
   things we expect to crop up in messy real-world data. so that we can
   actually visualize id91s the dataset is two dimensional; this is
   not something we expect from real-world data where you generally can   t
   just visualize and see what is going on.
data = np.load('clusterable_data.npy')

   so let   s have a look at the data and see what we have.
plt.scatter(data.t[0], data.t[1], c='b', **plot_kwds)
frame = plt.gca()
frame.axes.get_xaxis().set_visible(false)
frame.axes.get_yaxis().set_visible(false)

   _images/comparing_id91_algorithms_6_0.png

   it   s messy, but there are certainly some clusters that you can pick out
   by eye; determining the exact boundaries of those clusters is harder of
   course, but we can hope that our id91 algorithms will find at
   least some of those clusters. so, on to testing    

testing id91 algorithms[34]  

   to start let   s set up a little utility function to do the id91
   and plot the results for us. we can time the id91 algorithm while
   we   re at it and add that to the plot since we do care about
   performance.
def plot_clusters(data, algorithm, args, kwds):
    start_time = time.time()
    labels = algorithm(*args, **kwds).fit_predict(data)
    end_time = time.time()
    palette = sns.color_palette('deep', np.unique(labels).max() + 1)
    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]
    plt.scatter(data.t[0], data.t[1], c=colors, **plot_kwds)
    frame = plt.gca()
    frame.axes.get_xaxis().set_visible(false)
    frame.axes.get_yaxis().set_visible(false)
    plt.title('clusters found by {}'.format(str(algorithm.__name__)), fontsize=2
4)
    plt.text(-0.5, 0.7, 'id91 took {:.2f} s'.format(end_time - start_time)
, fontsize=14)

   before we try doing the id91, there are some things to keep in
   mind as we look at the results.
     * in real use cases we can   t look at the data and realise points are
       not really in a cluster; we have to take the id91 algorithm
       at its word.
     * this is a small dataset, so poor performance here bodes very badly.

   on to the id91 algorithms.

id116[35]  

   id116 is the    go-to    id91 algorithm for many simply because it
   is fast, easy to understand, and available everywhere (there   s an
   implementation in almost any statistical or machine learning tool you
   care to use). id116 has a few problems however. the first is that it
   isn   t a id91 algorithm, it is a partitioning algorithm. that is
   to say id116 doesn   t    find clusters    it partitions your dataset into
   as many (assumed to be globular) chunks as you ask for by attempting to
   minimize intra-partition distances. that leads to the second problem:
   you need to specify exactly how many clusters you expect. if you know a
   lot about your data then that is something you might expect to know.
   if, on the other hand, you are simply exploring a new dataset then
      number of clusters    is a hard parameter to have any good intuition
   for. the usually proposed solution is to run id116 for many different
      number of clusters    values and score each id91 with some
      cluster goodness    measure (usually a variation on intra-cluster vs
   inter-cluster distances) and attempt to find an    elbow   . if you   ve ever
   done this in practice you know that finding said elbow is usually not
   so easy, nor does it necessarily correlate as well with the actual
      natural    number of clusters as you might like. finally id116 is also
   dependent upon initialization; give it multiple different random starts
   and you can get multiple different id91s. this does not engender
   much confidence in any individual id91 that may result.

   so, in summary, here   s how id116 seems to stack up against out
   desiderata: - don   t be wrong!: id116 is going to throw points into
   clusters whether they belong or not; it also assumes you clusters are
   globular. id116 scores very poorly on this point. - intuitive
   parameters: if you have a good intuition for how many clusters the
   dataset your exploring has then great, otherwise you might have a
   problem. - stability: hopefully the id91 is stable for your data.
   best to have many runs and check though. - performance: this is id116
   big win. it   s a simple algorithm and with the right tricks and
   optimizations can be made exceptionally efficient. there are few
   algorithms that can compete with id116 for performance. if you have
   truly huge data then id116 might be your only option.

   but enough opinion, how does id116 perform on our test dataset? let   s
   have look. we   ll be generous and use our knowledge that there are six
   natural clusters and give that to id116.
plot_clusters(data, cluster.kmeans, (), {'n_clusters':6})

   _images/comparing_id91_algorithms_12_0.png

   we see some interesting results. first, the assumption of perfectly
   globular clusters means that the natural clusters have been spliced and
   clumped into various more globular shapes. worse, the noise points get
   lumped into clusters as well: in some cases, due to where relative
   cluster centers ended up, points very distant from a cluster get lumped
   in. having noise pollute your clusters like this is particularly bad in
   an eda world since they can easily mislead your intuition and
   understanding of the data. on a more positive note we completed
   id91 very quickly indeed, so at least we can be wrong quickly.

affinity propagation[36]  

   affinity propagation is a newer id91 algorithm that uses a graph
   based approach to let points    vote    on their preferred    exemplar   . the
   end result is a set of cluster    exemplars    from which we derive
   clusters by essentially doing what id116 does and assigning each
   point to the cluster of it   s nearest exemplar. affinity propagation has
   some advantages over id116. first of all the graph based exemplar
   voting means that the user doesn   t need to specify the number of
   clusters. second, due to how the algorithm works under the hood with
   the graph representation it allows for non-metric dissimilarities (i.e.
   we can have dissimilarities that don   t obey the triangle inequality, or
   aren   t symmetric). this second point is important if you are ever
   working with data isn   t naturally embedded in a metric space of some
   kind; few id91 algorithms support, for example, non-symmetric
   dissimilarities. finally affinity propagation does, at least, have
   better stability over runs (but not over parameter ranges!).

   the weak points of affinity propagation are similar to id116. since
   it partitions the data just like id116 we expect to see the same
   sorts of problems, particularly with noisy data. while affinity
   propagation eliminates the need to specify the number of clusters, it
   has    preference    and    damping    parameters. picking these parameters
   well can be difficult. the implementation in sklearn default preference
   to the median dissimilarity. this tends to result in a very large
   number of clusters. a better value is something smaller (or negative)
   but data dependent. finally affinity propagation is slow; since it
   supports non-metric dissimilarities it can   t take any of the shortcuts
   available to other algorithms, and the basic operations are expensive
   as data size grows.

   so, in summary, over our desiderata we have:
     * don   t be wrong: the same issues as id116; affinity propagation is
       going to throw points into clusters whether they belong or not; it
       also assumes you clusters are globular.
     * intuitive parameters: it can be easier to guess at preference and
       damping than number of clusters, but since affinity propagation is
       quite sensitive to preference values it can be fiddly to get
          right   . this isn   t really that much of an improvement over
       id116.
     * stability: affinity propagation is deterministic over runs.
     * performance: affinity propagation tends to be very slow. in
       practice running it on large datasets is essentially impossible
       without a carefully crafted and optimized implementation (i.e. not
       the default one available in sklearn).

   and how does it look in practice on our chosen dataset? i   ve tried to
   select a preference and damping value that gives a reasonable number of
   clusters (in this case six) but feel free to play with the parameters
   yourself and see if you can come up with a better id91.
plot_clusters(data, cluster.affinitypropagation, (), {'preference':-5.0, 'dampin
g':0.95})

   _images/comparing_id91_algorithms_15_0.png

   the result is eerily similar to id116 and has all the same problems.
   the globular clusters have lumped together splied parts of various
      natural    clusters. the noise points have been assigned to clusters
   regardless of being significant outliers. in other words, we   ll have a
   very poor intuitive understanding of our data based on these
      clusters   . worse still it took us several seconds to arrive at this
   unenlightening conclusion.

mean shift[37]  

   mean shift is another option if you don   t want to have to specify the
   number of clusters. it is centroid based, like id116 and affinity
   propagation, but can return clusters instead of a partition. the
   underlying idea of the mean shift algorithm is that there exists some
   id203 density function from which the data is drawn, and tries to
   place centroids of clusters at the maxima of that density function. it
   approximates this via kernel density estimation techniques, and the key
   parameter is then the bandwidth of the kernel used. this is easier to
   guess than the number of clusters, but may require some staring at,
   say, the distributions of pairwise distances between data points to
   choose successfully. the other issue (at least with the sklearn
   implementation) is that it is fairly slow depsite potentially having
   good scaling!

   how does mean shift fare against out criteria? in principle proming,
   but in practice    
     * don   t be wrong!: mean shift doesn   t cluster every point, but it
       still aims for globular clusters, and in practice it can return
       less than ideal results (see below for example). without visual
       validation it can be hard to know how wrong it may be.
     * intuitive parameters: mean shift has more intuitive and meaningful
       parameters; this is certainly a strength.
     * stability: mean shift results can vary a lot as you vary the
       bandwidth parameter (which can make selection more difficult than
       it first appears. it also has a random initialisation, which means
       stability under runs can vary (if you reseed the random start).
     * performance: while mean shift has good scalability in principle
       (using ball trees) in practice the sklearn implementation is slow;
       this is a serious weak point for mean shift.

   let   s see how it works on some actual data. i spent a while trying to
   find a good bandwidth value that resulted in a reasonable id91.
   the choice below is about the best i found.
plot_clusters(data, cluster.meanshift, (0.175,), {'cluster_all':false})

   _images/comparing_id91_algorithms_18_0.png

   we at least aren   t polluting our clusters with as much noise, but we
   certainly have dense regions left as noise and clusters that run across
   and split what seem like natural clusters. there is also the outlying
   yellow cluster group that doesn   t make a lot of sense. thus while mean
   shift had good promise, and is certainly better than id116, it   s
   still short of our desiderata. worse still it took over 4 seconds to
   cluster this small dataset!

spectral id91[38]  

   spectral id91 can best be thought of as a graph id91. for
   spatial data one can think of inducing a graph based on the distances
   between points (potentially a id92 graph, or even a dense graph). from
   there spectral id91 will look at the eigenvectors of the
   laplacian of the graph to attempt to find a good (low dimensional)
   embedding of the graph into euclidean space. this is essentially a kind
   of manifold learning, finding a transformation of our original space so
   as to better represent manifold distances for some manifold that the
   data is assumed to lie on. once we have the transformed space a
   standard id91 algorithm is run; with sklearn the default is
   id116. that means that the key for spectral id91 is the
   transformation of the space. presuming we can better respect the
   manifold we   ll get a better id91     we need worry less about
   id116 globular clusters as they are merely globular on the
   transformed space and not the original space. we unfortunately retain
   some of id116 weaknesses: we still partition the data instead of
   id91 it; we have the hard to guess    number of clusters   
   parameter; we have stability issues inherited from id116. worse, if
   we operate on the dense graph of the distance matrix we have a very
   expensive initial step and sacrifice performance.

   so, in summary:
     * don   t be wrong!: we are less wrong, in that we don   t have a purely
       globular cluster assumption; we do still have partitioning and
       hence are polluting clusters with noise, messing with our
       understanding of the clusters and hence the data.
     * intuitive parameters: we are no better than id116 here; we have
       to know the correct number of clusters, or hope to guess by
       id91 over a range of parameter values and finding some way to
       pick the    right one   .
     * stability: slightly more stable than id116 due to the
       transformation, but we still suffer from those issues.
     * performance: for spatial data we don   t have a sparse graph (unless
       we prep one ourselves) so the result is a somewhat slower
       algorithm.

   let   s have a look at how it operates on our test dataset. again, we   ll
   be generous and give it the six clusters to look for.
plot_clusters(data, cluster.spectralid91, (), {'n_clusters':6})

   _images/comparing_id91_algorithms_21_0.png

   spectral id91 performed better on the long thin clusters, but
   still ended up cutting some of them strangely and dumping parts of them
   in with other clusters. we also still have the issue of noise points
   polluting our clusters, so again our intuitions are going to be led
   astray. performance was a distinct improvement of affinity propagation
   however. over all we are doing better, but are still a long way from
   achieving our desiderata.

agglomerative id91[39]  

   agglomerative id91 is really a suite of algorithms all based on
   the same idea. the fundamental idea is that you start with each point
   in it   s own cluster and then, for each cluster, use some criterion to
   choose another cluster to merge with. do this repeatedly until you have
   only one cluster and you get get a hierarchy, or binary tree, of
   clusters branching down to the last layer which has a leaf for each
   point in the dataset. the most basic version of this, single linkage,
   chooses the closest cluster to merge, and hence the tree can be ranked
   by distance as to when clusters merged/split. more complex variations
   use things like mean distance between clusters, or distance between
   cluster centroids etc. to determine which cluster to merge. once you
   have a cluster hierarchy you can choose a level or cut (according to
   some criteria) and take the clusters at that level of the tree. for
   sklearn we usually choose a cut based on a    number of clusters   
   parameter passed in.

   the advantage of this approach is that clusters can grow    following the
   underlying manifold    rather than being presumed to be globular. you can
   also inspect the dendrogram of clusters and get more information about
   how clusters break down. on the other hand, if you want a flat set of
   clusters you need to choose a cut of the dendrogram, and that can be
   hard to determine. you can take the sklearn approach and specify a
   number of clusters, but as we   ve already discussed that isn   t a
   particularly intuitive parameter when you   re doing eda. you can look at
   the dendrogram and try to pick a natural cut, but this is similar to
   finding the    elbow    across varying k values for id116: in principle
   it   s fine, and the textbook examples always make it look easy, but in
   practice on messy real world data the    obvious    choice is often far
   from obvious. we are also still partitioning rather than id91 the
   data, so we still have that persistent issue of noise polluting our
   clusters. fortunately performance can be pretty good; the sklearn
   implementation is fairly slow, but `fastcluster
   <[40]https://pypi.python.org/pypi/fastcluster>`__ provides high
   performance agglomerative id91 if that   s what you need.

   so, in summary:
     * don   t be wrong!: we have gotten rid of the globular assumption, but
       we are still assuming that all the data belongs in clusters with no
       noise.
     * intuitive parameters: similar to id116 we are stuck choosing the
       number of clusters (not easy in eda), or trying to discern some
       natural parameter value from a plot that may or may not have any
       obvious natural choices.
     * stability: agglomerative id91 is stable across runs and the
       dendrogram shows how it varies over parameter choices (in a
       reasonably stable way), so stability is a strong point.
     * performance: performance can be good if you get the right
       implementation.

   so, let   s see it id91 data. i chose to provide the correct number
   of clusters (six) and use ward as the linkage/merge method. this is a
   more robust method than say single linkage, but it does tend toward
   more globular clusters.
plot_clusters(data, cluster.agglomerativeid91, (), {'n_clusters':6, 'linka
ge':'ward'})

   _images/comparing_id91_algorithms_24_0.png

   similar to the spectral id91 we have handled the long thin
   clusters much better than id116 or affinity propagation. we in fact
   improved on spectral id91 a bit on that front. we do still have
   clusters that contain parts of several different natural clusters, but
   those    mis-id91s    are smaller. we also still have all the noise
   points polluting our clusters. the end result is probably the best
   id91 we   ve seen so far, but given the mis-id91 and noise
   issues we are still not going to get as good an intuition for the data
   as we might reasonably hope for.

dbscan[41]  

   dbscan is a density based algorithm     it assumes clusters for dense
   regions. it is also the first actual id91 algorithm we   ve looked
   at: it doesn   t require that every point be assigned to a cluster and
   hence doesn   t partition the data, but instead extracts the    dense   
   clusters and leaves sparse background classified as    noise   . in
   practice dbscan is related to agglomerative id91. as a first step
   dbscan transforms the space according to the density of the data:
   points in dense regions are left alone, while points in sparse regions
   are moved further away. applying single linkage id91 to the
   transformed space results in a dendrogram, which we cut according to a
   distance parameter (called epsilon or eps in many implementations) to
   get clusters. importantly any singleton clusters at that cut level are
   deemed to be    noise    and left unclustered. this provides several
   advantages: we get the manifold following behaviour of agglomerative
   id91, and we get actual id91 as opposed to partitioning.
   better yet, since we can frame the algorithm in terms of local region
   queries we can use various tricks such as kdtrees to get exceptionally
   good performance and scale to dataset sizes that are otherwise
   unapproachable with algorithms other than id116. there are some
   catches however. obviously epsilon can be hard to pick; you can do some
   data analysis and get a good guess, but the algorithm can be quite
   sensitive to the choice of the parameter. the density based
   transformation depends on another parameter (min_samples in sklearn).
   finally the combination of min_samples and eps amounts to a choice of
   density and the id91 only finds clusters at or above that
   density; if your data has variable density clusters then dbscan is
   either going to miss them, split them up, or lump some of them together
   depending on your parameter choices.

   so, in summary:
     * don   t be wrong!: clusters don   t need to be globular, and won   t have
       noise lumped in; varying density clusters may cause problems, but
       that is more in the form of insufficient detail rather than
       explicitly wrong. dbscan is the first id91 algorithm we   ve
       looked at that actually meets the    don   t be wrong!    requirement.
     * intuitive parameters: epsilon is a distance value, so you can
       survey the distribution of distances in your dataset to attempt to
       get an idea of where it should lie. in practice, however, this
       isn   t an especially intuitive parameter, nor is it easy to get
       right.
     * stability: dbscan is stable across runs (and to some extent
       subsampling if you re-parameterize well); stability over varying
       epsilon and min samples is not so good.
     * performance: this is dbscan   s other great strength; few id91
       algorithms can tackle datasets as large as dbscan can.

   so how does it cluster our test dataset? i played with a few epsilon
   values until i got somethign reasonable, but there was little science
   to this     getting the parameters right can be hard.
plot_clusters(data, cluster.dbscan, (), {'eps':0.025})

   _images/comparing_id91_algorithms_27_0.png

   this is a pretty decent id91; we   ve lumped natural clusters
   together a couple of times, but at least we didn   t carve them up to do
   so. we also picked up a few tiny clusters in amongst the large sparse
   cluster. these problems are artifacts of not handling variable density
   clusters     to get the sparser clusters to cluster we end up lumping
   some of the denser clusters with them; in the meantime the very sparse
   cluster is still broken up into several clusters. all in all we   re
   finally doing a decent job, but there   s still plenty of room for
   improvement.

hdbscan[42]  

   hdbscan is a recent algorithm developed by some of the same people who
   write the original dbscan paper. their goal was to allow varying
   density clusters. the algorithm starts off much the same as dbscan: we
   transform the space according to density, exactly as dbscan does, and
   perform single linkage id91 on the transformed space. instead of
   taking an epsilon value as a cut level for the dendrogram however, a
   different approach is taken: the dendrogram is condensed by viewing
   splits that result in a small number of points splitting off as points
      falling out of a cluster   . this results in a smaller tree with fewer
   clusters that    lose points   . that tree can then be used to select the
   most stable or persistent clusters. this process allows the tree to be
   cut at varying height, picking our varying density clusters based on
   cluster stability. the immediate advantage of this is that we can have
   varying density clusters; the second benefit is that we have eliminated
   the epsilon parameter as we no longer need it to choose a cut of the
   dendrogram. instead we have a new parameter min_cluster_size which is
   used to determine whether points are    falling out of a cluster    or
   splitting to form two new clusters. this trades an unintuitive
   parameter for one that is not so hard to choose for eda (what is the
   minimum size cluster i am willing to care about?).

   so, in summary:
     * don   t be wrong!: we inherited all the benefits of dbscan and
       removed the varying density clusters issue. hdbscan is easily the
       strongest option on the    don   t be wrong!    front.
     * intuitive parameters: choosing a mimnimum cluster size is very
       reasonable. the only remaining parameter is min_samples inherited
       from dbscan for the density based space transformation. sadly
       min_samples is not that intuitive; hdbscan is not that sensitive to
       it and we can choose some sensible defaults, but this remains the
       biggest weakness of the algorithm.
     * stability: hdbscan is stable over runs and subsampling (since the
       variable density id91 will still cluster sparser subsampled
       clusters with the same parameter choices), and has good stability
       over parameter choices.
     * performance: when implemented well hdbscan can be very efficient.
       the current implementation has similar performance to fastcluster   s
       agglomerative id91 (and will use fastcluster if it is
       available), but we expect future implementations that take
       advantage of newer data structure such as cover trees to scale
       significantly better.

   how does hdbscan perform on our test dataset? unfortunately hdbscan is
   not part of sklearn. fortunately we can just import the [43]hdbscan
   library and use it as if it were part of sklearn.
import hdbscan

plot_clusters(data, hdbscan.hdbscan, (), {'min_cluster_size':15})

   _images/comparing_id91_algorithms_31_0.png

   i think the picture speaks for itself.

   [44]next [45]previous
     __________________________________________________________________

      copyright 2016, leland mcinnes, john healy, steve astels revision
   2bf8792a.
   built with [46]sphinx using a [47]theme provided by [48]read the docs.

   read the docs v: latest

   versions
          [49]latest
          [50]stable
          [51]0.8.18
          [52]0.8.17
          [53]0.8.16
          [54]0.8.15
          [55]0.8.14
          [56]0.8.13
          [57]0.8.12
          [58]0.8.11
          [59]0.8.10
          [60]0.8.9
          [61]0.8.8.1
          [62]0.8.8
          [63]0.8.6
          [64]0.8.5
          [65]0.8.4
          [66]0.8.3
          [67]0.8.2
          [68]0.8.1

   downloads
          [69]pdf
          [70]htmlzip
          [71]epub

   on read the docs
          [72]project home
          [73]builds
     __________________________________________________________________

   free document hosting provided by [74]read the docs.

references

   1. https://hdbscan.readthedocs.io/en/latest/genindex.html
   2. https://hdbscan.readthedocs.io/en/latest/search.html
   3. https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html
   4. https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html
   5. https://hdbscan.readthedocs.io/en/latest/index.html
   6. https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html
   7. https://hdbscan.readthedocs.io/en/latest/advanced_hdbscan.html
   8. https://hdbscan.readthedocs.io/en/latest/parameter_selection.html
   9. https://hdbscan.readthedocs.io/en/latest/outlier_detection.html
  10. https://hdbscan.readthedocs.io/en/latest/prediction_tutorial.html
  11. https://hdbscan.readthedocs.io/en/latest/soft_id91.html
  12. https://hdbscan.readthedocs.io/en/latest/faq.html
  13. https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html
  14. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html
  15. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#some-rules-for-eda-id91
  16. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#getting-set-up
  17. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#testing-id91-algorithms
  18. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#id116
  19. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#affinity-propagation
  20. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#mean-shift
  21. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#spectral-id91
  22. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#agglomerative-id91
  23. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#dbscan
  24. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#hdbscan
  25. https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html
  26. https://hdbscan.readthedocs.io/en/latest/soft_id91_explanation.html
  27. https://hdbscan.readthedocs.io/en/latest/api.html
  28. https://hdbscan.readthedocs.io/en/latest/index.html
  29. https://hdbscan.readthedocs.io/en/latest/index.html
  30. https://github.com/scikit-learn-contrib/hdbscan/blob/master/docs/comparing_id91_algorithms.rst
  31. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#comparing-python-id91-algorithms
  32. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#some-rules-for-eda-id91
  33. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#getting-set-up
  34. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#testing-id91-algorithms
  35. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#id116
  36. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#affinity-propagation
  37. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#mean-shift
  38. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#spectral-id91
  39. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#agglomerative-id91
  40. https://pypi.python.org/pypi/fastcluster
  41. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#dbscan
  42. https://hdbscan.readthedocs.io/en/latest/comparing_id91_algorithms.html#hdbscan
  43. https://github.com/scikit-learn-contrib/hdbscan
  44. https://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html
  45. https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html
  46. http://sphinx-doc.org/
  47. https://github.com/rtfd/sphinx_rtd_theme
  48. https://readthedocs.org/
  49. https://hdbscan.readthedocs.io/en/latest/
  50. https://hdbscan.readthedocs.io/en/stable/
  51. https://hdbscan.readthedocs.io/en/0.8.18/
  52. https://hdbscan.readthedocs.io/en/0.8.17/
  53. https://hdbscan.readthedocs.io/en/0.8.16/
  54. https://hdbscan.readthedocs.io/en/0.8.15/
  55. https://hdbscan.readthedocs.io/en/0.8.14/
  56. https://hdbscan.readthedocs.io/en/0.8.13/
  57. https://hdbscan.readthedocs.io/en/0.8.12/
  58. https://hdbscan.readthedocs.io/en/0.8.11/
  59. https://hdbscan.readthedocs.io/en/0.8.10/
  60. https://hdbscan.readthedocs.io/en/0.8.9/
  61. https://hdbscan.readthedocs.io/en/0.8.8.1/
  62. https://hdbscan.readthedocs.io/en/0.8.8/
  63. https://hdbscan.readthedocs.io/en/0.8.6/
  64. https://hdbscan.readthedocs.io/en/0.8.5/
  65. https://hdbscan.readthedocs.io/en/0.8.4/
  66. https://hdbscan.readthedocs.io/en/0.8.3/
  67. https://hdbscan.readthedocs.io/en/0.8.2/
  68. https://hdbscan.readthedocs.io/en/0.8.1/
  69. https://readthedocs.org/projects/hdbscan/downloads/pdf/latest/
  70. https://readthedocs.org/projects/hdbscan/downloads/htmlzip/latest/
  71. https://readthedocs.org/projects/hdbscan/downloads/epub/latest/
  72. https://readthedocs.org/projects/hdbscan/?fromdocs=hdbscan
  73. https://readthedocs.org/builds/hdbscan/?fromdocs=hdbscan
  74. http://www.readthedocs.org/
