   #[1]giga thoughts ...    feed [2]giga thoughts ...    comments feed
   [3]giga thoughts ...    deep learning from first principles in python, r
   and octave     part 1 comments feed [4]the 3rd paperback & kindle
   editions of my books on cricket, now on amazon [5]deep learning from
   first principles in python, r and octave     part 2 [6]alternate
   [7]alternate [8]giga thoughts ... [9]wordpress.com

   [10]skip to content

   [11]giga thoughts    

   insights into technology

     * [12]linkedin
     * [13]github
     * [14]twitter

   (button) menu

     * [15]home
     * [16]index of posts
     * [17]books i authored
     * [18]who am i?
     * [19]published posts
     * [20]about giga thoughts   

deep learning from first principles in python, r and octave     part 1

   [21]tinniam v ganesh [22]chain rule, [23]deep learning, [24]git,
   [25]github, [26]id28, [27]loss, [28]machine learning,
   [29]neural networks, [30]numpy, [31]octave, [32]python, [33]r, [34]r
   language, [35]r markdown, [36]r package, [37]r project, [38]sklearn,
   [39]technology january 4, 2018january 14, 2019

      you don   t perceive objects as they are. you perceive them as you are.   
      your interpretation of physical objects has everything to do with the
   historical trajectory of your brain     and little to do with the objects
   themselves.   
      the brain generates its own reality, even before it receives
   information coming in from the eyes and the other senses. this is known
   as the internal model   
                          david eagleman - the brain: the story of you

   this is the first in the series of posts, i intend to write on deep
   learning. this post is inspired by the [40]deep learning
   specialization by prof andrew ng on coursera and [41]neural networks
   for machine learning by prof geoffrey hinton also on coursera. in this
   post i implement id28 with a 2 layer neural network
   i.e. a neural network that just has an input layer and an output layer
   and with no hidden layer.i am certain that any self-respecting deep
   learning/neural network would consider a neural network without hidden
   layers as no neural network at all!

   this 2 layer network is implemented in python, r and octave languages.
   i have included octave, into the mix, as octave is a close cousin of
   matlab. these implementations in python, r and octave are equivalent
   vectorized implementations. so, if you are familiar in any one of the
   languages, you should be able to look at the corresponding code in the
   other two. you can download this r markdown file and octave code from
   [42]deeplearning -part 1

   check out my video presentation which discusses the derivations in
   detail
   1. [43]elements of neural networks and deep le- part 1
   2. [44]elements of neural networks and deep learning     part 2

   to start with, id28 is performed using sklearn   s
   id28 package for the cancer data set also from sklearn.
   this is shown below

1. id28

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import logisticregression
from sklearn.datasets import make_classification, make_blobs

from sklearn.metrics import confusion_matrix
from matplotlib.colors import listedcolormap
from sklearn.datasets import load_breast_cancer
# load the cancer data
(x_cancer, y_cancer) = load_breast_cancer(return_x_y = true)
x_train, x_test, y_train, y_test = train_test_split(x_cancer, y_cancer,
                                                   random_state = 0)
# call the logisitic regression function
clf = logisticregression().fit(x_train, y_train)
print('accuracy of id28 classifier on training set: {:.2f}'
     .format(clf.score(x_train, y_train)))
print('accuracy of id28 classifier on test set: {:.2f}'
     .format(clf.score(x_test, y_test)))
## accuracy of id28 classifier on training set: 0.96
## accuracy of id28 classifier on test set: 0.96

   to check on other classification algorithms, check my post
   [45]practical machine learning with r and python     part 2.

   checkout my book    deep learning from first principles: second edition    
   in vectorized python, r and octave   . my book starts with the
   implementation of a simple 2-layer neural network and works its way to
   a generic l-layer deep learning network, with all the bells and
   whistles. the derivations have been discussed in detail. the code has
   been extensively commented and included in its entirety in the appendix
   sections. my book is available on amazon as [46]paperback ($14.99) and
   in [47]kindle version($9.99/rs449).

   you may also like my companion book    practical machine learning with r
   and python:second edition- machine learning in stereo    available in
   amazon in [48]paperback($10.99) and [49]kindle($7.99/rs449) versions.
   this book is ideal for a quick reference of the various ml functions
   and associated measurements in both r and python which are essential to
   delve deep into deep learning.

2. id28 as a 2 layer neural network

   in the following section id28 is implemented as a 2
   layer neural network in python, r and octave. the same cancer data set
   from sklearn will be used to train and test the neural network in
   python, r and octave. this can be represented diagrammatically as below


   the cancer data set has 30 input features, and the target variable
      output    is either 0 or 1. hence the sigmoid activation function will
   be used in the output layer for classification.

   this simple 2 layer neural network is shown below
   at the input layer there are 30 features and the corresponding weights
   of these inputs which are initialized to small random values.
   z= w_{1}x_{1} +w_{2}x_{2} +..+ w_{30}x_{30} + b
   where    b    is the bias term

   the activation function is the sigmoid function which is a=
   1/(1+e^{-z})
   the loss, when the sigmoid function is used in the output layer, is
   given by
   l=-(ylog(a) + (1-y)log(1-a)) (1)

id119

forward propagation

   in forward propagation cycle of the neural network the output z and the
   output of activation function, the sigmoid function, is first computed.
   then using the output    y    for the given features, the    loss    is
   computed using equation (1) above.

backward propagation

   the backward propagation cycle determines how the    loss    is impacted
   for small variations from the previous layers upto the input layer. in
   other words, backward propagation computes the changes in the weights
   at the input layer, which will minimize the loss. several cycles of
   id119 are performed in the path of steepest descent to find
   the local minima. in other words the set of weights and biases, at the
   input layer, which will result in the lowest loss is computed by
   id119. the weights at the input layer are decreased by a
   parameter known as the    learning rate   . too big a    learning rate    can
   overshoot the local minima, and too small a    learning rate    can take a
   long time to reach the local minima. this is done for    m    training
   examples.

   chain rule of differentiation
   let y=f(u)
   and u=g(x) then
   \partial y/\partial x = \partial y/\partial u * \partial u/\partial x

   derivative of sigmoid
   \sigma=1/(1+e^{-z})
   let x= 1 + e^{-z}  then
   \sigma = 1/x
   \partial \sigma/\partial x = -1/x^{2}
   \partial x/\partial z = -e^{-z}
   using the chain rule of differentiation we get
   \partial \sigma/\partial z = \partial \sigma/\partial x * \partial
   x/\partial z
   =-1/(1+e^{-z})^{2}* -e^{-z} = e^{-z}/(1+e^{-z})^{2}
   therefore \partial \sigma/\partial z = \sigma(1-\sigma)         -(2)

   the 3 equations for the 2 layer neural network representation of
   id28 are
   l=-(y*log(a) + (1-y)*log(1-a))       -(a)
   a=1/(1+e^{-z})       -(b)
   z= w_{1}x_{1} +w_{2}x_{2} +...+ w_{30}x_{30} +b = z = \sum_{i}
   w_{i}*x_{i} + b -(c)

   the back propagation step requires the computation of dl/dw_{i} and
   dl/db_{i} . in the case of regression it would be de/dw_{i} and
   de/db_{i} where de is the mean squared error function.
   computing the derivatives for back propagation we have
   dl/da = -(y/a + (1-y)/(1-a))           -(d)
   because d/dx(logx) = 1/x
   also from equation (2) we get
   da/dz = a (1-a)                                       (e)
   by chain rule
   \partial l/\partial z = \partial l/\partial a * \partial a/\partial z
   therefore substituting the results of (d) & (e) we get
   \partial l/\partial z = -(y/a + (1-y)/(1-a)) * a(1-a) = a-y
   (f)
   finally
   \partial l/\partial w_{i}= \partial l/\partial a * \partial a/\partial
   z * \partial z/\partial w_{i}
                    -(g)
   \partial z/\partial w_{i} = x_{i}                 (h)
   and from (f) we have  \partial l/\partial z =a-y
   therefore  (g) reduces to
   \partial l/\partial w_{i} = x_{i}* (a-y) -(i)
   also
   \partial l/\partial b = \partial l/\partial a * \partial a/\partial z *
   \partial z/\partial b -(j)
   since
   \partial z/\partial b = 1 and using (f) in (j)
   \partial l/\partial b = a-y

   the gradient computes the weights at the input layer and the
   corresponding bias by using the values
   of dw_{i} and db
   w_{i} := w_{i} -\alpha * dw_{i}
   b := b -\alpha * db
   i found the computation graph representation in the book deep learning:
   ian goodfellow, yoshua bengio, aaron courville, very useful to
   visualize and also compute the backward propagation. for the 2 layer
   neural network of id28 the computation graph is shown
   below

3. neural network for id28 -python code (vectorized)

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# define the sigmoid function
def sigmoid(z):
    a=1/(1+np.exp(-z))
    return a

# initialize
def initialize(dim):
    w = np.zeros(dim).reshape(dim,1)
    b = 0
    return w

# compute the loss
def computeloss(numtraining,y,a):
    loss=-1/numtraining *np.sum(y*np.log(a) + (1-y)*(np.log(1-a)))
    return(loss)

# execute the forward propagation
def forwardpropagation(w,b,x,y):
    # compute z
    z=np.dot(w.t,x)+b
    # determine the number of training samples
    numtraining=float(len(x))
    # compute the output of the sigmoid activation function
    a=sigmoid(z)
    #compute the loss
    loss = computeloss(numtraining,y,a)
    # compute the gradients dz, dw and db
    dz=a-y
    dw=1/numtraining*np.dot(x,dz.t)
    db=1/numtraining*np.sum(dz)

    # return the results as a dictionary
    gradients = {"dw": dw,
             "db": db}
    loss = np.squeeze(loss)
    return gradients,loss

# compute id119
def gradientdescent(w, b, x, y, numierations, learningrate):
    losses=[]
    idx =[]
    # iterate
    for i in range(numierations):
        gradients,loss=forwardpropagation(w,b,x,y)
        #get the derivates
        dw = gradients["dw"]
        db = gradients["db"]
        w = w-learningrate*dw
        b = b-learningrate*db

        # store the loss
        if i % 100 == 0:
            idx.append(i)
            losses.append(loss)
        # set params and grads
        params = {"w": w,
                  "b": b}
        grads = {"dw": dw,
                 "db": db}

    return params, grads, losses,idx

# predict the output for a training set
def predict(w,b,x):
    size=x.shape[1]
    ypredicted=np.zeros((1,size))
    z=np.dot(w.t,x)
    # compute the sigmoid
    a=sigmoid(z)
    for i in range(a.shape[1]):
        #if the value is > 0.5 then set as 1
        if(a[0][i] > 0.5):
            ypredicted[0][i]=1
        else:
        # else set as 0
            ypredicted[0][i]=0

    return ypredicted

#normalize the data
def normalize(x):
    x_norm = none
    x_norm = np.linalg.norm(x,axis=1,keepdims=true)
    x= x/x_norm
    return x


# run the 2 layer neural network on the cancer data set

from sklearn.datasets import load_breast_cancer
# load the cancer data
(x_cancer, y_cancer) = load_breast_cancer(return_x_y = true)
# create train and test sets
x_train, x_test, y_train, y_test = train_test_split(x_cancer, y_cancer,
                                                   random_state = 0)
# normalize the data for better performance
x_train1=normalize(x_train)


# create weight vectors of zeros. the size is the number of features in the data
 set=30
w=np.zeros((x_train.shape[1],1))
#w=np.zeros((30,1))
b=0

#normalize the training data so that id119 performs better
x_train1=normalize(x_train)
#transpose x_train so that we have a matrix as (features, numsamples)
x_train2=x_train1.t

# reshape to remove the rank 1 array and then transpose
y_train1=y_train.reshape(len(y_train),1)
y_train2=y_train1.t

# run id119 for 4000 times and compute the weights
parameters, grads, costs,idx = gradientdescent(w, b, x_train2, y_train2, numiera
tions=4000, learningrate=0.75)
w = parameters["w"]
b = parameters["b"]


# normalize x_test
x_test1=normalize(x_test)
#transpose x_train so that we have a matrix as (features, numsamples)
x_test2=x_test1.t

#reshape y_test
y_test1=y_test.reshape(len(y_test),1)
y_test2=y_test1.t

# predict the values for
ypredictiontest = predict(w, b, x_test2)
ypredictiontrain = predict(w, b, x_train2)

# print the accuracy
print("train accuracy: {} %".format(100 - np.mean(np.abs(ypredictiontrain - y_tr
ain2)) * 100))
print("test accuracy: {} %".format(100 - np.mean(np.abs(ypredictiontest - y_test
)) * 100))

# plot the costs vs the number of iterations
fig1=plt.plot(idx,costs)
fig1=plt.title("id119-cost vs no of iterations")
fig1=plt.xlabel("no of iterations")
fig1=plt.ylabel("cost")
fig1.figure.savefig("fig1", bbox_inches='tight')
## train accuracy: 90.3755868545 %
## test accuracy: 89.5104895105 %

   note: it can be seen that the accuracy on the training and test set is
   90.37% and 89.51%. this is comparatively poorer than the 96% which the
   id28 of sklearn achieves! but this is mainly because of
   the absence of hidden layers which is the real power of neural
   networks.

4. neural network for id28 -r code (vectorized)

source("rfunctions-1.r")
# define the sigmoid function
sigmoid <- function(z){
    a <- 1/(1+ exp(-z))
    a
}

# compute the loss
computeloss <- function(numtraining,y,a){
    loss <- -1/numtraining* sum(y*log(a) + (1-y)*log(1-a))
    return(loss)
}

# compute forward propagation
forwardpropagation <- function(w,b,x,y){
    # compute z
    z <- t(w) %*% x +b
    #set the number of samples
    numtraining <- ncol(x)
    # compute the activation function
    a=sigmoid(z)

    #compute the loss
    loss <- computeloss(numtraining,y,a)

    # compute the gradients dz, dw and db
    dz<-a-y
    dw<-1/numtraining * x %*% t(dz)
    db<-1/numtraining*sum(dz)

    fwdprop <- list("loss" = loss, "dw" = dw, "db" = db)
    return(fwdprop)
}

# perform one cycle of id119
gradientdescent <- function(w, b, x, y, numierations, learningrate){
    losses <- null
    idx <- null
    # loop through the number of iterations
    for(i in 1:numierations){
        fwdprop <-forwardpropagation(w,b,x,y)
        #get the derivatives
        dw <- fwdprop$dw
        db <- fwdprop$db
        #perform id119
        w = w-learningrate*dw
        b = b-learningrate*db
        l <- fwdprop$loss
        # stoe the loss
        if(i %% 100 == 0){
            idx <- c(idx,i)
            losses <- c(losses,l)
        }
    }

    # return the weights and losses
    graddescnt <- list("w"=w,"b"=b,"dw"=dw,"db"=db,"losses"=losses,"idx"=idx)

    return(graddescnt)
}

# compute the predicted value for input
predict <- function(w,b,x){
    m=dim(x)[2]
    # create a ector of 0's
    ypredicted=matrix(rep(0,m),nrow=1,ncol=m)
    z <- t(w) %*% x +b
    # compute sigmoid
    a=sigmoid(z)
    for(i in 1:dim(a)[2]){
        # if a > 0.5 set value as 1
        if(a[1,i] > 0.5)
        ypredicted[1,i]=1
       else
        # else set as 0
        ypredicted[1,i]=0
    }

    return(ypredicted)
}

# normalize the matrix
normalize <- function(x){
    #create the norm of the matrix.perform the frobenius norm of the matrix
    n<-as.matrix(sqrt(rowsums(x^2)))
    #sweep by rows by norm. note '1' in the function which performing on every r
ow
    normalized<-sweep(x, 1, n, fun="/")
    return(normalized)
}

# run the 2 layer neural network on the cancer data set
# read the data (from sklearn)
cancer <- read.csv("cancer.csv")
# rename the target variable
names(cancer) <- c(seq(1,30),"output")
# split as training and test sets
train_idx <- traintestsplit(cancer,trainpercent=75,seed=5)
train <- cancer[train_idx, ]
test <- cancer[-train_idx, ]

# set the features
x_train <-train[,1:30]
y_train <- train[,31]
x_test <- test[,1:30]
y_test <- test[,31]
# create a matrix of 0's with the number of features
w <-matrix(rep(0,dim(x_train)[2]))
b <-0
x_train1 <- normalize(x_train)
x_train2=t(x_train1)

# reshape  then transpose
y_train1=as.matrix(y_train)
y_train2=t(y_train1)

# perform id119
graddescent= gradientdescent(w, b, x_train2, y_train2, numierations=3000, learni
ngrate=0.77)


# normalize x_test
x_test1=normalize(x_test)
#transpose x_train so that we have a matrix as (features, numsamples)
x_test2=t(x_test1)

#reshape y_test and take transpose
y_test1=as.matrix(y_test)
y_test2=t(y_test1)

# use the values of the weights generated from id119
ypredictiontest = predict(graddescent$w, graddescent$b, x_test2)
ypredictiontrain = predict(graddescent$w, graddescent$b, x_train2)

sprintf("train accuracy: %f",(100 - mean(abs(ypredictiontrain - y_train2)) * 100
))
## [1] "train accuracy: 90.845070"
sprintf("test accuracy: %f",(100 - mean(abs(ypredictiontest - y_test)) * 100))
## [1] "test accuracy: 87.323944"
df <-data.frame(graddescent$idx, graddescent$losses)
names(df) <- c("iterations","losses")
ggplot(df,aes(x=iterations,y=losses)) + geom_point() + geom_line(col="blue") +
    ggtitle("id119 - losses vs no of iterations") +
    xlab("no of iterations") + ylab("losses")

4. neural network for id28 -octave code (vectorized)

   1;
   # define sigmoid function
   function a = sigmoid(z)
   a = 1 ./ (1+ exp(-z));
   end
   # compute the loss
   function loss=computeloss(numtraining,y,a)
   loss = -1/numtraining * sum((y .* log(a)) + (1-y) .* log(1-a));
   end
   # perform forward propagation
   function [loss,dw,db,dz] = forwardpropagation(w,b,x,y)
   % compute z
   z = w' * x + b;
   numtraining = size(x)(1,2);
   # compute sigmoid
   a = sigmoid(z);
   #compute loss. note this is element wise product
   loss =computeloss(numtraining,y,a);
   # compute the gradients dz, dw and db
   dz = a-y;
   dw = 1/numtraining* x * dz';
   db =1/numtraining*sum(dz);

   end
   # compute id119
   function [w,b,dw,db,losses,index]=gradientdescent(w, b, x, y,
   numierations, learningrate)
   #initialize losses and idx
   losses=[];
   index=[];
   # loop through the number of iterations
   for i=1:numierations,
   [loss,dw,db,dz] = forwardpropagation(w,b,x,y);
   # perform id119
   w = w - learningrate*dw;
   b = b - learningrate*db;
   if(mod(i,100) ==0)
   # append index and loss
   index = [index i];
   losses = [losses loss];
   endif

   end
   end
   # determine the predicted value for dataset
   function ypredicted = predict(w,b,x)
   m = size(x)(1,2);
   ypredicted=zeros(1,m);
   # compute z
   z = w' * x + b;
   # compute sigmoid
   a = sigmoid(z);
   for i=1:size(x)(1,2),
   # set predicted as 1 if a > 0,5
   if(a(1,i) >= 0.5)
   ypredicted(1,i)=1;
   else
   ypredicted(1,i)=0;
   endif
   end
   end
   # normalize by dividing each value by the sum of squares
   function normalized = normalize(x)
   # compute frobenius norm. square the elements, sum rows and then find
   square root
   a = sqrt(sum(x .^ 2,2));
   # perform element wise division
   normalized = x ./ a;
   end
   # split into train and test sets
   function [x_train,y_train,x_test,y_test] =
   traintestsplit(dataset,trainpercent)
   # create a random index
   ix = randperm(length(dataset));
   # split into training
   trainsize = floor(trainpercent/100 * length(dataset));
   train=dataset(ix(1:trainsize),:);
   # and test
   test=dataset(ix(trainsize+1:length(dataset)),:);
   x_train = train(:,1:30);
   y_train = train(:,31);
   x_test = test(:,1:30);
   y_test = test(:,31);
   end

   cancer=csvread("cancer.csv");
   [x_train,y_train,x_test,y_test] = traintestsplit(cancer,75);
   w=zeros(size(x_train)(1,2),1);
   b=0;
   x_train1=normalize(x_train);
   x_train2=x_train1';
   y_train1=y_train';
   [w1,b1,dw,db,losses,idx]=gradientdescent(w, b, x_train2, y_train1,
   numierations=3000, learningrate=0.75);
   # normalize x_test
   x_test1=normalize(x_test);
   #transpose x_train so that we have a matrix as (features, numsamples)
   x_test2=x_test1';
   y_test1=y_test';
   # use the values of the weights generated from id119
   ypredictiontest = predict(w1, b1, x_test2);
   ypredictiontrain = predict(w1, b1, x_train2);

   trainaccuracy=100-mean(abs(ypredictiontrain - y_train1))*100
   testaccuracy=100- mean(abs(ypredictiontest - y_test1))*100
   trainaccuracy = 90.845
   testaccuracy = 89.510
   graphics_toolkit('gnuplot')
   plot(idx,losses);
   title ('id119- cost vs no of iterations');
   xlabel ("no of iterations");
   ylabel ("cost");

   conclusion
   this post starts with a simple 2 layer neural network implementation of
   id28. clearly the performance of this simple neural
   network is comparatively poor to the highly optimized sklearn   s
   id28. this is because the above neural network did not
   have any hidden layers. deep learning & neural networks achieve
   extraordinary performance because of the presence of deep hidden layers

   the deep learning journey has begun    don   t miss the bus!
   stay tuned for more interesting posts in deep learning!!

   references
   1. [50]deep learning specialization
   2. [51]neural networks for machine learning
   3. [52]deep learning, ian goodfellow, yoshua bengio and aaron courville
   4. [53]neural networks: the mechanics of id26
   5. [54]machine learning

   also see
   1. [55]my book    practical machine learning with r and python    on amazon
   2. [56]simplifying machine learning: bias, variance, id173 and
   odd facts     part 4
   3. [57]the 3rd paperback & kindle editions of my books on cricket, now
   on amazon
   4. [58]practical machine learning with r and python     part 4
   5. [59]introducing qcsimulator: a 5-qubit quantum computing simulator
   in r
   6. [60]a bluemix recipe with mongodb and node.js
   7. [61]my travels through the realms of data science, machine learning,
   deep learning and (ai)

   to see all posts check [62]index of posts

rate this:

share:

     *
     *
     * [63]pocket
     * [64]tweet
     *

       iframe:
       [65]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fgigadom.in%2f2018%2f01%2f04%2fdeep-le
       arning-from-basic-principles-in-python-r-and-octave-part-1%2f&title
       =deep%20learning%20from%20first%20principles%20in%20python%2c%20r%2
       0and%20octave%20-%20part%201

     * [66][pinit_fg_en_rect_gray_20.png]
     * [67]more
     *

     * [68]email
     * [69]share on tumblr
     *
     * [70]telegram
     * [71]print
     *
     *

like this:

   like loading...

related

     * tagged
     * [72]cost
     * [73]deep learning
     * [74]id28
     * [75]neural network
     * [76]numpy
     * [77]octave
     * [78]python
     * [79]r
     * [80]r language
     * [81]r markdown
     * [82]r project

published by tinniam v ganesh

   visionary, thought leader and pioneer with 27+ years of experience in
   the software industry. [83]view all posts by tinniam v ganesh
   published january 4, 2018january 14, 2019

post navigation

   [84]previous post the 3rd paperback & kindle editions of my books on
   cricket, now on amazon
   [85]next post deep learning from first principles in python, r and
   octave     part 2

27 thoughts on    deep learning from first principles in python, r and octave    
part 1   

    1. pingback: [86]deep learning from first principles in python, r and
       octave     part 1     mubashir qasim
    2.
   dan says:
       [87]january 4, 2018 at 10:25 pm
       this is really well done. nice to open up the black box and show
       everything all the way down to reminding us of the chain rule from
       high school!
       [88]likelike
       [89]reply
         1.
        [90]tinniam v ganesh says:
            [91]january 5, 2018 at 5:33 am
            dan,
            thanks. appreciate it!
            regards
            ganesh
            [92]likelike
            [93]reply
    3. pingback: [94]distilled news | data analytics & r
    4. pingback: [95]deep learning from first principles in python, r and
       octave     part 2 | giga thoughts    
    5. pingback: [96]deep learning from first principles in python, r and
       octave     part 2     cloud data architect
    6. pingback: [97]deep learning per r, python i octave | bloc
       d'estad  stica oficial
    7.
   eafpres says:
       [98]january 12, 2018 at 5:46 pm
       hi   i   m having trouble due to the    source    command; where can i find
       the    rfunctions-1.r   ?
       [99]likelike
       [100]reply
         1.
        [101]tinniam v ganesh says:
            [102]january 12, 2018 at 6:26 pm
            hi,
            sorry. i forgot to include in github. i have now uploaded the
            file in github
            [103]https://github.com/tvganesh/deeplearning-part1
            regards
            ganesh
            [104]likelike
            [105]reply
              1.
             [106]blaine bateman says:
                 [107]january 12, 2018 at 9:26 pm
                 thank you!
                 [108]likelike
    8.
   [109]blaine bateman says:
       [110]january 12, 2018 at 9:52 pm
       one more challenge i   m having. i   m not sure the data are in the
       correct structure. when i download the file there are 32 columns.
       the first is an id, and the second is a string value for the
       diagnosis, and the rest are predictors. the code seems to think
       there are 31 columns and the last is the outcome. can you clarify
       for me? thanks again.
       [111]likelike
       [112]reply
         1.
        [113]blaine bateman says:
            [114]january 12, 2018 at 10:08 pm
            i sorted it out. converted the outcome to binary, put in last
            column, and removed 1st 2 and it runs as per your examples.
            [115]likelike
            [116]reply
              1.
             [117]tinniam v ganesh says:
                 [118]january 13, 2018 at 2:52 am
                 cool!
                 [119]likelike
         2.
        [120]tinniam v ganesh says:
            [121]january 13, 2018 at 2:52 am
            yes. you will have to skip the 1st column
            [122]likelike
            [123]reply
    9. pingback: [124]deep learning from first principles in python, r and
       octave     part 3 | giga thoughts    
   10. pingback: [125]deep learning from first principles in python, r and
       octave     part 4 | giga thoughts    
   11. pingback: [126]deep learning from first principles in python, r and
       octave     part 5 | giga thoughts    
   12. pingback: [127]deep learning from first principles in python, r and
       octave     part 5     cloud data architect
   13. pingback: [128]deep learning from first principles in python, r and
       octave     part 6 | giga thoughts    
   14. pingback: [129]deep learning from first principles in python, r and
       octave     part 6 | r-bloggers
   15. pingback: [130]deep learning from first principles in python, r and
       octave     part 8 | giga thoughts    
   16. pingback: [131]deep learning from first principles in python, r and
       octave     part 8 | r-bloggers
   17.
   franco says:
       [132]june 7, 2018 at 5:09 pm
       hi!
       it seams the id172 is calculated column wise, if so can
       anyone explain why?
       thank you in advance
       [133]likelike
       [134]reply
         1.
        [135]tinniam v ganesh says:
            [136]june 8, 2018 at 2:20 am
            franco     all the features are in columns. id172 tries
            to make the values between 0     1 for all features and hence is
            column wise. id119 performs better when features
            are normalized. ganesh
            [137]likelike
            [138]reply
              1.
             franco says:
                 [139]june 8, 2018 at 10:54 am
                 i was making a mistake, calculating a l2 norm column
                 wise    now i got it   
                 thank you ganesh,
                 franco
                 [140]likelike
              2.
             [141]tinniam v ganesh says:
                 [142]june 8, 2018 at 1:22 pm
                 you are welcome!
                 [143]likelike
   18. pingback: [144]my presentations on    elements of neural networks &
       deep learning    -part1,2,3 | giga thoughts    

leave a reply [145]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [146]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [147]log out /
   [148]change )
   google photo

   you are commenting using your google account. ( [149]log out /
   [150]change )
   twitter picture

   you are commenting using your twitter account. ( [151]log out /
   [152]change )
   facebook photo

   you are commenting using your facebook account. ( [153]log out /
   [154]change )
   [155]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

connect with me:

     * [156]linkedin
     * [157]github
     * [158]twitter

   search for: ____________________ search

blog stats

     * 440,452 hits

visitors to giga thoughts (click to see details)

   [159]map

   [160]follow giga thoughts     on wordpress.com

popular posts

     * [161]working with node.js and postgresql
     * [162]simplifying ml: impact of degree of polynomial degree on bias
       & variance and other insights
     * [163]introducing cricketr! : an r package to analyze performances
       of cricketers
     * [164]re-introducing cricketr! : an r package to analyze
       performances of cricketers
     * [165]experiments with deblurring using opencv
     * [166]deep learning from first principles in python, r and octave -
       part 1
     * [167]my presentations on    elements of neural networks & deep
       learning    -parts 4,5
     * [168]practical machine learning with r and python     part 5
     * [169]introducing cricpy:a python package to analyze performances of
       cricketers
     * [170]r vs python: different similarities and similar differences

category cloud

   [171]analytics [172]android [173]android app [174]app [175]batsman
   [176]big data [177]bluemix [178]bowler [179]cloud computing
   [180]cricket [181]cricketr [182]cricsheet [183]data mining [184]deep
   learning [185]distributed systems [186]git [187]github [188]gradient
   descent [189]id75 [190]id28 [191]machine
   learning [192]neural networks [193]python [194]r [195]r language [196]r
   markdown [197]r package [198]r project [199]technology [200]yorkr

follow blog via email

   join 1,212 other followers

   ____________________

   (button) follow

subscribe

   [201]rss feed  [202]rss - posts

giga thoughts community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

archives

   archives [select month_______]

navigate

     * [203]home
     * [204]index of posts
     * [205]books i authored
     * [206]who am i?
     * [207]published posts
     * [208]about giga thoughts   

latest posts

     * [209]analyzing performances of cricketers using cricketr template
       march 30, 2019
     * [210]the clash of the titans in test and odi cricket march 15, 2019
     * [211]analyzing t20 matches with yorkpy templates march 10, 2019
     * [212]yorkpy takes a hat-trick, bowls out intl. t20s, bbl and
       natwest t20!!! march 3, 2019
     * [213]pitching yorkpy     in the block hole     part 4 february 26, 2019
     * [214]take 4+: presentations on    elements of neural networks and
       deep learning        parts 1-8 february 16, 2019
     * [215]pitching yorkpy   swinging away from the leg stump to ipl    
       part 3 february 3, 2019
     * [216]pitching yorkpy   on the middle and outside off-stump to ipl    
       part 2 january 27, 2019

   [217]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [218]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [219]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [220]likes-master

   %d bloggers like this:

references

   visible links
   1. https://gigadom.in/feed/
   2. https://gigadom.in/comments/feed/
   3. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/feed/
   4. https://gigadom.in/2017/12/16/the-3rd-paperback-edition-of-my-books-on-cricket-on-amazon/
   5. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/&for=wpcom-auto-discovery
   8. https://gigadom.in/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#content
  11. https://gigadom.in/
  12. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
  13. https://github.com/tvganesh
  14. https://twitter.com/tvganesh_85
  15. https://gigadom.in/
  16. https://gigadom.in/aa-2/
  17. https://gigadom.in/and-you-are/
  18. https://gigadom.in/who-am-i/
  19. https://gigadom.in/published-posts/
  20. https://gigadom.in/about-giga-thoughts/
  21. https://gigadom.in/author/gigadom/
  22. https://gigadom.in/category/chain-rule/
  23. https://gigadom.in/category/deep-learning/
  24. https://gigadom.in/category/git/
  25. https://gigadom.in/category/github/
  26. https://gigadom.in/category/logistic-regression/
  27. https://gigadom.in/category/loss/
  28. https://gigadom.in/category/machine-learning/
  29. https://gigadom.in/category/neural-networks/
  30. https://gigadom.in/category/numpy/
  31. https://gigadom.in/category/octave/
  32. https://gigadom.in/category/python-2/
  33. https://gigadom.in/category/r/
  34. https://gigadom.in/category/r-language/
  35. https://gigadom.in/category/r-markdown/
  36. https://gigadom.in/category/r-package/
  37. https://gigadom.in/category/r-project/
  38. https://gigadom.in/category/sklearn/
  39. https://gigadom.in/category/technology/
  40. https://www.coursera.org/specializations/deep-learning
  41. https://www.coursera.org/learn/neural-networks
  42. https://github.com/tvganesh/deeplearning-part1
  43. https://www.youtube.com/watch?v=fcoqmulxib8&t=9s
  44. https://www.youtube.com/watch?v=77dumwh6heu&t=5s
  45. https://gigadom.wordpress.com/2017/10/13/practical-machine-learning-with-r-and-python-part-2/
  46. https://www.amazon.com/dp/1791596177
  47. https://www.amazon.com/dp/b07lbg542l
  48. https://www.amazon.com/dp/1983035661
  49. https://www.amazon.com/dp/b07dfkscwz
  50. https://www.coursera.org/specializations/deep-learning
  51. https://www.coursera.org/learn/neural-networks
  52. http://www.deeplearningbook.org/
  53. https://gigadom.wordpress.com/2017/01/21/neural-networks-the-mechanics-of-id26/
  54. https://www.coursera.org/learn/machine-learning
  55. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  56. https://gigadom.wordpress.com/2014/01/03/simplifying-machine-learning-bias-variance-id173-and-odd-facts-part-4/
  57. https://gigadom.wordpress.com/2017/12/16/the-3rd-paperback-edition-of-my-books-on-cricket-on-amazon/
  58. https://gigadom.wordpress.com/2017/10/29/practical-machine-learning-with-r-and-python-part-4/
  59. https://gigadom.wordpress.com/2016/06/23/introducing-qcsimulator-a-5-qubit-quantum-computing-simulator-in-r/
  60. https://gigadom.wordpress.com/2014/07/27/a-bluemix-recipe-with-mongodb-and-node-js/
  61. https://gigadom.wordpress.com/2017/09/11/my-travels-through-the-realms-of-data-science-machine-learning-deep-learning-and-ai/
  62. https://gigadom.wordpress.com/aa-2/
  63. https://getpocket.com/save
  64. https://twitter.com/share
  65. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/&title=deep learning from first principles in python, r and octave - part 1
  66. https://www.pinterest.com/pin/create/button/?url=https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/&media=https://gigadom.files.wordpress.com/2018/01/lr-11.png&description=deep learning from first principles in python, r and octave - part 1
  67. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  68. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?share=email
  69. https://www.tumblr.com/share
  70. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?share=telegram
  71. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#print
  72. https://gigadom.in/tag/cost/
  73. https://gigadom.in/tag/deep-learning/
  74. https://gigadom.in/tag/logistic-regression/
  75. https://gigadom.in/tag/neural-network/
  76. https://gigadom.in/tag/numpy/
  77. https://gigadom.in/tag/octave/
  78. https://gigadom.in/tag/python/
  79. https://gigadom.in/tag/r/
  80. https://gigadom.in/tag/r-language-2/
  81. https://gigadom.in/tag/r-markdown/
  82. https://gigadom.in/tag/r-project/
  83. https://gigadom.in/author/gigadom/
  84. https://gigadom.in/2017/12/16/the-3rd-paperback-edition-of-my-books-on-cricket-on-amazon/
  85. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  86. http://mqasim.me/?p=150446
  87. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5143
  88. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5143&_wpnonce=5c177eecc5
  89. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5143#respond
  90. https://gigadom.wordpress.com/
  91. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5144
  92. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5144&_wpnonce=cc577b7e66
  93. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5144#respond
  94. http://advanceddataanalytics.net/2018/01/06/distilled-news-673/
  95. https://gigadom.wordpress.com/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  96. http://www.dataarchitect.cloud/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  97. https://blocestadistica.wordpress.com/2018/01/12/deep-learning-per-r-python-i-octave/
  98. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5181
  99. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5181&_wpnonce=3a10ae9695
 100. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5181#respond
 101. https://gigadom.wordpress.com/
 102. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5182
 103. https://github.com/tvganesh/deeplearning-part1
 104. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5182&_wpnonce=5a70da7fac
 105. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5182#respond
 106. https://plus.google.com/+blainebateman
 107. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5184
 108. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5184&_wpnonce=98bf8c01f5
 109. https://plus.google.com/+blainebateman
 110. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5185
 111. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5185&_wpnonce=b522bf3364
 112. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5185#respond
 113. https://plus.google.com/+blainebateman
 114. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5186
 115. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5186&_wpnonce=98e2c8bb4f
 116. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5186#respond
 117. https://gigadom.wordpress.com/
 118. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5189
 119. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5189&_wpnonce=03bda00004
 120. https://gigadom.wordpress.com/
 121. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5188
 122. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5188&_wpnonce=93f8b815d0
 123. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5188#respond
 124. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 125. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 126. https://gigadom.wordpress.com/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 127. http://www.dataarchitect.cloud/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 128. https://gigadom.wordpress.com/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
 129. https://www.r-bloggers.com/deep-learning-from-first-principles-in-python-r-and-octave-part-6/?utm_source=feedburner&utm_medium=feed&utm_campaign=feed:+rbloggers+(r+bloggers)
 130. https://gigadom.wordpress.com/2018/05/06/deep-learning-from-first-principles-in-python-r-and-octave-part-8/
 131. https://www.r-bloggers.com/deep-learning-from-first-principles-in-python-r-and-octave-part-8/?utm_source=feedburner&utm_medium=feed&utm_campaign=feed:+rbloggers+(r+bloggers)
 132. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5955
 133. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5955&_wpnonce=1ce28f2059
 134. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5955#respond
 135. https://gigadom.wordpress.com/
 136. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5958
 137. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5958&_wpnonce=b4c048a4c8
 138. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?replytocom=5958#respond
 139. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5960
 140. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5960&_wpnonce=5bfe9811a2
 141. https://gigadom.wordpress.com/
 142. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-5961
 143. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/?like_comment=5961&_wpnonce=7a4a59156b
 144. https://gigadom.in/2019/01/10/my-presentations-on-elements-of-neural-networks-deep-learning-part123/
 145. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#respond
 146. https://gravatar.com/site/signup/
 147. javascript:highlandercomments.doexternallogout( 'wordpress' );
 148. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 149. javascript:highlandercomments.doexternallogout( 'googleplus' );
 150. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 151. javascript:highlandercomments.doexternallogout( 'twitter' );
 152. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 153. javascript:highlandercomments.doexternallogout( 'facebook' );
 154. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 155. javascript:highlandercomments.cancelexternalwindow();
 156. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
 157. https://github.com/tvganesh
 158. https://twitter.com/tvganesh_85
 159. https://www.revolvermaps.com/?target=enlarge&i=0z8r51l0ucz
 160. https://gigadom.in/
 161. https://gigadom.in/2014/07/20/working-with-node-js-and-postgresql/
 162. https://gigadom.in/2014/01/04/simplifying-ml-impact-of-degree-of-polynomial-degree-on-bias-variance-and-other-insights/
 163. https://gigadom.in/2015/07/04/introducing-cricketr-a-r-package-to-analyze-performances-of-cricketers/
 164. https://gigadom.in/2016/05/14/re-introducing-cricketr-an-r-package-to-analyze-performances-of-cricketers/
 165. https://gigadom.in/2011/11/09/experiments-with-deblurring-using-opencv/
 166. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 167. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 168. https://gigadom.in/2017/11/07/practical-machine-learning-with-r-and-python-part-5/
 169. https://gigadom.in/2018/10/28/introducing-cricpya-python-package-to-analyze-performances-of-cricketrs/
 170. https://gigadom.in/2017/05/22/r-vs-python-different-similarities-and-similar-differences/
 171. https://gigadom.in/category/analytics/
 172. https://gigadom.in/category/android/
 173. https://gigadom.in/category/android-app/
 174. https://gigadom.in/category/app/
 175. https://gigadom.in/category/batsman/
 176. https://gigadom.in/category/big-data/
 177. https://gigadom.in/category/bluemix/
 178. https://gigadom.in/category/bowler/
 179. https://gigadom.in/category/cloud-computing/
 180. https://gigadom.in/category/cricket/
 181. https://gigadom.in/category/cricketr/
 182. https://gigadom.in/category/cricsheet/
 183. https://gigadom.in/category/data-mining/
 184. https://gigadom.in/category/deep-learning/
 185. https://gigadom.in/category/distributed-systems/
 186. https://gigadom.in/category/git/
 187. https://gigadom.in/category/github/
 188. https://gigadom.in/category/gradient-descent/
 189. https://gigadom.in/category/linear-regression/
 190. https://gigadom.in/category/logistic-regression/
 191. https://gigadom.in/category/machine-learning/
 192. https://gigadom.in/category/neural-networks/
 193. https://gigadom.in/category/python-2/
 194. https://gigadom.in/category/r/
 195. https://gigadom.in/category/r-language/
 196. https://gigadom.in/category/r-markdown/
 197. https://gigadom.in/category/r-package/
 198. https://gigadom.in/category/r-project/
 199. https://gigadom.in/category/technology/
 200. https://gigadom.in/category/yorkr/
 201. https://gigadom.in/feed/
 202. https://gigadom.in/feed/
 203. https://gigadom.in/
 204. https://gigadom.in/aa-2/
 205. https://gigadom.in/and-you-are/
 206. https://gigadom.in/who-am-i/
 207. https://gigadom.in/published-posts/
 208. https://gigadom.in/about-giga-thoughts/
 209. https://gigadom.in/2019/03/30/analyzing-performances-of-cricketers-using-cricketr-template/
 210. https://gigadom.in/2019/03/15/the-clash-of-the-titans-in-test-and-odi-cricket/
 211. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
 212. https://gigadom.in/2019/03/03/yorkpy-takes-a-hat-trick-bowls-out-intl-t20s-bbl-and-natwest-t20/
 213. https://gigadom.in/2019/02/26/pitching-yorkpy-in-the-block-hole-part-4/
 214. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 215. https://gigadom.in/2019/02/03/pitching-yorkpyswinging-away-from-the-leg-stump-to-ipl-part-3/
 216. https://gigadom.in/2019/01/27/pitching-yorkpyon-the-middle-and-outside-off-stump-to-ipl-part-2/
 217. https://wordpress.com/?ref=footer_blog
 218. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 219. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#cancel
 220. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 222. https://gigadom.in/
 223. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-form-guest
 224. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-form-load-service:wordpress.com
 225. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-form-load-service:twitter
 226. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/#comment-form-load-service:facebook
 227. http://lemanshots.wordpress.com/
 228. https://vinodsblog.com/about
 229. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 230. http://friartuck2012.wordpress.com/
 231. http://webastion.wordpress.com/
 232. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 233. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 234. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 235. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 236. http://micvdotin.wordpress.com/
