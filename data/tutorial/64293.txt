   #[1]giga thoughts ...    feed [2]giga thoughts ...    comments feed
   [3]giga thoughts ...    deep learning from first principles in python, r
   and octave     part 2 comments feed [4]deep learning from first
   principles in python, r and octave     part 1 [5]deep learning from first
   principles in python, r and octave     part 3 [6]alternate [7]alternate
   [8]giga thoughts ... [9]wordpress.com

   [10]skip to content

   [11]giga thoughts    

   insights into technology

     * [12]linkedin
     * [13]github
     * [14]twitter

   (button) menu

     * [15]home
     * [16]index of posts
     * [17]books i authored
     * [18]who am i?
     * [19]published posts
     * [20]about giga thoughts   

deep learning from first principles in python, r and octave     part 2

   [21]tinniam v ganesh [22]id26, [23]backward propagation,
   [24]chain rule, [25]contours, [26]deep learning, [27]github,
   [28]id28, [29]neural networks, [30]octave, [31]python,
   [32]r, [33]r language, [34]r markdown, [35]r package, [36]r project,
   [37]regression, [38]sigmoid, [39]technology january 11, 2018february 7,
   2019

      what does the world outside your head really    look    like? not only is
   there no color, there   s also no sound: the compression and expansion of
   air is picked up by the ears, and turned into electrical signals. the
   brain then presents these signals to us as mellifluous tones and
   swishes and clatters and jangles. reality is also odorless: there   s no
   such thing as smell outside our brains. molecules floating through the
   air bind to receptors in our nose and are interpreted as different
   smells by our brain. the real world is not full of rich sensory events;
   instead, our brains light up the world with their own sensuality.   
   the brain: the story of you    by david eagleman

      the world is maya, illusory. the ultimate reality, the brahman, is
   all-pervading and all-permeating, which is colourless, odourless,
   tasteless, nameless and forid113ss   
   bhagavad gita

1. introduction

   this post is a follow-up post to my earlier post [40]deep learning from
   first principles in python, r and octave-part 1. in the first part, i
   implemented id28, in vectorized python,r and octave,
   with a wannabe neural network (a neural network with no hidden layers).
   in this second part, i implement a regular, but somewhat primitive
   neural network (a neural network with just 1 hidden layer). the 2nd
   part implements classification of manually created datasets, where the
   different clusters of the 2 classes are not linearly separable.

   neural network perform really well in learning all sorts of non-linear
   boundaries between classes. initially id28 is used
   perform the classification and the decision boundary is plotted.
   vanilla id28 performs quite poorly. using id166s with a
   radial basis kernel would have performed much better in creating
   non-linear boundaries. to see r and python implementations of id166s take
   a look at my post [41]practical machine learning with r and python    
   part 4.

   checkout my book    deep learning from first principles: second edition    
   in vectorized python, r and octave   . my book starts with the
   implementation of a simple 2-layer neural network and works its way to
   a generic l-layer deep learning network, with all the bells and
   whistles. the derivations have been discussed in detail. the code has
   been extensively commented and included in its entirety in the appendix
   sections. my book is available on amazon as [42]paperback ($18.99) and
   in [43]kindle version($9.99/rs449).

   you may also like my companion book    practical machine learning with r
   and python:second edition- machine learning in stereo    available in
   amazon in [44]paperback($10.99) and [45]kindle($7.99/rs449) versions.
   this book is ideal for a quick reference of the various ml functions
   and associated measurements in both r and python which are essential to
   delve deep into deep learning.

   take a look at my video presentation which discusses the below
   derivation step-by- step [46]elements of neural networks and deep
   learning     part 3

   you can clone and fork this r markdown file along with the vectorized
   implementations of the 3 layer neural network for python, r and octave
   from github [47]deeplearning-part2

2. the 3 layer neural network

   a simple representation of a 3 layer neural network (nn) with 1 hidden
   layer is shown below.
   in the above neural network, there are 2 input features at the input
   layer, 3 hidden units at the hidden layer and 1 output layer as it
   deals with binary classification. the activation unit at the hidden
   layer can be a tanh, sigmoid, relu etc. at the output layer the
   activation is a sigmoid to handle binary classification

   # superscript indicates layer 1
   z_{11} = w_{11}^{1}x_{1} + w_{21}^{1}x_{2} + b_{1}
   z_{12} = w_{12}^{1}x_{1} + w_{22}^{1}x_{2} + b_{1}
   z_{13} = w_{13}^{1}x_{1} + w_{23}^{1}x_{2} + b_{1}

   also a_{11} = tanh(z_{11})
   a_{12} = tanh(z_{12})
   a_{13} = tanh(z_{13})

   # superscript indicates layer 2
   z_{21} = w_{11}^{2}a_{11} + w_{21}^{2}a_{12} + w_{31}^{2}a_{13} + b_{2}
   a_{21} = sigmoid(z21)

   hence
   z1= \begin{pmatrix} z11\\ z12\\ z13 \end{pmatrix} =\begin{pmatrix}
   w_{11}^{1} & w_{21}^{1} \\ w_{12}^{1} & w_{22}^{1} \\ w_{13}^{1} &
   w_{23}^{1} \end{pmatrix} * \begin{pmatrix} x1\\ x2 \end{pmatrix} +
   b_{1}
   and
   a1= \begin{pmatrix} a11\\ a12\\ a13 \end{pmatrix} = \begin{pmatrix}
   tanh(z11)\\ tanh(z12)\\ tanh(z13) \end{pmatrix}

   similarly
   z2= z_{21} = \begin{pmatrix} w_{11}^{2} & w_{21}^{2} & w_{31}^{2}
   \end{pmatrix} *\begin{pmatrix} z_{11}\\ z_{12}\\ z_{13} \end{pmatrix}
   +b_{2}
   and a2 = a_{21} = sigmoid(z_{21})

   these equations can be written as
   z1 = w1 * x + b1
   a1 = tanh(z1)
   z2 = w2 * a1 + b2
   a2 = sigmoid(z2)

   i) some important results (a memory refresher!)
   d/dx(e^{x}) = e^{x} and d/dx(e^{-x}) = -e^{-x} -(a) and
   sinhx = (e^{x} - e^{-x})/2 and coshx = (e^{x} + e^{-x})/2
   using (a) we can shown that d/dx(sinhx) = coshx and d/dx(coshx) = sinhx
   (b)
   now d/dx(f(x)/g(x)) = (g(x)*d/dx(f(x)) - f(x)*d/dx(g(x)))/g(x)^{2} -(c)

   since tanhx =z= sinhx/coshx and using (b) we get
   tanhx = (coshx*d/dx(sinhx) - sinhx*d/dx(coshx))/(cosh^{2})
   using the values of the derivatives of sinhx and coshx from (b) above
   we get
   d/dx(tanhx) = (coshx^{2} - sinhx{2})/coshx{2} = 1 - tanhx^{2}
   since tanhx =z
   d/dx(tanhx) = 1 - tanhx^{2}= 1 - z^{2} -(d)

   ii) derivatives
   l=-(ylog(a2) + (1-y)log(1-a2))
   dl/da2 = -(y/a2 + (1-y)/(1-a2))
   since a2 = sigmoid(z2) therefore da2/dz2 = a2(1-a2) see [48]part1
   z2 = w2a1 +b2
   dz2/dw2 = a1
   dz2/db2 = 1
   a1 = tanh(z1) and da1/dz1 = 1 - a1^{2}
   z1 = w1x + b1
   dz1/dw1 = x
   dz1/db1 = 1

   iii) back propagation
   using the derivatives from ii) we can derive the following results
   using chain rule
   \partial l/\partial z2 = \partial l/\partial a2 * \partial a2/\partial
   z2
   = -(y/a2 + (1-y)/(1-a2)) * a2(1-a2) = a2 - y
   \partial l/\partial w2 = \partial l/\partial a2 * \partial a2/\partial
   z2 * \partial z2/\partial w2
   = (a2-y) *a1 -(a)
   \partial l/\partial b2 = \partial l/\partial a2 * \partial a2/\partial
   z2 * \partial z2/\partial b2 = (a2-y) -(b)

   \partial l/\partial z1 = \partial l/\partial a2 * \partial a2/\partial
   z2 * \partial z2/\partial a1 *\partial a1/\partial z1 = (a2-y) * w2 *
   (1-a1^{2})
   \partial l/\partial w1 = \partial l/\partial a2 * \partial a2/\partial
   z2 * \partial z2/\partial a1 *\partial a1/\partial z1 *\partial
   z1/\partial w1
   =(a2-y) * w2 * (1-a1^{2}) * x -(c)
   \partial l/\partial b1 = \partial l/\partial a2 * \partial a2/\partial
   z2 * \partial z2/\partial a1 *da1/dz1 *dz1/db1
   = (a2-y) * w2 * (1-a1^{2}) -(d)

   iv) id119
   the key computations in the backward cycle are
   w1 = w1-learningrate * \partial l/\partial w1     from (c)
   b1 = b1-learningrate * \partial l/\partial b1     from (d)
   w2 = w2-learningrate * \partial l/\partial w2     from (a)
   b2 = b2-learningrate * \partial l/\partial b2     from (b)

   the weights and biases (w1,b1,w2,b2) are updated for each iteration
   thus minimizing the loss/cost.

   these derivations can be represented pictorially using the computation
   graph (from the book deep learning by ian goodfellow, joshua bengio and
   aaron courville)

3. manually create a data set that is not lineary separable

   initially i create a dataset with 2 classes which has around 9 clusters
   that cannot be separated by linear boundaries. note: this data set is
   saved as data.csv and is used for the r and octave neural networks to
   see how they perform on the same dataset.
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import listedcolormap
import sklearn
import sklearn.datasets


colors=['black','gold']
cmap = matplotlib.colors.listedcolormap(colors)
x, y = make_blobs(n_samples = 400, n_features = 2, centers = 7,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y=y.reshape(400,1)
y = y % 2
#plot the figure
plt.figure()
plt.title('non-linearly separable classes')
plt.scatter(x[:,0], x[:,1], c=y,
           marker= 'o', s=50,cmap=cmap)
plt.savefig('fig1.png', bbox_inches='tight')

4. id28

   on the above created dataset, classification with id28
   is performed, and the decision boundary is plotted. it can be seen that
   id28 performs quite poorly
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import listedcolormap
import sklearn
import sklearn.datasets

#from dlfunctions import plot_decision_boundary
execfile("./dlfunctions.py") # since import does not work in rmd!!!

colors=['black','gold']
cmap = matplotlib.colors.listedcolormap(colors)
x, y = make_blobs(n_samples = 400, n_features = 2, centers = 7,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y=y.reshape(400,1)
y = y % 2

# train the id28 classifier
clf = sklearn.linear_model.logisticregressioncv();
clf.fit(x, y);

# plot the decision boundary for id28
plot_decision_boundary_n(lambda x: clf.predict(x), x.t, y.t,"fig2.png")

5. the 3 layer neural network in python (vectorized)

   the vectorized implementation is included below. note that in the case
   of python a learning rate of 0.5 and 3 hidden units performs very well.
## random data set with 9 clusters
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import sklearn.linear_model
import pandas as pd

from sklearn.datasets import make_classification, make_blobs
execfile("./dlfunctions.py") # since import does not work in rmd!!!

x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t

#perform id119
parameters,costs = computenn(x2, y2, numhidden = 4, learningrate=0.5, numiterati
ons = 10000)
plot_decision_boundary(lambda x: predict(parameters, x.t), x2, y2,str(4),str(0.5
),"fig3.png")
## cost after iteration 0: 0.692669
## cost after iteration 1000: 0.246650
## cost after iteration 2000: 0.227801
## cost after iteration 3000: 0.226809
## cost after iteration 4000: 0.226518
## cost after iteration 5000: 0.226331
## cost after iteration 6000: 0.226194
## cost after iteration 7000: 0.226085
## cost after iteration 8000: 0.225994
## cost after iteration 9000: 0.225915



6. the 3 layer neural network in r (vectorized)

   for this the dataset created by python is saved  to see how r performs
   on the same dataset. the vectorized implementation of a neural network
   was just a little more interesting as r does not have a similar package
   like    numpy   . while numpy handles broadcasting implicitly, in r i had
   to use the    sweep    command to broadcast. the implementaion is included
   below. note that since the initialization with random weights is
   slightly different, r performs best with a learning rate of 0.1 and
   with 6 hidden units
source("dlfunctions2_1.r")
z <- as.matrix(read.csv("data.csv",header=false)) #
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)
#perform id119
nn <-computenn(x1, y1, 6, learningrate=0.1,numiterations=10000) # good
## [1] 0.7075341
## [1] 0.2606695
## [1] 0.2198039
## [1] 0.2091238
## [1] 0.211146
## [1] 0.2108461
## [1] 0.2105351
## [1] 0.210211
## [1] 0.2099104
## [1] 0.2096437
## [1] 0.209409
plotdecisionboundary(z,nn,6,0.1)

   [fig31.png?w=1100]

 7.  the 3 layer neural network in octave (vectorized)

   this uses the same dataset that was generated using python code.
   source("dl-function2.m")
   data=csvread("data.csv");
   x=data(:,1:2);
   y=data(:,3);
   # make sure that the model parameters are correct. take the transpose
   of x & y
   #perform id119
   [w1,b1,w2,b2,costs]= computenn(x', y',4, learningrate=0.5,
   numiterations = 10000);

8a. performance  for different learning rates (python)

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import sklearn.linear_model
import pandas as pd

from sklearn.datasets import make_classification, make_blobs
execfile("./dlfunctions.py") # since import does not work in rmd!!!
# create data
x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t
# create a list of learning rates
learningrate=[0.5,1.2,3.0]
df=pd.dataframe()
#compute costs for each learning rate
for lr in learningrate:
   parameters,costs = computenn(x2, y2, numhidden = 4, learningrate=lr, numitera
tions = 10000)
   print(costs)
   df1=pd.dataframe(costs)
   df=pd.concat([df,df1],axis=1)
#set the iterations
iterations=[0,1000,2000,3000,4000,5000,6000,7000,8000,9000]
#create data frame
#set index
df1=df.set_index([iterations])
df1.columns=[0.5,1.2,3.0]
fig=df1.plot()
fig=plt.title("cost vs no of iterations for different learning rates")
plt.savefig('fig4.png', bbox_inches='tight')

8b. performance  for different hidden units (python)

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import sklearn.linear_model
import pandas as pd

from sklearn.datasets import make_classification, make_blobs
execfile("./dlfunctions.py") # since import does not work in rmd!!!
#create data set
x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t
# make a list of hidden unis
numhidden=[3,5,7]
df=pd.dataframe()
#compute costs for different hidden units
for numhid in numhidden:
   parameters,costs = computenn(x2, y2, numhidden = numhid, learningrate=1.2, nu
miterations = 10000)
   print(costs)
   df1=pd.dataframe(costs)
   df=pd.concat([df,df1],axis=1)
#set the iterations
iterations=[0,1000,2000,3000,4000,5000,6000,7000,8000,9000]
#set index
df1=df.set_index([iterations])
df1.columns=[3,5,7]
#plot
fig=df1.plot()
fig=plt.title("cost vs no of iterations for different no of hidden units")
plt.savefig('fig5.png', bbox_inches='tight')

9a. performance  for different learning rates (r)

source("dlfunctions2_1.r")
# read data
z <- as.matrix(read.csv("data.csv",header=false)) #
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)
#loop through learning rates and compute costs
learningrate <-c(0.1,1.2,3.0)
df <- null
for(i in seq_along(learningrate)){
   nn <-  computenn(x1, y1, 6, learningrate=learningrate[i],numiterations=10000)

   cost <- nn$costs
   df <- cbind(df,cost)

}
#create dataframe
df <- data.frame(df)
iterations=seq(0,10000,by=1000)
df <- cbind(iterations,df)
names(df) <- c("iterations","0.5","1.2","3.0")
library(reshape2)
df1 <- melt(df,id="iterations")  # melt the data
#plot
ggplot(df1) + geom_line(aes(x=iterations,y=value,colour=variable),size=1)  +
    xlab("iterations") +
    ylab('cost') + ggtitle("cost vs no iterations for  different learning rates"
)

   [fig5.png?w=1100]

9b. performance  for different hidden units (r)

source("dlfunctions2_1.r")
# loop through num hidden units
numhidden <-c(4,6,9)
df <- null
for(i in seq_along(numhidden)){
    nn <-  computenn(x1, y1, numhidden[i], learningrate=0.1,numiterations=10000)

    cost <- nn$costs
    df <- cbind(df,cost)

}
df <- data.frame(df)
iterations=seq(0,10000,by=1000)
df <- cbind(iterations,df)
names(df) <- c("iterations","4","6","9")
library(reshape2)
# melt
df1 <- melt(df,id="iterations")
# plot
ggplot(df1) + geom_line(aes(x=iterations,y=value,colour=variable),size=1)  +
    xlab("iterations") +
    ylab('cost') + ggtitle("cost vs no iterations for  different number of hidde
n units")

   [fig2-1.png?w=1024&#038;h=732]

10a. performance of the neural network for different learning rates (octave)

   source("dl-function2.m")
   plotlrcostvsiterations()
   print -djph figa.jpg

10b. performance of the neural network for different number of hidden units
(octave)

   source("dl-function2.m")
   plothiddencostvsiterations()
   print -djph figa.jpg

11. turning the heat on the neural network

   in this 2nd part i create a a central region of positives and and the
   outside region as negatives. the points are generated using the
   equation of a circle (x     a)^{2} + (y -b) ^{2} = r^{2} . how does the 3
   layer neural network perform on this?  here   s a look! note: the same
   dataset is also used for r and octave neural network constructions

12. manually creating a circular central region

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import listedcolormap
import sklearn
import sklearn.datasets

colors=['black','gold']
cmap = matplotlib.colors.listedcolormap(colors)
x1=np.random.uniform(0,10,800).reshape(800,1)
x2=np.random.uniform(0,10,800).reshape(800,1)
x=np.append(x1,x2,axis=1)
x.shape
# create (x-a)^2 + (y-b)^2 = r^2
# create a subset of values where squared is <0,4. perform ravel() to flatten th
is vector
a=(np.power(x[:,0]-5,2) + np.power(x[:,1]-5,2) <= 6).ravel()
y=a.reshape(800,1)

cmap = matplotlib.colors.listedcolormap(colors)

plt.figure()
plt.title('non-linearly separable classes')
plt.scatter(x[:,0], x[:,1], c=y,
           marker= 'o', s=15,cmap=cmap)
plt.savefig('fig6.png', bbox_inches='tight')

13a. decision boundary with hidden units=4 and learning rate = 2.2 (python)

   with the above hyper parameters the decision boundary is triangular
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model
execfile("./dlfunctions.py")
x1=np.random.uniform(0,10,800).reshape(800,1)
x2=np.random.uniform(0,10,800).reshape(800,1)
x=np.append(x1,x2,axis=1)
x.shape

# create a subset of values where squared is <0,4. perform ravel() to flatten th
is vector
a=(np.power(x[:,0]-5,2) + np.power(x[:,1]-5,2) <= 6).ravel()
y=a.reshape(800,1)

x2=x.t
y2=y.t

parameters,costs = computenn(x2, y2, numhidden = 4, learningrate=2.2, numiterati
ons = 10000)
plot_decision_boundary(lambda x: predict(parameters, x.t), x2, y2,str(4),str(2.2
),"fig7.png")

## cost after iteration 0: 0.692836
## cost after iteration 1000: 0.331052
## cost after iteration 2000: 0.326428
## cost after iteration 3000: 0.474887
## cost after iteration 4000: 0.247989
## cost after iteration 5000: 0.218009
## cost after iteration 6000: 0.201034
## cost after iteration 7000: 0.197030
## cost after iteration 8000: 0.193507
## cost after iteration 9000: 0.191949

13b. decision boundary with hidden units=12 and learning rate = 2.2 (python)

   with the above hyper parameters the decision boundary is triangular
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model
execfile("./dlfunctions.py")
x1=np.random.uniform(0,10,800).reshape(800,1)
x2=np.random.uniform(0,10,800).reshape(800,1)
x=np.append(x1,x2,axis=1)
x.shape

# create a subset of values where squared is <0,4. perform ravel() to flatten th
is vector
a=(np.power(x[:,0]-5,2) + np.power(x[:,1]-5,2) <= 6).ravel()
y=a.reshape(800,1)

x2=x.t
y2=y.t

parameters,costs = computenn(x2, y2, numhidden = 12, learningrate=2.2, numiterat
ions = 10000)
plot_decision_boundary(lambda x: predict(parameters, x.t), x2, y2,str(12),str(2.
2),"fig8.png")

## cost after iteration 0: 0.693291
## cost after iteration 1000: 0.383318
## cost after iteration 2000: 0.298807
## cost after iteration 3000: 0.251735
## cost after iteration 4000: 0.177843
## cost after iteration 5000: 0.130414
## cost after iteration 6000: 0.152400
## cost after iteration 7000: 0.065359
## cost after iteration 8000: 0.050921
## cost after iteration 9000: 0.039719

14a. decision boundary with hidden units=9 and learning rate = 0.5 (r)

   when the number of hidden units is 6 and the learning rate is 0,1, is
   also a triangular shape in r
source("dlfunctions2_1.r")
z <- as.matrix(read.csv("data1.csv",header=false)) # n
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)
nn <-computenn(x1, y1, 9, learningrate=0.5,numiterations=10000) # triangular
## [1] 0.8398838
## [1] 0.3303621
## [1] 0.3127731
## [1] 0.3012791
## [1] 0.3305543
## [1] 0.3303964
## [1] 0.2334615
## [1] 0.1920771
## [1] 0.2341225
## [1] 0.2188118
## [1] 0.2082687
plotdecisionboundary(z,nn,6,0.1)

   [fig81.png?w=1100]

14b. decision boundary with hidden units=8 and learning rate = 0.1 (r)

source("dlfunctions2_1.r")
z <- as.matrix(read.csv("data1.csv",header=false)) # n
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)
nn <-computenn(x1, y1, 8, learningrate=0.1,numiterations=10000) # hemisphere
## [1] 0.7273279
## [1] 0.3169335
## [1] 0.2378464
## [1] 0.1688635
## [1] 0.1368466
## [1] 0.120664
## [1] 0.111211
## [1] 0.1043362
## [1] 0.09800573
## [1] 0.09126161
## [1] 0.0840379
plotdecisionboundary(z,nn,8,0.1)

15a. decision boundary with hidden units=12 and learning rate = 1.5 (octave)

   source("dl-function2.m")
   data=csvread("data1.csv");
   x=data(:,1:2);
   y=data(:,3);
   # make sure that the model parameters are correct. take the transpose
   of x & y
   [w1,b1,w2,b2,costs]= computenn(x', y',12, learningrate=1.5,
   numiterations = 10000);
   plotdecisionboundary(data, w1,b1,w2,b2)
   print -djpg fige.jpg

   conclusion: this post implemented a 3 layer neural network to create
   non-linear boundaries while performing classification. clearly the
   neural network performs very well when the number of hidden units and
   learning rate are varied.

   to be continued   
   watch this space!!

   references
   1. [49]deep learning specialization
   2. [50]neural networks for machine learning
   3. [51]deep learning, ian goodfellow, yoshua bengio and aaron courville
   4. [52]neural networks: the mechanics of id26
   5. [53]machine learning

   also see
   1. [54]my book    practical machine learning with r and python    on amazon
   2. [55]googlyplus: yorkr analyzes ipl players, teams, matches with
   plots and tables
   3. [56]the 3rd paperback & kindle editions of my books on cricket, now
   on amazon
   4. [57]exploring quantum gate operations with qcsimulator
   5. [58]simulating a web joint in android
   6. [59]my travels through the realms of data science, machine learning,
   deep learning and (ai)
   7. [60]presentation on wireless technologies     part 1

   to see all posts check [61]index of posts

rate this:

share:

     *
     *
     * [62]pocket
     * [63]tweet
     *

       iframe:
       [64]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fgigadom.in%2f2018%2f01%2f11%2fdeep-le
       arning-from-first-principles-in-python-r-and-octave-part-2%2f&title
       =deep%20learning%20from%20first%20principles%20in%20python%2c%20r%2
       0and%20octave%20-%20part%202

     * [65][pinit_fg_en_rect_gray_20.png]
     * [66]more
     *

     * [67]email
     * [68]share on tumblr
     *
     * [69]telegram
     * [70]print
     *
     *

like this:

   like loading...

related

     * tagged
     * [71]backward propagation
     * [72]deep learning
     * [73]id119
     * [74]loss
     * [75]neural network
     * [76]octave
     * [77]python
     * [78]r
     * [79]r language
     * [80]r project

published by tinniam v ganesh

   visionary, thought leader and pioneer with 27+ years of experience in
   the software industry. [81]view all posts by tinniam v ganesh
   published january 11, 2018february 7, 2019

post navigation

   [82]previous post deep learning from first principles in python, r and
   octave     part 1
   [83]next post deep learning from first principles in python, r and
   octave     part 3

16 thoughts on    deep learning from first principles in python, r and octave    
part 2   

    1. pingback: [84]deep learning from first principles in python, r and
       octave     part 2     mubashir qasim
    2. pingback: [85]distilled news | data analytics & r
    3.
   larry reed says:
       [86]january 15, 2018 at 4:04 am
       having trouble with this:
       #from dlfunctions import plot_decision_boundary
       execfile(   ./dlfunctions.py   ) # since import does not work in rmd!!!
       [87]likelike
       [88]reply
         1.
        [89]tinniam v ganesh says:
            [90]january 15, 2018 at 4:15 am
            larry     just point the path to which whichever directory
            /dlfunctions.py is saved. this will just run a shell command
            that will source in all the functions from dlfunctions into
            your environment. r markdown does not seem to allow to import
            python functions. if you are creating a regular .py script or
            .ipynb notebook you can directly use the    from dlfunctions
            import    .   
            the above statement is only if you are trying to execute a
            python chunk in r markdown     ganesh
            [91]likelike
            [92]reply
    4. pingback: [93]deep learning from first principles in python, r and
       octave     part 3 | giga thoughts    
    5. pingback: [94]deep learning from first principles in python, r and
       octave     part 3     cloud data architect
    6. pingback: [95]deep learning from first principles in python, r and
       octave     part 4 | giga thoughts    
    7. pingback: [96]deep learning from first principles in python, r and
       octave     part 4 - biva
    8. pingback: [97]presentation on    machine learning in plain english    
       part 1    | giga thoughts    
    9. pingback: [98]presentation on    machine learning in plain english    
       part 3 | giga thoughts    
   10. pingback: [99]deep learning from first principles in python, r and
       octave     part 5 | giga thoughts    
   11. pingback: [100]deep learning from first principles in python, r and
       octave     part 5     cloud data architect
   12. pingback: [101]deep learning from first principles in python, r and
       octave     part 6 | giga thoughts    
   13. pingback: [102]deep learning from first principles in python, r and
       octave     part 8 | giga thoughts    
   14. pingback: [103]my presentations on    elements of neural networks &
       deep learning    -part1,2,3 | giga thoughts    
   15. pingback: [104]take 4+: presentations on    elements of neural
       networks and deep learning        parts 1-8 | giga thoughts    

leave a reply [105]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [106]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [107]log out /
   [108]change )
   google photo

   you are commenting using your google account. ( [109]log out /
   [110]change )
   twitter picture

   you are commenting using your twitter account. ( [111]log out /
   [112]change )
   facebook photo

   you are commenting using your facebook account. ( [113]log out /
   [114]change )
   [115]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

connect with me:

     * [116]linkedin
     * [117]github
     * [118]twitter

   search for: ____________________ search

blog stats

     * 440,452 hits

visitors to giga thoughts (click to see details)

   [119]map

   [120]follow giga thoughts     on wordpress.com

popular posts

     * [121]working with node.js and postgresql
     * [122]simplifying ml: impact of degree of polynomial degree on bias
       & variance and other insights
     * [123]introducing cricketr! : an r package to analyze performances
       of cricketers
     * [124]re-introducing cricketr! : an r package to analyze
       performances of cricketers
     * [125]experiments with deblurring using opencv
     * [126]deep learning from first principles in python, r and octave -
       part 1
     * [127]my presentations on    elements of neural networks & deep
       learning    -parts 4,5
     * [128]practical machine learning with r and python     part 5
     * [129]introducing cricpy:a python package to analyze performances of
       cricketers
     * [130]r vs python: different similarities and similar differences

category cloud

   [131]analytics [132]android [133]android app [134]app [135]batsman
   [136]big data [137]bluemix [138]bowler [139]cloud computing
   [140]cricket [141]cricketr [142]cricsheet [143]data mining [144]deep
   learning [145]distributed systems [146]git [147]github [148]gradient
   descent [149]id75 [150]id28 [151]machine
   learning [152]neural networks [153]python [154]r [155]r language [156]r
   markdown [157]r package [158]r project [159]technology [160]yorkr

follow blog via email

   join 1,212 other followers

   ____________________

   (button) follow

subscribe

   [161]rss feed  [162]rss - posts

giga thoughts community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

archives

   archives [select month_______]

navigate

     * [163]home
     * [164]index of posts
     * [165]books i authored
     * [166]who am i?
     * [167]published posts
     * [168]about giga thoughts   

latest posts

     * [169]analyzing performances of cricketers using cricketr template
       march 30, 2019
     * [170]the clash of the titans in test and odi cricket march 15, 2019
     * [171]analyzing t20 matches with yorkpy templates march 10, 2019
     * [172]yorkpy takes a hat-trick, bowls out intl. t20s, bbl and
       natwest t20!!! march 3, 2019
     * [173]pitching yorkpy     in the block hole     part 4 february 26, 2019
     * [174]take 4+: presentations on    elements of neural networks and
       deep learning        parts 1-8 february 16, 2019
     * [175]pitching yorkpy   swinging away from the leg stump to ipl    
       part 3 february 3, 2019
     * [176]pitching yorkpy   on the middle and outside off-stump to ipl    
       part 2 january 27, 2019

   [177]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [178]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [179]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [180]likes-master

   %d bloggers like this:

references

   visible links
   1. https://gigadom.in/feed/
   2. https://gigadom.in/comments/feed/
   3. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/feed/
   4. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
   5. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/&for=wpcom-auto-discovery
   8. https://gigadom.in/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#content
  11. https://gigadom.in/
  12. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
  13. https://github.com/tvganesh
  14. https://twitter.com/tvganesh_85
  15. https://gigadom.in/
  16. https://gigadom.in/aa-2/
  17. https://gigadom.in/and-you-are/
  18. https://gigadom.in/who-am-i/
  19. https://gigadom.in/published-posts/
  20. https://gigadom.in/about-giga-thoughts/
  21. https://gigadom.in/author/gigadom/
  22. https://gigadom.in/category/id26/
  23. https://gigadom.in/category/backward-propagation/
  24. https://gigadom.in/category/chain-rule/
  25. https://gigadom.in/category/contours/
  26. https://gigadom.in/category/deep-learning/
  27. https://gigadom.in/category/github/
  28. https://gigadom.in/category/logistic-regression/
  29. https://gigadom.in/category/neural-networks/
  30. https://gigadom.in/category/octave/
  31. https://gigadom.in/category/python-2/
  32. https://gigadom.in/category/r/
  33. https://gigadom.in/category/r-language/
  34. https://gigadom.in/category/r-markdown/
  35. https://gigadom.in/category/r-package/
  36. https://gigadom.in/category/r-project/
  37. https://gigadom.in/category/regression/
  38. https://gigadom.in/category/sigmoid/
  39. https://gigadom.in/category/technology/
  40. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  41. https://gigadom.wordpress.com/2017/10/29/practical-machine-learning-with-r-and-python-part-4/
  42. https://www.amazon.com/dp/1791596177
  43. https://www.amazon.com/dp/b07lbg542l
  44. https://www.amazon.com/dp/1983035661
  45. https://www.amazon.com/dp/b07dfkscwz
  46. https://www.youtube.com/watch?v=jpcyv2yhfwm&t=19s
  47. https://github.com/tvganesh/deeplearning-part2
  48. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  49. https://www.coursera.org/specializations/deep-learning
  50. https://www.coursera.org/learn/neural-networks
  51. http://www.deeplearningbook.org/
  52. https://gigadom.wordpress.com/2017/01/21/neural-networks-the-mechanics-of-id26/
  53. https://www.coursera.org/learn/machine-learning
  54. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  55. https://gigadom.wordpress.com/2017/01/05/googlyplus-yorkr-analyzes-ipl-players-teams-matches-with-plots-and-tables/
  56. https://gigadom.wordpress.com/2017/12/16/the-3rd-paperback-edition-of-my-books-on-cricket-on-amazon/
  57. https://gigadom.wordpress.com/2016/06/05/exploring-quantum-gate-operations-with-qcsimulator/
  58. https://gigadom.wordpress.com/2013/06/11/simulating-a-web-joint-in-android/
  59. https://gigadom.wordpress.com/2017/09/11/my-travels-through-the-realms-of-data-science-machine-learning-deep-learning-and-ai/
  60. https://gigadom.wordpress.com/2013/07/24/presentation-on-wireless-technologies-part-1/
  61. https://gigadom.wordpress.com/aa-2/
  62. https://getpocket.com/save
  63. https://twitter.com/share
  64. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/&title=deep learning from first principles in python, r and octave - part 2
  65. https://www.pinterest.com/pin/create/button/?url=https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/&media=https://gigadom.files.wordpress.com/2018/01/fige.jpg&description=deep learning from first principles in python, r and octave - part 2
  66. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  67. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?share=email
  68. https://www.tumblr.com/share
  69. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?share=telegram
  70. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#print
  71. https://gigadom.in/tag/backward-propagation/
  72. https://gigadom.in/tag/deep-learning/
  73. https://gigadom.in/tag/gradient-descent/
  74. https://gigadom.in/tag/loss/
  75. https://gigadom.in/tag/neural-network/
  76. https://gigadom.in/tag/octave/
  77. https://gigadom.in/tag/python/
  78. https://gigadom.in/tag/r/
  79. https://gigadom.in/tag/r-language-2/
  80. https://gigadom.in/tag/r-project/
  81. https://gigadom.in/author/gigadom/
  82. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  83. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  84. http://mqasim.me/?p=151412
  85. http://advanceddataanalytics.net/2018/01/13/distilled-news-678/
  86. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-5202
  87. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?like_comment=5202&_wpnonce=41e6f8e256
  88. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?replytocom=5202#respond
  89. https://gigadom.wordpress.com/
  90. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-5203
  91. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?like_comment=5203&_wpnonce=40af42e4a5
  92. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/?replytocom=5203#respond
  93. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  94. http://www.dataarchitect.cloud/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  95. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  96. http://www.biva-ags.com/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  97. https://gigadom.wordpress.com/2018/03/06/presentation-on-machine-learning-in-plain-english-part-1/
  98. https://gigadom.wordpress.com/2018/03/09/presentation-on-machine-learning-in-plain-english-part-3/
  99. https://gigadom.wordpress.com/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 100. http://www.dataarchitect.cloud/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 101. https://gigadom.wordpress.com/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
 102. https://gigadom.wordpress.com/2018/05/06/deep-learning-from-first-principles-in-python-r-and-octave-part-8/
 103. https://gigadom.in/2019/01/10/my-presentations-on-elements-of-neural-networks-deep-learning-part123/
 104. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 105. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#respond
 106. https://gravatar.com/site/signup/
 107. javascript:highlandercomments.doexternallogout( 'wordpress' );
 108. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 109. javascript:highlandercomments.doexternallogout( 'googleplus' );
 110. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 111. javascript:highlandercomments.doexternallogout( 'twitter' );
 112. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 113. javascript:highlandercomments.doexternallogout( 'facebook' );
 114. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 115. javascript:highlandercomments.cancelexternalwindow();
 116. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
 117. https://github.com/tvganesh
 118. https://twitter.com/tvganesh_85
 119. https://www.revolvermaps.com/?target=enlarge&i=0z8r51l0ucz
 120. https://gigadom.in/
 121. https://gigadom.in/2014/07/20/working-with-node-js-and-postgresql/
 122. https://gigadom.in/2014/01/04/simplifying-ml-impact-of-degree-of-polynomial-degree-on-bias-variance-and-other-insights/
 123. https://gigadom.in/2015/07/04/introducing-cricketr-a-r-package-to-analyze-performances-of-cricketers/
 124. https://gigadom.in/2016/05/14/re-introducing-cricketr-an-r-package-to-analyze-performances-of-cricketers/
 125. https://gigadom.in/2011/11/09/experiments-with-deblurring-using-opencv/
 126. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 127. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 128. https://gigadom.in/2017/11/07/practical-machine-learning-with-r-and-python-part-5/
 129. https://gigadom.in/2018/10/28/introducing-cricpya-python-package-to-analyze-performances-of-cricketrs/
 130. https://gigadom.in/2017/05/22/r-vs-python-different-similarities-and-similar-differences/
 131. https://gigadom.in/category/analytics/
 132. https://gigadom.in/category/android/
 133. https://gigadom.in/category/android-app/
 134. https://gigadom.in/category/app/
 135. https://gigadom.in/category/batsman/
 136. https://gigadom.in/category/big-data/
 137. https://gigadom.in/category/bluemix/
 138. https://gigadom.in/category/bowler/
 139. https://gigadom.in/category/cloud-computing/
 140. https://gigadom.in/category/cricket/
 141. https://gigadom.in/category/cricketr/
 142. https://gigadom.in/category/cricsheet/
 143. https://gigadom.in/category/data-mining/
 144. https://gigadom.in/category/deep-learning/
 145. https://gigadom.in/category/distributed-systems/
 146. https://gigadom.in/category/git/
 147. https://gigadom.in/category/github/
 148. https://gigadom.in/category/gradient-descent/
 149. https://gigadom.in/category/linear-regression/
 150. https://gigadom.in/category/logistic-regression/
 151. https://gigadom.in/category/machine-learning/
 152. https://gigadom.in/category/neural-networks/
 153. https://gigadom.in/category/python-2/
 154. https://gigadom.in/category/r/
 155. https://gigadom.in/category/r-language/
 156. https://gigadom.in/category/r-markdown/
 157. https://gigadom.in/category/r-package/
 158. https://gigadom.in/category/r-project/
 159. https://gigadom.in/category/technology/
 160. https://gigadom.in/category/yorkr/
 161. https://gigadom.in/feed/
 162. https://gigadom.in/feed/
 163. https://gigadom.in/
 164. https://gigadom.in/aa-2/
 165. https://gigadom.in/and-you-are/
 166. https://gigadom.in/who-am-i/
 167. https://gigadom.in/published-posts/
 168. https://gigadom.in/about-giga-thoughts/
 169. https://gigadom.in/2019/03/30/analyzing-performances-of-cricketers-using-cricketr-template/
 170. https://gigadom.in/2019/03/15/the-clash-of-the-titans-in-test-and-odi-cricket/
 171. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
 172. https://gigadom.in/2019/03/03/yorkpy-takes-a-hat-trick-bowls-out-intl-t20s-bbl-and-natwest-t20/
 173. https://gigadom.in/2019/02/26/pitching-yorkpy-in-the-block-hole-part-4/
 174. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 175. https://gigadom.in/2019/02/03/pitching-yorkpyswinging-away-from-the-leg-stump-to-ipl-part-3/
 176. https://gigadom.in/2019/01/27/pitching-yorkpyon-the-middle-and-outside-off-stump-to-ipl-part-2/
 177. https://wordpress.com/?ref=footer_blog
 178. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 179. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#cancel
 180. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 182. https://gigadom.in/
 183. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-form-guest
 184. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-form-load-service:wordpress.com
 185. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-form-load-service:twitter
 186. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/#comment-form-load-service:facebook
 187. http://lemanshots.wordpress.com/
 188. https://vinodsblog.com/about
 189. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 190. http://friartuck2012.wordpress.com/
 191. http://webastion.wordpress.com/
 192. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 193. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 194. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 195. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
 196. http://micvdotin.wordpress.com/
