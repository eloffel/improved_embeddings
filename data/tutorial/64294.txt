   #[1]giga thoughts ...    feed [2]giga thoughts ...    comments feed
   [3]giga thoughts ...    deep learning from first principles in python, r
   and octave     part 3 comments feed [4]deep learning from first
   principles in python, r and octave     part 2 [5]deep learning from first
   principles in python, r and octave     part 4 [6]alternate [7]alternate
   [8]giga thoughts ... [9]wordpress.com

   [10]skip to content

   [11]giga thoughts    

   insights into technology

     * [12]linkedin
     * [13]github
     * [14]twitter

   (button) menu

     * [15]home
     * [16]index of posts
     * [17]books i authored
     * [18]who am i?
     * [19]published posts
     * [20]about giga thoughts   

deep learning from first principles in python, r and octave     part 3

   [21]tinniam v ganesh [22]artificial intelligence, [23]id26,
   [24]backward propagation, [25]chain rule, [26]contours, [27]deep
   learning, [28]dplyr, [29]git, [30]github, [31]neural networks,
   [32]octave, [33]python, [34]r, [35]r language, [36]r markdown, [37]r
   package, [38]r project, [39]technology january 30, 2018january 16, 2019

      once upon a time, i, chuang tzu, dreamt i was a butterfly, fluttering
   hither and thither, to all intents and purposes a butterfly. i was
   conscious only of following my fancies as a butterfly, and was
   unconscious of my individuality as a man. suddenly, i awoke, and there
   i lay, myself again. now i do not know whether i was then a man
   dreaming i was a butterfly, or whether i am now a butterfly dreaming
   that i am a man.   
   from the brain: the story of you     david eagleman

      thought is a great big vector of neural activity   
   prof geoffrey hinton

introduction

   this is the third part in my series on deep learning from first
   principles in python, r and octave. in the first part [40]deep learning
   from first principles in python, r and octave-part 1, i implemented
   id28 as a 2 layer neural network. the 2nd part [41]deep
   learning from first principles in python, r and octave-part 2, dealt
   with the implementation of 3 layer neural networks with 1 hidden layer
   to perform classification tasks, where the 2 classes cannot be
   separated by a linear boundary. in this third part, i implement a
   multi-layer, deep learning (dl) network of arbitrary depth (any number
   of hidden layers) and arbitrary height (any number of activation units
   in each hidden layer). the implementations of these deep learning
   networks, in all the 3 parts, are based on vectorized versions in
   python, r and octave. the implementation in the 3rd part is for a
   l-layer deep netwwork, but without any id173, early stopping,
   momentum or learning rate adaptation techniques. however even the
   barebones multi-layer dl, is a handful and has enough hyperparameters
   to fine-tune and adjust.

   checkout my book    deep learning from first principles: second edition    
   in vectorized python, r and octave   . my book starts with the
   implementation of a simple 2-layer neural network and works its way to
   a generic l-layer deep learning network, with all the bells and
   whistles. the derivations have been discussed in detail. the code has
   been extensively commented and included in its entirety in the appendix
   sections. my book is available on amazon as [42]paperback ($18.99) and
   in [43]kindle version($9.99/rs449).

   the implementation of the vectorized l-layer deep learning network in
   python, r and octave were both exhausting, and exacting!! keeping track
   of the indices, layer number and matrix dimensions required quite bit
   of focus. while the implementation was demanding, it was also very
   exciting to get the code to work. the trick was to be able to shift
   gears between the slight quirkiness between the languages. here are
   some of challenges i faced.

   1. python and octave allow multiple return values to be unpacked in a
   single statement. with r, unpacking multiple return values from a list,
   requires the list returned, to be unpacked separately. i did see that
   there is a package [44]gsubfn, which does this.  i hope this feature
   becomes a base r feature.
   2. python and r allow dissimilar elements to be saved and returned from
   functions using dictionaries or lists respectively. however there is no
   real equivalent in octave. the closest i got to this functionality in
   octave, was the    cell array   . but the cell array can be accessed only
   by the index, and not with the key as in a python dictionary or r list.
   this makes things just a bit more difficult in octave.
   3. python and octave include implicit broadcasting. in r, broadcasting
   is not implicit, but r has a nifty function, the sweep(), with which we
   can broadcast either by columns or by rows
   4. the closest equivalent of python   s dictionary, or r   s list, in
   octave is the cell array. however i had to manage separate cell arrays
   for weights and biases and during id119 and separate
   gradients dw and db
   5. in python the rank-1 numpy arrays can be annoying at times. this
   issue is not present in r and octave.

   though the number of lines of code for deep learning functions in
   python, r and octave are about ~350 apiece, they have been some of the
   most difficult code i have implemented. the current vectorized
   implementation supports the relu, sigmoid and tanh id180
   as of now. i will be adding other id180 like the    leaky
   relu   ,    softmax    and others, to the implementation in the weeks to
   come.

   while testing with different hyper-parameters namely i) the number of
   hidden layers, ii) the number of activation units in each layer, iii)
   the activation function and iv) the number iterations, i found the
   l-layer deep learning network to be very sensitive to these
   hyper-parameters. it is not easy to tune the parameters. adding more
   hidden layers, or more units per layer, does not help and mostly
   results in id119 getting stuck in some local minima. it does
   take a fair amount of trial and error and very close observation on how
   the dl network performs for logical changes. we then can zero in on the
   most the optimal solution. feel free to download/fork my code from
   github [45]deeplearning-part 3 and play around with the
   hyper-parameters for your own problems.

derivation of a multi layer deep learning network

   note: a detailed discussion of the derivation below is available in my
   video presentation [46]neural network 4
   lets take a simple 3 layer neural network with 3 hidden layers and an
   output layer
   in the forward propagation cycle the equations are

   z_{1} = w_{1}a_{0} +b_{1}  and   a_{1} = g(z_{1})
   z_{2} = w_{2}a_{1} +b_{2}  and   a_{2} = g(z_{2})
   z_{3} = w_{3}a_{2} +b_{3}  and a_{3} = g(z_{3})

   the id168 is given by
   l = -(yloga3 + (1-y)log(1-a3))
   and dl/da3 = -(y/a_{3} + (1-y)/(1-a_{3}))

   for a binary classification the output activation function is the
   sigmoid function given by
   a_{3} = 1/(1+ e^{-z3}) . it can be shown that
   da_{3}/dz_{3} = a_{3}(1-a_3) see equation 2 in [47]part 1

   \partial l/\partial z_{3} = \partial l/\partial a_{3}* \partial
   a_{3}/\partial z_{3} = a3-y see equation (f) in  [48]part 1
   and since
   \partial l/\partial a_{2} = \partial l/\partial z_{3} * \partial
   z_{3}/\partial a_{2} = (a_{3} -y) * w_{3} because \partial
   z_{3}/\partial a_{2} = w_{3} -(1a)
   and \partial l/\partial z_{2} =\partial l/\partial a_{2} * \partial
   a_{2}/\partial z_{2} = (a_{3} -y) * w_{3} *g'(z_{2}) -(1b)
   \partial l/\partial w_{2} = \partial l/\partial z_{2} * a_{1} -(1c)
   since \partial z_{2}/\partial w_{2} = a_{1}
   and
   \partial l/\partial b_{2} = \partial l/\partial z_{2} -(1d)
   because
   \partial z_{2}/\partial b_{2} =1

   also

   \partial l/\partial a_{1} =\partial l/\partial z_{2} * \partial
   z_{2}/\partial a_{1} = \partial l/\partial z_{2} * w_{2}          (2a)
   \partial l/\partial z_{1} =\partial l/\partial a_{1} * \partial
   a_{1}/\partial z_{1} = \partial l/\partial a_{1} * w_{2} *g'(z_{1})
             (2b)
   \partial l/\partial w_{1} = \partial l/\partial z_{1} * a_{0}     (2c)
   \partial l/\partial b_{1} = \partial l/\partial z_{1}     (2d)

   inspecting the above equations (1a     1d & 2a-2d), our    uber deep,
   bottoid113ss    brain  can easily discern the pattern in these equations.
   the equation for any layer    l    is of the form
   z_{l} = w_{l}a_{l-1} +b_{l}      and  a_{l} = g(z_{l})
   the equation for the backward propagation have the general form
   \partial l/\partial a_{l} = \partial l/\partial z_{l+1} * w^{l+1}
   \partial l/\partial z_{l}=\partial l/\partial a_{l} *g'(z_{l})
   \partial l/\partial w_{l} =\partial l/\partial z_{l} *a^{l-1}
   \partial l/\partial b_{l} =\partial l/\partial z_{l}

   some other important results the derivatives of the activation
   functions in the implemented deep learning network
   g(z) = sigmoid(z) = 1/(1+e^{-z}) = a g   (z) = a(1-a)     see [49]part 1
   g(z) = tanh(z) = a g   (z) = 1 - a^{2}
   g(z) = relu(z) = z  when z>0 and 0 when z 0 and 0 when z <= 0
   while it appears that there is a discontinuity for the derivative at 0
   the small value at the discontinuity does not present a problem

   the implementation of the multi layer vectorized deep learning network
   for python, r and octave is included below. for all these
   implementations, initially i create the size and configuration of the
   the deep learning network with the layer dimennsions so for example
   layersdimension vector    v    of length l indicating    l    layers where

   v (in python)= [v_{0}, v_{1}, v_{2} ,     v_{l-1}]
   v (in r)= c(v_{1}, v_{2}, v_{3} ,     v_{l})
   v (in octave)= [ v_{1} v_{2} v_{3}     v_{l}]

   in all of these implementations the first element is the number of
   input features to the deep learning network and the last element is
   always a    sigmoid    activation function since all the problems deal with
   binary classification.

   the number of elements between the first and the last element are the
   number of hidden layers and the magnitude of each v_{i} is the number
   of activation units in each hidden layer, which is specified while
   actually executing the deep learning network using the function
   l_layer_deepmodel(), in all the implementations python, r and octave

1a. classification with multi layer deep learning network     relu
activation(python)

   in the code below a 4 layer neural network is trained to generate a
   non-linear boundary between the classes. in the code below the    relu   
   activation function is used. the number of activation units in each
   layer is 9. the cost vs iterations is plotted in addition to the
   decision boundary. further the accuracy, precision, recall and f1 score
   are also computed
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import listedcolormap
import sklearn
import sklearn.datasets

#from dlfunctions import plot_decision_boundary
execfile("./dlfunctions34.py") #
os.chdir("c:\\software\\deeplearning-posts\\part3")

# create clusters of 2 classes
x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t
# set the dimensions of dl network
#  below we have
#  2 - 2 input features
#  9,9 - 2 hidden layers with 9 activation units per layer and
#  1 - 1 sigmoid activation unit in the output layer as this is a binary classif
ication
# the activation in the hidden layer is the 'relu' specified in l_layer_deepmode
l

layersdimensions = [2, 9, 9,1] #  4-layer model
parameters = l_layer_deepmodel(x2, y2, layersdimensions,hiddenactivationfunc='re
lu', learning_rate = 0.3,num_iterations = 2500, fig="fig1.png")
#plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.t), x2,y2,str(0.3),"fig2.
png")

# compute the confusion matrix
yhat = predict(parameters,x2)
from sklearn.metrics import confusion_matrix
a=confusion_matrix(y2.t,yhat.t)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
print('accuracy: {:.2f}'.format(accuracy_score(y2.t, yhat.t)))
print('precision: {:.2f}'.format(precision_score(y2.t, yhat.t)))
print('recall: {:.2f}'.format(recall_score(y2.t, yhat.t)))
print('f1: {:.2f}'.format(f1_score(y2.t, yhat.t)))
## accuracy: 0.90
## precision: 0.91
## recall: 0.87
## f1: 0.89

   for more details on metrics like accuracy, recall, precision etc. used
   in classification take a look at my post [50]practical machine learning
   with r and python     part 2. more details about these and other metrics
   besides implementation of the most common machine learning algorithms
   are available in my book [51]my book    practical machine learning with r
   and python    on amazon

1b. classification with multi layer deep learning network     relu
activation(r)

   in the code below, binary classification is performed on the same data
   set as above using the relu activation function. the dl network is same
   as above
library(ggplot2)
# read the data
z <- as.matrix(read.csv("data.csv",header=false))
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)

# set the dimensions of the deep learning network
# no of input features =2, 2 hidden layers with 9 activation units and 1 output
layer
layersdimensions = c(2, 9, 9,1)
# execute the deep learning neural network
retvals = l_layer_deepmodel(x1, y1, layersdimensions,
                               hiddenactivationfunc='relu',
                               learningrate = 0.3,
                               numiterations = 5000,
                               print_cost = true)
library(ggplot2)
source("dlfunctions33.r")
# get the computed costs
costs <- retvals[['costs']]
# create a sequence of iterations
numiterations=5000
iterations <- seq(0,numiterations,by=1000)
df <-data.frame(iterations,costs)
# plot the costs vs number of iterations
ggplot(df,aes(x=iterations,y=costs)) + geom_point() +geom_line(color="blue") +
    xlab('no of iterations') + ylab('cost') + ggtitle("cost vs no of iterations"
)

   [fig21.png?w=1100]
# plot the decision boundary
plotdecisionboundary(z,retvals,hiddenactivationfunc="relu",0.3)

   [fig1-11.png?w=1024&#038;h=732]
library(caret)
# predict the output for the data values
yhat <-predict(retvals$parameters,x1,hiddenactivationfunc="relu")
yhat[yhat==false]=0
yhat[yhat==true]=1
# compute the confusion matrix
confusionmatrix(yhat,y1)
## confusion matrix and statistics
##
##           reference
## prediction   0   1
##          0 201  10
##          1  21 168
##
##                accuracy : 0.9225
##                  95% ci : (0.8918, 0.9467)
##     no information rate : 0.555
##     p-value [acc > nir] : < 2e-16
##
##                   kappa : 0.8441
##  mcnemar's test p-value : 0.07249
##
##             sensitivity : 0.9054
##             specificity : 0.9438
##          pos pred value : 0.9526
##          neg pred value : 0.8889
##              prevalence : 0.5550
##          detection rate : 0.5025
##    detection prevalence : 0.5275
##       balanced accuracy : 0.9246
##
##        'positive' class : 0
##

1c. classification with multi layer deep learning network     relu
activation(octave)

   included below is the code for performing classification. incidentally
   octave does not seem to have implemented the confusion matrix,  but
   confusionmat is available in matlab.
   # read the data
   data=csvread("data.csv");
   x=data(:,1:2);
   y=data(:,3);
   # set layer dimensions
   layersdimensions = [2 9 7 1] #tanh=-0.5(ok), #relu=0.1 best!
   # execute deep network
   [weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
   hiddenactivationfunc='relu',
   learningrate = 0.1,
   numiterations = 10000);
   plotcostvsiterations(10000,costs);
   plotdecisionboundary(data,weights, biases,hiddenactivationfunc="tanh")

2a. classification with multi layer deep learning network     tanh
activation(python)

   below the tanh activation function is used to perform the same
   classification. i found the tanh activation required a simpler neural
   network of 3 layers.
# tanh activation
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors
import sklearn.linear_model

from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from matplotlib.colors import listedcolormap
import sklearn
import sklearn.datasets

#from dlfunctions import plot_decision_boundary
os.chdir("c:\\software\\deeplearning-posts\\part3")
execfile("./dlfunctions34.py")
# create the dataset
x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,
                       cluster_std = 1.3, random_state = 4)
#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t
# set the dimensions of the neural network
layersdimensions = [2, 4, 1] #  3-layer model
# compute the dl network
parameters = l_layer_deepmodel(x2, y2, layersdimensions, hiddenactivationfunc='t
anh', learning_rate = .5,num_iterations = 2500,fig="fig3.png")
#plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.t), x2,y2,str(0.5),"fig4.
png")

2b. classification with multi layer deep learning network     tanh
activation(r)

   r performs better with a tanh activation than the relu as can be seen
   below
 #set the dimensions of the neural network
layersdimensions = c(2, 9, 9,1)
library(ggplot2)
# read the data
z <- as.matrix(read.csv("data.csv",header=false))
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)
# execute the deep model
retvals = l_layer_deepmodel(x1, y1, layersdimensions,
                            hiddenactivationfunc='tanh',
                            learningrate = 0.3,
                            numiterations = 5000,
                            print_cost = true)
# get the costs
costs <- retvals[['costs']]
iterations <- seq(0,numiterations,by=1000)
df <-data.frame(iterations,costs)
# plot cost vs number of iterations
ggplot(df,aes(x=iterations,y=costs)) + geom_point() +geom_line(color="blue") +
    xlab('no of iterations') + ylab('cost') + ggtitle("cost vs no of iterations"
)

   [fig41.png?w=1100]
#plot the decision boundary
plotdecisionboundary(z,retvals,hiddenactivationfunc="tanh",0.3)

   [fig2-11.png?w=1024&#038;h=732]

2c. classification with multi layer deep learning network     tanh
activation(octave)

   the code below uses the   tanh activation in the hidden layers for
   octave
   # read the data
   data=csvread("data.csv");
   x=data(:,1:2);
   y=data(:,3);
   # set layer dimensions
   layersdimensions = [2 9 7 1] #tanh=-0.5(ok), #relu=0.1 best!
   # execute deep network
   [weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
   hiddenactivationfunc='tanh',
   learningrate = 0.1,
   numiterations = 10000);
   plotcostvsiterations(10000,costs);
   plotdecisionboundary(data,weights, biases,hiddenactivationfunc="tanh")

3. bernoulli   s lemniscate

   to make things  more interesting, i create a 2d figure of the
   bernoulli   s lemniscate to perform non-linear classification. the
   lemniscate is given by the equation
   (x^{2} + y^{2})^{2} = 2a^{2}*(x^{2}-y^{2})

3a. classifying a lemniscate with deep learning network     relu
activation(python)

import os
import numpy as np
import matplotlib.pyplot as plt
os.chdir("c:\\software\\deeplearning-posts\\part3")
execfile("./dlfunctions33.py")
x1=np.random.uniform(0,10,2000).reshape(2000,1)
x2=np.random.uniform(0,10,2000).reshape(2000,1)

x=np.append(x1,x2,axis=1)
x.shape

# create a subset of values where squared is <0,4. perform ravel() to flatten th
is vector
# create the equation
# (x^{2} + y^{2})^2 - 2a^2*(x^{2}-y^{2}) <= 0
a=np.power(np.power(x[:,0]-5,2) + np.power(x[:,1]-5,2),2)
b=np.power(x[:,0]-5,2) - np.power(x[:,1]-5,2)
c= a - (b*np.power(4,2)) <=0
y=c.reshape(2000,1)
# create a scatter plot of the lemniscate
plt.scatter(x[:,0], x[:,1], c=y, marker= 'o', s=15,cmap="viridis")
z=np.append(x,y,axis=1)
plt.savefig("fig50.png",bbox_inches='tight')
plt.clf()

# set the data for classification
x2=x.t
y2=y.t
# these settings work the best
# set the deep learning layer dimensions for a relu activation
layersdimensions = [2,7,4,1]
#execute the dl network
parameters = l_layer_deepmodel(x2, y2, layersdimensions, hiddenactivationfunc='r
elu', learning_rate = 0.5,num_iterations = 10000, fig="fig5.png")
#plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.t), x2, y2,str(2.2),"fig6
.png")

# compute the confusion matrix
yhat = predict(parameters,x2)
from sklearn.metrics import confusion_matrix
a=confusion_matrix(y2.t,yhat.t)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
print('accuracy: {:.2f}'.format(accuracy_score(y2.t, yhat.t)))
print('precision: {:.2f}'.format(precision_score(y2.t, yhat.t)))
print('recall: {:.2f}'.format(recall_score(y2.t, yhat.t)))
print('f1: {:.2f}'.format(f1_score(y2.t, yhat.t)))
## accuracy: 0.93
## precision: 0.77
## recall: 0.76
## f1: 0.76

   we could get better performance by tuning further. do play around if
   you fork the code.
   note:: the lemniscate data is saved as a csv and then read in r and
   also in octave. i do this instead of recreating the lemniscate shape

3b. classifying a lemniscate with deep learning network     relu activation(r
code)

   the r decision boundary for the bernoulli   s lemniscate is shown below
z <- as.matrix(read.csv("lemniscate.csv",header=false))
z1=data.frame(z)
# create a scatter plot of the lemniscate
ggplot(z1,aes(x=v1,y=v2,col=v3)) +geom_point()
#set the data for the dl network
x=z[,1:2]
y=z[,3]

x1=t(x)
y1=t(y)

# set the layer dimensions for the tanh activation function
layersdimensions = c(2,5,4,1)
# execute the deep learning network with tanh activation
retvals = l_layer_deepmodel(x1, y1, layersdimensions,
                               hiddenactivationfunc='tanh',
                               learningrate = 0.3,
                               numiterations = 20000, print_cost = true)
# plot cost vs iteration
costs <- retvals[['costs']]
numiterations = 20000
iterations <- seq(0,numiterations,by=1000)
df <-data.frame(iterations,costs)
ggplot(df,aes(x=iterations,y=costs)) + geom_point() +geom_line(color="blue") +
    xlab('no of iterations') + ylab('cost') + ggtitle("cost vs no of iterations"
)

   [fig61.png?w=1100]
#plot the decision boundary
plotdecisionboundary(z,retvals,hiddenactivationfunc="tanh",0.3)

   [fig3-2.png?w=1024&#038;h=732]

3c. classifying a lemniscate with deep learning network     relu
activation(octave code)

   octave is used to generate the non-linear lemniscate boundary.
   # read the data
   data=csvread("lemniscate.csv");
   x=data(:,1:2);
   y=data(:,3);
   # set the dimensions of the layers
   layersdimensions = [2 9 7 1]
   # compute the dl network
   [weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
   hiddenactivationfunc='relu',
   learningrate = 0.20,
   numiterations = 10000);
   plotcostvsiterations(10000,costs);
   plotdecisionboundary(data,weights, biases,hiddenactivationfunc="relu")

4a. binary classification using mnist     python code

   finally i perform a simple classification using the mnist handwritten
   digits, which according to prof geoffrey hinton is    the drosophila of
   deep learning   .
   the python code for reading the mnist data is taken from alex kesling   s
   github link [52]mnist.

   in the python code below, i perform a simple binary classification
   between the handwritten digit    5    and    not 5    which is all other
   digits. i will perform the proper classification of all digits using
   the  softmax classifier some time later.
import os
import numpy as np
import matplotlib.pyplot as plt
os.chdir("c:\\software\\deeplearning-posts\\part3")
execfile("./dlfunctions34.py")
execfile("./load_mnist.py")
training=list(read(dataset='training',path="./mnist"))
test=list(read(dataset='testing',path="./mnist"))
lbls=[]
pxls=[]
print(len(training))

# select the first 10000 training data and the labels
for i in range(10000):
       l,p=training[i]
       lbls.append(l)
       pxls.append(p)
labels= np.array(lbls)
pixels=np.array(pxls)

#  sey y=1  when labels == 5 and 0 otherwise
y=(labels==5).reshape(-1,1)
x=pixels.reshape(pixels.shape[0],-1)

# create the necessary feature and target variable
x1=x.t
y1=y.t

# create the layer dimensions. the number of features are 28 x 28 = 784 since th
e 28 x 28
# pixels is flattened to single vector of length 784.
layersdimensions=[784, 15,9,7,1] # works very well
parameters = l_layer_deepmodel(x1, y1, layersdimensions, hiddenactivationfunc='r
elu', learning_rate = 0.1,num_iterations = 1000, fig="fig7.png")

# test data
lbls1=[]
pxls1=[]
for i in range(800):
       l,p=test[i]
       lbls1.append(l)
       pxls1.append(p)

testlabels=np.array(lbls1)
testdata=np.array(pxls1)

ytest=(testlabels==5).reshape(-1,1)
xtest=testdata.reshape(testdata.shape[0],-1)
xtest1=xtest.t
ytest1=ytest.t

yhat = predict(parameters,xtest1)
from sklearn.metrics import confusion_matrix
a=confusion_matrix(ytest1.t,yhat.t)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
print('accuracy: {:.2f}'.format(accuracy_score(ytest1.t, yhat.t)))
print('precision: {:.2f}'.format(precision_score(ytest1.t, yhat.t)))
print('recall: {:.2f}'.format(recall_score(ytest1.t, yhat.t)))
print('f1: {:.2f}'.format(f1_score(ytest1.t, yhat.t)))

probs=predict_proba(parameters,xtest1)
from sklearn.metrics import precision_recall_curve

precision, recall, thresholds = precision_recall_curve(ytest1.t, probs.t)
closest_zero = np.argmin(np.abs(thresholds))
closest_zero_p = precision[closest_zero]
closest_zero_r = recall[closest_zero]
plt.xlim([0.0, 1.01])
plt.ylim([0.0, 1.01])
plt.plot(precision, recall, label='precision-recall curve')
plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none
', c='r', mew=3)
plt.xlabel('precision', fontsize=16)
plt.ylabel('recall', fontsize=16)
plt.savefig("fig8.png",bbox_inches='tight')

## accuracy: 0.99
## precision: 0.96
## recall: 0.89
## f1: 0.92

   in addition to plotting the cost vs iterations, i also plot the
   precision-recall curve to show how the precision and recall, which are
   complementary to each other vary with respect to the other. to know
   more about precision-recall, please check my post [53]practical machine
   learning with r and python     part 4.

   check out my compact and minimal book     practical machine learning with
   r and python:second edition- machine learning in stereo     available in
   amazon in [54]paperback($10.99) and [55]kindle($7.99) versions. my book
   includes implementations of key ml algorithms and associated measures
   and metrics. the book is ideal for anybody who is familiar with the
   concepts and would like a quick reference to the different ml
   algorithms that can be applied to problems and how to select the best
   model. pick your copy today!!

   a physical copy of the book is much better than scrolling down a
   webpage. personally, i tend to use my own book quite frequently to
   refer to r, python constructs,  subsetting, machine learning function
   calls and the necessary parameters etc. it is useless to commit any of
   this to memory, and a physical copy of a book is much easier to thumb
   through for the relevant code snippet. pick up your copy today!

4b. binary classification using mnist     r code

   in the r code below the same binary classification of the digit    5    and
   the    not 5    is performed. the code to read and display the mnist data
   is taken from brendan o    connor   s github link at [56]mnist
source("mnist.r")
load_mnist()
#show_digit(train$x[2,]
layersdimensions=c(784, 7,7,3,1) # works at 1500
x <- t(train$x)
# choose only 5000 training data
x2 <- x[,1:5000]
y <-train$y
# set labels for all digits that are 'not 5' to 0
y[y!=5] <- 0
# set labels of digit 5 as 1
y[y==5] <- 1
# set the data
y1 <- as.matrix(y)
y2 <- t(y1)
# choose the 1st 5000 data
y3 <- y2[,1:5000]

#execute the deep learning model
retvals = l_layer_deepmodel(x2, y3, layersdimensions,
                               hiddenactivationfunc='tanh',
                               learningrate = 0.3,
                               numiterations = 3000, print_cost = true)
# plot cost vs iteration
costs <- retvals[['costs']]
numiterations = 3000
iterations <- seq(0,numiterations,by=1000)
df <-data.frame(iterations,costs)
ggplot(df,aes(x=iterations,y=costs)) + geom_point() +geom_line(color="blue") +
    xlab('no of iterations') + ylab('cost') + ggtitle("cost vs no of iterations"
)

   [untitled5.png?w=676&amp;h=281]
# compute id203 scores
scores <- computescores(retvals$parameters, x2,hiddenactivationfunc='relu')
a=y3==1
b=y3==0

# compute probabilities of class 0 and class 1
class1=scores[a]
class0=scores[b]

# plot roc curve
pr <-pr.curve(scores.class0=class1,
        scores.class1=class0,
       curve=t)

plot(pr)

   the auc curve hugs the top left corner and hence the performance of the
   classifier is quite good.

4c. binary classification using mnist     octave code

   this code to load mnist data was taken from [57]daniel e blog.
   precision recall curves are available in matlab but are yet to be
   implemented in octave   s statistics package.
   load('./mnist/mnist.txt.gz'); % load the dataset
   # subset the 'not 5' digits
   a=(trainy != 5);
   # subset '5'
   b=(trainy == 5);
   #make a copy of trainy
   #set 'not 5' as 0 and '5' as 1
   y=trainy;
   y(a)=0;
   y(b)=1;
   x=trainx(1:5000,:);
   y=y(1:5000);
   # set the dimensions of layer
   layersdimensions=[784, 7,7,3,1];
   # compute the dl network
   [weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
   hiddenactivationfunc='relu',
   learningrate = 0.1,
   numiterations = 5000);

conclusion

   it was quite a challenge coding a deep learning network in python, r
   and octave. the deep learning network implementation, in this post,is
   the base deep learning network, without any of the id173
   methods included. here are some key learning that i got while playing
   with different multi-layer networks on different problems

   a. deep learning networks come with many levers, the hyper-parameters,
       learning rate
       activation unit
       number of hidden layers
       number of units per hidden layer
       number of iterations while performing id119
   b. deep networks are very sensitive. a change in any of the
   hyper-parameter makes it perform very differently
   c. initially i thought adding more hidden layers, or more units per
   hidden layer will make the dl network better at learning. on the
   contrary, there is a performance degradation after the optimal dl
   configuration
   d. at a sub-optimal number of hidden layers or number of hidden units,
   id119 seems to get stuck at a local minima
   e. there were occasions when the cost came down, only to increase
   slowly as the number of iterations were increased. probably early
   stopping would have helped.
   f. i also did come across situations of    exploding/vanishing gradient   ,
   cost went to inf/-inf. here i would think inclusion of    momentum
   method    would have helped

   i intend to add the additional hyper-parameters of l1, l2
   id173, momentum method, early stopping etc. into the code in
   my future posts.
   feel free to fork/clone the code from github [58]deep learning     part
   3, and take the dl network apart and play around with it.

   i will be continuing this series with more hyper-parameters to handle
   vanishing and exploding gradients, early stopping and id173 in
   the weeks to come. i also intend to add some more id180
   to this basic multi-layer network.
   hang around, there are more exciting things to come.

   watch this space!

   references
   1. [59]deep learning specialization
   2. [60]neural networks for machine learning
   3. [61]deep learning, ian goodfellow, yoshua bengio and aaron courville
   4. [62]neural networks: the mechanics of id26
   5. [63]machine learning

   also see
   1.[64]my book    practical machine learning with r and python    on amazon
   2. [65]my travels through the realms of data science, machine learning,
   deep learning and (ai)
   3. [66]designing a social web portal
   4. [67]googlyplus: yorkr analyzes ipl players, teams, matches with
   plots and tables
   4. [68]introducing qcsimulator: a 5-qubit quantum computing simulator
   in r
   6. [69]presentation on    intelligent networks, camel protocol, services
   & applications
   7. [70]design principles of scalable, distributed systems

   to see all posts see [71]index of posts

rate this:

share:

     *
     *
     * [72]pocket
     * [73]tweet
     *

       iframe:
       [74]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fgigadom.in%2f2018%2f01%2f30%2fdeep-le
       arning-from-first-principles-in-python-r-and-octave-part-3%2f&title
       =deep%20learning%20from%20first%20principles%20in%20python%2c%20r%2
       0and%20octave%20%e2%80%93%20part%203

     * [75][pinit_fg_en_rect_gray_20.png]
     * [76]more
     *

     * [77]email
     * [78]share on tumblr
     *
     * [79]telegram
     * [80]print
     *
     *

like this:

   like loading...

related

     * tagged
     * [81]deep learning
     * [82]octave
     * [83]python
     * [84]r
     * [85]r language
     * [86]r project

published by tinniam v ganesh

   visionary, thought leader and pioneer with 27+ years of experience in
   the software industry. [87]view all posts by tinniam v ganesh
   published january 30, 2018january 16, 2019

post navigation

   [88]previous post deep learning from first principles in python, r and
   octave     part 2
   [89]next post deep learning from first principles in python, r and
   octave     part 4

9 thoughts on    deep learning from first principles in python, r and octave    
part 3   

    1. pingback: [90]deep learning from first principles in python, r and
       octave     part 3     mubashir qasim
    2. pingback: [91]distilled news | data analytics & r
    3. pingback: [92]deep learning from first principles in python, r and
       octave     part 4 | giga thoughts    
    4. pingback: [93]deep learning from first principles in python, r and
       octave     part 5 | giga thoughts    
    5. pingback: [94]deep learning from first principles in python, r and
       octave     part 6 | giga thoughts    
    6. pingback: [95]deep learning from first principles in python, r and
       octave     part 7 | giga thoughts    
    7. pingback: [96]deep learning from first principles in python, r and
       octave     part 8 | giga thoughts    
    8. pingback: [97]my presentations on    elements of neural networks &
       deep learning    -parts 4,5 | giga thoughts    
    9. pingback: [98]take 4+: presentations on    elements of neural
       networks and deep learning        parts 1-8 | giga thoughts    

leave a reply [99]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [100]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [101]log out /
   [102]change )
   google photo

   you are commenting using your google account. ( [103]log out /
   [104]change )
   twitter picture

   you are commenting using your twitter account. ( [105]log out /
   [106]change )
   facebook photo

   you are commenting using your facebook account. ( [107]log out /
   [108]change )
   [109]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

connect with me:

     * [110]linkedin
     * [111]github
     * [112]twitter

   search for: ____________________ search

blog stats

     * 440,452 hits

visitors to giga thoughts (click to see details)

   [113]map

   [114]follow giga thoughts     on wordpress.com

popular posts

     * [115]working with node.js and postgresql
     * [116]simplifying ml: impact of degree of polynomial degree on bias
       & variance and other insights
     * [117]introducing cricketr! : an r package to analyze performances
       of cricketers
     * [118]re-introducing cricketr! : an r package to analyze
       performances of cricketers
     * [119]experiments with deblurring using opencv
     * [120]deep learning from first principles in python, r and octave -
       part 1
     * [121]my presentations on    elements of neural networks & deep
       learning    -parts 4,5
     * [122]practical machine learning with r and python     part 5
     * [123]introducing cricpy:a python package to analyze performances of
       cricketers
     * [124]r vs python: different similarities and similar differences

category cloud

   [125]analytics [126]android [127]android app [128]app [129]batsman
   [130]big data [131]bluemix [132]bowler [133]cloud computing
   [134]cricket [135]cricketr [136]cricsheet [137]data mining [138]deep
   learning [139]distributed systems [140]git [141]github [142]gradient
   descent [143]id75 [144]id28 [145]machine
   learning [146]neural networks [147]python [148]r [149]r language [150]r
   markdown [151]r package [152]r project [153]technology [154]yorkr

follow blog via email

   join 1,212 other followers

   ____________________

   (button) follow

subscribe

   [155]rss feed  [156]rss - posts

giga thoughts community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

archives

   archives [select month_______]

navigate

     * [157]home
     * [158]index of posts
     * [159]books i authored
     * [160]who am i?
     * [161]published posts
     * [162]about giga thoughts   

latest posts

     * [163]analyzing performances of cricketers using cricketr template
       march 30, 2019
     * [164]the clash of the titans in test and odi cricket march 15, 2019
     * [165]analyzing t20 matches with yorkpy templates march 10, 2019
     * [166]yorkpy takes a hat-trick, bowls out intl. t20s, bbl and
       natwest t20!!! march 3, 2019
     * [167]pitching yorkpy     in the block hole     part 4 february 26, 2019
     * [168]take 4+: presentations on    elements of neural networks and
       deep learning        parts 1-8 february 16, 2019
     * [169]pitching yorkpy   swinging away from the leg stump to ipl    
       part 3 february 3, 2019
     * [170]pitching yorkpy   on the middle and outside off-stump to ipl    
       part 2 january 27, 2019

   [171]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [172]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [173]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [174]likes-master

   %d bloggers like this:

references

   visible links
   1. https://gigadom.in/feed/
   2. https://gigadom.in/comments/feed/
   3. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/feed/
   4. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
   5. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/&for=wpcom-auto-discovery
   8. https://gigadom.in/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#content
  11. https://gigadom.in/
  12. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
  13. https://github.com/tvganesh
  14. https://twitter.com/tvganesh_85
  15. https://gigadom.in/
  16. https://gigadom.in/aa-2/
  17. https://gigadom.in/and-you-are/
  18. https://gigadom.in/who-am-i/
  19. https://gigadom.in/published-posts/
  20. https://gigadom.in/about-giga-thoughts/
  21. https://gigadom.in/author/gigadom/
  22. https://gigadom.in/category/artificial-intelligence/
  23. https://gigadom.in/category/id26/
  24. https://gigadom.in/category/backward-propagation/
  25. https://gigadom.in/category/chain-rule/
  26. https://gigadom.in/category/contours/
  27. https://gigadom.in/tag/deep-learning/
  28. https://gigadom.in/category/dplyr/
  29. https://gigadom.in/category/git/
  30. https://gigadom.in/category/github/
  31. https://gigadom.in/category/neural-networks/
  32. https://gigadom.in/tag/octave/
  33. https://gigadom.in/category/python-2/
  34. https://gigadom.in/tag/r/
  35. https://gigadom.in/category/r-language/
  36. https://gigadom.in/category/r-markdown/
  37. https://gigadom.in/category/r-package/
  38. https://gigadom.in/tag/r-project/
  39. https://gigadom.in/category/technology/
  40. https://gigadom.wordpress.com/2017/10/06/practical-machine-learning-with-r-and-python-part-1/
  41. https://gigadom.wordpress.com/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  42. https://www.amazon.com/dp/1791596177
  43. https://www.amazon.com/dp/b07lbg542l
  44. https://stackoverflow.com/questions/1826519/how-to-assign-from-a-function-which-returns-more-than-one-value
  45. https://github.com/tvganesh/deeplearning-part3
  46. https://www.youtube.com/watch?v=fwze35vsav0
  47. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  48. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  49. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  50. https://gigadom.wordpress.com/2017/10/13/practical-machine-learning-with-r-and-python-part-2/
  51. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  52. https://gist.github.com/akesling/5358964
  53. https://gigadom.wordpress.com/2017/10/29/practical-machine-learning-with-r-and-python-part-4/
  54. https://www.amazon.com/dp/1983035661
  55. https://www.amazon.com/dp/b07dfkscwz
  56. https://gist.github.com/brendano/39760
  57. http://daniel-e.github.io/2017-10-20-loading-mnist-handwritten-digits-with-octave-or-matlab/
  58. https://github.com/tvganesh/deeplearning-part3
  59. https://www.coursera.org/specializations/deep-learning
  60. https://www.coursera.org/learn/neural-networks
  61. http://www.deeplearningbook.org/
  62. https://gigadom.wordpress.com/2017/01/21/neural-networks-the-mechanics-of-id26/
  63. https://www.coursera.org/learn/machine-learning
  64. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  65. https://gigadom.wordpress.com/2017/09/11/my-travels-through-the-realms-of-data-science-machine-learning-deep-learning-and-ai/
  66. https://gigadom.wordpress.com/2013/02/14/designing-a-social-web-portal/
  67. https://gigadom.wordpress.com/2017/01/05/googlyplus-yorkr-analyzes-ipl-players-teams-matches-with-plots-and-tables/
  68. https://gigadom.wordpress.com/2016/06/23/introducing-qcsimulator-a-5-qubit-quantum-computing-simulator-in-r/
  69. https://gigadom.wordpress.com/2013/07/23/presentation-on-intelligent-networks-camel-protocol-services-applications/
  70. https://gigadom.wordpress.com/2011/05/13/design-principles-of-scalable-distributed-systems/
  71. https://gigadom.wordpress.com/aa-2/
  72. https://getpocket.com/save
  73. https://twitter.com/share
  74. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/&title=deep learning from first principles in python, r and octave     part 3
  75. https://www.pinterest.com/pin/create/button/?url=https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/&media=https://gigadom.files.wordpress.com/2018/01/digits.jpg&description=deep learning from first principles in python, r and octave     part 3
  76. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  77. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/?share=email
  78. https://www.tumblr.com/share
  79. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/?share=telegram
  80. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#print
  81. https://gigadom.in/tag/deep-learning/
  82. https://gigadom.in/tag/octave/
  83. https://gigadom.in/tag/python/
  84. https://gigadom.in/tag/r/
  85. https://gigadom.in/tag/r-language-2/
  86. https://gigadom.in/tag/r-project/
  87. https://gigadom.in/author/gigadom/
  88. https://gigadom.in/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  89. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  90. http://mqasim.me/?p=153936
  91. http://advanceddataanalytics.net/2018/02/01/distilled-news-691/
  92. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  93. https://gigadom.wordpress.com/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
  94. https://gigadom.wordpress.com/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
  95. https://gigadom.wordpress.com/2018/04/29/deep-learning-from-first-principles-in-python-r-and-octave-part-7/
  96. https://gigadom.wordpress.com/2018/05/06/deep-learning-from-first-principles-in-python-r-and-octave-part-8/
  97. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
  98. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
  99. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#respond
 100. https://gravatar.com/site/signup/
 101. javascript:highlandercomments.doexternallogout( 'wordpress' );
 102. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 103. javascript:highlandercomments.doexternallogout( 'googleplus' );
 104. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 105. javascript:highlandercomments.doexternallogout( 'twitter' );
 106. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 107. javascript:highlandercomments.doexternallogout( 'facebook' );
 108. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 109. javascript:highlandercomments.cancelexternalwindow();
 110. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
 111. https://github.com/tvganesh
 112. https://twitter.com/tvganesh_85
 113. https://www.revolvermaps.com/?target=enlarge&i=0z8r51l0ucz
 114. https://gigadom.in/
 115. https://gigadom.in/2014/07/20/working-with-node-js-and-postgresql/
 116. https://gigadom.in/2014/01/04/simplifying-ml-impact-of-degree-of-polynomial-degree-on-bias-variance-and-other-insights/
 117. https://gigadom.in/2015/07/04/introducing-cricketr-a-r-package-to-analyze-performances-of-cricketers/
 118. https://gigadom.in/2016/05/14/re-introducing-cricketr-an-r-package-to-analyze-performances-of-cricketers/
 119. https://gigadom.in/2011/11/09/experiments-with-deblurring-using-opencv/
 120. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 121. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 122. https://gigadom.in/2017/11/07/practical-machine-learning-with-r-and-python-part-5/
 123. https://gigadom.in/2018/10/28/introducing-cricpya-python-package-to-analyze-performances-of-cricketrs/
 124. https://gigadom.in/2017/05/22/r-vs-python-different-similarities-and-similar-differences/
 125. https://gigadom.in/category/analytics/
 126. https://gigadom.in/category/android/
 127. https://gigadom.in/category/android-app/
 128. https://gigadom.in/category/app/
 129. https://gigadom.in/category/batsman/
 130. https://gigadom.in/category/big-data/
 131. https://gigadom.in/category/bluemix/
 132. https://gigadom.in/category/bowler/
 133. https://gigadom.in/category/cloud-computing/
 134. https://gigadom.in/category/cricket/
 135. https://gigadom.in/category/cricketr/
 136. https://gigadom.in/category/cricsheet/
 137. https://gigadom.in/category/data-mining/
 138. https://gigadom.in/category/deep-learning/
 139. https://gigadom.in/category/distributed-systems/
 140. https://gigadom.in/category/git/
 141. https://gigadom.in/category/github/
 142. https://gigadom.in/category/gradient-descent/
 143. https://gigadom.in/category/linear-regression/
 144. https://gigadom.in/category/logistic-regression/
 145. https://gigadom.in/category/machine-learning/
 146. https://gigadom.in/category/neural-networks/
 147. https://gigadom.in/category/python-2/
 148. https://gigadom.in/category/r/
 149. https://gigadom.in/category/r-language/
 150. https://gigadom.in/category/r-markdown/
 151. https://gigadom.in/category/r-package/
 152. https://gigadom.in/category/r-project/
 153. https://gigadom.in/category/technology/
 154. https://gigadom.in/category/yorkr/
 155. https://gigadom.in/feed/
 156. https://gigadom.in/feed/
 157. https://gigadom.in/
 158. https://gigadom.in/aa-2/
 159. https://gigadom.in/and-you-are/
 160. https://gigadom.in/who-am-i/
 161. https://gigadom.in/published-posts/
 162. https://gigadom.in/about-giga-thoughts/
 163. https://gigadom.in/2019/03/30/analyzing-performances-of-cricketers-using-cricketr-template/
 164. https://gigadom.in/2019/03/15/the-clash-of-the-titans-in-test-and-odi-cricket/
 165. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
 166. https://gigadom.in/2019/03/03/yorkpy-takes-a-hat-trick-bowls-out-intl-t20s-bbl-and-natwest-t20/
 167. https://gigadom.in/2019/02/26/pitching-yorkpy-in-the-block-hole-part-4/
 168. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 169. https://gigadom.in/2019/02/03/pitching-yorkpyswinging-away-from-the-leg-stump-to-ipl-part-3/
 170. https://gigadom.in/2019/01/27/pitching-yorkpyon-the-middle-and-outside-off-stump-to-ipl-part-2/
 171. https://wordpress.com/?ref=footer_blog
 172. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 173. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#cancel
 174. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 176. https://gigadom.in/
 177. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#comment-form-guest
 178. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#comment-form-load-service:wordpress.com
 179. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#comment-form-load-service:twitter
 180. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/#comment-form-load-service:facebook
 181. http://lemanshots.wordpress.com/
 182. https://vinodsblog.com/about
 183. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 184. http://friartuck2012.wordpress.com/
 185. http://webastion.wordpress.com/
 186. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 187. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 188. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 189. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
 190. http://micvdotin.wordpress.com/
