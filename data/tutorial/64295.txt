   #[1]giga thoughts ...    feed [2]giga thoughts ...    comments feed
   [3]giga thoughts ...    deep learning from first principles in python, r
   and octave     part 4 comments feed [4]deep learning from first
   principles in python, r and octave     part 3 [5]presentation on    machine
   learning in plain english     part 1    [6]alternate [7]alternate [8]giga
   thoughts ... [9]wordpress.com

   [10]skip to content

   [11]giga thoughts    

   insights into technology

     * [12]linkedin
     * [13]github
     * [14]twitter

   (button) menu

     * [15]home
     * [16]index of posts
     * [17]books i authored
     * [18]who am i?
     * [19]published posts
     * [20]about giga thoughts   

deep learning from first principles in python, r and octave     part 4

   [21]tinniam v ganesh [22]artificial intelligence, [23]id26,
   [24]backward fit, [25]backward propagation, [26]confusion matrix,
   [27]deep learning, [28]git, [29]github, [30]jacobian, [31]machine
   learning, [32]neural networks, [33]numpy, [34]python, [35]r, [36]r
   language, [37]r markdown, [38]r package, [39]r project, [40]sklearn,
   [41]softmax, [42]technology february 26, 2018january 16, 2019

   in this 4th post of my series on deep learning from first principles in
   python, r and octave     part 4, i explore the details of creating a
   multi-class classifier using the softmax activation unit in a neural
   network. the earlier posts in this series were

   1. [43]deep learning from first principles in python, r and octave    
   part 1. in this post i implemented id28 as a simple
   neural network in vectorized python, r and octave
   2. [44]deep learning from first principles in python, r and octave    
   part 2. this 2nd part implemented the most elementary neural network
   with 1 hidden layer and any number of activation units in the hidden
   layer with sigmoid activation at the output layer
   3. [45]deep learning from first principles in python, r and octave    
   part 3. the 3rd implemented a multi-layer deep learning network with an
   arbitrary number if hidden layers and activation units per hidden
   layer. the output layer was for binary classification which was based
   on the sigmoid unit. this multi-layer deep network was implemented in
   vectorized python, r and octave.

   checkout my book    deep learning from first principles: second edition    
   in vectorized python, r and octave   . my book starts with the
   implementation of a simple 2-layer neural network and works its way to
   a generic l-layer deep learning network, with all the bells and
   whistles. the derivations have been discussed in detail. the code has
   been extensively commented and included in its entirety in the appendix
   sections. my book is available on amazon as [46]paperback ($18.99) and
   in [47]kindle version($9.99/rs449).

   this 4th part takes a swing at multi-class classification and uses the
   softmax as the activation unit in the output layer. inclusion of the
   softmax activation unit in the activation layer requires us to compute
   the derivative of softmax, or rather the    jacobian    of the softmax
   function, besides also computing the log loss for this softmax
   activation during back propagation. since the derivation of the
   jacobian of a softmax and the computation of the cross id178/log loss
   is very involved, i have implemented a basic neural network with just 1
   hidden layer with the softmax activation at the output layer. i also
   perform multi-class classification based on the    spiral    data set
   from [48]cs231n convolutional neural networks stanford course, to test
   the performance and correctness of the implementations in python, r and
   octave. you can clone download the code for the python, r and octave
   implementations from github at [49]deep learning     part 4

   note: a detailed discussion of the derivation below can also be seen in
   my video presentation [50]neural networks 5

   the softmax function takes an n dimensional vector as input and
   generates a n dimensional vector as output.
   the softmax function is given by
   s_{j}= \frac{e_{j}}{\sum_{i}^{n}e_{k}}
   there is a probabilistic interpretation of the softmax, since the sum
   of the softmax values of a set of vectors will always add up to 1,
   given that each softmax value is divided by the total of all values.

   as mentioned earlier, the softmax takes a vector input and returns a
   vector of outputs.  for e.g. the softmax of a vector a=[1, 3, 6]  is
   another vector s=[0.0063,0.0471,0.9464]. notice that vector output is
   proportional to the input vector.  also, taking the derivative of a
   vector by another vector, is known as the jacobian. by the way, [51]the
   matrix calculus you need for deep learning by terence parr and jeremy
   howard, is very good paper that distills all the main mathematical
   concepts for deep learning in one place.

   let us take a simple 2 layered neural network with just 2 activation
   units in the hidden layer is shown below
   z_{1}^{1} =w_{11}^{1}x_{1} + w_{21}^{1}x_{2} + b_{1}^{1}
   z_{2}^{1} =w_{12}^{1}x_{1} + w_{22}^{1}x_{2} + b_{2}^{1}
   and
   a_{1}^{1} = g'(z_{1}^{1})
   a_{2}^{1} = g'(z_{2}^{1})
   where g'() is the activation unit in the hidden layer which can be a
   relu, sigmoid or a
   tanh function

   note: the superscript denotes the layer. the above denotes the equation
   for layer 1
   of the neural network. for layer 2 with the softmax activation, the
   equations are
   z_{1}^{2} =w_{11}^{2}x_{1} + w_{21}^{2}x_{2} + b_{1}^{2}
   z_{2}^{2} =w_{12}^{2}x_{1} + w_{22}^{2}x_{2} + b_{2}^{2}
   and
   a_{1}^{2} = s(z_{1}^{2})
   a_{2}^{2} = s(z_{2}^{2})
   where s() is the softmax activation function
   s=\begin{pmatrix} s(z_{1}^{2})\\ s(z_{2}^{2}) \end{pmatrix}
   s=\begin{pmatrix} \frac{e^{z1}}{e^{z1}+e^{z2}}\\
   \frac{e^{z2}}{e^{z1}+e^{z2}} \end{pmatrix}

   the jacobian of the softmax    s    is given by
   \begin{pmatrix} \frac {\partial s_{1}}{\partial z_{1}} & \frac
   {\partial s_{1}}{\partial z_{2}}\\ \frac {\partial s_{2}}{\partial
   z_{1}} & \frac {\partial s_{2}}{\partial z_{2}} \end{pmatrix}
   \begin{pmatrix} \frac{\partial}{\partial z_{1}} \frac {e^{z1}}{e^{z1}+
   e^{z2}} & \frac{\partial}{\partial z_{2}} \frac {e^{z1}}{e^{z1}+
   e^{z2}}\\ \frac{\partial}{\partial z_{1}} \frac {e^{z2}}{e^{z1}+
   e^{z2}} & \frac{\partial}{\partial z_{2}} \frac {e^{z2}}{e^{z1}+
   e^{z2}} \end{pmatrix}          (a)

   now the    division-rule     of derivatives is as follows. if u and v are
   functions of x, then
   \frac{d}{dx} \frac {u}{v} =\frac {vdu -udv}{v^{2}}
   using this to compute each element of the above jacobian matrix, we see
   that
   when i=j we have
   \frac {\partial}{\partial z1}\frac{e^{z1}}{e^{z1}+e^{z2}} = \frac {\sum
   e^{z1} - e^{z1^{2}}}{\sum ^{2}}
   and when i \neq j
   \frac {\partial}{\partial z1}\frac{e^{z2}}{e^{z1}+e^{z2}} = \frac {0 -
   e^{z1}e^{z2}}{\sum ^{2}}
   this is of the general form
   \frac {\partial s_{j}}{\partial z_{i}} = s_{i}( 1-s_{j})   when i=j
   and
   \frac {\partial s_{j}}{\partial z_{i}} = -s_{i}s_{j}   when i \neq j
   note: since the softmax essentially gives the id203 the following
   notation is also used
   \frac {\partial p_{j}}{\partial z_{i}} = p_{i}( 1-p_{j}) when i=j
   and
   \frac {\partial p_{j}}{\partial z_{i}} = -p_{i}p_{j} when i \neq j
   if you throw the    kronecker delta    into the equation, then the above
   equations can be expressed even more concisely as
   \frac {\partial p_{j}}{\partial z_{i}} = p_{i} (\delta_{ij} - p_{j})
   where \delta_{ij} = 1 when i=j and 0 when i \neq j

   this reduces the jacobian of the simple 2 output softmax vectors
   equation (a) as
   \begin{pmatrix} p_{1}(1-p_{1}) & -p_{1}p_{2} \\ -p_{2}p_{1} &
   p_{2}(1-p_{2}) \end{pmatrix}
   the loss of softmax is given by
   l = -\sum y_{i} log(p_{i})
   for the 2 valued softmax output this is
   \frac {dl}{dp1} = -\frac {y_{1}}{p_{1}}
   \frac {dl}{dp2} = -\frac {y_{2}}{p_{2}}
   using the chain rule we can write
   \frac {\partial l}{\partial w_{pq}} = \sum _{i}\frac {\partial
   l}{\partial p_{i}} \frac {\partial p_{i}}{\partial w_{pq}} (1)
   and
   \frac {\partial p_{i}}{\partial w_{pq}} = \sum _{k}\frac {\partial
   p_{i}}{\partial z_{k}} \frac {\partial z_{k}}{\partial w_{pq}} (2)
   in expanded form this is
   \frac {\partial l}{\partial w_{pq}} = \sum _{i}\frac {\partial
   l}{\partial p_{i}} \sum _{k}\frac {\partial p_{i}}{\partial z_{k}}
   \frac {\partial z_{k}}{\partial w_{pq}}
   also
   \frac {\partial l}{\partial z_{i}} =\sum _{i} \frac {\partial
   l}{\partial p} \frac {\partial p}{\partial z_{i}}
   therefore
   \frac {\partial l}{\partial z_{1}} =\frac {\partial l}{\partial p_{1}}
   \frac {\partial p_{1}}{\partial z_{1}} +\frac {\partial l}{\partial
   p_{2}} \frac {\partial p_{2}}{\partial z_{1}}
   \frac {\partial l}{\partial z_{1}}=-\frac {y1}{p1} p1(1-p1) - \frac
   {y2}{p2}*(-p_{2}p_{1})
   since
   \frac {\partial p_{j}}{\partial z_{i}} = p_{i}( 1-p_{j}) when i=j
   and
   \frac {\partial p_{j}}{\partial z_{i}} = -p_{i}p_{j} when i \neq j
   which simplifies to
   \frac {\partial l}{\partial z_{1}} = -y_{1} + y_{1}p_{1} + y_{2}p_{1} =
   p_{1}\sum (y_{1} + y_2) - y_{1}
   \frac {\partial l}{\partial z_{1}}= p_{1} - y_{1}
   since
   \sum_{i} y_{i} =1
   similarly
   \frac {\partial l}{\partial z_{2}} =\frac {\partial l}{\partial p_{1}}
   \frac {\partial p_{1}}{\partial z_{2}} +\frac {\partial l}{\partial
   p_{2}} \frac {\partial p_{2}}{\partial z_{2}}
   \frac {\partial l}{\partial z_{2}}=-\frac {y1}{p1}*(p_{1}p_{2}) - \frac
   {y2}{p2}*p_{2}(1-p_{2})
   y_{1}p_{2} + y_{2}p_{2} - y_{2}
   \frac {\partial l}{\partial z_{2}} =p_{2}\sum (y_{1} + y_2) - y_{2}\\ =
   p_{2} - y_{2}
   in general this is of the form
   \frac {\partial l}{\partial z_{i}} = p_{i} -y_{i}
   for e.g if the probabilities computed were p=[0.1, 0.7, 0.2] then this
   implies that the class with id203 0.7 is the likely class. this
   would imply that the    one hot encoding    for  yi  would be yi=[0,1,0]
   therefore the gradient pi-yi = [0.1,-0.3,0.2]

   <strong>note: further, we could extend this derivation for a softmax
   activation output that outputs 3 classes
   s=\begin{pmatrix} \frac{e^{z1}}{e^{z1}+e^{z2}+e^{z3}}\\
   \frac{e^{z2}}{e^{z1}+e^{z2}+e^{z3}} \\
   \frac{e^{z3}}{e^{z1}+e^{z2}+e^{z3}} \end{pmatrix}

   we could derive
   \frac {\partial l}{\partial z1}= \frac {\partial l}{\partial p_{1}}
   \frac {\partial p_{1}}{\partial z_{1}} +\frac {\partial l}{\partial
   p_{2}} \frac {\partial p_{2}}{\partial z_{1}} +\frac {\partial
   l}{\partial p_{3}} \frac {\partial p_{3}}{\partial z_{1}} which
   similarly reduces to
   \frac {\partial l}{\partial z_{1}}=-\frac {y1}{p1} p1(1-p1) - \frac
   {y2}{p2}*(-p_{2}p_{1}) - \frac {y3}{p3}*(-p_{3}p_{1})
   -y_{1}+ y_{1}p_{1} + y_{2}p_{1} + y_{3}p1 = p_{1}\sum (y_{1} + y_2 +
   y_3) - y_{1} = p_{1} - y_{1}
   interestingly, despite the lengthy derivations the final result is
   simple and intuitive!

   as seen in my post    [52]deep learning from first principles with
   python, r and octave     part 3 the key equations for forward and
   backward propagation are
   forward propagation equations layer 1
   z_{1} = w_{1}x +b_{1}      and  a_{1} = g(z_{1})
   forward propagation equations layer 1
   z_{2} = w_{2}a_{1} +b_{2}   and  a_{2} = s(z_{2})

   using the result (a) in the back propagation equations below we have
   backward propagation equations layer 2
   \partial l/\partial w_{2} =\partial l/\partial
   z_{2}*a_{1}=(p_{2}-y_{2})*a_{1}
   \partial l/\partial b_{2} =\partial l/\partial z_{2}=p_{2}-y_{2}
   \partial l/\partial a_{1} = \partial l/\partial z_{2} *
   w_{2}=(p_{2}-y_{2})*w_{2}
   backward propagation equations layer 1
   \partial l/\partial w_{1} =\partial l/\partial z_{1}
   *a_{0}=(p_{1}-y_{1})*a_{0}
   \partial l/\partial b_{1} =\partial l/\partial z_{1}=(p_{1}-y_{1})

2.0 spiral data set

   as i mentioned earlier, i will be using the    spiral    data
   from [53]cs231n convolutional neural networks to ensure that my
   vectorized implementations in python, r and octave are correct. here is
   the    spiral    data set.
import numpy as np
import matplotlib.pyplot as plt
import os
os.chdir("c:/junk/dl-4/dl-4")
exec(open("././dlfunctions41.py").read())

# create an input data set - taken from cs231n convolutional neural networks
# http://cs231n.github.io/neural-networks-case-study/
n = 100 # number of points per class
d = 2 # dimensionality
k = 3 # number of classes
x = np.zeros((n*k,d)) # data matrix (each row = single example)
y = np.zeros(n*k, dtype='uint8') # class labels
for j in range(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j*4,(j+1)*4,n) + np.random.randn(n)*0.2 # theta
  x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
# plot the data
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.spectral)
plt.savefig("fig1.png", bbox_inches='tight')

   the implementations of the vectorized python, r and octave code are
   shown diagrammatically below

2.1 multi-class classification with softmax     python code

   a simple 2 layer neural network with a single hidden layer , with 100
   relu activation units in the hidden layer and the softmax activation
   unit in the output layer is used for multi-class classification. this
   deep learning network, plots the non-linear boundary of the 3 classes
   as shown below
import numpy as np
import matplotlib.pyplot as plt
import os
os.chdir("c:/junk/dl-4/dl-4")
exec(open("././dlfunctions41.py").read())

# read the input data
n = 100 # number of points per class
d = 2 # dimensionality
k = 3 # number of classes
x = np.zeros((n*k,d)) # data matrix (each row = single example)
y = np.zeros(n*k, dtype='uint8') # class labels
for j in range(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j*4,(j+1)*4,n) + np.random.randn(n)*0.2 # theta
  x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j

# set the number of features, hidden units in hidden layer and number of classes
s
numhidden=100 # no of hidden units in hidden layer
numfeats= 2 # dimensionality
numoutput = 3 # number of classes

# initialize the model
parameters=initializemodel(numfeats,numhidden,numoutput)
w1= parameters['w1']
b1= parameters['b1']
w2= parameters['w2']
b2= parameters['b2']

# set the learning rate
learningrate=0.6

# initialize losses
losses=[]
# perform id119
for i in range(10000):
    # forward propagation through hidden layer with relu units
    a1,cache1= layeractivationforward(x.t,w1,b1,'relu')

    # forward propagation through output layer with softmax
    a2,cache2 = layeractivationforward(a1,w2,b2,'softmax')

    # no of training examples
    numtraining = x.shape[0]
    # compute log probs. take the log prob of correct class based on output y
    correct_logprobs = -np.log(a2[range(numtraining),y])
    # conpute loss
    loss = np.sum(correct_logprobs)/numtraining

    # print the loss
    if i % 1000 == 0:
        print("iteration %d: loss %f" % (i, loss))
        losses.append(loss)

    da=0

    # backward  propagation through output layer with softmax
    da1,dw2,db2 = layeractivationbackward(da, cache2, y, activationfunc='softmax
')
    # backward  propagation through hidden layer with relu unit
    da0,dw1,db1 = layeractivationbackward(da1.t, cache1, y, activationfunc='relu
')

    #update paramaters with the learning rate
    w1 += -learningrate * dw1
    b1 += -learningrate * db1
    w2 += -learningrate * dw2.t
    b2 += -learningrate * db2.t

#plot losses vs iterations
i=np.arange(0,10000,1000)
plt.plot(i,losses)

plt.xlabel('iterations')
plt.ylabel('loss')
plt.title('losses vs iterations')
plt.savefig("fig2.png", bbox="tight")

#compute the multi-class confusion matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore

# we need to determine the predicted values from the learnt data
# forward propagation through hidden layer with relu units
a1,cache1= layeractivationforward(x.t,w1,b1,'relu')

# forward propagation through output layer with softmax
a2,cache2 = layeractivationforward(a1,w2,b2,'softmax')
#compute predicted values from weights and biases
yhat=np.argmax(a2, axis=1)

a=confusion_matrix(y.t,yhat.t)
print("multi-class confusion matrix")
print(a)
## iteration 0: loss 1.098507
## iteration 1000: loss 0.214611
## iteration 2000: loss 0.043622
## iteration 3000: loss 0.032525
## iteration 4000: loss 0.025108
## iteration 5000: loss 0.021365
## iteration 6000: loss 0.019046
## iteration 7000: loss 0.017475
## iteration 8000: loss 0.016359
## iteration 9000: loss 0.015703
## multi-class confusion matrix
## [[ 99   1   0]
##  [  0 100   0]
##  [  0   1  99]]

   check out my compact and minimal book     practical machine learning with
   r and python:second edition- machine learning in stereo     available in
   amazon in [54]paperback($10.99) and [55]kindle($7.99) versions. my book
   includes implementations of key ml algorithms and associated measures
   and metrics. the book is ideal for anybody who is familiar with the
   concepts and would like a quick reference to the different ml
   algorithms that can be applied to problems and how to select the best
   model. pick your copy today!!

2.2 multi-class classification with softmax     r code

   the spiral data set created with python was saved, and is used as the
   input with r code. the r neural network seems to perform much,much
   slower than both python and octave. not sure why! incidentally the
   computation of loss and the softmax derivative are identical for both r
   and octave. yet r is much slower. to compute the softmax derivative i
   create matrices for the one hot encoded yi and then stack them before
   subtracting pi-yi. i am sure there is a more elegant and more efficient
   way to do this, much like python. any suggestions?
library(ggplot2)
library(dplyr)
library(rcolorbrewer)
source("dlfunctions41.r")
# read the spiral dataset
z <- as.matrix(read.csv("spiral.csv",header=false))
z1=data.frame(z)
#plot the dataset
ggplot(z1,aes(x=v1,y=v2,col=v3)) +geom_point() +
  scale_colour_gradientn(colours = brewer.pal(10, "spectral"))

# setup the data
x <- z[,1:2]
y <- z[,3]
x1 <- t(x)
y1 <- t(y)

# initialize number of features, number of hidden units in hidden layer and
# number of classes
numfeats<-2 # no features
numhidden<-100 # no of hidden units
numoutput<-3 # no of classes

# initialize model
parameters <-initializemodel(numfeats, numhidden,numoutput)

w1 <-parameters[['w1']]
b1 <-parameters[['b1']]
w2 <-parameters[['w2']]
b2 <-parameters[['b2']]

# set the learning rate
learningrate <- 0.5
# initialize losses
losses <- null
# perform id119
for(i in 0:9000){

# forward propagation through hidden layer with relu units
retvals <- layeractivationforward(x1,w1,b1,'relu')
a1 <- retvals[['a']]
cache1 <- retvals[['cache']]
forward_cache1 <- cache1[['forward_cache1']]
activation_cache <- cache1[['activation_cache']]

# forward propagation through output layer with softmax units
retvals = layeractivationforward(a1,w2,b2,'softmax')
a2 <- retvals[['a']]
cache2 <- retvals[['cache']]
forward_cache2 <- cache2[['forward_cache1']]
activation_cache2 <- cache2[['activation_cache']]

# no oftraining examples
numtraining <- dim(x)[1]
da <-0

# select the elements where the y values are 0, 1 or 2 and make a vector
a=c(a2[y==0,1],a2[y==1,2],a2[y==2,3])
# take log
correct_probs = -log(a)
# compute loss
loss= sum(correct_probs)/numtraining

if(i %% 1000 == 0){
sprintf("iteration %d: loss %f",i, loss)
print(loss)
}
# backward propagation through output layer with softmax units
retvals = layeractivationbackward(da, cache2, y, activationfunc='softmax')
da1 = retvals[['da_prev']]
dw2= retvals[['dw']]
db2= retvals[['db']]
# backward propagation through hidden layer with relu units
retvals = layeractivationbackward(t(da1), cache1, y, activationfunc='relu')
da0 = retvals[['da_prev']]
dw1= retvals[['dw']]
db1= retvals[['db']]

# update parameters
w1 <- w1 - learningrate * dw1
b1 <- b1 - learningrate * db1
w2 <- w2 - learningrate * t(dw2)
b2 <- b2 - learningrate * t(db2)
}
## [1] 1.212487
## [1] 0.5740867
## [1] 0.4048824
## [1] 0.3561941
## [1] 0.2509576
## [1] 0.7351063
## [1] 0.2066114
## [1] 0.2065875
## [1] 0.2151943
## [1] 0.1318807


#create iterations
iterations <- seq(0,10)
#df=data.frame(iterations,losses)
ggplot(df,aes(x=iterations,y=losses)) + geom_point() + geom_line(color="blue") +
    ggtitle("losses vs iterations") + xlab("iterations") + ylab("loss")

plotdecisionboundary(z,w1,b1,w2,b2)

   [untitled.png?w=1024&#038;h=526]
   multi-class confusion matrix
library(caret)
library(e1071)

# forward propagation through hidden layer with relu units
retvals <- layeractivationforward(x1,w1,b1,'relu')
a1 <- retvals[['a']]

# forward propagation through output layer with softmax units
retvals = layeractivationforward(a1,w2,b2,'softmax')
a2 <- retvals[['a']]
yhat <- apply(a2, 1,which.max) -1
confusion matrix and statistics
          reference
prediction  0  1  2
         0 97  0  1
         1  2 96  4
         2  1  4 95

overall statistics
               accuracy : 0.96
                 95% ci : (0.9312, 0.9792)
    no information rate : 0.3333
    p-value [acc > nir] : <2e-16

                  kappa : 0.94
 mcnemar's test p-value : 0.5724
statistics by class:

                     class: 0 class: 1 class: 2
sensitivity            0.9700   0.9600   0.9500
specificity            0.9950   0.9700   0.9750
pos pred value         0.9898   0.9412   0.9500
neg pred value         0.9851   0.9798   0.9750
prevalence             0.3333   0.3333   0.3333
detection rate         0.3233   0.3200   0.3167
detection prevalence   0.3267   0.3400   0.3333
balanced accuracy      0.9825   0.9650   0.9625

   my book    [56]practical machine learning with r and python    includes the
   implementation for many machine learning algorithms and associated
   metrics. pick up your copy today!

2.3 multi-class classification with softmax     octave code

   a 2 layer neural network with the softmax activation unit in the output
   layer is constructed in octave. the same spiral data set is used for
   octave also
   source("dl41functions.m")
   # read the spiral data
   data=csvread("spiral.csv");
   # setup the data
   x=data(:,1:2);
   y=data(:,3);
   # set the number of features, number of hidden units in hidden layer
   and number of classes
   numfeats=2; #no features
   numhidden=100; # no of hidden units
   numoutput=3; # no of classes
   # initialize model
   [w1 b1 w2 b2] = initializemodel(numfeats,numhidden,numoutput);
   # initialize losses
   losses=[]
   #initialize learningrate
   learningrate=0.5;
   for k =1:10000
   # forward propagation through hidden layer with relu units
   [a1,cache1 activation_cache1]=
   layeractivationforward(x',w1,b1,activationfunc ='relu');
   # forward propagation through output layer with softmax units
   [a2,cache2 activation_cache2] =
   layeractivationforward(a1,w2,b2,activationfunc='softmax');
   # no of training examples
   numtraining = size(x)(1);
   # select rows where y=0,1,and 2 and concatenate to a long vector
   a=[a2(y==0,1) ;a2(y==1,2) ;a2(y==2,3)];
   #select the correct column for log prob
   correct_probs = -log(a);
   #compute log loss
   loss= sum(correct_probs)/numtraining;
   if(mod(k,1000) == 0)
   disp(loss);
   losses=[losses loss];
   endif
   da=0;
   # backward propagation through output layer with softmax units
   [da1 dw2 db2] = layeractivationbackward(da, cache2,
   activation_cache2,y,activationfunc='softmax');
   # backward propagation through hidden layer with relu units
   [da0,dw1,db1] = layeractivationbackward(da1', cache1,
   activation_cache1, y, activationfunc='relu');
   #update parameters
   w1 += -learningrate * dw1;
   b1 += -learningrate * db1;
   w2 += -learningrate * dw2';
   b2 += -learningrate * db2';
   endfor
   # plot losses vs iterations
   iterations=0:1000:9000
   plotcostvsiterations(iterations,losses)
   # plot the decision boundary
   plotdecisionboundary( x,y,w1,b1,w2,b2)

   the code for the python, r and octave implementations can be downloaded
   from github at [57]deep learning     part 4

conclusion

   in this post i have implemented a 2 layer neural network with the
   softmax classifier. in [58]part 3, i implemented a multi-layer deep
   learning network. i intend to include the softmax activation unit into
   the generalized multi-layer deep network along with the other
   activation units of sigmoid,tanh and relu.

   stick around, i   ll be back!!
   watch this space!

   references
   1. [59]deep learning specialization
   2. [60]neural networks for machine learning
   3. [61]cs231 convolutional neural networks for visual recognition
   4. [62]eli bendersky   s website     the softmax function and its
   derivative
   5. [63]cross validated     id26 with softmax / cross id178
   6. [64]stackoverflow     cs231n: how to calculate gradient for softmax
   id168?
   7. [65]math stack exchange     derivative of softmax
   8. [66]the matrix calculus for deep learning

   you may like
   1.[67]my book    practical machine learning with r and python    on amazon
   2. [68]my travels through the realms of data science, machine learning,
   deep learning and (ai)
   3. [69]deblurring with opencv: weiner filter reloaded
   4. [70]a method to crowd source pothole marking on (indian) roads
   5. [71]rock n    roll with bluemix, cloudant & nodeexpress
   6. [72]sea shells on the seashore
   7. [73]design principles of scalable, distributed systems

   to see all post click [74]index of posts

rate this:

share:

     *
     *
     * [75]pocket
     * [76]tweet
     *

       iframe:
       [77]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fgigadom.in%2f2018%2f02%2f26%2fdeep-le
       arning-from-first-principles-in-python-r-and-octave-part-4%2f&title
       =deep%20learning%20from%20first%20principles%20in%20python%2c%20r%2
       0and%20octave%20%e2%80%93%20part%204

     * [78][pinit_fg_en_rect_gray_20.png]
     * [79]more
     *

     * [80]email
     * [81]share on tumblr
     *
     * [82]telegram
     * [83]print
     *
     *

like this:

   like loading...

related

     * tagged
     * [84]jacobian
     * [85]kronecker product
     * [86]numpy
     * [87]python
     * [88]r
     * [89]r project
     * [90]rlanguage
     * [91]softmax

published by tinniam v ganesh

   visionary, thought leader and pioneer with 27+ years of experience in
   the software industry. [92]view all posts by tinniam v ganesh
   published february 26, 2018january 16, 2019

post navigation

   [93]previous post deep learning from first principles in python, r and
   octave     part 3
   [94]next post presentation on    machine learning in plain english    
   part 1   

11 thoughts on    deep learning from first principles in python, r and octave    
part 4   

    1. pingback: [95]deep learning from first principles in python, r and
       octave     part 4     mubashir qasim
    2. pingback: [96]distilled news | data analytics & r
    3. pingback: [97]deep learning from first principles in python, r and
       octave     part 4     cloud data architect
    4. pingback: [98]deep learning from first principles in python, r and
       octave     part 4 - biva
    5. pingback: [99]presentation on    machine learning in plain english    
       part 2    | giga thoughts    
    6. pingback: [100]deep learning from first principles in python, r and
       octave     part 5 | giga thoughts    
    7. pingback: [101]deep learning from first principles in python, r and
       octave     part 6 | giga thoughts    
    8. pingback: [102]deep learning from first principles in python, r and
       octave     part 8 | giga thoughts    
    9. pingback: [103]big data-2: move into the big league:graduate from r
       to sparkr | giga thoughts    
   10. pingback: [104]my presentations on    elements of neural networks &
       deep learning    -parts 4,5 | giga thoughts    
   11. pingback: [105]take 4+: presentations on    elements of neural
       networks and deep learning        parts 1-8 | giga thoughts    

leave a reply [106]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [107]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [108]log out /
   [109]change )
   google photo

   you are commenting using your google account. ( [110]log out /
   [111]change )
   twitter picture

   you are commenting using your twitter account. ( [112]log out /
   [113]change )
   facebook photo

   you are commenting using your facebook account. ( [114]log out /
   [115]change )
   [116]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

connect with me:

     * [117]linkedin
     * [118]github
     * [119]twitter

   search for: ____________________ search

blog stats

     * 440,452 hits

visitors to giga thoughts (click to see details)

   [120]map

   [121]follow giga thoughts     on wordpress.com

popular posts

     * [122]working with node.js and postgresql
     * [123]simplifying ml: impact of degree of polynomial degree on bias
       & variance and other insights
     * [124]introducing cricketr! : an r package to analyze performances
       of cricketers
     * [125]re-introducing cricketr! : an r package to analyze
       performances of cricketers
     * [126]experiments with deblurring using opencv
     * [127]deep learning from first principles in python, r and octave -
       part 1
     * [128]my presentations on    elements of neural networks & deep
       learning    -parts 4,5
     * [129]practical machine learning with r and python     part 5
     * [130]introducing cricpy:a python package to analyze performances of
       cricketers
     * [131]r vs python: different similarities and similar differences

category cloud

   [132]analytics [133]android [134]android app [135]app [136]batsman
   [137]big data [138]bluemix [139]bowler [140]cloud computing
   [141]cricket [142]cricketr [143]cricsheet [144]data mining [145]deep
   learning [146]distributed systems [147]git [148]github [149]gradient
   descent [150]id75 [151]id28 [152]machine
   learning [153]neural networks [154]python [155]r [156]r language [157]r
   markdown [158]r package [159]r project [160]technology [161]yorkr

follow blog via email

   join 1,212 other followers

   ____________________

   (button) follow

subscribe

   [162]rss feed  [163]rss - posts

giga thoughts community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

archives

   archives [select month_______]

navigate

     * [164]home
     * [165]index of posts
     * [166]books i authored
     * [167]who am i?
     * [168]published posts
     * [169]about giga thoughts   

latest posts

     * [170]analyzing performances of cricketers using cricketr template
       march 30, 2019
     * [171]the clash of the titans in test and odi cricket march 15, 2019
     * [172]analyzing t20 matches with yorkpy templates march 10, 2019
     * [173]yorkpy takes a hat-trick, bowls out intl. t20s, bbl and
       natwest t20!!! march 3, 2019
     * [174]pitching yorkpy     in the block hole     part 4 february 26, 2019
     * [175]take 4+: presentations on    elements of neural networks and
       deep learning        parts 1-8 february 16, 2019
     * [176]pitching yorkpy   swinging away from the leg stump to ipl    
       part 3 february 3, 2019
     * [177]pitching yorkpy   on the middle and outside off-stump to ipl    
       part 2 january 27, 2019

   [178]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [179]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [180]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [181]likes-master

   %d bloggers like this:

references

   visible links
   1. https://gigadom.in/feed/
   2. https://gigadom.in/comments/feed/
   3. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/feed/
   4. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
   5. https://gigadom.in/2018/03/06/presentation-on-machine-learning-in-plain-english-part-1/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/&for=wpcom-auto-discovery
   8. https://gigadom.in/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#content
  11. https://gigadom.in/
  12. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
  13. https://github.com/tvganesh
  14. https://twitter.com/tvganesh_85
  15. https://gigadom.in/
  16. https://gigadom.in/aa-2/
  17. https://gigadom.in/and-you-are/
  18. https://gigadom.in/who-am-i/
  19. https://gigadom.in/published-posts/
  20. https://gigadom.in/about-giga-thoughts/
  21. https://gigadom.in/author/gigadom/
  22. https://gigadom.in/category/artificial-intelligence/
  23. https://gigadom.in/category/id26/
  24. https://gigadom.in/category/backward-fit/
  25. https://gigadom.in/category/backward-propagation/
  26. https://gigadom.in/category/confusion-matrix/
  27. https://gigadom.in/category/deep-learning/
  28. https://gigadom.in/category/git/
  29. https://gigadom.in/category/github/
  30. https://gigadom.in/tag/jacobian/
  31. https://gigadom.in/category/machine-learning/
  32. https://gigadom.in/category/neural-networks/
  33. https://gigadom.in/tag/numpy/
  34. https://gigadom.in/category/python-2/
  35. https://gigadom.in/tag/r/
  36. https://gigadom.in/category/r-language/
  37. https://gigadom.in/category/r-markdown/
  38. https://gigadom.in/category/r-package/
  39. https://gigadom.in/tag/r-project/
  40. https://gigadom.in/category/sklearn/
  41. https://gigadom.in/tag/softmax/
  42. https://gigadom.in/category/technology/
  43. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  44. https://gigadom.wordpress.com/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  45. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  46. https://www.amazon.com/dp/1791596177
  47. https://www.amazon.com/dp/b07lbg542l
  48. http://cs231n.github.io/neural-networks-case-study/
  49. https://github.com/tvganesh/deeplearning-part4
  50. https://www.youtube.com/watch?v=qyeetztzajk&t=136s
  51. https://arxiv.org/pdf/1802.01528.pdf
  52. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  53. http://cs231n.github.io/neural-networks-case-study/
  54. https://www.amazon.com/dp/1983035661
  55. https://www.amazon.com/dp/b07dfkscwz
  56. https://www.amazon.com/dp/1973443503
  57. https://github.com/tvganesh/deeplearning-part4
  58. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  59. https://www.coursera.org/specializations/deep-learning
  60. https://www.coursera.org/learn/neural-networks
  61. http://cs231n.github.io/neural-networks-case-study/
  62. https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
  63. https://stats.stackexchange.com/questions/235528/id26-with-softmax-cross-id178
  64. https://stackoverflow.com/questions/41663874/cs231n-how-to-calculate-gradient-for-softmax-loss-function
  65. https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function
  66. https://arxiv.org/abs/1802.01528
  67. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  68. https://gigadom.wordpress.com/2017/09/11/my-travels-through-the-realms-of-data-science-machine-learning-deep-learning-and-ai/
  69. https://gigadom.wordpress.com/2012/05/11/deblurring-with-opencv-weiner-filter-reloaded/
  70. https://gigadom.wordpress.com/2013/12/11/a-method-to-crowd-source-pothole-marking-on-indian-roads/
  71. https://gigadom.wordpress.com/2014/08/19/rock-n-roll-with-bluemix-cloudant-nodeexpress/
  72. https://gigadom.wordpress.com/2012/01/03/sea-shells-on-the-seashore/
  73. https://gigadom.wordpress.com/2011/05/13/design-principles-of-scalable-distributed-systems/
  74. https://gigadom.wordpress.com/aa-2/
  75. https://getpocket.com/save
  76. https://twitter.com/share
  77. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/&title=deep learning from first principles in python, r and octave     part 4
  78. https://www.pinterest.com/pin/create/button/?url=https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/&media=https://gigadom.files.wordpress.com/2018/02/dl-3.png&description=deep learning from first principles in python, r and octave     part 4
  79. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  80. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/?share=email
  81. https://www.tumblr.com/share
  82. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/?share=telegram
  83. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#print
  84. https://gigadom.in/tag/jacobian/
  85. https://gigadom.in/tag/kronecker-product/
  86. https://gigadom.in/tag/numpy/
  87. https://gigadom.in/tag/python/
  88. https://gigadom.in/tag/r/
  89. https://gigadom.in/tag/r-project/
  90. https://gigadom.in/tag/rlanguage/
  91. https://gigadom.in/tag/softmax/
  92. https://gigadom.in/author/gigadom/
  93. https://gigadom.in/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  94. https://gigadom.in/2018/03/06/presentation-on-machine-learning-in-plain-english-part-1/
  95. http://mqasim.me/?p=157500
  96. http://advanceddataanalytics.net/2018/02/27/distilled-news-709/
  97. http://www.dataarchitect.cloud/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  98. http://www.biva-ags.com/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  99. https://gigadom.wordpress.com/2018/03/08/presentation-on-machine-learning-in-plain-english-part-2/
 100. https://gigadom.wordpress.com/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 101. https://gigadom.wordpress.com/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
 102. https://gigadom.wordpress.com/2018/05/06/deep-learning-from-first-principles-in-python-r-and-octave-part-8/
 103. https://gigadom.in/2018/10/09/big-data-2-move-into-the-big-leaguegraduate-from-r-to-sparkr/
 104. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 105. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 106. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#respond
 107. https://gravatar.com/site/signup/
 108. javascript:highlandercomments.doexternallogout( 'wordpress' );
 109. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 110. javascript:highlandercomments.doexternallogout( 'googleplus' );
 111. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 112. javascript:highlandercomments.doexternallogout( 'twitter' );
 113. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 114. javascript:highlandercomments.doexternallogout( 'facebook' );
 115. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 116. javascript:highlandercomments.cancelexternalwindow();
 117. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
 118. https://github.com/tvganesh
 119. https://twitter.com/tvganesh_85
 120. https://www.revolvermaps.com/?target=enlarge&i=0z8r51l0ucz
 121. https://gigadom.in/
 122. https://gigadom.in/2014/07/20/working-with-node-js-and-postgresql/
 123. https://gigadom.in/2014/01/04/simplifying-ml-impact-of-degree-of-polynomial-degree-on-bias-variance-and-other-insights/
 124. https://gigadom.in/2015/07/04/introducing-cricketr-a-r-package-to-analyze-performances-of-cricketers/
 125. https://gigadom.in/2016/05/14/re-introducing-cricketr-an-r-package-to-analyze-performances-of-cricketers/
 126. https://gigadom.in/2011/11/09/experiments-with-deblurring-using-opencv/
 127. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 128. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 129. https://gigadom.in/2017/11/07/practical-machine-learning-with-r-and-python-part-5/
 130. https://gigadom.in/2018/10/28/introducing-cricpya-python-package-to-analyze-performances-of-cricketrs/
 131. https://gigadom.in/2017/05/22/r-vs-python-different-similarities-and-similar-differences/
 132. https://gigadom.in/category/analytics/
 133. https://gigadom.in/category/android/
 134. https://gigadom.in/category/android-app/
 135. https://gigadom.in/category/app/
 136. https://gigadom.in/category/batsman/
 137. https://gigadom.in/category/big-data/
 138. https://gigadom.in/category/bluemix/
 139. https://gigadom.in/category/bowler/
 140. https://gigadom.in/category/cloud-computing/
 141. https://gigadom.in/category/cricket/
 142. https://gigadom.in/category/cricketr/
 143. https://gigadom.in/category/cricsheet/
 144. https://gigadom.in/category/data-mining/
 145. https://gigadom.in/category/deep-learning/
 146. https://gigadom.in/category/distributed-systems/
 147. https://gigadom.in/category/git/
 148. https://gigadom.in/category/github/
 149. https://gigadom.in/category/gradient-descent/
 150. https://gigadom.in/category/linear-regression/
 151. https://gigadom.in/category/logistic-regression/
 152. https://gigadom.in/category/machine-learning/
 153. https://gigadom.in/category/neural-networks/
 154. https://gigadom.in/category/python-2/
 155. https://gigadom.in/category/r/
 156. https://gigadom.in/category/r-language/
 157. https://gigadom.in/category/r-markdown/
 158. https://gigadom.in/category/r-package/
 159. https://gigadom.in/category/r-project/
 160. https://gigadom.in/category/technology/
 161. https://gigadom.in/category/yorkr/
 162. https://gigadom.in/feed/
 163. https://gigadom.in/feed/
 164. https://gigadom.in/
 165. https://gigadom.in/aa-2/
 166. https://gigadom.in/and-you-are/
 167. https://gigadom.in/who-am-i/
 168. https://gigadom.in/published-posts/
 169. https://gigadom.in/about-giga-thoughts/
 170. https://gigadom.in/2019/03/30/analyzing-performances-of-cricketers-using-cricketr-template/
 171. https://gigadom.in/2019/03/15/the-clash-of-the-titans-in-test-and-odi-cricket/
 172. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
 173. https://gigadom.in/2019/03/03/yorkpy-takes-a-hat-trick-bowls-out-intl-t20s-bbl-and-natwest-t20/
 174. https://gigadom.in/2019/02/26/pitching-yorkpy-in-the-block-hole-part-4/
 175. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 176. https://gigadom.in/2019/02/03/pitching-yorkpyswinging-away-from-the-leg-stump-to-ipl-part-3/
 177. https://gigadom.in/2019/01/27/pitching-yorkpyon-the-middle-and-outside-off-stump-to-ipl-part-2/
 178. https://wordpress.com/?ref=footer_blog
 179. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 180. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#cancel
 181. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 183. https://gigadom.in/
 184. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#comment-form-guest
 185. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#comment-form-load-service:wordpress.com
 186. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#comment-form-load-service:twitter
 187. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/#comment-form-load-service:facebook
 188. http://lemanshots.wordpress.com/
 189. https://vinodsblog.com/about
 190. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 191. http://friartuck2012.wordpress.com/
 192. http://webastion.wordpress.com/
 193. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 194. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 195. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 196. https://gigadom.in/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
 197. http://micvdotin.wordpress.com/
