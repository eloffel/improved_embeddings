   #[1]giga thoughts ...    feed [2]giga thoughts ...    comments feed
   [3]giga thoughts ...    deep learning from first principles in python, r
   and octave     part 5 comments feed [4]presentation on    machine learning
   in plain english     part 3 [5]deep learning from first principles in
   python, r and octave     part 6 [6]alternate [7]alternate [8]giga
   thoughts ... [9]wordpress.com

   [10]skip to content

   [11]giga thoughts    

   insights into technology

     * [12]linkedin
     * [13]github
     * [14]twitter

   (button) menu

     * [15]home
     * [16]index of posts
     * [17]books i authored
     * [18]who am i?
     * [19]published posts
     * [20]about giga thoughts   

deep learning from first principles in python, r and octave     part 5

   [21]tinniam v ganesh [22]id26, [23]backward propagation,
   [24]cost function, [25]deep learning, [26]id119,
   [27]id28, [28]mnist, [29]numpy, [30]python, [31]r, [32]r
   language, [33]r markdown, [34]r package, [35]r project, [36]sigmoid,
   [37]softmax, [38]technology march 23, 2018january 16, 2019

introduction

   a. a robot may not injure a human being or, through inaction, allow a
   human being to come to harm.
   b. a robot must obey orders given it by human beings except where such
   orders would conflict with the first law.
   c. a robot must protect its own existence as long as such protection
   does not conflict with the first or second law.
      isaac asimov's three laws of robotics

   any sufficiently advanced technology is indistinguishable from magic.
      arthur c clarke.

   in this 5th part on deep learning from first principles in python, r
   and octave, i solve the mnist data set of handwritten digits (shown
   below), from the basics. to do this, i construct a l-layer, vectorized
   deep learning implementation in python, r and octave from scratch and
   classify the  mnist data set. the mnist training data set  contains
   60000 handwritten digits from 0-9, and a test set of 10000 digits.
   mnist, is a popular dataset for running deep learning tests, and has
   been rightfully termed as the    drosophila    of deep learning, by none
   other than the venerable prof geoffrey hinton.

   the    deep learning from first principles in python, r and octave   
   series, so far included  [39]part 1 , where i had implemented logistic
   regression as a simple neural network. [40]part 2 implemented the most
   elementary neural network with 1 hidden layer, but  with any number of
   activation units in that layer, and a sigmoid activation at the output
   layer.

   this post,    deep learning from first principles in python, r and octave
       part 5    largely builds upon [41]part3. in which i implemented a
   multi-layer deep learning network, with an arbitrary number of hidden
   layers and activation units per hidden layer and with the output layer
   was based on the sigmoid unit, for binary classification. in [42]part
   4, i derive the jacobian of a softmax, the cross id178 loss and the
   gradient equations for a multi-class softmax classifier. i also
   implement a simple neural network using softmax classifications in
   python, r and octave.

   in this post i combine [43]part 3 and [44]part 4 to to build a l-layer
   deep learning network, with arbitrary number of hidden layers and
   hidden units, which can do both binary (sigmoid) and multi-class
   (softmax) classification.

   note: a detailed discussion of the derivation for multi-class
   clasification can be seen in my video presentation [45]neural networks
   5

   the generic, vectorized l-layer deep learning network implementations
   in python, r and octave can be cloned/downloaded from github at
   [46]deeplearning-part5. this implementation allows for arbitrary number
   of hidden layers and hidden layer units. the activation function at the
   hidden layers can be one of sigmoid, relu and tanh (will be adding
   leaky relu soon). the output activation can be used for binary
   classification with the    sigmoid   , or multi-class classification with
      softmax   . feel free to download and play around with the code!

   i thought the exercise of combining the two parts(part 3, & part 4)
   would be a breeze. but it was anything but. incorporating a softmax
   classifier into the generic l-layer deep learning model was a
   challenge. moreover i found that i could not use the id119
   on 60,000 training samples as my laptop ran out of memory. so i had to
   implement stochastic id119 (sgd) for python, r and octave.
   in addition, i had to also implement the numerically stable version of
   softmax, as the softmax and its derivative would result in nans.

numerically stable softmax

   the softmax function s_{j} =\frac{e^{z_{j}}}{\sum_{i}^{k}e^{z_{i}}} can
   be numerically unstable because of the division of large exponentials.
   to handle this problem we have to implement stable softmax function as
   below

   s_{j} =\frac{e^{z_{j}}}{\sum_{i}^{k}e^{z_{i}}}
   s_{j} =\frac{e^{z_{j}}}{\sum_{i}^{k}e^{z_{i}}} =
   \frac{ce^{z_{j}}}{c\sum_{i}^{k}e^{z_{i}}} =
   \frac{e^{z_{j}+log(c)}}{\sum_{i}^{k}e^{z_{i}+log(c)}}
   therefore s_{j}  = \frac{e^{z_{j}+ d}}{\sum_{i}^{k}e^{z_{i}+ d}}
   here    d    can be anything. a common choice is
   d=-max(z_{1},z_{2},... z_{k})

   here is the stable softmax implementation in python
# a numerically stable softmax implementation
def stablesoftmax(z):
    #compute the softmax of vector x in a numerically stable way.
    shiftz = z.t - np.max(z.t,axis=1).reshape(-1,1)
    exp_scores = np.exp(shiftz)
    # normalize them for each example
    a = exp_scores / np.sum(exp_scores, axis=1, keepdims=true)
    cache=z
    return a,cache

   while trying to create a l-layer generic deep learning network in the 3
   languages, i found it useful to ensure that the model executed
   correctly on smaller datasets.  you can run into numerous problems
   while setting up the matrices, which becomes extremely difficult to
   debug. so in this post, i run the model on 2 smaller data for sets used
   in my earlier posts(part 3 & part4) , in each of the languages, before
   running the generic model on mnist.

   here is a fair warning. if you think you can dive directly into deep
   learning, with just some basic knowledge of machine learning, you are
   bound to run into serious issues. moreover, your knowledge will be
   incomplete. it is essential that you have a good grasp of machine and
   statistical learning, the different algorithms, the measures and
   metrics for selecting the models etc.it would help to be conversant
   with all the ml models, ml concepts, validation techniques,
   classification measures  etc. check out the internet/books for
   background.

   checkout my book    deep learning from first principles: second edition    
   in vectorized python, r and octave   . my book starts with the
   implementation of a simple 2-layer neural network and works its way to
   a generic l-layer deep learning network, with all the bells and
   whistles. the derivations have been discussed in detail. the code has
   been extensively commented and included in its entirety in the appendix
   sections. my book is available on amazon as [47]paperback ($18.99) and
   in [48]kindle version($9.99/rs449).

   you may also like my companion book    practical machine learning with r
   and python:second edition- machine learning in stereo    available in
   amazon in [49]paperback($10.99) and [50]kindle($7.99/rs449) versions.
   this book is ideal for a quick reference of the various ml functions
   and associated measurements in both r and python which are essential to
   delve deep into deep learning.

1. random dataset with sigmoid activation     python

   this random data with 9 clusters, was used in my post [51]deep learning
   from first principles in python, r and octave     part 3 , and was used
   to test the complete l-layer deep learning network with sigmoid
   activation.
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_classification, make_blobs
exec(open("dlfunctions51.py").read()) # cannot import in rmd.
# create a random data set with 9 centeres
x1, y1 = make_blobs(n_samples = 400, n_features = 2, centers = 9,cluster_std = 1
.3, random_state =4)

#create 2 classes
y1=y1.reshape(400,1)
y1 = y1 % 2
x2=x1.t
y2=y1.t
# set the dimensions of l -layer dl network
layersdimensions = [2, 9, 9,1] #  4-layer model
# execute dl network with hidden activation=relu and sigmoid output function
parameters = l_layer_deepmodel(x2, y2, layersdimensions, hiddenactivationfunc='r
elu', outputactivationfunc="sigmoid",learningrate = 0.3,num_iterations = 2500, p
rint_cost = true)

2. spiral dataset with softmax activation     python

   the spiral data was used in my post [52]deep learning from first
   principles in python, r and octave     part 4 and was used to test the
   complete l-layer deep learning network with multi-class softmax
   activation at the output layer
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import make_classification, make_blobs
exec(open("dlfunctions51.py").read())

# create an input data set - taken from cs231n convolutional neural networks
# http://cs231n.github.io/neural-networks-case-study/
n = 100 # number of points per class
d = 2 # dimensionality
k = 3 # number of classes
x = np.zeros((n*k,d)) # data matrix (each row = single example)
y = np.zeros(n*k, dtype='uint8') # class labels
for j in range(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j*4,(j+1)*4,n) + np.random.randn(n)*0.2 # theta
  x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j

x1=x.t
y1=y.reshape(-1,1).t
numhidden=100 # no of hidden units in hidden layer
numfeats= 2 # dimensionality
numoutput = 3 # number of classes
# set the dimensions of the layers
layersdimensions=[numfeats,numhidden,numoutput]
parameters = l_layer_deepmodel(x1, y1, layersdimensions, hiddenactivationfunc='r
elu', outputactivationfunc="softmax",learningrate = 0.6,num_iterations = 9000, p
rint_cost = true)
## cost after iteration 0: 1.098759
## cost after iteration 1000: 0.112666
## cost after iteration 2000: 0.044351
## cost after iteration 3000: 0.027491
## cost after iteration 4000: 0.021898
## cost after iteration 5000: 0.019181
## cost after iteration 6000: 0.017832
## cost after iteration 7000: 0.017452
## cost after iteration 8000: 0.017161

3. mnist dataset with softmax activation     python

   in the code below, i execute stochastic id119 on the mnist
   training data of 60000. i used a mini-batch size of 1000. python takes
   about 40 minutes to crunch the data. in addition i also compute the
   confusion matrix and other metrics like accuracy, precision and recall
   for the mnist data set. i get an accuracy of 0.93 on the mnist test
   set. this accuracy can be improved by choosing more hidden layers or
   more hidden units and possibly also tweaking the learning rate and the
   number of epochs.
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import math
from sklearn.datasets import make_classification, make_blobs
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
exec(open("dlfunctions51.py").read())
exec(open("load_mnist.py").read())
# read the mnist training and test sets
training=list(read(dataset='training',path=".\\mnist"))
test=list(read(dataset='testing',path=".\\mnist"))
# create labels and pixel arrays
lbls=[]
pxls=[]
print(len(training))
#for i in range(len(training)):
for i in range(60000):
       l,p=training[i]
       lbls.append(l)
       pxls.append(p)
labels= np.array(lbls)
pixels=np.array(pxls)
y=labels.reshape(-1,1)
x=pixels.reshape(pixels.shape[0],-1)
x1=x.t
y1=y.t
# set the dimensions of the layers. the mnist data is 28x28 pixels= 784
# hence input layer is 784. for the 10 digits the softmax classifier
# has to handle 10 outputs
layersdimensions=[784, 15,9,10] # works very well,lr=0.01,mini_batch =1000, tota
l=20000
np.random.seed(1)
costs = []
# run stochastic id119 with learning rate=0.01, mini batch size=1000
# number of epochs=3000
parameters = l_layer_deepmodel_sgd(x1, y1, layersdimensions, hiddenactivationfun
c='relu', outputactivationfunc="softmax",learningrate = 0.01 ,mini_batch_size =1
000, num_epochs = 3000, print_cost = true)

# compute the confusion matrix on training set
# compute the training accuracy, precision and recall
proba=predict_proba(parameters, x1,outputactivationfunc="softmax")
#a2, cache = forwardpropagationdeep(x1, parameters)
#proba=np.argmax(a2, axis=0).reshape(-1,1)
a=confusion_matrix(y1.t,proba)
print(a)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
print('accuracy: {:.2f}'.format(accuracy_score(y1.t, proba)))
print('precision: {:.2f}'.format(precision_score(y1.t, proba,average="micro")))
print('recall: {:.2f}'.format(recall_score(y1.t, proba,average="micro")))

# read the test data
lbls=[]
pxls=[]
print(len(test))
for i in range(10000):
       l,p=test[i]
       lbls.append(l)
       pxls.append(p)
testlabels= np.array(lbls)
testpixels=np.array(pxls)
ytest=testlabels.reshape(-1,1)
xtest=testpixels.reshape(testpixels.shape[0],-1)
x1test=xtest.t
y1test=ytest.t

# compute the confusion matrix on test set
# compute the test accuracy, precision and recall
probatest=predict_proba(parameters, x1test,outputactivationfunc="softmax")
#a2, cache = forwardpropagationdeep(x1, parameters)
#proba=np.argmax(a2, axis=0).reshape(-1,1)
a=confusion_matrix(y1test.t,probatest)
print(a)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_sc
ore
print('accuracy: {:.2f}'.format(accuracy_score(y1test.t, probatest)))
print('precision: {:.2f}'.format(precision_score(y1test.t, probatest,average="mi
cro")))
print('recall: {:.2f}'.format(recall_score(y1test.t, probatest,average="micro"))
)

##1.  confusion matrix of training set
       0     1    2    3    4    5    6    7    8    9
## [[5854    0   19    2   10    7    0    1   24    6]
##  [   1 6659   30   10    5    3    0   14   20    0]
##  [  20   24 5805   18    6   11    2   32   37    3]
##  [   5    4  175 5783    1   27    1   58   60   17]
##  [   1   21    9    0 5780    0    5    2   12   12]
##  [  29    9   21  224    6 4824   18   17  245   28]
##  [   5    4   22    1   32   12 5799    0   43    0]
##  [   3   13  148  154   18    3    0 5883    4   39]
##  [  11   34   30   21   13   16    4    7 5703   12]
##  [  10    4    1   32  135   14    1   92  134 5526]]

##2. accuracy, precision, recall of  training set
## accuracy: 0.96
## precision: 0.96
## recall: 0.96

##3. confusion matrix of test set
       0     1    2    3    4    5    6    7    8    9
## [[ 954    1    8    0    3    3    2    4    4    1]
##  [   0 1107    6    5    0    0    1    2   14    0]
##  [  11    7  957   10    5    0    5   20   16    1]
##  [   2    3   37  925    3   13    0    8   18    1]
##  [   2    6    1    1  944    0    7    3    4   14]
##  [  12    5    4   45    2  740   24    8   42   10]
##  [   8    4    4    2   16    9  903    0   12    0]
##  [   4   10   27   18    5    1    0  940    1   22]
##  [  11   13    6   13    9   10    7    2  900    3]
##  [   8    5    1    7   50    7    0   20   29  882]]
##4. accuracy, precision, recall of  training set
## accuracy: 0.93
## precision: 0.93
## recall: 0.93

4. random dataset with sigmoid activation     r code

   this is the random data set used in the python code above which was
   saved as a csv. the code is used to test a l -layer dl network with
   sigmoid activation in r.
source("dlfunctions5.r")
# read the random data set
z <- as.matrix(read.csv("data.csv",header=false))
x <- z[,1:2]
y <- z[,3]
x <- t(x)
y <- t(y)
# set the dimensions of the  layer
layersdimensions = c(2, 9, 9,1)

# run id119 on the data set with relu hidden unit activation
# sigmoid activation unit in the output layer
retvals = l_layer_deepmodel(x, y, layersdimensions,
                            hiddenactivationfunc='relu',
                            outputactivationfunc="sigmoid",
                            learningrate = 0.3,
                            numiterations = 5000,
                            print_cost = true)
#plot the cost vs iterations
iterations <- seq(0,5000,1000)
costs=retvals$costs
df=data.frame(iterations,costs)
ggplot(df,aes(x=iterations,y=costs)) + geom_point() + geom_line(color="blue") +
 ggtitle("costs vs iterations") + xlab("iterations") + ylab("loss")

5. spiral dataset with softmax activation     r

   the spiral data set used in the python code above, is reused to test
   multi-class classification with softmax.
source("dlfunctions5.r")
z <- as.matrix(read.csv("spiral.csv",header=false))

# setup the data
x <- z[,1:2]
y <- z[,3]
x <- t(x)
y <- t(y)

# initialize number of features, number of hidden units in hidden layer and
# number of classes
numfeats<-2 # no features
numhidden<-100 # no of hidden units
numoutput<-3 # no of classes

# set the layer dimensions
layersdimensions = c(numfeats,numhidden,numoutput)

# perform id119 with relu activation unit for hidden layer
# and softmax activation in the output
retvals = l_layer_deepmodel(x, y, layersdimensions,
                            hiddenactivationfunc='relu',
                            outputactivationfunc="softmax",
                            learningrate = 0.5,
                            numiterations = 9000,
                            print_cost = true)
#plot cost vs iterations
iterations <- seq(0,9000,1000)
costs=retvals$costs
df=data.frame(iterations,costs)
ggplot(df,aes(x=iterations,y=costs)) + geom_point() + geom_line(color="blue") +
 ggtitle("costs vs iterations") + xlab("iterations") + ylab("costs")

6. mnist dataset with softmax activation     r

   the code below executes a l     layer deep learning network with softmax
   output activation, to classify the 10 handwritten digits from mnist
   with stochastic id119. the entire 60000 data set was used to
   train the data. r takes almost 8 hours to process this data set with a
   mini-batch size of 1000.  the use of    for    loops is limited to
   iterating through epochs, mini batches and for creating the mini
   batches itself. all other code is vectorized. yet, it seems to crawl.
   most likely the use of    lists    in r, to return multiple values is
   performance intensive. some day, i will try to profile the code, and
   see where the issue is. however the code works!

   having said that, the confusion matrix in r dumps a lot of interesting
   statistics! there is a bunch of statistical measures for each class.
   for e.g. the balanced accuracy for the digits    6    and    9    is around
   50%. looks like, the classifier is confused by the fact that 6 is
   inverted 9 and vice-versa. the accuracy on the test data set is just
   around 75%. i could have played around with the number of layers,
   number of hidden units, learning rates, epochs etc to get a much higher
   accuracy. but since each test took about 8+ hours, i may work on this,
   some other day!
source("dlfunctions5.r")
source("mnist.r")
#load the mnist data
load_mnist()
show_digit(train$x[2,])
#set the layer dimensions
layersdimensions=c(784, 15,9, 10) # works at 1500
x <- t(train$x)
x <- x[,1:60000]
y <-train$y
y1 <- y[1:60000]
y2 <- as.matrix(y1)
y=t(y2)

# subset 32768 random samples from mnist
permutation = c(sample(2^15))
# randomly shuffle the training data
x1 = x[, permutation]
y1 = y[1, permutation]
y2 <- as.matrix(y1)
y1=t(y2)

# execute stochastic id119 on the entire training set
# with softmax activation
retvalssgd= l_layer_deepmodel_sgd(x1, y1, layersdimensions,
                            hiddenactivationfunc='relu',
                            outputactivationfunc="softmax",
                            learningrate = 0.05,
                            mini_batch_size = 512,
                            num_epochs = 1,
                            print_cost = true)

# compute the confusion matrix
library(caret)
library(e1071)
predictions=predictproba(retvalssgd[['parameters']], x,hiddenactivationfunc='rel
u',
                   outputactivationfunc="softmax")
confusionmatrix(predictions,y)
# confusion matrix on the training set
> confusionmatrix(predictions,y)
confusion matrix and statistics

          reference
prediction    0    1    2    3    4    5    6    7    8    9
         0 5738    1   21    5   16   17    7   15    9   43
         1    5 6632   21   24   25    3    2   33   13  392
         2   12   32 5747  106   25   28    3   27   44 4779
         3    0   27   12 5715    1   21    1   20    1   13
         4   10    5   21   18 5677    9   17   30   15  166
         5  142   21   96  136   93 5306 5884   43   60  413
         6    0    0    0    0    0    0    0    0    0    0
         7    6    9   13   13    3    4    0 6085    0   55
         8    8   12    7   43    1   32    2    7 5703   69
         9    2    3   20   71    1    1    2    5    6   19

overall statistics

               accuracy : 0.777
                 95% ci : (0.7737, 0.7804)
    no information rate : 0.1124
    p-value [acc > nir] : < 2.2e-16

                  kappa : 0.7524
 mcnemar's test p-value : na

statistics by class:

                     class: 0 class: 1 class: 2 class: 3 class: 4 class: 5 class
: 6
sensitivity           0.96877   0.9837  0.96459  0.93215  0.97176  0.97879  0.00
000
specificity           0.99752   0.9903  0.90644  0.99822  0.99463  0.87380  1.00
000
pos pred value        0.97718   0.9276  0.53198  0.98348  0.95124  0.43513
nan
neg pred value        0.99658   0.9979  0.99571  0.99232  0.99695  0.99759  0.90
137
prevalence            0.09872   0.1124  0.09930  0.10218  0.09737  0.09035  0.09
863
detection rate        0.09563   0.1105  0.09578  0.09525  0.09462  0.08843  0.00
000
detection prevalence  0.09787   0.1192  0.18005  0.09685  0.09947  0.20323  0.00
000
balanced accuracy     0.98314   0.9870  0.93551  0.96518  0.98319  0.92629  0.50
000
                     class: 7 class: 8  class: 9
sensitivity            0.9713  0.97471 0.0031938
specificity            0.9981  0.99666 0.9979464
pos pred value         0.9834  0.96924 0.1461538
neg pred value         0.9967  0.99727 0.9009521
prevalence             0.1044  0.09752 0.0991500
detection rate         0.1014  0.09505 0.0003167
detection prevalence   0.1031  0.09807 0.0021667
balanced accuracy      0.9847  0.98568 0.5005701

# confusion matrix on the training set xtest <- t(test$x) xtest <- xtest[,1:1000
0] ytest <-test$y ytest1 <- ytest[1:10000] ytest2 <- as.matrix(ytest1) ytest=t(y
test2)
confusion matrix and statistics

          reference
prediction    0    1    2    3    4    5    6    7    8    9
         0  950    2    2    3    0    6    9    4    7    6
         1    3 1110    4    2    9    0    3   12    5   74
         2    2    6  965   21    9   14    5   16   12  789
         3    1    2    9  908    2   16    0   21    2    6
         4    0    1    9    5  938    1    8    6    8   39
         5   19    5   25   35   20  835  929    8   54   67
         6    0    0    0    0    0    0    0    0    0    0
         7    4    4    7   10    2    4    0  952    5    6
         8    1    5    8   14    2   16    2    3  876   21
         9    0    0    3   12    0    0    2    6    5    1

overall statistics

               accuracy : 0.7535
                 95% ci : (0.7449, 0.7619)
    no information rate : 0.1135
    p-value [acc > nir] : < 2.2e-16

                  kappa : 0.7262
 mcnemar's test p-value : na

statistics by class:

                     class: 0 class: 1 class: 2 class: 3 class: 4 class: 5 class
: 6
sensitivity            0.9694   0.9780   0.9351   0.8990   0.9552   0.9361   0.0
000
specificity            0.9957   0.9874   0.9025   0.9934   0.9915   0.8724   1.0
000
pos pred value         0.9606   0.9083   0.5247   0.9390   0.9241   0.4181
nan
neg pred value         0.9967   0.9972   0.9918   0.9887   0.9951   0.9929   0.9
042
prevalence             0.0980   0.1135   0.1032   0.1010   0.0982   0.0892   0.0
958
detection rate         0.0950   0.1110   0.0965   0.0908   0.0938   0.0835   0.0
000
detection prevalence   0.0989   0.1222   0.1839   0.0967   0.1015   0.1997   0.0
000
balanced accuracy      0.9825   0.9827   0.9188   0.9462   0.9733   0.9043   0.5
000
                     class: 7 class: 8  class: 9
sensitivity            0.9261   0.8994 0.0009911
specificity            0.9953   0.9920 0.9968858
pos pred value         0.9577   0.9241 0.0344828
neg pred value         0.9916   0.9892 0.8989068
prevalence             0.1028   0.0974 0.1009000
detection rate         0.0952   0.0876 0.0001000
detection prevalence   0.0994   0.0948 0.0029000
balanced accuracy      0.9607   0.9457 0.4989384

7. random dataset with sigmoid activation     octave

   the octave code below uses the random data set used by python. the code
   below implements a l-layer deep learning with sigmoid activation.
source("dl5functions.m")
# read the data
data=csvread("data.csv");

x=data(:,1:2);
y=data(:,3);
#set the layer dimensions
layersdimensions = [2 9 7  1]; #tanh=-0.5(ok), #relu=0.1 best!
# perform id119
[weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
                               hiddenactivationfunc='relu',
                               outputactivationfunc="sigmoid",
                               learningrate = 0.1,
                               numiterations = 10000);
# plot cost vs iterations
plotcostvsiterations(10000,costs);

8. spiral dataset with softmax activation     octave

   the  code below uses the spiral data set used by python above. the code
   below implements a l-layer deep learning with softmax activation.
# read the data
data=csvread("spiral.csv");

# setup the data
x=data(:,1:2);
y=data(:,3);

# set the number of features, number of hidden units in hidden layer and number
of classess
numfeats=2; #no features
numhidden=100; # no of hidden units
numoutput=3; # no of  classes
# set the layer dimensions
layersdimensions = [numfeats numhidden  numoutput];
#perform id119 with softmax activation unit
[weights biases costs]=l_layer_deepmodel(x', y', layersdimensions,
                               hiddenactivationfunc='relu',
                               outputactivationfunc="softmax",
                               learningrate = 0.1,
                               numiterations = 10000);

9. mnist dataset with softmax activation     octave

   the code below implements a l-layer deep learning network in octave
   with softmax output activation unit, for classifying the 10 handwritten
   digits in the mnist dataset. unfortunately, octave can only index to
   around 10000 training at a time,  and i was getting an error    error:
   out of memory or dimension too large for octave   s index type error:
   called from      , when i tried to create a batch size of 20000.  so i had
   to come with a work around to create a batch size of 10000 (randomly)
   and then use a mini-batch of 1000 samples and execute stochastic
   id119. the performance was good. octave takes about 15
   minutes, on a batch size of 10000 and a mini batch of 1000.

   i thought if the performance was not good, i could iterate through
   these random batches and refining the gradients as follows
# pseudo code that could be used since octave only allows 10k batches
# at a time
# randomly create weights
[weights biases] = initialize_weights()
for i=1:k
    # create a random permutation and create a random batch
    permutation = randperm(10000);
    x=trainx(permutation,:);
    y=trainy(permutation,:);
    # compute weights from sgd and update weights in the next batch update
    [weights biases costs]=l_layer_deepmodel_sgd(x,y,mini_bactch=1000,weights, b
iases,...);
    ...
endfor
# load the mnist data
load('./mnist/mnist.txt.gz');
#create a random permutatation from 60k
permutation = randperm(10000);
disp(length(permutation));

# use this 10k as the batch
x=trainx(permutation,:);
y=trainy(permutation,:);

# set layer dimensions
layersdimensions=[784, 15, 9, 10];

# run stochastic id119 with batch size=10k and mini_batch_size=1000
[weights biases costs]=l_layer_deepmodel_sgd(x', y', layersdimensions,
                       hiddenactivationfunc='relu',
                       outputactivationfunc="softmax",
                       learningrate = 0.01,
                       mini_batch_size = 2000, num_epochs = 5000);

9. final thoughts

   here are some of my final thoughts after working on python, r and
   octave in this series and in other projects
   1. python, with its highly optimized numpy library, is ideally suited
   for creating deep learning models, which have a lot of matrix
   manipulations. python is a real workhorse when it comes to deep
   learning computations.
   2. r is somewhat clunky in comparison to its cousin python in handling
   matrices or in returning multiple values. but r   s statistical
   libraries, dplyr, and ggplot are really superior to the python peers.
   also, i find r handles  dataframes,  much better than python.
   3. octave is a no-nonsense,minimalist language which is very efficient
   in handling matrices. it is ideally suited for implementing machine
   learning and deep learning from scratch. but octave has its problems
   and cannot handle large matrix sizes, and also lacks the statistical
   libaries of r and python. they possibly exist in its sibling, matlab

   feel free to clone/download the code from  github at
   [53]deeplearning-part5.

conclusion

   building a deep learning network from scratch is quite challenging,
   time-consuming but nevertheless an exciting task.  while the statements
   in the different languages for manipulating matrices, summing up
   columns, finding columns which have ones don   t take more than a single
   statement, extreme care has to be taken to ensure that the statements
   work well for any dimension.  the lessons learnt from creating l -layer
   deep learning network  are many and well worth it. give it a try!

   hasta la vista! i   ll be back, so stick around!
   watch this space!

   references
   1. [54]deep learning specialization
   2. [55]neural networks for machine learning
   3. [56]cs231 convolutional neural networks for visual recognition
   4. [57]eli bendersky   s website     the softmax function and its
   derivative

   also see
   1. [58]my book    practical machine learning with r and python    on amazon
   2. [59]presentation on wireless technologies     part 1
   3. [60]exploring quantum gate operations with qcsimulator
   4. [61]what   s up watson? using ibm watson   s qaapi with bluemix,
   nodeexpress     part 1
   5. [62]tws-4: gossip protocol: epidemics and rumors to the rescue
   6. [63]cricketr plays the odis!
   7. [64]   is it an animal? is it an insect?    in android
   8. [65]the 3rd paperback & kindle editions of my books on cricket, now
   on amazon
   9. [66]deblurring with opencv: weiner filter reloaded
   10. [67]googlyplus: yorkr analyzes ipl players, teams, matches with
   plots and tables

   to see all posts click [68]index of posts


rate this:

share:

     *
     *
     * [69]pocket
     * [70]tweet
     *

       iframe:
       [71]https://www.reddit.com/static/button/button1.html?newwindow=tru
       e&width=120&url=https%3a%2f%2fgigadom.in%2f2018%2f03%2f23%2fdeep-le
       arning-from-first-principles-in-python-r-and-octave-part-5%2f&title
       =deep%20learning%20from%20first%20principles%20in%20python%2c%20r%2
       0and%20octave%20%e2%80%93%20part%205

     * [72][pinit_fg_en_rect_gray_20.png]
     * [73]more
     *

     * [74]email
     * [75]share on tumblr
     *
     * [76]telegram
     * [77]print
     *
     *

like this:

   like loading...

related

     * tagged
     * [78]mnist
     * [79]python
     * [80]r
     * [81]r language
     * [82]r project
     * [83]sigmoid
     * [84]softmax

published by tinniam v ganesh

   visionary, thought leader and pioneer with 27+ years of experience in
   the software industry. [85]view all posts by tinniam v ganesh
   published march 23, 2018january 16, 2019

post navigation

   [86]previous post presentation on    machine learning in plain english    
   part 3
   [87]next post deep learning from first principles in python, r and
   octave     part 6

6 thoughts on    deep learning from first principles in python, r and octave    
part 5   

    1. pingback: [88]machine learning with r | mfepe
    2. pingback: [89]deep learning from first principles in python, r and
       octave     part 6 | giga thoughts    
    3. pingback: [90]deep learning from first principles in python, r and
       octave     part 8 | giga thoughts    
    4. pingback: [91]my presentations on    elements of neural networks &
       deep learning    -parts 4,5 | giga thoughts    
    5. pingback: [92]take 4+: presentations on    elements of neural
       networks and deep learning        parts 1-8 | giga thoughts    
    6. pingback: [93]analyzing t20 matches with yorkpy templates     giga
       thoughts    

leave a reply [94]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [95]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [96]log out /
   [97]change )
   google photo

   you are commenting using your google account. ( [98]log out /
   [99]change )
   twitter picture

   you are commenting using your twitter account. ( [100]log out /
   [101]change )
   facebook photo

   you are commenting using your facebook account. ( [102]log out /
   [103]change )
   [104]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

connect with me:

     * [105]linkedin
     * [106]github
     * [107]twitter

   search for: ____________________ search

blog stats

     * 440,452 hits

visitors to giga thoughts (click to see details)

   [108]map

   [109]follow giga thoughts     on wordpress.com

popular posts

     * [110]working with node.js and postgresql
     * [111]simplifying ml: impact of degree of polynomial degree on bias
       & variance and other insights
     * [112]introducing cricketr! : an r package to analyze performances
       of cricketers
     * [113]re-introducing cricketr! : an r package to analyze
       performances of cricketers
     * [114]experiments with deblurring using opencv
     * [115]deep learning from first principles in python, r and octave -
       part 1
     * [116]my presentations on    elements of neural networks & deep
       learning    -parts 4,5
     * [117]practical machine learning with r and python     part 5
     * [118]introducing cricpy:a python package to analyze performances of
       cricketers
     * [119]r vs python: different similarities and similar differences

category cloud

   [120]analytics [121]android [122]android app [123]app [124]batsman
   [125]big data [126]bluemix [127]bowler [128]cloud computing
   [129]cricket [130]cricketr [131]cricsheet [132]data mining [133]deep
   learning [134]distributed systems [135]git [136]github [137]gradient
   descent [138]id75 [139]id28 [140]machine
   learning [141]neural networks [142]python [143]r [144]r language [145]r
   markdown [146]r package [147]r project [148]technology [149]yorkr

follow blog via email

   join 1,212 other followers

   ____________________

   (button) follow

subscribe

   [150]rss feed  [151]rss - posts

giga thoughts community

     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

archives

   archives [select month_______]

navigate

     * [152]home
     * [153]index of posts
     * [154]books i authored
     * [155]who am i?
     * [156]published posts
     * [157]about giga thoughts   

latest posts

     * [158]analyzing performances of cricketers using cricketr template
       march 30, 2019
     * [159]the clash of the titans in test and odi cricket march 15, 2019
     * [160]analyzing t20 matches with yorkpy templates march 10, 2019
     * [161]yorkpy takes a hat-trick, bowls out intl. t20s, bbl and
       natwest t20!!! march 3, 2019
     * [162]pitching yorkpy     in the block hole     part 4 february 26, 2019
     * [163]take 4+: presentations on    elements of neural networks and
       deep learning        parts 1-8 february 16, 2019
     * [164]pitching yorkpy   swinging away from the leg stump to ipl    
       part 3 february 3, 2019
     * [165]pitching yorkpy   on the middle and outside off-stump to ipl    
       part 2 january 27, 2019

   [166]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [167]cancel reblog post

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [168]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [169]likes-master

   %d bloggers like this:

references

   visible links
   1. https://gigadom.in/feed/
   2. https://gigadom.in/comments/feed/
   3. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/feed/
   4. https://gigadom.in/2018/03/09/presentation-on-machine-learning-in-plain-english-part-3/
   5. https://gigadom.in/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/&for=wpcom-auto-discovery
   8. https://gigadom.in/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#content
  11. https://gigadom.in/
  12. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
  13. https://github.com/tvganesh
  14. https://twitter.com/tvganesh_85
  15. https://gigadom.in/
  16. https://gigadom.in/aa-2/
  17. https://gigadom.in/and-you-are/
  18. https://gigadom.in/who-am-i/
  19. https://gigadom.in/published-posts/
  20. https://gigadom.in/about-giga-thoughts/
  21. https://gigadom.in/author/gigadom/
  22. https://gigadom.in/category/id26/
  23. https://gigadom.in/category/backward-propagation/
  24. https://gigadom.in/category/cost-function/
  25. https://gigadom.in/category/deep-learning/
  26. https://gigadom.in/category/gradient-descent/
  27. https://gigadom.in/category/logistic-regression/
  28. https://gigadom.in/tag/mnist/
  29. https://gigadom.in/category/numpy/
  30. https://gigadom.in/category/python-2/
  31. https://gigadom.in/tag/r/
  32. https://gigadom.in/category/r-language/
  33. https://gigadom.in/category/r-markdown/
  34. https://gigadom.in/category/r-package/
  35. https://gigadom.in/tag/r-project/
  36. https://gigadom.in/tag/sigmoid/
  37. https://gigadom.in/tag/softmax/
  38. https://gigadom.in/category/technology/
  39. https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
  40. https://gigadom.wordpress.com/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/
  41. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  42. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  43. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  44. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  45. https://www.youtube.com/watch?v=qyeetztzajk&t=136s
  46. https://github.com/tvganesh/deeplearning-part5
  47. https://www.amazon.com/dp/1791596177
  48. https://www.amazon.com/dp/b07lbg542l
  49. https://www.amazon.com/dp/1983035661
  50. https://www.amazon.com/dp/b07dfkscwz
  51. https://gigadom.wordpress.com/2018/01/30/deep-learning-from-first-principles-in-python-r-and-octave-part-3/
  52. https://gigadom.wordpress.com/2018/02/26/deep-learning-from-first-principles-in-python-r-and-octave-part-4/
  53. https://github.com/tvganesh/deeplearning-part5
  54. https://www.coursera.org/specializations/deep-learning
  55. https://www.coursera.org/learn/neural-networks
  56. http://cs231n.github.io/neural-networks-case-study/
  57. https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
  58. https://gigadom.wordpress.com/2017/12/05/my-book-practical-machine-learning-with-r-and-python-on-amazon/
  59. https://gigadom.wordpress.com/2013/07/24/presentation-on-wireless-technologies-part-1/
  60. https://gigadom.wordpress.com/2016/06/05/exploring-quantum-gate-operations-with-qcsimulator/
  61. https://gigadom.wordpress.com/2014/10/11/whats-up-watson-using-ibm-watsons-qaapi-with-bluemix-nodeexpress-part-1/
  62. https://gigadom.wordpress.com/2015/04/04/tws-4-gossip-protocol-epidemics-and-rumors-to-the-rescue/
  63. https://gigadom.wordpress.com/2015/08/02/cricketr-plays-the-odis/
  64. https://gigadom.wordpress.com/2013/07/05/is-it-an-animal-is-it-an-insect-in-android/
  65. https://gigadom.wordpress.com/2017/12/16/the-3rd-paperback-edition-of-my-books-on-cricket-on-amazon/
  66. https://gigadom.wordpress.com/2012/05/11/deblurring-with-opencv-weiner-filter-reloaded/
  67. https://gigadom.wordpress.com/2017/01/05/googlyplus-yorkr-analyzes-ipl-players-teams-matches-with-plots-and-tables/
  68. https://gigadom.wordpress.com/aa-2/
  69. https://getpocket.com/save
  70. https://twitter.com/share
  71. https://www.reddit.com/static/button/button1.html?newwindow=true&width=120&url=https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/&title=deep learning from first principles in python, r and octave     part 5
  72. https://www.pinterest.com/pin/create/button/?url=https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/&media=https://gigadom.files.wordpress.com/2018/03/mnist-r-plot1.png&description=deep learning from first principles in python, r and octave     part 5
  73. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
  74. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/?share=email
  75. https://www.tumblr.com/share
  76. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/?share=telegram
  77. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#print
  78. https://gigadom.in/tag/mnist/
  79. https://gigadom.in/tag/python/
  80. https://gigadom.in/tag/r/
  81. https://gigadom.in/tag/r-language-2/
  82. https://gigadom.in/tag/r-project/
  83. https://gigadom.in/tag/sigmoid/
  84. https://gigadom.in/tag/softmax/
  85. https://gigadom.in/author/gigadom/
  86. https://gigadom.in/2018/03/09/presentation-on-machine-learning-in-plain-english-part-3/
  87. https://gigadom.in/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
  88. https://tuanvanle.wordpress.com/2017/04/19/machine-learning-with-r/
  89. https://gigadom.wordpress.com/2018/04/16/deep-learning-from-first-principles-in-python-r-and-octave-part-6/
  90. https://gigadom.wordpress.com/2018/05/06/deep-learning-from-first-principles-in-python-r-and-octave-part-8/
  91. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
  92. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
  93. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
  94. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#respond
  95. https://gravatar.com/site/signup/
  96. javascript:highlandercomments.doexternallogout( 'wordpress' );
  97. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
  98. javascript:highlandercomments.doexternallogout( 'googleplus' );
  99. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 100. javascript:highlandercomments.doexternallogout( 'twitter' );
 101. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 102. javascript:highlandercomments.doexternallogout( 'facebook' );
 103. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 104. javascript:highlandercomments.cancelexternalwindow();
 105. https://www.linkedin.com/in/tinniam-v-ganesh-tv-0223817/
 106. https://github.com/tvganesh
 107. https://twitter.com/tvganesh_85
 108. https://www.revolvermaps.com/?target=enlarge&i=0z8r51l0ucz
 109. https://gigadom.in/
 110. https://gigadom.in/2014/07/20/working-with-node-js-and-postgresql/
 111. https://gigadom.in/2014/01/04/simplifying-ml-impact-of-degree-of-polynomial-degree-on-bias-variance-and-other-insights/
 112. https://gigadom.in/2015/07/04/introducing-cricketr-a-r-package-to-analyze-performances-of-cricketers/
 113. https://gigadom.in/2016/05/14/re-introducing-cricketr-an-r-package-to-analyze-performances-of-cricketers/
 114. https://gigadom.in/2011/11/09/experiments-with-deblurring-using-opencv/
 115. https://gigadom.in/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/
 116. https://gigadom.in/2019/01/15/my-presentations-on-elements-of-neural-networks-deep-learning-parts-45/
 117. https://gigadom.in/2017/11/07/practical-machine-learning-with-r-and-python-part-5/
 118. https://gigadom.in/2018/10/28/introducing-cricpya-python-package-to-analyze-performances-of-cricketrs/
 119. https://gigadom.in/2017/05/22/r-vs-python-different-similarities-and-similar-differences/
 120. https://gigadom.in/category/analytics/
 121. https://gigadom.in/category/android/
 122. https://gigadom.in/category/android-app/
 123. https://gigadom.in/category/app/
 124. https://gigadom.in/category/batsman/
 125. https://gigadom.in/category/big-data/
 126. https://gigadom.in/category/bluemix/
 127. https://gigadom.in/category/bowler/
 128. https://gigadom.in/category/cloud-computing/
 129. https://gigadom.in/category/cricket/
 130. https://gigadom.in/category/cricketr/
 131. https://gigadom.in/category/cricsheet/
 132. https://gigadom.in/category/data-mining/
 133. https://gigadom.in/category/deep-learning/
 134. https://gigadom.in/category/distributed-systems/
 135. https://gigadom.in/category/git/
 136. https://gigadom.in/category/github/
 137. https://gigadom.in/category/gradient-descent/
 138. https://gigadom.in/category/linear-regression/
 139. https://gigadom.in/category/logistic-regression/
 140. https://gigadom.in/category/machine-learning/
 141. https://gigadom.in/category/neural-networks/
 142. https://gigadom.in/category/python-2/
 143. https://gigadom.in/category/r/
 144. https://gigadom.in/category/r-language/
 145. https://gigadom.in/category/r-markdown/
 146. https://gigadom.in/category/r-package/
 147. https://gigadom.in/category/r-project/
 148. https://gigadom.in/category/technology/
 149. https://gigadom.in/category/yorkr/
 150. https://gigadom.in/feed/
 151. https://gigadom.in/feed/
 152. https://gigadom.in/
 153. https://gigadom.in/aa-2/
 154. https://gigadom.in/and-you-are/
 155. https://gigadom.in/who-am-i/
 156. https://gigadom.in/published-posts/
 157. https://gigadom.in/about-giga-thoughts/
 158. https://gigadom.in/2019/03/30/analyzing-performances-of-cricketers-using-cricketr-template/
 159. https://gigadom.in/2019/03/15/the-clash-of-the-titans-in-test-and-odi-cricket/
 160. https://gigadom.in/2019/03/10/analyzing-t20-matches-with-yorkpy-templates/
 161. https://gigadom.in/2019/03/03/yorkpy-takes-a-hat-trick-bowls-out-intl-t20s-bbl-and-natwest-t20/
 162. https://gigadom.in/2019/02/26/pitching-yorkpy-in-the-block-hole-part-4/
 163. https://gigadom.in/2019/02/16/take-4-presentations-on-elements-of-neural-networks-and-deep-learning-parts-1-8/
 164. https://gigadom.in/2019/02/03/pitching-yorkpyswinging-away-from-the-leg-stump-to-ipl-part-3/
 165. https://gigadom.in/2019/01/27/pitching-yorkpyon-the-middle-and-outside-off-stump-to-ipl-part-2/
 166. https://wordpress.com/?ref=footer_blog
 167. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 168. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#cancel
 169. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 171. https://gigadom.in/
 172. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#comment-form-guest
 173. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#comment-form-load-service:wordpress.com
 174. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#comment-form-load-service:twitter
 175. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/#comment-form-load-service:facebook
 176. http://lemanshots.wordpress.com/
 177. https://vinodsblog.com/about
 178. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 179. http://friartuck2012.wordpress.com/
 180. http://webastion.wordpress.com/
 181. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 182. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 183. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 184. https://gigadom.in/2018/03/23/deep-learning-from-first-principles-in-python-r-and-octave-part-5/
 185. http://micvdotin.wordpress.com/
