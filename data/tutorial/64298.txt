   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

semi-supervised learning with id3 (gans)

   [16]go to the profile of thalles silva
   [17]thalles silva (button) blockedunblock (button) followfollowing
   dec 27, 2017
   [1*2b43ejochp8strrokcylaa.jpeg]

   if you ever heard or studied about deep learning, you probably heard
   about [18]mnist, [19]svhn, [20]id163, [21]pascalvoc and others. each
   of these datasets has one thing in common. they consist of hundreds and
   thousands of labeled data. in other words, these collections are
   composed of (x,y) pairs where (x) is the raw data, an image matrix for
   instance, and (y) is a description of what that data point (x)
   represents.

   take the mnist dataset as an example. each of the 60,000 data points is
   a (input, label) pair. input is a 28x28 grayscale image and label is a
   sign representing what the input is. for the mnist dataset, an input
   image may be a zero, one, two, three and so on up to 10 possible
   categories.
   [1*nqqavd952xasl59aesdizw.png]
   mnist pixel representation.

   the most common usage of datasets like these is to develop supervised
   models. to train such algorithms, we usually provide a tremendous
   amount of data samples.

   supervised learning has been the center of most researching in deep
   learning. but, the necessity of creating models capable of learning
   from fewer data is increasing faster.

   with that in mind, semi-supervised learning is a technique in which
   both labeled and unlabeled data are used to train a classifier.

   this type of classifier takes a tiny portion of labeled data and a much
   larger amount of unlabeled data (from the same domain). the goal is to
   combine these sources of data to train a deep convolution neural
   networks (did98) to learn an inferred function capable of mapping a new
   datapoint to its desirable outcome.

   in this frontier, we present a gan model to classify street view house
   numbers using a very small labeled training set. in fact, the model
   uses roughly 1.3% of the original svhn training labels i.e. 1000 (one
   thousand) labeled examples. we use some of the techniques described in
   the paper [22]improved techniques for training gans from openai.

   if you are not familiar with image generation gans, refer to [23]a
   short introduction to id3. this article
   refers to some of the contents described in that piece. the complete
   code notebook can found [24]here.

intuition

   when building a [25]gan for generating images, we trained both         the
   generator and the discriminator at the same time. after training, we
   could discard the discriminator because we used it only for training
   the generator.
   [1*hflbpfjuiwal5jz45qtjfw.png]
   semi-supervided learning gan architecture for an 11 class
   classification problem.

   for semi-supervised learning, we need to transform the discriminator
   into a multi-class classifier. this new model has to be able to
   generalize well on the test set, even through we do not have many
   labeled examples for training.

   additionally, this time, by the end of training, we can actually throw
   away the generator. note that the roles changed. now, the generator is
   only used for helping the discriminator during training.

   putting it differently, the generator acts as a different source of
   information from which the discriminator gets raw unlabeled training
   data. as we will see, this unlabelled data is key to improve the
   discriminator performance.

   also, for a regular image generation gan, the discriminator has only
   one role. compute the id203 of whether its inputs are real or
   not         let   s call it the gan problem.

   however, to turn the discriminator into a semi-supervised classifier,
   besides to the gan problem, the discriminator also has to learn the
   probabilities of each of the original dataset classes.

   in other words, for each input image, the discriminator has to learn
   the probabilities of it being a one, two, three and so on.

   recall that for an [26]image generation gan discriminator, we have a
   single sigmoid unit output. this value represents the id203 of an
   input image being real (value close to 1), or fake (value near 0 ).

   put in other words, from the discriminator point of view, values close
   to 1 means that the samples are likely to come from the training set.
   likewise, value near 0 means a higher change that the samples come from
   the generator network.

   by using this id203, the discriminator is able to send a signal
   back to the generator. this signal allows the generator to adapt its
   parameters during training making it possible to improve its
   capabilities of creating realistic images.

   we have to convert the discriminator ([27]from the previous gan) into
   an 11 class classifier. to do that, we can turn its sigmoid output into
   a softmax with 11 class outputs. the first 10 for the individual class
   probabilities of the svhn dataset (zero to nine), and the 11th class
   for all the fake images that come from the generator.

   note that if we set the 11th class id203 to 0, then the sum of
   the first 10 probabilities represents the same id203 computed
   using the sigmoid function.

   finally, we need to setup the losses in such a way that the
   discriminator can do both:

   - (i) help the generator learning to produce realistic images. to do
   that, we have to instruct the discriminator to distinguish between real
   and fake samples.

   - (ii) use the generator   s images, along with the labeled and unlabeled
   training data, to help classify the dataset.

   to summarize, the discriminator has three different sources of training
   data.
     * real images with labels. these are image label pairs like in any
       regular supervised classification problem.
     * real images without labels. for those, the classifier only learns
       that these images are real.
     * images from the generator. to these ones, the discriminator learns
       to classify as fake.

   the combination of these different sources of data will make the
   classifier able to learn from a broader perspective. that in turn,
   allows the model to perform id136 much more precisely than it would
   be if only using the 1000 labeled examples for training.

generator

   the generator follows a very standard implementation described in the
   [28]dcgan paper. this approach consists of taking a random vector z as
   input. reshape it to a 4d tensor and then feed it to a sequence of
   transpose convolutions, batch id172 (bn) and leaky relu
   operations.

   this sequence of computations increases the spatial dimensions of the
   input vector while reduces its number of channels. as a result, the
   network outputs a 32x32x3 rgb tensor shape squashed between values of
   -1 and 1 through the hyperbolic tangent function.
def generator(z, output_dim, reuse=false, alpha=0.2, training=true, size_mult=12
8):
    with tf.variable_scope('generator', reuse=reuse):
        # first fully connected layer
        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)
        # reshape it to start the convolutional stack
        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))
        x1 = tf.layers.batch_id172(x1, training=training)
        x1 = tf.maximum(alpha * x1, x1)
        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding
='same')
        x2 = tf.layers.batch_id172(x2, training=training)
        x2 = tf.maximum(alpha * x2, x2)
        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='sa
me')
        x3 = tf.layers.batch_id172(x3, training=training)
        x3 = tf.maximum(alpha * x3, x3)
        # output layer
        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, paddin
g='same')
        out = tf.tanh(logits)
        return out

discriminator

   the discriminator, now a multi-class classifier, is the most relevant
   network. here, we setup an also similar dcgan architecture in which we
   use a stack of convolutions with bn and relu.

   we use strided convolutions for reducing the dimensions of the
   feature-vectors. note that not all convolutions perform this type of
   computation. when we want to keep the feature vector   s dimensions
   intact, we use strides of 1 otherwise, we use strides of 2. finally, to
   stabilize learning we make extensive use of bn (except the first layer
   of the network).

   the 2d convolution window (kernel or filter) is set to have a width and
   height of 3 across all the convolutions. also, note that we have some
   layers with dropout. it is important to understand that our
   discriminator behaves (in part) like any other regular classifier.
   because of that, it may suffer from the same problems any classifier
   would if not well designed.

   when training a big classifier on a very limited dataset, one of the
   most likely drawbacks one might encounter, is the immense of
   overfitting. one thing to watch on    over trained    classifiers is that
   they typically show a notably difference between the training error
   (smaller) and the testing error (higher).

   this situation shows that the model did a good job capturing the
   structure of the training dataset. however, because it believed too
   much in the training data, it fails to generalize for unseen examples.

   to prevent that, we make an extensive usage of id173 through
   dropout. even for the first layer of the network.
def discriminator(x, reuse=false, alpha=0.2, drop_rate=0., num_classes=10, size_
mult=64):
    with tf.variable_scope('discriminator', reuse=reuse):
        x = tf.layers.dropout(x, rate=drop_rate/2.5)
        # input layer is ?x32x32x3
        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')
        relu1 = tf.maximum(alpha * x1, x1)
        relu1 = tf.layers.dropout(relu1, rate=drop_rate) # [?x16x16x?]
        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')
        bn2 = tf.layers.batch_id172(x2, training=true) # [?x8x8x?]
        relu2 = tf.maximum(alpha * bn2, bn2)
        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same') #
[?x4x4x?]
        bn3 = tf.layers.batch_id172(x3, training=true)
        relu3 = tf.maximum(alpha * bn3, bn3)
        relu3 = tf.layers.dropout(relu3, rate=drop_rate)
        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same'
) # [?x4x4x?]
        bn4 = tf.layers.batch_id172(x4, training=true)
        relu4 = tf.maximum(alpha * bn4, bn4)
        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same'
) # [?x4x4x?]
        bn5 = tf.layers.batch_id172(x5, training=true)
        relu5 = tf.maximum(alpha * bn5, bn5)
        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same'
) # [?x2x2x?]
        bn6 = tf.layers.batch_id172(x6, training=true)
        relu6 = tf.maximum(alpha * bn6, bn6)
        relu6 = tf.layers.dropout(relu6, rate=drop_rate)
...

   in the end, instead of applying a fully connected layer on top of the
   convolution stack, we perform global average pooling (gap). in gap, we
   take the average over the spatial dimensions of a feature vector. this
   operation results in squashing the tensor dimensions to a single value.
...
# flatten it by global average pooling
# in global average pooling, for every feature map we take the average over all
the spatial
# domain and return a single value
# in: [batch_size,height x width x channels] --> [batch_size, channels]
features = tf.reduce_mean(relu7, axis=[1,2])
# set class_logits to be the inputs to a softmax distribution over the different
 classes
class_logits = tf.layers.dense(features, num_classes)
...

   for instance, suppose that after a series of convolutions, we get a
   tensor of shape [batch_size, 8, 8, num_channels]. to apply gap, we take
   the average value over the [8x8] tensor slice. this results in a tensor
   of shape [batch_size, 1, 1, num_channels] that can be reshaped to
   [batch_size, num_channels].

   in [29]network in network, the authors describe some advantages of gap
   over traditional fully connected layers. these include: higher
   robustness for spatial translation and less overfitting concerns. after
   gap, we apply a fully connected layer to output the final logits. these
   have shape [batch_size, num_classes] and corresponds to the unscaled
   final class values.

   to get the classification probabilities, we feed the logits through the
   softmax function. however, we still need a way to represent the
   id203 of an input image being real rather than fake. that is, we
   still need to account for the binary classification problem of regular
   a gan.
...
# get the id203 that the input is real rather than fake
out = tf.nn.softmax(class_logits) # class probabilities for the 10 real classes
...

   we know that the logits are in terms of softmax id203 values.
   yet, we need a way to represent them as sigmoid logits as well. we know
   that the id203 of an input being real corresponds to the sum over
   all real class logits. with that mind, we can feed these values to a
   logsumexp function that will model the binary classification value.
   after that, we feed the result from the logsumexp to a sigmoid
   function.

   we can use the tensorflow   s logsumexp built-in function to avoid
   numerical problems. this routine prevents over/under flow issues that
   may occur when logsumexp encounters very extreme, either positive or
   negative values.
...
# this function is more numerically stable than log(sum(exp(input))).
# it avoids overflows caused by taking the exp of large inputs and underflows
# caused by taking the log of small inputs.
gan_logits = tf.reduce_logsumexp(class_logits, 1)
...

model loss

   as we mentioned, we can divide the discriminator loss in two parts. one
   that represents the gan problem, the unsupervised loss. and the other
   that computes the individual real class probabilities, the supervised
   loss.

   for the unsupervised loss, the discriminator has to differentiate
   between real training images and fake images from the generator.

   as for a regular [30]gan, half of the time the discriminator receives
   unlabeled images from the training set and the other half, imaginary
   unlabeled images from the generator.

   in both cases we are dealing with a binary classification problem.
   since we want a id203 value near 1 for real images and close to 0
   for unreal ones, we can use the sigmoid cross id178 function to
   compute the loss.

   for images coming from the training set, we maximize their
   probabilities of being real by assigning labels of 1s. for fabricated
   images coming from the generator, we maximize their probabilities to be
   fake by giving them labels of 0s.
...
# here we compute `d_loss`, the loss for the discriminator.
# this should combine two different losses:
# 1. the loss for the gan problem, where we minimize the cross-id178 for the b
inary
#    real-vs-fake classification problem.
tf.reduce_mean(tf.nn.sigmoid_cross_id178_with_logits(logits=gan_logits_on_data
,
                                                        labels=tf.ones_like(gan_
logits_on_data) * (1 - smooth)))
fake_data_loss = tf.reduce_mean(tf.nn.sigmoid_cross_id178_with_logits(logits=g
an_logits_on_samples,
                                                     labels=tf.zeros_like(gan_lo
gits_on_samples)))
# this way, the unsupervised
unsupervised_loss = real_data_loss + fake_data_loss
...

   for the supervised loss, we need to use the logits from the
   discriminator. since this is a multi-class classification problem, we
   can use the softmax cross id178 function with the real labels we have
   available.

   note that this part is similar to any other classification model. in
   the end, the discriminator loss is the sum of both the supervised loss
   and the unsupervised loss. also, because we are pretending we do not
   have most of the labels, we need to ignore them in the supervised loss.
   to do that, we multiply the loss by the masks variable that indicates
   which set of labels are available for usage.
#  2. the loss for the svhn digit classification problem, where we minimize the
cross-id178
#     for the multi-class softmax. for this one we use the labels. don't forget
to ignore
#     use `label_mask` to ignore the examples that we are pretending are unlabel
ed for the
#     semi-supervised learning problem.
y = tf.squeeze(y)
suppervised_loss = tf.nn.softmax_cross_id178_with_logits(logits=class_logits_o
n_data,
                                                              labels=tf.one_hot(
y, num_classes, dtype=tf.float32))
label_mask = tf.squeeze(tf.to_float(label_mask))
# ignore the labels that we pretend does not exist for the loss
suppervised_loss = tf.reduce_sum(tf.multiply(suppervised_loss, label_mask))
# get the mean
suppervised_loss = suppervised_loss / tf.maximum(1.0, tf.reduce_sum(label_mask))
d_loss = unsupervised_loss + suppervised_loss

   as described in the [31]improved techniques for training gans paper, we
   use feature matching for the generator loss.

   as the authors describe:

     feature matching is the concept of penalizing the mean absolute
     error between the average value of some set of features on the
     training data and the average values of that set of features on the
     generated samples.

   to do that, we take some set of statistics (the moments) from two
   different sources and force them to be similar.

   first, we take the average of the features extracted from the
   discriminator when a real training minibatch is being processed.

   second, we compute the moments in the same way, but now for when a
   minibatch composed of fake images that come from the generator was
   being analyzed by the discriminator.

   finally, with these two sets of moments, the generator loss is the mean
   absolute difference between them. in other words, as the paper
   emphasizes:

     we train the generator to match the expected values of the features
     on an intermediate layer of the discriminator.

# here we set `g_loss` to the "feature matching" loss invented by tim salimans a
t openai.
# this loss consists of minimizing the absolute difference between the expected
features
# on the data and the expected features on the generated samples.
# this loss works better for semi-supervised learning than the tradition gan los
ses.
# make the generator output features that are on average similar to the features
# that are found by applying the real data to the discriminator
data_moments = tf.reduce_mean(data_features, axis=0)
sample_moments = tf.reduce_mean(sample_features, axis=0)
g_loss = tf.reduce_mean(tf.abs(data_moments - sample_moments))
pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)
eq = tf.equal(tf.squeeze(y), pred_class)
correct = tf.reduce_sum(tf.to_float(eq))
masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))

   although feature-matching loss performs well on the task of
   semi-supervised learning, the images produced by generator are not as
   good as the ones created in the [32]last post.
   [1*b-dp_hl4rcn4zxi0ztonvq.png]
   sample images created by the generator network using the feature
   matching loss.

   in the [33]improved techniques for training gans paper, openai reports
   state-of-the-art results for semi-supervised classification learning on
   mnist, cifar-10 and svhn.

   our implementation reaches train and test accuracies of nearly 93% and
   68% respectively. these results are better than the ones reported in
   this [34]nips 2014 paper which got something around 64%.

   this notebook is not intended for demonstrating best practices such
   cross-validation techniques. it only uses some of the techniques
   described in the original openai paper.

   the notebook was based on the udacity deep learning fundamentals
   nanodegree program in which i graduated from.

concluding

   many researches considers unsupervised learning as the missing link to
   general ai systems.

   to break these obstacles, attempts to solve already established
   problems using less labeled data are key. in this scenario, gans pose a
   real alternative for learning complicated tasks with less labeled
   samples.

   yet, the performance gap between supervised and semi-supervised
   learning is still far from being equal. but we certainly can expect
   this gap to become shorter as new approaches come in to play.

   if you are curious to dig deeper in these subjects, take a look at:
   [35]dive head first into advanced gans: exploring self-attention and
   spectral norm
   lately, generative models are drawing a lot of attention. much of that
   comes from id3   medium.freecodecamp.org
   [36]an intuitive introduction to id3 (gans)
   in a gan, two differentiable functions are locked in a game. the two
   players, the generator and the discriminator
   have   medium.freecodecamp.org

   and if you need more, that is my [37]deep learning blog.

   thanks for reading!
   [1*rzohqzrjeirb5zy0njacvg.gif]

   credits to [38]sam williams for this awesome    clap    gif! check it out
   in [39]his post.

     * [40]machine learning
     * [41]deep learning
     * [42]semi supervided learning
     * [43]tensorflow
     * [44]towards data science

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [45]go to the profile of thalles silva

[46]thalles silva

   id161 & deep learning. personal blog:
   [47]https://sthalles.github.io/

     (button) follow
   [48]towards data science

[49]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [50]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [51]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9f3cb128c5e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_q3cevh02fstr---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@thalles.silva?source=post_header_lockup
  17. https://towardsdatascience.com/@thalles.silva
  18. http://yann.lecun.com/exdb/mnist/
  19. http://ufldl.stanford.edu/housenumbers/
  20. http://www.image-net.org/
  21. http://host.robots.ox.ac.uk/pascal/voc/
  22. https://arxiv.org/abs/1606.03498
  23. https://sthalles.github.io/intro-to-gans/
  24. https://github.com/sthalles/blog-resources/blob/master/semi-supervised/semi-supervised_learning.ipynb
  25. https://sthalles.github.io/intro-to-gans/
  26. https://sthalles.github.io/intro-to-gans/
  27. https://sthalles.github.io/intro-to-gans/
  28. https://arxiv.org/abs/1511.06434
  29. https://arxiv.org/abs/1312.4400
  30. https://sthalles.github.io/intro-to-gans/
  31. https://arxiv.org/abs/1606.03498
  32. https://sthalles.github.io/intro-to-gans/
  33. https://arxiv.org/abs/1606.03498
  34. https://arxiv.org/abs/1406.5298
  35. https://medium.freecodecamp.org/dive-head-first-into-advanced-gans-exploring-self-attention-and-spectral-norm-d2f7cdb55ede
  36. https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394
  37. https://sthalles.github.io/
  38. https://medium.com/@samwsoftware
  39. https://medium.freecodecamp.org/want-more-claps-and-followers-how-to-make-a-clap-me-gif-in-5-minutes-db85a24950f6
  40. https://towardsdatascience.com/tagged/machine-learning?source=post
  41. https://towardsdatascience.com/tagged/deep-learning?source=post
  42. https://towardsdatascience.com/tagged/semi-supervided-learning?source=post
  43. https://towardsdatascience.com/tagged/tensorflow?source=post
  44. https://towardsdatascience.com/tagged/towards-data-science?source=post
  45. https://towardsdatascience.com/@thalles.silva?source=footer_card
  46. https://towardsdatascience.com/@thalles.silva
  47. https://sthalles.github.io/
  48. https://towardsdatascience.com/?source=footer_card
  49. https://towardsdatascience.com/?source=footer_card
  50. https://towardsdatascience.com/
  51. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  53. https://medium.freecodecamp.org/dive-head-first-into-advanced-gans-exploring-self-attention-and-spectral-norm-d2f7cdb55ede
  54. https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394
  55. https://medium.com/p/9f3cb128c5e/share/twitter
  56. https://medium.com/p/9f3cb128c5e/share/facebook
  57. https://medium.com/p/9f3cb128c5e/share/twitter
  58. https://medium.com/p/9f3cb128c5e/share/facebook
