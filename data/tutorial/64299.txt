   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id28         detailed overview

   [16]go to the profile of saishruthi swaminathan
   [17]saishruthi swaminathan (button) blockedunblock (button)
   followfollowing
   mar 15, 2018
   [1*ugybimgpxf6xxxmy2yqrlw.png]
   figure 1: id28 model
   (source:[18]http://dataaspirant.com/2017/03/02/how-logistic-regression-
   model-works/)

   id28 was used in the biological sciences in early
   twentieth century. it was then used in many social science
   applications. id28 is used when the dependent
   variable(target) is categorical.

   for example,
     * to predict whether an email is spam (1) or (0)
     * whether the tumor is malignant (1) or not (0)

   consider a scenario where we need to classify whether an email is spam
   or not. if we use id75 for this problem, there is a need
   for setting up a threshold based on which classification can be done.
   say if the actual class is malignant, predicted continuous value 0.4
   and the threshold value is 0.5, the data point will be classified as
   not malignant which can lead to serious consequence in real time.

   from this example, it can be inferred that id75 is not
   suitable for classification problem. id75 is unbounded,
   and this brings id28 into picture. their value strictly
   ranges from 0 to 1.

   simple id28

   (full source code:
   [19]https://github.com/ssaishruthi/logisticregression_vectorized_implem
   entation/blob/master/logistic_regression.ipynb)

   model

   output = 0 or 1

   hypothesis => z = wx + b

   h  (x) = sigmoid (z)

   sigmoid function
   [1*rqxfpingwdikbwyljc_e7g.png]
   figure 2: sigmoid activation function

   if    z    goes to infinity, y(predicted) will become 1 and if    z    goes to
   negative infinity, y(predicted) will become 0.

   analysis of the hypothesis

   the output from the hypothesis is the estimated id203. this is
   used to infer how confident can predicted value be actual value when
   given an input x. consider the below example,

   x = [x0 x1] = [1 ip-address]

   based on the x1 value, let   s say we obtained the estimated id203
   to be 0.8. this tells that there is 80% chance that an email will be
   spam.

   mathematically this can be written as,
   [1*i_qqvuzxcetjeelf4mlx8q.png]
   figure 3: mathematical representation

   this justifies the name    id28   . data is fit into linear
   regression model, which then be acted upon by a logistic function
   predicting the target categorical dependent variable.

   types of id28

   1. binary id28

   the categorical response has only two 2 possible outcomes. example:
   spam or not

   2. multinomial id28

   three or more categories without ordering. example: predicting which
   food is preferred more (veg, non-veg, vegan)

   3. ordinal id28

   three or more categories with ordering. example: movie rating from 1 to
   5

   decision boundary

   to predict which class a data belongs, a threshold can be set. based
   upon this threshold, the obtained estimated id203 is classified
   into classes.

   say, if predicted_value     0.5, then classify email as spam else as not
   spam.

   decision boundary can be linear or non-linear. polynomial order can be
   increased to get complex decision boundary.

   cost function
   [1*tqz9myxidlukid48orceew.png]
   figure 4: cost function of id28

   why cost function which has been used for linear can not be used for
   logistic?

   id75 uses mean squared error as its cost function. if this
   is used for id28, then it will be a non-convex function
   of parameters (theta). id119 will converge into global
   minimum only if the function is convex.
   [1*zyjej3a_qyr4wy7y5cwiwq.png]
   figure 5: convex and non-convex cost function

   cost function explanation
   [1*5ayagpv-gjyuf37d2ihgtq.jpeg]
   figure 6: cost function part 1
   [1*mfmieuc_dobhjrrjgk7pbg.jpeg]
   figure 7: cost function part 2

   simplified cost function
   [1*ueewu1de0yu-kpmjanf9aq.png]
   figure 8: simplified cost function

   why this cost function?
   [1*hegae4az-dn-rlsfx2-p9g.jpeg]
   figure 9: maximum likelihood explanation part-1
   [1*jipaau-jffvx2yr9l1yz6a.jpeg]
   figure 10: maximum likelihood explanation part-2

   this negative function is because when we train, we need to maximize
   the id203 by minimizing id168. decreasing the cost will
   increase the maximum likelihood assuming that samples are drawn from an
   identically independent distribution.

   deriving the formula for id119 algorithm
   [1*r7fhk417iouq7mexictgxg.jpeg]
   figure 11: id119 algorithm part 1
   [1*pjei5f4gdvgezyev9mchbw.jpeg]
   figure 12: id119 part 2

   python implementation
def weightinitialization(n_features):
    w = np.zeros((1,n_features))
    b = 0
    return w,b
def sigmoid_activation(result):
    final_result = 1/(1+np.exp(-result))
    return final_result
def model_optimize(w, b, x, y):
    m = x.shape[0]

    #prediction
    final_result = sigmoid_activation(np.dot(w,x.t)+b)
    y_t = y.t
    cost = (-1/m)*(np.sum((y_t*np.log(final_result)) + ((1-y_t)*(np.log(1-final_
result)))))
    #

    #gradient calculation
    dw = (1/m)*(np.dot(x.t, (final_result-y.t).t))
    db = (1/m)*(np.sum(final_result-y.t))

    grads = {"dw": dw, "db": db}

    return grads, cost
def model_predict(w, b, x, y, learning_rate, no_iterations):
    costs = []
    for i in range(no_iterations):
        #
        grads, cost = model_optimize(w,b,x,y)
        #
        dw = grads["dw"]
        db = grads["db"]
        #weight update
        w = w - (learning_rate * (dw.t))
        b = b - (learning_rate * db)
        #

        if (i % 100 == 0):
            costs.append(cost)
            #print("cost after %i iteration is %f" %(i, cost))

    #final parameters
    coeff = {"w": w, "b": b}
    gradient = {"dw": dw, "db": db}

    return coeff, gradient, costs
def predict(final_pred, m):
    y_pred = np.zeros((1,m))
    for i in range(final_pred.shape[1]):
        if final_pred[0][i] > 0.5:
            y_pred[0][i] = 1
    return y_pred

   cost vs number_of_iterations
   [1*uraetkf5ig_dyzwr8hijmq.png]
   figure 13: cost reduction

   train and test accuracy of the system is 100 %

   this implementation is for binary id28. for data with
   more than 2 classes, softmax regression has to be used.

   this is an educational post and inspired from prof. andrew ng   s deep
   learning course.

   full code :
   [20]https://github.com/ssaishruthi/logisticregression_vectorized_implem
   entation/blob/master/logistic_regression.ipynb

     * [21]data
     * [22]statistics
     * [23]id28
     * [24]predictive analytics

   (button)
   (button)
   (button) 2.8k claps
   (button) (button) (button) 25 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of saishruthi swaminathan

[26]saishruthi swaminathan

   passionate about transforming data into useful products. happy sharing
   my knowledge in data science to all!!

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.8k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/46c4da4303bc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_sybz3wqwip6x---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@saishruthi.tn?source=post_header_lockup
  17. https://towardsdatascience.com/@saishruthi.tn
  18. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/
  19. https://github.com/ssaishruthi/logisticregression_vectorized_implementation/blob/master/logistic_regression.ipynb
  20. https://github.com/ssaishruthi/logisticregression_vectorized_implementation/blob/master/logistic_regression.ipynb
  21. https://towardsdatascience.com/tagged/data?source=post
  22. https://towardsdatascience.com/tagged/statistics?source=post
  23. https://towardsdatascience.com/tagged/logistic-regression?source=post
  24. https://towardsdatascience.com/tagged/predictive-analytics?source=post
  25. https://towardsdatascience.com/@saishruthi.tn?source=footer_card
  26. https://towardsdatascience.com/@saishruthi.tn
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/46c4da4303bc/share/twitter
  33. https://medium.com/p/46c4da4303bc/share/facebook
  34. https://medium.com/p/46c4da4303bc/share/twitter
  35. https://medium.com/p/46c4da4303bc/share/facebook
