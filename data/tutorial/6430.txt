   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

training mxnet         part 2: cifar-10

   [14]go to the profile of julien simon
   [15]julien simon (button) blockedunblock (button) followfollowing
   apr 23, 2017

   in [16]part 1, we used the famous [17]lenet convolutional neural
   network to reach 99+% validation accuracy in just 10 epochs. we also
   saw how to use multiple gpus to speed up training.

   in this article, we   re going to tackle a more difficult data set:
   [18]cifar-10. in the process, we   re going to learn a few new tricks.
   read on :)

   the cifar-10 data set

   the cifar-10 dataset consists of 60,000 32 x 32 colour images. they are
   divided in 10 classes containing 6,000 images each. there are 50,000
   training images and 10,000 test images. categories are stored in a
   separate metadata file.
   [1*6xqqoifwnmpls22zcrrvaw.png]
   samples images from the cifar-10 data set

   let   s download the data set.
$ wget [19]https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
$ tar xfz cifar-10-python.tar.gz
$ ls cifar-10-batches-py/
batches.meta  data_batch_1  data_batch_2  data_batch_3  data_batch_4  data_batch
_5  readme.html  test_batch

   each file contains 10,000 pickled images, which we need to turn into an
   array shaped (10000, 3, 32, 32). the    3    dimension comes for the three
   rgb channels, remember? :)

   let   s open the first file, save its first 10 images to disk and print
   their category. nothing really complicated here, except some opencv
   tricks (see comments in the code below).

   iframe: [20]/media/424fd63cecceaf2da0b4a549e20fa370?postid=c7b0b729c33c

   here   s the output.
(10000, 3, 32, 32)
10000
['frog', 'truck', 'truck', 'deer', 'automobile', 'automobile', 'bird', 'horse',
'ship', 'cat']

   [1*uhuynxcfod1rrysh2ku49a.png]
   frog, truck, truck, deer, automobile, automobile, bird, horse,
   ship, cat

   now let   s load the data set.

loading cifar-10 in ndarrays

   just like we did for the the mnist data, the cifar-10 images and labels
   could be loaded in ndarrays and then fed to an iterator. this is how we
   would to it.

   iframe: [21]/media/d1ba54af42f4060884d6bd0b20acb9bd?postid=c7b0b729c33c

   here   s the output.
(50000l, 3l, 32l, 32l)
(50000l,)
(10000l, 3l, 32l, 32l)
(10000l,)

   the next logical step would be to bind these arrays to a module and
   start training (just like we did for the mnist data set). however, i   d
   like to show you another way to load image data: the recordio file.

loading cifar-10 with recordio files

   being able to load data efficiently is a very important part of mxnet:
   you   ll find architecture details [22]here. in a nutshell, recordio
   files allow large data sets to be packed and split in multiple files,
   which can then be loaded and processed in parallel by multiple servers
   for distributed training.

   we won   t cover how to build these files today. let   s use pre-existing
   files hosted on the mxnet website.
$ wget [23]http://data.mxnet.io/data/cifar10/cifar10_val.rec
$ wget [24]http://data.mxnet.io/data/cifar10/cifar10_train.rec

   the first file contains 50,000 samples, which we   ll use for training.
   the second one contains 10,000 samples, which we   ll use for validation.
   image resolution has been set to 28x28.

   loading these files and building an iterator is extremely simple. we
   just have to be careful to :
     * match data_name and label_name with the names of the input and
       output layers.
     * shuffle samples in the training set in case they   ve been stored in
       some kind of sequence.

   iframe: [25]/media/aafc645e111c9059373c91418875030c?postid=c7b0b729c33c

   data is ready for training. we   ve learned from previous examples that
   convolutional neural networks are a good fit for id164, so
   that   s where we should look.

training vs. fine-tuning

   in previous examples, we picked models from the [26]model zoo and
   retrained them from scratch on our data set. we   re going to do that
   again with the [27]resnext-101 model, but we   re going to try something
   different in parallel: fine-tuning the model.

   fine-tuning means that we   re going to keep all layers and pre-trained
   weights unchanged, except for the last layer: it will be removed and
   replaced by a new layer having the number of outputs of the new data
   set. then, we will train the output layer on the new data set.

   since our model has been pre-trained on the large [28]id163 data
   set, the rationale for fine-tuning is to benefit from the very large
   number of patterns that the model has learned while training on
   id163. although image sizes are quite different, it   s reasonable to
   expect that they will also apply to cifar-10.

training on resnext-101

   let   s first start by training the model from scratch. we   ve done this a
   few times before, so no difficulty here.

   iframe: [29]/media/2c9a7fd8a94df079586fa8e6fcceb4fc?postid=c7b0b729c33c

   this is the result after 100 epochs.
   [1*3edr6zmvwihxam_nnvjywg.png]

fine-tuning on resnext-101

   replacing layers sounds complicated, doesn   t it? fortunately, the mxnet
   sources provide python code to do this. it   s located in
   example/image-classification/fine-tune.py. basically, it   s going to
   download the pre-trained model, remove its output layer, add a new one
   and start training.

   this is how to use it:
$ python fine-tune.py
--pretrained-model resnext-101 --load-epoch 0000
--gpus 0,1,2,3 --batch-size 128
--data-train cifar10_train.rec --data-val cifar10_val.rec
--num-examples 50000 --num-classes 10 --image-shape 3,28,28
--num-epoch 300 --lr 0.05

   this is going to download resnext-101   0000.params and
   resnext-101-symbol.json from the model zoo. most of the parameters
   should be familiar. here   s the result after 100 epochs.
   [1*gttmhai5--u1fg3euzsgaq.png]
   comparing training and fine-tuning

   what do we see here?

   early on, fine-tuning delivers much higher training and validation
   accuracy. this makes sense, since the model has been pre-trained. so,
   if you have limited time and resources, fine-tuning is definitely an
   interesting way to get quick results on a new data set.

   over time, fine-tuning delivers about 5% additional validation accuracy
   than training from scratch. i   m guessing that the pre-trained model
   generalizes better on new data thanks to the large id163 data set.

   last but not least, validation accuracy stops improving after 50 epochs
   or so. surely, we can do something to improve this?

   yes, of course. we   ll see how in part 3 :)
     __________________________________________________________________

   next:
     * [30]part 3         cifar-10 redux
     * [31]part 4         distributed training
     * [32]part 5         distributed training, efs edition

   iframe: [33]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=c7b0b729c33c

   [1*bqlrszfhjemf4q7pyrlgng.gif]
   [34][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [35][1*v-ppfkswhbvlwwamsvhhwg.png]
   [36][1*wt2auqisieaozxj-i7brdq.png]

     * [37]deep learning
     * [38]artificial intelligence
     * [39]neural networks
     * [40]open source
     * [41]computer science

   (button)
   (button)
   (button) 80 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [42]go to the profile of julien simon

[43]julien simon

   hacker. headbanger. harley rider. hunter.
   [44]https://aws.amazon.com/evangelists/julien-simon/

     (button) follow
   [45]becoming human: artificial intelligence magazine

[46]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 80
     * (button)
     *
     *

   [47]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [48]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c7b0b729c33c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/training-mxnet-part-2-cifar-10-c7b0b729c33c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/training-mxnet-part-2-cifar-10-c7b0b729c33c&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_queifcu3eoun---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@julsimon?source=post_header_lockup
  15. https://becominghuman.ai/@julsimon
  16. https://medium.com/@julsimon/training-mxnet-part-1-mnist-6f0dc4210c62
  17. http://yann.lecun.com/exdb/lenet/
  18. https://www.cs.toronto.edu/~kriz/cifar.html
  19. https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
  20. https://becominghuman.ai/media/424fd63cecceaf2da0b4a549e20fa370?postid=c7b0b729c33c
  21. https://becominghuman.ai/media/d1ba54af42f4060884d6bd0b20acb9bd?postid=c7b0b729c33c
  22. http://mxnet.io/architecture/note_data_loading.html
  23. http://data.mxnet.io/data/cifar10/cifar10_val.rec
  24. http://data.mxnet.io/data/cifar10/cifar10_train.rec
  25. https://becominghuman.ai/media/aafc645e111c9059373c91418875030c?postid=c7b0b729c33c
  26. http://mxnet.io/model_zoo/
  27. http://data.mxnet.io/models/id163/resnext/101-layers/
  28. http://www.image-net.org/
  29. https://becominghuman.ai/media/2c9a7fd8a94df079586fa8e6fcceb4fc?postid=c7b0b729c33c
  30. https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0
  31. https://medium.com/@julsimon/training-mxnet-part-4-distributed-training-91def5ea3bb7
  32. https://medium.com/@julsimon/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460
  33. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=c7b0b729c33c
  34. https://medium.com/becoming-human/artificial-intelligence-communities-c305f28e674c
  35. https://upscri.be/8f5f8b
  36. https://medium.com/becoming-human/write-for-us-48270209de63
  37. https://becominghuman.ai/tagged/deep-learning?source=post
  38. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  39. https://becominghuman.ai/tagged/neural-networks?source=post
  40. https://becominghuman.ai/tagged/open-source?source=post
  41. https://becominghuman.ai/tagged/computer-science?source=post
  42. https://becominghuman.ai/@julsimon?source=footer_card
  43. https://becominghuman.ai/@julsimon
  44. https://aws.amazon.com/evangelists/julien-simon/
  45. https://becominghuman.ai/?source=footer_card
  46. https://becominghuman.ai/?source=footer_card
  47. https://becominghuman.ai/
  48. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  50. https://medium.com/p/c7b0b729c33c/share/twitter
  51. https://medium.com/p/c7b0b729c33c/share/facebook
  52. https://medium.com/p/c7b0b729c33c/share/twitter
  53. https://medium.com/p/c7b0b729c33c/share/facebook
