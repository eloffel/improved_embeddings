should we care about 

linguistics?

ellie pavlick 

department of computer science 

brown university

deep learning is taking 

over nlp!

deep learning is taking 

over nlp!

titles of acl papers, 1979

deep learning is taking 

over nlp!

titles of acl papers, 1987

deep learning is taking 

over nlp!

titles of acl papers, everything pre-2015

deep learning is taking 

over nlp!

titles of acl papers, 2017

sota on classic nlp tasks

language 
modeling 
bengio et al. 

(2003)

perplexity
best id165
best mlp

320

290

260

230

200

46

44.5

43

41.5

40

sentiment 
analysis 

socher et al. 

(2013)

dependency 

parsing 
chen and 
manning 
(2014)

machine 
translation 
devlin et al. 

(2014)

93

92.25

91.5

90.75

53

51

49

47

accuracy
naive bayes
id56

90

45
unlabelled attachment score

id7 (ar-en)

graph-based model
nueral model

best phrase-based
best nueral

25231291.890.752.849.545.741.9new enthusiasm for end-to-

end nlu tasks

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

a man inspects the uniform of a 
   gure in some east asian country. 

+

the man is sleeping.

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

premise

a man inspects the uniform of a 
   gure in some east asian country. 

+

the man is sleeping.

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

premise

a man inspects the uniform of a 
   gure in some east asian country. 

+

the man is sleeping.

hypothesis

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

a man inspects the uniform of a 
   gure in some east asian country. 

+

the man is sleeping.

system

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)

a man inspects the uniform of a 
   gure in some east asian country. 

+

the man is sleeping.

system

false

new enthusiasm for end-to-

end nlu tasks

recognizing id123 (rte)
performance of sentence encoding models on snli dataset

y
c
a
r
u
c
c
a

 
t
s
e
t

87

84.5

82

79.5

77

models on snli leaderboard

a large annotated corpus for learning natural language id136. 
bowman et al. (2015)

86.386.086.085.985.785.685.685.584.684.584.283.383.282.181.480.677.6new enthusiasm for end-to-

end nlu tasks
reading comprehension

what is southern california often abbreviated as?

squad: 100,000+ questions for machine comprehension of text. 
rajpurkar et al. (2016)

new enthusiasm for end-to-

end nlu tasks
reading comprehension

what is southern california often abbreviated as?

+

southern california, often abbreviated socal, is a geographic and cultural region that 
generally comprises california's southernmost 10 counties. the region is traditionally 
described as "eight counties", based on demographics and economic ties: imperial, 
los angeles, orange, riverside, san bernardino, san diego, santa barbara, and 
ventura. the more extensive 10-county de   nition, including kern and san luis 
obispo counties, is also used based on historical political divisions. southern 

california is a major economic center for the state of california and the united states.

squad: 100,000+ questions for machine comprehension of text. 
rajpurkar et al. (2016)

new enthusiasm for end-to-

end nlu tasks
reading comprehension

what is southern california often abbreviated as?

+

southern california, often abbreviated socal, is a geographic and cultural region that 
generally comprises california's southernmost 10 counties. the region is traditionally 
described as "eight counties", based on demographics and economic ties: imperial, 
los angeles, orange, riverside, san bernardino, san diego, santa barbara, and 
ventura. the more extensive 10-county de   nition, including kern and san luis 
obispo counties, is also used based on historical political divisions. southern 

california is a major economic center for the state of california and the united states.

system

squad: 100,000+ questions for machine comprehension of text. 
rajpurkar et al. (2016)

new enthusiasm for end-to-

end nlu tasks
reading comprehension

what is southern california often abbreviated as?

+

southern california, often abbreviated socal, is a geographic and cultural region that 
generally comprises california's southernmost 10 counties. the region is traditionally 
described as "eight counties", based on demographics and economic ties: imperial, 
los angeles, orange, riverside, san bernardino, san diego, santa barbara, and 
ventura. the more extensive 10-county de   nition, including kern and san luis 
obispo counties, is also used based on historical political divisions. southern 

california is a major economic center for the state of california and the united states.

system
socal

squad: 100,000+ questions for machine comprehension of text. 
rajpurkar et al. (2016)

new enthusiasm for end-to-

end nlu tasks
reading comprehension

performance on squad reading comprehension dataset

y
c
a
r
u
c
c
a

83
81
79
76
74

july

aug

oct

sept
nov
best model by month

dec

jan

squad: 100,000+ questions for machine comprehension of text. 
rajpurkar et al. (2016)

should we care about 

linguistics?

320

290

260

230

200

language 
modeling

sentiment 
analysis

dependency 

parsing

machine 
translation

46

44.5

43

41.5

40

perplexity
best id165
best mlp

accuracy
naive bayes
id56

natural language id136 

(snli)

93

92.25

91.5

90.75

53

51

49

47

90

45
unlabelled attachment score

id7 (ar-en)

graph-based model
nueral model

best phrase-based
best nueral

reading comprehension 

(squad)

y
c
a
r
u
c
c
a

87
85
82
80
77

y
c
a
r
u
c
c
a

83
81
79
76
74

25231291.890.752.849.545.741.9language 
modeling

sentiment 
analysis

dependency 

parsing

machine 
translation

320

290

260

230

200

perplexity
best id165
best mlp

53

51

49

47

46

44.5

93

92.25

41.5

43

40

91.5

90.75

as is, we are 
doing lots of 
tasks very well.

accuracy
naive bayes
id56

graph-based model
nueral model

90

45
unlabelled attachment score

natural language id136 

(snli)

reading comprehension 

(squad)

id7 (ar-en)

best phrase-based
best nueral

y
c
a
r
u
c
c
a

87
85
82
80
77

y
c
a
r
u
c
c
a

83
81
79
76
74

25231291.890.752.849.545.741.9what are our systems 

learning?

a man inspects the uniform of a 
   gure in some east asian country. 

the man is sleeping.

snli dataset (bowman, 2015)

what are our systems 

learning?

a man inspects the uniform of a 
   gure in some east asian country. 

the man is sleeping.

snli dataset (bowman, 2015)

what are our systems 

learning?

a man inspects the uniform of a 
   gure in some east asian country. 

the man is sleeping.

snli dataset (bowman, 2015)

what are our systems 

learning?

a man inspects the uniform of a 
   gure in some east asian country. 

the man is sleeping.

contradiction
entailment
neutral

snli dataset (bowman, 2015)

7%10%83%what are our systems 

learning?

current sota is 86% 

a man inspects the uniform of a 
tao shen et al. 2018 
   gure in some east asian country. 

300d reinforced self-attention 

network

the man is sleeping.

contradiction
entailment
neutral

snli dataset (bowman, 2015)

7%10%83%what are our systems 

learning?

what is southern california often abbreviated as?

+

southern california, often abbreviated socal, is a geographic and cultural region that 
generally comprises california's southernmost 10 counties. the region is traditionally 
described as "eight counties", based on demographics and economic ties: imperial, 
los angeles, orange, riverside, san bernardino, san diego, santa barbara, and 
ventura. the more extensive 10-county de   nition, including kern and san luis 
obispo counties, is also used based on historical political divisions. southern 

california is a major economic center for the state of california and the united states.

socal

adversarial examples for evaluating reading comprehension systems  
jia and liang (2017)

what are our systems 

learning?

what is southern california often abbreviated as?

+

southern california, often abbreviated socal, is a geographic and cultural region that 
generally comprises california's southernmost 10 counties. the region is traditionally 
described as "eight counties", based on demographics and economic ties: imperial, 
los angeles, orange, riverside, san bernardino, san diego, santa barbara, and 
ventura. the more extensive 10-county de   nition, including kern and san luis 
obispo counties, is also used based on historical political divisions. southern 

california is a major economic center for the state of california and the united states. 

norther california is often abbreviated norcal.

socal

adversarial examples for evaluating reading comprehension systems  
jia and liang (2017)

what are our systems 

learning?

accuracy on squad (reading comprehension) before and after 

inserting adversarial examples

before
after

reasonet-e 

sedt-e

bidaf-e mnemonic-e  ruminating

jnet 

adversarial examples for evaluating reading comprehension systems  
jia and liang (2017)

y
c
a
r
u
c
c
a

90

67.5

45

22.5

0

what are our systems 

learning?

accuracy on squad (reading comprehension) before and after 

inserting adversarial examples

before
after

reasonet-e 

sedt-e

bidaf-e mnemonic-e  ruminating

jnet 

adversarial examples for evaluating reading comprehension systems  
jia and liang (2017)

y
c
a
r
u
c
c
a

90

67.5

45

22.5

0

what do we want our 

systems to learn?

what do we want our 

systems to learn?

id27-based antonym detection using thesauri 

and distributional information (ono et al. 2015)

pre-trained id27s

retro   tting word vectors to semantic lexicons (faruqui et al 2015)

low-dimensional embeddings of logic (rocktaschel et al. 2014)

 learning semantic id27s based on 
ordinal knowledge constraints (liu et al  2015)

counter-   tting word vectors to linguistic constraints (mrk  i   et al. 2016)
integrating distributional lexical contrast into id27s 

for antonym-synonym distinction (nguyen et al. 2016) 

identifying and exploiting hearst patterns in distributional vectors for 

sensembed: learning sense embeddings    (iacobacci et al 2015)

lexical entailment (roller and erk 2016)

autoextend: extending id27s   (rothe and schutze 2015)

what do we want our 

systems to learn?

id27-based antonym detection using thesauri 

and distributional information (ono et al. 2015)

pre-trained id27s

retro   tting word vectors to semantic lexicons (faruqui et al 2015)

low-dimensional embeddings of logic (rocktaschel et al. 2014)

 learning semantic id27s based on 
ordinal knowledge constraints (liu et al  2015)

counter-   tting word vectors to linguistic constraints (mrk  i   et al. 2016)
integrating distributional lexical contrast into id27s 

for antonym-synonym distinction (nguyen et al. 2016) 

identifying and exploiting hearst patterns in distributional vectors for 

sensembed: learning sense embeddings    (iacobacci et al 2015)

lexical entailment (roller and erk 2016)

autoextend: extending id27s   (rothe and schutze 2015)

 skip-thought vectors (kiros et al. 2015)

distributed representations of sentences and 

documents (quoc et al. 2014)

 a structured self-attentive sentence embedding (lin et al. 2017)
task-independent sentence embeddings

 supervised learning of universal sentence representations from 

natural language id136 data (conneau et al. 2017)

 dynamic pooling and unfolding recursive autoencoders for 

paraphrase detection (socher et al. 2011)

 siamese cbow: optimizing id27s for 

sentence representations (kenter et al. 2016)

towards universal paraphrastic sentence embeddings (wieting et al. 2016)

what do we want our 

systems to learn?

id27-based antonym detection using thesauri 

and distributional information (ono et al. 2015)

pre-trained id27s

retro   tting word vectors to semantic lexicons (faruqui et al 2015)

low-dimensional embeddings of logic (rocktaschel et al. 2014)

 learning semantic id27s based on 
ordinal knowledge constraints (liu et al  2015)

counter-   tting word vectors to linguistic constraints (mrk  i   et al. 2016)
integrating distributional lexical contrast into id27s 

for antonym-synonym distinction (nguyen et al. 2016) 

identifying and exploiting hearst patterns in distributional vectors for 

sensembed: learning sense embeddings    (iacobacci et al 2015)

lexical entailment (roller and erk 2016)

autoextend: extending id27s   (rothe and schutze 2015)

 skip-thought vectors (kiros et al. 2015)

distributed representations of sentences and 

documents (quoc et al. 2014)

 a structured self-attentive sentence embedding (lin et al. 2017)
task-independent sentence embeddings

 supervised learning of universal sentence representations from 

natural language id136 data (conneau et al. 2017)

 dynamic pooling and unfolding recursive autoencoders for 

paraphrase detection (socher et al. 2011)

id99 and grounding
 embedding multimodal relational data (open review 2018)

 siamese cbow: optimizing id27s for 

sentence representations (kenter et al. 2016)

towards universal paraphrastic sentence embeddings (wieting et al. 2016)

 learning structured embeddings of knowledge bases (bordes et al. 2011)

 multimodal neural language models (kiros et al 2014)

 devise: a deep visual-semantic embedding model (frome et al 2013)

learning semantic hierarchies via id27s (fu et al. 2014)

improved representation learning for predicting commonsense 

ontologies (li et al 2017) 

 deep visual-semantic alignments for generating image 

descriptions (karpathy and fei fei 2015)

this workshop deals with the evaluation of general-purpose 
vector representations for linguistic units (morphemes, words, 

phrases, sentences, etc). what distinguishes these 

representations (or embeddings) is that they are not trained with a 
speci   c application in mind, but rather to capture broadly useful 
features of the represented units. another way to view their usage 

is through the lens of id21: the embeddings are 

trained with one objective, but applied on others. 

evaluating general-purpose representation learning systems is 

fundamentally dif   cult. they can be trained on a variety of 
objectives, making simple intrinsic evaluations useless as a 

means of comparing methods. they are also meant to be applied 

to a variety of downstream tasks, which will place different 

demands on them   

repeval 2017  
(bowman, goldberg, hill, lazaridou, levy, reichart, and s  gaard)

this workshop deals with the evaluation of general-purpose 

vector representations for linguistic units 
(morphemes, words, phrases, sentences, etc). what 

distinguishes these representations (or embeddings) is that 
they are not trained with a speci   c application in mind, but 
rather to capture broadly useful features of the represented 
units. another way to view their usage is through the lens of 

id21: the embeddings are trained with one 

objective, but applied on others. 

evaluating general-purpose representation learning systems is 

fundamentally dif   cult. they can be trained on a variety of 
objectives, making simple intrinsic evaluations useless as a 
means of comparing methods. they are also meant to be 

applied to a variety of downstream tasks   

repeval 2017  
(bowman, goldberg, hill, lazaridou, levy, reichart, and s  gaard)

this workshop deals with the evaluation of general-purpose 
vector representations for linguistic units (morphemes, words, 

phrases, sentences, etc). what distinguishes these 

representations (or embeddings) is that they are not trained with a 
speci   c application in mind, but rather to capture broadly 
useful features of the represented units. another way to view 
embeddings are trained with one objective, but applied on others. 

their usage is through the lens of id21: the 

evaluating general-purpose representation learning systems is 

fundamentally dif   cult. they can be trained on a variety of 
objectives, making simple intrinsic evaluations useless as a 

means of comparing methods. they are also meant to be applied 

to a variety of downstream tasks, which will place different 

demands on them   

repeval 2017  
(bowman, goldberg, hill, lazaridou, levy, reichart, and s  gaard)

this workshop deals with the evaluation of general-purpose 
vector representations for linguistic units (morphemes, words, 

phrases, sentences, etc). what distinguishes these 

representations (or embeddings) is that they are not trained with a 
speci   c application in mind, but rather to capture broadly useful 
features of the represented units. another way to view their usage 

is through the lens of id21: the embeddings are 

trained with one objective, but applied on others. 

evaluating general-purpose representation learning systems is 

fundamentally dif   cult. they can be trained on a variety of 
objectives, making simple intrinsic evaluations useless as a 
means of comparing methods. they are also meant to be 
applied to a variety of downstream tasks, which will 

place different demands on them   

repeval 2017  
(bowman, goldberg, hill, lazaridou, levy, reichart, and s  gaard)

   there is in my opinion no important theoretical difference 
between natural languages and the arti   cial languages of 
logicians; indeed i consider it possible to comprehend the 
syntax and semantics of both kinds of languages with a 

single natural and mathematically precise theory.   

   richard montague  

   there is in my opinion no important theoretical 
difference between natural languages and the 
artificial languages of logicians; indeed i 
consider it possible to comprehend the syntax and 

semantics of both kinds of languages with a single natural 

and mathematically precise theory.   

   richard montague  

   there is in my opinion no important theoretical difference 
between natural languages and the arti   cial languages of 
comprehend the syntax and semantics of both kinds of 

logicians; indeed i consider it possible to 
languages with a single natural and 
mathematically precise theory.   

   richard montague  

language     math

language        x   y(p(f(x))      (q(f(y),x)))

language        g.(  x.g (x x))(  x.g (x x))

language     ct=ft   ct-1+it     c(wcxt+ucht-1+bc) 

ht = ot       h(ct)

language     0101101101010010101010

language     0101101101010010101010

the semantics-pragmatics 

interface

the semantics-pragmatics 

interface

the semantics-pragmatics 

interface

i went to the beach over vacation.

the semantics-pragmatics 

interface

i went to the beach over vacation.

the semantics-pragmatics 

interface

i went to the beach over vacation.

s
c

i
t

n
a
m
e
s

the semantics-pragmatics 

interface

i went to the beach over vacation.

s
c

i
t

n
a
m
e
s

s
c
i
t
a
m
g
a
r
p

the semantics-pragmatics 

interface

i went to the beach over vacation.

s
c
i
t
n
a
m
e
s

s
c

i
t

a
m
g
a

context tbd

the semantics-pragmatics 

interface

i went to the beach over vacation.

i laid out in the sun.

context tbd

s
c
i
t
n
a
m
e
s

s
c

i
t

a
m
g
a

the semantics-pragmatics 

interface

i went to the beach over vacation.

i laid out in the sun.

s
c

i
t
n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

what    belongs    in the 
representation of a word?

beach

what    belongs    in the 
representation of a word?

location near 

the water

beach

what    belongs    in the 
representation of a word?

location near 

the water

is a place

beach

what    belongs    in the 
representation of a word?

beach

location near 

the water

is a place

is a popular 
vacation place

what    belongs    in the 
representation of a word?

location near 

the water

beach

is a place

not indoors

is a popular 
vacation place

what    belongs    in the 
representation of a word?

location near 

the water

beach

is a place

not indoors

is a popular 
vacation place

may have palm trees

what    belongs    in the 
representation of a word?

location near 

the water

beach

is a place

not indoors

is a popular 
vacation place

p(palm trees)

may have palm trees

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

not indoors

is a popular 
vacation place

p(palm trees)

may have palm trees

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

is a popular 
vacation place

   dictionary    
not indoors
representation

may have palm trees

p(palm trees)

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

not indoors

is a popular 
vacation place

p(palm trees)

taxonomic 
representation
may have palm trees

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

knowledge base 
not indoors
representation

is a popular 
vacation place

p(palm trees)

may have palm trees

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

prototype 
not indoors
representation

is a popular 
vacation place

may have palm trees

p(palm trees)

what    belongs    in the 
representation of a word?

location near 

the water

p(it is okay for a listener to 
imagine that it has palm trees)
beach

is a place

is a popular 
vacation place

   encyclopedic    
representation
not indoors

p(palm trees)

may have palm trees

is skipgram enough?

p(occurs after    on   )

p(occurs after    the   )

p(occurs after    sandy   )

p(occurs before    vacation   )

beach

p(occurs after    clandestine   )

p(occurs before    grinning   )

distributional contextual 

representation

should we care about 

linguistics?

should we care about 

linguistics?

yes. 

because we have to form and 
test hypotheses about what 
our word representations 

should capture. 

should we care about 

linguistics?

yes. 

because we have to form and 
test hypotheses about what 
our word representations 

should capture. 

what    belongs    in the 
representation of a word?

beach

model theoretic 
representation

what    belongs    in the 
representation of a word?

beach

model theoretic 
representation

what    belongs    in the 
representation of a word?

beach

what    belongs    in the 
representation of a word?
sandy 
beach

beach

what    belongs    in the 
representation of a word?
sandy 
beach

beach

tropical 
beach

what    belongs    in the 
representation of a word?
sandy 
beach

beach

snowy 
beach

tropical 
beach

set-theoretic semantics

beach

sandy 
beach

a little boy doing a hand 

stand on the beach.

set-theoretic semantics

beach

sandy 
beach

a little boy doing a hand 

stand on the beach.

a little boy doing a hand 
stand on the sandy beach.

set-theoretic semantics

   x(beach(x)     sandy_beach(x))

beach

sandy 
beach

a little boy doing a hand 

stand on the beach.

a little boy doing a hand 
stand on the sandy beach.

set-theoretic semantics

   x(beach(x)     sandy_beach(x))

beach

sandy 
beach

a little boy doing a hand 

stand on the beach.

a little boy doing a hand 
stand on the sandy beach.

set-theoretic semantics does not 

allow this id136.

set-theoretic semantics?

a little boy doing a hand stand on the beach.

a little boy doing a hand stand on the sandy beach.

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

a little boy doing a hand stand on the beach.

+

a little boy doing a hand stand on the sandy beach.

human annotators

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

a little boy doing a hand stand on the beach.

+

a little boy doing a hand stand on the sandy beach.

human annotators

yes

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

   x(beach(x)     sandy_beach(x))

beach

sandy 
beach

a little boy doing a hand 

stand on the beach.

a little boy doing a hand 
stand on the sandy beach.

set-theoretic semantics does not 

allow this id136.

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

   x(beach(x)     sandy_beach(x))

a little boy doing a hand 

stand on the beach.

a little boy doing a hand 
stand on the sandy beach.

beach

sandy 
beach

hu m an subjects 
unanim ously agree 
that this id136 in valid. 

set-theoretic semantics does not 

allow this id136.

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

model-theoretic  

prediction is  
incorrect

model-theoretic  

prediction is  

correct

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

news

model-theoretic  

prediction is  
incorrect

model-theoretic  

prediction is  

correct

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

images

news

literature

debate forums

model-theoretic  

prediction is  
incorrect

model-theoretic  

prediction is  

correct

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

set-theoretic semantics?

images

news

literature

debate forums

model-theoretic  

prediction is  
incorrect

model-theoretic  

prediction is  

correct

in the worst case, model-theoretic 
representation makes incorrect 
predictions 47% of the time!

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

human id136s

p entails h

p contradicts h

somehow, i feel there will be a lack of 

evidence forthcoming 

evidence -> credible evidence

pen   eld evans grasped his hand and 

shook it warmly. 

hand -> outstretched hand

bush travels monday to michigan to 
make remarks on the economy. 
economy -/-> japanese economy

government is the only thing holding 
government -/-> small government

back large corporations.    

his body is found a week later.   

body -> dead body

a child rides on a man   s shoulders.    
most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

man -/-> homeless man

what    belongs    in the 
representation of a word?

somehow, i feel there will be a lack of 

evidence forthcoming 

evidence -> credible evidence

pen   eld evans grasped his hand and 

shook it warmly. 

hand -> outstretched hand

bush travels monday to michigan to 

make remarks on the economy. 
economy -/-> japanese economy

government is the only thing holding 

back large corporations.    

government -/-> small government

his body is found a week later.   

body -> dead body

a child rides on a man   s shoulders.    

man -/-> homeless man

what    belongs    in the 
representation of a word?

evidence -> is credible?

body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?

economy -> isn   t japanese?

what    belongs    in the 
representation of a word?

evidence -> is credible?

body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?

economy -> isn   t japanese?

s
c

i
t

n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

what    belongs    in the 
representation of a word?

evidence -> is credible?

body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?

economy -> isn   t japanese?

s
c

i
t

n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

what    belongs    in the 
representation of a word?

evidence -> is credible?

body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?

economy -> isn   t japanese?

s
c

i
t

n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

s
c

i
t

n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

evidence -> is credible?
body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?
economy -> isn   t japanese?

should we care about 

linguistics?

s
c

i
t

n
a
m
e
s

s
c

i
t

a
m
g
a
r
p

evidence -> is credible?
body -> is dead?

government -> isn   t small?

man -> isn   t homeless?

hand -> is outstretched?
economy -> isn   t japanese?

recognizing textual 

entailment task

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

recognizing textual 

entailment task

a group of hikers walk a path that leads 

from a sandy beach towards a hill

+

the hikers are walking outside

rte system

true

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

a hiker walking on a path at the foot of 

snow capped mountains

a hiker walking on a sandy path at the 

foot of snow capped mountains

+

rte system

false

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

    5,378 add-one pairs 

    4,991 for training (4,481 training, 510 dev) 

    387 test (removed pairs with low human 

agreement) 

    500k general rte pairs from snli

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

    5,378 add-one pairs 

    4,991 for training (4,481 training, 510 dev) 

    387 test (removed pairs with low human 

agreement) 

    500k general rte pairs from snli

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

    5,378 add-one pairs 

    4,991 for training (4,481 training, 510 dev) 

    387 test (removed pairs with low human 

agreement) 

    500k general rte pairs from snli

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

    5,378 add-one pairs 

    4,991 for training (4,481 training, 510 dev) 

    387 test (removed pairs with low human 

agreement) 

    500k general rte pairs from snli

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

simpli   ed rte task

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

85.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.285.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.28685.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

s
r
o

t

c
e
v

 
f

 

o
g
a
b

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.286.68685.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

s
r
o

t

c
e
v

 
f

 

o
g
a
b

n
n
r

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.286.886.687.386.68685.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

s
r
o

t

c
e
v

 
f

 

o
g
a
b

n
n
r

m
t
s
l

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.286.886.687.386.68685.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

s
r
o

t

c
e
v

 
f

 

o
g
a
b

n
n
r

m
t
s
l

 
r
e

 

f
s
n
a
r
t
+
m
t
s
l

 

)
5
1
0
2
(
 
.
l

a

 
t

e

 

n
a
m
w
o
b

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.286.886.687.386.68685.3simpli   ed rte task

   
t

 
t

i

c
d
e
r
p
 
s
y
a
w
a

l

n
e
m

l
i

t

a
n
e
-
n
o
n
   

s
d
r
o
w

 
f

 

o
g
a
b

s
r
o

t

c
e
v

 
f

 

o
g
a
b

n
n
r

m
t
s
l

 
r
e

 

f
s
n
a
r
t
+
m
t
s
l

 

)
5
1
0
2
(
 
.
l

a

 
t

e

 

n
a
m
w
o
b

 
s
s
a
c

l

 
t

e
v
i
t

n
e
u
q
e
r
f

j

c
e
d
a
 
y
b

 
t
s
o
m

most    babies    are    little    and most    problems    are    huge   :  
compositional entailment in adjective-nouns 
pavlick and callison-burch (2016)

y
c
a
r
u
c
c
a

100

92

84

76

68

60

92.286.886.687.386.68685.3simpli   ed rte task

add-one adjective

sick

y
c
a
r
u
c
c
a

90

84

78

72

66

60

l

y
n
o

 

m
t
s
l

t

 

a
a
d
e
n
o
-
d
d
a

r
e

 

f
s
n
a
r
t
+
m
t
s
l

 

r
e

 

f
s
n
a
r
t
+
m
t
s
l

 

m
t
s
l

l

 

t

y
n
o
a
a
d
k
c
s

 

i

y
c
a
r
u
c
c
a

90

84

78

72

66

60

86.886.680.871.3   
   
takeaways

    should we care about linguistics? yes! 

    because we want to learn task-independent representations 

of language, which requires asking and answering: 

1. what components of linguistic meaning are    intrinsic   , and 

what is derived in context/at    runtime   ? 

2. if these representation can   t be trained in end-to-end tasks: 

how to we know what is the    right    representation? which 
tasks should be viewed as    fundamental    and trained/test 
explicitly, and which ones should come along    for free   ?

takeaways

    should we care about linguistics? yes! 

    because we want to learn task-independent representations 

of language, which requires asking and answering: 

1. what components of linguistic meaning are    intrinsic   , and 

what is derived in context/at    runtime   ? 

2. if these representation can   t be trained in end-to-end tasks: 

how to we know what is the    right    representation? which 
tasks should be viewed as    fundamental    and trained/test 
explicitly, and which ones should come along    for free   ?

takeaways

    should we care about linguistics? yes! 

    because we want to learn task-independent representations 

of language, which requires asking and answering: 

1. what components of linguistic meaning are    intrinsic   , and 

what is derived in context/at    runtime   ? 

2. if these representation can   t be trained in end-to-end tasks: 

how to we know what is the    right    representation? which 
tasks should be viewed as    fundamental    and trained/test 
explicitly, and which ones should come along    for free   ?

takeaways

    should we care about linguistics? yes! 

    because we want to learn task-independent representations 

of language, which requires asking and answering: 

1. what components of linguistic meaning are    intrinsic   , and 

what is derived in context/at    runtime   ? 

2. if these representation can   t be trained in end-to-end tasks: 

how to we know what is the    right    representation? which 
tasks should be viewed as    fundamental    and trained/test 
explicitly, and which ones should come along    for free   ?

takeaways

    should we care about linguistics? yes! 

    because we want to learn task-independent representations 

of language, which requires asking and answering: 

1. what components of linguistic meaning are    intrinsic   , and 

what is derived in context/at    runtime   ? 

2. if these representation can   t be trained in end-to-end tasks: 

how to we know what is the    right    representation? which 
tasks should be viewed as    fundamental    and trained/test 
explicitly, and which ones should come along    for free   ?

thank you!

