interpretable neural network models 

for granger causality discovery

emily fox

university of washington 

amazon professor of machine learning

cse and statistics

modern sources of time series

until recently, ml (mostly) ignored time series
it   s hard!
# parameters (naively) grows rapidly with

(cid:1) # of series
(cid:1) complexity of dynamics captured

more 
data

algorithms more computationally intensive

more compute

theory not applicable because typically 
assume no time dependencies

importance of modeling dynamics

accuracy

independent 
observations
50%

time-dependent
observations
90%

with dynamic model, can get improved prediction accuracy

now time series are    in    

web-scale 
time series 
/ seq. data

large 
compute

deep 
learning 
advances 

id56s

lstms

grus

wavenet

id195

...

success

reinforcement 

learning

speech 
generation 

machine 
translation

speech 
recognition

nlp

medical 
records / 
healthcare

but, success also relies on   

   lots of correspondence data
   lots of trials of a robot navigating every part of 
the maze
   lots of transcribed audio

   seen this structure in a maze before
   seen these words in this context before
   seen patient with these symptoms and test 
results before

lots of 
replicated 
series

manageable 
contextual 
memory
clear 
prediction 
objective

demand forecasting of new item:
tons of data, but not for question 
of interest

but, success also relies on   

   lots of correspondence data
   lots of trials of a robot navigating every part of 
the maze
   lots of transcribed audio

   seen this structure in a maze before
   seen these words in this context before
   seen patient with these symptoms and test 
results before

lots of 
replicated 
series

manageable 
contextual 
memory
clear 
prediction 
objective

demand forecasting of new item:
extremely complicated context:
tons of data, but not for question 
air temperature, dew point, 
of interest
relative humidity, wind direction, 
wind speed, altimeter, sea level 
pressure, precipitation, visibility, 
wind gust, cloud coverage, cloud 
height, present weather code

but, success also relies on   

lots of 
replicated 
series

manageable 
contextual 
memory
clear 
prediction 
objective

   lots of correspondence data
   lots of trials of a robot navigating every part of 
the maze
   lots of transcribed audio

   seen this structure in a maze before
   seen these words in this context before
   seen patient with these symptoms and test 
results before

   word error rate for id103
   id7 score for machine translation
   reward function in id23

beyond prediction on big data

characterizing 

dynamics

efficiently 
sharing 
information

interpretable 
interactions

non-stationarity 
& measurement 

bias

spectral analysis

- frequency domain analysis
- local stationarity
- time-frequency analysis

spectral analysis of neuroimaging data

magnetoencephalography (meg) data of brain activation over time

time

time

goal: 
infer functional 
connectivity

discovering human motion behaviors

parse videos into underlying behaviors without training labels 

)
)
g
g
e
e
d
d
(
(
 
 
e
e
g
g
n
n
a
a

l
l

50
50

25
25

0
0

   25
   25

recording 
segmen@ng 
one recording
modeled as 
switches 
between 
simple 
behaviors

a
a

 
 

20
20

f
f

40
40

60
60

80
80

 
 

torso pos.
torso pos.
neck
neck
waist x
waist x
waist y
waist y
right arm
right arm
left arm
left arm
right wrist
right wrist
left wrist
left wrist
right leg
right leg
left leg
left leg
right foot
right foot
left foot
left foot

d
d

c
c

b
b

100
100

120
120

timesteps
timesteps

140
140

160
160

180
180

200
200

challenges:	
       segmenta@on into behaviors
       parameters per behavior
       how many behaviors?

fox, hughes, sudderth, and jordan, aoas 2014 

s7cky	

hdp-id48	

automatically parsing large collections

joint modeling of multiple time series via the beta process 31

ballet

dance

playground swing

tai chi

climb

jump jack

arm circle

squat

slide step

cartwheels

dribble

swordplay

boxing

knee raise

lambada

toe touch

ideas appear in many domains   
example applications:
    parsing eeg recordings
    speech segmentation
    volatility regimes in financial time 

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16

series

    genomics
        

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16

01
02
03
04
05
06
07
08
09
10
11
12
13
14
15
16

0
.
0
1

0
.
0
2

0
.
0
3

0
.
0
4

0
.
1

0
.
2

0
.
3

0
.
4

0
.
5

0 1 2

0 5 1
0

beyond prediction on big data

characterizing 

dynamics

efficiently 
sharing 
information

interpretable 
interactions

non-stationarity 
& measurement 

bias

predicting demand for products

long-range and cold-start forecasting

long-range forecasting
cold start (new product)

time

prediction task with lots of 
data, but not much for 
question of interest

leveraging low-dim structure + side info

low-rank description of observed 
years of available products

function approx. via 
neural network

+

product-
specific 
meta data

 

xie, tank, and fox, nips time series workshop 2016.

analysis of wikipedia data

4500 wikipedia articles

daily page traffic counts 2008-2014

per article, 1 to 6 years of data
  29,000 columns

features = tf-idf of article summary 
   22,000 dimensions, but sparse

xie, tank, and fox, nips time series workshop 2016.

(wiki trends: georges st-pierre)

long-range forecasts

apollo 2014

true

predicted

long-range forecasts

economics 2014

cold start forecasts

calvin coolidge 2014

cold start forecasts

list of ncaa men's division i basketball champions 2014

modeling a local housing index

census tracts in 
seattle, wa

9

22

?  

63

 n
 
o
 
 
t
 
 
g
 
 
n
 
 
i
 
 
h
 
 
s
 
 
a

what is the value of housing 
in each region over time?

40

41

census 2010

seattle, washington
census tracts

s  o   u   n   d

w
n
 
e
v
 a
w
e
i
v
a
e
s

32

15

 

w
n
e
v
a
h
t
4
2

 

31

nw 70th st

 

w
n
e
v
a
h
t
4
2

 

57

w government way

 

w
e
v
a
h
t
0
3

 

w emerson st

56

 

w
e
v
a
h
t
4
3

 

 

w
e
v
a
h
t
7
2

 

f

1

miles

1.5

2

0

0.25

0.5

96

alki ave sw

sw admiral way

97.01

97.02

 

w
s
e
v
a
h
t
5
4

 

n 145th st

n 137th st

4.01

bitter
  lake

n 130th st

3

r

o

o

s

 

e
n
e
v
a
h
t
5

 

e

v

e

!"`$

lt w

a
y n

haller
 lake

6

n 107th st

 

 

n
e
v
a
n
a
d
r
e
m

i

i

5

h

16

 

w
n
e
v
a
h
t
4
1

 

nw 85th st

 

w
n
e
v
a
d
r
3

 

4.02

14

n 105th st

 

 

n
e
v
a
a
r
o
r
u
a

?  

w

d n

o l m a n r
17.02

 

 

n
e
v
a
t
n
o
m
e
r
f

17.01

13

n 95th st

18

 

w
n
e
v
a
h
t
0
2

 

30

 

w
n
e
v
a
h
t
8

 

29

 

 

n
e
v
a
d
o
o
w
n
e
e
r
g

28

 

w
n
e
v
a
t
s
1

 

33

nw 60th st

34

nw market st
47

green
lake

46

n 50th st

35

 

 

n
e
v
a
a
r
o
r
u
a

?  

 

e
n
e
v
a
h
t
0
3

 

2

ne 125th st

e

ay n
7
y w
e cit

k
a
l

1

s

a

n

d

 

p

o

i

n

8

 

e
n
e
v
a
h
t
5
1

 

ne 115th st

12

19

27

 

 

e
n
y
a
w
t
l
e
v
e
s
o
o
r

36

45

52

11

10

ne 105th st

t

 

w

a

y

 

n

e

ne 100th st

20

21

 

e
n
e
v
a
h
t
5
3

 

?  

 

e
n
e
v
a
h
t
0
2

 

26

44

ne 85th st

25

24

ne 75th st

 

e
n
e
v
a
h
t
5
4

 

38

39

ne 65th st

 

e
n
e
v
a
h
t
0
5

 

 

e
n
e
v
a
h
t
5
2

 

43.01

sand point w ay ne

42

?  

43.02

53.01
ne 45th st

 

r
b
d
r
a
l
l
a
b

58.01

w dravus st

 

w
e
v
a
h
t
4
1

 

g

i

l

m

a

e w
e av

k
y
d
n
r
o
th

magnolia br

58.02

e

l

l
i

o

t

 

 

n
e
v
a
d
o
o
w
n
e
e
r
g

48

l

e

a

r

y

 

w

a

y

 

n

w

59

 

w
e
v
a
d
r
3

 

w mcgraw st

60

 

 

n
e
v
a
l
l
h
b
o
n

 

i

n

 

d

r

 

69

w

 

w
e
v
a
h
t
4

 

68

w galer st

t
 
a

v

e
 

w

70

mercer st
71
denny way

50

n 41st st

49

 

e
n
e
v
a
t
s
1

 

51

 

 

n
e
v
a
d
r
o
f
g
n
l
l
a
w

i

6

t

h

 

a

v

e

 

n

67

roy st

 

 

n
e
v
a
e
k
a
l
t
s
e
w

?  
72

3

r

d a

v

e

n 37th st

54

61

e lynn st

lake
union

66

 

e
e
v
a
h
t
0
1

 

65

74.01

 

e
y
a
w
d
a
o
r
b

75

 

 

e
e
v
a
e
s
o
r
l
e
m

73

li v

o

5

t

y

a

e   w
82 83

h

 

a

v

74.02
84

n

i

m

o

r

 

a

v

e

81

e

r i o

a

m

t
n  s
85

86

53.02

 

 

e
n
r
d
r
e
b
r
u
s

n

e p
a

cific s

t

62

e galer st

64

e roy st

76

e denny way

 

 

e
r
d
m
u
t
e
r
o
b
r
a

77

e m adiso n st
hillside 
dr e

79

e union st

e marion st

87

 

e
v
a
d
r
3
2

e
v
a
 
t
s
1
3

88

78

 

e
e
v
a
h
t
5
1

 

80.01

w all st

80.02

elliott bay

92

s jackson st

91

yesler way
90

s dearborn st

?  

s holgate st

?  

93

h
a

r

b

o

r

 

a
v

e

 

s
w

99

 

 

s
y
a
w
t
r
o
p
r
a

i

94

s bayview st

100.02

 

s
e
v
a
d
r
3
2

 

r

a

i

n

i

e

r

 

a

v

e

 

s

 

s
e
v
a
h
t
3
1

 

89

s judkins st

98

west seattle br

s spokane st

s hinds st

!"b$

 

 

w

95

 

 

 

e

 

s
e
v
a
h
t
2
1

 

100.01

s snoqualmie st

101

s genesee st

s alaska st

s dawson st

s lucile st

104.02

103

b

e

a

c

o

n

 

a
v

e

 

s

m

 

l

 

k

i

n

104.01

g

 

j

r

 

w

a

y

 

s

s graham st

109

 

s
e
v
a
h
t
6
4

 

e

a

s

t m

a

r

gin

a

l w

a

y s

 

s
v
a
 
t
s
1

a

i

r

p

o

r

110.02

t
 

w

a

r

a

i

n

111.01

i

e

r

 

a

v

e

 

s

y

 

s

!"`$

110.01

s kenyon st

 
 
k

  
a
  

l

102

111.02

s

w

if

t a

v

e s

s myrtle st

118

117

r

e

n

t

o

n

 

a

v

e

 

s

 

s
e
v
a
t
s
1
5

 

s norfolk st

119

260.01

 

w
s
e
v
a
h
t
4
5

 

note:
tracts 260.01, 264 and 265 
extend beyond seattle city 
limits. only the portion of each 
tract within city limits
is shown here.

data source:
census 2010 tiger/line 
redistricting data
u.s. census bureau

116

t
 
 
 

e

 

 

 

g

 
 
 

u

 
 
 

p

no warranties of any sort, including
accuracy, fitness, or merchantability
accompany this product.

 

w
s
e
v
a
h
t
9
4

 

sw oregon st

105

sw findlay st

 

w
s
e
v
a
h
t
5
3

 

106

107.02

 

 

w
s
y
a
w
e
g
d
r
l
e
d

i

108

w
e

s

t

 

m
a
r
g

i

n
a

l

 

w
a
y

 

s
w

sw myrtle st

107.01

sw holden st

 

w
s
e
v
a
h
t
0
3

 

115

sw thistle st

114.01

 

w
s
e
v
a
h
t
9
3

 

 

w
s
e
v
a
h
t
6
1

 

sw trenton st

sw barton st

114.02
sw roxbury st

120

sw 106th st

121

?  

112

113

?  

olson pl sw

m

y

e

r

s

 

w

a

y

 

s

265

264

challenge: spatiotemporally sparse data

0.97

0.99

0.81

s
t
c
a
r
t
 
f

o
 
n
o
i
t
c
a
r
f

0.41

0.11

<1

<3

<5

<7

<9

average # of monthly house sales 

0

.

1
   

challenge: spatiotemporally sparse data

2001   01

2005   01

2001   01

2005   01

2009   01

2013   01

2009   01

1997   01

1
   

0

.

2013   01

y. ren et al.

1997   01

(c) tract 340184 ( #obs = 4 )
(b) tract 281980

(d) tract 340184

.

0
1

0

.

1

5
0

.

5

.

0

)
e
c
0
i
r
.
p
0
(
g
o

l

0

.

0

5
.
0
   

5

.

0
   

0

.

1
   

0
.
1
   

)
e
c
i
r

p
(
g
o

l

.

0
1

5
0

.

0
.
0

5
.
0
   

0

.

1
   

1997   01

1997   01

2001   01

2001   01

2005   01

2005   01

2009   01

2009   01

2013   01

2013   01

1997   01

2001   01

2005   01

2009   01

2013   01

(d) tract 340184

solution: cluster regions based on 
underlying price dynamics

y. ren et al.

20

cluster 1

cluster 2

discover 
groups of 
tracts with 
correlated 
dynamics

s
s
e
c
o
r
p
 
t
n
e
t
a
l

s
s
e
c
o
r
p

 
t

n
e
a
l

t

2
.
0

1
.
0

0
.
0

1
.
0
   

2
.
0
   

2

.

0

1

.

0

0

.

0

1

.

0
   

.

2
0
   

2010

2000

2005
time

cluster 3

s
s
e
c
o
r
p
 
t
n
e
t
a
l

s
s
e
c
o
r
p

 
t

n
e
a
l

t

2
.
0

1
.
0

0
.
0

1
.
0
   

2
.
0
   

2

.

0

1

.

0

0

.

0

1

.

0
   

.

2
0
   

2000

2010

2005
time

ren, fox, and bruce, aoas 2017

2010

2000

2005
time

cluster 4

2000

2005
time

2010

0
0
0
0
0
2

fig 11. estimated global trend using the seasonality decomposition approach of cleveland
et al. (1990), after adjusting for hedonic e   ects.

seattle city analysis 
(17 years, 140 tracts, 125k transactions)

(b)
2005   01

2009   01

1997   01

2001   01

2013   01

2005   01

y. ren et al.

2001   01

2009   01

1997   01

2013   01

(b)

26

fig 11. estimated global trend using the seasonality decomposition approach of cleveland
et al. (1990), after adjusting for hedonic e   ects.

cluster 1 has 14 tracts

improvement over case shiller

tl_2010_53_tract10
clusterid_reassign
cluster id

in house sales predictions

tl_2010_53_tract10
clusterid_reassign
cluster id

1997   01 2001   01 2005   01 2009   01 2013   01

1997   01 2001   01 2005   01 2009   01 2013   01

cluster 2 has 3 tracts

cluster average latent process

cluster average latent process

p
(
g
o

p
(
g
o

p
(
g
o

)
e
c
i
r

)
e
c
i
r

)
e
c
i
r

0
   

0
   

0
   

0
   

0
   

0
   

3

1

0

1

0

3

3

1

0

1

0

3

3

1

0

1

0

3

.

.

.

.

.

.

.

.

.

.

.

.

l

l

l

cluster 3 has 45 tracts

cluster average latent process

cluster 4 has 1 tracts

cluster average latent process

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

18
16
14
12
10
8
6
4
2
0

cluster map

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

1997   01 2001   01 2005   01 2009   01 2013   01

cluster 8 has 8 tracts

time

cluster average latent process

cluster 7 has 2 tracts

time

cluster average latent process

3

.

0

1

.

0

1

.

0
   

)
e
c
i
r

p
(
g
o

l

cluster 6 has 5 tracts

time

cluster average latent process

cluster 5 has 1 tracts

time

cluster average latent process

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

cluster 10 has 17 tracts

time

cluster average latent process

cluster 9 has 2 tracts

time

cluster average latent process

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

mean ape

cluster 11 has 29 tracts

time

3

.

0
   

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

cluster average latent process

90% ape

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

1997   01 2001   01 2005   01 2009   01 2013   01

cluster 12 has 5 tracts

time

cluster average latent process

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

cluster 14 has 1 tracts

time

cluster average latent process

cluster 13 has 4 tracts

time

cluster average latent process

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

cluster 15 has 2 tracts

time

cluster average latent process

)
e
c
i
r

p
(
g
o

l

3

.

0

1

.

0

1

.

0
   

3

.

cluster 16 has 1 tracts

time

cluster average latent process

0
   

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

0
   

1997   01 2001   01 2005   01 2009   01 2013   01

fig 12. map of clusters under the map sample. the cluster labels and associated map
colors are selected to indicate the level of deviance of the cluster   s average (across tracts)
latent trend from the global trend. blue (1) represents a small deviance while red (16)

fig 12. map of clusters under the map sample. the cluster labels and associated map
colors are selected to indicate the level of deviance of the cluster   s average (across tracts)
latent trend from the global trend. blue (1) represents a small deviance while red (16)
represents the largest.

densest 5% middle 50% sparsest 5%
ren, fox, and bruce, aoas 2017

(colored by deviation
from global trend)

cluster-mean (log) latent price dynamics

fig 13. under the map sample, cluster-average intrinsic price dynamics computed by
averaging x1:t,i over all i with zi = k for k = 1, . . . , 16. the color scheme is the same as
in figure 12.

recap: mechanisms for coping with limited data

e

ti m

clusters and hierarchies

e

ti m

sparse directed interactions

e

ti m

switching between simpler 

dynamics

e

ti m

e
low-dimensional embeddings

ti m

beyond prediction on big data

characterizing 

dynamics

efficiently 
sharing 
information

interpretable 
interactions

non-stationarity 
& measurement 

bias

another data-scarce study:
dynamics of homelessness
(cid:1) counts occur on single night in 

january

(cid:1) count method varies from metro 

to metro and across time
(cid:1) observe most of those in 

shelters and only a fraction of 
those on the streets

(cid:1) % sheltered varies largely 

between metros

measurement bias!

what is the 1-yr-ahead forecast of homeless population? 

san francisco

      

   

   

   

   

9000

s
s
e

l

e
m
o
h
#

 

8000

   

7000

   

2012

bayesian model-based 
approach accounts for:
    imperfect measurement 
mechanism and changes 
in count quality
    predicted increase in 
total population 
(nonstationary process)

2014
year

2016
glynn and fox, arxiv 2017

beyond prediction on big data

characterizing 

dynamics

efficiently 
sharing 
information

interpretable 
interactions

non-stationarity 
& measurement 

bias

why are interactions important?

maintain

postcentralgyrus-lh

fef-lh

paracentralgyrus-rh

postcentralgyrus-rh

fef-rh

aud-rh

bankssts-rh

switch

postcentralgyrus-lh

paracentralgyrus-rh

fef-lh

postcentralgyrus-rh

ips-rh

rtpj-rh

aud-rh
bankstss-rh

fef-rh

functional networks in the brain

gene regulatory networks

why are interactions important?

maintain

postcentralgyrus-lh

fef-lh

paracentralgyrus-rh

postcentralgyrus-rh

fef-rh

aud-rh

bankssts-rh

switch

postcentralgyrus-lh

paracentralgyrus-rh

fef-lh

postcentralgyrus-rh

ips-rh

rtpj-rh

aud-rh
bankstss-rh

fef-rh

functional networks in the brain

gene regulatory networks

interactions between players on the court

(video of results from benshitrit et al. iccv 2011)

discovering interactions between players

identify directed interactions 
between players and ball

e.g., position of point guard at 
time t influences ball at time t+1

granger causality selection     linear model

=

+

+

xt

a1

xt-1

a2

xt-2

et

xt =

kxk=1

akxt k + et

granger causality selection     linear model

=

+

+

xt

a1

series i does not granger cause series j iff aji,k = 0     k  

xt-1

xt-2

a2

et

lag k interaction

granger causality selection     linear model

=

+

+

xt

a1

xt-1

a2

xt-2

et

series i

series j

granger causality selection     linear model

=

+

+

xt

a1

xt-1

a2

xt-2

et

max loglike(x1,   ,xt ; a1,   ,ak)
a1,   ,ak

-       j,i penalty(aji,1,   ,aji,k     0)

explain data well

encourage (structured) 0   s

granger causality selection     linear model

=

+

+

xt

a1

xt-1

a2

xt-2

et

min

a1,...,ak

txt=k xt  

kxk=1

akxt k!2

+  xij

||(aji,1, . . . , aji,k)||2,

reconstruction error

group lasso penalty

the issue with a linear approach

maintain

postcentralgyrus-lh

fef-lh

paracentralgyrus-rh

postcentralgyrus-rh

fef-rh

aud-rh

bankssts-rh

switch

postcentralgyrus-lh

paracentralgyrus-rh

fef-lh

postcentralgyrus-rh

ips-rh

rtpj-rh

aud-rh
bankstss-rh

fef-rh

functional networks in the brain

what if interactions are 
nonlinear?

gene regulatory networks

interactions between players on the court

(video of results from benshitrit et al. iccv 2011)

modeling nonlinear dynamics

=

g1

g2

,

,

xt-1

,    

xt-2

nonlinear maps of 
+
past values

,    

xt

xt-1

xt-2

et

identifying granger causality

=

g1

g2

,

,

xt-1

xt-2

,    
series i not granger 
+
causal of j if   
invariant
,    

xt

xt-1

xt-2

et

using penalized neural networks

model 
=
using 
nns

g1

g2

,

,

xt-1

xt-2

,    

+

,    
penalize weights

xt

xt-1

xt-2

et

a multilayer id88 (mlp) approach

xt

x(t 1):(t k)

full set of outputs

mlp

lag k past values 

as inputs

a multilayer id88 (mlp) approach

xt

difficult to identify 

granger causality with 
shared hidden units 

x(t 1):(t k)

a multilayer id88 (mlp) approach

xt

all gi must rely on 
same set of lags

x(t 1):(t k)

penalized multilayer id88 (mlp)

for function gi
max

loglike(x1i,   ,xti ; w1,   ,wl) 
-       j penalty(wj     0)

x(t 1):(t k)

specifying the mlp     function gi

linear output decoder:

ohl

xti = wt
t +    t
layer l hidden values:
h`

layer 1 hidden values:

t =   w `h` 1
t =    kxk=1

t + b` 
w 1kxt k + b1!

h1

lag-specific weights

x(t 1):(t k)

disentangling input to output effects

x(t 1):(t k)
(x(t 1)2 . . . x(t k)2)

disentangling input to output effects
series j does not granger 
cause series i if group j 

weights are 0

place group-wise penalty 

on layer 1 weights
group inputs by:
(  k lags of series j )

(x(t 1)2 . . . x(t k)2)

penalized multilayer id88 (mlp)

reconstruction error

min
w

txt=k xit   gi(x(t 1):(t k)) 2
pxj=1

weights from series j at all lags
||(w 11

:j , . . . , w 1k

:j )||f

+  

group lasso penalty

(x(t 1)2 . . . x(t k)2)

algorithmic notes   
often, focus of deep learning evaluation is on prediction error   
can get away with optimizing approximately

sgd

we care about zeros of solution, so important to get very close 
to a stationary point

proximal id119 with line search 

simulated results     mlp 

lorenz-96 (nonlinear) data

var (linear) data

min
w

lag selection via hierarchical group lasso
txt=k xit   gi(x(t 1):(t k)) 2
pxj=1
pxj=1

||(w 11
kxk=1

:j , . . . , w 1k

:j , . . . , w 1k

group lasso penalty

||(w 1k

:j )||f

:j )||f

+  

 

hierarchical

group lasso penalty

(x(t 1)2 . . . x(t k)2)

lag selection results

increasing sparsity penalty !

multilayer id88

recurrent network

xt1

ht

xt1

h(t 1)1

x(t 1)1
x(t 1)2

 x(t 1)2 . . . x(t k)2 

long-range dependencies 
between series via 
nonlinear hidden variables

generic id56 formulation

hidden state evolution:

nonlinear fcn depending on architecture

ht = f (xt, ht 1)

hidden state capturing historical context

linear output layer (for simplicity):
xit = wt

oht +    t

h(t 1)1

x(t 1)1
x(t 1)2

ht

xt1

lstm specification
introduce cell state ct
in addition to ht

forget gate
input gate
output gate
cell state 
evolution
hidden state 
evolution

ft =   w f xt + u f h(t 1) 
it =   w inxt + u inh(t 1) 
ot =   w oxt + u oh(t 1)i 
ct = ft   ct 1 + it     w cxt + u ch(t 1) 

ht = ot    (ct)

control how cell state is 
updated and transferred to 
predicted hidden state

weights of the lstm

define effect of input on prediction

forget gate
input gate
output gate
cell state 
evolution
hidden state 
evolution

ft =   w f xt + u f h(t 1) 
it =   w inxt + u inh(t 1) 
ot =   w oxt + u oh(t 1)i 
ct = ft   ct 1 + it     w cxt + u ch(t 1) 

ht = ot    (ct)

a penalized lstm

define effect of input on prediction

series j does not granger cause series i if 

jth column of weights w is 0

min

w,u,wo

txt=2

(xit   gi(x<t))2 +  

pxj=1

||w:j||2

reconstruction error

group lasso penalty

dream3 challenge

difficult non-linear dataset used to benchmark 
granger causality detection

simulated gene expression and regulation dynamics for:

    2 e.coli and 3 yeast
    100 series (network size)
    46 replicates 
    21 time points

very different 
structures

structure extracted from currently established gene regulatory networks

dream3 results

% auroc

cmlp
clstm
okvar
lasso
g1dbn

e.c.1

e.c.2

y.1

y.2

y.3

75
70
65
60
55
50
45
40

interactions of the human body

30

25

20

15

10

5

y

   2

0

2

4

   10

   15

0

   5

x

z

25

20

15

10

5

0

y

10

5

30

25

20

y

15

10

5

0
   4

   2

z

y

25

20

15

10

5

0

   10

   5

   10

10

   5

5

0

z

5

10

   10

15

   15

0

   5

x

30

25

20

y

15

10

5

10

5

10

5

0

2

   10

   15

0

   5

x

   4

   2

0

2

z

   5

   10

0

x

30

25

20

y

15

10

5

10

5

5

10

   5

   10

0

x

   5

z

0

   5

5

   10

0

x

y

30

25

20

15

10

5

0

z

30

25

20

y

15

10

5

10

5

   2

0

2

4

   5

   10

25

20

15

y

10

5

25

20

15

y

10

5

35

30

25

20

15

10

5

y

10

5

0

x

   15

   10

   5

5

10

5

   5

0

z

   10

   5

5

10

5

   5

0

z

   10

   5

0

x

0

5

10

x

0

5

   10

   5

z

0

x

25

20

15

10

5

y

0
   15
10

   10

0

z

5

10

0

   10

   20

x

   5

z

0

5

   5

   10

10

   15

0

x

25

20

15

y

10

5

   15

   10

10

5

0

z

5

10

   5

   10

0

x

   5

z

0

5

   5

5

0

x

30

25

20

y

15

10

5

   5

z

0

5

10

5

0

   5

x

   10

   5

0

5

x

25

20

y

15

10

5

   5

0

5
10

z

30

25

20

y

15

10

5

   10

   5

5

0

x

   10

   5

0

z

   10

   5

0

5

x

   10

   5

z

0

5

z

25

20

15

y

10

5

   10

10

25

20

15

y

10

5

0

   5

10

5

30

25

20

15

10

5

y

6 videos, 56 dims

2000 total time points

30

25

20

y

15

10

5

0

z

30

25

20

15

10

5

y

30

25

20

y

15

10

5

10

5

0

5

10

   15

   10

   5

x

5

   10

10

   15

0

   5

x

0

z

30

25

20

15

10

5

y

10

5

5

10

0

z

   5

   10

   15

0

x

10

5

   5

0

z

5

10

   10

0

x

10

25

20

15

y

10

5

30

25

20

15

10

5

y

   15

   10

   5

0

   5

0

z

5

10

15

   5

0

5

10

15

10

20

15

z

5

x

25

20

15

y

10

5

25

20

15

y

10

5

0
0

z

2

4

6

   10

5

0

   5

x

25

20

15

y

10

5

0
0

z

25

20

15

y

10

5

2

4

6

   5

   10

5

0

x

5

z

10

15

   10

   5

x

5

0

30

25

20

15

y

10

5

30

25

20

15

y

10

5

25

20

y

15

10

5

   5

z

5

0

0

5

   10

   5

x

30

25

20

y

15

10

5

   5

z

0

5

   5

5

0

x

25

20

15

y

10

5

30

25

20

y

15

10

5

   5

z

   10

   5

0

5

5

10

x

   5

0

z

0

5

   15

   10

   5

x

0

5

10

   10

   5

0

5

x

0

5

   5

z

   10

   5

5

0

x

   10

   5

0

5

10

x

0

5

   5

z

0

2

4

6

   4

   2

z

   8    6    4    2 0

2

x

   2

0

2

4

6

8

z

2

4

   6    4    2 0
x

learned mocap interactions

increasing sparsity penalty !

multilayer id88

recurrent network

xt1

gc selection 
on encoding

gc selection 
on decoding

h(t 1)1

x(t 1)1
x(t 1)2

h(t 1)1

x(t 1)1

h(t 1)2

x(t 1)2

 x(t 1)2 . . . x(t k)2 

xt1

 x(t 1)2 . . . x(t k)2 

ht

ht1

ht2

xt1

xt1

linear combination of
nonlinear features

multilayer id88

recurrent network

xt1

gc selection 
on encoding

 x(t 1)2 . . . x(t k)2 

dilated and/or causal 
convolutions,    

xt1

gc selection 
on decoding

 x(t 1)2 . . . x(t k)2 

ht

ht1

ht2

xt1

echo state, gru,     

xt1

h(t 1)1

x(t 1)1
x(t 1)2

h(t 1)1

x(t 1)1

h(t 1)2

x(t 1)2

multilayer id88

recurrent network

xt1

gc selection 
on encoding

gc selection 
on decoding

h(t 1)1

x(t 1)1
x(t 1)2

h(t 1)1

x(t 1)1

h(t 1)2

x(t 1)2

 x(t 1)2 . . . x(t k)2 

xt1

 x(t 1)2 . . . x(t k)2 

ht

ht1

ht2

xt1

xt1

weights 
grouped 
to 0

summary
deep learning offers tremendous opportunities for 
modeling complex dynamics

    traditional approaches often limited to linear, 
gaussian, stationary,    

but, time series problems are much vaster than 
just prediction with large corpora

characterizing 

dynamics

efficiently 
sharing 
information

interpretable 
interactions

non-stationarity 
& measurement 

bias

credit for the hard work   

ian covert 
(cse phd)

nick foti

(research scientist)

chris glynn
(postdoc, 

asst prof at unh)

alec greaves-tunnell

(stat phd)

mike hughes 
(brown cs phd, 
postdoc at harvard)

alex tank
(stat phd)

chris xie
(cse phd)

shirley you ren

(stat phd, 

data scientist at 

microsoft)

