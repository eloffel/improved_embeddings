nyu center 
for data 
science

center for  
cosmology and 
particle physics

d e e p   l e a r n i n g   i n   t h e

p h y s i c a l   s c i e n c e s

@kylecranmer 
new york university  
department of physics 
center for data science 
cilvr lab

reductionist

mechanistic models 
clear causal structure

descriptive models 

unclear causal structure

ecology

health

documents

nuclear & particle  

physics

astrophysics

climate

cosmology

connectome

perception

lattice 

simulations

protein folding

quantum chemistry

systems biology

language

psychology

reductionist

maybe ai should start with 

problems where causal structure is  

clear and mechanistic models are available?

mechanistic models 
clear causal structure

descriptive models 

unclear causal structure

ecology

health

documents

nuclear & particle  

physics

astrophysics

climate

cosmology

connectome

perception

lattice 

simulations

protein folding

quantum chemistry

systems biology

language

psychology

pa n e l   d i s c u s s i o n

   1. there is a lot of low hanging fruit, we can use m.l. to 

    improve what we normally do 

    speed up accelerate what we normally do 

   2. more profound changes to how we approach physics 

    new capabilities to be exploited 

    attack previously intractable problems

4

generative models: simulators

diederik (durk) 

kingma 

max 
welling 

simulators can produce labeled training data for 

supervised learning 

(note: some simulation are very computationally expensive)

p h y s i c s   at   t h e   i n t e r s e c t i o n  

   we can leverage both the power of deep learning and inject 
our expert / domain knowledge

diederik (durk) 

kingma 

discriminative or generative? 

-deep learning

-kernel methods

-id79s

-boosting

r
e
d
o
c
n
e

-
o

t

u
a

 
l

a
n
o

i
t

a
i
r
a
v

-id110s

-probabilistic programs

-simulator models

    advantages discriminative models:

flexible map from input to target (low bias)

   
    efficient training algorithms available 
    solve the problem you are evaluating on.
    very successful and accurate!

    advantages generative models:
inject expert knowledge 

   
    model causal relations
   
interpretable
    data efficient
    more robust to domain shift
   

facilitate un/semi-supervised learning

max 
welling 

7

p h y s i c s   at   t h e   i n t e r s e c t i o n  

   we can leverage both the power of deep learning and inject 
our expert / domain knowledge

diederik (durk) 

kingma 

discriminative or generative? 

-deep learning

-kernel methods

-id79s

-boosting

r
e
d
o
c
n
g
e
-
n
o
t
i
u
t
a
i
 
c
l
a
x
n
o
e
a
i
r
a
v

i
t

 

-id110s

-probabilistic programs

-simulator models

    advantages discriminative models:

flexible map from input to target (low bias)

   
    efficient training algorithms available 
    solve the problem you are evaluating on.
    very successful and accurate!

    advantages generative models:
inject expert knowledge 

   
    model causal relations
   
interpretable
    data efficient
    more robust to domain shift
   

facilitate un/semi-supervised learning

max 
welling 

7

n o tat i o n   /   t e r m i n o l o g y

   

parameters of interest

   

nuisance parameters

forward modeling 

generation 
simulation

prediction
p( x, z |   ,    )

z 

latent variables

id136

inverse problem 
measurement 

parameter estimation

x 

observed data 
simulated data

quiz: 

the standard model has 19 parameters. 

the lhc has collected 1015 collisions. 

is this a parametric or non-parametric problem?

pa r t i c l e   p h y s i c s :   1 9   pa r a m e t e r s

symbol
me
m  
m  
mu
md
ms
mc
mb
mt
  12
  23
  13
  
g1
g2 
g3 
  qcd
v
mh

description
electron mass
muon mass
tau mass
up quark mass
down quark mass
strange quark mass
charm quark mass
bottom quark mass
top quark mass
ckm 12-mixing angle
ckm 23-mixing angle
ckm 13-mixing angle
ckm cp-violating phase
u(1) gauge coupling
su(2) gauge coupling
su(3) gauge coupling
qcd vacuum angle
higgs vacuum expectation value
higgs mass

value
511 kev
105.7 mev
1.78 gev
1.9 mev
4.4 mev
87 mev
1.32 gev
4.24 gev
172.7 gev
13.1  
2.4  
0.2  
0.995
0.357
0.652
1.221
~0
246 gev
125  gev 

10

u
  

c

  

t

d

  

w

h

g

z

e

    

s

    

b
  e

p r e d i c t i o n   i n   pa r t i c l e   p h y s i c s  

we begin with quantum field theory

1)

11

p r e d i c t i o n   i n   pa r t i c l e   p h y s i c s  

we begin with quantum field theory

1)

uses of multivariate methods

t

2)

used 7 variables:
     ll,      ll, mll,      jj,      jj, mjj, mt

complex    nal state of vbf h     w w     llemiss
theory gives detailed 
prediction for high-
energy collisions
hierarchical: 2     o(10)     o(100) particles

compared neural networks, genetic program-
ming, and support vector regression

well-suited for multivariate methods

w

w

q

h

q

w +

w    

  

    

l+

l   

ref. cuts

120 ee
120 e  
120     
combined
130 e  

0.87
2.30
1.16
2.97
4.94

low-mh cuts nn
1.72
3.92
2.28
4.98
7.55

1.25
2.97
1.71
3.91
6.14

gp svr
1.44
1.66
3.33
3.60
2.26
2.08
4.26
4.57
7.22
6.59

table 1: expected signi   cance in sigma after 30 fb   1 for two cut analyses and three multivariate analyses for
di   erent higgs masses and    nal state topologies.

march 14, 2006

university of pennsylvania seminar

higgs searches at the lhc:

challenges, prospects, and developments (page 25)

kyle cranmer

brookhaven national laboratory

11

p r e d i c t i o n   i n   pa r t i c l e   p h y s i c s  

1)

2)

we begin with quantum field theory

theory gives detailed 
prediction for high-
energy collisions
hierarchical: 2     o(10)     o(100) particles

11

p r e d i c t i o n   i n   pa r t i c l e   p h y s i c s  

1)

2)

we begin with quantum field theory

theory gives detailed 
prediction for high-
energy collisions
hierarchical: 2     o(10)     o(100) particles

3)

the interaction of outgoing particles 
with the detector is simulated.  

>100 million sensors

11

d e t e c t o r   s i m u l at i o n
   conceptually: prob(detector response | particles ) 
   implementation: monte carlo integration over micro-physics 
   consequence: evaluation of the likelihood is intractable 

12

d e t e c t o r   s i m u l at i o n
   conceptually: prob(detector response | particles ) 
   implementation: monte carlo integration over micro-physics 
   consequence: evaluation of the likelihood is intractable 

   the crux: 

observed

monte carlo  

sampling

what happened  

in simulation 

p(x|   ) =z dz p(x, z|   )

13

pa r a m e t r i c   v s .   n o n - pa r a m e t r i c

14

pa r a m e t r i c   v s .   n o n - pa r a m e t r i c

   parametric: 

    num parameters < num data points 

    model is highly constrained & tractable

14

pa r a m e t r i c   v s .   n o n - pa r a m e t r i c

   parametric: 

    num parameters < num data points 

    model is highly constrained & tractable

   non-parametric 

    num parameters > num data points 

    model is very flexible, but tractable

14

pa r a m e t r i c   v s .   n o n - pa r a m e t r i c

   parametric: 

    num parameters < num data points 

    model is highly constrained & tractable

   non-parametric 

    num parameters > num data points 

    model is very flexible, but tractable

   implicit models / simulation-based id136 / likelihood-free id136 

    num parameters of simulator < num data points 

    but data distribution is very complicated and density is intractable 

    hard to identify the relevant    degrees of freedom    in the data (sufficient statistics) 

    model is highly constrained, but hard to leverage that structure 

    deep learning can help! learn a surrogate that captures relevant aspects of p(x|  )

14

the traditional approach

1 0      s e n s o r s          1   r e a l - va l u e d   q u a n t i t y

   most measurements and searches for new particles at the lhc are based on the 
distribution of a single variable / feature / summary statistic 

    choosing a good variable (feature engineering) is a task for a skilled physicist 

and tailored to the goal of measurement or new particle search 

    likelihood p(x|  ) approximated using histograms (univariate density estimation)

v
e
g
3

 

 
/
 
s
t

n
e
v
e

35
30
25
20
15
10
5
0

cms

s

 = 7 tev, l = 5.1 fb
 ; 
-1

s

 = 8 tev, l = 19.7 fb
-1

data
hm
  z
z+x

 = 126 gev
*, zz

80 100

200

300 400

600
l4m

800
 (gev)

figure 9: distribution of the four-lepton reconstructed mass in the full mass range for the sum
of the 4e, 2e2   and 4   channels. points with error bars represent the data, shaded histograms
represent the backgrounds, and the unshaded histogram the signal expectation for a mass hy-
pothesis of mh = 126 gev. signal and zz background are normalized to the sm expectation,
z + x background to the estimation from data. the expected distributions are presented as
stacked histograms. no events are observed with m4` > 800 gev.
table 3: the number of observed candidate events compared to the mean expected background

16

h i g h   f i d e l i t y   s i m u l at i o n

 

v
e
g

 
/
 
s
t
n
e
v
e

]

v
e
g
/
b
  

[
 

t

/

p
d
  
d

1
0
.
0
 
/
 
s
n
o
t
p
e
l

610

510

410

310

210

10

1

810

710

610

510

410

310

210

1
-110
-210
-310
-410
-510
-610
-710

tp

atlas
| < 2.4
 > 20 gev, |
  

data
/ee              
multi-jet
         z 

=8 tev, 20.3 fb
s

-1

ee-channel

 ee   z 
ww, wz, zz
   l    w 
tt 

 + single top

60

80

100

120

140
 [gev]

llm

atlas

       - fonll

data 2010 (
 = 7 tev)
s
c+cbb
         w 
   - mc@nlo
         z 
    - mc@nlo
drell yan  - pythia
         w 
   - pythia
         z 
     - pythia 
               - pythia
tt
-1
   
 l dt = 1.4 pb

10

20 30 40 50

60 70 80 90 100
 [gev]

tp

-1

=8 tev, 20.3 fb
s
atlas
ee-channel

data
 ee   z 
multi-jet
/ee               
ww, wz, zz
   l    w 
        z 
tt 

 + single top

0.4   

0.2   

0

0.2

0.4

0.6

0.8
min
ei

17

h i g h   f i d e l i t y   s i m u l at i o n

 

v
e
g

 
/
 
s
t
n
e
v
e

]

v
e
g
/
b
  

[
 

t

/

p
d
  
d

1
0
.
0
 
/
 
s
n
o
t
p
e
l

610

510

410

310

210

10

1

810

710

610

510

410

310

210

1
-110
-210
-310
-410
-510
-610
-710

tp

atlas
| < 2.4
 > 20 gev, |
  

data
/ee              
multi-jet
         z 

=8 tev, 20.3 fb
s

-1

ee-channel

 ee   z 
ww, wz, zz
   l    w 
tt 

 + single top

60

80

100

120

140
 [gev]

llm

atlas

       - fonll

data 2010 (
 = 7 tev)
s
c+cbb
         w 
   - mc@nlo
         z 
    - mc@nlo
drell yan  - pythia
         w 
   - pythia
         z 
     - pythia 
               - pythia
tt
-1
   
 l dt = 1.4 pb

10

20 30 40 50

60 70 80 90 100
 [gev]

tp

=8 tev, 20.3 fb
s
atlas
ee-channel

-1

2.4 tev!

data
 ee   z 
multi-jet
/ee               
ww, wz, zz
   l    w 
        z 
tt 

 + single top

0.4   

0.2   

0

0.2

0.4

0.6

0.8
min
ei

17

h i g h   f i d e l i t y   s i m u l at i o n

   detector is 44m long 

    detector resolves details at <mm scale; simulation accurate!

18

track reconstruction at lhc

h i g h   f i d e l i t y   s i m u l at i o n

   detector is 44m long 

    detector resolves details at <mm scale; simulation accurate!

figure:   
atlas pixel model as described in simulation (left), tomography from vertices built from tracks for hadronic interactions (right)

slide credit: a. salzburger (cern) 

18

t h e   d i s c o v e r y   o f   t h e   h i g g s   b o s o n

ftot(dsim,g|   ) = yc2channels"pois(nc|   c(   ))

fc(xce|   )#   yp2s

ncye=1

fp(ap|   p)

19

s i m u l at i o n        t e m p l at e

layne c. price   
shirley ho   
jeff schneider?
barnab  as p  oczos?
? school of computer science, carnegie mellon university, 5000 forbes ave., pittsburgh, pa 15213, usa
    mcwilliams center for cosmology, department of physics, carnegie mellon university, carnegie 5000 forbes ave.,
pittsburgh, pa 15213, usa

laynep@andrew.cmu.edu
shirleyh@andrew.cmu.edu
jeff.schneider@cs.cmu.edu
bapoczos@cs.cmu.edu

abstract

a grand challenge of the 21st century cosmol-
ogy is to accurately estimate the cosmological
parameters of our universe. a major approach
in estimating the cosmological parameters is to
use the large scale matter distribution of the uni-
verse. galaxy surveys provide the means to map
out cosmic large-scale structure in three dimen-
sions. information about galaxy locations is typ-
ically summarized in a    single    function of scale,
such as the galaxy correlation function or power-
spectrum. we show that it is possible to estimate
these cosmological parameters directly from the
distribution of matter. this paper presents the
application of deep 3d convolutional networks
to volumetric representation of dark-matter sim-
ulations as well as the results obtained using a
recently proposed distribution regression frame-
work, showing that machine learning techniques
 = 8 tev, l = 19.7 fb
s
-1
are comparable to, and can sometimes outper-
form, maximum-likelihood point estimates using
   cosmological models   . this opens the way to
estimating the parameters of our universe with
higher accuracy.

 = 126 gev
*, zz

data
hm
  z
z+x

 = 7 tev, l = 5.1 fb
 ; 
-1

s

cms

1. introduction
the 21st century has brought us tools and methods to ob-
serve and analyze the universe in far greater detail than
before, allowing us to deeply probe the fundamental prop-
erties of cosmology. we have a suite of cosmological ob-
80 100
proceedings of the 33 rd international conference on machine
learning, new york, ny, usa, 2016. jmlr: w&cp volume
48. copyright 2016 by the author(s).

800
 (gev)

600
l4m

300 400

200

v
e
g
 
3
 
/
 
s
t
n
e
v
e

35
30
25
20
15
10
5
0

25

figure 1. dark matter distribution in three cubes produced using
different sets of parameters. each cube is divided into small sub-
cubes for training and prediction. note that although cubes in
this    gure are produced using very different cosmological param-
eters in our constrained sampled set, the effect is not visually dis-
cernible.
servations that allow us to make serious inroads to the un-
derstanding of our own universe, including the cosmic mi-
crowave background (cmb) (planck collaboration et al.,
2015; hinshaw et al., 2013), supernovae (perlmutter et al.,
1999; riess et al., 1998) and the large scale structure of
galaxies and galaxy clusters (cole et al., 2005; anderson
et al., 2014; parkinson et al., 2012).
in particular, large
scale structure involves measuring the positions and other
properties of bright sources in great volumes of the sky.
the amount of information is overwhelming, and modern
methods in machine learning and statistics can play an in-
creasingly important role in modern cosmology. for ex-
ample, the common method to compare large scale struc-
ture observation and theory is to compare the compressed

figure 9: distribution of the four-lepton reconstructed mass in the full mass range for the sum
of the 4e, 2e2   and 4   channels. points with error bars represent the data, shaded histograms
represent the backgrounds, and the unshaded histogram the signal expectation for a mass hy-
pothesis of mh = 126 gev. signal and zz background are normalized to the sm expectation,
z + x background to the estimation from data. the expected distributions are presented as
stacked histograms. no events are observed with m4` > 800 gev.

20

the problem: 

this doesn   t scale if x is high dimensional! 

how much are we loosing in feature engineering? 

what if we don   t know how to design a good feature?

generative models: simulators

diederik (durk) 

kingma 

max 
welling 

c o m p u tat i o n a l   t o p o g r a p h y

23

e p i d e m i o l o g y   &   p o p u l at i o n   g e n e t i c s

24

l at t i c e   f i e l d   t h e o r y

phases, phase transitions, and the order parameter

ising ferromagnet in two dimensions

e =  jxhi,ji

 i j

temperature

ferromagnet

paramagnet

25

l at t i c e   f i e l d   t h e o r y

phases, phase transitions, and the order parameter

ising ferromagnet in two dimensions

e =  jxhi,ji

 i j

temperature

ferromagnet

paramagnet

25

l at t i c e   q c d
   very expensive simulations, ~1000 examples with x       10   

26

27

w h y   s c i e n t i s t s   s h o u l d   c a r e
   many areas of science have simulations based on some well-
motivated  mechanistic model. 

   however, the aggregate effect of many interactions between these 
low-level components leads to an intractable inverse problem. 

   the developments in machine learning and ai have the potential to 
effectively bridge the microscopic - macroscopic divide & aid in the 
inverse problem. 

    they can provide effective statistical models that describe 

macroscopic phenomena that are tied back to the low-level 
microscopic (reductionist) model 

    generative models and likelihood-free id136 are two 

particularly exciting areas 

28

g o a l s   &   s t r at e g i e s
   goals:  

    use machine learning to do better science 

   strategies: 

    import domain knowledge into models (inductive bias) 

    export knowledge from learned models 

    leverage machine learning for intractable inverse problems 

    incorporate traditional scientific concerns into the learning paradigm 

    include impact of domain shift / systematics uncertainties into objective 

    maintain an actionable, scientifically-useful notion of    interpretability    

    use real-world data for training when possible 

    be data efficient 

    modify codebase to facilitate use of these techniques

30

s t r at e g i e s   &   ta c t i c s
   strategy: import domain knowledge into models 

    tactic: exploit symmetries in the data  

    tactic: exploit geometric structure of the data 

    tactic: exploit causal structure of the generative process 

    tactic: exploit hierarchical / compositional structure 

    tactic: exploit markov property of the generative process 

    tactic: exploit tangent space of statistical manifold 

   strategy: export knowledge from learned models 

    tactic: learnable components that can be interpreted 

   strategy: maintain an actionable, scientifically-useful notion of    interpretability    

    tactic: compose model from reusable components that perform a specific task and can 

be individually characterized & validated

31

s t r at e g i e s   &   ta c t i c s
   strategy: leverage machine learning for intractable inverse problems 

    tactic: use the likelihood ratio trick to convert a discriminative classifier into  a density ratio 

    tactic: use autoregressive models & normalizing flows for conditional density estimation 

    tactic:  use universal probabilistic programming  

    tactic: approximate gradients of non-differentiable, black-box models (avo, relax,    ) 

   strategy: include impact of systematics uncertainties into objective 

    tactic: design id168s more relevant to scientific goals 

    tactic: adversarial training for continuous id20 & fairness (   learning to pivot   ) 

   strategy: use real-world data for training when possible 

    tactic: weakly supervised learning  

   strategy: be data efficient 

    tactic: exploiting domain knowledge can dramatically reduce number of parameters

32

ta c t i c s   f o r   i n t r a c ta b l e   i n v e r s e   p r o b l e m s

use simulator  

(much more efficiently)

learn simulator  
(with deep learning)

    approximate bayesian 

computation (abc) 

    id3 (gans), 

variational auto-encoders (vae) 

    probabilistic programming 

    adversarial variational 

optimization (avo)

    likelihood ratio from classifiers (carl) 
    autoregressive models,    

normalizing flows

[image credit: a.p. goucher]

33

example:  

more powerful higgs measurements

h i g h   d i m e n s i o n a l   e x a m p l e

   when looking for deviations from the standard model higgs, 
we would like to look at all sorts of kinematic correlations 
    thus each observation x is high-dimensional

16

2
2

2
2

6000
6000

6000
6000

8000
8000

6000
6000

)
)
 
 
8
8
0
0
0
0

.
.

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

4000
4000

2000
2000

)
)
 
 
1
1
2
2
0
0

.
.

4000
4000

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

2000
2000

)
)
 
 
8
8
0
0
0
0

.
.

6000
6000

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

4000
4000

2000
2000

)
)
 
 
1
1
2
2
0
0

.
.

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

4000
4000

2000
2000

0
0
-1
-1

3000
3000

-0.5
-0.5

0
0
cos 
cos 

*  
*  

0.5
0.5

1
1

0
0

-2
-2

0
0
1  
1  

2
2

0
0
-1
-1

-0.5
-0.5

2000
2000

1  cos
1  cos

0
0
 or cos
 or cos

2  
2  

0.5
0.5

1
1

0
0

-2
-2

0
0
  
  

2000
2000

)
)
 
 
8
8
0
0
0
0

.
.

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

2000
2000

1000
1000

0
0
-1
-1

-0.5
-0.5

0
0
cos 
cos 

*  
*  

10000
10000

)
)
 
 
8
8
0
0

.
.

0
0

 
 
(
(
 
 
/
/
 
 
s
s
t
t

n
n
e
e
v
v
e
e

5000
5000

0
0
-1
-1

-0.5
-0.5

0
0
cos 
cos 

*  
*  

10000
10000

)
)
 
 
8
8
0
0

.
.

0
0

 
 
(
(
 
 
/
/
 
 
s
s
t
t

n
n
e
e
v
v
e
e

5000
5000

0
0
-1
-1

-0.5
-0.5

0
0
cos 
cos 

*  
*  

1500
1500

)
)
 
 
1
1
2
2
0
0

.
.

2000
2000

)
)
 
 
8
8
0
0
0
0

.
.

1500
1500

)
)
 
 
1
1
2
2
0
0

.
.

1000
1000

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

1000
1000

1000
1000

 
 
(
(
 
 
/
/
 
 
s
s
t
t
n
n
e
e
v
v
e
e

.
.

.
.

.
.

0
0

1
1

0
0

2
2

1
1

0
0

2
2

1
1

0
0

0
0

0
0

-2
-2

-2
-2

-2
-2

2  
2  

0.5
0.5

0.5
0.5

0.5
0.5

0
0
  
  

500
500

500
500

0
0
-1
-1

0
0
-1
-1

-0.5
-0.5

-0.5
-0.5

3000
3000

2000
2000

1000
1000

4000
4000

2000
2000

3000
3000

2000
2000

1000
1000

)
)
 
 
1
1
2
2

)
)
 
 
8
8
0
0

)
)
 
 
1
1
2
2

0
0
1  
1  

0
0
1  
1  

 
 
(
(
 
 
/
/
 
 
s
s
t
t

 
 
(
(
 
 
/
/
 
 
s
s
t
t

 
 
(
(
 
 
/
/
 
 
s
s
t
t

n
n
e
e
v
v
e
e

n
n
e
e
v
v
e
e

n
n
e
e
v
v
e
e

1  cos
1  cos

0
0
 or cos
 or cos

weak boson fusion, h    4   
    production vs decay
    hzz decay vertex:
many angular structures
    very clean
ob= i g

    same operators as before:
2(d        )(d     ) b    

0
0
 or cos
 or cos

0
0
 or cos
 or cos

1  cos
1  cos

1  cos
1  cos

n
n
e
e
v
v
e
e

n
n
e
e
v
v
e
e

n
n
e
e
v
v
e
e

 
 
(
(
 
 
/
/
 
 
s
s
t
t

 
 
(
(
 
 
/
/
 
 
s
s
t
t

 
 
(
(
 
 
/
/
 
 
s
s
t
t

0
0
1  
1  

)
)
 
 
1
1
2
2

)
)
 
 
8
8
0
0

)
)
 
 
1
1
2
2

1000
1000

2000
2000

3000
3000

4000
4000

4000
4000

2000
2000

4000
4000

1000
1000

2000
2000

3000
3000

-0.5
-0.5

0
0
-1
-1

0
0
  
  

0.5
0.5

0.5
0.5

0.5
0.5

2  
2  

2  
2  

-2
-2

0
0

0
0

0
0

1
1

2
2

0
0

1
1

0
0

1
1

0
0

.
.

.
.

.
.

fig. 2: distribution of the cos       (left),   1 (second from the left), cos   1 and cos   2 (second from the right), and    (right)

q

illustration of an exotic x particle production and decay in pp collision gg or q  q     x     zz     4l  . six angles fully
fig. 1:
characterize orientation of the decay chain:       and       of the    rst z boson in the x rest frame, two azimuthal angles    and   1
between the three planes de   ned in the x rest frame, and two z-boson helicity angles   1 and   2 de   ned in the corresponding
z rest frames. the o   set of angle       is arbitrarily de   ned and therefore this angle is not shown.
-2
-2

2
2

w, z

h

z

discussed in refs. [21   23] kk graviton decays into pairs of gauge bosons are enhanced relative to direct decays into
leptons. similar situations may occur in    hidden-valley   -type models [24]. an example of a    heavy photon    is given
in ref. [25].

w, z

z

motivated by this, we consider the production of a resonance x at the lhc in gluon-gluon and quark-antiquark
partonic collisions, with the subsequent decay of x into two z bosons which, in turn, decay leptonically. in fig. 1,
we show the decay chain x     zz     e+e     +     . however, our analysis is equally applicable to any combination of
-2
-2
decays z     e+e    or   +     . it may also be applicable to z decays into    leptons since      s from z decays will often be
highly boosted and their decay products collimated. we study how the spin and parity of x, as well as information

q   

35

q

0
0
  
  

2
2

q   

   +
      
   +
      

ow= i g

2(d     )      k(d     ) w k

    

e x t e n d i n g   t h e   l i k e l i h o o d   r at i o   t r i c k

   a binary classifier approximates 

s(x) =

p(x|h1)

p(x|h0) + p(x|h1)

   which is one-to-one with the likelihood ratio  

p(x|y = 0)
p(x|h1)
p(x|y = 1)
p(x|h0)

= 1  

1

s(x)

   can do the same thing for any two points       &       in 
parameter space   . i call this a parametrized classi   er 

s(x;    0,    1) =

p(x|   1)

p(x|   0) + p(x|   1)

k.c., g. louppe, j. pavez: approximating likelihood ratios with calibrated discriminative classifiers [arxiv:1506.02169]

36

c a l i b r at i n g   t h e   l i k e l i h o o d - r at i o   t r i c k

   the intractable likelihood ratio based on high-dimensional features x is: 

2.8 getting help

tmva output for classifier: likelihood

tmva output for classifier: pders

p(x|   0)
p(x|   1)

signal
background

d
d
e
e
z
z
i
i
l
l

a
a
m
m
r
r
o
o
n
n

10
10

8
8

6
6

4
4

signal
background

2.5
2.5

2
2

1.5
1.5

1
1

d
d
e
e
z
z
i
i
l
l

a
a
m
m
r
r
o
o
n
n

%
)
0
.
0
 
,
0
.
0
(
 
/
 

%
)
0
.
0
 
,
0
.
0
(
 
:
)

11

%
)
0
.
0
 
,
0
.
0
(
 
/
 

%
)
0
.
0
 
,
0
.
0
(
 
:
)

b
s

,

(
 

w
o
l
f
-

0.6
0.6

12

0.8
0.8

o
u

/

1
1
pders
pders

background rejection versus signal efficiency

   we can show that an equivalent test can be made from 1-d projection 

0.5
0.5

0.2
0.2

0.4
0.4

0.6
0.6

0.8
0.8

0.2
0.2

0.4
0.4

w
o
l
f
-

o
u

b
s

2
2

0
0

0
0

0
0

0
0

(
 

/

,

tmva output for classifier: mlp

tmva output for classifier: bdt

p(x|   0)
p(x|   1)

=

7
7

signal
background

d
d
e
e
z
z
i
i
l
l

a
a
m
m
r
r
o
o
n
n

6
6

5
5

p(s(x;    0,    1)|   0)
p(s(x;    0,    1)|   1)

0.2
0.2

0.4
0.4

0
0

1
1

2
2

3
3

4
4

1
1
likelihood
likelihood

d
d
e
e
z
z
i
i
l
l

 
 
 
)
s
a
a
m
m
(
r
r
o
o
n
n
p

%
)
0
0

.

 
,

.

0
0
(
 
/
 

%
)
0
0

.

 
,

.

0
0
(
 
:
)

0.6
0.6

0.8
0.8

b
s

,

(
 

w
o
l
f
-

o
u

/

1
1
mlp
mlp

1.8
1.8
1.6
1.6
1.4
1.4
1.2
1.2
1
1
0.8
0.8
0.6
0.6
0.4
0.4
0.2
0.2
0
0

signal
background

   1

   0

-0.8
-0.8

-0.6
-0.6

-0.4
-0.4

-0.2
-0.2

-0
-0

0.2
0.2

0.4
0.4

0.6
0.6

j
j

n
n
o
o
i
i
t
t
c
c
e
e
e
e
r
r
 
 
d
d
n
n
u
u
o
o
r
r
g
g
k
k
c
c
a
a
b
b

%
)
0
0

.

 
,

.

0
0
(
 
/
 

%
)
0
0

.

 
,

.

0
0
(
 
:
)

b
s

,

(
 

w
o
l
f
-

o
u

/

1
1

0.9
0.9

0.8
0.8

0.7
0.7

0.6
0.6

0.5
0.5

0.4
0.4

0.3
0.3

0.2
0.2

0.8
0.8
bdt
bdt

s     

   if the scalar map s: x         has the same level sets as the likelihood ratio 

s(x;    0;    1) = monotonic[ p(x|   0)/p(x|   1) ]

nye=1
   estimating the density of                       via the simulator calibrates the ratio. 

    the web address of this users guide: http://tmva.sourceforge.net/docu/tmvausersguide.pdf.
    the tmva talk collection: http://tmva.sourceforge.net/talks.shtml.

s(x;    0,    1)

p({xe}|   ,    ) =

figure 5: example for the background rejection versus signal e ciency (   roc curve   ) obtained by cutting
on the classi   er outputs for the events of the test sample.

figure 4: example plots for classi   er output distributions for signal and background events from the academic
test sample. shown are likelihood (upper left), pde range search (upper right), multilayer id88 (mlp
    lower left) and boosted id90.

    tmva tutorial: https://twiki.cern.ch/twiki/bin/view/tmva.
    an up-to-date reference of all con   guration options for the tmva factory, the    tters, and all

figure 1: left: an example of the distributions f0(s|   ) and f1(s|   ) when the classi   er s is
a boosted-decision tree (bdt). right: the corresponding roc curve (right) for this and
    tmva versions in root releases: http://tmva.sourceforge.net/versionref.html.
other classi   ers. figures taken from tmva manual.
    direct code views via viewvc: http://tmva.svn.sourceforge.net/viewvc/tmva/trunk/tmva.
    class index of tmva in root: http://root.cern.ch/root/htmldoc/tmva index.html.
these steps lead to a subsequent statistical analysis where one observes in data {xe},
    please send questions and/or report problems to the tmva-users mailing list:
where e is an event index running from 1 to n. for each event, the classi   er is evaluated and
    on request, the tmva methods provide a help message with a brief description of the method,
one performs id136 on a parameter    related to the presence of the signal contribution.
and hints for improving the performance by tuning the available con   guration options. the
in particular, one forms the statistical model
message is printed when the option    h    is added to the con   guration string while booking
the method (switch o    by setting    !h   ). the very same help messages are also obtained by
clicking the    info    button on the top of the reference tables on the options reference web page:
http://tmva.sourceforge.net/optionref.html.

[   f1(s(xe)|    ) + (1     ) f0(s(xe)|    ) ] ,

the mva methods: http://tmva.sourceforge.net/optionref.html.

3 using tmva

http://sourceforge.net/mailarchive/forum.php?forum name=tmva-users (posting messages requires
prior subscription: https://lists.sourceforge.net/lists/listinfo/tmva-users).

where    = 0 is the null (background-only) hypothesis and    > 0 is the alternate (signal-
plus-background) hypothesis.1 typically, we are interested in id136 on    and     are
nuisance parameters; though, sometimes     may include some components that we are also
wish to infer (like the mass of a new particle that a   ects the distribution x for the signal
events).

a typical tmva classi   cation or regression analysis consists of two independent phases: the training
phase, where the multivariate methods are trained, tested and evaluated, and an application phase,
where the chosen methods are applied to the concrete classi   cation or regression problem they have
been trained for. an overview of the code    ow for these two phases as implemented in the examples
tmvaclassification.c and tmvaclassificationapplication.c (for classi   cation     see sec. 2.5),
and tmvaregression.c and tmvaregressionapplication.c (for regression) are sketched in fig. 7.
multiclass classi   cation does not di   er much from two class classi   cation from a technical point of
view and di   erences will only be highlighted where neccessary.

in the training phase, the communication of the user with the data sets and the mva methods
is performed via a factory object, created at the beginning of the program. the tmva factory
provides member functions to specify the training and test data sets, to register the discriminating

k.c., g. louppe, j. pavez: approximating likelihood ratios with calibrated discriminative classifiers [arxiv:1506.02169]

nuisance parameters are an after thought in the typical usage of machine learning in hep.
in fact, most machine learning discussions would only consider f0(x) and f1(x). however,
as experimentalists we know that we must account for various forms of systematic uncer-
tainty, parametrized by    . in practice, we take the classi   er as    xed and then propagate
uncertainty through the classi   er as in eq. 1. building the distribution f (s(x)|   ) for values
of     other than the nominal    0 used to train the classi   er can be thought of as a calibration

37

1.2 comments on typical usage of machine learning in hep

m a c h i n e   l e a r n i n g   t h e   h i g g s   e f f e c t i v e   f i e l d   t h e o r y

(based on a 16-d observation x)

d
o
o
h

i
l

e
k

i
l
 

d
e
t
a
m

i
t
s
e

weak boson fusion, h    4   
    production vs decay
    hzz decay vertex:
many angular structures
    very clean
ob= i g
obb=    g   2

    same operators as before:
2(d        )(d     ) b    

true likelihood

q

equivalent to 3x more data!

figure       : id136 from truth likelihood ratio and carl   s estimate for the fully di   erential case
with regression. le   : scatter plot showing the di   erence between the exact expected
likelihood ratio for           randomly sampled points and   1 and carl   s estimate. right:
true (white) and approximate (cyan) likelihood contours, using a gaussian process for
interpolation.    e white and cyan dots show the exact and approximate maximum-
likelihood estimators.    e green and red dots show   observed and   1, respectively. finally,
the small grey dots show the sampled parameter points at which the likelihood ratio
was evaluated.

   +
      
   +
      

w, z

w, z

z

z

h

q

q   

q   

ow= i g

2(d     )      k(d     ) w k

work with juan pavez, gilles louppe, cyril becot, and lukas heinrich; johann brehmer, felix kling, and tilman plehn 
    
   better higgs measurements through information geometry    [arxiv:1612.05261] & carl  [arxiv:1506.02169] 

38

example:  

jet classification

jet physics

previous work

proposed model

experiments

conclusions

introduction
j e t s
atlas

40

is more di   use for the qcd background which consists largely of gluon jets, which have an octet
-1
radiation pattern, compared to the singlet radiation pattern of the w jets, where the radiation is
mostly restricted to the region between the two hard cores.

c l a s s i f i c at i o n

[translated] pseudorapidity (

-0.5

0.5

)  

0

1

-710
-810
-910

-1

-0.5

0

0.5

1

[translated] pseudorapidity (

)  

-710
-810
-910

/gev < 260 gev, 65 < mass/gev < 95
240 < p
t

pythia 8, w'

 wz, 

   

 = 13 tev
s

average boosted w jet

average qcd jet
/gev < 260 gev, 65 < mass/gev < 95
240 < p
t

pythia 8, qcd dijets, 

 = 13 tev
s

-1

-1

]

v
e
g

[
 

p

 
l

e
x
p

i

t

0.5

)
  
(
 

l

e
g
n
a

)
  
(
 
e
g
n
a

l

 
l

 
l

1

t

a
h
u
m
z
a

i

a
h

t

0.5
u
m
z
a

i

0

 
]

 
]

l

l

t

t

d
e
a
s
n
a
r
t

d
e
a
s
n
a
r
t

[

[

-0.5
experiments

-0.5

t

t

]

v
e
g

[
 

p

 
l

e
x
p

i

i

]

 
l

   

1

[
 

p

e
x
p

v
e
g

pythia 8, w'

 wz, 
s
 = 13 tev
 = 13 tev
s

240 < p
/gev < 260 gev, 65 < mass/gev < 95
/gev < 260 gev, 65 < mass/gev < 95
240 < p
t
t
pythia 8, qcd dijets, 
310
310
210
210
10
10
1
1
-110
-110
-210
-210
-310
-310
-410
-410
-510
-510
-610
-610
-710
-710
-810
-810
-910
-910

conclusions

0

-1

-1

-1

-0.5

0

0

-0.5
[translated] pseudorapidity (

1
[translated] pseudorapidity (
)  

0.5

0.5

1

-1

1

0.5

0

)
  
(
 

l

e
g
n
a

 
l

t

a
h
u
m
z
a

i

 
]

l

t

d
e
a
s
n
a
r
t

310
210
10
1
-110
-210
-310
-410
-510
-610
-710
-810
-910

310
210
10
1
-110
-210
-310
-410
-510
-610
-710
-810
-910

t

p
 
l
e
x
p

i

)
  
(
 
e
g
n
a

l

 
l
a
h
t
u
m
z
a

i

0.5

-0.5

t

v
e
g

[
 

p
 
l
e
x
p

i

310
210
10
1
-110
-210
-310
-410
-510
-610
-710
-810
-910

    4    

proposed model

-1

-0.5

0

0.5

1

[translated] pseudorapidity (

)  

[

-0.5

-1

-1

-0.5

0

introduction
jet graphs

0.5

1

)  

[translated] pseudorapidity (

)  

jet physics

previous work

/gev < 260 gev, 65 < mass/gev < 95
240 < p
t

pythia 8, qcd dijets, 

 = 13 tev
s

240 < p
/gev < 260 gev, 65 < mass/gev < 95
t

figure 2: the average jet image for signal w jets (top) and background qcd jets (bottom) before
(left) and after (right) applying the rotation, re-pixelation, and inversion steps of the pre-processing.
the average is taken over images of jets with 240 gev < pt < 260 gev and 65 gev < mass < 95 gev.

pythia 8, qcd dijets, 

 = 13 tev
s

]

typical boosted w jet

v
e
g

[
 

1

]

typical qcd w jet

one standard pre-processing step that is often additionally applied in id161 tasks is
i = 1 where ii is the
intensity of pixel i. this is particularly useful for the jet images where pixel intensities can span many

id172. a common id172 scheme is the l2 norm such thatp i 2

 
]
d
e
t
a
s
n
a
r
t

0

[

l

-1

-0.5

0

0.5

1

[translated] pseudorapidity (

)  

-1

-1

-0.5

0

0.5

1

[translated] pseudorapidity (

)  

]

v
e
g

[
 

p

 
l

e
x
p

i

t

310
210
10
1
-110
-210
-310
-410
-510
-610
-710
-810
-910

figure 2: the average jet image for signal w jets (top) and background qcd jets (bottom) before
(left) and after (right) applying the rotation, re-pixelation, and inversion steps of the pre-processing.
the average is taken over images of jets with 240 gev < pt < 260 gev and 65 gev < mass < 95 gev.

w jet

qcd jet

41

j e t   i m a g e s

introduction
jet images

jet physics

previous work

proposed model

experiments

conclusions

[image: komiske, metodiev, schwartz arxiv:1612.01551]

 

[oliveira et al arxiv:1511.05190]
[baldi et al arxiv:1603.09349]
[barnard et al arxiv:1609.00607]

 

m
a
e
b

pre-process

convolutional layer

dense layer

quark jet

gluon jet

42

 

max-pooling

 

 3

  

n o n - u n i f o r m   g e o m e t r y

43

n o n - u n i f o r m   g e o m e t r y

44

f r o m   i m a g e s   t o   s e n t e n c e s

   id56s showing great performance for 
natural language processing tasks 

    neural network   s topology given by parsing of sentence!

45

f r o m   i m a g e s   t o   s e n t e n c e s

   id56s showing great performance for 
natural language processing tasks 

    neural network   s topology given by parsing of sentence!

analogy: 
word     particle 
parsing     jet algorithm

45

q c d - i n s p i r e d   r e c u r s i v e   n e u r a l   n e t w o r k s

kt

particles

towers 

images

anti-kt

6

    generative process is a 

    physics algorithms exist 

tree-like, ~stationary 
markov process 

further supported by the poor performance of the random
binary tree topology. we expected however that a simple
sequence (represented as a degenerate binary tree) based
on ascending and descending pt ordering would not per-
form particularly well, particularly since the topology
does not use any angular information. surprisingly, the
simple descending pt ordering slightly outperforms the
id56s based on kt and c/a topologies. the descending
pt network has the highest pt 4-momenta near the root
of the tree, which we expect to be the most important.
we suspect this is the reason that the descending pt out-
performs the ascending pt ordering on particles, but this
is not supported by the performance on towers. a similar
observation was already made in the context of natural
languages [24   26], where tree-based models have at best
only slightly outperformed simpler sequence-based net-
works. while recursive networks appear as a principled
choice, it is conjectured that recurrent networks may in
fact be able to discover and implicitly use recursive com-
positional structure by themselves, without supervision.
that we varied was

to estimate the tree 

less data to train!

d. gating the last

factor

    tree-id56 needs much 

46

neural message passing for jet

physics

isaac henrion, johann brehmer, joan bruna,
kyunghyun cho, kyle cranmer, gilles louppe,

gaspar rochette

courant institute & center for data science

paper: https://dl4physicalsciences.github.io/files/nips_dlps_2017_29.pdf
talk:    https://dl4physicalsciences.github.io/files/nips_dlps_2017_slides_henrion.pdf

47

isaac henrion

j e t s   a s   a   g r a p h

   using message passing neural networks over a fully connected graph on the particles 

    two approaches for adjacency matrix for edges 

previous work

    import physics knowledge by using metric of jet algorithms 

proposed model

experiments

conclusions

    learn adjacency matrix and export new jet algorithm

c/a algorithm with   =0 

i   

dii   

i

i   

i

48
48

classi   cation results

p r e l i m i n a r y   r e s u lt s

1/fpr @ tpr = 50%

model
rec-nn (no gating)
rec-nn (gating)
mpnn (directed)
mpnn (directed)
mpnn (directed)
mpnn (identity)
relation network

iterations

1
1
1
2
3
3
1

r   =50%
70.4    3.6
83.3    3.1
89.4    3.5
98.3    4.3
85.9    8.5
74.5    5.2
67.7    6.8

signi   cant improvement on w vs. qcd jet classi   cation! 
this is with a learned adjacency matrix 

- what did it learn? is that adjacency matrix useful? 
- we are working mpnn with qcd-motivated adjacency matrix

49

example: 

optimizing non-differentiable simulators

catch me if you can

i.

introduction

n e w !   av o

adversarial variational optimization of non-di   erentiable simulators

gilles louppe1 and kyle cranmer1

1new york university

complex computer simulators are increasingly used across    elds of science as generative models
tying parameters of an underlying theory to experimental observations. id136 in this setup is
often di cult, as simulators rarely admit a tractable density or likelihood function. we introduce
adversarial variational optimization (avo), a likelihood-free id136 algorithm for    tting a non-
di   erentiable generative model incorporating ideas from empirical bayes and variational id136.
we adapt the training procedure of id3 by replacing the di   erentiable
generative network with a domain-speci   c simulator. we solve the resulting non-di   erentiable mini-
max problem by minimizing variational upper bounds of the two adversarial objectives. e   ectively,
the procedure results in learning a proposal distribution over simulator parameters, such that the
corresponding marginal distribution of the generated data matches the observations. we present
results of the method with simulators producing both discrete and continuous data.

in many    elds of science such as particle physics, epi-
demiology, and population genetics, computer simulators
are used to describe complex data generation processes.
these simulators relate observations x to the parame-
ters     of an underlying theory or mechanistic model.
in most cases, these simulators are speci   ed as proce-
dural implementations of forward, stochastic processes
involving latent variables z. rarely do these simulators
admit a tractable density (or likelihood) p(x|   ). the
prevalence and signi   cance of this problem has motivated
an active research e   ort in so-called likelihood-free infer-
ence algorithms such as approximate bayesian compu-
tation (abc) and density estimation-by-comparison al-
gorithms [1   6].

in parallel, with the introduction of variational auto-
encoders [7] and id3 [8],
there has been a vibrant research program around im-
plicit generative models based on neural networks [9].
while some of these models also do not admit a tractable
density, they are all di   erentiable by construction. in ad-
dition, generative models based on neural networks are
highly parametrized and the model parameters have no
obvious interpretation. in contrast, scienti   c simulators
can be thought of as highly regularized generative mod-
els as they typically have relatively few parameters and
they are endowed with some level of interpretation. in
this setting, id136 on the model parameters     is often
of more interest than the latent variables z.

in this note, we develop two likelihood-free id136 al-
gorithms for non-di   erentiable, implicit generative mod-
els. the    rst allows us to perform empirical bayes
through variational id136, and the second provides
a point estimator of the parameters    . we adapt the
adversarial training procedure of generative adversarial
networks [8] by replacing the implicit generative network
with a domain-based scienti   c simulator, and solve the
resulting non-di   erentiable minimax problem by mini-
mizing variational upper bounds [10, 11] of the adver-
sarial objectives. the objective of both algorithms is to

leo is g

match marginal distribution of the generated data to the
empirical distribution of the observations.

ii. problem statement

we consider a family of parametrized densities p(x|   )
de   ned implicitly through the simulation of a stochas-
tic generative process, where x 2 rd is the data and    
are the parameters of interest. the simulation may in-
volve some complicated latent process where z 2 z is a
latent variable providing an external source of random-
ness. unlike implicit generative models de   ned by neural
networks, we do not assume z to be a    xed-size vector
with a simple density. instead, the dimension of z and
the nature of its components (uniform, normal, discrete,
continuous, etc.) are inherited from the control    ow of
the simulation code and may depend on     in some in-
tricate way. moreover, the dimension of z may be much
larger than the dimension of x.

we assume that the stochastic generative process that
de   nes p(x|   ) is speci   ed through a non-di   erentiable
deterministic function g(  ;    ) : z ! rd. operationally,
(1)

x     p(x|   )     z     p(z|   ), x = g(z;    )
such that the density p(x|   ) can be written as

p(x|   ) =z{z:g(z;   )=x}

p(z|   )  (dz),

(2)

where    is a id203 measure.

given some observed data {xi|i = 1, . . . , n} drawn
from the (unknown) true distribution pr(x), our goal is to
estimate the parameters        that minimize the divergence
between pr(x) and the implicit model p(x|   ). that is,
(3)

       = arg min
   

   (pr(x), p(x|   )),

g. louppe & k.c. arxiv:1707.07113

   similar to w-gan setup, but 
instead of using a neural network 
as the generator, use the actual 
simulation (eg. pythia, geant)

   continue to use a neural network 
discriminator / critic. 

   dif   culty: the simulator isn   t 
differentiable, but there   s a trick! 
   allows us to efficiently fit /    
tune simulation with stochastic 
gradient techniques!

where     is some distance or divergence.

tom is d

51

example: 

lattice field theory

l at t i c e   q u a n t u m   c h r o m o   d y n a m i c s
   very expensive simulations, ~1000 examples with x       10   

53

l at t i c e   q u a n t u m   c h r o m o   d y n a m i c s
   very expensive simulations, ~1000 examples with x       10   

53

l at t i c e   q c d
   each of the 10    lattice locations has data x                with non-trivial data with continuous local symmetry.  

(a) sl

    space-time translation invariance     convolutional architecture 
    local gauge symmetry     design group-invariant convolutional filters 
    coarse graining & reid172 group     hierarchical convolutions shared weights 
    few very, large training examples     rethink minibatching & sgd

bonus:  

network discovered 

something unexpected, 
a feature that has a long 
auto-correlation time.

(b) cp

(c) slcp

fig. 13: diagrams of the neural network structure used. in the    rst layer, sl, cp, or slcp structures are
formed, e.g., in the cp case, products of the 18 di   erent types of loops separated by lattice distance r < 13
(averaged in integer space bins of r) are allowed, for a total of 18    18    13 = 4212 loop products. the    rst

[explainer ]

54

example: 

systematics uncertainty 

continuous id20 
fairness on continuous attributes

l e a r n i n g   t o   p i v o t   w i t h   a d v e r s a r i a l   n e t w o r k s
    typically classifier f(x) trained to 
minimize loss lf.  
    want classifier output to be 
insensitive to systematics 
(nuisance parameter   ) 
    introduce an adversary r that 
tries to predict    based on f.  
    setup as a minimax game:

normal training

  =+1

  =-1

  =0

    

    

  =+1

  =0

  =-1

adversarial training

classi   er f

adversary r

z

2

x

...

   f

f (x;    f )

lf (   f )

...

   r

 1(f (x;    f );    r)

 2(f (x;    f );    r)

. . .

p( 1,  2, . . . )

p   r (z|f (x;    f ))

lr(   f ,    r)

  =+1
  =0
  =-1

  =+1
  =0
  =-1

insensitive!

fig. 1. architecture for the adversarial training of a binary classi   er f against a nuisance parameters z. the adversary r
models the distribution p(z|f (x;    f ) = s) of the nuisance parameters as observed only through the output f (x;    f ) of the
classi   er. by maximizing the antagonistic objective lr(   f ,    r) (as part of minimizing lf (   f )    lr(   f ,    r)), the classi   er f
forces p(z|f (x;    f ) = s) towards the prior p(z), which happens when f (x;    f ) is independent of the nuisance parameter z and
therefore pivotal.

p(   |f )

parameters. this implies that f (x;    f ) and z are inde-
pendent random variables.

as stated in eqn. 1, the pivotal quantity criterion is
imposed with respect to p(x|z) where y is marginalized
out. in some situations however (see e.g., sec. v b), class
conditional independence of f (x;    f ) on the nuisance z
is preferred, which can then be stated as requiring
p(f (x;    f ) = s|z, y) = p(f (x;    f ) = s|z0, y)

(2)

for one or several speci   ed values y 2 y.

sity p   r (z|f (x;    f ) = s). intuitively, if p(f (x;    f ) = s|z)
varies with z, then the corresponding correlation can be
captured by r. by contrast, if p(f (x;    f ) = s|z) is invari-
ant with z, as we require, then r should perform poorly
and be close to random guessing. training f such that
it additionally minimizes the performance of r therefore
acts as a id173 towards eqn. 1.

if z takes discrete values, then p   r can be represented
e.g. as a probabilistic classi   er r 7! r|z| whose jth out-
put (for j = 1, . . . ,|z|) is the estimated id203 mass
p   r (zj|f (x;    f ) = s). similarly, if z takes continuous

f (x)

g. louppe, m. kagan, k. cranmer, learning to pivot with adversarial networks [arxiv:1611.01046]

56

l e a r n i n g   t o   p i v o t   w i t h   a d v e r s a r i a l   n e t w o r k s
    typically classifier f(x) trained to 
minimize loss lf.  
    want classifier output to be 
insensitive to systematics 
(nuisance parameter   ) 
    introduce an adversary r that 
tries to predict    based on f.  
    setup as a minimax game:

normal training

  =+1

  =-1

  =0

    

    

  =+1

  =0

  =-1

adversarial training

classi   er f

adversary r

z

2

x

...

   f

f (x;    f )

lf (   f )

...

   r

 1(f (x;    f );    r)

 2(f (x;    f );    r)

. . .

p( 1,  2, . . . )

p   r (z|f (x;    f ))

lr(   f ,    r)

  =+1
  =0
  =-1

  =+1
  =0
  =-1

insensitive!

fig. 1. architecture for the adversarial training of a binary classi   er f against a nuisance parameters z. the adversary r
models the distribution p(z|f (x;    f ) = s) of the nuisance parameters as observed only through the output f (x;    f ) of the
classi   er. by maximizing the antagonistic objective lr(   f ,    r) (as part of minimizing lf (   f )    lr(   f ,    r)), the classi   er f
forces p(z|f (x;    f ) = s) towards the prior p(z), which happens when f (x;    f ) is independent of the nuisance parameter z and
therefore pivotal.

p(   |f )

parameters. this implies that f (x;    f ) and z are inde-
pendent random variables.

as stated in eqn. 1, the pivotal quantity criterion is
imposed with respect to p(x|z) where y is marginalized
out. in some situations however (see e.g., sec. v b), class
conditional independence of f (x;    f ) on the nuisance z
is preferred, which can then be stated as requiring
p(f (x;    f ) = s|z, y) = p(f (x;    f ) = s|z0, y)

(2)

for one or several speci   ed values y 2 y.

sity p   r (z|f (x;    f ) = s). intuitively, if p(f (x;    f ) = s|z)
varies with z, then the corresponding correlation can be
captured by r. by contrast, if p(f (x;    f ) = s|z) is invari-
ant with z, as we require, then r should perform poorly
and be close to random guessing. training f such that
it additionally minimizes the performance of r therefore
acts as a id173 towards eqn. 1.

if z takes discrete values, then p   r can be represented
e.g. as a probabilistic classi   er r 7! r|z| whose jth out-
put (for j = 1, . . . ,|z|) is the estimated id203 mass
p   r (zj|f (x;    f ) = s). similarly, if z takes continuous

f (x)

g. louppe, m. kagan, k. cranmer, learning to pivot with adversarial networks [arxiv:1611.01046]

56

fa i r   c l a s s i f i e r s

k.c, j. pavez, and g. louppe, arxiv:1506.02169 
p. baldi, k.c, t. faucett, p. sadowski, d. whiteson  arxiv:1601.07913   
g. louppe, m. kagan, k.c, arxiv:1611.01046   
shimmin, et. al. arxiv:1703.03507

   adversarial approach of 
   learning to pivot    can also be 
used to train a classifier that is 
independent from some other 
continuous variable.  

t

u
p

t

 

u
o
n
n

    fairness to continuous 

attribute 

t
u
p
t
u
o
 
n
n

    motivation for doing this is 

related to robustnesss to 
uncertainties and 
interpretability

adv. trained nn

z' mass=20 gev

z' mass=35 gev

z' mass=50 gev

z' mass=100 gev

z' mass=200 gev

z' mass=300 gev

50

100

150

200
jet invariant mass [gev]

trad. nn

z' mass=20 gev

z' mass=35 gev

z' mass=50 gev

z' mass=100 gev

z' mass=200 gev

z' mass=300 gev

50

100

150

200
jet invariant mass [gev]

1

0.5

0
0

1

0.5

0
0

fig. 11.

pro   le of the paramterized nn responses

57

example:   

reusable components

   of course, particle physicists are among the first 

to realize that nature is compositional.    

      ya n n   l e c u n

   the world is compositional, or there is a god" 

      j a s o n   e i s n e r

s u b - at o m i c   s c a l e

60

s u b - at o m i c   s c a l e

pencil & paper calculable from first principles 

p(z    |   )

60

s u b - at o m i c   s c a l e

pencil & paper calculable from first principles 

p(z    |   )

controlled approximation of first principles 

p(z2 | z1,      )

60

s u b - at o m i c   s c a l e

pencil & paper calculable from first principles 

p(z    |   )

controlled approximation of first principles 

p(z2 | z1,      )

phenomenological model 

p(z3 | z2,      )

60

s u b - at o m i c   s c a l e

pencil & paper calculable from first principles 

p(z    |   )

controlled approximation of first principles 

p(z2 | z1,      )

phenomenological model 

p(z3 | z2,      )

exploit markov property:

60

s u b - at o m i c   s c a l e

pencil & paper calculable from first principles 

p(z    |   )

controlled approximation of first principles 

p(z2 | z1,      )

phenomenological model 

p(z3 | z2,      )

exploit markov property:

p(x|  ) =     p(x | z3,      ) p(z3 | z2,      )  p(z2 | z1,      ) p(z    |   ) dz

60

detector simulation p(x | z3,      ):  
    detailed engineering (cad) 
    in situ measurements of temperature, magnetic field, alignment, calibration constants 
    first-principles description of interaction of particles with matter 
    look up tables of measured interaction of particles with matter

detector simulation p(x | z3,      ):  
    detailed engineering (cad) 
    in situ measurements of temperature, magnetic field, alignment, calibration constants 
    first-principles description of interaction of particles with matter 
    look up tables of measured interaction of particles with matter
exploit markov property:

detector simulation p(x | z3,      ):  
    detailed engineering (cad) 
    in situ measurements of temperature, magnetic field, alignment, calibration constants 
    first-principles description of interaction of particles with matter 
    look up tables of measured interaction of particles with matter
exploit markov property:

p(x|  ) =     p(x | z3,      ) p(z3 | z2,      )  p(z2 | z1,      ) p(z    |   ) dz

s i m u l at i o n

theory 

parameters

events ~1015

partons ~10

momenta,
particle type

nuisance 
parameters

hadrons ~100

momenta,
particle type

measured 
parton 
density 

functions, etc.

detector
design,
alignment

measured 
interactions 
with matter

calibration
constants

sensors 108

energy 
deposit

sensor readout 108

raw data

legend

parameter 
of interest

nuisance 
parameter

latent
variable

observed
covariate

derived
quantities

62

s i m u l at i o n

theory 

parameters

  

events ~1015

partons ~10

momenta,
particle type

  

nuisance 
parameters

hadrons ~100

momenta,
particle type

z   

z2

measured 
parton 
density 

functions, etc.

detector
design,
alignment

measured 
interactions 
with matter

calibration
constants

sensors 108

energy 
deposit

z3

sensor readout 108

raw data

x

legend

parameter 
of interest

nuisance 
parameter

latent
variable

observed
covariate

derived
quantities

62

s i m u l at i o n   +   r e c o n s t r u c t i o n

theory 

parameters

  

events ~1015

partons ~10

momenta,
particle type

  

nuisance 
parameters

hadrons ~100

momenta,
particle type

    

parameter
estimates,
likelihood,
posterior

event-level
features

momenta,
summary 

stats

jets ~10

reconstructed particles ~100

momenta
particle type

   3

   2

z   

z2

measured 
parton 
density 

functions, etc.

detector
design,
alignment

measured 
interactions 
with matter

calibration
constants

clusters ~100

tracks ~100

energy,
summary 
stats

momenta,
impact 
parameter

sensors 108

energy 
deposit

z3

sensor readout 108

raw data

x

legend

parameter 
of interest

nuisance 
parameter

latent
variable

observed
covariate

derived
quantities

63

s i m u l at i o n   +   r e c o n s t r u c t i o n

theory 

parameters

  

    

parameter
estimates,
likelihood,
posterior

events ~1015

partons ~10

momenta,
particle type

  

nuisance 
parameters

hadrons ~100

correspondence
z   

jets ~10

   3

event-level
features

momenta,
summary 

stats

momenta,
particle type

z2

sensors 108

energy 
deposit

z3

measured 
parton 
density 

functions, etc.

detector
design,
alignment

measured 
interactions 
with matter

calibration
constants

   2

reconstructed particles ~100

momenta
particle type

clusters ~100

tracks ~100

energy,
summary 
stats

momenta,
impact 
parameter

sensor readout 108

raw data

x

legend

parameter 
of interest

nuisance 
parameter

latent
variable

observed
covariate

derived
quantities

63

c o m p o s i t i o n   &   r e d u c t i o n i s m
   the traditional reconstruction algorithms can be seen as attempt to invert the 
generative process (point estimate / regression) 
    generative model:        z1     z2     z2     x 
    sequential inversion: x           (x)        2(      )        1(   2 ) 

   key points:  

    can characterize & validate p(   1 | z1), p(   2 | z2), p(   3 | z3) with simulation 
    these components are reusable (id21) 

    e.g. an algorithm that looks for electrons in the data (segmentation & 
classification) and estimates their energy and momentum (regression). 

    provides a notion of    interpretable    that is practical and actionable  

    composition is at the heart of the reductionist paradigm of science 

64

c o m p o s i t i o n   o f   r e u s a b l e   c o m p o n e n t s

slides from jeff dean of google brain @ jeju july 2017

https://drive.google.com/file/d/0b8z5oupb2dyszwf1rtfux1nezuk/view 

65

parameter
estimates,
likelihood,
posterior

event-level
features

momenta,
summary 

stats

jets ~10

reconstructed particles ~100

momenta
particle type

theory 

parameters

events ~1015

partons ~10

momenta,
particle type

nuisance 
parameters

hadrons ~100

momenta,
particle type

measured 
parton 
density 

functions, etc.

detector
design,
alignment

measured 
interactions 
with matter

calibration
constants

clusters ~100

tracks ~100

energy,
summary 
stats

momenta,
impact 
parameter

sensors 108

energy 
deposit

sensor readout 108

raw data

d i f f e r e n t i a b l e   r e d u c t i o n i s m

   the reconstruction algorithms can be seen as attempt to invert the 
generative process (point estimate / regression) sequentially 

    generative model:        z1     z2     z2     x 
    sequential inversion: x           (x)        

2(      )        
1(   
2 ) 

   currently both generative model and inversion algorithms involve hand-
engineered, code not developed for auto-diff / back propagation 
(effectively not differentiable)  

    big gain from just reimplementing what we have in a differentiable 

programming framework 

   we can keep the compositional structure and gradually enhance each of 
the stages of the with deep learning components 

    a high-level form of inductive bias (innate structure) on the networks 

    jointly optimize & borrow power from all the tasks that use a certain 

component 

    maintain ability to characterize, validate , and interpret individual 

components 

    transition from deterministic point estimate to probabilistic 

components for improved uncertainty estimation

legend

parameter 
of interest

nuisance 
parameter

latent
variable

observed
covariate

derived
quantities

66

c o n c l u s i o n
   the developments in machine learning have the potential to 
effectively bridge the microscopic - macroscopic divide & aid in the 
inverse problem. 

    leverage expert knowledge of the generative process 

    learn surrogates that extract relevant features for id136 task  

   several strategies to incorporate domain knowledge into the model 

    starting point: migrate current code bases to differentiable 

programming framework 

    gradually replace components with deep learning 

   helpful to establish more actionable notions of    interpretability    

67

joint work with

joint work with

c o l l a b o r at o r s

u. li  ge

meghan frate

juan pavez

tilman plehn

isaac henrion

tim head

michael kagan

david rousseau

peter sadowski

daniel whiteson

pierre baldi

lezcano casado

wahid bhimji 
nersc, berkeley lab

phiala shanahan

william detmold

karen ng

tuan anh le

38 / 38

68

backup

p r o b   p r o g :   h o w   d o e s   i t   w o r k ?

id136 compilation
   in short: hijack the random number generators and use 
nn   s to perform a very smart type of importance sampling

input: an id136 
problem denoted in 
a universal ppl
(anglican, cpprob)

output: a trained 
id136 network, 
or    compilation 
artifact   
(torch, pytorch)

le, baydin and wood. id136 compilation and universal probabilistic programming.  aistats 2017. 
arxiv:1610.09900

70

interpretable compiled id136 for large-scale

i n   p r o g r e s s :   c + + ,   s h e r pa ,   g e a n t 4

scienti   c simulations

mario lezcano casado, at  l  m g  ne  s baydin, tuan anh le, frank wood   

department of engineering science

university of oxford

{lezcano,gunes,tuananh,fwood}@robots.ox.ac.uk

lukas heinrich, gilles louppe, kyle cranmer
department of physics & center for data science

new york university

{kyle.cranmer,lukas.heinrich,g.louppe}@cern.ch

wahid bhimji, prabhat

lawrence berkeley national laboratory

{wbhimji,prabhat}@lbl.gov

karen ng

intel

karen.y.ng@intel.com

abstract

we consider the problem of bayesian id136 in the family of probabilistic
models implicitly de   ned by a stochastic generative model of the data. in scien-
ti   c    elds ranging from population biology to cosmology, low-level mechanistic
components are composed to create complex generative models. these models
lead to intractable likelihoods and are typically non-differentiable, which poses
challenges for traditional approaches to id136. we extend previous work in
   id136 compilation   , which combines universal probabilistic programming and
deep learning methods, to the real-world c++-based simulation codes. we illustrate
the scalability of the technique with a challenging id136 problem from particle
physics aimed at establishing the properties of the recently discovered higgs boson.
we highlight that id136 based on the domain-speci   c simulator naturally emits
interpretable posterior samples with rich semantics.

1

introduction

& geant

a case study in sherpa
complex simulations are used for stochastic generative models for data across a diverse segment
of the scienti   c community. these simulations typically lead to intractable likelihoods, which
probabilistic program analytics
yield traditional statistical id136 algorithms irrelevant and motivate a new class of so-called
likelihood-free id136 algorithms.
allows us to pinpoint    interesting    addresses in execution traces 
there are two broad strategies for this likelihood-free id136 problem. in the    rst, one attempts
and corresponding c++ code within sherpa
to learn the necessary ingredients for id136 from examples drawn from the simulator. this
includes bayesian neural networks, the variational autoencoder [??], extensions of these recognition
models such as mdn-svi [papamakarios and murray, 2016], and approaches based on density ratio
estimation [cranmer et al., 2015]. alternatively, approximate bayesian computation (abc) refers to
a large class approaches for sampling from the posterior distribution of these likelihood-free models
where the original simulator is used as part of the id136 engine.

   for all the    sh

31st conference on neural information processing systems (nips 2017), long beach, ca, usa.

probabilistic programming with c++
our new tool: cpprob
https://github.com/probprog/cpprob 

instrumenting c++ code to allow tools like sherpa and geant run 
with id136 compilation

nersc, lawrence berkeley national lab
our current tools:
- cpprob

- a new c++ ppl coupled with large-scale simulations using, e.g., 

sherpa and geant

- pytorch id136 compilation backend

- dynamic computation graphs for nn artifacts

designed to run on cori at nersc using shifter

shifterimg -v pull docker:gbaydin/pytorch-infcomp:latest
shifterimg -v pull docker:gbaydin/sherpa-infcomp-full:latest

slides from at  l  m g  ne   baydin @ hammers & nails

https://dl4physicalsciences.github.io/files/nips_dlps_2017_30.pdf

71

g a n s   f o r   p h y s i c s

calogan: simulating 3d high energy particle
showers in multi-layer electromagnetic calorimeters
with id3

creating virtual universes using id3

mustafa mustafa   1, deborah bard1, wahid bhimji1, rami al-rfou2, and zarija luki  1

1lawrence berkeley national laboratory, berkeley, ca 94720

2google research, mountain view, ca 94043

michela paganinia,b, luke de oliveiraa, and benjamin nachmana

alawrence berkeley national laboratory, 1 cyclotron rd, berkeley, ca, 94720, usa
bdepartment of physics, yale university, new haven, ct 06520, usa
e-mail: michela.paganini@yale.edu, lukedeoliveira@lbl.gov, bnachman@cern.ch

7
1
0
2

 

n
u
j
 

abstract: simulation is a key component of physics analysis in particle physics and nuclear physics.
the most computationally expensive simulation step is the detailed modeling of particle showers inside
calorimeters. full detector simulations are too slow to meet the growing demands resulting from large
quantities of data; current fast simulations are not precise enough to serve the entire physics program.
therefore, we introduce calogan, a new fast simulation based on generative adversarial neural
figure 8: average    + geant shower (top), and average    + calogan shower (bottom), with
networks (gans). we apply the calogan to model electromagnetic showers in a longitudinally
progressive calorimeter depth (left to right).
segmented calorimeter. this represents a signi   cant stepping stone toward a full neural network-based
detector simulation that could save signi   cant computing time and enable many analyses now and
in the future. in particular, the calogan achieves speedup factors comparable to or better than
existing fast simulation techniques on cpu (100   -1000   ) and even faster on gpu (up to     105   ))
and has the capability of faithfully reproducing many aspects of key shower shape variables for a variety
of particle types.
figure 9: five randomly selected e+ showers per calorimeter layer from the training set (top) and the
   ve nearest neighbors (by euclidean distance) from a set of calogan candidates.

h
p
-
o
r
t
s
a
[
 
 

m

 
 
]

i
.

7

1
v
0
9
3
2
0

figure 10: five randomly selected   showers per calorimeter layer from the training set (top) and the
   ve nearest neighbors (by euclidean distance) from a set of calogan candidates.

6
0
7
1
:
v
i
x
r
a

.

figure 11: five randomly selected    + showers per calorimeter layer from the training set (top) and
the    ve nearest neighbors (by euclidean distance) from a set of calogan candidates.

    10    

abstract

inferring model parameters from experimental data is a grand challenge in many sciences, including cosmol-
ogy. this often relies critically on high    delity numerical simulations, which are prohibitively computationally
expensive. the application of deep learning techniques to generative modeling is renewing interest in using high
dimensional density estimators as computationally inexpensive emulators of fully-   edged simulations. these
generative models have the potential to make a dramatic shift in the    eld of scienti   c simulations, but for that
shift to happen we need to study the performance of such generators in the precision regime needed for science
applications. to this end, in this letter we apply id3 to the problem of generating
cosmological weak lensing convergence maps. we show that our generator network produces maps that are
described by, with high statistical con   dence, the same summary statistics as the fully simulated maps.
as in the traditional approach to emulators.

the scienti   c success of the next generation of sky
surveys (e.g. [1   5]) to test the current    standard model   
of cosmology (   cdm), hinges critically on the success
of underlying simulations. answering questions in cos-
mology about the nature of cold dark matter, dark
energy and the in   ation of the early universe, requires
relating observations of a large number of astrophysical
objects which trace the underlying matter density    eld,
to simulations of    virtual universes    with di   erent cos-
mological parameters. currently the creation of each
virtual universe requires an extremely computationally
expensive simulation on high performance computing
resources. in order to make this inverse problem prac-
tically solvable, constructing a computationally cheap
surrogate model or an emulator [6,7] is imperative.

in this letter, we study the ability of a recent vari-
ant of generative models - generative adversarial net-
works (gans) [9] to generate weak lensing convergence
maps. the training and validation maps are produced
using n-body simulations of    cdm cosmology. we
show that maps generated by the neural network ex-
hibit, with high statistical con   dence, the same power
(fourier) spectrum of the fully-   edged simulator maps,
as well as higher order non-gaussian features, thus
demonstrating that such scienti   c data is amenable to
a gan treatment for generation. the very high level
of agreement we achieve o   ers promise for building em-
ulators out of deep neural networks. we    rst present
our results and analysis then outline the future inves-
tigations which we think are critical to build such em-
ulators in the discussion section.

however, traditional approaches to emulators re-
quire the use of a summary-statistic which is to be em-
ulated. an approach that does not require such math-
ematical templates of the simulation outcome would
be of considerable value in the    eld. the ability to
emulate these simulations with high    delity, in a frac-
tion of the computational time, would boost our ability
to understand the fundamental nature of the universe.
while in this letter we focus our attention on cosmol-
ogy, and in particular weak lensing convergence maps,
we believe that this approach is relevant to many areas
of science and engineering.

results
gravitational lensing has potential to be one of the
most sensitive probes of the nature of dark energy [10],
and a   ects the shape and apparent brightness of every
galaxy we observe. convergence    (   ) is the quantity
that de   nes the brightness of an observed object as it
is a   ected by the matter along the line of sight between
that galaxy and the observer. it can be interpreted as
recent developments in deep generative modeling
a measure of the density of the universe observed from
techniques open the potential to meet this need. the
a particular direction. a full n-body simulation cre-
figure 1: weak lensing convergence maps for a    cdm cosmological model with  8 = 0.798, w =  1.0,
density estimators in these models are built out of neu-
ates convergence maps corresponding to many random
   m = 0.26 and        = 0.74. randomly selected maps from validation dataset (top) and gan generated
ral networks which can serve as universal approxima-
realizations of the same cosmological model. we set
examples (bottom).
tors [8], thus having the ability to learn the underlying
out to train a gan model on 256     256 pixels conver-
distributions of data and emulate the observable with-
gence maps taken from these simulations. a descrip-
out being biased by the choice of summary-statistics,
tion of the simulations and data preparation methods
sults we    rst outline the objective of generative models
is in the methods section. before we describe our re-
and the gans framework.

   corresponding author: mmustafa@lbl.gov

z     [n0(0, 1), . . . ,n63(0, 1)]

72

validation0.00.20.40.60.8(deg)generated0.00.20.40.60.8(deg)0.00.20.40.60.8(deg)10 410 310 210 1the small grey dots show the sampled parameter points at which the likelihood ratio
s e pa r a b i l i t y
was evaluated.

weak boson fusion, h    4   
    production vs decay
    hzz decay vertex:
many angular structures
    very clean
ob= i g
obb=    g   2

    same operators as before:
2(d        )(d     ) b    

q

figure       : id136 from truth likelihood ratio and carl   s estimate for the fully di   erential case
with regression. le   : scatter plot showing the di   erence between the exact expected
likelihood ratio for           randomly sampled points and   1 and carl   s estimate. right:
true (white) and approximate (cyan) likelihood contours, using a gaussian process for
interpolation.    e white and cyan dots show the exact and approximate maximum-
likelihood estimators.    e green and red dots show   observed and   1, respectively. finally,
the small grey dots show the sampled parameter points at which the likelihood ratio
was evaluated.

   +
      
   +
      

w, z

w, z

z

z

h

q

q   

q   

2(d     )      k(d     ) w k

    

ow= i g
oww=    g2

73

va r i at i o n a l   o p t i m i z at i o n
variational optimization

   

f (   )     e      q(   | )[f (   )] = u ( )

min
r u ( ) = e      q(   | )[f (   )r  log q(   | )]

piecewise constant   sin(x)

x

q(   |  = (  ,  )) = n (  , e )

25 / 38
74

a d v e r s a r i a l   va r i at i o n a l   o p t i m i z at i o n

adversarial variational optimization

   like a gan, but generative model is non-differentiable 
and the parameters of simulator have meaning

credits: 1707.07113

    replace the generative network with a non-di   erentiable

forward simulator g(z;    ).

    with vo, optimize upper bounds of the adversarial objectives:
(1)

ud = e      q(   | )[ld]
ug = e      q(   | )[lg]

(2)

respectively over   and  .

   effectively sampling from 
marginal model

operationally, we get the marginal model:

26 / 38

x     q(x| )             q(   | ), z     p(z|   ), x = g(z;    )

   we use wasserstein distance, 
as in wgan

g. louppe & k.c. arxiv:1707.07113

75

t h e   a d v e r s a r i a l   m o d e l

g. louppe, m. kagan, k. cranmer, arxiv:1611.01046

2

classi   er f

adversary r

z

x

...

   f

f (x;    f )

lf (   f )

...

   r

 1(f (x;    f );    r)

 2(f (x;    f );    r)

. . .

p( 1,  2, . . . )

p   r (z|f (x;    f ))

lr(   f ,    r)

fig. 1. architecture for the adversarial training of a binary classi   er f against a nuisance parameters z. the adversary r
models the distribution p(z|f (x;    f ) = s) of the nuisance parameters as observed only through the output f (x;    f ) of the
classi   er. by maximizing the antagonistic objective lr(   f ,    r) (as part of minimizing lf (   f )    lr(   f ,    r)), the classi   er f
forces p(z|f (x;    f ) = s) towards the prior p(z), which happens when f (x;    f ) is independent of the nuisance parameter z and
therefore pivotal.

   the      ,      ,     are the mean, 
standard deviation, and amplitude 
for the gaussian mixture model. 

p(z|f )

parameters. this implies that f (x;    f ) and z are inde-
pendent random variables.

    the neural network takes in f 

and predicts      ,      ,    

as stated in eqn. 1, the pivotal quantity criterion is
imposed with respect to p(x|z) where y is marginalized
out. in some situations however (see e.g., sec. v b), class
conditional independence of f (x;    f ) on the nuisance z
is preferred, which can then be stated as requiring

sity p   r (z|f (x;    f ) = s). intuitively, if p(f (x;    f ) = s|z)
varies with z, then the corresponding correlation can be
captured by r. by contrast, if p(f (x;    f ) = s|z) is invari-
ant with z, as we require, then r should perform poorly
and be close to random guessing. training f such that
it additionally minimizes the performance of r therefore
acts as a id173 towards eqn. 1.

f (x)

76

reinforcement / active learning  

+ likelihood free id136

r e i n f o r c e m e n t   l e a r n i n g   &   s c i e n t i f i c   m e t h o d

   scientist trying to decide what experiment to do next

78

r e i n f o r c e m e n t   l e a r n i n g   &   s c i e n t i f i c   m e t h o d

   scientist trying to decide what experiment to do next

perform experiment, 
gather data

statistical analysis

updated knowledge 
based on analyzing 
data

decide which 
experiment to 
perform

78

o p t i m i z i n g   e x p e r i m e n t s

   proof-of-principle algorithm can: 

h t t p s : / / g i t h u b . c o m / c r a n m e r / a c t i v e _ s c i e n c i n g

    measure parameter of theory (eg. weinberg angle in 

standard model of particle physics) from raw data 

    optimize experiment (eg. beam energy) for most 

sensitive measurement

i

 

n
a
g
n
o
i
t
a
m
r
o
f
n

i
 

d
e
t
c
e
p
x
e

80                    90                   100

beam energy

79

p h y s i c s - a w a r e   m a c h i n e   l e a r n i n g

nents using the formulae given in the appendix. the
search was run to depth 10, using the base kernels from
section 2.

   physics goes into the construction of a 
   kernel    that defines m.l. model 

cylinders. some of their discrete graph structures have
continous analogues in our own space; e.g. se1     se2
and se1     per2 can be seen as mapping the data to
a plane and a cylinder, respectively.

    vocabulary of kernels + grammar for 

composition = powerful modeling

grosse et al. (2012) performed a greedy search over a
compositional model class for unsupervised learning,
using a grammar and a search procedure which parallel
our own. this model class contained a large number
of existing unsupervised models as special cases and
was able to discover such structure automatically from
data. our work is tackling a similar problem, but in a
supervised setting.

5. structure discovery in time series

to investigate our method   s ability to discover struc-
ture, we ran the kernel search on several time-series.

structure discovery in nonparametric regression through compositional kernel search

60

( lin    se + se    ( per + rq ) )

mauna loa atmospheric co2 using our method,
we analyzed records of carbon dioxide levels recorded
at the mauna loa observatory. since this dataset was
analyzed in detail by rasmussen & williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

   20

20

40

0

   40

1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010

=

 lin    se

rq

( per + rq )

40

30

20

1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010

60

60

40

20

40

0

   20

20

   40

50

40

30

20

10

as discussed in section 2, a gp whose kernel is a sum
of kernels can be viewed as a sum of functions drawn
from component gps. this provides another method
of visualizing the learned structures. in particular, all
kernels in our search space can be equivalently writ-
ten as sums of products of base kernels by applying
distributivity. for example,

0

5

   20

0

   5

+

se    per

10

2000

2005

2010

0

2000

2005

2010

se     (rq + lin) = se     rq + se     lin.

we visualize the decompositions into sums of compo-
nents using the formulae given in the appendix. the
search was run to depth 10, using the base kernels from
section 2.

mauna loa atmospheric co2 using our method,
we analyzed records of carbon dioxide levels recorded
at the mauna loa observatory. since this dataset was
analyzed in detail by rasmussen & williams (2006),
we can compare the kernel chosen by our method to a
kernel constructed by human experts.

rq

( per + rq )

se    ( per + rq )

60

40

20

0

   20

40

30

20

10

0

2000

2005

2010

50

40

30

20

10

2000

2005

2010

4

1989

1988

1987

1986

1985

1984

+

se    rq 

figure 3. posterior mean and variance for di   erent depths
of kernel search. the dashed line marks the extent of the
dataset. in the    rst column, the function is only modeled
as a locally smooth function, and the extrapolation is poor.
next, a periodic component is added, and the extrapolation
improves. at depth 3, the kernel can capture most of the
relevant structure, and is able to extrapolate reasonably.

1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010

   4

   2

0

2

+

residuals

0.5

0

figure 3. posterior mean and variance for di   erent depths
of kernel search. the dashed line marks the extent of the

figure 4. first row: the posterior on the mauna loa
dataset, after a search of depth 10. subsequent rows show

80

2000

2005

2010

   0.5

1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010

p h y s i c s - a w a r e   m a c h i n e   l e a r n i n g
   we can inject our knowledge of physics into the machine learning models!   
we can extract knowledge learned from the data!

fig. 1: three parameter covariance

fig. 3: jes covariance structure

physics-aware gaussian processes
    arxiv:1709.05681

qcd-aware id56s

    arxiv:1702.00748

fig. 3: jes covariance structure
final kernel = 

poisson fluctuations 

=
+ mass resolution
fig. 2: gaussian process covariance

qcd-aware graph convolutional neural networks
3

fig. 4: pdf covariance structure

    nips2017 workshop [http://bit.ly/2akwyrg] 

+ parton density  

in to the paper may be tricky
functions
    essentially, does our gaussian process have features
we   d expect from jes/pdf e   ects

+ 
   

fig. 4: pdf covariance structure

to better construct a kernel, we can also include our un-
derstanding of detector e   ects and physics e   ects. we
look at the covariance matrix of the 3 parameter    t func-
tion by    tting the atlas dataset and using markov
chain monte carlo [cite emcee?] to sample the posterior
distribution, and create a covariance matrix from these
(fig 1). one can see a visible structure in the covariance,
samples (fig 3). one can see a high degree of correlation
suggesting the in   exibility of the    t function causes an-
across all points in the distribution.
chor points which the    t pivots around. this hints that
pdf e   ects were implemented in the paper [cite] by
the parametric    ts have some sort of inherent structure
taking the 8 tev dijet analysis data [cite] and comput-

+ jet energy scale

+ 

distribution, and create a covariance matrix from these
samples (fig 3). one can see a high degree of correlation
across all points in the distribution.

pdf e   ects were implemented in the paper [cite] by
taking the 8 tev dijet analysis data [cite] and comput-
ing a covariance matrix from applying di   erent pdf sets
(fig 4).

for comparison, we also create a covariance from a
sliding window fit (swift). the swift solution to
the problems with    tting at high luminosities is to    t the
parametric form within smaller segments of the distribu-
tion, and piece together a    nal background estimation
across the whole spectrum. this method should create a

81

g r av i tat i o n a l   w av e s   &   n e u t r i n o s

gravitational waves

prepared for submission to jinst

convolutional neural networks applied to
neutrino events in a liquid argon time
projection chamber

sxs

source: ligo.org

2

6
1
0
2
 
v
o
n
 
7
1
 
 
]
t
e
d
-
s
n
i
.
s
c
i
s
y
h
p
[
 
 

1
v
1
3
5
5
0

.

1
1
6
1
:
v
23
i
x
r
a

microboone collaboration
r. acciarrig c. adamsz r. anh j. asaadiw m. augera l. bagbyg b. ballerg g. barrq
m. bassq f. bayx m. bishaib a. blake j t. boltoni l. bugelm l. camilleri f
d. caratelli f b. carlsg r. castillo fernandezg f. cavannag h. chenb e. churchr
d. ciancil, f g. h. collinm j. m. conradm m. converyu j. i. crespo-anad  n f
m. del tuttoq d. devitt j s. dytmans b. eberlyu a. ereditatoa l. escudero
sanchezc j. esquivelv b. t. flemingz w. foremand a. p. furmanskil g. t. garveyk
v. genty f d. goeldia s. gollapinnii n. grafs e. gramelliniz h. greenleeg
r. grossoe r. guenetteq a. hackenburgz p. hamiltonv o. henm j. hewesl c. hilll
j. hod g. horton-smithi c. jamesg j. jan de vriesc c.-m. jeny l. jiangs
r. a. johnsone b. j. p. jonesm j. joshib h. jostleing d. kaleko f g. karagiorgil, f
w. ketchumg b. kirbyb m. kirbyg t. kobilarcikg i. kresloa a. laubeq y. lib
a. lister j b. r. littlejohnh s. lockwitzg d. lorcaa w. c. louisk m. luethia
b. lundbergg x. luoz a. marchionnig c. marianiy j. marshallc
d. a. martinez caicedoh v. meddagei t. micelio g. b. millsk j. moonm m. mooneyb
c. d. mooreg j. mousseaun r. murrellsl d. napless p. nienabert j. nowak j
o. palamarag v. paolones v. papavassiliouo s.f. pateo z. pavlovicg d. porziol
g. pulliamv x. qianb j. l. raafg a. ra   quei l. rochesteru c. rudolf von rohra
b. russellz d. w. schmitzd a. schukraftg w. seligman f m. h. shaevitz f
j. sinclaira e. l. sniderg m. soderbergv s. s  ldner-remboldl s. r. soletiq
p. spentzourisg j. spitzn j. st. johne t. straussg a. m. szelcl n. taggp k. terao f
m. thomsonc m. toupsg y.-t. tsaiu s. tufanliz t. usheru r. g. van de waterk
b. virenb m. webera j. westonc d. a. wickremasinghes s. wolbersg
t. wongjiradm k. woodruffo t. yangg g. p. zellerg j. zennamod c. zhangb
auniversit  t bern, bern ch-3012, switzerland
bbrookhaven national laboratory (bnl), upton, ny, 11973, usa

82

live demo: 
www.tiny.cc/dlgw

detecting gw150914

data not included in training

trained with only non-spinning, 

non-eccentric simulations

~1s to analyze 4096s of data.

masses correct within error bars

no false alarms with two 

detectors!

j u a n   c a r r a s q u i l l a   @   n i p s

w   

w  

restricted id82 wave function
rbm id203 distribution:
j  j +pi log   1+e c 
p ( ) = epj b 
rbm wavefunction:
  ,  ( ) =s p ( )

    = log p  ( )

i +pj w  

ij  j   

ei   ( )

z 

h  

h 

 

widespread use of rbms to solve many-body physics: 

variational ansatz for quantum wave-functions (carleo & troyer, science 2017) 
exact representation of topological states (deng, li & das sarma, arxiv 2016) 
accelerate monte carlo simulations (huang & wang, prb 2017) 
topological quantum error correction (gt & melko, prl) 
and more . . . 

but other choices for the neural network are also possible (id98, mlp etc)

torlai, mazzola, carrasquilla, troyer,  melko and carleo 1703:05334

neural-network quantum 

state tomography for 
large many-body systems

jimmy ba

juan carrasquilla

neural-network quantum 

state tomography for 
large many-body systems

brendan frey

marzyeh ghassemi

a

pascal poupart

b ising square ice ground state

kitaev   s toric code ground state

sageev oore

h =  jpxp yi2p
| tci / lim

e

 z

i   jvxv yi2v
 !1 x 1,..., n
      

8

 x
i

 

c

2 jppqi2p  z
exp  jxp yi2p

8

    

ocold( 1, ...,  n ) / lim
 !1

i    >=

## "p
"

" "#
#v
i | 1, ...,  ni

high temperature state

d

8

 z
i

  >  i

peps : f. verstraete, m. m. wolf, d. perez-garcia, j. i. cirac phys. rev. lett. 96, 220601 (2006).

j. carrasquilla and r. g. melko. nature physics 13, 431   434 (2017) 
dong-ling deng  et al phys. rev. x 7, 021021 (2017)  
jing chen, song cheng, haidong xie, lei wang, tao xiang arxiv:1701.04831 rbms

p. (b) and (c) portray ground state and high temperature spin con   gurations of the square ice

the classical toric code hamiltonian is de   ned as a sum over the product of spins on a plaquette

hamiltonian, respectively. (d) a ground state con   guration of the toric code hamiltonian.

fig. 4: square ice and toric code models and their typical con   gurations. (a) the charge qv in

p. (b) and (c) portray ground state and high temperature spin con   gurations of the square ice

the classical toric code hamiltonian is de   ned as a sum over the product of spins on a plaquette

the square ice hamiltonian is de   ned as the sum over the spins on the bonds of a vertex v , while

hamiltonian, respectively. (d) a ground state con   guration of the toric code hamiltonian.

the square ice hamiltonian is de   ned as the sum over the spins on the bonds of a vertex v , while

fig. 4: square ice and toric code models and their typical con   gurations. (a) the charge qv in

the square ice hamiltonian is de   ned as the sum over the spins on the bonds of a vertex v , while

the classical toric code hamiltonian is de   ned as a sum over the product of spins on a plaquette

p. (b) and (c) portray ground state and high temperature spin con   gurations of the square ice

hamiltonian, respectively. (d) a ground state con   guration of the toric code hamiltonian.

fig. 4: square ice and toric code models and their typical con   gurations. (a) the charge qv in

toric code ground state

83

g e n e r at i v e   m o d e l s   f o r   c a l i b r at i o n

   use of generative models of 
galaxy images to help calibrate 
down-stream analysis in next-
generation surveys. 

2

fig. 2: samples from the galaxy-zoo dataset and generated samples using conditional generative adversarial network of section iii. each
synthetic image is a 128     128 colored image (here inverted) produced by conditioning on the same set of features y 2 [0, 1]37 as its real
pair. these instances are selected from the test-set and were unavailable to the model during the training.

84

1enablingdarkenergysciencewithdeepgenerativemodelsofgalaxyimagessiamakravanbakhsh1,franc  oislanusse2,rachelmandelbaum2,jeffschneider1,andbarnab  asp  oczos11schoolofcomputerscience,carnegiemellonuniversity2mcwilliamscenterforcosmology,carnegiemellonuniversityabstract   understandingthenatureofdarkenergy,themys-teriousforcedrivingtheacceleratedexpansionoftheuniverse,isamajorchallengeofmoderncosmology.thenextgenerationofcosmologicalsurveys,speci   callydesignedtoaddressthisissue,relyonaccuratemeasurementsoftheapparentshapesofdistantgalaxies.however,shapemeasurementmethodssufferfromvariousunavoidablebiasesandthereforewillrelyonaprecisecalibrationtomeettheaccuracyrequirementsofthescienceanalysis.thiscalibrationprocessremainsanopenchallengeasitrequireslargesetsofhighqualitygalaxyimages.tothisend,westudytheapplicationofdeepconditionalgenerativemodelsingeneratingrealisticgalaxyimages.inparticularweconsidervariationsonconditionalvariationalautoencoderandintroduceanewadversarialobjectivefortrainingofconditionalgenerativenetworks.ourresultssuggestareliablealternativetotheacquisitionofexpensivehighqualityobservationsforgeneratingthecalibrationdataneededbythenextgenerationofcosmologicalsurveys.thelasttwodecadeshavegreatlyclari   edthecontentsoftheuniverse,whileleavingseverallargemysteriesinourcos-mologicalmodel.wenowhavecompellingevidencethattheexpansionrateoftheuniverseisaccelerating,suggestingthatthevastmajorityofthetotalenergycontentoftheuniverseistheso-calleddarkenergy.yetwelackanunderstandingofwhatdarkenergyactuallyis,whichprovidesoneofthemainmotivationsbehindthenextgenerationofcosmologicalsurveyssuchaslsst(lsstsciencecollaborationetal.,2009),euclid(laureijsetal.,2011)andwfirst(greenetal.,2012).thesebilliondollarprojectsarespeci   callydesignedtoshedlightonthenatureofdarkenergybyprobingtheuniversethroughtheweakgravitationallensingeffect   i.e.,theminutede   ectionofthelightfromdistantobjectsbytheinterveningmassivelargescalestructuresoftheuniverse.oncosmologicalscales,thislensingeffectcausesverysmallbutcoherentdeformationsofbackgroundgalaxyimages,whichappearslightlysheared,providingawaytostatisticallymapthematterdistributionintheuniverse.tomeasurethelensingsignal,futuresurveyswillimageandmeasuretheshapesofbillionsofgalaxies,signi   cantlydrivingdownstatisticalerrorscomparedtothecurrentgenerationofsurveys,tothelevelwheredarkenergymodelsmaybecomedistinguishable.however,thequalityofthisanalysishingesontheaccuracyoftheshapemeasurementalgorithmstaskedwithestimatingtheellipticitiesofthegalaxiesinthesurvey.thispointisparticularlycrucialtothesuccessofthesemissions,asanyunaccountedformeasurementbiasesintheirensembleaverageswouldimpactthe   nalcosmologicalanalysisandpotentiallyleadtofalseconclusions.inordertodetectand/orcalibrateanysuchbiases,futuresurveyswillheavilyrelyonimagesimulations,closelymimickingrealobservationsbutwithaknowngroundtruthlensingsignal.fig.1:illustrationoftheprocessesinvolvedinthemeasurementofweakgravitationallensing.thelightfromdistantgalaxiesisde   ectedbythematterintheuniverse,causingashearingofthegalaxyimages,whicharethenfurtherblurredbytheatmosphereandthetelescopeopticsand   nallypixelatedintoanoisyimagebytheimagingsensor.imagecredit:mandelbaumetal.(2014),adaptedfromkitchingetal.(2010).producingtheseimagesimulations,however,ischallenginginitselfastheyrequirehighqualitygalaxyimagesastheinputofthesimulationpipeline.suchobservationscanonlybeobtainedbyextremelyexpensivespace-basedimagingsurveys,whichwillremainascarceresourcefortheforeseeablefuture.thelargestcurrentsurveybeingusedforimagesimulationpurposesisthecosmossurvey(scovilleetal.,2007),carriedoutusingthehubblespacetelescope(hst).despitebeingthelargestavailabledataset,cosmosisrelativelysmall,andthereisgreatinterestinincreasingthesizeofourgalaxyimagesamplestoimprovethequalityofthiscrucialcalibrationprocess.inthiswork,weproposeanalternativetotheexpensiveacquisitionofmorehighqualitycalibrationdatausingdeepconditionalgenerativemodels.inrecentyears,thesemodelshaveachievedremarkablesuccessinmodelingcomplexhigh-dimensionaldistributions,producingnaturalimagesthatcanpassthevisualturingtest.twoprominentapproachesfortrainingthesemodelsarevariationalautoencoder(vae)(kingmaandwelling,2013;rezendeetal.,2014)andgener-ativeadversarialnetwork(gan)(goodfellowetal.,2014).ouraimistotrainacoditionalvariationofthesemodelsusingexistinghstdataandgeneratenewgalaxyimagesarxiv:1609.05796v1  [astro-ph.im]  19 sep 20161enablingdarkenergysciencewithdeepgenerativemodelsofgalaxyimagessiamakravanbakhsh1,franc  oislanusse2,rachelmandelbaum2,jeffschneider1,andbarnab  asp  oczos11schoolofcomputerscience,carnegiemellonuniversity2mcwilliamscenterforcosmology,carnegiemellonuniversityabstract   understandingthenatureofdarkenergy,themys-teriousforcedrivingtheacceleratedexpansionoftheuniverse,isamajorchallengeofmoderncosmology.thenextgenerationofcosmologicalsurveys,speci   callydesignedtoaddressthisissue,relyonaccuratemeasurementsoftheapparentshapesofdistantgalaxies.however,shapemeasurementmethodssufferfromvariousunavoidablebiasesandthereforewillrelyonaprecisecalibrationtomeettheaccuracyrequirementsofthescienceanalysis.thiscalibrationprocessremainsanopenchallengeasitrequireslargesetsofhighqualitygalaxyimages.tothisend,westudytheapplicationofdeepconditionalgenerativemodelsingeneratingrealisticgalaxyimages.inparticularweconsidervariationsonconditionalvariationalautoencoderandintroduceanewadversarialobjectivefortrainingofconditionalgenerativenetworks.ourresultssuggestareliablealternativetotheacquisitionofexpensivehighqualityobservationsforgeneratingthecalibrationdataneededbythenextgenerationofcosmologicalsurveys.thelasttwodecadeshavegreatlyclari   edthecontentsoftheuniverse,whileleavingseverallargemysteriesinourcos-mologicalmodel.wenowhavecompellingevidencethattheexpansionrateoftheuniverseisaccelerating,suggestingthatthevastmajorityofthetotalenergycontentoftheuniverseistheso-calleddarkenergy.yetwelackanunderstandingofwhatdarkenergyactuallyis,whichprovidesoneofthemainmotivationsbehindthenextgenerationofcosmologicalsurveyssuchaslsst(lsstsciencecollaborationetal.,2009),euclid(laureijsetal.,2011)andwfirst(greenetal.,2012).thesebilliondollarprojectsarespeci   callydesignedtoshedlightonthenatureofdarkenergybyprobingtheuniversethroughtheweakgravitationallensingeffect   i.e.,theminutede   ectionofthelightfromdistantobjectsbytheinterveningmassivelargescalestructuresoftheuniverse.oncosmologicalscales,thislensingeffectcausesverysmallbutcoherentdeformationsofbackgroundgalaxyimages,whichappearslightlysheared,providingawaytostatisticallymapthematterdistributionintheuniverse.tomeasurethelensingsignal,futuresurveyswillimageandmeasuretheshapesofbillionsofgalaxies,signi   cantlydrivingdownstatisticalerrorscomparedtothecurrentgenerationofsurveys,tothelevelwheredarkenergymodelsmaybecomedistinguishable.however,thequalityofthisanalysishingesontheaccuracyoftheshapemeasurementalgorithmstaskedwithestimatingtheellipticitiesofthegalaxiesinthesurvey.thispointisparticularlycrucialtothesuccessofthesemissions,asanyunaccountedformeasurementbiasesintheirensembleaverageswouldimpactthe   nalcosmologicalanalysisandpotentiallyleadtofalseconclusions.inordertodetectand/orcalibrateanysuchbiases,futuresurveyswillheavilyrelyonimagesimulations,closelymimickingrealobservationsbutwithaknowngroundtruthlensingsignal.fig.1:illustrationoftheprocessesinvolvedinthemeasurementofweakgravitationallensing.thelightfromdistantgalaxiesisde   ectedbythematterintheuniverse,causingashearingofthegalaxyimages,whicharethenfurtherblurredbytheatmosphereandthetelescopeopticsand   nallypixelatedintoanoisyimagebytheimagingsensor.imagecredit:mandelbaumetal.(2014),adaptedfromkitchingetal.(2010).producingtheseimagesimulations,however,ischallenginginitselfastheyrequirehighqualitygalaxyimagesastheinputofthesimulationpipeline.suchobservationscanonlybeobtainedbyextremelyexpensivespace-basedimagingsurveys,whichwillremainascarceresourcefortheforeseeablefuture.thelargestcurrentsurveybeingusedforimagesimulationpurposesisthecosmossurvey(scovilleetal.,2007),carriedoutusingthehubblespacetelescope(hst).despitebeingthelargestavailabledataset,cosmosisrelativelysmall,andthereisgreatinterestinincreasingthesizeofourgalaxyimagesamplestoimprovethequalityofthiscrucialcalibrationprocess.inthiswork,weproposeanalternativetotheexpensiveacquisitionofmorehighqualitycalibrationdatausingdeepconditionalgenerativemodels.inrecentyears,thesemodelshaveachievedremarkablesuccessinmodelingcomplexhigh-dimensionaldistributions,producingnaturalimagesthatcanpassthevisualturingtest.twoprominentapproachesfortrainingthesemodelsarevariationalautoencoder(vae)(kingmaandwelling,2013;rezendeetal.,2014)andgener-ativeadversarialnetwork(gan)(goodfellowetal.,2014).ouraimistotrainacoditionalvariationofthesemodelsusingexistinghstdataandgeneratenewgalaxyimagesarxiv:1609.05796v1  [astro-ph.im]  19 sep 2016f r o m   i m a g e s   t o   s e n t e n c e s

   id56s showing great performance for 
natural language processing tasks 

    neural network   s topology given by parsing of sentence!

85

f r o m   i m a g e s   t o   s e n t e n c e s

   id56s showing great performance for 
natural language processing tasks 

    neural network   s topology given by parsing of sentence!

analogy: 
word     particle 
parsing     jet algorithm

85

q c d - i n s p i r e d   r e c u r s i v e   n e u r a l   n e t w o r k s

kt

anti-kt

   work with gilles louppe, kyunghyun cho, cyril becot 

    use sequential recombination jet algorithms to 
provide network topology (on a per-jet basis) 

    path towards ml models with good physics 

properties 

    top node of recursive network provides a fixed-length 

embedding of a jet that can be fed to a classifier

    arxiv:1702.00748  & follow up work with joan bruna using graph conv nets

86

q c d - i n s p i r e d   r e c u r s i v e   n e u r a l   n e t w o r k s

[19] nadine fischer, stefan prestel, mathias ritzmann, and
2016,

[20] m. ritzmann, d. a. kosower, and p. skands. an-
tenna showers with hadronic initial states. phys. lett.,

[21] junyoung chung, caglar gulcehre, kyunghyun cho,
and yoshua bengio. empirical evaluation of gated re-
current neural networks on sequence modeling. arxiv

[22] diederik kingma and jimmy ba. adam: a method for
stochastic optimization. arxiv preprint arxiv:1412.6980,

[23] jesse thaler and ken van tilburg. identifying boosted
jhep, 03:015, 2011,

[24] samuel r bowman, christopher d manning, and
christopher potts. tree-structured composition in neu-
ral networks without tree-structured architectures. arxiv

[25] samuel ryan bowman. modeling natural language se-
mantics in learned representations. phd thesis, stan-

[26] xing shi, inkit padhi, and kevin knight. does string-
in proc. of

[27] gilles louppe, michael kagan, and kyle cranmer.
2016,

[28] leif lonnblad, carsten peterson, and thorsteinn rogn-
valdsson. finding gluon jets with a neural trigger.

[29] leif lonnblad, carsten peterson, and thorsteinn rogn-
valdsson. using neural networks to identify jets. nucl.

[30] r. sinkus and t. voss. particle identi   cation with neural
networks using a rotational invariant moment represen-
tation. nucl. instrum. meth., a391:360   368, 1997.

[31] p. chiappetta, p. colangelo, p. de felice, g. nardulli,
and g. pasquariello. higgs search by neural networks at
lhc. phys. lett., b322:219   223, 1994, hep-ph/9401343.
[32] bruce h. denby. neural networks and cellular au-
tomata in experimental high-energy physics. comput.

[33] samuel r bowman, jon gauthier, abhinav rastogi,
raghav gupta, christopher d manning, and christo-
pher potts. a fast uni   ed model for parsing and sentence
understanding. arxiv preprint arxiv:1603.06021, 2016.
[34] dani yogatama, phil blunsom, chris dyer, edward
grefenstette, and wang ling. learning to compose
words into sentences with id23. arxiv

[35] joan bruna, wojciech zaremba, arthur szlam, and yann
lecun. spectral networks and locally connected net-

[39] micha  el de   errard, xavier bresson,

and pierre
vandergheynst.
convolutional neural networks on
graphs with fast localized spectral    ltering. corr,
abs/1606.09375, 2016.

[40] thomas n kipf and max welling.

semi-supervised
classi   cation with id197. arxiv
preprint arxiv:1609.02907, 2016.

[41] thomas n. kipf and max welling. semi-supervised clas-
si   cation with id197. corr,
abs/1609.02907, 2016.

[42] dougal maclaurin, david duvenaud, matthew johnson,
and ryan p. adams. autograd: reverse-mode di   eren-
tiation of native python, 2015.

kt

appendix a: gated recursive jet embedding

, hjet
kr

the recursive activation proposed in sec. iii a su   ers
from two critical issues. first, it assumes that left-child,
right-child and local node information hjet
, uk are
kl
all equally relevant for computing the new activation,
while only some of this information may be needed and
selected. second, it forces information to pass through
several levels of non-linearities and does not allow to
propagate unchanged from leaves to root. addressing
these issues and generalizing from [12   14], we recursively
de   ne a recursive activation equipped with reset and up-
date gates as follows:

if k is a leaf

+ otherwise

(a1)

k + zl   hjet

kl

+ zn   uk

if k is a leaf
otherwise

hjet

  hjet

kr

okl + okr

uk =   (wug(ok) + bu)

uk
zh     hjet
,! zr   hjet

k =8><>:
ok =(vi(k)
k =  0@w  h24
375 = softmax0bb@wz2664
35 = sigmoid0@wr24

rl   hjet
rr   hjet
rn   uk
  hjet
k
hjet
kl
hjet
kr
uk
hjet
kl
hjet
kr
uk

kr

kl

zh
zl
zr
zn

rl
rr
rn

264
24

35 + b  h1a
3775 + bz1cca
35 + br1a

where w  h 2 rq   3q, b  h 2 rq, wz 2 rq   4q, bz 2 rq,
wr 2 rq   3q, br 2 rq, wu 2 rq   4 and bu 2 rq form

    each node combines 4-momentum in (e-

scheme recombination of ok) and a non-linear 
transformation of hidden state of children hkl, 
hkr               

(a2)

(a3)

(a4)

    recursively applied (shared weights, markov) 

(a5)

       gating    allows for weighting of information of 
l/r children and for to flow directly along one 
branch

(a6)

87

q c d - i n s p i r e d   r e c u r s i v e   n e u r a l   n e t w o r k s

kt

particles

towers 

images

anti-kt

6

    down-sampling by 

    w-jet tagging example   

using data from dawe, et 
al arxiv:1609.00607 

further supported by the poor performance of the random
binary tree topology. we expected however that a simple
sequence (represented as a degenerate binary tree) based
on ascending and descending pt ordering would not per-
form particularly well, particularly since the topology
does not use any angular information. surprisingly, the
simple descending pt ordering slightly outperforms the
id56s based on kt and c/a topologies. the descending
pt network has the highest pt 4-momenta near the root
of the tree, which we expect to be the most important.
we suspect this is the reason that the descending pt out-
performs the ascending pt ordering on particles, but this
is not supported by the performance on towers. a similar
observation was already made in the context of natural
languages [24   26], where tree-based models have at best
only slightly outperformed simpler sequence-based net-
works. while recursive networks appear as a principled
choice, it is conjectured that recurrent networks may in
fact be able to discover and implicitly use recursive com-
positional structure by themselves, without supervision.
that we varied was

projecting into images 
looses information 

data to train!

d. gating the last

    id56 needs much less 

factor

88

q c d - i n s p i r e d   r e c u r s i v e   n e u r a l   n e t w o r k s

kt

fig. 3. jet classi   cation performance for various input rep-
resentations of the id56 classi   er, using kt topologies for the
embedding. the plot shows that there is signi   cant improve-
ment from removing the image processing step and that sig-
ni   cant gains can be made with more accurate measurements
of the 4-momenta.

kt

d. gating the last

observation was already made in the context of natural
languages [24   26], where tree-based models have at best
only slightly outperformed simpler sequence-based net-
works. while recursive networks appear as a principled
choice, it is conjectured that recurrent networks may in
fact be able to discover and implicitly use recursive com-
positional structure by themselves, without supervision.
that we varied was
whether or not to incorporate gating in the id56. adding
gating increases the number of parameters to 48,761, but
this is still about 20 times smaller than the number of
parameters in the maxout architectures used in previ-
ous jet image studies. table i shows the performance of
the various id56 topologies with gating. while results
improve signi   cantly with gating, most notably in terms
of r   =50%, the trends in terms of topologies remain un-
changed.

anti-kt

factor

    choice of jet 

e. other variants finally, we also considered a num-
ber of other variants. for example, we jointly trained
a classi   er with the concatenated embeddings obtained
over kt and anti-kt topologies, but saw no signi   cant
performance gain. we also tested the performance of
recursive activations transferred across topologies. for
algorithm matters 
instance, we used the recursive activation learned with
a kt topology when applied to an anti-kt topology and
observed a signi   cant loss in performance. we also con-
sidered particle and tower level inputs with an additional
trimming preprocessing step, which was used for the jet
image studies, but we saw a signi   cant loss in perfor-
mance. while the trimming degraded classi   cation per-
formance, we did not evaluate the robustness to pileup
that motivates trimming and other jet grooming proce-
dures.
anti-kt

performance

       gating    improves 

b.

infrared and collinear safety studies

in proposing variables to characterize substructure,

89

j e t- l e v e l   c l a s s i f i c at i o n   r e s u lt s

5

table i. summary of jet classi   cation performance for sev-
eral approaches applied either to particle-level inputs or tow-
ers from a delphes simulation.

input architecture

roc auc
projected into images

r   =50%

towers maxout
towers
towers

kt (gated)

kt

   21

c/a
anti-kt
asc-pt
desc-pt
random

towers
towers mass +    21
towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

kt

   

kt

0.8418
0.8321    0.0025 12.7    0.4
0.8277    0.0028
12.4    0.3
without image preprocessing
0.7644
6.79
11.31
0.8212
24.1    0.6
0.8807    0.0010
24.2    0.7
0.8831    0.0010
22.3    0.8
0.8737    0.0017
0.8835    0.0009 26.2    0.7
0.8838    0.0010 25.1    0.6
0.8704    0.0011
20.4    0.3
0.9185    0.0006
68.3    1.8
0.9192    0.0008 68.3    3.6
51.7    3.5
0.9096    0.0013
0.9130    0.0031
52.5    7.3
0.9189    0.0009 70.4    3.6
51.1    2.0
0.9121    0.0008
0.8822    0.0006
25.4    0.4
26.2    0.8
0.8861    0.0014
24.4    0.4
0.8804    0.0010
0.8849    0.0012
27.2    0.8
0.8864    0.0007 27.5    0.6
0.8751    0.0029
22.8    1.2
0.9195    0.0009
74.3    2.4
0.9222    0.0007 81.8    3.1
0.9156    0.0012
68.3    3.2
0.9137    0.0046 54.8    11.7
0.9212    0.0005 83.3    3.1
0.9106    0.0035
50.7    6.7

c/a
anti-kt
asc-pt
desc-pt
random
with gating (see appendix a)

c/a
anti-kt
asc-pt
desc-pt
random

c/a
anti-kt
asc-pt
desc-pt
random

kt

kt

towers
towers
towers
towers
towers
towers
particles
particles
particles
particles
particles
particles

   when working on images: 

performance to previous approaches 

to 0.8807 (resp., r   =50% from 12.7 to 24.1) in the case
of kt topologies. in addition, this result outperforms the
maxout architecture operating on images by a signi   -
cant margin. this suggests that the projection into an
image loses information and impacts classi   cation perfor-
    recursive network has similar 
mance. we suspect the loss of information to be due to
some of the construction steps of jet images (i.e., pixeli-
sation, rotation, zooming, cropping and id172).
in particular, all are applied at the image-level instead of
being performed directly on the 4-momenta, which might
   improved performance when working with 
induce artefacts due to the lower resolution, particle su-
perposition and aliasing. by contrast, the id56 is able
calo towers without image pre-processing  
to work directly with the 4-momenta of a variable-length
set of particles, without any loss of information. for
completeness, we also compare to the performance of a
classi   er based purely on the single n-subjettiness fea-
    loss of information depends on 
ture    21 :=    2/   1 and a classi   er based on two features
(the trimmed mass and    21) [23]. in agreement with pre-
vious results based on deep learning [2, 6], we see that
our id56 classi   er clearly outperforms this variable.

details of calorimeter, pixelation, etc. 

b. measurements of the 4-momenta the second fac-
   working on truth-level particles led to a 
tor we varied was the source of the 4-momenta. the
towers scenario, corresponds to the case where the
significant improvement 
4-momenta come from the calorimeter simulation in
delphes. while the calorimeter simulation is simplistic,
the granularity of the towers is quite large (10  in  )
and it does not take into account that tracking detectors
    generically expect information from 
can provide very accurate momenta measurements for
charged particles that can be combined with calorimetry
as in the particle    ow approach. thus, we also consider
the particles scenario, which corresponds to an idealized
case where the 4-momenta come from perfectly measured
stable hadrons from pythia. table i and fig. 3 show that
further gains could be made with more accurate measure-
ments of the 4-momenta, improving e.g. roc auc from
0.8807 to 0.9185 (resp., r   =50% from 24.1 to 68.3) in the

tracking, particle flow, etc. to be 
somewhere between towers and truth 
particle-level 

90

introduction

jet physics

previous work

proposed model

experiments

conclusions

neural message passing for jet

physics

isaac henrion, johann brehmer, joan bruna,
kyunghyun cho, kyle cranmer, gilles louppe,

gaspar rochette

courant institute & center for data science

paper: https://dl4physicalsciences.github.io/files/nips_dlps_2017_29.pdf
talk:    https://dl4physicalsciences.github.io/files/nips_dlps_2017_slides_henrion.pdf

91

jet physics

introduction
message passing neural network

previous work

proposed model

experiments

conclusions

algorithm 1 message passing neural network
require: n     d nodes x, adjacency matrix a
h  embed(x)
for t = 1, . . . , t do

m   message(a, h)
h   vertexupdate(h, m)

end for
r = readout(h)
return classify(r)

adjacency matrix generalizes receptive field of convolution kernel 
vertex update like pooling 
iterations like layers of a id98

92

jet physics

introduction
graph neural networks
introduction
graph neural networks

jet physics

previous work

previous work

proposed model

experiments

conclusions

proposed model

experiments

conclusions

j

j

a

a

b

b

c
c

d
d

g

g
mg!f

i

i

mi!f

f
f

mc!f

me!f

e
e

j
j

j = f (ht 1
  mt
j = f (ht 1
)
  mt
)
j!i =  (aij   mt
mt
j!i =  (aij   mt
mt
j )
j )
i = gru(ht 1
ht

i

h

h

,    jmt

j!i)

93

jet physics

introduction
message passing neural network

previous work

proposed model

experiments

conclusions

algorithm 2 message passing neural network
require: n     d array of jet constituents x
h  embed(x)
for t = 1, . . . , t do

a   adjacencymatrixt(h)
m   messaget(a, h)
h   vertexupdatet(h, m)

end for
r = readout(h)
return classify(r)

difference from alg 1:  
new weights for each iteration (layer) of message passing

94

jet physics

introduction
a problem with the adjacency matrix

previous work

proposed model

experiments

conclusions

question
where does adjacency matrix come from?

answer 1
use a physics-inspired adjacency matrix.
bonus: import physics knowledge

answer 2
learn the adjacency matrix from the data.
bonus: export physics algorithm

very interesting:  
adjacency matrix can be interpreted like a kt, c/a, anti-kt 
once learned, can export the adjacency function for other uses 
provides bi-directional interface between ml and jet physics.

95

jet physics

introduction
learning the adjacency matrix

previous work

ij = f (ht 1
s t

i

, ht 1

j

)

at

ij =

asym =

exp{s t
ij}
pk exp{s t
ik}
2 a + a> 

1

proposed model

experiments

conclusions

f (h, h0) = v>(h + h0) + b

(directed)

(undirected)

this is a simple starting point, not motivated by physics

96

jet physics

introduction
classi   cation results

previous work

proposed model

experiments

conclusions

1/fpr @ tpr = 50%

model
rec-nn (no gating)
rec-nn (gating)
mpnn (directed)
mpnn (directed)
mpnn (directed)
mpnn (identity)
relation network

iterations

1
1
1
2
3
3
1

r   =50%
70.4    3.6
83.3    3.1
89.4    3.5
98.3    4.3
85.9    8.5
74.5    5.2
67.7    6.8

signi   cant improvement on w vs. qcd tagging! 
this is with a learned adjacency matrix 

what did it learn? is that adjacency matrix useful? 
we are working mpnn with qcd-motivated adjacency matrix

97

