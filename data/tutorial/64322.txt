deep	learning	for	sentiment	analysis:	a	survey	

lei	zhang,	linkedin	corporation,	lzhang32@gmail.com	

shuai	wang,	university	of	illinois	at	chicago,	shuaiwanghk@gmail.com	

bing	liu,	university	of	illinois	at	chicago,	liub@uic.edu	

	

abstract	

deep	learning	has	emerged	as	a	powerful	machine	learning	technique	that	learns	multiple	layers	of	
representations	or	features	of	the	data	and	produces	state-of-the-art	prediction	results.	along	with	
the	success	of	deep	learning	in	many	other	application	domains,	deep	learning	is	also	popularly	used	
in	sentiment	analysis	in	recent	years.	this	paper	first	gives	an	overview	of	deep	learning	and	then	
provides	a	comprehensive	survey	of	its	current	applications	in	sentiment	analysis.		

introduction	

sentiment	analysis	or	opinion	mining	is	the	computational	study	of	people   s	opinions,	sentiments,	
emotions,	 appraisals,	 and	 attitudes	 towards	 entities	 such	 as	 products,	 services,	 organizations,	
individuals,	issues,	events,	topics,	and	their	attributes.1	the	inception	and	rapid	growth	of	the	field	
coincide	with	those	of	the	social	media	on	the	web,	for	example,	reviews,	forum	discussions,	blogs,	
micro-blogs,	 twitter,	 and	 social	 networks,	 because	 for	 the	 first	 time	 in	 human	 history,	 we	 have	 a	
huge	volume	of	opinionated	data	recorded	in	digital	forms.	since	early	2000,	sentiment	analysis	has	
grown	to	be	one	of	the	most	active	research	areas	in	natural	language	processing	(nlp).	it	is	also	
widely	 studied	 in	 data	 mining,	 web	 mining,	 text	 mining,	 and	 information	 retrieval.	 in	 fact,	 it	 has	
spread	 from	 computer	 science	 to	 management	 sciences	 and	 social	 sciences	 such	 as	 marketing,	
finance,	political	science,	communications,	health	science,	and	even	history,	due	to	its	importance	to	
business	 and	 society	 as	 a	 whole.	 this	 proliferation	 is	 due	 to	 the	 fact	 that	 opinions	 are	 central	 to	
almost	all	human	activities	and	are	key	influencers	of	our	behaviours.	our	beliefs	and	perceptions	of	
reality,	and	the	choices	we	make,	are,	to	a	considerable	degree,	conditioned	upon	how	others	see	
and	evaluate	the	world.	for	this	reason,	whenever	we	need	to	make	a	decision	we	often	seek	out	
the	opinions	of	others.	this	is	not	only	true	for	individuals	but	also	true	for	organizations.		
nowadays,	if	one	wants	to	buy	a	consumer	product,	one	is	no	longer	limited	to	asking	one   s	friends	
and	family	for	opinions	because	there	are	many	user	reviews	and	discussions	about	the	product	in	
public	forums	on	the	web.	for	an	organization,	it	may	no	longer	be	necessary	to	conduct	surveys,	
opinion	polls,	and	focus	groups	in	order	to	gather	public	opinions	because	there	is	an	abundance	of	
such	information	publicly	available.	in	recent	years,	we	have	witnessed	that	opinionated	postings	in	
social	media	have	helped	reshape	businesses,	and	sway	public	sentiments	and	emotions,	which	have	
profoundly	impacted	on	our	social	and	political	systems.	such	postings	have	also	mobilized	masses	
for	political	changes	such	as	those	happened	in	some	arab	countries	in	2011.	it	has	thus	become	a	
necessity	to	collect	and	study	opinions1.	
however,	finding	and	monitoring	opinion	sites	on	the	web	and	distilling	the	information	contained	
in	them	remains	a	formidable	task	because	of	the	proliferation	of	diverse	sites.	each	site	typically	
contains	a	huge	volume	of	opinion	text	that	is	not	always	easily	deciphered	in	long	blogs	and	forum	
postings.	the	average	human	reader	will	have	difficulty	identifying	relevant	sites	and	extracting	and	
summarizing	the	opinions	in	them.	automated	sentiment	analysis	systems	are	thus	needed.	because	
of	 this,	 there	 are	 many	 start-ups	 focusing	 on	 providing	 sentiment	 analysis	 services.	 many	 big	
corporations	 have	 also	 built	 their	 own	 in-house	 capabilities.	 these	 practical	 applications	 and	
industrial	interests	have	provided	strong	motivations	for	research	in	sentiment	analysis.	

existing	research	has	produced	numerous	techniques	for	various	tasks	of	sentiment	analysis,	which	
include	both	supervised	and	unsupervised	methods.	in	the	supervised	setting,	early	papers	used	all	
types	of	supervised	machine	learning	methods	(such	as	support	vector	machines	(id166),	maximum	
id178,	 na  ve	 bayes,	 etc.)	 and	 feature	 combinations.	 unsupervised	 methods	 include	 various	
methods	 that	 exploit	 sentiment	 lexicons,	 grammatical	 analysis,	 and	 syntactic	 patterns.	 several	
survey	books	and	papers	have	been	published,	which	cover	those	early	methods	and	applications	
extensively.1,2,3		
since	about	a	decade	ago,	deep	learning	has	emerged	as	a	powerful	machine	learning	technique4	
and	 produced	 state-of-the-art	 results	 in	 many	 application	 domains,	 ranging	 from	 computer	 vision	
and	speech	recognition	to	nlp.	applying	deep	learning	to	sentiment	analysis	has	also	become	very	
popular	 recently.	 this	 paper	 first	 gives	 an	 overview	 of	 deep	 learning	 and	 then	 provides	 a	
comprehensive	survey	of	the	sentiment	analysis	research	based	on	deep	learning.	

neural	networks		

deep	learning	is	the	application	of	artificial	neural	networks	(neural	networks	for	short)	to	learning	
tasks	using	networks	of	multiple	layers.	it	can	exploit	much	more	learning	(representation)	power	of	
neural	networks,	which	once	were	deemed	to	be	practical	only	with	one	or	two	layers	and	a	small	
amount	of	data.		

inspired	 by	 the	 structure	 of	 the	 biological	 brain,	 neural	 networks	 consist	 of	 a	 large	 number	 of	
information	processing	units	(called	neurons)	organized	in	layers,	which	work	in	unison.	it	can	learn	
to	 perform	 tasks	 (e.g.,	 classification)	 by	 adjusting	 the	 connection	 weights	 between	 neurons,	
resembling	the	learning	process	of	a	biological	brain.	

figure	1:		feedforward	neural	network	

	

based	on	network	topologies,	neural	networks	can	generally	be	categorized	into	feedforward	neural	
networks	and	recurrent/recursive	neural	networks,	which	can	also	be	mixed	and	matched.	we	will	
describe	 recurrent/recursive	 neural	 networks	 later.	 a	 simple	 example	 of	 a	 feedforward	 neural	

network	is	given	in	figure	1,	which	consists	of	three	layers	    !,	    !	and	    !.	    !	is	the	input	layer,	which	
corresponds	 to	 the	 input	 vector	(    !,    !,    !)	and	 intercept	 term	 +1.	    !	is	 the	 output	 layer,	 which	
corresponds	 to	 the	 output	 vector	(    !).	    !	is	 the	 hidden	 layer,	 whose	 output	 is	 not	 visible	 as	 a	
network	output.	a	circle	in	    !	represents	an	element	in	the	input	vector,	while	a	circle	in	    !	or	    !	

represents	 a	 neuron,	 the	 basic	 computation	 element	 of	 a	 neural	 network.	 we	 also	 call	 it	 an	
activation	function.	a	line	between	two	neurons	represents	a	connection	for	the	flow	of	information.	
each	connection	is	associated	with	a	weight,	a	value	controlling	the	signal	between	two	neurons.	
the	 learning	 of	 a	 neural	 network	 is	 achieved	 by	 adjusting	 the	 weights	 between	 neurons	 with	 the	

information	flowing	through	them.	neurons	read	output	from	neurons	in	the	previous	layer,	process	
the	information,	and	then	generate	output	to	neurons	in	the	next	layer.	as	in	figure	1,	the	neutral	

network	 alters	 weights	 based	 on	 training	 examples	(    (!),    (!)).	 after	 the	 training	 process,	 it	 will	
obtain	a	complex	form	of	hypotheses	   !,!(    )	that	fits	the	data.		
diving	into	the	hidden	layer,	we	can	see	that	each	neuron	in	    !	takes	input	    !,	    !,	    !	and	intercept	
+1	 from	    !,	 and	 outputs	 a	 value	    (    !    )=    (
	by	 the	 activation	 function	    .	    !	are	
    !    !+    )
!!!!
weights	of	the	connections;	    	is	the	intercept	or	bias;	    	is	normally	non-linear.	the	common	choices	
of	    	are	 sigmoid	 function,	 hyperbolic	 tangent	 function	 (tanh),	 or	 rectified	 linear	 function	 (relu).	
!!!"#!!!! 																								(1) 
																																																	        !     =                                !     = 
!
!!!!!!!!!!                                  (2)                                           
                                                          !     =tanh    !     = !!!!!!!!!!
																																																        !     =                    !     =max0,    !     																													(3)                                           

their	equations	are	as	follows.	

the	sigmoid	function	takes	a	real-valued	number	and	squashes	it	to	a	value	in	the	range	between	0	
and	1.	the	function	has	been	in	frequent	use	historically	due	to	its	nice	interpretation	as	the	firing	
rate	 of	 a	 neuron:	 0	 for	 not	 firing	 or	 1	 for	 firing.	 but	 the	 non-linearity	 of	 the	 sigmoid	 has	 recently	
fallen	out	of	favour	because	its	activations	can	easily	saturate	at	either	tail	of	0	or	1,	where	gradients	
are	almost	zero	and	the	information	flow	would	be	cut.	what	is	more	is	that	its	output	is	not	zero-
centered,	which	 could	 introduce	undesirable	zig-zagging	 dynamics	 in	 the	 gradient	 updates	 for	 the	
connection	 weights	 in	 training.	 thus,	 the	 tanh	 function	 is	 often	 more	 preferred	 in	 practice	 as	 its	
output	range	is	zero-centered,	[-1,	1]	instead	of	[0,	1].	the	relu	function	has	also	become	popular	
lately.	its	activation	is	simply	thresholded	at	zero	when	the	input	is	less	than	0.	compared	with	the	
sigmoid	function	and	the	tanh	function,	relu	is	easy	to	compute,	fast	to	converge	in	training	and	
yields	equal	or	better	performance	in	neural	networks.5	

in	    !,	we	can	use	the	softmax	function	as	the	output	neuron,	which	is	a	generalization	of	the	logistic	
function	that	squashes	a	k-dimensional	vector	    	of	arbitrary	real	values	to	a	k-dimensional	vector	
    (    )	of	real	values	in	the	range	(0,	1)	that	add	up	to	1.	the	function	definition	is	as	follows.		
																																																	         != !!!!!!
!!!!
by	 connecting	 together	 all	 neurons,	 the	 neural	 network	 in	 figure	 1	 has	 parameters	(    ,    ) =
 (    !,    !,    !,    (!)),	 where	    !"(!)
neuron	    	in	layer	    ,	and	neuron	     in	layer	    +1.	    !(!)
	is	the	bias	associated	with	neuron	     in	layer     +1.		

generally,	softmax	is	used	in	the	final	layer	of	neural	networks	for	final	classification	in	feedforward	
neural	networks.		

	denotes	 the	 weight	 associated	 with	 the	 connection	 between	

                              =1,   ,    																														(4)                                           

to	train	a	neural	network,	stochastic	gradient	descent	via	id266	is	usually	employed	to	
minimize	the	cross-id178	loss,	which	is	a	loss	function	for	softmax	output.	gradients	of	the	loss	
function	with	respect	to	weights	from	the	last	hidden	layer	to	the	output	layer	are	first	calculated,	
and	 then	 gradients	 of	 the	 expressions	 with	 respect	 to	 weights	 between	 upper	 network	 layers	 are	
calculated	recursively	by	applying	the	chain	rule	in	a	backward	manner.	with	those	gradients,	the	
weights	between	layers	are	adjusted	accordingly.	it	is	an	iterative	refinement	process	until	certain	
stopping	criteria	are	met.	the	pseudo	code	for	training	the	neural	network	in	figure	1	is	as	follows.	

											do																	

										training	algorithm:	stochastic	gradient	descent	via	id26	

									initialize	weights	    	and	biases	    	of	the	neural	network	    	with	random	values	
															for	each	training	example	(    ! ,    !)			
																					    ! 	=	neural-network-prediction	(    ,	    !	)							
																					calculate	gradients	of	loss	function	(     ! ,    ! )		with	respect	to	    !		at		layer		    ! 			
																					get	       !	for	all	weights	from	hidden	layer	    ! to	output	layer	    !				
																					calculate	gradient	with	respect	to	    !	by	chain	rule	at	layer	    !	
get	       !	for	all	weights	from	input	layer	    ! 	to	hidden	layer	    !								
																					update	(     !,    ! )			

												until	all	training	examples	are	classified	correctly	or	other	stopping	criteria	are	met	
												return	the	trained	neural	network																														

table	1:	training	the	neural	network	in	figure	1.	

the	above	algorithm	can	be	extended	to	generic	feedforward	neural	network	training	with	multiple	
hidden	 layers.	 note	 that	 stochastic	 gradient	 descent	 estimates	 the	 parameters	 for	 every	 training	
example	as	opposed	to	the	whole	set	of	training	examples	in	batch	gradient	descent.	therefore,	the	
parameter	 updates	 have	 a	 high	 variance	 and	 cause	 the	 loss	 function	 to	 fluctuate	 to	 different	
intensities,	which	helps	discover	new	and	possibly	better	local	minima.		

deep	learning		

the	research	community	lost	interests	in	neural	networks	in	late	1990s	mainly	because	they	were	
regarded	as	only	practical	for	   shallow   	neural	networks	(neural	networks	with	one	or	two	layers)	as	
training	 a	    deep   	 neural	 network	 (neural	 networks	 with	 more	 layers)	 is	 complicated	 and	
computationally	very	expensive.	however,	in	the	past	10	years,	deep	learning	made	breakthrough	
and	produced	state-of-the-art	results	in	many	application	domains,	starting	from	computer	vision,	
then	 speech	 recognition,	 and	 more	 recently,	 nlp.7,8	 the	 renaissance	 of	 neural	 networks	 can	 be	
attributed	 to	 many	 factors.	 most	 important	 ones	 include:	 (1)	 the	 availability	 of	 computing	 power	
due	to	the	advances	in	hardware	(e.g.,	gpus),	(2)	the	availability	of	huge	amounts	of	training	data,	
and	(3)	the	power	and	flexibility	of	learning	intermediate	representations.9		

in	 a	 nutshell,	 deep	 learning	 uses	 a	 cascade	 of	 multiple	 layers	 of	 nonlinear	 processing	 units	 for	
feature	extraction	and	transformation.	the	lower	layers	close	to	the	data	input	learn	simple	features,	
while	higher	layers	learn	more	complex	features	derived	from	lower	layer	features.	the	architecture	
forms	a	hierarchical	and	powerful	feature	representation.	figure	2	shows	the	feature	hierarchy	from	
the	 left	 (a	 lower	 layer)	 to	 the	 right	 (a	 higher	 layer)	 learned	 by	 deep	 learning	 in	 face	 image	
classification.10	 we	 can	 see	 that	 the	 learned	 image	 features	 grow	 in	 complexity,	 starting	 from	
blobs/edges,	then	noses/eyes/cheeks,	to	faces.	

																																																			figure	2:		feature	hierarchy	by	deep	learning	

	

in	recent	years,	deep	learning	models	have	been	extensively	applied	in	the	field	of	nlp	and	show	
great	 potentials.	 in	 the	 following	 several	 sections,	 we	 briefly	 describe	 the	 main	 deep	 learning	
architectures	and	related	techniques	that	have	been	applied	to	nlp	tasks.	

word	embedding		

many	 deep	 learning	 models	 in	 nlp	 need	 word	 embedding	 results	 as	 input	 features.7	 word	
embedding	is	a	technique	for	language	modelling	and	feature	learning,	which	transforms	words	in	a	
(e.g.,	
vocabulary	

                 "           "   (   ,0.15,   ,0.23,   ,0.41,   ) ).	 the	 technique	 normally	 involves	 a	 mathematic	

continuous	

numbers	

vectors	

real	

to	

of	

embedding	 from	 a	 high-dimensional	 sparse	 vector	 space	 (e.g.,	 one-hot	 encoding	 vector	 space,	 in	
which	each	word	takes	a	dimension)	to	a	lower-dimensional	dense	vector	space.	each	dimension	of	
the	 embedding	 vector	 represents	 a	 latent	 feature	 of	 a	 word.	 the	 vectors	 may	 encode	 linguistic	
regularities	and	patterns.		

the	learning	of	word	embeddings	can	be	done	using	neural	networks11-15	or	matrix	factorization.16,17	
one	commonly	used	word	embedding	system	is	id97i,	which	is	essentially	a	computationally-
efficient	 neural	 network	 prediction	 model	 that	 learns	 word	 embeddings	 from	 text.	 it	 contains	
continuous	bag-of-words	model	(cbow)13,	and	skip-gram	model	(sg)14.	the	cbow	model	predicts	
the	target	word	(e.g.,	   wearing   )	from	its	context	words	(   the	boy	is	_	a	hat   ,	where	   _   	denotes	the	
target	word),	while	the	sg	model	does	the	inverse,	predicting	the	context	words	given	the	target	
word.	statistically,	the	cbow	model	smoothens	over	a	great	deal	of	distributional	information	by	
treating	the	entire	context	as	one	observation.	it	is	effective	for	smaller	datasets.	however,	the	sg	
model	 treats	 each	 context-target	 pair	 as	 a	 new	 observation	 and	 is	 better	 for	 larger	 datasets.		
another	frequently	used	learning	approach	is	global	vectorii	(glove)17,	which	is	trained	on	the	non-
zero	entries	of	a	global	word-word	co-occurrence	matrix.	

autoencoder	and	denoising	autoencoder		

autoencoder	 neural	 network	 is	 a	 three-layer	 neural	 network,	 which	 sets	 the	 target	 values	 to	 be	
equal	to	the	input	values.	figure	3	shows	an	example	of	an	autoencoder	architecture.		

figure	3:		autoencoder	neural	network	

	

																																																													
i	source	code:	https://code.google.com/archive/p/id97/	
ii	source	code:	https://github.com/stanfordnlp/glove	

given	 the	 input	 vector	        [0,1]! ,	 the	 autoencoder	 first	 maps	 it	 to	 a	 hidden	 representation	
	by	 an	 encoder	 function	   (   )	(e.g.,	 the	 sigmoid	 function).	 the	 latent	 representation	    	is	
       [0,1]!!
then	 mapped	 back	 by	 a	 decoder	 function	    (   ) 	into	 a	 reconstruction	         =    (       ) .	 the	
autoencoder	 is	 typically	 trained	 to	 minimize	 a	 form	 of	 reconstruction	 error	                (    ,        ).	 the	
hidden	 layer.	 due	 to	 the	 nonlinear	 function	   (   )	and	    (   ),	 the	 autoencoder	 is	 able	 to	 learn	 non-

objective	of	the	autoencoder	is	to	learn	a	representation	of	the	input,	which	is	the	activation	of	the	

linear	representations,	which	give	it	much	more	expressive	power	than	its	linear	counterparts,	such	
as	principal	component	analysis	(pca)	or	latent	semantic	analysis	(lsa).	

one	often	stacks	autoencoders	into	layers.	a	higher	level	autoencoder	uses	the	output	of	the	lower	
one	 as	 its	 training	 data.	 the	 stacked	 autoencoders18	 along	 with	 restricted	 boltzmann	 machines	
(rbms)19	are	earliest	approaches	to	building	deep	neural	networks.	once	a	stack	of	autoencoders	
has	 been	 trained	 in	 an	 unsupervised	 fashion,	 their	 parameters	 describing	 multiple	 levels	 of	

representations	 for	    	(intermediate	 representations)	 can	 be	 used	 to	 initialize	 a	 supervised	 deep	
the	denoising	 autoencoder	(dae)20	is	an	extension	of	autoencoder,	in	which	the	input	vector	    	is	
stochastically	corrupted	into	a	vector	    .	and	the	model	is	trained	to	denoise	it,	that	is,	to	minimize	a	
denoising	 reconstruction	 error	                (    ,        ).	 the	 idea	 behind	 dae	 is	 to	 force	 the	 hidden	 layer	 to	

neural	network,	which	has	been	shown	empirically	better	than	random	parameter	initialization.		

discover	 more	 robust	 features	 and	 prevent	 it	 from	 simply	 learning	 the	 identity.	 a	 robust	 model	
should	be	able	to	reconstruct	the	input	well	even	in	the	presence	of	noises.	for	example,	deleting	or	
adding	a	few	of	words	from	or	to	a	document	should	not	change	the	semantic	of	the	document.		

convolutional	neural	network			

convolutional	 neural	 network	 (id98)	 is	 a	 special	 type	 of	 feedforward	 neural	 network	 originally	
employed	in	the	field	of	computer	vision.	its	design	is	inspired	by	the	human	visual	cortex,	a	visual	
mechanism	in	animal	brain.	the	visual	cortex	contains	a	lot	of	cells	that	are	responsible	for	detecting	
light	in	small	and	overlapping	sub-regions	of	the	visual	fields,	which	are	called	receptive	fields.	these	
cells	act	as	local	filters	over	the	input	space.	id98	consists	of	multiple	convolutional	layers,	each	of	
which	performs	the	function	that	is	processed	by	the	cells	in	the	visual	cortex.		

figure	 4	 shows	 a	 id98	 for	 recognizing	 traffic	 signs.21	 the	 input	 is	 a	 32x32x1	 pixel	 image	 (32	 x	 32	
represents	image	width	x	height;	1	represents	input	channel).	in	this	first	stage,	the	filter	(size	5x5x1)	
is	used	to	scan	the	image.	each	region	in	the	input	image	that	the	filter	projects	on	is	a	receptive	
field.	the	filter	is	actually	an	array	of	numbers	(called	weights	or	parameters).	as	the	filter	is	sliding	
(or	convolving),	it	is	multiplying	its	weight	values	with	the	original	pixel	values	of	the	image	(element	
wise	 multiplications).	 the	 multiplications	 are	 all	 summed	 up	 to	 a	 single	 number,	 which	 is	 a	
representative	 of	 the	 receptive	 field.	 every	 receptive	 field	 produces	 a	 number.	 after	 the	 filter	
finishes	scanning	over	the	image,	we	can	get	an	array	(size	28x28x1),	which	is	called	the	activation	
map	or	feature	map.	in	id98,	we	need	to	use	different	filters	to	scan	the	input.	in	figure	4,	we	apply	
108	kinds	of	filters	and	thus	have	108	stacked	feature	maps	in	the	first	stage,	which	consists	of	the	
first	 convolutional	 layer.	 following	 the	 convolutional	 layer,	 a	 subsampling	 (or	 pooling)	 layer	 is	
usually	 used	 to	 progressively	 reduce	 the	 spatial	 size	 of	 the	 representation,	 thus	 to	 reduce	 the	
number	 of	 features	 and	 the	 computational	 complexity	 of	 the	 network.	 for	 example,	 after	
subsampling	in	the	first	stage,	the	convolutional	layer	reduces	its	dimensions	to	(14x14x108).	note	
that	while	the	dimensionality	of	each	feature	map	is	reduced,	the	subsampling	step	retains	the	most	
important	 information,	 with	 a	 commonly	 used	 subsampling	 operation	 being	 the	 max	 pooling.	
afterwards,	the	output	from	the	first	stage	becomes	input	to	the	second	stage	and	the	new	filters	
are	employed.	the	new	filter	size	is	5x5x108,	where	108	is	the	feature	map	size	of	the	last	layer.	
after	 the	 second	 stage,	 id98	 uses	 a	 fully	 connected	 layer	 and	 then	 a	 softmax	 readout	 layer	 with	
output	classes	for	classification.		

convolutional	layers	in	id98	play	the	role	of	feature	extractor,	which	extracts	local	features	as	they	
restrict	the	receptive	fields	of	the	hidden	layers	to	be	local.	it	means	that	id98	has	a	special	spatially-
local	correlation	by	enforcing	a	local	connectivity	pattern	between	neurons	of	adjacent	layers.	such	
a	 characteristic	 is	 useful	 for	 classification	 in	 nlp,	 in	 which	 we	 expect	 to	 find	 strong	 local	 clues	
regarding	class	membership,	but	these	clues	can	appear	in	different	places	in	the	input.	for	example,	
in	 a	 document	 classification	 task,	 a	 single	 key	 phrase	 (or	 an	 id165)	 can	 help	 in	 determining	 the	
topic	of	the	document.	we	would	like	to	learn	that	certain	sequences	of	words	are	good	indicators	
of	 the	 topic,	 and	 do	 not	 necessarily	 care	 where	 they	 appear	 in	 the	 document.	 convolutional	 and	
pooling	layers	allow	the	id98	to	learn	to	find	such	local	indicators,	regardless	of	their	positions.8		
	

figure	4:		convolutional	neural	network	

	

recurrent	neural	network	

recurrent	 neural	 network	 (id56)22	 is	 a	 class	 of	 neural	 networks	 whose	 connections	 between	
neurons	 form	 a	 directed	 cycle.	 unlike	 feedforward	 neural	 networks,	 id56	 can	 use	 its	 internal	
   memory   	 to	 process	 a	 sequence	 of	 inputs,	 which	 makes	 it	 popular	 for	 processing	 sequential	
information.	 the	    memory   	 means	 that	 id56	 performs	 the	 same	 task	 for	 every	 element	 of	 a	
sequence	 with	 each	 output	 being	 dependent	 on	 all	 previous	 computations,	 which	
like	
   remembering   	information	about	what	has	been	processed	so	far.		

is	

figure	5:		recurrent	neural	network	

	

figure	5	shows	an	example	of	a	id56.	the	left	graph	is	an	unfolded	network	with	cycles,	while	the	
right	 graph	 is	 a	 folded	 sequence	 network	 with	 three	 time	 steps.	 the	 length	 of	 time	 steps	 is	
determined	by	the	length	of	input.	for	example,	if	the	word	sequence	to	be	processed	is	a	sentence	
of	six	words,	the	id56	would	be	unfolded	into	a	neural	network	with	six	time	steps	or	layers.	one	
layer	corresponds	to	a	word.		

calculated	based	on	the	previous	hidden	state	and	the	input	at	the	current	time	step.	

in	 figure	 5,	    !	is	 the	 input	 vector	 at	 time	 step	    .	   !	is	 the	 hidden	 state	 at	 time	 step	    ,	 which	 is	
																																																								   !=        !!   !!!+    !!    ! 																																													(5)	
in	equation	(5),	the	activation	function	    	is	usually	the	tanh	function	or	the	relu	function.	    !!	is	the	
weight	 matrix	 used	 to	 condition	 the	 input	    !.	    !!	is	 the	 weight	 matrix	 used	 to	 condition	 the	
previous	hidden	state	   !!!.	
    !	is	the	output	id203	distribution	over	the	vocabulary	at	step	t.	for	example,	if	we	want	to	
																																																								    !=                                !!   ! 																																																				(6)		
the	hidden	state	   !	is	regarded	as	the	memory	of	the	network.	it	captures	information	about	what	
happened	in	all	previous	time	steps.	    !	is	calculated	solely	based	on	the	memory	   !	at	time	    	and	the	
corresponding	weight	matrix	    !!.	
same	parameters	(    !!,    !!,    !!)	across	all	steps.	this	means	that	it	performs	the	same	task	at	

predict	the	next	word	in	a	sentence,	it	would	be	a	vector	of	probabilities	across	the	word	vocabulary.	

unlike	a	feedforward	neural	network,	which	uses	different	parameters	at	each	layer,	id56	shares	the	

each	step,	just	with	different	inputs.	this	greatly	reduces	the	total	number	of	parameters	needed	to	
learn.		

theoretically,	id56	can	make	use	of	the	information	in	arbitrarily	long	sequences,	but	in	practice,	the	
standard	id56	is	limited	to	looking	back	only	a	few	steps	due	to	the	vanishing	gradient	or	exploding	
gradient	problem.23		

	

figure	6:		bidirectional	id56	(left)	and	deep	bidirectional	id56	(right)	

researchers	have	developed	more	sophisticated	types	of	id56	to	deal	with	the	shortcomings	of	the	
standard	 id56	 model:	 bidirectional	 id56,	 deep	 bidirectional	 id56	 and	 long	 short	 term	 memory	
network.	bidirectional	id56	is	based	on	the	idea	that	the	output	at	each	time	may	not	only	depend	
on	the	previous	elements	in	the	sequence,	but	also	depend	on	the	next	elements	in	the	sequence.	
for	instance,	to	predict	a	missing	word	in	a	sequence,	we	may	need	to	look	at	both	the	left	and	the	
right	context.	a	bidirectional	id5624	consists	of	two	id56s,	which	are	stacked	on	the	top	of	each	other.	
the	one	that	processes	the	input	in	its	original	order	and	the	one	that	processes	the	reversed	input	
sequence.	the	output	is	then	computed	based	on	the	hidden	state	of	both	id56s.	deep	bidirectional	
id56	is	similar	to	bidirectional	id56.	the	only	difference	is	that	it	has	multiple	layers	per	time	step,	

which	provides	higher	learning	capacity	but	needs	a	lot	of	training	data.	figure	6	shows	examples	of	
bidirectional	id56	and	deep	bidirectional	id56	(with	two	layers)	respectively.	

lstm	network	

long	 short	 term	 memory	network	(lstm)25	is	a	special	type	of	id56,	which	is	capable	of	learning	
long-term	dependencies.		

all	id56s	have	the	form	of	a	chain	of	repeating	modules.	in	standard	id56s,	this	repeating	module	
normally	 has	 a	 simple	 structure.	 however,	 the	 repeating	 module	 for	 lstm	 is	 more	 complicated.	
instead	of	having	a	single	neural	network	layer,	there	are	four	layers	interacting	in	a	special	way.	
besides,	it	has	two	states:	hidden	state	and	cell	state.										

	

																																																																					(7)	

figure	7:		long	short	term	memory	network	

number	in	[0,	1],	where	1	means	   completely	keep   	and	0	means	   completely	dump   	in	equation	(7).		

then	 lstm	 decides	 what	 new	 information	 to	 store	 in	 the	 cell	 state.	 this	 has	 two	 steps.	 first,	 a	
sigmoid	 function/layer,	 called	 the	    input	 gate   	 as	 equation	 (8),	 decides	 which	 values	 lstm	 will	

figure	7	shows	an	example	of	lstm.	at	time	step	    ,	lstm	first	decides	what	information	to	dump	
from	the	cell	state.	this	decision	is	made	by	a	sigmoid	function/layer	    ,	called	the	   forget	gate   .		the	
function	 takes	   !!!	(output	 from	 the	 previous	 hidden	 layer)	 and	    !	(current	 input),	 and	 outputs	 a	
																																			    !=        !    !+    !   !!! 
update.	 next,	 a	 tanh	 function/layer	 creates	 a	 vector	 of	 new	 candidate	 values	    !	,	 which	 will	 be	
																																			    ! =          !    !+    !   !!! 
																																		    ! =tanh    !    !+    !   !!! 																																																																(9)	
it	 is	 now	 time	 to	 update	 the	 old	 cell	 state	    !!!	into	 new	 cell	 state	    !	as	 equation	 (10).	 note	 that	
forget	gate	    !	can	control	the	gradient	passes	through	it	and	allow	for	explicit	   memory   	deletes	and	
																																			    ! =     !         !!! +     !         !  																																																															(10)	

updates,	which	helps	alleviate	vanishing	gradient	or	exploding	gradient	problem	in	standard	id56.			

added	to	the	cell	state.	lstm	combines	these	two	to	create	an	update	to	the	state.	

																																																																				(8)	

finally,	lstm	decides	the	output,	which	is	based	on	the	cell	state.		lstm	first	runs	a	sigmoid	layer,	
which	decides	which	parts	of	the	cell	state	to	output	in	equation	(11),	called	   output	gate   .	then,	
lstm	puts	the	cell	state	through	the	tanh	function	and	multiplies	it	by	the	output	of	the	sigmoid	
gate,	so	that	lstm	only	outputs	the	parts	it	decides	to	as	equation	(12).		

                                       ! =         !    !+    !   !!!                                                                     (11) 
                                      ! =     !    tanh    !                                                                                (12)	

lstm	is	commonly	applied	to	sequential	data	but	can	also	be	used	for	tree-structured	data.	tai	et	
al.26	 introduced	 a	 generalization	 of	 the	 standard	 lstm	 to	 tree-structured	 lstm	 (tree-lstm)	 and	
showed	better	performances	for	representing	sentence	meaning	than	a	sequential	lstm.		

a	slight	variation	of	lstm	is	the	gated	recurrent	unit	(gru).27,28	it	combines	the	   forget   	and	   input   	
gates	into	a	single	update	gate.	it	also	merges	the	cell	state	and	hidden	state,	and	makes	some	other	
changes.	 the	 resulting	 model	 is	 simpler	 than	 the	 standard	 lstm	 model,	 and	 has	 been	 growing	 in	
popularity.	

attention	mechanism	with	recurrent	neural	network	

supposedly,	 bidirectional	 id56	 and	 lstm	 should	 be	 able	 to	 deal	 with	 long-range	 dependencies	 in	
data.	but	in	practice,	the	long-range	dependencies	are	still	problematic	to	handle.	thus,	a	technique	
called	the	attention	mechanism	was	proposed.			

the	attention	mechanism	in	neural	networks	is	inspired	by	the	visual	attention	mechanism	found	in	
humans.	that	is,	the	human	visual	attention	is	able	to	focus	on	a	certain	region	of	an	image	with	
   high	resolution   	while	perceiving	the	surrounding	image	in	   low	resolution   	and	then	adjusting	the	
focal	point	over	time.	in	nlp,	the	attention	mechanism	allows	the	model	to	learn	what	to	attend	to	
based	on	the	input	text	and	what	it	has	produced	so	far,	rather	than	encoding	the	full	source	text	
into	a	fixed-length	vector	like	standard	id56	and	lstm.	

	

figure	8:		attention	mechanism	in	bidirectional	recurrent	neural	network	

bahdanau	 et	 al.29	 first	 utilized	 the	 attention	 mechanism	 for	 machine	 translation	 in	 nlp.	 they	
proposed	an	encoder-decoder	framework	where	an	attention	mechanism	is	used	to	select	reference	
words	 in	 the	 original	 language	 for	 words	 in	 the	 target	 language	 before	 translation.	 figure	 8	
illustrates	the	use	of	the	attention	mechanism	in	their	bidirectional	id56.	note	that	each	decoder	

output	word	    !	depends	on	a	weighted	combination	of	all	the	input	states,	not	just	the	last	state	as	
in	the	normal	case.	    !,!		are	weights	that	define	in	how	much	of	each	input	state	should	be	weighted	
for	 each	 output.	 for	 example,	 if	    !,!	has	 a	 big	 value,	 it	 means	 that	 the	 decoder	 pays	 a	 lot	 of	
sentence.	the	weights	of	    !,! sum	to	1	normally.		

attention	to	the	second	state	in	the	source	sentence	while	producing	the	second	word	of	the	target	

memory	network	

weston	et	al.30	introduced	the	concept	of	memory	networks	(memnn)	for	the	question	answering	
problem.	it	works	with	several	id136	components	combined	with	a	large	long-term	memory.	the	
components	 can	 be	 neural	 networks.	 the	 memory	 acts	 as	 a	 dynamic	 knowledge	 base.	 the	 four	
learnable/id136	components	function	as	follows:	i	component	coverts	the	incoming	input	to	the	
internal	 feature	 representation;	 g	 component	 updates	 old	 memories	 given	 the	 new	 input;	 o	
component	generates	output	(also	in	the	feature	representation	space);	r	component	converts	the	
output	into	a	response	format.	for	instance,	given	a	list	of	sentences	and	a	question	for	question	
answering,	 memnn	 finds	 evidences	 from	 those	 sentences	 and	 generates	 an	 answer.	 during	
id136,	the	i	component	reads	one	sentence	at	a	time	and	encodes	it	into	a	vector	representation.	
then	the	g	component	updates	a	piece	of	memory	based	on	the	current	sentence	representation.	
after	all	sentences	are	processed,	a	memory	matrix	(each	row	representing	a	sentence)	is	generated,	
which	 stores	 the	 semantics	 of	 the	 sentences.	 for	 a	 question,	 memnn	 encodes	 it	 into	 a	 vector	
representation,	then	the	o	component	uses	the	vector	to	select	some	related	evidences	from	the	
memory	and	generates	an	output	vector.	finally,	the	r	component	takes	the	output	vector	as	the	
input	and	outputs	a	final	response.		

based	on	memnn,	sukhbaatar	et	al.31	proposed	an	end-to-end	memory	network	(memn2n),	which	
is	a	neural	network	architecture	with	a	recurrent	attention	mechanism	over	the	long-term	memory	
component	 and	 it	 can	 be	 trained	 in	 an	 end-to-end	 manner	 through	 standard	 id26.	 it	
demonstrates	 that	 multiple	 computational	 layers	 (hops)	 in	 the	 o	 component	 can	 uncover	 more	
abstractive	 evidences	 than	 a	 single	 layer	 and	 yield	 improved	 results	 for	 question	 answering	 and	
language	 modelling.	 it	 is	 worth	 noting	 that	 each	 computational	 layer	 can	 be	 a	 content-based	
attention	 model.	 thus,	 memn2n	 refines	 the	 attention	 mechanism	 to	 some	 extent.	 note	 also	 a	
similar	idea	is	the	neural	turing	machines	reported	by	graves	et	al.32	

recursive	neural	network	

recursive	 neural	 network	 (reid98)	 is	 a	 type	 of	 neural	 network	 that	 is	 usually	 used	 to	 learn	 a	
directed	acyclic	graph	structure	(e.g.,	tree	structure)	from	data.	a	recursive	neural	network	can	be	
seen	as	a	generalization	of	the	recurrent	neural	network.	given	the	structural	representation	of	a	
sentence	 (e.g.,	 a	 parse	 tree),	 reid98	 recursively	 generates	 parent	 representations	 in	 a	 bottom-up	
fashion,	by	combining	tokens	to	produce	representations	for	phrases,	eventually	the	whole	sentence.	
the	 sentence	 level	 representation	 can	 then	 be	 used	 to	 make	 a	 final	 classification	 (e.g.,	 sentiment	
classification)	 for	 a	 given	 input	 sentence.	 an	 example	 process	 of	 vector	 composition	 in	 reid98	 is	
shown	in	figure	9	33.	the	vector	of	node	   very	interesting   	is	composed	from	the	vectors	of	the	node	
   very   	 and	 the	 node	    interesting   .	 similarly,	 the	 node	    is	 very	 interesting   	 is	 composed	 from	 the	
phrase	node	   very	interesting   	and	the	word	node	   is   .	

figure	9:		recursive	neural	network	

	

sentiment	analysis	tasks	

we	are	now	ready	to	survey	deep	learning	applications	in	sentiment	analysis.	but	before	doing	that,	
we	 first	 briefly	 introduce	 the	 main	 sentiment	 analysis	 tasks	 in	 this	 section.	 for	 additional	 details,	
please	refer	to	liu   s	book1	on	sentiment	analysis.		

researchers	have	mainly	studied	sentiment	analysis	at	three	levels	of	granularity:	document	level,	
sentence	level,	and	aspect	level.	document	level	sentiment	classification	classifies	an	opinionated	
document	(e.g.,	a	product	review)	as	expressing	an	overall	positive	or	negative	opinion.	it	considers	
the	whole	document	as	the	basic	information	unit	and	assumes	that	the	document	is	known	to	be	
opinionated	 and	 contain	 opinions	 about	 a	 single	 entity	 (e.g.,	 a	 particular	 phone).	 sentence	 level	
sentiment	 classification	 classifies	 individual	 sentences	 in	 a	 document.	 however,	 each	 sentence	
cannot	 be	 assumed	 to	 be	 opinionated.	 traditionally,	 one	 often	 first	 classifies	 a	 sentence	 as	
opinionated	 or	 not	 opinionated,	 which	 is	 called	 subjectivity	 classification.	 then	 the	 resulting	
opinionated	 sentences	 are	 classified	 as	 expressing	 positive	 or	 negative	 opinions.	 sentence	 level	
sentiment	 classification	 can	 also	 be	 formulated	 as	 a	 three-class	 classification	 problem,	 that	 is,	 to	
classify	 a	 sentence	 as	 neutral,	 positive	 or	 negative.	 compared	 with	 document	 level	 and	 sentence	
level	sentiment	analysis,	aspect	level	sentiment	analysis	or	aspect-based	sentiment	analysis	is	more	
fine-grained.	 its	 task	 is	 to	 extract	 and	 summarize	 people   s	 opinions	 expressed	 on	 entities	 and	
aspects/features	of	entities,	which	are	also	called	targets.	for	example,	in	a	product	review,	it	aims	
to	 summarize	 positive	 and	 negative	 opinions	 on	 different	 aspects	 of	 the	 product	 respectively,	
although	 the	 general	 sentiment	 on	 the	 product	 could	 be	 positive	 or	 negative.	 the	 whole	 task	 of	
aspect-based	 sentiment	 analysis	 consists	 of	 several	 subtasks	 such	 as	 aspect	 extraction,	 entity	
extraction,	and	aspect	sentiment	classification.	for	example,	from	the	sentence,	   the	voice	quality	
of	iphone	is	great,	but	its	battery	sucks   ,	entity	extraction	should	identify	   iphone   	as	the	entity,	and	
aspect	 extraction	 should	 identify	 that	    voice	 quality   	 and	    battery   	 are	 two	 aspects.	 aspect	
sentiment	classification	should	classify	the	sentiment	expressed	on	the	voice	quality	of	the	iphone	as	
positive	and	on	the	battery	of	the	iphone	as	negative.	note	that	for	simplicity,	in	most	algorithms	
aspect	 extraction	 and	 entity	 extraction	 are	 combined	 and	 are	 called	 aspect	 extraction	 or	
sentiment/opinion	target	extraction.		

apart	from	these	core	tasks,	sentiment	analysis	also	studies	emotion	 analysis,	sarcasm	 detection,	
multilingual	sentiment	analysis,	etc.	see	liu   s	book1	for	more	details.	in	the	following	sections,	we	
survey	the	deep	learning	applications	in	all	these	sentiment	analysis	tasks.		

document	level	sentiment	classification	

sentiment	classification	at	the	document	level	is	to	assign	an	overall	sentiment	orientation/polarity	
to	an	opinion	document,	i.e.,	to	determine	whether	the	document	(e.g.,	a	full	online	review)	conveys	
an	overall	positive	or	negative	opinion.	in	this	setting,	it	is	a	binary	classification	task.	it	can	also	be	
formulated	as	a	regression	task,	for	example,	to	infer	an	overall	rating	score	from	1	to	5	stars	for	the	
review.	some	researchers	also	treat	this	as	a	5-class	classification	task.		

sentiment	classification	is	commonly	regarded	as	a	special	case	of	document	classification.	in	such	a	
classification,	 document	 representation	 plays	 an	 important	 role,	 which	 should	 reflect	 the	 original	
information	conveyed	by	words	or	sentences	in	a	document.	traditionally,	the	bag-of-words	model	
(bow)	 is	 used	 to	 generate	 text	 representations	 in	 nlp	 and	 text	 mining,	 by	 which	 a	 document	 is	
regarded	 as	 a	 bag	 of	 its	 words.	 based	 on	 bow,	 a	 document	 is	 transformed	 to	 a	 numeric	 feature	
vector	with	a	fixed	length,	each	element	of	which	can	be	the	word	occurrence	(absence	or	presence),	
word	 frequency,	 or	 tf-idf	 score.	 its	 dimension	 equals	 to	 the	 size	 of	 the	 vocabulary.	 a	 document	
vector	from	bow	is	normally	very	sparse	since	a	single	document	only	contains	a	small	number	of	
words	in	a	vocabulary.	early	neural	networks	adopted	such	feature	settings.		

despite	its	popularity,	bow	has	some	disadvantages.	firstly,	the	word	order	is	ignored,	which	means	
that	two	documents	can	have	exactly	the	same	representation	as	long	as	they	share	the	same	words.	
bag-of-id165s,	an	extension	for	bow,	can	consider	the	word	order	in	a	short	context	(id165),	but	
it	 also	 suffers	 from	 data	 sparsity	 and	 high	 dimensionality.	 secondly,	 bow	 can	 barely	 encode	 the	
semantics	 of	 words.	 for	 example,	 the	 words	    smart   ,	    clever   	 and	    book   	 are	 of	 equal	 distance	
between	them	in	bow,	but	   smart   	should	be	closer	to	   clever   	than	   book   	semantically.		

to	 tackle	 the	 shortcomings	 of	 bow,	 word	 embedding	 techniques	 based	 on	 neural	 networks	
(introduced	 in	 the	 aforementioned	 section)	 were	 proposed	 to	 generate	 dense	 vectors	 (or	 low-
dimensional	 vectors)	 for	 word	 representation,	 which	 are,	 to	 some	 extent,	 able	 to	 encode	 some	
semantic	 and	 syntactic	 properties	 of	 words.	 with	 word	 embeddings	 as	 input	 of	 words,	 document	
representation	 as	 a	 dense	 vector	 (or	 called	 dense	 document	 vector)	 can	 be	 derived	 using	 neural	
networks.	

notice	that	in	addition	to	the	above	two	approaches,	i.e.,	using	bow	and	learning	dense	vectors	for	
documents	through	 word	 embeddings,	one	can	also	learn	a	dense	document	vector	directly	from	
bow.	we	distinguish	the	different	approaches	used	in	related	studies	in	table	2.	

when	 documents	 are	 properly	 represented,	 sentiment	 classification	 can	 be	 conducted	 using	 a	
variety	of	neural	network	models	following	the	traditional	supervised	learning	setting.	in	some	cases,	
neural	networks	may	only	be	used	to	extract	text	features/text	representations,	and	these	features	
are	fed	into	some	other	non-neural	classifiers	(e.g.,	id166)	to	obtain	a	final	global	optimum	classifier.	
the	 properties	 of	 neural	 networks	 and	 id166	 complement	 each	 other	 in	 such	 a	 way	 that	 their	
advantages	are	combined.				

besides	sophisticated	document/text	representations,	researchers	also	leveraged	the	characteristics	
of	the	data	   	product	reviews,	for	sentiment	classification.	for	product	reviews,	several	researchers	
found	 it	 beneficial	 to	 jointly	 model	 sentiment	 and	 some	 additional	 information	 (e.g.,	 user	
information	 and	 product	 information)	 for	 classification.	 additionally,	 since	 a	 document	 often	
contains	long	dependency	relations,	the	attention	mechanism	is	also	frequently	used	in	document	
level	sentiment	classification.	we	summarize	the	existing	techniques	in	table	2.		

	

	

research	
work	

document/text	
representation	

neural	
model	

networks	

use	attention	
mechanism	

joint	 modelling	
with	sentiment	

moraes	et	al.34	

bow	

le	
mikolov35	

and	

learning	dense	vector	at	
sentence,	
paragraph,	
document	level		

(artificial	

ann	
network)	

paragraph	vector	

neural	

no	

no	

-	

-	

glorot	et	al.36	

bow	to	dense	document	
vector		

(stacked	 denoising	

sda	
autoencoder)	

no	

and	

and	

zhai	
zhang37	
johnson	
zhang38	
tang	et	al.39	

tang	et	al.40	

chen	et	al.41	

dou42	

xu	et	al.43	

yang	et	al.44	

yin	et	al.45	

zhou	et	al.46	

li	et	al.47	

bow	to	dense	document	
vector		

dae	
autoencoder)	

(denoising	

no	

bow	to	dense	document	
vector	

word	 embeddings	
to	
dense	document	vector		

bow-id98	and	seq-id98	

(to	

id98/lstm	
learn	
sentence	 representation)	 +	
gru	
learn	 document	
representation)	

(to	

word	 embeddings	
to	
dense	document	vector		

upnn	(user	product	neutral	
network)	based	on	id98	

to	
word	 embeddings	
dense	document	vector		

word	 embeddings	
dense	document	vector	

to	

upa	
attention)	based	on	lstm	

(user	

product	

memory	network	

word	 embeddings	
dense	document	vector	

to	

lstm	

no	

no	

no	

yes	

yes	

no	

word	 embeddings	
dense	document	vector	

to	

gru-based	
encoder	

sequence	

word	 embeddings	
dense	document	vector	

to	

input	encoder	and	lstm	

word	 embeddings	
dense	document	vector	

to	

lstm	

hierarchical	
attention	

hierarchical	
attention	

hierarchical	
attention	

word	 embeddings	
dense	document	vector	

to	

memory	network	

yes	

unsupervised	
data	
representation	 from	
target	 domains	
(in	
transfer	
learning	
settings)	

-	

-	

-	

user	information	and	
product	information	

user	information	and	
product	information	

user	information	and	
product	information	

-	

-	

aspect/target	
information	

cross-lingual	
information	

cross-domain	
information	

table	2:	deep	learning	methods	for	document	level	sentiment	classification	

below,	we	also	give	a	brief	description	of	these	existing	representative	works.		

moraes	 et	 al.34	 made	 an	 empirical	 comparison	 between	 support	 vector	 machines	 (id166)	 and	
artificial	 neural	 networks	 (ann)	 for	 document	 level	 sentiment	 classification,	 which	demonstrated	
that	ann	produced	competitive	results	to	id166   s	in	most	cases.	

to	overcome	the	weakness	of	bow,	le	and	mikolov35	proposed	paragraph	vector,	an	unsupervised	
learning	 algorithm	 that	 learns	 vector	 representations	 for	 variable-length	 texts	 such	 as	 sentences,	
paragraphs	and	documents.	the	vector	representations	are	learned	by	predicting	the	surrounding	
words	in	contexts	sampled	from	the	paragraph.	

glorot	 et	 al.36	 studied	 domain	 adaptation	 problem	 for	 sentiment	 classification.	 they	 proposed	 a	
deep	learning	system	based	on	stacked	denoising	autoencoder	with	sparse	rectifier	units,	which	can	
perform	an	unsupervised	text	feature/representation	extraction	 using	both	labeled	and	unlabeled	
data.	the	features	are	highly	beneficial	for	domain	adaption	of	sentiment	classifiers.	

zhai	and	zhang37	introduced	a	semi-supervised	autoencoder,	which	further	considers	the	sentiment	
information	 in	 its	 learning	 stage	 in	 order	 to	 obtain	 better	 document	 vectors,	 for	 sentiment	
classification.	more	specifically,	the	model	learns	a	task-specific	representation	of	the	textual	data	
by	 relaxing	 the	 loss	 function	 in	 the	 autoencoder	 to	 the	 bregman	 divergence	 and	 also	 deriving	 a	
discriminative	loss	function	from	the	label	information.	

johnson	 and	 zhang38	 proposed	 a	 id98	 variant	 named	 bow-id98	 that	 employs	 bag-of-word	
conversion	in	the	convolution	layer.	they	also	designed	a	new	model,	called	seq-id98,	which	keeps	
the	sequential	information	of	words	by	concatenating	the	one-hot	vector	of	multiple	words.	

tang	et	al.39	proposed	a	neural	network	to	learn	document	representation,	with	the	consideration	of	
sentence	 relationships.	 it	 first	 learns	 the	 sentence	 representation	 with	 id98	 or	 lstm	 from	 word	
embeddings.	then	a	gru	is	utilized	to	adaptively	encode	semantics	of	sentences	and	their	inherent	
relations	in	document	representations	for	sentiment	classification.		

tang	et	al.40	applied	user	representations	and	product	representations	in	review	classification.	the	
idea	is	that	those	representations	can	capture	important	global	clues	such	as	individual	preferences	
of	users	and	overall	qualities	of	products,	which	can	provide	better	text	representations.		

chen	et	al.41	also	incorporated	 user	information	and	product	information	for	classification	but	via	
word	and	sentence	level	attentions,	which	can	take	into	account	of	the	global	user	preference	and	
product	characteristics	at	both	the	word	level	and	the	semantic	level.	likewise,	dou42	used	a	deep	
memory	network	to	capture	user	and	product	information.	the	proposed	model	can	be	divided	into	
two	 separate	 parts.	 in	 the	 first	 part,	 lstm	 is	 applied	 to	 learn	 a	 document	 representation.	 in	 the	
second	part,	a	deep	memory	network	consisting	of	multiple	computational	layers	(hops)	is	used	to	
predict	the	review	rating	for	each	document.		

xu	et	al.43	proposed	a	cached	lstm	model	to	capture	the	overall	semantic	information	in	a	long	text.	
the	memory	in	the	model	is	divided	into	several	groups	with	different	forgetting	rates.	the	intuition	
is	to	enable	the	memory	groups	with	low	forgetting	rates	to	capture	global	semantic	features	and	
the	ones	with	high	forgetting	rates	to	learn	local	semantic	features.	

yang	 et	 al.44	 proposed	 a	 hierarchical	 attention	 network	 for	 document	 level	 sentiment	 rating	
prediction	of	reviews.	the	model	includes	two	levels	of	attention	mechanisms:	one	at	the	word	level	
and	 the	 other	 at	 the	 sentence	 level,	 which	 allow	 the	 model	 to	 pay	 more	 or	 less	 attention	 to	
individual	words	or	sentences	in	constructing	the	representation	of	a	document.		

yin	 et	 al.45	 formulated	 the	 document-level	 aspect-sentiment	 rating	 prediction	 task	 as	 a	 machine	
comprehension	problem	and	proposed	a	hierarchical	interactive	attention-based	model.	specifically,	
documents	 and	 pseudo	 aspect-questions	 are	
learn	 aspect-aware	 document	
representation.		

interleaved	 to	

zhou	et	al.46	designed	an	attention-based	lstm	network	for	cross-lingual	sentiment	classification	at	
the	document	level.	the	model	consists	of	two	attention-based	lstms	for	bilingual	representation,	
and	 each	 lstm	 is	 also	 hierarchically	 structured.	 in	 this	 setting,	 it	 effectively	 adapts	 the	 sentiment	
information	from	a	resource-rich	language	(english)	to	a	resource-poor	language	(chinese)	and	helps	
improve	the	sentiment	classification	performance.		

li	 et	 al.47	 proposed	 an	 adversarial	 memory	 network	 for	 cross-domain	 sentiment	 classification	 in	 a	
transfer	 learning	 setting,	 where	 the	 data	 from	 the	 source	 and	 the	 target	 domain	 are	 modelled	
together.	 it	 jointly	 trains	 two	 networks	 for	 sentiment	 classification	 and	 domain	 classification	 (i.e.,	
whether	a	document	is	from	the	source	or	target	domain).	

sentence	level	sentiment	classification	

sentence	 level	 sentiment	 classification	 is	 to	 determine	 the	 sentiment	 expressed	 in	 a	 single	 given	
sentence.	 as	 discussed	 earlier,	 the	 sentiment	 of	 a	 sentence	 can	 be	 inferred	 with	 subjectivity	
classification48	 and	 polarity	 classification,	 where	 the	 former	 classifies	 whether	 a	 sentence	 is	
subjective	or	objective	and	the	latter	decides	whether	a	subjective	sentence	expresses	a	negative	or	
positive	 sentiment.	 in	 existing	 deep	 learning	 models,	 sentence	 sentiment	 classification	 is	 usually	
formulated	 as	 a	 joint	 three-way	 classification	 problem,	 namely,	 to	 predict	 a	 sentence	 as	 positive,	
neural,	and	negative.		

same	 as	 document	 level	 sentiment	 classification,	 sentence	 representation	 produced	 by	 neural	
networks	is	also	important	for	sentence	level	sentiment	classification.	additionally,	since	a	sentence	
is	 usually	 short	 compared	 to	 a	 document,	 some	 syntactic	 and	 semantic	 information	 (e.g.,	 parse	
trees,	opinion	lexicons,	and	part-of-speech	tags)	may	be	used	to	help.	additional	information	such	as	
review	 ratings,	 social	 relationship,	 and	 cross-domain	 information	 can	 be	 considered	 too.	 for	
example,	 social	 relationships	 have	 been	 exploited	 in	 discovering	 sentiments	 in	 social	 media	 data	
such	as	tweets.	

in	early	research,	parse	trees	(which	provide	some	semantic	and	syntactic	information)	were	used	
together	with	the	original	words	as	the	input	to	neural	models,	so	that	the	sentiment	composition	
can	be	better	inferred.	but	lately,	id98	and	id56	become	more	popular,	and	they	do	not	need	parse	
trees	 to	 extract	 features	 from	 sentences.	 instead,	 id98	 and	 id56	 use	 word	 embeddings	 as	 input,	
which	already	encode	some	semantic	and	syntactic	information.	moreover,	the	model	architecture	
of	id98	or	id56	can	help	learn	intrinsic	relationships	between	words	in	a	sentence	too.	the	related	
works	are	introduced	in	detail	below.					

socher	 et	 al.49	 first	 proposed	 a	 semi-supervised	 recursive	 autoencoders	 network	 (rae)	 for	
sentence	level	sentiment	classification,	which	obtains	a	reduced	dimensional	vector	representation	
for	a	sentence.	later	on,	socher	et	al.50	proposed	a	matrix-vector	recursive	neural	network	(mv-
id56),	in	which	each	word	is	additionally	associated	with	a	matrix	representation	(besides	a	vector	
representation)	in	a	tree	structure.	the	tree	structure	is	obtained	from	an	external	parser.	in	socher	
et	al.51,	the	authors	further	introduced	the	recursive	neural	tensor	network	(rntn),	where	tensor-
based	compositional	functions	are	used	to	better	capture	the	interactions	between	elements.	qian	
et	 al.33	 proposed	 two	 more	 advanced	 models,	 tag-guided	 recursive	 neural	 network	 (tg-id56),	
which	 chooses	 a	 composition	 function	 according	 to	 the	 part-of-speech	 tags	 of	 a	 phrase,	 and	 tag-
embedded	 recursive	 neural	 network	/	recursive	 neural	 tenser	 network	(te-id56/rntn),	which	
learns	tag	embeddings	and	then	combines	tag	and	word	embeddings	together.		

kalchbrenner	et	al.52	proposed	a	dynamic	id98	(called	did98)	for	semantic	modelling	of	sentences.	
did98	uses	the	dynamic	k-max	pooling	operator	as	a	non-linear	subsampling	function.	the	feature	
graph	induced	by	the	network	is	able	to	capture	word	relations.	kim53	also	proposed	to	use	id98	for	
sentence-level	 sentiment	 classification	 and	 experimented	 with	 several	 variants,	 namely	 id98-rand	
(where	 word	 embeddings	 are	 randomly	 initialized),	 id98-static	 (where	 word	 embeddings	 are	 pre-
trained	 and	 fixed),	 id98-non-static	 (where	 word	 embeddings	 are	 pre-trained	 and	 fine-tuned)	 and	
id98-multichannel	(where	multiple	sets	of	word	embeddings	are	used).		

dos	santos	and	gatti54	proposed	a	character	to	sentence	id98	(charsid98)	model.	charsid98	uses	
two	 convolutional	 layers	 to	 extract	 relevant	 features	 from	 words	 and	 sentences	 of	 any	 size	 to	

perform	 sentiment	 analysis	 of	 short	 texts.	 wang	 et	 al.55	 utilized	 lstm	 for	 twitter	 sentiment	
classification	 by	 simulating	 the	
interactions	 of	 words	 during	 the	 compositional	 process.	
multiplicative	 operations	 between	 word	 embeddings	 through	 gate	 structures	 are	 used	 to	 provide	
more	flexibility	and	to	produce	better	compositional	results	compared	to	the	additive	ones	in	simple	
recurrent	neural	network.	similar	to	bidirectional	id56,	the	unidirectional	lstm	can	be	extended	to	a	
bidirectional	lstm56	by	allowing	bidirectional	connections	in	the	hidden	layer.		

wang	et	al.57	proposed	a	regional	id98-lstm	model,	which	consists	of	two	parts:	regional	id98	and	
lstm,	to	predict	the	valence	arousal	ratings	of	text.		

wang	et	al.58	described	a	joint	id98	and	id56	architecture	for	sentiment	classification	of	short	texts,	
which	 takes	 advantage	 of	 the	 coarse-grained	 local	 features	 generated	 by	 id98	 and	 long-distance	
dependencies	learned	via	id56.	

guggilla	 et	 al.59	 presented	 a	 lstm-	 and	 id98-based	 deep	 neural	 network	 model,	 which	 utilizes	
id97	 and	 linguistic	 embeddings	 for	 claim	 classification	 (classifying	 sentences	 to	 be	 factual	 or	
feeling).	

huang	 et	 al.60	 proposed	 to	 encode	 the	 syntactic	 knowledge	 (e.g.,	 part-of-speech	 tags)	 in	 a	 tree-
structured	lstm	to	enhance	phrase	and	sentence	representation.	

akhtar	et	 al.61	 proposed	 several	 multi-layer	 id88	 based	 ensemble	 models	 for	 fine-gained	
sentiment	classification	of	financial	microblogs	and	news.	

guan	 et	 al.62	 employed	 a	 weakly-supervised	 id98	 for	 sentence	 (and	 also	 aspect)	 level	 sentiment	
classification.	it	contains	a	two-step	learning	process:	it	first	learns	a	sentence	representation	weakly	
supervised	by	overall	review	ratings	and	then	uses	the	sentence	(and	aspect)	level	labels	for	fine-
tuning.		

teng	et	al.63	proposed	a	context-sensitive	lexicon-based	method	for	sentiment	classification	based	
on	 a	 simple	 weighted-sum	 model,	 using	 bidirectional	 lstm	 to	 learn	 the	 sentiment	 strength,	
intensification	and	negation	of	lexicon	sentiments	in	composing	the	sentiment	value	of	a	sentence.		

yu	and	jiang64	studied	the	problem	of	learning	generalized	sentence	embeddings	for	cross-domain	
sentence	 sentiment	 classification	 and	 designed	 a	 neural	 network	 model	 containing	 two	 separated	
id98s	 that	 jointly	 learn	 two	 hidden	 feature	 representations	 from	 both	 the	 labeled	 and	 unlabeled	
data.		

zhao	 et	 al.65	 introduced	 a	 recurrent	 random	 walk	 network	 learning	 approach	 for	 sentiment	
classification	 of	 opinionated	 tweets	 by	 exploiting	 the	 deep	 semantic	 representation	 of	 both	 user	
posted	tweets	and	their	social	relationships.		

mishra	et	al.66	utilized	id98	to	automatically	extract	cognitive	features	from	the	eye-movement	(or	
gaze)	data	of	human	readers	reading	the	text	and	used	them	as	enriched	features	along	with	textual	
features	for	sentiment	classification.		

qian	 et	 al.67	 presented	 a	 linguistically	 regularized	 lstm	 for	 the	 task.	 the	 proposed	 model	
incorporates	linguistic	resources	such	as	sentiment	lexicon,	negation	words	and	intensity	words	into	
the	lstm	so	as	to	capture	the	sentiment	effect	in	sentences	more	accurately.	

aspect	level	sentiment	classification	

different	 from	 the	 document	 level	 and	 the	 sentence	 level	 sentiment	 classification,	 aspect	 level	
sentiment	 classification	 considers	 both	 the	 sentiment	 and	 the	 target	 information,	 as	 a	 sentiment	

always	 has	 a	 target.	 as	 mentioned	 earlier,	 a	 target	 is	 usually	 an	 entity	 or	 an	 entity	 aspect.	 for	
simplicity,	both	entity	and	aspect	are	usually	just	called	aspect.	given	a	sentence	and	a	target	aspect,	
aspect	level	sentiment	classification	aims	to	infer	the	sentiment	polarity/orientation	of	the	sentence	
toward	the	target	aspect.	for	example,	in	the	sentence	   the	screen	is	very	clear	but	the	battery	life	is	
too	short.   	the	sentiment	is	positive	if	the	target	aspect	is	   screen   	but	negative	if	the	target	aspect	
is	   battery	life   .	we	will	discuss	automated	aspect	or	target	extraction	in	the	next	section.		

aspect	level	sentiment	classification	is	challenging	because	modelling	the	semantic	relatedness	of	a	
target	 with	 its	 surrounding	 context	 words	 is	 difficult.	 different	 context	 words	 have	 different	
influences	 on	 the	 sentiment	 polarity	 of	 a	 sentence	 towards	 the	 target.	 therefore,	 it	 is	 necessary	
capture	 semantic	 connections	 between	 the	 target	 word	 and	 the	 context	 words	 when	 building	
learning	models	using	neural	networks.		

there	are	three	important	tasks	in	aspect	level	sentiment	classification	using	neural	networks.	the	
first	task	is	to	represent	the	context	of	a	target,	where	the	context	means	the	contextual	words	in	a	
sentence	 or	 document.	 this	 issue	 can	 be	 similarly	 addressed	 using	 the	 text	 representation	
approaches	 mentioned	 in	 the	 above	 two	 sections.	 the	 second	 task	 is	 to	 generate	 a	 target	
representation,	which	can	properly	interact	with	its	context.	a	general	solution	is	to	learn	a	target	
embedding,	 which	 is	 similar	 to	 word	 embedding.	 the	 third	 task	 is	 to	 identify	 the	 important	
sentiment	 context	 (words)	 for	 the	 specified	 target.	 for	 example,	 in	 the	 sentence	    the	 screen	 of	
iphone	is	clear	but	batter	life	is	short   ,	   clear   	is	the	important	context	word	for	   screen   	and	   short   	
is	 the	 important	 context	 for	    battery	 life   .	 this	 task	 is	 recently	 addressed	 by	 the	 attention	
mechanism.	although	many	deep	learning	techniques	have	been	proposed	to	deal	with	aspect	level	
sentiment	classification,	to	our	knowledge,	there	are	still	no	dominating	techniques	in	the	literature.	
related	works	and	their	main	focuses	are	introduced	below.	

dong	 et	 al.68	 proposed	 an	 adaptive	 recursive	 neural	 network	 (adaid56)	 for	 target-dependent	
twitter	 sentiment	 classification,	 which	 learns	 to	 propagate	 the	 sentiments	 of	 words	 towards	 the	
target	depending	on	the	context	and	syntactic	structure.	it	uses	the	representation	of	the	root	node	
as	the	features,	and	feeds	them	into	the	softmax	classifier	to	predict	the	distribution	over	classes.				

vo	 and	 zhang69	 studied	 aspect-based	 twitter	 sentiment	 classification	 by	 making	 use	 of	 rich	
automatic	 features,	 which	 are	 additional	 features	 obtained	 using	 unsupervised	 learning	 methods.	
the	paper	showed	that	multiple	embeddings,	multiple	pooling	functions,	and	sentiment	lexicons	can	
offer	rich	sources	of	feature	information	and	help	achieve	performance	gains.	

since	 lstm	 can	 capture	 semantic	 relations	 between	 the	 target	 and	 its	 context	 words	 in	 a	 more	
flexible	way,	tang	et	al.70	proposed	target-dependent	lstm	(td-lstm)	and	target-connection	lstm	
(tc-lstm)	to	extend	lstm	by	taking	the	target	into	consideration.	they	regarded	the	given	target	as	
a	feature	and	concatenated	it	with	the	context	features	for	aspect	sentiment	classification.		

ruder	et	al.71	proposed	to	use	a	hierarchical	and	bidirectional	lstm	model	for	aspect	level	sentiment	
classification,	 which	 is	 able	 to	 leverage	 both	 intra-	 and	 inter-sentence	 relations.	 the	 sole	
dependence	 on	 sentences	 and	 their	 structures	 within	 a	 review	 renders	 the	 proposed	 model	
language-independent.	 word	 embeddings	 are	 fed	 into	 a	 sentence-level	 bidirectional	 lstm.	 final	
states	of	the	forward	and	backward	lstm	are	concatenated	together	with	the	target	embedding	and	
fed	 into	 a	 bidirectional	 review-level	 lstm.	 at	 every	 time	 step,	 the	 output	 of	 the	 forward	 and	
backward	lstm	is	concatenated	and	fed	into	a	final	layer,	which	outputs	a	id203	distribution	
over	sentiments.	

considering	the	limitation	of	work	by	dong	et	al.68	and	vo	and	zhang69,	zhang	et	al.72	proposed	a	
sentence	level	neural	model	to	address	the	weakness	of	pooling	functions,	which	do	not	explicitly	
model	tweet-level	semantics.	to	achieve	that,	two	gated	neural	networks	are	presented.	first,	a	bi-

directional	gated	neural	network	is	used	to	connect	the	words	in	a	tweet	so	that	pooling	functions	
can	 be	 applied	 over	 the	 hidden	 layer	 instead	 of	 words	 for	 better	 representing	 the	 target	 and	 its	
contexts.	 second,	 a	 three-way	 gated	 neural	 network	 structure	 is	 used	 to	 model	 the	 interaction	
between	the	target	mention	and	its	surrounding	contexts,	addressing	the	limitations	by	using	gated	
neural	 network	 structures	 to	 model	 the	 syntax	 and	 semantics	 of	 the	 enclosing	 tweet,	 and	 the	
interaction	 between	 the	 surrounding	 contexts	 and	 the	 target	 respectively.	 gated	 neural	 networks	
have	been	shown	to	reduce	the	bias	of	standard	recurrent	neural	networks	towards	the	ends	of	a	
sequence	by	better	propagation	of	gradients.		

wang	et	al.73	proposed	an	attention-based	lstm	method	with	target	embedding,	which	was	proven	
to	be	an	effective	way	to	enforce	the	neural	model	to	attend	to	the	related	part	of	a	sentence.	the	
attention	mechanism	is	used	to	enforce	the	model	to	attend	to	the	important	part	of	a	sentence,	in	
response	 to	 a	 specific	 aspect.	 likewise,	 yang	 et	 al.74	 proposed	 two	 attention-based	 bidirectional	
lstms	to	improve	the	classification	performance.	liu	and	zhang75	extended	the	attention	modelling	
by	 differentiating	 the	 attention	 obtained	 from	 the	 left	 context	 and	 the	 right	 context	 of	 a	 given	
target/aspect.	they	further	controlled	their	attention	contribution	by	adding	multiple	gates.		

tang	 et	 al.76	 introduced	 an	 end-to-end	 memory	 network	 for	 aspect	 level	 sentiment	 classification,	
which	employs	an	attention	mechanism	with	an	external	memory	to	capture	the	importance	of	each	
context	 word	 with	 respect	 to	 the	 given	 target	 aspect.	 this	 approach	 explicitly	 captures	 the	
importance	 of	 each	 context	 word	 when	 inferring	 the	 sentiment	 polarity	 of	 the	 aspect.	 such	
importance	degree	and	text	representation	are	calculated	with	multiple	computational	layers,	each	
of	which	is	a	neural	attention	model	over	an	external	memory.				

lei	et	al.77	proposed	to	use	a	neural	network	approach	to	extract	pieces	of	input	text	as	rationales	
(reasons)	 for	 review	 ratings.	 the	 model	 consists	 of	 a	 generator	 and	 a	 decoder.	 the	 generator	
specifies	a	distribution	over	possible	rationales	(extracted	text)	and	the	encoder	maps	any	such	text	
to	a	task-specific	target	vector.	for	multi-aspect	sentiment	analysis,	each	coordinate	of	the	target	
vector	represents	the	response	or	rating	pertaining	to	the	associated	aspect.				

li	et	al.78	integrated	the	target	identification	task	into	sentiment	classification	task	to	better	model	
aspect-sentiment	interaction.	they	showed	that	sentiment	identification	can	be	solved	with	an	end-
to-end	machine	learning	architecture,	in	which	the	two	sub-tasks	are	interleaved	by	a	deep	memory	
network.	 in	 this	 way,	 signals	 produced	 in	 target	 detection	 provide	 clues	 for	 polarity	 classification,	
and	reversely,	the	predicted	polarity	provides	feedback	to	the	identification	of	targets.			

ma	 et	 al.79	 proposed	 an	 interactive	 attention	 network	 (ian)	 that	 considers	 both	 attentions	 on	
target	 and	 context.	 that	 is,	 it	 uses	 two	 attention	 networks	 to	 interactively	 detect	 the	 important	
words	of	the	target	expression/description	and	the	important	words	of	its	full	context.		

chen	et	al.80	proposed	to	utilize	a	recurrent	attention	network	to	better	capture	the	sentiment	of	
complicated	 contexts.	 to	 achieve	 that,	 their	 proposed	 model	 uses	 a	 recurrent/dynamic	 attention	
structure	and	learns	non-linear	combination	of	the	attention	in	grus.		

tay	 et	 al.81	 designed	 a	 dyadic	 memory	 network	 (dymemnn)	 that	 models	 dyadic	 interactions	
between	 aspect	 and	 context,	 by	 using	 either	 neural	 tensor	 compositions	 or	 holographic	
compositions	for	memory	selection	operation.	

aspect	extraction	and	categorization	

to	perform	aspect	level	sentiment	classification,	one	needs	to	have	aspects	(or	targets),	which	can	
be	manually	given	or	automatically	extracted.	in	this	section,	we	discuss	existing	work	for	automated	
aspect	 extraction	 (or	 aspect	 term	 extraction)	 from	 a	 sentence	 or	 document	 using	 deep	 learning	

models.	let	us	use	an	example	to	state	the	problem.	for	example,	in	the	sentence	   the	image	is	very	
clear   	the	word	   image   	is	an	aspect	term	(or	sentiment	target).	the	associated	problem	of	aspect	
categorization	 is	 to	 group	 the	 same	 aspect	 expressions	 into	 a	 category.	 for	 instance,	 the	 aspect	
terms	   image   ,	   photo   	and	   picture   	can	be	grouped	into	one	aspect	category	named	image.	in	the	
review	below,	we	include	the	extraction	of	both	aspect	and	entity	that	are	associated	with	opinions.		

one	reason	why	deep	learning	models	can	be	helpful	for	this	task	is	that,	deep	learning	is	essentially	
good	at	learning	(complicated)	feature	representations.	when	an	aspect	is	properly	characterized	in	
some	 feature	 space,	 for	 example,	 in	 one	 or	 some	 hidden	 layer(s),	 the	 semantics	 or	 correlation	
between	an	aspect	and	its	context	can	be	captured	with	the	interplay	between	their	corresponding	
feature	representations.	in	other	words,	deep	learning	provides	a	possible	approach	to	automated	
feature	engineering	without	human	involvement.		

katiyar	and	cardie82	investigated	the	use	of	deep	bidirectional	lstms	for	joint	extraction	of	opinion	
entities	and	the	is-form	and	is-about	relationships	that	connect	the	entities.	wang	et	al.83	further	
proposed	a	joint	model	integrating	id56	and	conditional	random	fields	(crf)	to	co-extract	aspects	
and	opinion	terms	or	expressions.	the	proposed	model	can	learn	high-level	discriminative	features	
and	double-propagate	information	between	aspect	and	opinion	terms	simultaneously.	wang	et	al.84	
further	 proposed	 a	 coupled	 multi-layer	 attention	 model	 (cmla)	 for	 co-extracting	 of	 aspect	 and	
opinion	terms.	the	model	consists	of	an	aspect	attention	and	an	opinion	attention	using	gru	units.	
an	 improved	 lstm-based	 approach	 was	 reported	 by	 li	 and	 lam85,	 specifically	 for	 aspect	 term	
extraction.	it	consists	of	three	lstms,	of	which	two	lstms	are	for	capturing	aspect	and	sentiment	
interactions.	the	third	lstm	is	to	use	the	sentiment	polarity	information	as	an	additional	guidance.		

he	 et	 al.86	 proposed	 an	 attention-based	 model	 for	 unsupervised	 aspect	 extraction.	 the	 main	
intuition	 is	 to	 utilize	 the	 attention	 mechanism	 to	 focus	 more	 on	 aspect-related	 words	 while	 de-
emphasizing	 aspect-irrelevant	 words	 during	 the	 learning	 of	 aspect	 embeddings,	 similar	 to	 the	
autoencoder	framework.	

zhang	 et	 al.87	 extended	 a	 crf	 model	 using	 a	 neural	 network	 to	 jointly	 extract	 aspects	 and	
corresponding	sentiments.	the	proposed	crf	variant	replaces	the	original	discrete	features	in	crf	
with	continuous	word	embeddings,	and	adds	a	neural	layer	between	the	input	and	output	nodes.		

zhou	 et	 al.88	 proposed	 a	 semi-supervised	 word	 embedding	 learning	 method	 to	 obtain	 continuous	
word	 representations	 on	 a	 large	 set	 of	 reviews	 with	 noisy	 labels.	 with	 the	 word	 vectors	 learned,	
deeper	and	hybrid	features	are	learned	by	stacking	on	the	word	vectors	through	a	neural	network.	
finally,	a	logistic	regression	classifier	trained	with	the	hybrid	features	is	used	to	predict	the	aspect	
category.	

yin	 et	 al.89	 first	 learned	 word	 embedding	 by	 considering	 the	 dependency	 path	 connecting	 words.	
then	 they	 designed	 some	 embedding	 features	 that	 consider	 the	 linear	 context	 and	 dependency	
context	information	for	crf-based	aspect	term	extraction.	

xiong	 et	 al.90	 proposed	 an	 attention-based	 deep	 distance	 metric	 learning	 model	 to	 group	 aspect	
phrases.	 the	 attention-based	 model	 is	 to	 learn	 feature	 representation	 of	 contexts.	 both	 aspect	
phrase	embedding	and	context	embedding	are	used	to	learn	a	deep	feature	subspace	metric	for	k-
means	id91.	

poria	 et	 al.91	 proposed	 to	 use	 id98	 for	 aspect	 extraction.	 they	 developed	 a	 seven-layer	 deep	
convolutional	neural	network	to	tag	each	word	in	opinionated	sentences	as	either	aspect	or	non-
aspect	word.	some	linguistic	patterns	are	also	integrated	into	the	model	for	further	improvement.	

ying	 et	 al.92	 proposed	 two	 id56-based	 models	 for	 cross-domain	 aspect	 extraction.	 they	 first	 used	
rule-based	methods	to	generate	an	auxiliary	label	sequence	for	each	sentence.	they	then	trained	
the	models	using	both	the	true	labels	and	auxiliary	labels,	which	shows	promising	results.			

opinion	expression	extraction	

in	 this	 and	 the	 next	 few	 sections,	 we	 discuss	 deep	 learning	 applications	 to	 some	 other	 sentiment	
analysis	 related	 tasks.	 this	 section	 focuses	 on	 the	 problem	 of	 opinion	 expression	 extraction	 (or	
opinion	 term	 extraction,	 or	 opinion	 identification),	 which	 aims	 to	 identify	 the	 expressions	 of	
sentiment	in	a	sentence	or	a	document.		

similar	 to	 the	 aspect	 extraction,	 opinion	 expression	 extraction	 using	 deep	 learning	 models	 is	
workable	because	their	characteristics	could	be	identified	in	some	feature	space	as	well.	

irsoy	 and	 cardie93	 explored	 the	 application	 of	 deep	 bidirectional	 id56	 for	 the	 task,	 which	
outperforms	traditional	shallow	id56s	with	the	same	number	of	parameters	and	also	previous	crf	
methods.94		

liu	et	al.95	presented	a	general	class	of	discriminative	models	based	on	the	id56	architecture	and	
word	 embedding.	 the	 authors	 used	 pre-trained	 word	 embeddings	 from	 three	 external	 sources	 in	
different	id56	architectures	including	elman-type,	jordan-type,	lstm	and	their	variations.		

wang	et	al.83	proposed	a	model	integrating	recursive	neural	networks	and	crf	to	co-extract	aspect	
and	 opinion	 terms.	 the	 aforementioned	 cmla	 is	 also	 proposed	 for	 co-extraction	 of	 aspect	 and	
opinion	terms.84		

sentiment	composition		

sentiment	composition	claims	that	the	sentiment	orientation	of	an	opinion	expression	is	determined	
by	the	meaning	of	its	constituents	as	well	as	the	grammatical	structure.	due	to	their	particular	tree-
structure	 design,	 reid98	 is	 naturally	 suitable	 for	 this	 task.51	 irsoy	 and	 cardie96	 reported	 that	 the	
reid98	with	a	deep	architecture	can	more	accurately	capture	different	aspects	of	compositionality	in	
language,	 which	 benefits	 sentiment	 compositionality.	 zhu	 et	 al.97	 proposed	 a	 neural	 network	 for	
integrating	 the	 compositional	 and	 non-compositional	 sentiment	 in	 the	 process	 of	 sentiment	
composition.	

opinion	holder	extraction	

opinion	 holder	 (or	 source)	 extraction	 is	 the	 task	 of	 recognizing	 who	 holds	 the	 opinion	 (or	
whom/where	the	opinion	is	from).1	for	example,	in	the	sentence	   john	hates	his	car   ,	the	opinion	
holder	is	   john   .	this	problem	is	commonly	formulated	as	a	sequence	labelling	problem	like	opinion	
expression	extraction	or	aspect	extraction.	notice	that	opinion	holder	can	be	either	explicit	(from	a	
noun	phrase	in	the	sentence)	or	implicit	(from	the	writer)	as	shown	by	yang	and	cardie98.	deng	and	
wiebe99	 proposed	 to	 use	 word	 embeddings	 of	 opinion	 expressions	 as	 features	 for	 recognizing	
sources	 of	 participant	 opinions	 and	 non-participant	 opinions,	 where	 a	 source	 can	 be	 the	 noun	
phrase	or	writer.		

temporal	opinion	mining	

time	is	also	an	important	dimension	in	problem	definition	of	sentiments	analysis	(see	liu   s	book1).	
as	 time	 passes	 by,	 people	 may	 maintain	 or	 change	 their	 mind,	 or	 even	 give	 new	 viewpoints.	

therefore,	predicting	future	opinion	is	important	in	sentiment	analysis.	some	research	using	neural	
networks	has	been	reported	recently	to	tackle	this	problem.	

chen	et	al.100	proposed	a	content-based	social	influence	model	(cim)	to	make	opinion	behaviour	
predictions	of	twitter	users.	that	is,	it	uses	the	past	tweets	to	predict	users   	future	opinions.	it	is	
based	on	a	neural	network	framework	to	encode	both	the	user	content	and	social	relation	factor	
(one   s	opinion	about	a	target	is	influenced	by	one   s	friends).			

rashkin	 et	 al.101	 used	 lstms	 for	 targeted	 sentiment	 forecast	 in	 the	 social	 media	 context.	 they	
introduced	 multilingual	 connotation	 frames,	 which	 aim	 at	 forecasting	 implied	 sentiments	 among	
world	event	participants	engaged	in	a	frame.	

sentiment	analysis	with	word	embedding	

it	is	clear	that	word	embeddings	played	an	important	role	in	deep	learning	based	sentiment	analysis	
models.	it	is	also	shown	that	even	without	the	use	of	deep	learning	models,	word	embeddings	can	
be	used	as	features	for	non-neural	learning	models	for	various	tasks.	the	section	thus	specifically	
highlights	word	embeddings   	contribution	to	sentiment	analysis.		

we	first	present	the	works	of	sentiment-encoded	word	embeddings.	for	sentiment	analysis,	directly	
applying	regular	word	methods	like	cbow	or	skip-gram	to	learn	word	embeddings	from	context	can	
encounter	 problems,	 because	 words	 with	 similar	 contexts	 but	 opposite	 sentiment	 polarities	 (e.g.,	
   good   	or	   bad   )	may	be	mapped	to	nearby	vectors	in	the	embedding	space.	therefore,	sentiment-
encoded	word	embedding	methods	have	been	proposed.	mass	el	al.102	learned	word	embeddings	
that	can	capture	both	semantic	and	sentiment	information.	bespalov	et	al.103	showed	that	an	id165	
model	 combined	 with	 latent	 representation	 would	 produce	 a	 more	 suitable	 embedding	 for	
sentiment	 classification.	 labutov	 and	 lipson104	 re-embed	 existing	 word	 embeddings	 with	 logistic	
regression	by	regarding	sentiment	supervision	of	sentences	as	a	id173	term.		

le	and	mikolov35	proposed	the	concept	of	paragraph	vector	to	first	learn	fixed-length	representation	
for	 variable-length	 pieces	 of	 texts,	
including	 sentences,	 paragraphs	 and	 documents.	 they	
experimented	 on	 both	 sentence	 and	 document-level	 sentiment	 classification	 tasks	 and	 achieved	
performance	 gains,	 which	 demonstrates	 the	 merit	 of	 paragraph	 vectors	 in	 capturing	 semantics	 to	
help	sentiment	classification.	tang	et	al.105,106	presented	models	to	learn	sentiment-specific	 word	
embeddings	(sswe),	in	which	not	only	the	semantic	but	also	sentiment	information	is	embedded	in	
the	 learned	 word	 vectors.	 wang	 and	 xia107	 developed	 a	 neural	 architecture	 to	 train	 a	 sentiment-
bearing	word	embedding	by	integrating	the	sentiment	supervision	at	both	the	document	and	word	
levels.	yu	et	al.108	adopted	a	refinement	strategy	to	obtain	joint	semantic-sentiment	bearing	word	
vectors.	

feature	enrichment	and	multi-sense	word	embeddings	are	also	investigated	for	sentiment	analysis.	
vo	 and	 zhang69	 studied	 aspect-based	 twitter	 sentiment	 classification	 by	 making	 use	 of	 rich	
automatic	features,	which	are	additional	features	obtained	using	unsupervised	learning	techniques.	
li	and	jurafsky109	experimented	with	the	utilization	of	multi-sense	word	embeddings	on	various	nlp	
tasks.	experimental	results	show	that	while	such	embeddings	do	improve	the	performance	of	some	
tasks,	they	offer	little	help	to	sentiment	classification	tasks.	ren	et	al.110	proposed	methods	to	learn	
topic-enriched	multi-prototype	word	embeddings	for	twitter	sentiment	classification.	

multilinguistic	 word	 embeddings	 have	 also	 been	 applied	 to	 sentiment	 analysis.	 zhou	 et	 al.111	
reported	 a	 bilingual	 sentiment	 word	 embedding	 (bswe)	 model	 for	 cross-language	 sentiment	
classification.	it	incorporates	the	sentiment	information	into	english-chinese	bilingual	embeddings	
by	employing	labeled	corpora	and	their	translation,	instead	of	large-scale	parallel	corpora.	barnes	et	

al.112	 compared	 several	 types	 of	 bilingual	 word	 embeddings	 and	 neural	 machine	 translation	
techniques	for	cross-lingual	aspect-based	sentiment	classification.	

zhang	et	al.113	integrated	word	embeddings	with	matrix	factorization	for	personalized	review-based	
rating	 prediction.	 specifically,	 the	 authors	 refine	 existing	 semantics-oriented	 word	 vectors	 (e.g.,	
id97	 and	 glove)	 using	 sentiment	 lexicons.	 sharma	 et	 al.114	 proposed	 a	 semi-supervised	
technique	to	use	sentiment	bearing	word	embeddings	for	ranking	sentiment	intensity	of	adjectives.	
word	embedding	techniques	have	also	been	utilized	or	improved	to	help	address	various	sentiment	
analysis	tasks	in	many	other	recent	studies.55,62,87,89,95	

sarcasm	analysis	

sarcasm	is	a	form	verbal	irony	and	a	closely	related	concept	to	sentiment	analysis.	recently,	there	is	
a	growing	interest	in	nlp	communities	in	sarcasm	detection.	researchers	have	attempted	to	solve	it	
using	deep	learning	techniques	due	of	their	impressive	success	in	many	other	nlp	problems.		

zhang	et	al.115	constructed	a	deep	neural	network	model	for	tweet	sarcasm	detection.	their	network	
first	uses	a	bidirectional	gru	model	to	capture	the	syntactic	and	semantic	information	over	tweets	
locally,	 and	 then	 uses	 a	 pooling	 neural	 network	 to	 extract	 contextual	 features	 automatically	 from	
history	tweets	for	detecting	sarcastic	tweets.		

joshi	 et	 al.116	
investigated	 word	 embeddings-based	 features	 for	 sarcasm	 detection.	 they	
experimented	 four	 past	 algorithms	 for	 sarcasm	 detection	 with	 augmented	 word	 embeddings	
features	and	showed	promising	results.	

poria	et	al.117	developed	a	id98-based	model	for	sarcasm	detection	(sarcastic	or	non-sarcastic	tweets	
classification),	by	jointly	modelling	pre-trained	emotion,	sentiment	and	personality	features,	along	
with	the	textual	information	in	a	tweet.	

peled	 and	 reichart118	 proposed	 to	 interpret	 sarcasm	 tweets	 based	 on	 a	 id56	 neural	 machine	
translation	model.		

ghosh	and	veale119	proposed	a	id98	and	bidirectional	lstm	hybrid	for	sarcasm	detection	in	tweets,	
which	models	both	linguistic	and	psychological	contexts.		

mishra	et	al.66	utilized	id98	to	automatically	extract	cognitive	features	from	the	eye-movement	(or	
gaze)	data	to	enrich	information	for	sarcasm	detection.	word	embeddings	are	also	used	for	irony	
recognition	in	english	tweets120	and	for	controversial	words	identification	in	debates.121	

emotion	analysis	

emotions	are	the	subjective	feelings	and	thoughts	of	human	beings.	the	primary	emotions	include	
love,	joy,	surprise,	anger,	sadness	and	fear.	the	concept	of	emotion	is	closely	related	to	sentiment.	
for	example,	the	strength	of	a	sentiment	can	be	linked	to	the	intensity	of	certain	emotion	like	joy	
and	anger.	thus,	many	deep	learning	models	are	also	applied	to	emotion	analysis	following	the	way	
in	sentiment	analysis.	

wang	 et	 al.	122	 built	 a	 bilingual	 attention	 network	 model	 for	 code-switched	 emotion	 prediction.	 a	
lstm	model	is	used	to	construct	a	document	level	representation	of	each	post,	and	the	attention	
mechanism	is	employed	to	capture	the	informative	words	from	both	the	monolingual	and	bilingual	
contexts.	

zhou	 et	 al.	123	 proposed	 an	 emotional	 chatting	 machine	 to	 model	 the	 emotion	 influence	 in	 large-
scale	conversation	generation	based	on	gru.	the	technique	has	also	been	applied	in	other	papers.	
39,72,115	

abdul-mageed	and	ungar124	first	built	a	large	dataset	for	emotion	detection	automatically	by	using	
distant	supervision	and	then	used	a	gru	network	for	fine-grained	emotion	detection.		

felbo	et	 al.	125	 used	 millions	 of	 emoji	 occurrences	 in	 social	 media	 for	pretraining	 neural	 models	 in	
order	to	learn	better	representations	of	emotional	contexts.	

a	 question-answering	 approach	 is	 proposed	 using	 a	 deep	 memory	 network	 for	 emotion	 cause	
extraction.126	 emotion	 cause	 extraction	 aims	 to	 identify	 the	 reasons	 behind	 a	 certain	 emotion	
expressed	in	text.	

multimodal	data	for	sentiment	analysis	

multimodal	data,	such	as	the	data	carrying	textual,	visual,	and	acoustic	information,	has	been	used	
to	help	sentiment	analysis	as	it	provides	additional	sentiment	signals	to	the	traditional	text	features.	
since	 deep	 learning	 models	 can	 map	 inputs	 to	 some	 latent	 space	 for	 feature	 representation,	 the	
inputs	from	multimodal	data	can	also	be	projected	simultaneously	to	learn	multimodal	data	fusion,	
for	example,	by	using	feature	concatenation,	joint	latent	space,	or	other	more	sophisticated	fusion	
approaches.	there	is	now	a	growing	trend	of	using	multimodal	data	with	deep	learning	techniques.	

poria	et	al.127	proposed	a	way	of	extracting	features	from	short	texts	based	on	the	activation	values	
of	an	inner	layer	of	id98.	the	main	novelty	of	the	paper	is	the	use	of	a	deep	id98	to	extract	features	
from	text	and	the	use	of	multiple	kernel	learning	(mkl)	to	classify	heterogeneous	multimodal	fused	
feature	vectors.	

bertero	et	al.128	described	a	id98	model	for	emotion	and	sentiment	recognition	in	acoustic	data	from	
interactive	dialog	systems.		

fung	et	al.129	demonstrated	a	virtual	interaction	dialogue	system	that	have	incorporated	sentiment,	
emotion	and	personality	recognition	capabilities	trained	by	deep	learning	models.		

wang	et	al.130	reported	a	id98	structured	deep	network,	named	deep	coupled	adjective	and	noun	
(dcan)	neural	network,	for	visual	sentiment	classification.	the	key	idea	of	dcan	is	to	harness	the	
adjective	and	noun	text	descriptions,	treating	them	as	two	(weak)	supervision	signals	to	learn	two	
intermediate	sentiment	representations.	those	learned	representations	are	then	concatenated	and	
used	for	sentiment	classification.	

yang	et	al.131	developed	two	algorithms	based	on	a	conditional	id203	neural	network	to	analyse	
visual	sentiment	in	images.	

zhu	 et	 al.132	 proposed	 a	 unified	 id98-id56	 model	 for	 visual	 emotion	 recognition.	 the	 architecture	
leverages	id98	with	multiple	layers	to	extract	different	levels	of	features	(e.g.,	colour,	texture,	object,	
etc.)	within	a	multi-task	learning	framework.	and	a	bidirectional	id56	is	proposed	to	integrate	the	
learned	features	from	different	layers	in	the	id98	model.	

you	 et	 al.133	 adopted	 the	 attention	 mechanism	 for	 visual	 sentiment	 analysis,	 which	 can	 jointly	
discover	 the	 relevant	 local	 image	 regions	 and	 build	 a	 sentiment	 classifier	 on	 top	 of	 these	 local	
regions.		

poria	et	al.134	proposed	some	a	deep	learning	model	for	multi-modal	sentiment	analysis	and	emotion	
recognition	 on	 video	 data.	 particularly,	 a	 lstm-based	 model	 is	 proposed	 for	 utterance-level	

sentiment	analysis,	which	can	capture	contextual	information	from	their	surroundings	in	the	same	
video.		

tripathi	 et	 al.135	 used	 deep	 and	 id98-based	 models	 for	 emotion	 classification	 on	 a	 multimodal	
dataset	deap,	which	contains	electroencephalogram	and	peripheral	physiological	and	video	signals.		

zadeh	et	al.136	formulated	the	problem	of	multimodal	sentiment	analysis	as	modelling	intra-modality	
and	inter-modality	dynamics	and	introduced	a	new	neural	model	named	tensor	fusion	network	to	
tackle	it.		

long	et	al.137	proposed	an	attention	neural	model	trained	with	cognition	grounded	eye-tracking	data	
for	 sentence-level	 sentiment	 classification.	 a	 cognition	 based	 attention	 (cba)	 layer	 is	 built	 for	
neural	sentiment	analysis.	

in	 multimodal	 sentiment	 analysis,	 which	 removes	 the	

wang	et	al.	138	proposed	a	select-additive	learning	(sal)	approach	to	tackle	the	confounding	factor	
problem	
latent	
representations	 learned	 by	 neural	 networks	 (e.g.,	 id98).	 to	 achieve	 it,	 two	 learning	 phases	 are	
involved,	 namely,	 a	 selection	 phase	 for	 confounding	 factor	 identification	 and	 a	 removal	phase	for	
confounding	factor	removal.	

individual	 specific	

resource-poor	language	and	multilingual	sentiment	analysis	

recently,	 sentiment	 analysis	 in	 resource-poor	 languages	 (compared	 to	 english)	 has	 also	 achieved	
significant	progress	due	to	the	use	of	deep	learning	models.	additionally,	multilingual	features	also	
can	 help	 sentiment	 analysis	 just	 like	 multimodal	 data.	 in	 the	 same	 way,	 deep	 learning	 has	 been	
applied	to	the	multilingual	sentiment	analysis	setting.	

akhtar	et	al.139	reported	a	id98-based	hybrid	architecture	for	sentence	and	aspect	level	sentiment	
classification	in	a	resource-poor	language,	hindi.		

dahou	et	al.140	used	word	embeddings	and	a	id98-based	model	for	arabic	sentiment	classification	at	
the	sentence	level.		

singhal	 and	 bhattacharyya141	 designed	 a	 solution	 for	 multilingual	 sentiment	 classification	 at	
review/sentence	level	and	experimented	with	multiple	languages,	including	hindi,	marathi,	russian,	
dutch,	french,	spanish,	italian,	german,	and	portuguese.	the	authors	applied	machine	translation	
tools	to	translate	these	languages	into	english	and	then	used	english	word	embeddings,	polarities	
from	a	sentiment	lexicon	and	a	id98	model	for	classification.		

joshi	 et	 al.142	 introduced	 a	 sub-word	 level	 representation	 in	 a	 lstm	 architecture	 for	 sentiment	
classification	of	hindi-english	code-mixed	sentences.	

other	related	tasks	

there	are	also	applications	of	deep	learning	in	some	other	sentiment	analysis	related	tasks.		

sentiment	intersubjectivity:	gui	et	al.143	tackled	the	intersubjectivity	problem	in	sentiment	analysis,	
where	 the	 problem	 is	 to	 study	 the	 gap	 between	 the	 surface	 form	 of	 a	 language	 and	 the	
corresponding	abstract	concepts,	and	incorporate	the	modelling	of	intersubjectivity	into	a	proposed	
id98.	

lexicon	expansion:	wang	et	al.144	proposed	a	pu	learning-based	neural	approach	for	opinion	lexicon	
expansion.	

financial	volatility	prediction:	rekabsaz	et	al.145	made	volatility	predictions	using	financial	disclosure	
sentiment	with	word	embedding-based	information	retrieval	models,	where	word	embeddings	are	
used	in	similar	word	set	expansion.	

opinion	 recommendation:	 wang	 and	 zhang146	 introduced	 the	 task	 of	 opinion	 recommendation,	
which	aims	to	generate	a	customized	review	score	of	a	product	that	the	particular	user	is	likely	to	
give,	as	well	as	a	customized	review	that	the	user	would	have	written	for	the	target	product	if	the	
user	had	reviewed	the	product.	a	multiple-attention	memory	network	was	proposed	to	tackle	the	
problem,	which	considers	users   	reviews,	product   s	reviews,	and	users   	neighbours	(similar	users).	

stance	 detection:	augenstein	et	al.147	proposed	a	bidirectional	lstms	with	a	conditional	encoding	
mechanism	for	stance	detection	in	political	twitter	data.	du	et	al.148	designed	a	target-specific	neural	
attention	model	for	stance	classification.	

conclusion	

applying	 deep	 learning	 to	 sentiment	 analysis	 has	 become	 a	 popular	 research	 topic	 lately.	 in	 this	
paper,	 we	 introduced	 various	 deep	 learning	 architectures	 and	 their	 applications	 in	 sentiment	
analysis.	 many	 of	 these	 deep	 learning	 techniques	 have	 shown	 state-of-the-art	 results	 for	 various	
sentiment	analysis	tasks.	with	the	advances	of	deep	learning	research	and	applications,	we	believe	
that	there	will	be	more	exciting	research	of	deep	learning	for	sentiment	analysis	in	the	near	future.	

	

acknowledgments	

bing	liu	and	shuai	wang   s	work	was	supported	in	part	by	national	science	foundation	(nsf)	under	
grant	no.	iis1407927	and	iis-1650900,	and	by	huawei	technologies	co.	ltd	with	a	research	gift.		

references	

[1]	 liu	 b.	 sentiment	 analysis:	 mining	 opinions,	 sentiments,	 and	 emotions.	 the	 cambridge	 university	 press,	
2015.		

[2]	liu	b.	sentiment	analysis	and	opinion	mining	(introduction	and	survey),	morgan	&	claypool,	may	2012.	

[3]	pang	b	and	lee	l.	opinion	mining	and	sentiment	analysis.	foundations	and	trends	in	information	retrieval,	
2008.	2(1   2):	pp.	1   135.	

[4]	goodfellow	i,	bengio	y,	courville	a.	deep	learning.	the	mit	press.	2016.	

[5]	 glorot	 x,	 bordes	 a,	 bengio	 y.	 deep	 sparse	 rectifier	 neural	 networks.	 in	 proceedings	 of	 the	 international	
conference	on	artificial	intelligence	and	statistics	(aistats	2011),	2011.	

[6]	 rumelhart	 d.e,	 hinton	 g.e,	 williams	 r.j.	 learning	 representations	 by	 back-propagating	 errors.	 cognitive	
modelling,	1988.	

[7]	collobert	r,	weston	j,	bottou	l,	karlen	m,	kavukcuoglu	k,	and	kuksa	p.	natural	language	processing	(almost)	
from	scratch.	journal	of	machine	learning	research,	2011.	

[8]	 goldberg	 y.	 a	 primer	 on	 neural	 network	 models	 for	 natural	 language	 processing.	 journal	 of	 artificial	
intelligence	research,	2016.	

[9]	bengio	y,	courville	a,	vincent	p.	representation	learning:		a	review	and	new	perspectives.	ieee	transactions	
on	pattern	analysis	and	machine	intelligence,	2013.		

[10]	lee	h,	grosse	r,	ranganath	r,	and	ng	a.y.	convolutional	deep	belief	networks	for	scalable	unsupervised	
learning	of	hierarchical	representations.	in	proceedings	of	the	international	conference	on	machine	learning	
(icml	2009),	2009.	

[11]	bengio	y,	ducharme	r,	vincent	p,	and	jauvin	c.	a	neural	probabilistic	language	model.	journal	of	machine	
learning	research,	2003.	

[12]	 morin	 f,	 bengio	 y.	 hierarchical	 probabilistic	 neural	 network	 language	 model.	 in	 proceedings	 of	 the	
international	workshop	on	artificial	intelligence	and	statistics,	2005.		

[13]	mikolov	t,	chen	k,	corrado	g,	and	dean	j.	efficient	estimation	of	word	representations	in	vector	space.	in	
proceedings	of	international	conference	on	learning	representations	(iclr	2013),	2013.	

[14]	mikolov	t,	sutskever	i,	chen	k,	corrado	g,	and	dean	j.	distributed	representations	of	words	and	phrases	
and	 their	 compositionality.	 in	 proceedings	 of	 the	 annual	 conference	 on	 advances	 in	 neural	 information	
processing	systems	(nips	2013),	2013.	

[15]	 mnih	 a,	 kavukcuoglu	 k.	 learning	 word	 embeddings	 efficiently	 with	 noise-contrastive	 estimation.	 in	
proceedings	 of	 the	 annual	 conference	 on	 advances	 in	 neural	 information	 processing	 systems	 (nips	 2013),	
2013.	

[16]	huang	e.h,	socher	r,	manning	c.d.	and	ng	a.y.	improving	word	representations	via	global	context	and	
multiple	 word	 prototypes.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	 computational	
linguistics	(acl	2012),	2012.	

[17]	pennington	j,	socher	r,	manning	c.d.	glove:	global	vectors	for	word	representation.	in	proceedings	of	the	
conference	on	empirical	methods	on	natural	language	processing	(emnlp	2014),	2014.	

[18]	 bengio	 y,	 lamblin	 p,	 popovici	 d,	 and	 larochelle	 h.	 greedy	 layer-wise	 training	 of	 deep	 networks.	 in	
proceedings	 of	 the	 annual	 conference	 on	 advances	 in	 neural	 information	 processing	 systems	 (nips	 2006),	
2006.	

[19]	hinton	g.e,	salakhutdinov	r.r.	reducing	the	dimensionality	of	data	with	neural	networks.	science,	july	
2006.	

[20]	 vincent	 p,	 larochelle	 h,	 bengio	 y,	 and	 manzagol	 p-a.	 extracting	 and	 composing	 robust	 features	 with	
denoising	 autoencoders.	 in	 proceedings	 of	 the	 international	 conference	 on	 machine	 learning	 (icml	 2008),	
2008.		

[21]	sermanet	p,	lecun	y.	traffic	sign	recognition	with	multi-scale	convolutional	networks.	in	proceedings	of	
the	international	joint	conference	on	neural	networks	(ijid98	2011),	2011.	

[22]	elman	j.l.	finding	structure	in	time.	cognitive	science,	1990.		

[23]	bengio	y,	simard	p,	frasconi	p.	learning	long-term	dependencies	with	gradient	descent	is	difficult.	ieee	
transactions	on	neural	networks,	1994.		

[24]	schuster	m,	paliwal	k.k.	bidirectional	recurrent	neural	networks.	ieee	transactions	on	signal	processing,	
1997.	

[25]	hochreiter	s,	schmidhuber	j.	long	short-term	memory.	neural	computation,	9(8):	1735-1780,	1997.	

[26]	tai	k.s,	socher	r,	manning	c.	d.	improved	semantic	representations	from	tree-structured	long	short-term	
memory	networks.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	
2015),	2015.	

[27]	 cho	 k,	 bahdanau	 d,	 bougares	 f,	 schwenk	 h	 and	 bengio	 y.	 learning	 phrase	 representations	 using	 id56	
encoder-decoder	for	statistical	machine	translation.	in	proceedings	of	the	conference	on	empirical	methods	in	
natural	language	processing	(emnlp	2014),	2014.	

[28]	 chung	 j,	 gulcehre	 c,	 cho	 k,	 bengio	 y.	 empirical	 evaluation	 of	 gated	 recurrent	 neural	 networks	 on	
sequence	modelling.	arxiv	preprint	arxiv:1412.3555,	2014.		

[29]	bahdanau	d,	cho	k,	bengio	y.	neural	machine	translation	by	jointly	learning	to	align	and	translate.	arxiv	
preprint	arxiv:1409.0473,	2014.			

[30]	weston	j,	chopra	s,	bordes	a.	memory	networks.	arxiv	preprint	arxiv:1410.3916.	2014.	

[31]	sukhbaatar	s,	weston	j,	fergus	r.	end-to-end	memory	networks.	in	proceedings	of	the	29th	conference	on	
neural	information	processing	systems	(nips	2015),	2015.	

[32]	graves	a,	wayne	g,	danihelka	i.	neural	turing	machines.	preprint	arxiv:1410.5401.	2014.		

[33]	qian	q,	tian	b,	huang	m,	liu	y,	zhu	x	and	zhu	x.	learning	tag	embeddings	and	tag-specific	composition	
functions	 in	 the	 recursive	 neural	 network.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	
computational	linguistics	(acl	2015),	2015.	

[34]	 moraes	 r,	 valiati	 j.f,	 neto	 w.p.	 document-level	 sentiment	 classification:	 an	 empirical	 comparison	
between	id166	and	ann.	expert	systems	with	applications.	2013.		

[35]	 le	 q,	 mikolov	 t.	 distributed	 representations	 of	 sentences	 and	 documents.	 in	 proceedings	 of	 the	
international	conference	on	machine	learning	(icml	2014),	2014.	

[36]	glorot	x,	bordes	a,	bengio	y.	domain	adaption	for	large-scale	sentiment	classification:	a	deep	learning	
approach.	in	proceedings	of	the	international	conference	on	machine	learning	(icml	2011),	2011.	

[37]	zhai	s,	zhongfei	(mark)	zhang.	semisupervised	autoencoder	for	sentiment	analysis.	in	proceedings	of	aaai	
conference	on	artificial	intelligence	(aaai	2016),	2016.	

[38]	johnson	r,	zhang	t.	effective	use	of	word	order	for	text	categorization	with	convolutional	neural	networks.		
in	 proceedings	 of	 the	 conference	 of	 the	 north	 american	 chapter	 of	 the	 association	 for	 computational	
linguistics:	human	language	technologies	(naacl-hlt	2015),	2015.	

[39]	tang	d,	qin	b,	liu	t.	document	modelling	with	gated	recurrent	neural	network	for	sentiment	classification.	
in	proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2015),	2015.			

[40]	 tang	 d,	 qin	 b,	 liu	 t.	 learning	 semantic	 representations	 of	 users	 and	 products	 for	 document	 level	
sentiment	classification.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	
(acl	2015),	2015.	

[41]	chen	h,	sun	m,	tu	c,	lin	y,	and	liu	z.	neural	sentiment	classification	with	user	and	product	attention.	in	
proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.	

[42]	dou	zy.	capturing	user	and	product	information	for	document	level	sentiment	analysis	with	deep	memory	
network.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	 natural	 language	 processing	 (emnlp	
2017),	2017.	

[43]	xu	j,	chen	d,	qiu	x,	and	huang	x.	cached	long	short-term	memory	neural	networks	for	document-level	
sentiment	 classification.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 in	 natural	 language	
processing	(emnlp	2016),	2016.			

[44]	 yang	 z,	 yang	 d,	 dyer	 c,	 he	 x,	 smola	 aj,	 and	 hovy	 eh.	 hierarchical	 attention	 networks	 for	 document	
classification.	 in	 proceedings	 of	 the	 conference	 of	 the	 north	 american	 chapter	 of	 the	 association	 for	
computational	linguistics:	human	language	technologies	(naacl-hlt	2016),	2016.	

[45]	yin	y,	song	y,	zhang	m.	document-level	multi-aspect	sentiment	classification	as	machine	comprehension.	
in	proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2017),	2017.			

[46]	 zhou	 x,	 wan	 x,	 xiao	 j.	 attention-based	 lstm	 network	 for	 cross-lingual	 sentiment	 classification.	 in	
proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.			

[47]	 li	 z,	 zhang	 y,	 wei	 y,	 wu	 y,	 and	 yang	 q.	 end-to-end	 adversarial	 memory	 network	 for	 cross-domain	
sentiment	 classification.	 in	 proceedings	 of	 the	 international	 joint	 conference	 on	 artificial	 intelligence	 (ijcai	
2017),	2017.	

[48]	 wiebe	 j,	 bruce	 r,	 and	 o   hara	 t.	 development	 and	 use	 of	 a	 gold	 standard	 data	 set	 for	 subjectivity	
classifications.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	 computational	 linguistics	 (acl	
1999),	1999.		

[49]	socher	r,	pennington	j,	huang	e.h,	ng	a.y,	and	manning	c.d.	semi-supervised	recursive	autoencoders	for	
predicting	 sentiment	 distributions.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 in	 natural	
language	processing	(emnlp	2011),	2011.			

[50]	socher	r,	huval	b,	manning	c.d,	and	ng	a.y.	semantic	compositionality	through	recursive	matrix-vector	
spaces.	in	proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2012),	
2012.	

[51]	socher	r,	perelygin	a,	wu	j.	y,	chuang	j,	manning	c.d,	ng	a.	y,	and	potts	c.	recursive	deep	models	for	
semantic	compositionality	over	a	sentiment	treebank.	in	proceedings	of	the	conference	on	empirical	methods	
on	natural	language	processing	(emnlp	2013),	2013.	

[52]	kalchbrenner	n,	grefenstette	e,	blunsom	p.	a	convolutional	neural	network	for	modelling	sentences.	in	
proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2014),	2014.	

[53]	kim	y.	convolutional	neural	networks	for	sentence	classification.	in	proceedings	of	the	annual	meeting	of	
the	association	for	computational	linguistics	(acl	2014),	2014.																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																														

[54]	dos	santos,	c.	n.,	gatti	m.	deep	convolutional	neural	networks	for	sentiment	analysis	for	short	texts.	in	
proceedings	of	the	international	conference	on	computational	linguistics	(coling	2014),	2014.	

[55]	wang	x,	liu	y,	sun	c,	wang	b,	and	wang	x.	predicting	polarities	of	tweets	by	composing	word	embeddings	
with	 long	 short-term	 memory.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	 computational	
linguistics	(acl	2015),	2015.	

[56]	 graves	 a,	 schmidhuber	 j.	 framewise	 phoneme	 classification	 with	 bidirectional	 lstm	 and	 other	 neural	
network	architectures.	neural	networks,	2005.	

[57]	wang	j,	yu	l-c,	lai	r.k.,	and	zhang	x.	dimensional	sentiment	analysis	using	a	regional	id98-lstm	model.		
in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2016),	2016.	

[58]	 wang	 x,	 jiang	 w,	 luo	 z.	 combination	 of	 convolutional	 and	 recurrent	 neural	 network	 for	 sentiment	
analysis	of	short	texts.	in	proceedings	of	the	international	conference	on	computational	linguistics	(coling	
2016),	2016.	

[59]	 guggilla	 c,	 miller	 t,	 gurevych	 i.	 id98-and	 lstm-based	 claim	 classification	 in	 online	 user	 comments.	 in	
proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[60]	huang	m,	qian	q,	zhu	x.	encoding	syntactic	knowledge	in	neural	networks	for	sentiment	classification.	
acm	transactions	on	information	systems,	2017		

[61]	akhtar	ms,	kumar	a,	ghosal	d,	ekbal	a,	and	bhattacharyya	p.	a	multilayer	id88	based	ensemble	
technique	for	fine-grained	financial	sentiment	analysis.	in	proceedings	of	the	conference	on	empirical	methods	
on	natural	language	processing	(emnlp	2017),	2017.	

[62]	guan	z,	chen	l,	zhao	w,	zheng	y,	tan	s,	and	cai	d.	weakly-supervised	deep	learning	for	customer	review	
sentiment	 classification.	 in	 proceedings	 of	 the	 international	 joint	 conference	 on	 artificial	 intelligence	 (ijcai	
2016),	2016.		

[63]	 teng	 z,	 vo	 d-t,	 and	 zhang	 y.	 context-sensitive	 lexicon	 features	 for	 neural	 sentiment	 analysis.	 in	
proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.			

[64]	yu	j,	jiang	j.	learning	sentence	embeddings	with	auxiliary	tasks	for	cross-domain	sentiment	classification.	
in	proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.			

[65]	zhao	z,	lu	h,	cai	d,	he	x,	zhuang	y.	microblog	sentiment	classification	via	recurrent	random	walk	network	
learning.	in	proceedings	of	the	internal	joint	conference	on	artificial	intelligence	(ijcai	2017),	2017.	

[66]	mishra	a,	dey	k,	bhattacharyya	p.	learning	cognitive	features	from	gaze	data	for	sentiment	and	sarcasm	
classification	using	convolutional	neural	network.	in	proceedings	of	the	annual	meeting	of	the	association	for	
computational	linguistics	(acl	2017),	2017.	

[67]	 qian	 q,	 huang	 m,	 lei	 j,	 and	 zhu	 x.	 linguistically	 regularized	 lstm	 for	 sentiment	 classification.	 in	
proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2017),	2017.	

[68]	dong	l,	wei	f,	tan	c,	tang	d,	zhou	m,	and	xu	k.	adaptive	recursive	neural	network	for	target-dependent	
twitter	 sentiment	 classification.	 in	proceedings	of	the	annual	meeting	of	the	association	for	computational	
linguistics	(acl	2014),	2014.		

[69]	 vo	 d-t,	 zhang	 y.	 target-dependent	 twitter	 sentiment	 classification	 with	 rich	 automatic	 features.	 in	
proceedings	of	the	internal	joint	conference	on	artificial	intelligence	(ijcai	2015),	2015.		

[70]	 tang	 d,	 qin	 b,	 feng	 x,	 and	 liu	 t.	 effective	 lstms	 for	 target-dependent	 sentiment	 classification.	 in	
proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[71]	ruder	s,	ghaffari	p,	breslin	j.g.	a	hierarchical	model	of	reviews	for	aspect-based	sentiment	analysis.	in	
proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2016),	2016.	

[72]	zhang	m,	zhang	y,	vo	d-t.	gated	neural	networks	for	targeted	sentiment	analysis.	in	proceedings	of	aaai	
conference	on	artificial	intelligence	(aaai	2016),	2016.			

[73]	wang	y,	huang	m,	zhu	x,	and	zhao	l.	attention-based	lstm	for	aspect-level	sentiment	classification.	in	
proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.			

[74]	 yang	 m,	 tu	 w,	 wang	 j,	 xu	 f,	 and	 chen	 x.	 attention-based	 lstm	 for	 target-dependent	 sentiment	
classification.		in	proceedings	of	aaai	conference	on	artificial	intelligence	(aaai	2017),	2017.	

[75]	 liu	 j,	 zhang	 y.	 attention	 modeling	 for	 targeted	 sentiment.	 in	 proceedings	 of	 the	 conference	 of	 the	
european	chapter	of	the	association	for	computational	linguistics	(eacl	2017),	2017.	

[76]	tang	d,	qin	b,	and	liu	t.	aspect-level	sentiment	classification	with	deep	memory	network.	arxiv	preprint	
arxiv:1605.08900,	2016.	

[77]	lei	t,	barzilay	r,	jaakkola	t.	rationalizing	neural	predictions.	in	proceedings	of	the	conference	on	empirical	
methods	on	natural	language	processing	(emnlp	2016),	2016.	

[78]	 li	 c,	 guo	 x,	 mei	 q.	 deep	 memory	 networks	 for	 attitude	 identification.	 in	 proceedings	 of	 the	 acm	
international	conference	on	web	search	and	data	mining	(wsdm	2017),	2017.	

[79]	ma	d,	li	s,	zhang	x,	wang	h.	interactive	attention	networks	for	aspect-level	sentiment	classification.	in	
proceedings	of	the	internal	joint	conference	on	artificial	intelligence	(ijcai	2017),	2017.		

[80]	chen	p,	sun	z,	bing	l,	and	yang	w.	recurrent	attention	network	on	memory	for	aspect	sentiment	analysis.	
in	proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[81]	tay	y,	tuan	la,	hui	sc.	dyadic	memory	networks	for	aspect-based	sentiment	analysis.	in	proceedings	of	
the	international	conference	on	information	and	knowledge	management	(cikm	2017),	2017.	

[82]	 katiyar	 a,	 cardie	 c.	 investigating	 lstms	 for	 joint	 extraction	 of	 opinion	 entities	 and	 relations.	 in	
proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2016),	2016.	

[83]	wang	w,	pan	sj,	dahlmeier	d,	and	xiao	x.	recursive	neural	conditional	random	fields	for	aspect-based	
sentiment	 analysis.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 in	 natural	 language	 processing	
(emnlp	2016),	2016.			

[84]	wang	w,	pan	sj,	dahlmeier	d,	and	xiao	x.	coupled	multi-layer	attentions	for	co-extraction	of	aspect	and	
opinion	terms.	in	proceedings	of	aaai	conference	on	artificial	intelligence	(aaai	2017),	2017.	

[85]	li	x,	lam	w.	deep	multi-task	learning	for	aspect	term	extraction	with	memory	interaction.	in	proceedings	
of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[86]	he	r,	lee	ws,	ng	ht,	and	dahlmeier	d.	an	unsupervised	neural	attention	model	for	aspect	extraction.	in	
proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2017),	2017.	

[87]	zhang	m,	zhang	y,	vo	d-t.	neural	networks	for	open	domain	targeted	sentiment.	in	proceedings	of	the	
conference	on	empirical	methods	in	natural	language	processing	(emnlp	2015),	2015.			

[88]	 zhou	 x,	 wan	 x,	 xiao	 j.	 representation	 learning	 for	 aspect	 category	 detection	 in	 online	 reviews.	 in	
proceeding	of	aaai	conference	on	artificial	intelligence	(aaai	2015),	2015.	

[89]	yin	y,	wei	f,	dong	l,	xu	k,	zhang	m,	and	zhou	m.	unsupervised	word	and	dependency	path	embeddings	
for	aspect	term	extraction.	in	proceedings	of	the	international	joint	conference	on	artificial	intelligence	(ijcai	
2016),	2016.	

[90]	xiong	s,	zhang	y,	ji	d,	and	lou	y.	distance	metric	learning	for	aspect	phrase	grouping.	in	proceedings	of	
the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[91]	 poria	 s,	 cambria	 e,	 gelbukh	 a.	 aspect	 extraction	 for	 opinion	 mining	 with	 a	 deep	 convolutional	 neural	
network.	journal	of	knowledge-based	systems.	2016.		

[92]	 ying	 d,	 yu	 j,	 jiang	 j.	 recurrent	 neural	 networks	 with	 auxiliary	 labels	 for	 cross-domain	 opinion	 target	
extraction.	in	proceedings	of	aaai	conference	on	artificial	intelligence	(aaai	2017),	2017	

[93]	irsoy	o,	cardie	c.	opinion	mining	with	deep	recurrent	neural	networks.	in	proceedings	of	the	conference	
on	empirical	methods	on	natural	language	processing	(emnlp	2014),	2014.	

[94]	 yang	 b,	 cardie	 c.	 extracting	 opinion	 expressions	 with	 semi-markov	 conditional	 random	 fields.	 in	
proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2012),	2012.			

[95]	liu	p,	joty	s,	meng	h.	fine-grained	opinion	mining	with	recurrent	neural	networks	and	word	embeddings.	
in	proceedings	of	the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2015),	2015.	

[96]	irsoy	o,	cardie	c.	deep	recursive	neural	networks	for	compositionality	in	language.	in	proceedings	of	the	
annual	conference	on	advances	in	neural	information	processing	systems	(nips	2014),	2014.	

[97]	zhu	x,	guo	h,	sobhani	p.	neural	networks	for	integrating	compositional	and	non-compositional	sentiment	
in	sentiment	composition.	in	proceedings	of	the	conference	of	the	north	american	chapter	of	the	association	
for	computational	linguistics:	human	language	technologies	(naacl-hlt	2015),	2015.	

[98]	yang	b,	cardie	c.	joint	id136	for	fine-grained	opinion	extraction.	in	proceedings	of	the	annual	meeting	
of	the	association	for	computational	linguistics	(acl	2013),	2013.	

[99]	 deng	 l,	 wiebe	 j.	 recognizing	 opinion	 sources	 based	 on	 a	 new	 categorization	 of	 opinion	 types.	 in	
proceedings	of	the	international	joint	conference	on	artificial	intelligence	(ijcai	2016),	2016.	

[100]	chen	c,	wang	z,	lei	y,	and	li	w.	content-based	influence	modelling	for	opinion	behaviour	prediction.	in	
proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[101]	rashkin	h,	bell	e,	choi	y,	and	volkova	s.	multilingual	connotation	frames:	a	case	study	on	social	media	
for	 targeted	 sentiment	 analysis	 and	 forecast.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	
computational	linguistics	(acl	2017),	2017.	

[102]	 mass	 a.	 l,	 daly	 r.	 e,	 pham	 p.	 t,	 huang	 d,	 ng	 a.	 y.	 and	 potts	 c.	 learning	 word	 vectors	 for	 sentiment	
analysis.	 in	 proceedings	 of	 the	 annual	 meeting	 of	 the	 association	 for	 computational	 linguistics	 (acl	 2011),	
2011.		

[103]	bespalov	d,	bai	b,	qi	y,	and	shokoufandeh	a.	sentiment	classification	based	on	supervised	latent	id165	
analysis.	 in	 proceedings	 of	 the	 international	 conference	 on	 information	 and	 knowledge	 management	 (cikm	
2011),	2011.	

[104]	labutov	i,	lipson	h.	re-embedding	words.	in	proceedings	of	the	annual	meeting	of	the	association	for	
computational	linguistics	(acl	2013),	2013.	

[105]	tang	d,	wei	f,	yang	n,	zhou	m,	liu	t,	and	qin	b.	learning	sentiment-specific	word	embedding	for	twitter	
sentiment	classification.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	
(acl	2014),	2014.	

[106]	tang	d,	wei	f,	qin	b,	yang	n,	liu	t,	and	zhoug	m.	sentiment	embeddings	with	applications	to	sentiment	
analysis.	ieee	transactions	on	knowledge	and	data	engineering,	2016.	

[107]	 wang	 l,	 xia	 r.	 sentiment	 lexicon	 construction	 with	 representation	 learning	 based	 on	 hierarchical	
sentiment	 supervision.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	 natural	 language	
processing	(emnlp	2017),	2017.	

[108]	yu	lc,	wang	j,	lai	kr,	and	zhang	x.	refining	word	embeddings	for	sentiment	analysis.	in	proceedings	of	
the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[109]	li	j,	jurafsky	d.	do	multi-sense	embeddings	improve	natural	language	understanding?	in	proceedings	of	
the	conference	on	empirical	methods	in	natural	language	processing	(emnlp	2015),	2015.	

[110]	ren	y,	zhang	y,	zhang,	m	and	ji	d.	improving	twitter	sentiment	classification	using	topic-enriched	multi-
prototype	word	embeddings.	in	proceeding	of	aaai	conference	on	artificial	intelligence	(aaai	2016),	2016.	

[111]	 zhou	 h,	 chen	 l,	 shi	 f,	 huang	 d.	 learning	 bilingual	 sentiment	 word	 embeddings	 for	 cross-language	
sentiment	classification.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	
(acl	2015),	2015.	

[112]	barnes	j,	lambert	p,	badia	t.	exploring	distributional	representations	and	machine	translation	for	aspect-
based	 cross-lingual	 sentiment	 classification.	 in	 proceedings	 of	 the	 27th	 international	 conference	 on	
computational	linguistics	(coling	2016),	2016.	

[113]	 zhang	 w,	 yuan	 q,	 han	 j,	 and	 wang	 j.	 collaborative	 multi-level	 embedding	 learning	 from	 reviews	 for	
rating	 prediction.	 in	 proceedings	 of	 the	 international	 joint	 conference	 on	 artificial	 intelligence	 (ijcai	 2016),	
2016.	

[114]	sharma	r,	somani	a,	kumar	l,	and	bhattacharyya	p.	sentiment	intensity	ranking	among	adjectives	using	
sentiment	 bearing	 word	 embeddings.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	 natural	
language	processing	(emnlp	2017),	2017.	

[115]	 zhang	 m,	 zhang	 y,	 fu	 g.	 tweet	 sarcasm	 detection	 using	 deep	 neural	 network.	 in	 proceedings	 of	 the	
international	conference	on	computational	linguistics	(coling	2016),	2016.		

[116]	joshi	a,	tripathi	v,	patel	k,	bhattacharyya	p,	and	carman	m.	are	word	embedding-based	features	useful	
for	 sarcasm	 detection?	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	 natural	 language	
processing	(emnlp	2016),	2016.	

[117]	poria	s,	cambria	e,	hazarika	d,	and	vij	p.	a	deeper	look	into	sarcastic	tweets	using	deep	convolutional	
neural	networks.	in	proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	
2016.	

[118]	 peled	 l,	 reichart	 r.	 sarcasm	 sign:	 interpreting	 sarcasm	 with	 sentiment	 based	 monolingual	 machine	
translation.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2017),	
2017.	

[119]	ghosh	a,	veale	t.	magnets	for	sarcasm:	making	sarcasm	detection	timely,	contextual	and	very	personal.	
in	proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[120]	van	hee	c,	lefever	e,	hoste	v.	monday	mornings	are	my	fave:)#	not	exploring	the	automatic	recognition	
of	irony	in	english	tweets.	in	proceedings	of	the	international	conference	on	computational	linguistics	(coling	
2016),	2016.	

[121]	chen	wf,	lin	fy,	ku	lw.	wordforce:	visualizing	controversial	words	in	debates.	in	proceedings	of	the	
conference	on	empirical	methods	in	natural	language	processing	(emnlp	2016),	2016.	

[122]	 wang	 z,	 zhang	 y,	 lee	 s,	 li	 s,	 and	 zhou	 g.	 a	 bilingual	 attention	 network	 for	 code-switched	 emotion	
prediction.	in	proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[123]	 zhou	 h,	 huang	 m,	 zhang	 t,	 zhu	 x	 and	 liu	 b.	 emotional	 chatting	 machine:	 emotional	 conversation	
generation	with	internal	and	external	memory.		arxiv	preprint.	arxiv:1704.01074,	2017.	

[124]	 abdul-mageed	 m,	 ungar	 l.	 emonet:	 fine-grained	 emotion	 detection	 with	 gated	 recurrent	 neural	
networks.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	linguistics	(acl	2017),	
2017.	

[125]	felbo	b,	mislove	a,	s  gaard	a,	rahwan	i,	and	lehmann	s.	using	millions	of	emoji	occurrences	to	learn	
any-domain	representations	for	detecting	sentiment,	emotion	and	sarcasm.	in	proceedings	of	the	conference	
on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[126]	gui	l,	hu	j,	he	y,	xu	r,	lu	q,	and	du	j.	a	question	answering	approach	to	emotion	cause	extraction.	in	
proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[127]	poria	s,	cambria	e,	gelbukh	a.	deep	convolutional	neural	text	features	and	multiple	kernel	learning	for	
utterance-level	 multimodal	 sentiment	 analysis.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	
natural	language	processing	(emnlp	2015),	2015.	

[128]	bertero	d,	siddique	fb,	wu	cs,	wan	y,	chan	r.h,	and	fung	p.	real-time	speech	emotion	and	sentiment	
recognition	for	interactive	dialogue	systems.	in	proceedings	of	the	conference	on	empirical	methods	in	natural	
language	processing	(emnlp	2016),	2016.	

[129]	 fung	 p,	 dey	 a,	 siddique	 fb,	 lin	 r,	 yang	 y,	 bertero	 d,	 wan	 y,	 chan	 rh,	 and	 wu	 cs.	 zara:	 a	 virtual	
interactive	dialogue	system	incorporating	emotion,	sentiment	and	personality	recognition.	in	proceedings	of	
the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[130]	wang	j,	fu	j,	xu	y,	and	mei	t.	beyond	object	recognition:	visual	sentiment	analysis	with	deep	coupled	
adjective	and	noun	neural	networks.	in	proceedings	of	the	internal	joint	conference	on	artificial	intelligence	
(ijcai	2016),	2016.	

[131]	 yang	 j,	 sun	 m,	 sun	 x.	 learning	 visual	 sentiment	 distributions	 via	 augmented	 conditional	 id203	
neural	network.	in	proceedings	of	aaai	conference	on	artificial	intelligence	(aaai	2017),	2017.	

[132]	zhu	x,	li	l,	zhang	w,	rao	t,	xu	m,	huang	q,	and	xu	d.	dependency	exploitation:	a	unified	id98-id56	
approach	 for	 visual	 emotion	 recognition.	 in	 proceedings	 of	 the	 internal	 joint	 conference	 on	 artificial	
intelligence	(ijcai	2017),	2017.	

[133]	you	q,	jin	h,	luo	j.	visual	sentiment	analysis	by	attending	on	local	image	regions.	in	proceedings	of	aaai	
conference	on	artificial	intelligence	(aaai	2017),	2017.	

[134]	poria	s,	cambria	e,	hazarika	d,	majumder	n,	zadeh	a,	and	morency	lp.	context-dependent	sentiment	
analysis	in	user-generated	videos.	in	proceedings	of	the	annual	meeting	of	the	association	for	computational	
linguistics	(acl	2017),	2017.	

[135]	 tripathi	 s,	 acharya	 s,	 sharma	 rd,	 mittal	 s,	 and	 bhattacharya	 s.	 using	 deep	 and	 convolutional	 neural	
networks	for	accurate	emotion	classification	on	deap	dataset.	in	proceedings	of	aaai	conference	on	artificial	
intelligence	(aaai	2017),	2017.	

[136]	zadeh	a,	chen	m,	poria	s,	cambria	e,	and	morency	lp.	tensor	fusion	network	for	multimodal	sentiment	
analysis.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 on	 natural	 language	 processing	 (emnlp	
2017),	2017.	

[137]	long	y,	qin	l,	xiang	r,	li	m,	and	huang	cr.	a	cognition	based	attention	model	for	sentiment	analysis.	in	
proceedings	of	the	conference	on	empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[138]	wang	h,	meghawat	a,	morency	lp,	and	xing	e.x.		select-additive	learning:	improving	generalization	in	
multimodal	sentiment	analysis.	in	proceedings	of	the	international	conference	on	multimedia	and	expo	(icme	
2017),	2017.	

[139]	akhtar	ms,	kumar	a,	ekbal	a,	and	bhattacharyya	p.	a	hybrid	deep	learning	architecture	for	sentiment	
analysis.	in	proceedings	of	the	international	conference	on	computational	linguistics	(coling	2016),	2016.	

[140]	 dahou	 a,	 xiong	 s,	 zhou	 j,	 haddoud	 mh,	 and	 duan	 p.	 word	 embeddings	 and	 convolutional	 neural	
network	for	arabic	sentiment	classification.	in	proceedings	of	the	international	conference	on	computational	
linguistics	(coling	2016),	2016.	

[141]	 singhal	 p,	 bhattacharyya	 p.	 borrow	 a	 little	 from	 your	 rich	 cousin:	 using	 embeddings	 and	 polarities	 of	
english	 words	 for	 multilingual	 sentiment	 classification.	 in	 proceedings	 of	 the	 international	 conference	 on	
computational	linguistics	(coling	2016),	2016.	

[142]	 joshi	 a,	 prabhu	 a,	 shrivastava	 m,	 and	 varma	 v.	 towards	 sub-word	 level	 compositions	 for	 sentiment	
analysis	 of	 hindi-english	 code	 mixed	 text.	 in	 proceedings	 of	 the	 international	 conference	 on	 computational	
linguistics	(coling	2016),	2016.	

[143]	 gui	 l,	 xu	 r,	 he	 y,	 lu	 q,	 and	 wei	 z.	 intersubjectivity	 and	 sentiment:	 from	 language	 to	 knowledge.	 in	
proceedings	of	the	international	joint	conference	on	artificial	intelligence	(ijcai	2016),	2016.	

[144]	 wang	 y,	 zhang	 y,	 liu	 b.	 sentiment	 lexicon	 expansion	 based	 on	 neural	 pu	 learning,	 double	 dictionary	
lookup,	and	polarity	association.	in	proceedings	of	the	conference	on	empirical	methods	on	natural	language	
processing	(emnlp	2017),	2017.	

[145]	rekabsaz	n,	lupu	m,	baklanov	a,	hanbury	a,	d  r	a,	and	anderson	l.	volatility	prediction	using	financial	
disclosures	sentiments	with	word	embedding-based	ir	models.	in	proceedings	of	the	annual	meeting	of	the	
association	for	computational	linguistics	(acl	2017),	2017.	

[146]	wang	z,	zhang	y.	opinion	recommendation	using	a	neural	model.	in	proceedings	of	the	conference	on	
empirical	methods	on	natural	language	processing	(emnlp	2017),	2017.	

[147]	 augenstein	 i,	 rockt  schel	 t,	 vlachos	 a,	 bontcheva	 k.	 stance	 detection	 with	 bidirectional	 conditional	
encoding.	 in	 proceedings	 of	 the	 conference	 on	 empirical	 methods	 in	 natural	 language	 processing	 (emnlp	
2016),	2016.			

[148]	du	j,	xu	r,	he	y,	gui	l.	stance	classification	with	target-specific	neural	attention	networks.	in	proceedings	
of	the	internal	joint	conference	on	artificial	intelligence	(ijcai	2017),	2017.	

	

