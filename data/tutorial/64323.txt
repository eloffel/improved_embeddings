8
1
0
2

 

n
a
j
 

1
2

 
 
]
l
c
.
s
c
[
 
 

1
v
6
3
5
9
0

.

1
0
8
1
:
v
i
x
r
a

a survey of id27s evaluation

methods

amir bakarov

institute for system analysis of russian academy of sciences (isa ras),

the national research university higher school of economics,

moscow, russia

amirbakarov at gmail.com

abstract. id27s are real-valued word representations able
to capture lexical semantics and trained on natural language corpora.
models proposing these representations have gained popularity in the
recent years, but the issue of the most adequate evaluation method still
remains open. this paper presents an extensive overview of the    eld of
id27s evaluation, highlighting main problems and proposing
a typology of approaches to evaluation, summarizing 16 intrinsic methods
and 12 extrinsic methods. i describe both widely-used and experimental
methods, systematize information about evaluation datasets and discuss
some key challenges.

1 introduction

id27s, real-valued representations of words produced by distribu-
tional semantic models (dsms), are one of the most popular tools in modern
nlp, but their nature and limitations are still not well understood. one of the
most important questions in the studies of id65 is how to
evaluate the quality of dsms. there is still no consensus in the scienti   c com-
munity about which evaluation method should be used: nlp engineers who are
more interested in dealing with downstream tasks (for instance, semantic role
labeling) usually evaluate the performance of embeddings on such tasks, while
computational linguists exploring the nature of semantics tend to investigate
id27s through experimental methods from cognitive sciences.

the aim of this paper is to systematize and classify all existing approaches
to the task of id27s evaluation, and by    all existing    i mean both
widely-used (and at the same time widely-criticized) and less mainstream, ex-
perimental approaches. in this work, i suggest a hierarchical typology of word
embeddings evaluation methods, highlighting the main problems and systematiz-
ing the existing datasets. i consider this paper to be the most extensive survey
in the    eld of id27s evaluation as of now.

the paper is organized as follows. in section 2 i describe the recent advances
in id27s evaluation and brie   y summarize the ideas proposed in
the key works. sections 3 and 4 are dedicated to the two primary classes of
evaluation methods, extrinsic methods and intrinsic methods. in these sections i

shortly explain how each of the proposed method in each of the sections works.
section 5 concludes the work, and there i propose my thoughts on some future
challenges in the    eld of id27s evaluation.

2 brief history

in this section i propose an overview of previous works dedicated to the task of
id27s evaluation that seem to be important in this    eld. notably, this
section does not cover all the existing studies     more works would be considered
in the following sections devoted directly to overview of evaluation methods.

the    rst work in which id27s evaluation was addressed (there
were no id27s, though, so the similar concepts were called distri-
butional semantics) was carried out in [gri   ths et al., 2007], even before the
id65 gained its main popularity. in 2010 a big survey of
tasks that could be solved with the help of id65 models was
proposed (hence, all of these tasks could be considered as a measure of word em-
beddings performance) [turney and pantel, 2010]. a year after, in 2011, the    rst
comparison of performance of various dsms was published [mcnamara, 2011].
in 2013, the popular id97 tool was released, carrying out novel ap-
proaches to evaluation (like the word analogy task, king     man + woman =
queen) [mikolov et al., 2013a]. in 2014, [baroni et al., 2014] proposed an exten-
sive overview of approaches to id27s evaluation. then, in 2015,
[schnabel et al., 2015] systematized the existing approaches into two major
classes which are extrinsic evaluation and intrinsic evaluation. in 2016, the    rst
workshop on id27s evaluation took place at the annual meeting of
association of computational linguistics (repeval 2016: the first workshop
on evaluating vector space representations for nlp ). this workshop provided
a lot of interesting works and research proposals, with various aspects of word
embeddings evaluation considered. particularly, this workshop helped to high-
light the most important problems in the    eld [faruqui et al., 2016]:

1. obscureness of the notion of semantics. id27s are usu-
ally considered to be    good    if they re   ect our understandings of semantics.
but at the same time, we are not aware whether out understandings are
absolutely correct. moreover, it is unclear which type of relationships be-
tween words id27s should re   ect, because there are many di   er-
ent types of relations between words (like semantic relatedness and semantic
similarity, their de   nitions are notably also quite obscure). it is also not clear
whether the model should be considered    bad    if it takes into account not the
relationships between words that we used to de   ne as a semantic relatedness
or similarity.

2. lack of proper training data. most of the evaluation datasets are not
divided into training and test sets. hence, researchers training the word
embeddings adjust them to the data trying to increase their quality. they
are trying to capture not the actual relationships between words, but the
relationships existing in the data.

3. absence of correlation between intrinsic and extrinsic methods.
performance scores of id27s, when measured with two exist-
ing evaluation approaches (intrinsic and extrinsic), do not correlate between
themselves. it is unclear what class of methods is more adequate.

4. lack of signi   cance tests. statistical signi   cance tests are sometimes
not performed in the key experiments with new distributional models and
evaluation methods. thus, certain results of evaluation proposed in certain
papers are not as correct as it is desirable.

5. the hubness problem. it is unclear how to deal with so-called hubs which
are word vectors representing very frequent words. such vectors are close
to a disproportionately large number of other word vectors, hence, cosine
distances between any two word vectors would probably be noised by the
hubs, and the any evaluation in this case is biased.

among other key papers presented at repeval-2016, i would point at the
overview of the problems of existing id27s evaluation datasets (sub-
jectiveness of rating scales, lack of penalties for overestimating semantic similar-
ity of two dissimilar words, etc.) [avraham and goldberg, 2016] and the overview
of methods of intrinsic id27s evaluation (which, however, does not
address all the methods presented in this survey) [gladkova and drozd, 2016].
also, there were a lot of other signi   cant works outside of repeval, which
raised important questions (the problem of the proper size of the dataset,
uncertainty about which words to include in a representative dataset, etc.)
[jastrzebski et al., 2017]. in 2017, the second workshop was held (repeval 2017:
the second workshop on evaluating vector space representations for nlp ),
but i would not say that works presented there addressed such range of signi   -
cant problems.

thus, by e   orts of a lot of researchers many questions in the task of word
embeddings evaluation were raised. some of these questions were already an-
swered, but much more still remain open. i hope that this work could help to
investigate and resolve the remaining problems.

3 extrinsic evaluation

methods of extrinsic evaluation are based on the ability of id27s
to be used as the feature vectors of supervised machine learning algorithms (like
maximum id178 model) used in one of various downstream nlp task. the
performance of the supervised model (being measured on a dataset for nlp task)
functions as a measure of id27s quality. some researches assume that
id27s showing a good result on one task will show a good result on
others, and the results of id27s on di   erent tasks correlate, de   ning
some kind of a global evaluation score for id65.

id27s probably could be used in almost any nlp task, and cer-
tain researchers (while describing possible options of using id27s in
other downstream tasks) do not mention the task of id27s evalu-
ation. nevertheless, by the de   nition of extrinsic evaluation given above, any

downstream task could be considered as an evaluation method. in this section,
i select and describe the following tasks used by other researchers for extrinsic
evaluation either implicitly (without mentioning the problem of word embed-
dings evaluation) or explicitly (as in [nayak et al., 2016], for instance). i do not
try to mention all existing nlp tasks1, but only the tasks in which i am aware
the id27s were used.

1. noun phrase chunking, to identify noun phrases and their bound-
aries within the sentence (i.e., to mark all the bigrams noun + dependent
word ). [schnabel et al., 2015,turian et al., 2010,collobert et al., 2011]. i am
aware of a dataset for this task prepared on a conll-2000 shared task
[tjong kim sang and buchholz, 2000].

2. id39, to identify types of named entities (names
of organizations, people, brands, etc.) in the sentence and their boundaries
[turian et al., 2010,collobert et al., 2011]. there are several datasets for this
evaluation method, including datasets prepared for the conll-2002 and
conll-2003 shared tasks [tjong kim sang and de meulder, 2003], and a
dataset made for the muc-7 shared task [chinchor and marsh, 1998].

reporting positive or negative polarity of

3. id31, a particular case of a text classi   cation prob-
should be marked with a binary la-
lem, when a text
bel
sentiment
[schnabel et al., 2015,tsvetkov et al., 2015]. there are certain user reviews
datasets (for example, movie reviews [maas et al., 2011]) that could be used
for evaluation on this task.

fragment

text

the

4. shallow syntax parsing, to decompose the sentence into phrase groups
(not only noun phrases, but also verb phrases, adjective phrases, etc.)
[bansal et al., 2014,k  hn, 2016]. in some papers a similar task is called as
parse tree level 0 construction [collobert et al., 2011], in some pa-
pers the task is extent to the full dependency or constituent parsing task
[andreas and klein, 2014]. any datasets constructed for evaluation on such
parsing task (like [marcus et al., 1993]) could be used for evaluation.

5. id14, to identify thematic (semantic) roles of ar-
guments for various predicates within the sentence [collobert et al., 2011].
some researchers formulate the task as a classi   cation task which is to clas-
sify each sentence in a set by the thematic role of a speci   ed word which
occurs in an each sentence in the set [ettinger et al., 2016a]. there are sev-
eral datasets that could be used for the    rst type of the id14
task, like the proposition bank [palmer et al., 2005].

6. negation scope. this task also could be considered a text classi   cation
task. it is to identify whether a speci   ed action in a sentence determined is a
negation or not (such actions are contained in each sentence in a certain set)
[ettinger et al., 2016a]. however, there are no existing datasets that could
be used for evaluation on this task.

7. part-of-speech tagging, to identify part of speech of each word
in the sentence [collobert et al., 2011]. part-of-speech datasets could be

1 like at https://aclweb.org/aclwiki/state_of_the_art

considered suitable datasets for this task,
[toutanova et al., 2003].

like the stanford treebank

8. text classi   cation, which is in general to mark a text fragment with
a label depending on its content: for example, categorizing sport news
based on the type of sport activity they are about (football or basketball)
[tsvetkov et al., 2015]; there exist various datasets with text classi   ed by
their semantics, for instance, 20 newsgroups 2.

9. metaphor detection is another classi   cation task which is to identify
whether the speci   ed phrase (like adjective-noun or subject-verb-object)
is metaphorical or literal [tsvetkov et al., 2014,tsvetkov et al., 2015]. i am
aware of two datasets for this task, which are trope finder dataset 3 and
the dataset proposed by yulia tsvetkov in a study dedicated to metaphor
detection [tsvetkov et al., 2014].

10. paraphrase detection (also called duplicate detection, paraphrase
identi   cation, record linkage, approximate string matching, text-
to-text similarity detection) is to determine whether two text fragments
are paraphrases of each other (however, the notion of paraphrase is not
so clear, and di   erent researchers de   ne this relation in di   erent ways).
the pair of text fragments could be labeled by a score reporting degree
of text similarity or by a binary mark reporting the existence of a similar-
ity [baumel et al., 2016,bakarov and gureenkova, 2017]. there are several
datasets for this task, including the microsoft research paraphrase corpus
[dolan and brockett, 2005] and the quora question pairs dataset 4.

11. id123 detection (also called natural language infer-
ence task). the task is in a some way similar to the previously mentioned
paraphrase detection task since it is also to label a pair of two text frag-
ments. but this task, however, is to identify whether one of these frag-
ments is a continuation of another (so the relationship is not bi-directional)
[baumel et al., 2016]. among datasets designed speci   cally for evaluation
on id123 detection task one can mention the sentences involv-
ing compositional knowledge dataset [marelli et al., 2014] and the stanford
natural language id136 corpus [bowman et al., 2015]. notably, certain
researchers consider a very similar task suitable for id27s evalua-
tion which is to pick one of two possible one-sentence continuations for a short
story of several sentences (story cloze task). there is a special dataset for
this type of tasks named story cloze dataset [mostafazadeh et al., 2016].

12. input for arti   cial neural networks. i put in a separate category the
methods in which id27s are used as initial weights in the input
layers for various types of arti   cial neural networks that are later employed
to resolve downstream tasks like machine translation, morphological analysis
and id38. authors of this idea did not consider the problem
of id27s evaluation, but they compared di   erent types of dsms,

2 http://qwone.com/~jason/20newsgroups
3 http://www.cs.sfu.ca/~anoop/students/jbirke
4 https://data.quora.com/first-quora-dataset-release-question-pairs

so i believe such a method could also be used as a framework for evaluation
[kocmi and bojar, 2017].

however, i argue that there are signi   cant issues in this group of methods.
of course, extrinsic evaluation has certain advantages, and in the cases where
id27s are supposed to be used only to resolve a speci   c downstream
task, evaluation of performance of supervised model on this task will give the
most adequate score of id27s performance. but extrinsic evaluation
fails if the embeddings that one wants to evaluate are trained to serve in a wide
range of di   erent tasks since id27s performance scores in various
downstream tasks do not correlate between themselves (as it was hypothesized
earlier) [schnabel et al., 2015]. however, this fact is not surprising, since di   erent
downstream tasks di   er very much, and they use completely di   erent features
of id27s (like, embeddings which are used in a pos-tagging task
should consider words with the same part of speech to be similar independently
of their semantics). hence, no global evaluation score for id27s could
be obtained through extrinsic evaluation methods.

additionally, some researchers also highlight (among other shortcomings of
extrinsic methods) high complexity of creating gold standard downstream tasks
datasets (and, after all, in such datasets the possible subjectivity of human
assessors is always an issue).

i argue that the ideas of extrinsic evaluation could be useful if one wants to
highlight advantages of a certain dsm (showing its performance on a speci   c
downstream task), but, due to the lack of performance correlation on di   erent
downstream tasks, such techniques can not be used as an absolute metric of
id27s quality.

4 intrinsic evaluation

methods of intrinsic evaluation are experiments in which id27s
are compared with human judgments on words relations. manually created sets
of words are often used to get human assessments, and then these assessments
are compared with id27s (this method of collecting the judgments is
called absolute intrinsic evaluation). the collection of assessments could be
conducted either in the laboratory on a limited set of examinees (judgments
collected in-house) or on crowd-sourcing web platforms like mechanical turk,
attracting an unlimited number of participants (judgments collected through
crowd-sourcing) [liza and grzes, 2016].

sometimes the assessors are asked to evaluate the quality of id27s
directly, for instance, when di   erent models produce di   erent judgments on word
relations, and the task of an assessor is to tell which model works better (such
a method is called comparative intrinsic evaluation) [schnabel et al., 2015].
comparative intrinsic estimation allows not to estimate the absolute quality of
embeddings, but to    nd the most adequate embeddings in a given set.

absolute intrinsic evaluation uses in vivo experiments to obtain human judg-
ments from assessors. the process of collecting the same data from word em-
beddings could be called in vitro experiment. the evaluation process is that two
datasets obtained through these experiments are compared, and an aggregated
estimate (for example, spearman correlation coe   cient) is calculated. such an
estimate could be used as an absolute measure of the quality of embeddings
since it reports the similarity of lexical semantics inferred by embeddings to the
lexical semantics determined by humans.

most of methods of absolute intrinsic evaluation are designed to collect as-
sessments which are results of conscious processes in a human brain (in other
words, assessors have time to think about their answers). hence, there is a prob-
ability that such answers are biased by certain subjective factors (for example,
due to the absence of a clear de   nition of meaning every person interprets words
relations in her own way, introducing the variability to the estimates). and it is
not clear if the conscious assessments are really able to report the structure of
semantics in a natural language.

some researchers argue that such structure lies somewhere in the so-called
subconscious level of cognition [kutas and federmeier, 2011]. if one could collect
assessments directly from this level, then the evaluation probably will be far less
biased. the attempts of collecting such data are becoming common in the    eld of
id27s evaluation, and the evaluation becomes more interdisciplinary.
novel approaches to evaluation are based on using various neuroimaging methods
which were previously used only in a    eld of psycholinguistics.

to this end, i would like to propose a new extensive typology of intrinsic word
embeddings evaluation methods. in this typology i divide all existing methods
into methods of intrinsic conscious evaluation and methods of intrinsic
subconscious evaluation. this typology is inspired by the classi   cation of data
collection methods in psycholinguistic research which proposes o   -line methods,
in which examinees have time to think about decisions, and on-line methods, in
which re   ective responses are being collected.

however, i am also aware of other types of intrinsic evaluation methods that
do not fall into these categories. such methods are based not on a comparison
with results of in vivo experiments, but on a comparison with knowledge bases,
semantic networks and thesauri manually constructed by professional linguists
and ontology engineers. i put these methods into another class, methods of
intrinsic thesaurus-based evaluation.

additionally, i argue that there is one more possible category of evaluation
methods which is based neither on in vivo experiments nor on knowledge bases.
this category proposes methods that are based on a comparison with data under-
lying in a language itself. such data could be found, for instance, in graphematic
representations of words, in speech sound signals or in frequency of occurrence of
a pair of words in a corpus. i call them methods of intrinsic language-driven
evaluation.

it is not obvious why the last three categories of the methods (proposed by
myself) can be called evaluation methods at all, and i would like to explain this.

in other works researchers tend to use the notion of exploration of word em-
beddings for such experiments since it is unclear whether the data they use as
the gold standard really contains information about lexical semantics. but is the
existence of lexical semantics information contained in consciously assessed word
similarity datasets truly clear? if one calls one type of experiments exploration,
and calls another type evaluation, what is that threshold of evaluation method
representativeness that would allow us to    t some methods into the evaluation
category, and others into the exploration category? and then what is the de   ni-
tion of an evaluation method in the    eld of id65? after all, if
the notion of word meaning could not be even de   ned properly, how the notion
of its modeling evaluation could be de   ned?

my message is that i suggest to call a method of id27s evaluation
any technique of    nding embeddings correlation with any data that hypotheti-
cally could carry information about lexical semantics. of course, the representa-
tiveness of such method will be questioned depending on the degree of plausibility
of the hypothetical amount of necessary information in this data. but now we
are not able to correctly evaluate this amount, and i argue that one should not
say    this is not evaluation    just because of the lack of our capabilities to properly
estimate the actual amount of semantic information contained in our data.

to this end, i have highlighted four classes of absolute intrinsic evaluation:

1. methods of conscious evaluation;
2. methods of subconscious evaluation;
3. thesaurus-based methods;
4. language-driven methods.

now i am going to discuss more properly the design of each evaluation method
proposed by other researchers which i    t into one of these classes.

4.1 conscious intrinsic evaluation

4.1.1. word semantic similarity method is based on an idea that the dis-
tances between words in an embedding space could be evaluated through the hu-
man heuristic judgments on the actual semantic distances between these words
(e.g., the distance between cup and mug de   ned in an continuous interval 0, 1
would be 0.8 since these words are synonymous, but not really the same thing).
the assessor is given a set of pairs of words and asked to assess the degree of
similarity for each pair. the distances between these pairs are also collected in a
id27s space, and the two obtained distances sets are compared. the
more similar they are, the better are embeddings [baroni et al., 2014].

this method is one of the most popular methods for evaluation nowadays.
its roots go back to 1965 when the    rst experiments on human judgments on
word semantic similarity were conducted to test the distributional hypothesis
[rubenstein and goodenough, 1965] (in 1978 a similar work was carried out in
[osgood et al., 1978]). however, despite the strong psycholinguistic background
of this method, it is one of the most frequently criticized in the community.

of

in

the

the

bulk

1999

(even

before

notion

critique

factor of

subjective

there are more

than 50 potential

of id27s

[friedman and amoo, 1999]. the

factors, which could introduce bias

to the method of word similarity has

started no later
ap-
than
criticism addresses
peared)
such judgments which i mentioned earlier
the
[faruqui et al., 2016,batchkarov et al., 2016]. by
several
linguistic, psychologi-
researchers,
in the assessments
cal and social
[gladkova and drozd, 2016].
in some papers the problem of connotative
words was also addressed: while denotative words (neutral common notions)
do not address any assessors    associations, connotative words tend to cause
subjectivity based on cultural or personal criteria [liza and grzes, 2016]). other
researchers also criticize the ambiguity of the task, since di   erent experiments
tend to propose di   erent de   nitions of semantic similarity: some researchers
de   ne it as co-hyponymy (like the relation between the words machine and
bicycle) [turney and pantel, 2010], while others de   ne it as synonymy (like in
a word pair mug and cup) [hill et al., 2016].

claims

of

it was also argued that the notion of semantic similarity inherits not only
semantic connections of words, but also some morphological and graphematic
features of word representations [kiela et al., 2015]. among other criticized fea-
tures of word semantic similarity method are the lack of correlation between
these human assessments and the performance of id27s on extrin-
sic methods [chiu et al., 2016,tsvetkov et al., 2015] (other researchers, however,
explain this by the fact that such assessments are not su   ciently representa-
tive [camacho-collados and navigli, 2016]), low inter-rater agreement between
annotators [hill et al., 2016], the factor of assessors getting tired when annotat-
ing a large number of pairs [bruni et al., 2014], poor ability of numerical labels
to fully describe all types of relations between words (it is suggested that it
will be better to describe the degree of word similarity in a natural language
[milajevs and gri   ths, 2016]), and the mis-conduction of thematic roles rela-
tions [erk, 2016].

systematizing the results accumulated by other researchers, i propose all the
datasets designed for evaluation in the task of word semantic similarity ranked
by their size. notably, di   erent datasets use di   erent notions of lexical seman-
tic similarity, so the same embeddings could have di   erent results on di   erent
datasets:

1. simverb-3500, 3 500 pairs of verbs assessed by semantic similarity (that
means that pairs that are related but not similar have a fairly low rating)
with a scale from 0 to 4 [gerz et al., 2016].

2. men (acronym for marco, elia and nam), 3 000 pairs assessed by semantic

relatedness with a discrete scale from 0 to 50 [bruni et al., 2014].

3. rw (acronym for rare word), 2 034 pairs of words with low occurrences
(rare words) assessed by semantic similarity with a scale from 0 to 10
[luong et al., 2013].

4. siid113x-999, 999 pairs assessed with a strong respect to semantic similarity

with a scale from 0 to 10 [hill et al., 2016].

5. semeval-2017, 500 pairs assessed by semantic similarity with a scale from
0 to 4 prepared for the semeval-2017 task 2 (multilingual and cross-
lingual semantic word similarity) [camacho-collados et al., 2017]. notably,
dataset contains not only words, but also collocations (e.g. climate change).
6. mturk-771 (acronym for mechanical turk), 771 pairs assessed by semantic

relatedness with a scale from 0 to 5 [halawi et al., 2012].

7. wordsim-353, 353 pairs assessed by semantic similarity (however, some
researchers    nd the instructions for assessors ambiguous with respect to
similarity and association) with a scale from 0 to 10 [finkelstein et al., 2001].
8. mturk-287, 287 pairs assessed by semantic relatedness with a scale from

0 to 5 [radinsky et al., 2011].

9. wordsim-353-rel, 252 pairs, a subset of wordsim-353 containing no

pairs of similar concepts [agirre et al., 2009].

10. wordsim-353-sim, 203 pairs, a subset of wordsim-353 containing similar
or unassociated (to mark all pairs that receive a low rating as unassociated)
pairs [agirre et al., 2009].

11. verb-143, 143 pairs of verbs assessed by semantic similarity with a scale

from 0 to 4 [baker et al., 2014].

12. yp-130 (acronym for yang and powers), 130 pairs of verbs assessed by

semantic similarity with a scale from 0 to 4 [yang and powers, 2006].

13. rg-65
assessed
[rubenstein and goodenough, 1965].

(acronym for rubenstein
by

semantic

similarity with

and goodenough),

65
from 0

pairs
to
4

a

scale

14. mc-30 (acronym for miller and charles), 30 pairs, a subset of rg-65
which contains 10 pairs with high similarity, 10 with middle similarity and
10 with low similarity [miller and charles, 1991]. also, there is a subset of
mc-30 called mc-28 which excludes 2 pairs not represented in id138
[resnik, 1995].

4.1.2. word analogy method (in certain works also called analogical rea-
soning,
linguistic regularities and word semantic coherence) is the second
most popular method of id27s evaluation. it is based on the
idea that arithmetic operations in a word vector space could be predicted
by humans: given a set of three words, a, a    and b, the task is to iden-
tify such word b    that the relation b:b    is the same as the relation a:a   
[turian et al., 2010,pereira et al., 2016,baroni et al., 2014]. for instance, one
has words a = p aris, b = f rance, c = m oscow. then the target word would
be russia since the relation a : b is capital : country, so one needs need to    nd
the capital of which country is moscow.

the main criticism to this method addresses the lack of a precise evaluation
metric. if in the word semantic similarity task the cosine distance between word
vectors was intuitively adequate, then in this task the adequateness of such
metric for relationship transfer is questioned. i am aware of three metrics used
in the word analogy task:

    3cosadd (and a similar metric 3cosmul ) proposed in the original id97
paper is based on arithmetic operations in vector space (addition and mul-
tiplication of cosine distances) [mikolov et al., 2013b].

    pairdir modi   es 3cosadd, taking into account the direction of the resulting

vectors in these operations [levy and goldberg, 2014].

    analogy space evaluation metric compares the distances between words di-

rectly without    nding the nearest neighbors [che et al., 2017].

i also provide a list of datasets which could be used for evaluation on this
task. as [gladkova et al., 2016] notes, datasets designed for semantic relation
extraction task could also be used to compile a word analogy set. in this case, it
also worth looking at the lexical relation set which is a compilation of di   erent
semantic relation datasets including bless [baroni and lenci, 2011] (12 458
word pairs with a relation comprising 15 relation types) [vylomova et al., 2015]
and the semantic neighbors set (14 682 word pairs with a relation comprising 2
relation types, meaningful and random) [panchenko et al., 2013].

1. wordrep, 118 292 623 analogy questions (4-word tuples) divided into 26
semantic classes, a superset of google analogy with additional data from
id138 [gao et al., 2014].

2. bats (acronym for bigger analogy test set), 99 200 questions divided
into 4 classes (in   ectional morphology, derivational morphology,
lexico-
graphic semantics and encyclopedic semantics) and 10 smaller subclasses.
[gladkova et al., 2016].

3. google analogy (also called semantic-syntactic word relationship
dataset), 19 544 questions divided into 2 classes (morphological relations
and semantic relations) and 10 smaller subclasses (8 869 semantic questions
and 10 675 morphological questions) [mikolov et al., 2013a].

4. semeval-2012, 10 014 questions divided into 10 semantic classes and 79
subclasses prepared for the semeval-2017 task 2 (measuring degrees of
relational similarity) [jurgens et al., 2012].

5. msr (acronym for microsoft research syntactic analogies), 8 000 questions

divided into 16 morphological classes [mikolov et al., 2013b].

6. sat (acronym for scholastic aptitude test), 5 610 questions divided into

374 semantic classes [turney et al., 2003].

7. jair (acronym for journal of arti   cial intelligence research), 430 ques-
tions divided into 20 semantic classes. notably, dataset contains not only
words but collocations (like solar system) [turney, 2008].

4.1.3. thematic    t method evaluates the ability of a model to separate dif-
ferent thematic roles of arguments of a predicate (also called selectional prefer-
ence [baroni et al., 2014]). the idea is to    nd how well id27s could
   nd most semantically similar noun for a certain verb that is used in a certain
role. for humans, a certain verb could cause a person to expect that a certain
role must be    lled with a certain noun (e.g., for the argument to cut the most
expected argument in the object role is pie) [sayeed et al., 2016]. experiments

propose assessments of adequacy score of the tuple verb, noun, thematicrole (for
example, people eat is more common phrase than eat people, so the pair people
and eat would have the higher score) [vandekerckhove et al., 2009].

some researchers consider another variation of this method, proposing the
task of assessing a pair of words n (noun) and v (verb) by the most frequent
role in which n used with v (e.g., pair people, eat would be classi   ed as the
subject since it is more common to use people as a subject with that verb)
[baroni and lenci, 2010].

in my opinion, the main problem of this method lies in two of its features.
first, it needs a corpus annotated with thematic roles. second, it is unclear
which method of obtaining an embedding for a thematic role to distinguish
di   erent roles of the argument is the most adequate. some researchers propose a
method of vectorization of    slots    for certain thematic roles, which are obtained
by averaging several most frequent nouns encountered in a given role     but
applicability of such method is not obvious [baroni and lenci, 2010].

the following datasets could be used for evaluation with the thematic    t

task:

1. mstnn (abbreviation mentioned in [sayeed et al., 2016]), 1 444 verb-

object-subject pairs [mcrae et al., 1997].

2. gds (acronym for greenberg, sayeed and danberg), 720 verb-object pairs.
the dataset is additionally divided into a subsample containing only pol-
ysemous verbs (gds-poly) and a subsample containing monosemous verbs
(gds-mono) [greenberg et al., 2015].

3. f-inst & f-loc (acronym for ferretti-instrument and ferretti-location),
522 verbs pairs which are split to a subset of 248 verbs with associated
instruments (f-inst ) and a subset of 274 verbs with associated locations
(f-loc) [ferretti et al., 2001].

4. p07 (acronym for pado), 414 verb-object-subject pairs [pad  , 2007].
5. up (acronym for ulrike and pado), 211 verb-noun pairs, the set of roles is

unlimited [pad   and lapata, 2007].

6. mt98 (acronym for mcrae and tanenhaus), a subset of 200 verbs from
mstnn where each verb has two nouns, one is a good subject, but a bad ob-
ject, and one which is a good object, but a bad subject [mcrae et al., 1998].

4.1.4. concept categorization method (also called word id91) evaluates
a id27s space to be clustered. given a set of words, the task is to split
it into subsets of words belonging to di   erent categories (for example, for words
dog, elephant, robin, crow the    rst two make one cluster which is mammals
and the last two form another second cluster which is birds; the cluster name
is not necessary to be formulated) [baroni et al., 2014]. the amount of clusters
should be de   ned. possible critique of such method could address the question of
either choosing the most appropriate id91 algorithm or choosing the most
adequate metric for evaluating id91 quality.

below i enumerate datasets which could be used as a gold standard for the

task of word categorization:

1. bm (acronym for battig and montague), 5 321 words divided into 56 cate-

gories [baroni et al., 2010].

2. ap (acronym for almuhareb and poesio), 402 words divided into 21 cate-

gories [almuhareb, 2006].

3. bless (acronym for baroni and lenci evaluation of semantic spaces),
200 words divided into 27 semantics classes [baroni and lenci, 2011]. de-
spite the fact that bless was designed for another type for evaluation,
it is also possible to use this dataset in a word categorization task, as in
[jastrzebski et al., 2017].

4. esslli-2008 (acronym for the european summer school in logic, lan-
guage and information), 45 words divided into 9 semantic classes (or 5 in less
detailed categorization); the dataset was used in a shared task on a lexical
semantics workshop on essli-2008 [baroni et al., 2008].

4.1.5. synonym detection method, such as the word semantic similarity
method, tries to evaluate the ability of id27s to form a vector space
with predictable distances between words, but it does not propose an absolute
degree of similarity: it is based on the idea that word similarity could be mea-
sured through    nding the most similar word relative to a set of other words.
given a word a and a set k = b1, b2, b3, the task is to    nd bi which is the most
synonymous (semantically similar in terms of the word semantic similarity task)
to a [baroni et al., 2014]: for example, for the target levied one must choose
between imposed (correct), believed, requested and correlated. the task of a
dsm is to    nd the word vector with the smallest distance to the vector of the
speci   ed word.

taking into account all the criticism of the word semantic similarity method,
moving from the absolute measure to the relative measure could probably ex-
clude a lot of problems of this task (score bias, lack of assessments interpretabil-
ity, etc). on the other hand, the creation of a dataset for evaluation in this
task is more complicated and raises certain new questions (for example, how to
properly choose the words to form the set k).

datasets that could be used for evaluation on this task are listed below. they
are presented in a form of 5-word tuples in which one word is a target word, and
4 words are potential synonyms where the only one is a correct answer.

1. rdwp (acronym for reader   s digest word power game), 300 synonym

questions (5-word tuples) [jarmasz and szpakowicz, 2004].

2. toefl (acronym for test of english as a foreign language), 80 questions

[landauer and dumais, 1997].

3. esl (acronym for english as a second language), 50 questions

[turney, 2001].

4.1.6. outlier word detection method evaluates the same feature of word em-
beddings as the word categorization method (it also proposes id91), but the
task is not to divide a set of words into certain amount of clusters, but to identify

a semantically anomalous word in an already formed cluster (for example, for
a set {orange, banana, lemon, book, orange} which are mostly fruits, the word
book is the outlier since it is not a fruit) [camacho-collados and navigli, 2016].
some researchers propose a very similar method called evaluation of coher-
ence in semantic space. the idea of this method is, given a set of three words
    word a, the two words a1 and a2 which are the closest to a in an embedding
space are found,     a word b is chosen randomly from the model   s dictionary
(this word probably would not be so semantically similar to a), and the task
of a human assessor is to correctly identify b (the outlier) in the set a, a1, a2, b
[schnabel et al., 2015]. the more words are identi   ed correctly, the better is the
model.

there are two publicly available datasets for evaluation on this task:

1. 8-8-8 dataset, 8 clusters, each is represented by a set of 8 words with 8

outliers [camacho-collados and navigli, 2016].

2. wordsim-500, 500 clusters, each is represented by a set of 8 words with 5

to 7 outliers [blair et al., 2016].

4.2 subconscious intrinsic evaluation

4.2.1. semantic priming evaluation method is based on the same-name psy-
cholinguistic behavioral experiment. it hypothesizes that a human reads a word
faster if it is preceded by a semantically related word. the idea of an experiment
is to measure the time of reading a speci   ed word a (called the target word ) in
case it occurs after a word b1 and in case it occurs after a word b2. if the reading
time of the word b1 is lower than the reading time of the word b2, than the
word b1 is claimed to be semantically related to a (b1 is called prime, or prime
[ettinger and linzen, 2016,auguste et al., 2017].
word,
the time of reading could be obtained with the help of eye-tracking
[mandera et al., 2017,lapesa and evert, 2013],
or
[jones et al., 2006,herda  delen et al., 2009,mcdonald and brew, 2004].

stimulus word )

safe-paced

reading

or

i am aware of only one dataset that could be used for evaluation on the
semantic priming task. it is the semantic priming project, containing 6 337
pairs of words. the data is collected from 768 subjects for 1 661 target words.
every word pair presented in four versions:    rst, depending of the time inter-
val on the demonstration of the target and non-target words which is 70 and
200ms (this interval is called stimulus onset asynchronies, soa), and, second,
depending on the task for the priming, naming task or lexical decision task
[hutchison et al., 2013].

4.2.2. neural activation patterns when a person reads words, their mean-
ings are hypothetically re   ected in some patterns in her brain. thus, such pat-
terns could be used as input data for id27s. however, the consistency
of such brain data is questioned since the neural activation patterns do not cor-
relate in a large number of subjects (because the size and structure of the brain

di   er in di   erent subjects). another issue is that it is not clear to what extent
these patterns have to do with lexical semantics, and with other linguistic data
contained in words, like the number of characters, stress location, the number
of syllables, etc. in the end, even the state-of-the-art techniques of neuroimag-
ing are expensive, and it is unlikely that in the near future brain activity data
can completely replace statistical corpus data. nevertheless, some researchers
propose that explorations of neural activation patterns could help to shed light
on the nature of semantics, and therefore, to consistently evaluate the existing
methods of id27s evaluation.

    functional magnetic resonance imaging (fmri) evaluation method
is based on using as a gold standard the data of the same-name neuroimag-
ing experiment which measures changes associated with blood    ow in certain
parts of the brain by    xating regions of the blood    ow at certain time inter-
vals (once a second, for instance). the idea is that the blood    ow and the
neuronal activation patterns correlate, so one could identify parts of brain
which are activated. in the    eld of neurolinguistics, reading or listening the
text is usually considered to be a stimulus for this activity. the obtained
data is presented as a set of voxels reporting the level of neuronal activity in
di   erent small parts of brain. it is not clear how to obtain data on reading
single words, since the minimum time interval on    xating blood    ow is about
1 second; some researchers try to train a regression model to compute the
average brain activation vectors for each word or to use aggregate statistics
to obtain vector representations of fmri data using it is as a gold stan-
dard [huth et al., 2016,s  gaard, 2016,abnar et al., 2017]. one could try to
use studyforrest [hanke et al., 2014] dataset which o   ers data on listening
to the audio track of the    forrest gump    movie in german, or the word pro-
cessing dataset which contains readings for various natural language words
on english [duncan et al., 2009].

    electroencephalography (eeg) evaluation method is another method
based on using neuroimaging data as a gold standard. electroencephalog-
raphy records the electrical activity of the brain, and the idea is that the
amplitude of the impulses in the brain that occur on words (such response
is called n400, it is an early response elicited by every word of a sentence)
stores information about lexical semantics since the interpretation of the re-
sponse is usually generalized by the hypothesis that the worse the word    ts
to the context (which could be both sentence context and word context), the
higher is the amplitude of the signal. the amplitude di   erences of a tuple
of words is able be simulated through the average cosine distances of word
embeddings, so it is hypothetically could be used as a gold standard data for
evaluation [parviz et al., 2011,ettinger et al., 2016b]. however, to this mo-
ment i am not aware of any publicly available eeg dataset that could be
used for evaluation on this task.

4.2.3. eye movement data evaluation method is based on using as a gold
standard the data of human eye movement obtained. such data could be ob-

tained through instrument called eye-tracker tracks the movement of a pupil
and a time of    xation on certain words while a person reads text from the com-
puter screen, and such data hypothetically could carry some information about
lexical semantics. the eye-tracker assigns to each word a set of features report-
ing characteristics of its reading: how many milliseconds the gaze was    xated
on this word, how many times the gaze returned to it, etc. it is assumed that
such feature sets could be compared with id27 vectors by converting
them to the vectors of aggregate statistics, and hypothetically the correlation
between such vectors and id27s (for instance, on predicting k nearest
neighbors to a certain word) could report the quality of a dsm [s  gaard, 2016].
i am aware only of two publicly available english eye movement datasets
that one could use in their experiments. the    rst is the the provo corpus
[luke and christianson, 2017] which consists of data of reading 55 paragraphs
from 84 native speakers. this dataset could be converted in a list of 1 185 words
each of which is associated with a set of 26 eye movement features.

the second dataset is the ghent eye-tracking corpus (geco)
[cop et al., 2017] containing data of reading 5 000 sentences from monolingual
and bilingual english speakers (33 participants overall). after converting one
could obtain a dataset of 987 words, each associated with 48 features.

4.3 thesaurus evaluation

4.3.1. thesaurus vectors evaluation method (called qvec in the original
paper [tsvetkov et al., 2015]) is based on the idea that id27s can
be evaluated with the vectors of the inverted index of a collection of documents
(   thesaurus vectors   ) in a which each is responsible for a certain category of
human knowledge, like super-senses in id138 (e.g. food, animal, etc). the di-
mensionality of the thesaurus vectors is equal to the size of collection, and each
component reports the number of occurrences of the word in a certain document;
if the collection is too big, it is possible to use some kind of dimensionality reduc-
tion and map one component of an embeddings vector to multiple components
of thesaurus vectors (or vice versa if the collection is too small). so, the gold
standard is represented by the thesaurus vectors.

i believe that any set of documents which claims to contain comprehensive
set of knowledge categories, can be used for evaluation (not only the conceptual
thesaurus of id138 super-senses). the most extensive one is wikipedia, which
is used for document vectorization with a similar thesaurus vector-based tech-
nique called explicit semantic analysis [gabrilovich and markovitch, 2007]. it
is usually applied in cross-language information retrieval.

4.3.2. dictionary de   nition graph evaluation method is based on the idea
that co-occurrences of words in dictionary de   nitions could carry information
about their relationships [acs and kornai, 2016]. a digraph of a set of dictio-
naries in which the nodes are words could be constructed, and the value of the
edge connecting the word a to the word b is equal to the number of occurrences of

the word b in all de   nitions of the word a. this graph could be transformed into
matrix, and for each word a dictionary vector could be obtained. such vectors
could be used as a gold standard in id27s evaluation.

another variation of this graph exists, when the weights on the edges are
not the frequencies of occurrences, but the numbers of using b as a head in the
dependency syntax tree (such an idea can help to identify similarities based on
phrases like a cat is an animal ).

so, any dictionary can be used as a gold standard dataset for this task.

4.3.3. cross-match test evaluation method is based on the same-name tech-
nique of    nding similarity between two high-dimensional sets used to compare
blood samples in medicine based on determining whether these two sets are sam-
pled from the same distribution. being applied to id27s evaluation,
this method claimed to measure statistical signi   cance of a model. using it on
two sets of word vectors trained on the same corpus, one could compute the
correlation between these sets using the cross-match test. if the correlation is
low, then the two compared models probably use di   erent features of the corpus,
so it is probably a good result [gurnani, 2017].

4.3.4. semantic di   erence evaluation method is based on using the char-
acterizing words of distinctive features (called attributes). each word in a pair
is associated with a certain set of attributes, and the distance between words
is calculated as the di   erence between the cartesian product multiplied by the
attributes of the word vectors. it is assumed that it is possible to select a pair
of attributes of the same category for each pair of non-abstract words (e.g. the
category could be size, and the distinctive attributed could be big and small ).
[krebs and paperno, 2016].

there is a certain amount of databases where words are associated with
sets of di   erent attributes. one of examples of such bases is a previously men-
tioned bless dataset which contains 200 pairs of words (for example, for the
[motorcycle, moped] word pair these are the two sets of attributes: [large, small]
and [f ast, slow]) [baroni and lenci, 2011]. another example is feature norms
dataset containing 24 963 pairs of words, for which a least one pair of distinctive
features is selected (for example, for the pair [airplane, helicopter] the existence
of wings is selected) [krebs and paperno, 2016].

4.3.5. semantic networks evaluation method uses manually constructed
id13s (semantic networks) as a gold standard. in semantic networks,
the words are organized in a graph in accordance with their semantic distinctive
features based on judgments of teams of professional linguists. semantic net-
works also feature a measure of similarity for word pairs based on the shortest
path in a graph, so it could be compared with the similarity measure of the same
pair calculated by id27s to evaluate its quality [agirre et al., 2009].

the most well-known example of such semantic networks is the id138,
a graph containing 155 287 words5 organized in 117 659 synsets. [hearst, 1998].
another popular semantic network is dbpedia, a graph of concepts extracted
from the wikipedia and containing about 4.22 million words. of course, there
are even more di   erent semantic networks, but they probably are less extensive
then id138 and dbpedia, so their representativeness in this task could be
questioned.

4.4 linguistic-driven methods

4.4.1. phonosemantic analysis evaluation method is based on the idea that
the form of a linguistic sign is not arbitrary, since it somehow correlates with its
semantics. if that is true, it is possible to obtain certain data about semantics of
a word through phonosemantic patterns of its phonemes or characters. in order
to calculate phonosemantic di   erence between two words, one could measure use
levenshtein distance measure, and such metric could be used as a gold standard
for evaluation [guti  rrez et al., 2016]. notably, this observation was con   rmed
not only for latin alphabet, but also for cyrillic [kutuzov, 2017].

i am not aware of any open datasets designed speci   cally for evaluation on
this task, and in each of the studies mentioned, the authors used their own data.

4.4.2. bi-gram co-occurrence frequency evaluation method is based on
the idea that the distance between the words vectors representing words
of a phrase group (e.g. noun + adjective) should correlate with the fre-
quency of this group in a corpus (bi-gram co-occurrence frequency). in other
words, bi-gram co-occurrence frequency could be used as a gold standard
[kornai and kracht, 2015].

i think that any representative corpus or dictionary of id165 co-occurrence
frequency dataset like the google 1t frequency dataset 6 can be used for evalu-
ation.

5 future challenges

in this survey i attempted to systematize the existing attempts to explore and
evaluate id27s. i highlighted existing problems in this    eld and de-
scribed both mainstream and less well-known evaluation methods (16 intrinsic
methods and 12 extrinsic methods, 28 evaluation methods overall). in conclusion
i would like to brie   y discuss the new problems that propose future challenges
in the    eld of id27s evaluation.

the trends in the    eld of id27s are moving towards multi-
language and multi-sense embeddings. di   erent approaches should be used for
their evaluation, due to their di   erent nature: in the case of multi-language

5 on the moment of writing this work.
6 https://books.google.com/ngrams

embeddings, there is one vector for translating a word to each of the sup-
ported languages, while in multi-sense embeddings, one word corresponds
to multiple vectors, depending on the number of its senses. first attempts
to investigate possible methods of evaluating such models are already made
[borb  ly et al., 2016,reisinger and mooney, 2010,upadhyay et al., 2016], but i
argue that mainstream approaches to evaluation like word similarity datasets
would be even less applicable to such embeddings than to the    classic    mono-
language and mono-sense embeddings since of strong di   erences in semantics of
words in di   erent languages.

there are also certain issues related to the nature of distributional word
representations, that are not so implicitly related to the task of id27s
evaluation. among them are questions about distributional representations of
compound linguistic units (phrases and sentences) [lenci, 2017], interpretability
of the vector components [senel et al., 2017], connection with other models of
formal and cognitive semantics [lenci, 2008], etc.

additionally, many studies dedicated to id27s evaluation miss
one important factor which is a bias of vector space (i mean, bias in the terms
of fairness) related to certain gender, racial or sexual orientation prejudices (for
example, the similarity of gender-neutral words like programmer and the word
woman should not be lower than the same similarity with the word man, but in
some dsms it is) [bolukbasi et al., 2016,garg et al., 2017]. hence, if one consid-
ers that a    good    model should not be biased, then while evaluating the model
she must take into account the robustness of the model to that bias. due to this,
the problems of evaluation and the problems of bias detection go hand in hand,
and the complete solution of the    rst problem is impossible until the second one
is solved.

finally, i consider it an important problem that the success of resolving
the task of id27s evaluation strongly depends on the existence of
data; in other words, the task of evaluation is too supervised. many researchers
create a lot of materials and tools in english, and english embeddings can be
evaluated very extensively, while for low-resource languages (like urdu), even the
simplest evaluation cannot be done. i believe it is important to make language-
independent data for projecting models and data sets into a multi-language space
in which the presence or absence of data for this language would not a   ect the
ability to evaluate a distributional model.

acknowledgements

i want to thank my colleagues, andrey kutuzov (university of oslo) and roman
suvorov (isa ras), for productive discussions on this paper, critique, sugges-
tions and proofreading.

references

abnar et al., 2017. abnar, s., ahmed, r., mijnheer, m., and zuidema, w. (2017).
experiential, distributional and dependency-based id27s have comple-

mentary roles in decoding brain activity. arxiv preprint arxiv:1711.09285.

acs and kornai, 2016. acs, j. and kornai, a. (2016). evaluating embeddings on

dictionary-based similarity. association for computational linguistics.

agirre et al., 2009. agirre, e., alfonseca, e., hall, k., kravalova, j., pa  ca, m., and
soroa, a. (2009). a study on similarity and relatedness using distributional and
id138-based approaches.
in proceedings of human language technologies: the
2009 annual conference of the north american chapter of the association for com-
putational linguistics, pages 19   27. association for computational linguistics.

almuhareb, 2006. almuhareb, a. (2006). attributes in lexical acquisition. phd thesis,

university of essex.

andreas and klein, 2014. andreas, j. and klein, d. (2014). how much do word em-

beddings encode about syntax? in acl (2), pages 822   827.

auguste et al., 2017. auguste, j., rey, a., and favre, b. (2017). evaluation of word
embeddings against cognitive processes: primed reaction times in lexical decision
and naming tasks. in proceedings of the 2nd workshop on evaluating vector space
representations for nlp, pages 21   26.

avraham and goldberg, 2016. avraham, o. and goldberg, y. (2016). improving reli-
ability of word similarity evaluation by redesigning annotation task and performance
measure. arxiv preprint arxiv:1611.03641.

bakarov and gureenkova, 2017. bakarov, a. and gureenkova, o. (2017). automated
detection of non-relevant posts on the russian imageboard" 2ch": importance of the
choice of word representations. arxiv preprint arxiv:1707.04860.

baker et al., 2014. baker, s., reichart, r., and korhonen, a. (2014). an unsupervised

model for instance level subcategorization acquisition. in emnlp, pages 278   289.

bansal et al., 2014. bansal, m., gimpel, k., and livescu, k. (2014). tailoring contin-

uous word representations for id33. in acl (2), pages 809   815.

baroni et al., 2014. baroni, m., dinu, g., and kruszewski, g. (2014). don   t count,
predict! a systematic comparison of context-counting vs. context-predicting semantic
vectors. in acl (1), pages 238   247.

baroni et al., 2008. baroni, m., evert, s., and lenci, a. (2008). esslli 2008 work-
shop on distributional lexical semantics. hamburg, germany: association for logic,
language and information.

baroni and lenci, 2010. baroni, m. and lenci, a. (2010). distributional memory: a
general framework for corpus-based semantics. computational linguistics, 36(4):673   
721.

baroni and lenci, 2011. baroni, m. and lenci, a. (2011). how we blessed distribu-
tional semantic evaluation. in proceedings of the gems 2011 workshop on geomet-
rical models of natural language semantics, pages 1   10. association for computa-
tional linguistics.

baroni et al., 2010. baroni, m., murphy, b., barbu, e., and poesio, m. (2010). strudel:
a corpus-based semantic model based on properties and types. cognitive science,
34(2):222   254.

batchkarov et al., 2016. batchkarov, m., kober, t., re   n, j., weeds, j., and weir,
d. (2016). a critique of word similarity as a method for evaluating distributional
semantic models.

baumel et al., 2016. baumel, t., cohen, r., and elhadad, m. (2016). sentence em-

bedding evaluation using pyramid annotation. acl 2016, page 145.

blair et al., 2016. blair, p., merhav, y., and barry, j. (2016). automated generation of
multilingual clusters for the evaluation of distributed representations. arxiv preprint
arxiv:1611.01547.

bolukbasi et al., 2016. bolukbasi, t., chang, k.-w., zou, j. y., saligrama, v., and
kalai, a. t. (2016). man is to computer programmer as woman is to homemaker?
debiasing id27s. in advances in neural information processing systems,
pages 4349   4357.

borb  ly et al., 2016. borb  ly, g., makrai, m., nemeskey, d., and kornai, a. (2016).
evaluating multi-sense embeddings for semantic resolution monolingually and in word
translation. association for computational linguistics.

bowman et al., 2015. bowman, s. r., angeli, g., potts, c., and manning, c. d.
(2015). a large annotated corpus for learning natural language id136. arxiv
preprint arxiv:1508.05326.

bruni et al., 2014. bruni, e., tran, n.-k., and baroni, m. (2014). multimodal distri-

butional semantics. j. artif. intell. res.(jair), 49(2014):1   47.

camacho-collados and navigli, 2016. camacho-collados, j. and navigli, r. (2016).
find the word that does not belong: a framework for an intrinsic evaluation of word
vector representations. in acl workshop on evaluating vector space representa-
tions for nlp, pages 43   50.

camacho-collados et al., 2017. camacho-collados, j., pilehvar, m. t., collier, n., and
navigli, r. (2017). semeval-2017 task 2: multilingual and cross-lingual semantic word
similarity. in proceedings of the 11th international workshop on semantic evaluation
(semeval 2017). vancouver, canada.

che et al., 2017. che, x., ring, n., raschkowski, w., yang, h., and meinel, c. (2017).
in proceedings of the 2nd

traversal-free word vector evaluation in analogy space.
workshop on evaluating vector space representations for nlp, pages 11   15.

chinchor and marsh, 1998. chinchor, n. and marsh, e. (1998). muc-7 information
extraction task de   nition. in proceeding of the seventh message understanding con-
ference (muc-7), appendices, pages 359   367.

chiu et al., 2016. chiu, b., korhonen, a., and pyysalo, s. (2016). intrinsic evaluation
in proceedings of the 1st

of word vectors fails to predict extrinsic performance.
workshop on evaluating vector space representations for nlp, pages 1   6.

collobert et al., 2011. collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu,
k., and kuksa, p. (2011). natural language processing (almost) from scratch. journal
of machine learning research, 12(aug):2493   2537.

cop et al., 2017. cop, u., dirix, n., drieghe, d., and duyck, w. (2017). presenting
geco: an eyetracking corpus of monolingual and bilingual sentence reading. behavior
research methods, 49(2):602   615.

dolan and brockett, 2005. dolan, w. b. and brockett, c. (2005). automatically con-

structing a corpus of sentential paraphrases. in proc. of iwp.

duncan et al., 2009. duncan, k. j., pattamadilok, c., knierim, i., and devlin, j. t.
(2009). consistency and variability in functional localisers. neuroimage, 46(4):1018   
1026.

erk, 2016. erk, k. (2016). what do you know about an alligator when you know the

company it keeps? semantics and pragmatics, 9:17   1.

ettinger et al., 2016a. ettinger, a., elgohary, a., and resnik, p. (2016a). probing for
semantic evidence of composition by means of simple classi   cation tasks. acl 2016,
page 134.

ettinger et al., 2016b. ettinger, a., feldman, n. h., resnik, p., and phillips, c.
(2016b). modeling n400 amplitude using vector space models of word representa-
tion. in proceedings of the 38th annual conference of the cognitive science society,
pages 1445   1450.

ettinger and linzen, 2016. ettinger, a. and linzen, t. (2016). evaluating vector
space models using human semantic priming results. in proceedings of the 1st work-
shop on evaluating vector space representations for nlp, pages 72   77.

faruqui et al., 2016. faruqui, m., tsvetkov, y., rastogi, p., and dyer, c. (2016). prob-
lems with evaluation of id27s using word similarity tasks. arxiv preprint
arxiv:1605.02276.

ferretti et al., 2001. ferretti, t. r., mcrae, k., and hatherell, a. (2001). integrat-
ing verbs, situation schemas, and thematic role concepts. journal of memory and
language, 44(4):516   547.

finkelstein et al., 2001. finkelstein, l., gabrilovich, e., matias, y., rivlin, e., solan,
z., wolfman, g., and ruppin, e. (2001). placing search in context: the concept
revisited. in proceedings of the 10th international conference on world wide web,
pages 406   414. acm.

friedman and amoo, 1999. friedman, h. h. and amoo, t. (1999). rating the rating

scales.

gabrilovich and markovitch, 2007. gabrilovich, e. and markovitch, s. (2007). com-
in

puting semantic relatedness using wikipedia-based explicit semantic analysis.
ijcai, volume 7, pages 1606   1611.

gao et al., 2014. gao, b., bian, j., and liu, t.-y. (2014). wordrep: a benchmark for

research on learning word representations. arxiv preprint arxiv:1407.1640.

garg et al., 2017. garg, n., schiebinger, l., jurafsky, d., and zou, j. (2017). word
embeddings quantify 100 years of gender and ethnic stereotypes. arxiv preprint
arxiv:1711.08412.

gerz et al., 2016. gerz, d., vuli  , i., hill, f., reichart, r., and korhonen, a. (2016).
arxiv preprint

simverb-3500: a large-scale evaluation set of verb similarity.
arxiv:1608.00869.

gladkova and drozd, 2016. gladkova, a. and drozd, a. (2016). intrinsic evaluations
of id27s: what can we do better? in proceedings of the 1st workshop on
evaluating vector-space representations for nlp, pages 36   42.

gladkova et al., 2016. gladkova, a., drozd, a., and matsuoka, s. (2016). analogy-
based detection of morphological and semantic relations with id27s: what
works and what doesn   t. in proceedings of the naacl student research workshop,
pages 8   15.

greenberg et al., 2015. greenberg, c., demberg, v., and sayeed, a. (2015). verb pol-
ysemy and frequency e   ects in thematic    t modeling. in proceedings of the 6th work-
shop on cognitive modeling and computational linguistics. association for compu-
tational linguistics, denver, colorado, pages 48   57.

gri   ths et al., 2007. gri   ths, t. l., steyvers, m., and tenenbaum, j. b. (2007). top-

ics in semantic representation. psychological review, 114(2):211.

gurnani, 2017. gurnani, n. (2017). hypothesis testing based intrinsic evaluation of

id27s. arxiv preprint arxiv:1709.00831.

guti  rrez et al., 2016. guti  rrez, e. d., levy, r., and bergen, b. (2016). finding
non-arbitrary form-meaning systematicity using string-metric learning for kernel re-
gression. in acl (1).

halawi et al., 2012. halawi, g., dror, g., gabrilovich, e., and koren, y. (2012). large-
scale learning of word relatedness with constraints. in proceedings of the 18th acm
sigkdd international conference on knowledge discovery and data mining, pages
1406   1414. acm.

hanke et al., 2014. hanke, m., baumgartner, f. j., ibe, p., kaule, f. r., pollmann, s.,
speck, o., zinke, w., and stadler, j. (2014). a high-resolution 7-tesla fmri dataset
from complex natural stimulation with an audio movie. scienti   c data, 1:140003.

hearst, 1998. hearst, m. (1998). id138: an electronic lexical database and some of

its applications.

herda  delen et al., 2009. herda  delen, a., erk, k., and baroni, m. (2009). measuring
semantic relatedness with vector space models and id93. in proceedings of
the 2009 workshop on graph-based methods for natural language processing, pages
50   53. association for computational linguistics.

hill et al., 2016. hill, f., reichart, r., and korhonen, a. (2016). siid113x-999: evaluat-
ing semantic models with (genuine) similarity estimation. computational linguistics.
hutchison et al., 2013. hutchison, k. a., balota, d. a., neely, j. h., cortese, m. j.,
cohen-shikora, e. r., tse, c.-s., yap, m. j., bengson, j. j., niemeyer, d., and
buchanan, e. (2013). the semantic priming project. behavior research methods,
45(4):1099   1114.

huth et al., 2016. huth, a. g., de heer, w. a., gri   ths, t. l., theunissen, f. e.,
and gallant, j. l. (2016). natural speech reveals the semantic maps that tile human
cerebral cortex. nature, 532(7600):453   458.

jarmasz and szpakowicz, 2004. jarmasz, m. and szpakowicz, s. (2004). roget   s the-
saurus and semantic similarity. recent advances in natural language processing iii:
selected papers from ranlp, 2003:111.

jastrzebski et al., 2017. jastrzebski, s., le  niak, d., and czarnecki, w. m. (2017).
how to evaluate id27s? on importance of data e   ciency and simple
supervised tasks. arxiv preprint arxiv:1702.02170.

jones et al., 2006. jones, m. n., kintsch, w., and mewhort, d. j. (2006). high-
dimensional semantic space accounts of priming. journal of memory and language,
55(4):534   552.

jurgens et al., 2012. jurgens, d. a., turney, p. d., mohammad, s. m., and holyoak,
k. j. (2012). semeval-2012 task 2: measuring degrees of relational similarity.
in
proceedings of the first joint conference on lexical and computational semantics-
volume 1: proceedings of the main conference and the shared task, and volume 2:
proceedings of the sixth international workshop on semantic evaluation, pages 356   
364. association for computational linguistics.

kiela et al., 2015. kiela, d., hill, f., and clark, s. (2015). specializing word embed-

dings for similarity or relatedness. in emnlp, pages 2044   2048.

kocmi and bojar, 2017. kocmi, t. and bojar, o. (2017). an exploration of word

embedding initialization in deep-learning tasks. arxiv preprint arxiv:1711.09160.

k  hn, 2016. k  hn, a. (2016). evaluating embeddings using syntax-based classi   cation

tasks as a proxy for parser performance. acl 2016, page 67.

kornai and kracht, 2015. kornai, a. and kracht, m. (2015). lexical semantics and

model theory: together at last? acl.

krebs and paperno, 2016. krebs, a. and paperno, d. (2016). capturing discrimina-

tive attributes in a distributional space: task proposal. acl 2016, page 51.

kutas and federmeier, 2011. kutas, m. and federmeier, k. d. (2011). thirty years
and counting:    nding meaning in the n400 component of the event-related brain
potential (erp). annual review of psychology, 62:621   647.

kutuzov, 2017. kutuzov, a. (2017). arbitrariness of linguistic sign questioned: cor-
relation between word form and meaning in russian. komp   yuternaya lingvistika i
intellektual   nye tekhnologii, 1(16 (23)):109   120.

landauer and dumais, 1997. landauer, t. k. and dumais, s. t. (1997). a solution
to plato   s problem: the latent semantic analysis theory of acquisition, induction, and
representation of knowledge. psychological review, 104(2):211.

lapesa and evert, 2013. lapesa, g. and evert, s. (2013). evaluating neighbor rank
and distance measures as predictors of semantic priming.
in proceedings of the
acl workshop on cognitive modeling and computational linguistics (cmcl 2013),
pages 66   74.

lenci, 2008. lenci, a. (2008). id65 in linguistic and cognitive re-

search. italian journal of linguistics, 20(1):1   31.

lenci, 2017. lenci, a. (2017). distributional models of word meaning. annual review

of linguistics, (0).

levy and goldberg, 2014. levy, o. and goldberg, y. (2014). linguistic regularities in
sparse and explicit word representations. in proceedings of the eighteenth conference
on computational natural language learning, pages 171   180.

liza and grzes, 2016. liza, f. f. and grzes, m. (2016). an improved id104

based evaluation technique for id27 methods. acl 2016, page 55.

luke and christianson, 2017. luke, s. g. and christianson, k. (2017). the provo
corpus: a large eye-tracking corpus with predictability norms. behavior research
methods, pages 1   8.

luong et al., 2013. luong, t., socher, r., and manning, c. d. (2013). better word
in conll, pages

representations with id56s for morphology.
104   113.

maas et al., 2011. maas, a. l., daly, r. e., pham, p. t., huang, d., ng, a. y., and
potts, c. (2011). learning word vectors for id31. in proceedings of
the 49th annual meeting of the association for computational linguistics: human
language technologies-volume 1, pages 142   150. association for computational lin-
guistics.

mandera et al., 2017. mandera, p., keuleers, e., and brysbaert, m. (2017). explaining
human performance in psycholinguistic tasks with models of semantic similarity based
on prediction and counting: a review and empirical validation. journal of memory
and language, 92:57   78.

marcus et al., 1993. marcus, m. p., marcinkiewicz, m. a., and santorini, b. (1993).
building a large annotated corpus of english: the id32. computational
linguistics, 19(2):313   330.

marelli et al., 2014. marelli, m., bentivogli, l., baroni, m., bernardi, r., menini, s.,
and zamparelli, r. (2014). semeval-2014 task 1: evaluation of compositional distri-
butional semantic models on full sentences through semantic relatedness and textual
entailment. in semeval@ coling, pages 1   8.

mcdonald and brew, 2004. mcdonald, s. and brew, c. (2004). a distributional model
of semantic context e   ects in lexical processing. in proceedings of the 42nd annual
meeting on association for computational linguistics, page 17. association for com-
putational linguistics.

mcnamara, 2011. mcnamara, d. s. (2011). computational methods to extract mean-
ing from text and advance theories of human cognition. topics in cognitive science,
3(1):3   17.

mcrae et al., 1997. mcrae, k., ferretti, and liane amyote, t. r. (1997). thematic

roles as verb-speci   c concepts. language and cognitive processes, 12(2-3):137   176.

mcrae et al., 1998. mcrae, k., spivey-knowlton, m. j., and tanenhaus, m. k.
(1998). modeling the in   uence of thematic    t (and other constraints) in on-line
sentence comprehension. journal of memory and language, 38(3):283   312.

mikolov et al., 2013a. mikolov, t., chen, k., corrado, g., and dean, j. (2013a).
arxiv preprint

e   cient estimation of word representations in vector space.
arxiv:1301.3781.

mikolov et al., 2013b. mikolov, t., yih, w.-t., and zweig, g. (2013b). linguistic reg-
ularities in continuous space word representations. in hlt-naacl, volume 13, pages
746   751.

milajevs and gri   ths, 2016. milajevs, d. and gri   ths, s. (2016).

for linguistic similarity datasets based on commonality lists.
arxiv:1605.04553.

a proposal
arxiv preprint

miller and charles, 1991. miller, g. a. and charles, w. g. (1991). contextual corre-

lates of semantic similarity. language and cognitive processes, 6(1):1   28.

mostafazadeh et al., 2016. mostafazadeh, n., vanderwende, l., yih, w.-t., kohli, p.,
and allen, j. (2016). story cloze evaluator: vector space representation evaluation
by predicting what happens next. acl 2016, page 24.

nayak et al., 2016. nayak, n., angeli, g., and manning, c. d. (2016). evaluating
id27s using a representative suite of practical tasks. acl 2016, page 19.
osgood et al., 1978. osgood, c. e., suci, g. j., and tannenbaum, p. h. (1978). the

measurement of meaning. 1957. urbana: university of illinois press.

pad   and lapata, 2007. pad  , s. and lapata, m. (2007). dependency-based construc-

tion of semantic space models. computational linguistics, 33(2):161   199.

pad  , 2007. pad  , u. (2007). the integration of syntax and semantic plausibility in a

wide-coverage model of human sentence processing.

palmer et al., 2005. palmer, m., gildea, d., and kingsbury, p. (2005). the proposition
bank: an annotated corpus of semantic roles. computational linguistics, 31(1):71   
106.

panchenko et al., 2013. panchenko, a. et al. (2013). similarity measures for semantic
id36. phd thesis, phd thesis, universit   catholique de louvain &
bauman moscow state technical university.

parviz et al., 2011. parviz, m., johnson, m., johnson, b., and brock, j. (2011). us-
ing language models and latent semantic analysis to characterise the n400m neural
response. in proceedings of the australasian language technology association work-
shop 2011, pages 38   46.

pereira et al., 2016. pereira, f., gershman, s., ritter, s., and botvinick, m. (2016).
a comparative evaluation of o   -the-shelf distributed semantic representations for
modelling behavioural data. cognitive neuropsychology, 33(3-4):175   190.

radinsky et al., 2011. radinsky, k., agichtein, e., gabrilovich, e., and markovitch,
s. (2011). a word at a time: computing word relatedness using temporal semantic
analysis.
in proceedings of the 20th international conference on world wide web,
pages 337   346. acm.

reisinger and mooney, 2010. reisinger, j. and mooney, r. j. (2010). multi-prototype
vector-space models of word meaning. in human language technologies: the 2010
annual conference of the north american chapter of the association for computa-
tional linguistics, pages 109   117. association for computational linguistics.

resnik, 1995. resnik, p. (1995). using information content to evaluate semantic sim-

ilarity in a taxonomy. arxiv preprint cmp-lg/9511007.

rubenstein and goodenough, 1965. rubenstein, h. and goodenough, j. b. (1965).

contextual correlates of synonymy. communications of the acm, 8(10):627   633.

sayeed et al., 2016. sayeed, a., greenberg, c., and demberg, v. (2016). thematic    t

evaluation: an aspect of selectional preferences. acl 2016, page 99.

schnabel et al., 2015. schnabel, t., labutov, i., mimno, d. m., and joachims, t.
(2015). evaluation methods for unsupervised id27s. in emnlp, pages
298   307.

senel et al., 2017. senel, l. k., utlu, i., yucesoy, v., koc, a., and cukur, t.
(2017). semantic structure and interpretability of id27s. arxiv preprint
arxiv:1711.00331.

s  gaard, 2016. s  gaard, a. (2016). evaluating id27s with fmri and eye-

tracking. acl 2016, page 116.

tjong kim sang and buchholz, 2000. tjong kim sang, e. f. and buchholz, s. (2000).
introduction to the conll-2000 shared task: chunking.
in proceedings of the 2nd
workshop on learning language in logic and the 4th conference on computational
natural language learning-volume 7, pages 127   132. association for computational
linguistics.

tjong kim sang and de meulder, 2003. tjong kim sang, e. f. and de meulder, f.
introduction to the conll-2003 shared task: language-independent named
(2003).
entity recognition.
language
learning at hlt-naacl 2003-volume 4, pages 142   147. association for computa-
tional linguistics.

in proceedings of the seventh conference on natural

toutanova et al., 2003. toutanova, k., klein, d., manning, c. d., and singer, y.
(2003). feature-rich part-of-speech tagging with a cyclic dependency network. in
proceedings of the 2003 conference of the north american chapter of the associa-
tion for computational linguistics on human language technology-volume 1, pages
173   180. association for computational linguistics.

tsvetkov et al., 2014. tsvetkov, y., boytsov, l., gershman, a., nyberg, e., and dyer,

c. (2014). metaphor detection with cross-lingual model transfer.

tsvetkov et al., 2015. tsvetkov, y., faruqui, m., ling, w., lample, g., and dyer, c.

(2015). evaluation of word vector representations by subspace alignment.

turian et al., 2010. turian, j., ratinov, l., and bengio, y. (2010). word representa-
tions: a simple and general method for semi-supervised learning. in proceedings of the
48th annual meeting of the association for computational linguistics, pages 384   394.
association for computational linguistics.

turney, 2001. turney, p. (2001). mining the web for synonyms: pmi-ir versus lsa on

toe   . machine learning: ecml 2001, pages 491   502.

turney, 2008. turney, p. d. (2008). the latent relation mapping engine: algorithm

and experiments. journal of arti   cial intelligence research, 33:615   655.

turney et al., 2003. turney, p. d., littman, m. l., bigham, j., and shnayder, v.
(2003). combining independent modules to solve multiple-choice synonym and anal-
ogy problems. arxiv preprint cs/0309035.

turney and pantel, 2010. turney, p. d. and pantel, p. (2010). from frequency to
meaning: vector space models of semantics. journal of arti   cial intelligence research,
37:141   188.

upadhyay et al., 2016. upadhyay, s., faruqui, m., dyer, c., and roth, d. (2016).
cross-lingual models of id27s: an empirical comparison. arxiv preprint
arxiv:1604.00425.

vandekerckhove et al., 2009. vandekerckhove, b., sandra, d., and daelemans, w.
(2009). a robust and extensible exemplar-based model of thematic    t. in proceedings
of the 12th conference of the european chapter of the association for computational
linguistics, pages 826   834. association for computational linguistics.

vylomova et al., 2015. vylomova, e., rimell, l., cohn, t., and baldwin, t. (2015).
take and took, gaggle and goose, book and read: evaluating the utility of vector
di   erences for lexical relation learning. arxiv preprint arxiv:1509.01692.

yang and powers, 2006. yang, d. and powers, d. m. (2006). verb similarity on the
taxonomy of id138. in the third international id138 conference: gwc 2006.
masaryk university.

