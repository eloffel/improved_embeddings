advanced tips 
for deep learning

hung-yi lee

prerequisite: https://www.youtube.com/watch?v=xki61j7z-30

recipe of deep learning

step 1: define a 
set of function              

no

overfitting!

step 2: goodness 

of function

step 3: pick the 
best function

yes

good results on 

testing data?

yes

no

good results on 
training data?

neural 
network

do not always blame overfitting

not well trained

overfitting?

training data

testing data

deep residual learning for image recognition
http://arxiv.org/abs/1512.03385

recipe of deep learning

different approaches for 
different problems.

e.g. dropout for good results 
on testing data

yes

good results on 

testing data?

yes

good results on 
training data?

neural 
network

outline

    batch id172
    new activation function
    tuning hyperparameters
    interesting facts (?) about deep learning
    capsule
    new models for qa

batch id172

sergey ioffe, christian szegedy,    batch id172: accelerating 
deep network training by reducing internal covariate shift   , 2015

feature scaling

make different features 
have the same scaling

1, 2       

100, 200       

1, 2       

1, 2       

loss l

loss l

a1w2w1x2x   b1w2wa1w2w1x2x   b1w2wfeature scaling
    1
1
    1
1
    2

    2
2
    1
2
    2

    3

      

        

        

      

   
   

   
   

   
   

   
   

   
   

for each 
dimension i:

mean:         
standard 
deviation:         

        
        

                 
        

        

the means of all dimensions are 0, 
and the variances are all 1 

in general, id119 converges much faster 
with feature scaling than without it.

how about hidden layer?

feature scaling

feature scaling ?

feature scaling ?

    1

layer 1

    1

layer 2

    2       

difficulty: their statistics 
change during the training    
batch id172

smaller learning rate can be 
helpful, but the training would 
be slower.

internal covariate shift 

batch

    1

    1

    1

    2

    1

    2

    3

    1

    3

s
i
g
m
o
d

i

s
i
g
m
o
d

i

s
i
g
m
o
d

i

    1

    2

    3

    2

      

    2

      

    2

      

batch

    1     2     3 =

    1

    1     2     3

batch id172

    1

    1

    1

    2

    1

    2

    3

    1

    3

     =

1
3

3

   
    =1

        

     =

1
3

3

   
    =1

                  2

note: batch id172 
cannot be applied on 
small batch.

     and     
depends on         

    

    

batch id172

    1

    1

    1

    2

    1

    2

    3

    1

    3

           =

                 

    

      1

      2

      3

s
i
g
m
o
d

i

s
i
g
m
o
d

i

s
i
g
m
o
d

i

    1

    2

    3

     and     
depends on         

    

    

how to do 
backpropogation?

batch id172

    1

    1

    1

    2

    1

    2

    3

    1

    3

           =

                 

    

           =                    +     

      1

      2

      3

      1

      2

      3

     and     
depends on         

    

    

    

    

batch id172

acc

    300

    100

    1

updates

    at testing stage:

    

    1

    

             

       =

    
    ,      are 
from batch

      

           =                    +     

    ,      are network 
parameters

      

we do not have batch at testing stage.

ideal solution:

computing      and      using the whole training dataset.

practical solution:

computing the moving average of      and      of the 
batches during training.

batch id172 - benefit

    bn reduces training times, and make very deep net 

trainable.

    because of less covariate shift, we can use larger 

learning rates.

    less exploding/vanishing gradients

    especially effective for sigmoid, tanh, etc.

    learning is less affected by initialization. 

       

       

        

    1

        

                

          

          

           =                    +     

    
    
                 
           =
    

    

    bn reduces the demand for id173. 

demo

activation function

g  nter klambauer, thomas unterthiner, andreas mayr, andreas mayr, 
   self-normalizing neural networks   , nips, 2017

relu

    rectified linear unit (relu)

         

    

     =     

reason:

1. fast to compute

2. biological reason

     = 0

    

3. infinite sigmoid 
with different biases

[xavier glorot, aistats   11]
[andrew l. maas, icml   13]
[kaiming he, arxiv   15]

4. vanishing gradient 
problem

relu - variant

leaky relu

parametric relu

    

     =     

    

     =     

    

    

     = 0.01    

     =         

   also learned by 
id119

relu - variant

randomized relu

    

     =     

dropout???

     =         

exponential linear 
unit (elu)

    

     =     

    

    

     =                    1

     is sampled from a 
distribution during training.
fixed during testing.

relu - variant

https://github.com/bioinf-jku/snns

exponential linear 
unit (elu)

    

     =     

scaled elu (selu)

    

     =     

       

    

     =                    1

    

     =                    1

       

     = 1.6732632423543772848170429916717
     = 1.0507009873554804934193349852946

selu

     = 1.673263242    
     = 1.050700987    

     =         

     =                        1

positive and negative values

the whole relu family has this property except the original relu.

saturation region

elu also has this property 

slope larger than 1

only selu also has this property 

         =          

    

=    
    =1

    

                      

    

=         
    =1

         =                      

=0

=0

selu

the inputs are i.i.d random 
variables with mean      and 
variance      2.

=0

=1

   

   

   

   

do not have to be gaussian

kkkkwawawaz                     11z1wkwkw1akaka         zfaselu

the inputs are i.i.d random 
variables with mean      and 
variance     2.

=0

=1

         = 0
         = 0
2 =                       
        

2 =          2

=          1    1 +     2    2 +     2

    

2    2 =     2                 
2

= 1

        

=    
    =1
2 =         

=1

=1
2             

2 =         
                     
                                      =                                             = 0

2    2

   

   

   

   

assume gaussian

     = 0,      = 1

target

kkkkwawawaz                     11z1wkwkw1akaka         zfademo

93             

source of joke: 
https://zhuanlan.zhihu.co
m/p/27336839

selu is actually 
more general.

mnist

cifar-10

                         :self-id172 neural 

network (selu)

demo

     =                                               

hyperparameters

source of iamge: https://medium.com/intuitionmachine/the-brute-force-method-of-
deep-learning-innovation-58b497323ae5 (denny britz   s graphic)

                                   

http://www.deeplearningbook.org/contents/guidelines.html

grid search v.s. random search

 

h
t
d
w

i

 
r
e
y
a
l

 

h
t
d
w

i

 
r
e
y
a
l

layer depth 

layer depth 

assumption: top k results are good enough

if there are n points, id203 k/n that your sample is in top k

sample x times:

1     1         /          > 90%

if n = 1000, k = 10
k = 100

x = 230
x = 22

model-based hyperparameter 
optimization

https://cloud.google.com/blog/big-
data/2017/08/hyperparameter-
tuning-in-cloud-machine-learning-
engine-using-bayesian-optimization

id23
it can design lstm as shown 
in the previous lecture.

one kind of meta 
learning (or learn to learn)

accuracy as 
reward 

train the network

design a 
network

swish ......

swish ......

learning rate

can transfer to 
new tasks

capsule

sara sabour, nicholas frosst, geoffrey e. hinton,    dynamic routing 
between capsules   , nips, 2017

capsule

a neuron detects a 
specific pattern. 

    neuron: output a value, capsule: output a vector 

neuron a neuron b

cap

    1

detect one type 
of patterns

cap

    

1.0
   

   1.0

   

cap

    2

each dimension of v represents 
the characteristics of patterns.
the norm of v represents the existence. 

capsule

    2 =     2    2

    1 =     1    1
     =     1    1 +     2    2
     =                             

     =

     2

1 +      2

    
    

       1

    1

    1

       1

+

    

squashing

    

       2

    2

    2

       2

coupling 
coefficients

c are determined by dynamic routing during the testing stage.

c.f. pooling

dynamic 
routing

    1

    2

    3

  ?

    
    1

  ?

    
    2

+

    

squashing

    

  ?

    
    3

=         

0 = 0,     2
    1
for      = 1 to t do

0 = 0,     3

0 = 0

       1,     2

       1,     3

       1

    ,     3

    ,     2
     =                                  1
    1
         =     1    1 +     2    2 +     3    3
         =                                 
     =         
        

       1 +                      

0 = 0

0 = 0,     2
    1
1,     2
    1

1 =                                  1

0
0,     2

     =         
        

       1 +                      

       1

    1

    1

       2

    2

    2

    1

squashing

    1

1
    1

+
1
    2

+

    2

squashing

    2

2
    1

2
    2
3
    1

2
    1
2
    2

3
    1
3
    2

3
    2

+

    3

squashing

    3

    

t=3

like id56 also learned by backprop

capsule

minimize 
reconstruction 
error

    capsule can also be convolutional. 

    simply replace filter with capsule 

nn

    output layer and loss

x1

x0

capsnet

cap

    1

cap

    2

   
   

   
   

    1

confidence 

of    1   

    2

confidence 

of    2   

experimental results

    mnist

    each example is an mnist digit with a random small affine 

transformation.

    however, models were never trained with affine 

transformations

    capsnet achieved 79% accuracy on the affnist test set. 
    a traditional convolutional model with a similar number of 

parameters which achieved 66%.

experimental results

minimize 
reconstruction 
error

    each dimension 
contains specific 
information.

    1

nn

experimental results

    multimnist

top: input
bottom: reconstructed
r: reconstructed digits
l: true labels

discussion

    invariance v.s. equivariance

nn

nn

nn

nn

invariance

equivariance

discussion

i know the difference, but i do not 
react to it.

both large

    invariance v.s. equivariance

i don   t know the difference.

3

3

    1

    1

cap

can be 
different

    1

    1

cap

3

-3

-1

1

-3

0

-1

3

max pooling has invariance, 
but not equivariance. 

capsule has both invariance 
and equivariance. 

dynamic routing

to learn more       

    hinton   s talk: 

https://www.youtube.com/watch?v=rtawfwuvnle

    keras:

    https://github.com/xifengguo/capsnet-keras

    tensorflow:

    https://github.com/naturomics/capsnet-tensorflow

    pytorch

    https://github.com/gram-ai/capsule-networks
    https://github.com/timomernick/pytorch-capsule
    https://github.com/nishnik/capsnet-pytorch

interesting facts (?) 
about deep learning

http://www.deeplearningbook.org/contents/optimization.html

training stuck because    . ? 

    people believe training stuck because the 

parameters are near a local minima

local minima

how about 
saddle point?

http://www.deeplearningbook.org/contents/optimization.html

training stuck because    . ? 

    people believe training stuck because the 

parameters are around a critical point

!!!

brute-force memorization ?

https://arxiv.org/pdf/1611.03530.pdf

final of 2017 spring: 
https://ntumlds.wordpress.com/2017/03/27/r05922018_drliao/

demo

https://arxiv.org/pdf/1706.05394.pdf

brute-force memorization ?

    simple pattern first, then memorize exception

knowledge 
distillation

knowledge distillation
https://arxiv.org/pdf/1503.02531.pdf
do deep nets really need to be deep?
https://arxiv.org/pdf/1312.6184.pdf

learning target

cross-id178 
minimization

   1   : 0.7,    7   : 0.2.    9   : 0.1

?

teacher net

(deep)

student net

(shallow)

training 
data

https://arxiv.org/pdf/1312.6184.pdf

deep learning for 
id53

id53 

    given a document and a query, output an answer

    babi: the answer is a word

    https://research.fb.com/downloads/babi/

    squad: the answer is a sequence of words (in the input 

document)

    https://rajpurkar.github.io/squad-explorer/
    ms marco: the answer is a sequence of words 

    http://www.msmarco.org

    movieqa: multiple choice question (output a number)

    http://movieqa.cs.toronto.edu/home/

    more: https://github.com/dapurv5/awesome-question-

answering

bi-directional attention flow

demo: http://35.165.153.16:1995

dynamic coattention networks

dynamic coattention networks

dynamic coattention networks

dynamic coattention networks

    experimental results

dcn+: https://arxiv.org/pdf/1711.00106.pdf

rnet

r-net

s-net

ms marco

attention-over-attention (aoa)

(not on squad)

reinforced mnemonic reader 

multiple-hop

reasonet
https://arxiv.org/abs/1609.05284

fusionnet

recurrent entity networks

    https://arxiv.org/pdf/1612.03969.pdf

query-reduction networks for 
id53
    https://arxiv.org/pdf/1606.04582.pdf

query-reduction networks for 
id53
    https://arxiv.org/pdf/1606.04582.pdf

acknowledgement 

                                                      

