setup

http://bit.ly/insightnlp

follow	along	(or	run	the	code	yourself)

1/ go	to	link	above

2/	click	on	nlp_notebook.ipynb and	follow	
along

3	(optional)/	if	you	have	the	necessary	libraries
installed	and	pretrained weights,	clone	the	
repository	and	run	   jupyter notebook   

1

concrete nlp solutions

@odsc

emmanuel ameisen
san francisco  |   november 3rd 2017

emmanuel ameisen
ai program director, ml engineer

@emmanuelameisen

/in/ameisen

past experience 

data scientist

data scientist

msc. computer science

msc. artificial 
intelligence

msc. management

3

insight fellows are data scientists and 
data engineers everywhere

silicon valley     new york     boston     los angeles     seattle

+ many others   

4

network of insight alumni

1,000+

data scientists and 

data engineers

5

example projects

fashion classifier

automatic review generation

reading text in videos

heart segmentation

support request classification

speech unsampling

6

concrete nlp solutions

    id91

    classification

    generation

7

a huge value to be unlocked

business models

every day uses

8

all you need is love labels

all you need is clean data

how do you represent data?

11

how do you represent images?

12

how do you represent words?

ascii

a
b
c
d
e
f
g
h
i
j
k
l
m

97
98
99
100
101
102
103
104
105
106
107
108
109

n
o
p
q
r
s
t
u
v
w
x
y
z

110
111
112
113
114
115
116
117
118
119
120
121
122

    discrete 
    need to learn language and words
    how do you represent a whole sentence

we need a sentence level representation

13

bag of words

14

bag of words 
(aka one hot embedding)

   mary is hungry for apples.   

   john is happy he is not 
hungry for apples.   

mary

is hungry happy for

apples

not

john he

1

0

1

2

1

1

0

1

1

1

1

1

0

1

0

1

0

1

[1, 1, 1, 0, 1, 1, 0, 0, 0]

[0, 2, 1, 1, 1, 1, 1, 1, 1]

classifier

15

bag of words

pros

cons

    quick setup
    works very well if some 
words are very important
    easy to directly interpret

    need fixed vocab size
    doesn   t capture semantic 
meaning of words
    does not conserve order

16

word vectors

17

word vectors

mikolov	et	al.,	2013

the dirty secret

https://www.tensorflow.org/tutorials/id97

id97 bag of words

id97 embedding

lookup

mary is hungry for apples.
john is happy he is not hungry for apples.

word	

mary

is

hungry

happy

for

apples

not

john

he

vector

[.24,	1.16,	.12,   ,	1.97,	.23,	.12]

[.55,	.11,	.15,   ,	.65,	1.30,	2.42]

average

   

   

   

   

   

[.68,	2.02,	.24,   ,	1.12,	.43,	.27]

[.21,	1.07,	1.01,   ,	0.94,	.07,	.16]

[.24, 1.16, .12,   , 1.97, .23, .12]
[.43, 1.46, .55,   , 1.13, .53, .78]

classifier

id97 bag of words

pros

cons

    captures semantics (similar 

words close together)
    dense representation
    benefit from having been trained 

on millions of sentences

    does not capture syntax
    loses direct 
interpretability

    quick embedding (lookup table)

can we do better?

for embeddings
    not really, risk simply overfitting

for other tasks

    translation
    dialog
    very tricky classification

convolutional neural networks

23

convolutional neural networks

24

recurrent neural networks

25

recurrent neural networks

26

recurrent neural networks

pros

cons

    keeps semantic 
meaning
    can adapt and learn 
embeddings

    needs much more 
compute
    slower
    doesn   t directly 
produce embeddings

27

what have you learned? 

1. build a simple prototype
2. inspect and validate the embedding
3. build a more complex model
4. evaluate and validate your predictions

thank you!

i   m emmanuel ameisen

@emmanuelameisen

/in/ameisen

special thanks to:

find out more and apply:

insightdata.ai

read our blog:

blog.insightdatascience.com

- our alumni and fellow community and staff.
- our industry partners.
- paperspace for providing us with a great compute platform

29

