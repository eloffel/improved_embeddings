id76 overview

zico kolter

october 19, 2007

1 introduction

many situations arise in machine learning where we would like to optimize the value of
some function. that is, given a function f : rn     r, we want to    nd x     rn that minimizes
(or maximizes) f (x). we have already seen several examples of optimization problems in
class:
least-squares, id28, and support vector machines can all be framed as
optimization problems.

it turns out that in the general case,    nding the global optimum of a function can be a
very di   cult task. however, for a special class of optimization problems, known as convex
optimization problems, we can e   ciently    nd the global solution in many cases. here,
   e   ciently    has both practical and theoretical connotations:
it means that we can solve
many real-world problems in a reasonable amount of time, and it means that theoretically
we can solve problems in time that depends only polynomially on the problem size.

the goal of these section notes and the accompanying lecture is to give a very brief
overview of the    eld of id76. much of the material here (including some
of the    gures) is heavily based on the book id76 [1] by stephen boyd and
lieven vandenberghe (available for free online), and ee364, a class taught here at stanford
by stephen boyd. if you are interested in pursuing id76 further, these are
both excellent resources.

2 convex sets

we begin our look at id76 with the notion of a convex set.

de   nition 2.1 a set c is convex if, for any x, y     c and        r with 0            1,

  x + (1       )y     c.

intuitively, this means that if we take any two elements in c, and draw a line segment
between these two elements, then every point on that line segment also belongs to c. figure
1 shows an example of one convex and one non-convex set. the point   x + (1       )y is called
a convex combination of the points x and y.

1

(a)

(b)

figure 1: examples of a convex set (a) and a non-convex set (b).

2.1 examples

    all of rn. it should be fairly obvious that given any x, y     rn,   x + (1       )y     rn.

    the non-negative orthant, rn

rn whose elements are all non-negative: rn
that this is a convex set, simply note that given any x, y     rn

+. the non-negative orthant consists of all vectors in
+ = {x : xi     0    i = 1, . . . , n}. to show

+ and 0            1,

(  x + (1       )y)i =   xi + (1       )yi     0    i.

    norm balls. let k    k be some norm on rn (e.g., the euclidean norm, kxk2 =
i ). then the set {x : kxk     1} is a convex set. to see this, suppose x, y     rn,

i=1 x2

with kxk     1, kyk     1, and 0            1. then

ppn

k  x + (1       )yk     k  xk + k(1       )yk =   kxk + (1       )kyk     1

where we used the triangle inequality and the positive homogeneity of norms.

    a   ne subspaces and polyhedra. given a matrix a     rm  n and a vector b     rm,
an a   ne subspace is the set {x     rn : ax = b} (note that this could possibly be empty
if b is not in the range of a). similarly, a polyhedron is the (again, possibly empty)
set {x     rn : ax (cid:22) b}, where    (cid:22)    here denotes componentwise inequality (i.e., all the
entries of ax are less than or equal to their corresponding element in b).1 to prove
this,    rst consider x, y     rn such that ax = ay = b. then for 0            1,

a(  x + (1       )y) =   ax + (1       )ay =   b + (1       )b = b.

similarly, for x, y     rn that satisfy ax     b and ay     b and 0            1,

a(  x + (1       )y) =   ax + (1       )ay       b + (1       )b = b.

1similarly, for two vectors x, y     rn, x (cid:23) y denotes that each element of x is greater than or equal to the
corresponding element in b. note that sometimes           and           are used in place of    (cid:22)    and    (cid:23)   ; the meaning
must be determined contextually (i.e., both sides of the inequality will be vectors).

2

    intersections of convex sets. suppose c1, c2, . . . , ck are convex sets. then their

intersection

k

ci = {x : x     ci    i = 1, . . . , k}

\i=1

is also a convex set. to see this, consider x, y    tk

  x + (1       )y     ci    i = 1, . . . , k

i=1 ci and 0            1. then,

by the de   nition of a convex set. therefore

k

note, however, that the union of convex sets in general will not be convex.

  x + (1       )y    

ci.

\i=1

    positive semide   nite matrices. the set of all symmetric positive semide   nite
matrices, often times called the positive semide   nite cone and denoted sn
+, is a convex
set (in general, sn     rn  n denotes the set of symmetric n    n matrices). recall that
a matrix a     rn  n is symmetric positive semide   nite if and only if a = at and for
all x     rn, xt ax     0. now consider two symmetric positive semide   nite matrices
a, b     sn

+ and 0            1. then for any x     rn,

xt (  a + (1       )b)x =   xt ax + (1       )xt bx     0.

the same logic can be used to show that the sets of all positive de   nite, negative
de   nite, and negative semide   nite matrices are each also convex.

3 convex functions

a central element in id76 is the notion of a convex function.

de   nition 3.1 a function f : rn     r is convex if its domain (denoted d(f )) is a convex
set, and if, for all x, y     d(f ) and        r, 0            1,

f (  x + (1       )y)       f (x) + (1       )f (y).

intuitively, the way to think about this de   nition is that if we pick any two points on the
graph of a convex function and draw a straight line between then, then the portion of the
function between these two points will lie below this straight line. this situation is pictured
in figure 2.2

we say a function is strictly convex if de   nition 3.1 holds with strict inequality for
x 6= y and 0 <    < 1. we say that f is concave if    f is convex, and likewise that f is
strictly concave if    f is strictly convex.

2don   t worry too much about the requirement that the domain of f be a convex set. this is just a
technicality to ensure that f (  x + (1       )y) is actually de   ned (if d(f ) were not convex, then it could be
that f (  x + (1       )y) is unde   ned even though x, y     d(f )).

3

figure 2: graph of a convex function. by the de   nition of convex functions, the line con-
necting two points on the graph must lie above the function.

3.1 first order condition for convexity

suppose a function f : rn     r is di   erentiable (i.e., the gradient3    xf (x) exists at all
points x in the domain of f ). then f is convex if and only if d(f ) is a convex set and for
all x, y     d(f ),

f (y)     f (x) +    xf (x)t (y     x).

the function f (x) +    xf (x)t (y     x) is called the    rst-order approximation to the
function f at the point x. intuitively, this can be thought of as approximating f with its
tangent line at the point x. the    rst order condition for convexity says that f is convex if
and only if the tangent line is a global underestimator of the function f . in other words, if
we take our function and draw a tangent line at any point, then every point on this line will
lie below the corresponding point on f .

similar to the de   nition of convexity, f will be strictly convex if this holds with strict
inequality, concave if the inequality is reversed, and strictly concave if the reverse inequality
is strict.

figure 3: illustration of the    rst-order condition for convexity.

3recall that the gradient is de   ned as    xf (x)     rn, (   xf (x))i =    f (x)
   xi

hessians, see the previous section notes on id202.

. for a review on gradients and

4

3.2 second order condition for convexity

suppose a function f : rn     r is twice di   erentiable (i.e., the hessian4    2
xf (x) is de   ned
for all points x in the domain of f ). then f is convex if and only if d(f ) is a convex set and
its hessian is positive semide   nite: i.e., for any x     d(f ),

   2

xf (x) (cid:23) 0.

here, the notation    (cid:23)    when used in conjunction with matrices refers to positive semide   -
niteness, rather than componentwise inequality. 5 in one dimension, this is equivalent to the
condition that the second derivative f       (x) always be positive (i.e., the function always has
positive curvature).

again analogous to both the de   nition and    rst order conditions for convexity, f is strictly
convex if its hessian is positive de   nite, concave if the hessian is negative semide   nite, and
strictly concave if the hessian is negative de   nite.

3.3 jensen   s inequality

suppose we start with the inequality in the basic de   nition of a convex function

f (  x + (1       )y)       f (x) + (1       )f (y)

for 0            1.

using induction, this can be fairly easily extended to convex combinations of more than one
point,

f  k
xi=1

  ixi!    

xi=1

k

k

  if (xi)

for

  i = 1,   i     0    i.

xi=1

in fact, this can also be extended to in   nite sums or integrals.
inequality can be written as

in the latter case, the

f(cid:18)z p(x)xdx(cid:19)    z p(x)f (x)dx for z p(x)dx = 1, p(x)     0    x.

because p(x) integrates to 1, it is common to consider it as a id203 density, in which
case the previous equation can be written in terms of expectations,

this last inequality is known as jensen   s inequality, and it will come up later in class.6

f (e[x])     e[f (x)].

4recall the hessian is de   ned as    2
5similarly, for a symmetric matrix x     sn, x (cid:22) 0 denotes that x is negative semide   nite. as with vector
inequalities,           and           are sometimes used in place of    (cid:22)    and    (cid:23)   . despite their notational similarity to
vector inequalities, these concepts are very di   erent; in particular, x (cid:23) 0 does not imply that xij     0 for
all i and j.

xf (x)     rn  n, (   2

   xi   xj

xf (x))ij =     2f (x)

6in fact, all four of these equations are sometimes referred to as jensen   s inequality, due to the fact that
they are all equivalent. however, for this class we will use the term to refer speci   cally to the last inequality
presented here.

5

3.4 sublevel sets

convex functions give rise to a particularly important type of convex set called an   -sublevel
set. given a convex function f : rn     r and a real number        r, the   -sublevel set is
de   ned as

{x     d(f ) : f (x)       }.

in other words, the   -sublevel set is the set of all points x such that f (x)       .

to show that this is a convex set, consider any x, y     d(f ) such that f (x)        and

f (y)       . then

f (  x + (1       )y)       f (x) + (1       )f (y)          + (1       )   =   .

3.5 examples

we begin with a few simple examples of convex functions of one variable, then move on to
multivariate functions.

    exponential. let f : r     r, f (x) = eax for any a     r. to show f is convex, we can

simply take the second derivative f       (x) = a2eax, which is positive for all x.

    negative logarithm. let f : r     r, f (x) =     log x with domain d(f ) = r++
(here, r++ denotes the set of strictly positive real numbers, {x : x > 0}). then
f       (x) = 1/x2 > 0 for all x.

    a   ne functions. let f : rn     r, f (x) = bt x + c for some b     rn, c     r. in
this case the hessian,    2
xf (x) = 0 for all x. because the zero matrix is both positive
semide   nite and negative semide   nite, f is both convex and concave. in fact, a   ne
functions of this form are the only functions that are both convex and concave.

    quadratic functions. let f : rn     r, f (x) = 1

2xt ax + bt x + c for a symmetric
matrix a     sn, b     rn and c     r. in our previous section notes on id202, we
showed the hessian for this function is given by

   2

xf (x) = a.

therefore, the convexity or non-convexity of f is determined entirely by whether or
not a is positive semide   nite: if a is positive semide   nite then the function is convex
(and analogously for strictly convex, concave, strictly concave). if a is inde   nite then
f is neither convex nor concave.
note that the squared euclidean norm f (x) = kxk2
functions where a = i, b = 0, c = 0, so it is therefore a strictly convex function.

2 = xt x is a special case of quadratic

6

    norms. let f : rn     r be some norm on rn. then by the triangle inequality and

positive homogeneity of norms, for x, y     rn, 0            1,

f (  x + (1       )y)     f (  x) + f ((1       )y) =   f (x) + (1       )f (y).

this is an example of a convex function where it is not possible to prove convexity based
on the second or    rst order conditions, because norms are not generally di   erentiable
i=1 |xi|, is non-di   erentiable at all points where

everywhere (e.g., the 1-norm, ||x||1 =pn

any xi is equal to zero).

    nonnegative weighted sums of convex functions. let f1, f2, . . . , fk be convex

functions and w1, w2, . . . , wk be nonnegative real numbers. then

is a convex function, since

f (x) =

wifi(x)

k

xi=1

k

f (  x + (1       )y) =

   

wifi(  x + (1       )y)

wi(  fi(x) + (1       )fi(y))

k

xi=1
xi=1
xi=1

k

=   

wifi(x) + (1       )

=   f (x) + (1       )f (x).

wifi(y)

k

xi=1

4 id76 problems

armed with the de   nitions of convex functions and sets, we are now equipped to consider
id76 problems. formally, a id76 problem in an opti-
mization problem of the form

minimize f (x)
subject to x     c

where f is a convex function, c is a convex set, and x is the optimization variable. however,
since this can be a little bit vague, we often write it often written as

minimize f (x)
subject to gi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p

where f is a convex function, gi are convex functions, and hi are a   ne functions, and x is
the optimization variable.

7

is it imporant to note the direction of these inequalities: a convex function gi must be
less than zero. this is because the 0-sublevel set of gi is a convex set, so the feasible region,
which is the intersection of many convex sets, is also convex (recall that a   ne subspaces are
convex sets as well). if we were to require that gi     0 for some convex gi, the feasible region
would no longer be a convex set, and the algorithms we apply for solving these problems
would not longer be guaranteed to    nd the global optimum. also notice that only a   ne
functions are allowed to be equality constraints. intuitively, you can think of this as being
due to the fact that an equality constraint is equivalent to the two inequalities hi     0 and
hi     0. however, these will both be valid constraints if and only if hi is both convex and
concave, i.e., hi must be a   ne.

the optimal value of an optimization problem is denoted p    (or sometimes f    ) and is

equal to the minimum possible value of the objective function in the feasible region7

p    = min{f (x) : gi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p}.

we allow p    to take on the values +    and        when the problem is either infeasible (the
feasible region is empty) or unbounded below (there exists feasible points such that f (x)    
      ), respectively. we say that x    is an optimal point if f (x   ) = p   . note that there can
be more than one optimal point, even when the optimal value is    nite.

4.1 global optimality in convex problems

before stating the result of global optimality in convex problems, let us formally de   ne
the concepts of local optima and global optima. intuitively, a feasible point is called locally
optimal if there are no    nearby    feasible points that have a lower objective value. similarly,
a feasible point is called globally optimal if there are no feasible points at all that have a
lower objective value. to formalize this a little bit more, we give the following two de   nitions.

de   nition 4.1 a point x is locally optimal if it is feasible (i.e., it satis   es the constraints
of the optimization problem) and if there exists some r > 0 such that all feasible points z
with kx     zk2     r, satisfy f (x)     f (z).

de   nition 4.2 a point x is globally optimal if it is feasible and for all feasible points z,
f (x)     f (z).

we now come to the crucial element of id76 problems, from which they
derive most of their utility. the key idea is that for a id76 problem
all locally optimal points are globally optimal .

let   s give a quick proof of this property by contradiction. suppose that x is a locally
optimal point which is not globally optimal, i.e., there exists a feasible point y such that

7math majors might note that the min appearing below should more correctly be an inf. we won   t worry

about such technicalities here, and use min for simplicity.

8

f (x) > f (y). by the de   nition of local optimality, there exist no feasible points z such that
kx     zk2     r and f (z) < f (x). but now suppose we choose the point

z =   y + (1       )x with    =

r

2kx     yk2

.

then

r

2kx     yk2

y +(cid:18)1    

kx     zk2 = (cid:13)(cid:13)(cid:13)(cid:13)
x    (cid:18)
= (cid:13)(cid:13)(cid:13)(cid:13)

r

= r/2     r.

2kx     yk2

(x     y)(cid:13)(cid:13)(cid:13)(cid:13)2

in addition, by the convexity of f we have

r

2kx     yk2(cid:19) x(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2

f (z) = f (  y + (1       )x)       f (y) + (1       )f (x) < f (x).

furthermore, since the feasible set is a convex set, and since x and y are both feasible
z =   y + (1       ) will be feasible as well. therefore, z is a feasible point, with kx     zk2 < r
and f (z) < f (x). this contradicts our assumption, showing that x cannot be locally optimal.

4.2 special cases of convex problems

for a variety of reasons, it is often times convenient to consider special cases of the general
convex programming formulation. for these special cases we can often devise extremely
e   cient algorithms that can solve very large problems, and because of this you will probably
see these special cases referred to any time people use id76 techniques.

    id135. we say that a id76 problem is a linear
program (lp) if both the objective function f and inequality constraints gi are a   ne
functions. in other words, these problems have the form

minimize ct x + d
subject to gx (cid:22) h
ax = b

where x     rn is the optimization variable, c     rn, d     r, g     rm  n, h     rm,
a     rp  n, b     rp are de   ned by the problem, and    (cid:22)    denotes elementwise inequality.

    quadratic programming. we say that a id76 problem is a quadratic
program (qp) if the inequality constraints gi are still all a   ne, but if the objective
function f is a convex quadratic function. in other words, these problems have the
form,

1

2xt p x + ct x + d

minimize
subject to gx (cid:22) h
ax = b

9

where again x     rn is the optimization variable, c     rn, d     r, g     rm  n, h     rm,
a     rp  n, b     rp are de   ned by the problem, but we also have p     sn
+, a symmetric
positive semide   nite matrix.

    quadratically constrained quadratic programming. we say that a convex
optimization problem is a quadratically constrained quadratic program (qcqp)
if both the objective f and the inequality constraints gi are convex quadratic functions,

1
minimize
subject to 1

2xt p x + ct x + d
2xt qix + rt
ax = b

i x + si     0,

i = 1, . . . , m

where, as before, x     rn is the optimization variable, c     rn, d     r, a     rp  n, b     rp,
p     sn

+, ri     rn, si     r, for i = 1, . . . , m.

+, but we also have qi     sn

    semide   nite programming. this last example is a bit more complex than the pre-
vious ones, so don   t worry if it doesn   t make much sense at    rst. however, semide   nite
programming is become more and more prevalent in many di   erent areas of machine
learning research, so you might encounter these at some point, and it is good to have an
idea of what they are. we say that a id76 problem is a semide   nite
program (sdp) if it is of the form

minimize tr(cx)
subject to tr(aix) = bi,

i = 1, . . . , p

x (cid:23) 0

where the symmetric matrix x     sn is the optimization variable, the symmetric ma-
trices c, a1, . . . , ap     sn are de   ned by the problem, and the constraint x (cid:23) 0 means
that we are constraining x to be positive semide   nite. this looks a bit di   erent than
the problems we have seen previously, since the optimization variable is now a matrix
instead of a vector. if you are curious as to why such a formulation might be useful,
you should look into a more advanced course or book on id76.

it should be fairly obvious from the de   nitions that quadratic programs are more general
than linear programs (since a linear program is just a special case of a quadratic program
where p = 0), and likewise that quadratically constrained quadratic programs are more
general than quadratic programs. however, what is not obvious at all is that semide   nite
programs are in fact more general than all the previous types. that is, any quadratically
constrained quadratic program (and hence any quadratic program or linear program) can
be expressed as a semide   nte program. we won   t discuss this relationship further in this
document, but this might give you just a small idea as to why semide   nite programming
could be useful.

10

4.3 examples

now that we   ve covered plenty of the boring math and formalisms behind convex optimiza-
tion, we can    nally get to the fun part: using these techniques to solve actual problems.
we   ve already encountered a few such optimization problems in class, and in nearly every
   eld, there is a good chance that someone has tried to apply id76 to solve
some problem.

    support vector machines. one of the most prevalent applications of convex op-
timization methods in machine learning is the support vector machine classi   er. as
discussed in class,    nding the support vector classi   er (in the case with slack variables)
can be formulated as the optimization problem

1

minimize
subject to y(i)(wt x(i) + b)     1       i,

i=1   i

2kwk2

2 + cpm

  i     0,

i = 1, . . . , m
i = 1, . . . , m

with optimization variables w     rn,        rm, b     r, and where c     r and x(i), y(i), i =
1, . . . m are de   ned by the problem. this is an example of a quadratic program, which
we try to put the problem into the form described in the previous section. in particular,
if de   ne k = m + n + 1, let the optimization variable be

and de   ne the matrices

w
  
b

   
   

x     rk       
   
    , c     rk =   
   
   
0 (cid:21)
0 (cid:21) , h     r2m =(cid:20)    1

i 0 0
0 0 0
0 0 0

   
    ,

c    1

0

0

p     rk  k =   
   

g     r2m  k =(cid:20)    diag(y)x    i    y

   i

0

where i is the identity, 1 is the vector of all ones, and x and y are de   ned as in class,

x     rm  n =

x(1)t
x(2)t
...

x(m)t

   
               

   
               

, y     rm =   
            

y(1)
y(2)
...
y(m)

.

   
            

you should try to convince yourself that the quadratic program described in the pre-
vious section, when using these matrices de   ned above, is equivalent to the id166
optimization problem. in reality, it is fairly easy to see that there the id166 optimiza-
tion problem has a quadratic objective and linear constraints, so we typically don   t
need to put it into standard form to    prove    that it is a qp, and would only do so if
we are using an o   -the-shelf solver that requires the input to be in standard form.

11

    constrained least squares. in class we have also considered the least squares prob-
lem, where we want to minimize kax     bk2
2 for some matrix a     rm  n and b     rm.
as we saw, this particular problem can actually be solved analytically via the normal
equations. however, suppose that we also want to constrain the entries in the solution
x to lie within some prede   ned ranges. in other words, suppose we weanted to solve
the optimization problem,

minimize
subject to l (cid:22) x (cid:22) u

2 kax     bk2

2

1

with optimization variable x and problem data a     rm  n, b     rm, l     rn, and u     rn.
this might seem like a fairly simple additional constraint, but it turns out that there
will no longer be an analytical solution. however, you should be able to convince
yourself that this optimization problem is a quadratic program, with matrices de   ned
by

p     rn  n =

at a, c     rn =    bt a, d     r =

bt b,

1
2

1
2

g     r2n  2n =(cid:20)    i 0

u (cid:21) .
i (cid:21) , h     r2n =(cid:20)    l

0

    maximum likelihood for id28. for homework one, you were
required to show that the log-likelihood of the data in a logistic model was concave.
this log likehood under such a model is

n

   (  ) =

xi=1 (cid:8)y(i) ln g(  t x(i)) + (1     y(i)) ln(1     g(  t x(i)))(cid:9)

where g(z) denotes the logistic function g(z) = 1/(1 + e   z). finding the maximum
likelihood estimate is then a task of maximizing the log-likelihood (or equivalently,
minimizing the negative log-likelihood, a convex function), i.e.,

minimize       (  )

with optimization variable        rn and no constraints.

unlike the previous two examples, it turns out that it is not so easy to put this prob-
lem into a    standard    form optimization problem. nevertheless, you   ve seen on the
homework that the fact that     is a concave function means that you can very e   ciently
   nd the global solution using an algorithm such as newton   s method.

references

[1] stephen boyd and lieven vandenberghe. id76. cambridge up, 2004.

online: http://www.stanford.edu/   boyd/cvxbook/

12

