on computational 
hardness and graph 
neural networks

joan bruna , cims + cds, nyu 

in collaboration with l.li (uc berkeley), soledad villar, 
afonso bandeira (nyu), alex nowak (inria-paris), 

d.folque (nyu).

the    deep learning slide   

cvpr
#1191

cvpr 2016 submission #1191. confidential review copy. do not distribute.

cvpr
#1191

unpaired image-to-image translation

using cycle-consistent adversarial networks

a

jun-yan zhu   

taesung park   

phillip isola

alexei a. efros

c

berkeley ai research (bair) laboratory, uc berkeley

monet        photos

zebras

horses

monet        photo

zebra        horse

summer 

winter

e

summer        winter

b

d

f

432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485

figure 3. images that combine the content of a photograph with the style of several well-known artworks. the images were created by
   nding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork.
the original photograph depicting the neckarfront in t  ubingen, germany, is shown in a (photo: andreas praefcke). the painting that
provided the style for the respective generated image is shown in the bottom left corner of each panel. b the shipwreck of the minotaur
by j.m.w. turner, 1805. c the starry night by vincent van gogh, 1889. d der schrei by edvard munch, 1893. e femme nue assise by
pablo picasso, 1910. f composition vii by wassily kandinsky, 1913.

photo       monet

horse        zebra

winter        summer

5

photograph

monet

van gogh

cezanne

ukiyo-e

figure 1: given any two unordered image collections x and y , our algorithm learns to automatically    translate    an image
from one into the other and vice versa: (left) 1074 monet paintings and 6753 landscape photos from flickr; (center) 1177 ze-

7
1
0
2
 
r
a

 

m
0
3

 
 
]

v
c
.
s
c
[
 
 

1
v
3

unpaired image-to-image translation

the    deep learning slide   

using cycle-consistent adversarial networks

taesung park   

jun-yan zhu   

berkeley ai research (bair) laboratory, uc berkeley

phillip isola

cvpr
#1191

alexei a. efros

cvpr 2016 submission #1191. confidential review copy. do not distribute.

monet        photos

zebras

horses

monet        photo

zebra        horse

photo       monet

horse        zebra

7
1
0
2
 
r
a

 

m
0
3
 
 
]

a

summer 

winter

c

summer        winter

winter        summer

e

b

d

f

432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485

cvpr
#1191

486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539

v
c
.
s
c
[
 
 
1
v
3
9
5
0
1
.
3
0
7
1
:
v
i
x
r
a

photograph

monet

van gogh

cezanne

ukiyo-e

figure 1: given any two unordered image collections x and y , our algorithm learns to automatically    translate    an image
from one into the other and vice versa: (left) 1074 monet paintings and 6753 landscape photos from flickr; (center) 1177 ze-
bras and 939 horses from id163; (right) 1273 summer and 854 winter yosemite photos from flickr. example application
(bottom): using a collection of paintings of a famous artist, learn to render a user   s photograph into their style.

5

figure 3. images that combine the content of a photograph with the style of several well-known artworks. the images were created by
   nding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork.
the original photograph depicting the neckarfront in t  ubingen, germany, is shown in a (photo: andreas praefcke). the painting that
provided the style for the respective generated image is shown in the bottom left corner of each panel. b the shipwreck of the minotaur
by j.m.w. turner, 1805. c the starry night by vincent van gogh, 1889. d der schrei by edvard munch, 1893. e femme nue assise by
pablo picasso, 1910. f composition vii by wassily kandinsky, 1913.

abstract

1. introduction

    despite mathematical mysteries, proven ability to extract 
robust information out of high-dimensional data, across 
di   erent domains and tasks. 

what did claude monet see as he placed his easel by the
bank of the seine near argenteuil on a lovely spring day
in 1873 (figure 1, top-left)? a color photograph, had it
been invented, may have documented a crisp blue sky and
a glassy river re   ecting it. monet conveyed his impression
of this same scene through wispy brush strokes and a bright
palette.

    most domains have regular spatial, temporal or sequential 

image-to-image translation is a class of vision and
graphics problems where the goal is to learn the mapping
between an input image and an output image using a train-
ing set of aligned image pairs. however, for many tasks,
paired training data will not be available. we present an
approach for learning to translate an image from a source
domain x to a target domain y in the absence of paired
examples. our goal is to learn a mapping g : x ! y
such that the distribution of images from g(x) is indistin-
guishable from the distribution y using an adversarial loss.
because this mapping is highly under-constrained, we cou-
ple it with an inverse mapping f : y ! x and introduce
a cycle consistency loss to push f (g(x))     x (and vice
versa). qualitative results are presented on several tasks
where paired training data does not exist, including col-
lection style transfer, object trans   guration, season trans-
fer, and photo enhancement, etc. quantitative comparisons
against several prior methods demonstrate the superiority
of our approach.

structure.

what if monet had happened upon the little harbor in
cassis on a cool summer evening (figure 1, bottom-left)?
a brief stroll through a gallery of monet paintings makes
it easy to imagine how he would have rendered the scene:
perhaps in pastel shades, with abrupt dabs of paint, and a
somewhat    attened dynamic range.

we can imagine all this despite never having seen a side
by side example of a monet painting next to a photo of the
scene he painted. instead we have knowledge of the set of
monet paintings and of the set of landscape photographs.

* indicates equal contribution

1

the    deep learning slide   

unpaired image-to-image translation

using cycle-consistent adversarial networks

jun-yan zhu   

taesung park   

phillip isola

alexei a. efros

berkeley ai research (bair) laboratory, uc berkeley

cvpr
#1191

cvpr 2016 submission #1191. confidential review copy. do not distribute.

cvpr
#1191

monet        photos

zebras

horses

monet        photo

zebra        horse

photo       monet

horse        zebra

7
1
0
2
 
r
a

m
 
0
3
 
 
]

a

summer 

winter

c

summer        winter

winter        summer

e

b

d

f

432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485

486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539

v
c
.
s
c
[
 
 
1
v
3
9
5
0
1
3
0
7
1
:
v
i
x
r
a

.

photograph

monet

van gogh

cezanne

ukiyo-e

figure 3. images that combine the content of a photograph with the style of several well-known artworks. the images were created by
   nding an image that simultaneously matches the content representation of the photograph and the style representation of the artwork.
the original photograph depicting the neckarfront in t  ubingen, germany, is shown in a (photo: andreas praefcke). the painting that
provided the style for the respective generated image is shown in the bottom left corner of each panel. b the shipwreck of the minotaur
by j.m.w. turner, 1805. c the starry night by vincent van gogh, 1889. d der schrei by edvard munch, 1893. e femme nue assise by
pablo picasso, 1910. f composition vii by wassily kandinsky, 1913.

5

abstract

1. introduction

figure 1: given any two unordered image collections x and y , our algorithm learns to automatically    translate    an image
from one into the other and vice versa: (left) 1074 monet paintings and 6753 landscape photos from flickr; (center) 1177 ze-
bras and 939 horses from id163; (right) 1273 summer and 854 winter yosemite photos from flickr. example application
(bottom): using a collection of paintings of a famous artist, learn to render a user   s photograph into their style.

    despite mathematical mysteries, proven ability to extract 
robust information out of high-dimensional data, across 
di   erent domains and tasks. 

what did claude monet see as he placed his easel by the
bank of the seine near argenteuil on a lovely spring day
in 1873 (figure 1, top-left)? a color photograph, had it
been invented, may have documented a crisp blue sky and
a glassy river re   ecting it. monet conveyed his impression
of this same scene through wispy brush strokes and a bright
palette.

what if monet had happened upon the little harbor in
cassis on a cool summer evening (figure 1, bottom-left)?
a brief stroll through a gallery of monet paintings makes
it easy to imagine how he would have rendered the scene:
perhaps in pastel shades, with abrupt dabs of paint, and a
somewhat    attened dynamic range.

image-to-image translation is a class of vision and
graphics problems where the goal is to learn the mapping
between an input image and an output image using a train-
ing set of aligned image pairs. however, for many tasks,
paired training data will not be available. we present an
approach for learning to translate an image from a source
domain x to a target domain y in the absence of paired
examples. our goal is to learn a mapping g : x ! y
such that the distribution of images from g(x) is indistin-
guishable from the distribution y using an adversarial loss.
because this mapping is highly under-constrained, we cou-
ple it with an inverse mapping f : y ! x and introduce
a cycle consistency loss to push f (g(x))     x (and vice
versa). qualitative results are presented on several tasks
where paired training data does not exist, including col-
lection style transfer, object trans   guration, season trans-
fer, and photo enhancement, etc. quantitative comparisons
against several prior methods demonstrate the superiority
of our approach.

    most domains have regular spatial, temporal or sequential 

we can imagine all this despite never having seen a side
by side example of a monet painting next to a photo of the
scene he painted. instead we have knowledge of the set of
monet paintings and of the set of landscape photographs.

* indicates equal contribution

1

structure. 

    at the core of this success, there is an inductive bias captured 

in particular by convolutional (or auto-regressive) models.  

    how to formalize this inductive bias?  
    and extend it to more general domains and tasks?

outline
    geometric stability 

    in euclidean domains: convolutional neural networks. 
    in non-euclidean domains: graph neural networks. 

    applications to inverse problems on graphs 

    community detection and statistical-to-computational gaps. 
    quadratic assignment problem 
    givens factorization of unitary operators.

geometric stability in euclidean domains
    consider data de   ned as functions over an euclidean domain: 
x = x(u) , u 2         rd

d = 1: time series
d = 2: images ..

    id161 task:  

y =    {c1, . . . , ck} classi   cation

    goal: estimate    from samples  

localization .

y = f (x)

   

f

f : l2(   ) ! y

{(xl, yl = f (xl)}l   l

geometric stability in euclidean domains
    consider data de   ned as functions over an euclidean domain: 
x = x(u) , u 2         rd

d = 1: time series
d = 2: images ..

    id161 task:  

y =    {c1, . . . , ck} classi   cation

    goal: estimate    from samples   

localization .

y = f (x)

   

f

f : l2(   ) ! y

{(xl, yl = f (xl)}l   l

    q: what assumptions on    ? 

f

geometric stability in euclidean domains

x(u) , u : pixels, time samples, etc.

    (u) , : deformation    eld

x    (u) := x(u       (u)) : warping

video of philipp scott johnson

   deformation cost:  

kr   k = sup
   models change in point of view in images 
   models frequency transpositions in sounds 
   consistent with local translation invariance

u |r    (u)|

geometric stability in euclidean domains
    most id161 and speech tasks   also satisfy:  
|f (x)   f (x    )|     kr   k , (geometric invariance)
|[f (x)]      f (x    )|     kr   k , (geometric equivariance)

f

e.g. image classification

e.g. image localization 

    in particular, these tasks are translation invariant/equivariant: 
translation operator: xv(u) = x(u   v), v 2    .
f (x) = f (xv) for all x. (translation invariance)
[f (x)]v = f (xv) for all x. (translation equivariance)

f

geometric stability in euclidean domains
    most id161 and speech tasks   also satisfy:  
|f (x)   f (x    )|     kr   k , (geometric invariance)
|[f (x)]      f (x    )|     kr   k , (geometric equivariance)
    in particular, these tasks are translation invariant/equivariant:  
translation operator: xv(u) = x(u   v), v 2    .
f (x) = f (xv) for all x. (translation invariance)
[f (x)]v = f (xv) for all x. (translation equivariance)

e.g. image classification

e.g. image localization 

    whereas translation and other symmetry groups are low-

dimensional, deformation stability is a high-dimensional prior. 

    q: how to leverage this stability prior? 

convolutional neural networks
    stack multiple layers of localized convolutional operators and 

[lecun, 80s,90s]

point-wise contractive non-linearities: 

max

pooling

convolutions 

+ relu

max

pooling

convolutions 

+ relu

...

airedale terrier (16)

fox terrier (5.7)

pomeranzian (2.7)

arctic fox (1.0)

eskimo dog (0.6)

wolf (0.4)

siberian husky (0.4)

input image

convolutions 

+ relu

input: x 2 l2(   , rp).

  x  j(u) =    0@
pxj=1

xj ?    j,  j(u)1a ,   j       p .

output:   x 2 l2(   , r  p).

   (z): point-wise nonlinearity
(e.g. max(0, z)).

    = (   j,  j): localized
convolutional kernel.

    down-sampling via pooling (can be either linear with average, 

or nonlinear with max) in invariant tasks:  
  x  j(  u) = k  x  j(n (u))k

n (u): neighborhood of u.

convolutional neural networks
    why are id98s geometrically stable?

x(u)

x    (u)

x   0(u)

convolutional neural networks
    why are id98s geometrically stable? 

x(u)

x    (u)

x   0(u)

kr   k

    a non-rigid deformation locally looks like a translation if            small: 

) x    ?    (u)     [x ?    ]    (u)
    a point-wise nonlinearity commutes with deformations: 
)     (x    ?    (u))         ([x ?    ]    (u)) = [    (x ?    )]    (u)
    pooling progressively creates invariance to geometric deformations:
kx    (n (u))k     kx(n (u))k if |   |small

convolutional neural networks

    convolutions to exploit translation invariance/equivariance. 
    localized to exploit geometric stability: leads to multi scale architecture. 
    these two properties lead to models with                  trainable parameters.  

o(log n )

    provable stability guarantees by    xing    lters to be complex wavelets in 

scattering networks [mallat   12] and generalizations [boelcksei et al   16]. 

    stability is only part of the story. discriminability via learning/optimization 

is another major component for success.

15

towards non-euclidean geometries
    how about problems/tasks de   ned over more general 

[in4] citation network analysis example: the
cora citation network [89] is a graph containing
2708 vertices representing papers and 5429 edges
representing citations. each paper is described by a
1433-dimensional bag-of-words
feature vector and
belongs to seven classes. for simplicity, the network
is
treated as an undirected graph. applying the
spectral id98 with two spectral convolutional layers
parametrized according to (51),
the authors of [75]
obtain classi   cation accuracy of 81.5% (compared to
75.7% previous best result).

domains?

community detection

id191

[figs4a] classifying research papers in the cora dataset
with spectral id98. shown is the citation graph, where each
node is a paper, and an edge represents a citation. vertex
   ll color represents the predicted label; vertex outline color
represents the groundtruth label
the two colors
should coincide).

(ideally,

in the domain of computer graphics, on the other hand,
working intrinsically with geometric shapes is a standard
practice. in this    eld, 3d shapes are typically modeled as rie-
mannian manifolds and are discretized as meshes. numerous
studies (see, e.g. [94], [95], [96], [97], [3]) have been devoted
to designing local and global features e.g. for establishing
similarity or correspondence between deformable shapes with
guaranteed invariance properties. two well-studied classes

graphics

high energy physics

quantum chemistry

ieeesigprocmag16correspondencesimilarityfig.4.left:featuresusedforshapecorrespondenceshouldideallymanifestinvarianceacrosstheshapeclass(e.g.,the   kneefeature   shownhereshouldnotdependonthespeci   cperson).right:onthecontrary,featuresusedforshaperetrievalshouldbespeci   ctoashapewithintheclasstoallowdistinguishingbetweendifferentpeople.similarfeaturesaremarkedwithsamecolor.hand-craftingtherightfeatureforeachapplicationisaverychallengingtask.graphicsmayrequirecompletelydifferentfeatures:forin-stance,inordertoestablishfeature-basedcorrespondencebetweenacollectionofhumanshapes,onewoulddesirethedescriptorsofcorrespondinganatomicalparts(noses,mouths,etc.)tobeassimilaraspossibleacrossthecollection.inotherwords,suchdescriptorsshouldbeinvarianttothecollectionvariability.conversely,forshapeclassi   cation,onewouldlikedescriptorsthatemphasizethesubject-speci   ccharacteristics,andforexample,distinguishbetweentwodifferentnoseshapes(seefigureviii).decidingaprioriwhichstructuresshouldbeusedandwhichshouldbeignoredisoftenhardorsometimesevenimpossible.moreover,axiomaticmodelingofgeometricnoisesuchas3dscanningartifactsturnsouttobeextremelyhard.putinasomewhatoversimpli   edmanner,thecomputervisioncommunityworkswithreal-world3ddata,butuseseuclideantechniquesoriginallydevelopedforimagesthatarenotsuitableforgeometricdata.atthesametime,themathematicallyrigorousmodelsusedincomputergraphicstodescribegeometricobjectscanhardlydealwithnoisydata,leadingtoatendencytoworkwithidealizedsyntheticshapes.webelievethatthegapbetweenthetwocommunitiescanbebridgedwiththedevelopmentofgeometricdeeplearningmethods.byresortingtointrinsicdeepneuralnetworks,theinvariancetoisometricdeformationsisautomaticallybuiltintothemodel,thusvastlyreducingthenumberofdegreesoffreedomrequiredtodescribetheinvarianceclass.roughlyspeaking,theintrinsicdeepmodelwilltrytolearn   residual   deformationsthatdeviatefromtheisometricmodel.intrinsicdeeplearningcanbeappliedtoseveralproblemsin3dshapeanalysis,whichcanbedividedintwoclasses.first,problemssuchaslocaldescriptorlearning[26],[77]orcorrespondencelearning[28](seeexampleintheinsertin5),inwhichtheoutputofthenetworkispoint-wise.theinputstothenetworkaresomepoint-wisefeatures,forexample,colortextureorsimplegeometricfeatures.usingaid98architecturewithmultipleintrinsicconvolutionallayers,itispossibletoproducenon-localfeaturesthatcapturethecontextaroundeachpoint.thesecondtypeofproblemssuchasshaperecognitionrequirethenetworktoproduceaglobalshapedescriptor,aggregatingallthelocalinformationintoasinglevectorusinge.g.thecovariancepooling.ix.openproblemsandfuturedirectionstherecentemergenceofgeometricdeeplearningmethodsinvariouscommunitiesandapplicationdomains,whichwetriedtooverviewinthispaper,allowustoproclaim,perhapswithsomecaution,thatwemightbewitnessinganew   eldbeingborn.weexpectthefollowingyearstobringexcitingnewapproachesandresults,andconcludeourreviewwithafewobservationsofcurrentkeydif   cultiesandpotentialdirectionsoffutureresearch.manydisciplinesdealingwithgeometricdataemploysomeempiricalmodelsor   handcrafted   features.thisisatypicalsituationincomputationalsociology,whereitiscommonto   rstcomeupwithahypothesisandthentestitonthedata[1],orgeometryprocessingandcomputergraphics,whereaxiomatically-constructedfeaturesareusedtoanalyze3dshapes.yet,suchmodelsassumesomepriorknowledge(e.g.isometricshapedeformationmodel),andoftenfailtocorrectlycapturethefullcomplexityandrichnessofthedata.incomputervision,departingfrom   handcrafted   featurestowardsgenericmodelslearnablefromthedatainatask-speci   cmannerhasbroughtabreakthroughinperformanceandledtoanoverwhelmingtrendinthecommunitytofavordeeplearningmethods.suchashifthasnotoccurredyetinthe   eldsdealingwithgeometricdataduetothelackofadequatemethods,butthereare   rstindicationsofacomingparadigmshift.insomeapplications,geometricdatacanalsobehan-dledasaeuclideanstructure,allowingtoresorttoclassicaldeeplearningtechniques.indeformation-invariant3dshapecorrespondenceapplicationwementionedinthecontextofcomputergraphics,3dshapescanbeconsideredbothas2dmanifoldsandassubsetsofthe3deuclideanspace.thelatterrepresentationfailstocorrectlycapturethegeometricstructureofthedata,asitisextrinsicandnotinvariantundernon-rigiddeformations.whileinprincipleitispossibletoapplyclassicaldeeplearningtoeuclideanrepresentationsofnon-rigidshapes,suchmodelstendtobeverycomplexandrequirelargeamountsoftrainingdata[91].themaincontributionofintrinsicdeeplearninginthesesettingsisusingamoresuitablemodelwithguaranteedinvariancepropertiesthatappeartobemuchsimplerthantheeuclideanones.anotherimportantaspectisgeneralizationcapabilitiesandtransferlearning.generalizingdeeplearningmodelstogeo-metricdatarequiresnotonly   ndingnon-euclideancounter-partsofbasicbuildingblocks(suchasconvolutionalandpool-inglayers),butalsogeneralizationacrossdifferentdomains.generalizationcapabilityisakeyrequirementinmanyappli-cations,includingcomputergraphics,whereamodelislearnedonatrainingsetofnon-euclideandomains(3dshapes)andthenappliedtopreviouslyunseenones.recallingtheap-proacheswementionedinthisreview,spectralformulationofconvolutionallowsdesigningid98sonagraph,butthemodellearnedthiswayononegraphcannotbestraightforwardlynon-euclidean geometric stability
    we replace the euclidean domain     by a general graph   
x(u) 2 l2(   ) ! x(u) 2 l2(g) , g = (v, e) .
    in some applications, the input is the graph itself:  

x $ g
    we focus on undirected, possibly weighted graphs:  

   

g = (v, e).

w 2 r|v |   |v |: similarity matrix

non-euclidean geometric stability
    we replace the euclidean domain     by a general graph   
x(u) 2 l2(   ) ! x(u) 2 l2(g) , g = (v, e) .
    in some applications, the input is the graph itself:  

x $ g
    we focus on undirected, possibly weighted graphs:   

   

g = (v, e).

w 2 r|v |   |v |: similarity matrix

    suppose    rst that     admits a low-dimensional embedding, ie,  
g
wi,j = '(xi, xj) , xi 2         rd , i, j     |v |.

'(  ,  ): psd kernel (e.g. rbf, dot-product).

particle collisions measured in lhc calorimeter

non-euclidean extrinsic geometric stability
    a deformation    eld     in     induces a deformation on    :
g

   

   

w    = (w    )i,j , (w    )i,j = '(    (xi),     (xj)) .

g = (v, w )

non-euclidean extrinsic geometric stability
    a deformation    eld     in     induces a deformation on    : 
g

   

   

w    = (w    )i,j , (w    )i,j = '(    (xi),     (xj)) .

g    = (v, w    )

    similarly as before, many tasks satisfy geometric stability: 

    particle physics / chemistry.  
    3d surfaces.  

f (g)     f (g    ) if kr   k small.

    can we de   ne geometric deformation/stability intrinsically?

deformations and metrics 
    a deformation in an euclidean domain     induces a change of 

[with f. gama and a. ribeiro (u penn) ]

   

metric in    : 
   

hx    , x0   il2 =z x(u       (u))x0(u       (u))du =z x(v)x0(v)|1   r    (v) 1|dv

=z x(v)x0(v)dg(v) = hx, x0i   

    a small deformation cost corresponds to a small change of the 

metric.

(1   o(k   k))dv     dg(v)     (1 + o(k   k))dv

deformations and metrics 
    a deformation in an euclidean domain     induces a change of 

[with f. gama and a. ribeiro (u penn) ]

   

metric in    : 
   

hx    , x0   il2 =z x(u       (u))x0(u       (u))du =z x(v)x0(v)|1   r    (v) 1|dv

=z x(v)x0(v)dg(v) = hx, x0i   

    a small deformation cost corresponds to a small change of the 

metric. 

(1   o(k   k))dv     dg(v)     (1 + o(k   k))dv

    can we generalize this notion of distance between metric 
spaces? ie on metrics associated with an arbitrary graph?

gromov-hausdorff distance
    an undirected graph                           generates a metric given 

g = (v, e; w )

[with f. gama and a. ribeiro (u penn) ]

by shortest-paths:

dg(i, j) = shortest path between nodes i and j.

gromov-hausdorff distance
    an undirected graph                           generates a metric given 

g = (v, e; w )

[with f. gama and a. ribeiro (u penn) ]

by shortest-paths: 

dg(i, j) = shortest path between nodes i and j.

    one can measure similarity between metric spaces using e.g. 

gromov-hausdor    distance:  
dgh(m,q) =

1
2

inf

' : m 7! q
  : q 7! m

max{k'k,k k,k(',  )k} .

k(',  )k = sup

    introduced on surfaces/point-clouds in [memoli & sapiro   05], 

m2m,q2q|dm(m,  (q))   dq(q, '(m))| ,k'k = sup
[bronstein et al   06].  

m,m02m|dm(m, m0)   dq('(m), '(m0))| .

    corresponds to a permutation distance when 

dp(g, g0) =

1
2

min
   2   n

max
i,j

|v | = |v 0| :
|dg(i, j)   dg0(   (i),    (j))| .

intrinsic geometric stability priors

[with f. gama and a. ribeiro (u penn) ]

    many id136 problems on graphs are stable to intrinsic 

geometric deformations, in the sense that  
|f (g)   f (g0)| . d(g, g0)

    community detection.  
    planning, routing.  

    how to leverage geometric stability on graphs?

linear stable generators
    in euclidean domains    , we have seen that localized, 

   

multiscale    lters provide the key to geometric stability. 
    these can be expressed as linear operators     of            that nearly 

l2(   )

a

[with f. gama and a. ribeiro (u penn) ]

commute with deformations      : 

t   

t    x(u) = x(u       (u))

kat      t    ak     kr   k

 1

1

a2

1 1
a3

1

 1

a4

 1
1

a1

linear stable generators
    in euclidean domains    , we have seen that localized, 

   

multiscale    lters provide the key to geometric stability. 
    these can be expressed as linear operators     of            that nearly 

l2(   )

a

[with f. gama and a. ribeiro (u penn) ]

commute with deformations      :  

t   

kat      t    ak     kr   k

 1
1

a1

 1

1

a2

1 1
a3

 1

a4

t    x(u) = x(u       (u))

1

    we can write a id98 layer as a linear combination of such operators:  

  x =     xk

(akx)   k! .    1, . . . ,    k,2 rp     p .

    what about general graphs?

linear stable generators
[with f. gama and a. ribeiro (u penn) ]
    linear di   usion on graphs is given by its adjacency matrix            
a(g)
a(g)i,j = 1 i    (i, j) 2 e .

wi,j in weighted graphs.
    by de   nition, this is a localized operator. local smoothing.

linear stable generators
[with f. gama and a. ribeiro (u penn) ]
    linear di   usion on graphs is given by its adjacency matrix            
a(g)
a(g)i,j = 1 i    (i, j) 2 e .

wi,j in weighted graphs.
    by de   nition, this is a localized operator. local smoothing.  

    q: stable to deformations? by de   nition,  

inf

p2   n kw   p w 0p >k = dp(g, g0) . dgh(g, g0)
with respect to metric deformations. 

    up to rigid (isometric) transformation, local di   usion is continuous 

linear stable generators
[with f. gama and a. ribeiro (u penn) ]
    linear di   usion on graphs is given by its adjacency matrix            
a(g)
a(g)i,j = 1 i    (i, j) 2 e .

wi,j in weighted graphs.
    by de   nition, this is a localized operator. local smoothing.  

    q: stable to deformations? by de   nition,  

inf

p2   n kw   p w 0p >k = dp(g, g0) . dgh(g, g0)
with respect to metric deformations.   

    up to rigid (isometric) transformation, local di   usion is continuous 

    together with the degree matrix                           , it de   nes a 

d = diag(w 1)
high-pass    lter, the graph laplacian:                          
  = d   w .
    it is also localized and stable to deformations in the sense of gh.                   

graph neural networks
    given a signal                     , a graph neural network (gnn) layer 
x 2 rv    p
    = (   1,    2) :

considers generators            and trainable coe   cients  

[d, w ]

[scarselli et al.,   09], [gori et al.    05]

  x =     (dx   1 + w x   2) .

   1,    2 2 rp     p .

    flexible model: does not require    xed input graphs.  
    initial version was inspired from the message-passing algorithm. 

fixed point of a trainable, non-linear di   usion.  

    modernized in [li et al.   15], [duvenaud et al.   15], [subkhaatar et 

al.   16], 

    authors also explored other forms of nonlinearity, e.g. gating. 
    similarly as in id98s, we can also consider pooling layers, (provided 

we have a graph coarsening scheme).

laplacian interpretation
    since we are learning a linear combination of          and          , we can 

d(g)
reparametrize the generator in terms of the graph laplacian:  

a(g)

    if we consider generators of the form                       , the resulting 

 (g) = d(g)   a(g)
gnn layer is expressed as a polynomial in    :  
 

[1,  ,  2, . . . ]

sxs=0

  x =     (   ( )x) ,    ( ) =

   s s .

    in spectral networks [b. et al   14], we train directly on the spectrum 

of the laplacian: 

  x =     v t diag(   )v x  ,     = k(   ) , k : spline kernel

    computationally expensive and unstable to deformations for varying graph. 
    issues addressed in subsequent chebyshev model [de   errard et al   16], and 

id197 [kipf & welling   16].

extensions/limitations
    as opposed to euclidean domains, in general graphs we only 
have an isotropic high-pass    lter (    ), but no oriented    lters. 

 

extensions/limitations
    as opposed to euclidean domains, in general graphs we only have 

an isotropic high-pass    lter (    ), but no oriented    lters.  

 

    inspired by message-passing algorithms, we can generalize gnns to 

alternate between vertex and edge representations: 
x1 2 rv    p1

x2 2 rv    p2

y1 2 re   q1

y2 2 re   q2

y1(e) =     (x1(i), x1(j)) , e = (i, j) 2 e .
x2(i) =     ({y1(e)}i2e) , i 2 v .
    used for example in [battaglia et al.   16] for n-body prediction 

dynamics and [gilmer et al.   17] for quantum chemistry.

surface representations
    in the particular case where     represents a 3d surface, we 

[joint work with i. kostrikov, d.panozzo, d.zorin (nyu)]

g

have a mesh representation: 
m = (v, e, f ) , f = {(i, j, k)} triangulation

credit: jonathanpuckey

surface representations
    in the particular case where     represents a 3d surface, we 

g
have a mesh representation:  
m = (v, e, f ) , f = {(i, j, k)} triangulation

[joint work with i. kostrikov, d.panozzo, d.zorin (nyu)]

    in that case, we can compute a    proper    square root of the 

laplacian, the dirac operator: 
  = d   d , d 2 hv    f
    de   ned over quaternion space. 
    captures principal curvature directions (ie orientation). 

credit: jonathanpuckey

surface representations
    in the particular case where     represents a 3d surface, we have a 

g

mesh representation:  
m = (v, e, f ) , f = {(i, j, k)} triangulation

    in that case, we can compute a    proper    square root of the 

laplacian, the dirac operator: 
  = d   d , d 2 hv    f
    de   ned over quaternion space. 
    captures principal curvature directions (ie orientation).  
    used in [kostrikov, b., panozzo, zorin.   17] for surface 

representation tasks and generative model.

credit: jonathanpuckey

laplace network stability
    stable graph generators result in stable gnn representations: 

theorem:
let  (x;  ) be a r-layer laplace gnn with generators {i,  },
and     a deformation    eld on    . then

[b, k, p, z   17] let g = (v, e) and suppose v         2 rd.

1. k (x;  )    (x0;  )k     c(   )kx   x0kh( ) ,
2. k (x;  )    (x;     ( ))k     c0(   )kr   kh( ) ,
where h( ) =qr   r

of feature maps.

 r 1
 r 1/2 measures smoothness (sobolev)

    in euclidean graphs, the laplacian is geometrically stable. 
    caveat: we currently require explicit smoothness decay of feature 

maps. 

    future work: extension to intrinsic deformations. 

inverse problems on graphs
    consider the problem of inferring communities within a 

network: 

inverse problems on graphs
    consider the problem of inferring communities within a 

network: 

adjacency matrix associative 2 communities

inverse problems on graphs
    consider the problem of inferring communities within a 

network: 

adjacency matrix associative 2 communities

inverse problems on graphs
    consider the problem of inferring communities within a 

network: 

adjacency matrix associative 2 communities

inverse problems on graphs
    community detection in graphs.  

    studied in the stochastic block model. 
    hardness of estimation is controlled by a signal-to-noise ratio: 

snr =

(a   b)2

k(a + (k   1)b)

a: inner connection id203.
b: outer connection id203.

inverse problems on graphs
    community detection in graphs.  

    studied in the stochastic block model. 
    hardness of estimation is controlled by a signal-to-noise ratio:  

snr =

(a   b)2

    two major algorithmic frameworks: 

k(a + (k   1)b)

a: inner connection id203.
b: outer connection id203.

    graph conductance/min-cut approach, leading to spectral id91 

algorithms. 

    probabilistic id114, leading to belief propagation.

min

yi=  1;  y=0

yta(g)y .
'(i,j)(yi, yj)yv2v

p(y|g) / y(i,j)2e

 i(yi)

inverse problems on graphs
    community detection in graphs.  

    studied in the stochastic block model. 
    hardness of estimation is controlled by a signal-to-noise ratio. 
    recent research program has uni   ed both approaches using 
tools from statistical physics, and identi   ed computational 
and information theoretic thresholds:  
    when is the detection statistically possible? 
    when is the detection feasible with polynomial-time algorithms? 

inverse problems on graphs
    community detection in graphs.  

    studied in the stochastic block model. 
    hardness of estimation is controlled by a signal-to-noise ratio. 
    recent research program has uni   ed both approaches using 
tools from statistical physics, and identi   ed computational 
and information theoretic thresholds:  
    when is the detection statistically possible? 
    when is the detection feasible with polynomial-time algorithms?  

    q: can we learn those algorithms from the data using graph 

neural networks? reaching detection thresholds?

data-driven community detection
        
a(g): linear operator de   ned on g, eg laplacian   = d   a.
    spectral id91 estimators (2-community case): 

[ joint work with lisha li (uc berkeley) ]

  y = sign (fiedler(a(g))) ,

2

fiedler(m ): eigenvector corresponding to 2nd smallest eigenvalue

0.25

0.20

0.15

0.10

0.05

-4

-2

0

2

4

 c

    iterative algorithm: projected power iterations on 

shifted            :
a(g)

fig. 1: the spectrum of the adjacency matrix of a sparse network generated by the block model (excluding the zero eigenvalues).
here n = 4000, cin = 5, and cout = 1, and we average over 20 realizations. even though the eigenvalue  c = 3.5 given by (2)
satis   es the threshold condition (1) and lies outside the semicircle of radius 2pc = 3.46, deviations from the semicircle law cause
it to get lost in the bulk, and the eigenvector of the second largest eigenvalue is uncorrelated with the community structure.
as a result, spectral algorithms based on a are unable to identify the communities in this case.

m = ka(g)k1   a(g)

data-driven community detection
    we consider a gnn generated by operators                 : 
{1, a, d}

[ joint work with lisha li (uc berkeley) ]

  x =     (   1x +    2dx +    3ax) .

   they generate the so-called bethe hessian: 

bh(r) = (r2   1)1   ra + d

    second-order approximation of bethe free energy at 
critical points of bp. 
    in that case, laplacian generator does not work: its 
spectrum is dominated by few nodes with dominant degree. 

    we train it by back propagation using a loss that is globally 

invariant to label permutations.

reaching detection threshold on sbm
    stochastic block model results: 

[ joint work with lisha li (uc berkeley) ]

binary, associative

binary, disassociative

snap collection (youtube, dblp and amazon), and we restrict the largest community
size to 800 nodes, which is a conservative bound, since the average community size on these
graphs is below 30.

we compare gnn   s performance with the community-a liation graph model (agm).
the agm is a generative model de   ned in [?] that allows for overlapping communities
where overlapping area have higher density. this was a statistical property observed in
many real datasets with ground truth communities, but not present in generative models
before agm and was shown to outperform algorithms before that. agm    ts the data to
the model parameters in order give community predictions, and we use the recommended
default parameters. table ?? compares the performance, measured with a 3-class {1, 2, 1 +
2} classi   cation accuracy up to global permutation 1 $ 2. we stress however that the
spectral method. 
experimental setup is di   erent from the one in [?], which may impact the performance
of agm. nonetheless, this experiment illustrates the bene   ts of data-driven models that
strike the right balance between expressive power to adapt to model mis-speci   cations and
structural assumptions of the task at hand.

    real-world community detection results on snap data 

    we reach the detection threshold, matching the speci   cally designed 

[leskovec et al]

table 1: snap dataset performance comparison between gnn and agm

subgraph instances

dataset
amazon
dblp
youtube

(train/test)

315 / 35
2831 / 510
48402 / 7794

avg vertices avg edges

60
26
61

346
164
274

overlap comparison

gnn

agmfit
0.74    0.13 0.76    0.08
0.78    0.03
0.64    0.01
0.9    0.02
0.57    0.01

belief propagation
    for small number of communities, the it detection threshold 

is provably matched by (loopy) bp. 

    bp performs message-passing updates of the form 
mij(xj)   xxi

0@    i(xi; y) ij(xi, xj) yk2n (i)\j
   j(xj; y) yi2n (j)

mki(xi)1a .

mij(xj) .

bj(xj) =

1
zj

    exact id136 on simply connected graphs. 
    on general graphs,    xed points of bp correspond to critical points of 

the bethe free energy.   

    messages are de   ned and propagated over edges of     , and 

g

account for non-backtracking paths. 

belief propagation
    for small number of communities, the it detection threshold 

is provably matched by (loopy) bp. 

    bp performs message-passing updates of the form 
mij(xj)   xxi

0@    i(xi; y) ij(xi, xj) yk2n (i)\j
   j(xj; y) yi2n (j)

mki(xi)1a .

mij(xj) .

bj(xj) =

1
zj

    exact id136 on simply connected graphs. 
    on general graphs,    xed points of bp correspond to critical points of 

the bethe free energy.   

    messages are de   ned and propagated over edges of     , and 

account for non-backtracking paths. gnn version?

g

graph neural networks on graph hierarchies
    the line graph of                     is a new graph                             
l(g) = (v 0, e0)

g = (v, e)

that models the adjacency of the edges:    

v 0    = e

(source:wikipedia)

    we augment the gnn with analogous operations on          . 
l(g)
    related to covariant compositional networks [kondor et 

al   18], and neural message-passing [gilmer et al.   17].

computational-to-statistical gaps
    when # of communities > 4, there is a gap between the 

information theoretical threshold and current known 
polynomial-time algorithms: 

[decelle, krzakala, moore, zdeborova,   13]

    preliminary results for 5 communities,                   :
deg = 14.5

n = 103
overlap

g

29.5    0.5

{g, l(g)}
30.1    0.5

bp

30.4    3

[with s. villar (nyu), a. nowak (nyu) and a. bandeira (nyu)]

quadratic assignment problem
    find an assignment that optimizes the transportation cost 

between two graphs: 

min
x2   n

tr(a1xa2x t ) .

   n: space of n     n permutation matrices.

    np-hard 
    contains the tsp as a particular instance.  
    relaxations using sdp and spectral approaches. 

[with s. villar (nyu), a. nowak (nyu) and a. bandeira (nyu)]

quadratic assignment problem
    we learn approximate solutions using siamese graph neural 

networks:  

g1

g2

gnn
 

gnn
 

sinkhorn-knopp

m

    we train the model to predict the correct permutation matrix 

m = softmax[ (g1) (g2)t )]

on a dataset of planted solutions:  

g1 = p g2 + n

n     erdos-renyi

g2     erdos-renyi
g2     random regular

[with s. villar (nyu), a. nowak (nyu) and a. bandeira (nyu)]

quadratic assignment problem

o(n2)

    our model runs in          , lowrankalign is           sdp in     
    current: what is the model learning? link to friendly graphs. 
    current: applications to shape correspondence, unaligned 

o(n3)

o(n4)

language translation 

givens factorization of unitary matrices
    suppose we have a unitary matrix    (e.g. an eigenbasis) that 

[with d. folque (nyu)]

u

we want to use extensively.  

    complexity of matrix-vector multiplication:  
    but structure on     can yield massive gains: 

u

   (n2) [winograd]

fft    (n log n) [tukey]

givens factorization of unitary matrices
    suppose we have a unitary matrix    (e.g. an eigenbasis) that 

[with d. folque (nyu)]

u

we want to use extensively.  

    complexity of matrix-vector multiplication:  
    but structure on     can yield massive gains:  
    general case?

u

   (n2) [winograd]

fft    (n log n) [tukey]

givens factorization of unitary matrices
    we consider sparse matrix transforms given by givens plane 

[with d. folque (nyu)]

rotations:  

u =

o(ik, jk,    k)

kyk=1

k =

n(n   1)

o(ik, jk,    k): rotation of    k in the plane {ik, jk}.

                           su   cient for any     ,   
    np-complete, non-commutative manifold-optimization problem.  

u fft : k = n log n

2

    use gnn model to learn such factors. 

givens factorization of unitary matrices
    we consider a simple inverse-problem setup with planted 

[with d. folque (nyu)]

solution: 

(u (l) :=

kyk=1

o(i(l)

k , j(l)

k ,    (l)

k ))l   l

input

labels

    model: a gnn on the fully-connected graph, where we learn 

both edge and node features. 
    uses multiset loss to account for partial permutation invariance.  

    preliminary results: matching greedy algorithm. 

open problems
    theory 

    quantify how smoothness is created in gnn layers. 
    intrinsic geometric stability. alternatives to gromov-hausor   ? 
    learnability thresholds in statistical id136. 

    useful to study computational-to-statistical gaps?  

    vast areas of application 

    algorithms: learning approximations to combinatorial optimization. 
    biostatistics 
    social networks: ranking, large-scale community detection. 
    physics: numerical methods for more complex pdes?

thanks!

