deep learning in the news

researcher dreams up machines 	
that learn without humans

google	taps	u	of	t	professor	to	
teach	context	to	computers	
03.11.13

scientists see promise in 
deep-learning programs	
john markoff								november 23, 2012

aaai  what   s hot! track       iclr: what   s hot in deep learning       aaron courville

1

review of fundamentals
notes taken from hugo larochelle (ift 725 - r  seaux neuronaux), 
yoshua bengio and olivier delalleau	

to learn more...

topics: online videos on neural networks	
!

!

!

!

!

!
!

http://info.usherbrooke.ca/hlarochelle/neural_networks

    used in his graduate course in sherbrooke (   ipped class)	
    covers many other topics: convolutional networks, neural language model, 

restricted id82s, autoencoders, sparse coding, etc.

3

!

x1
...
x1
x1
...
x1
x1
...
...
xd
...
xd
xd
xd
xd

id202

abstract
abstract
abstract
abstract
abstract

math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
id202
math for my slides    review of fundamentals   .
math for my slides    review of fundamentals   .
id202
x1
id202
id202
id202
...
xd

topics: matrix, vector, norms, products	
!
    vector:	

i x(2)
i=1 x(1)
i=1 x(1)
i
i x(2)
i=1 x(1)
i x(2)
i=1 x(1)
    product:	
i x(2)
i=1 x(1)
i
i x(2)
i=1 x(1)
i
i
    norm:                                                (euclidean) 	
i
!
    matrix:	

id202
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    x = [x1, . . . , xd]> =264
375
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    < x(1), x(2) >= x(1)>x(2) =pd
    ||x||2 = px>x =ppi x2
    < x(1), x(2) >= x(1)>x(2) =pd
    ||x||2 = px>x =ppi x2
    ||x||2 = px>x =ppi x2
    ||x||2 = px>x =ppi x2
i
    ||x|| = p< x>x > =ppi x2
    ||x|| = p< x>x > =ppi x2
    x =264
375
i
i
    x =264
375
. . . x1,m
    x =264
375
i
    x =264
375
    x =264
375
...
...
. . . x1,m
    x =264
375
. . . x1,w
. . . x1,m
. . . x1,w
. . . x1,w
...
...
...
...
...
...
...
...
...
...
. . . xn,m
. . . xn,m
. . . xh,w
. . . xh,w
. . . xn,m
  ,j =pk x(1)
. . . xh,w
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
  ,j =pk x (1)
  ,j =pk x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
  ,j =pk x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    ||x||f =ptrace(x>x) =qpipj x 2
    (x(1)x(2))i,j = x(1)
i,   x(2)
    ii,j =    1 si i = j
    ||x||f =ptrace(x>x) =qpipj x 2
    ||x||f =ptrace(x>x) =qpi x2
    ii,j =    1 si i = j
    ||x||f =ptrace(x>x) =qpi x2
0 si i 6= j
ii,j =    1 if i = j

    product:	
i,k x(2)
    norm:                                                                          (frobenius)
k,j

x1,1
...
x1,1
x1,1
x1,1
x1,1
x1,1
...
...
...
...
...
xn,1
xn,1
xh,1
xh,1
xn,1
xh,1

i,k x(2)
k,j
i,k x(2)
i,k x (2)
i,k x(2)
k,j
i,k x(2)
k,j
k,j
k,j
i,j
i,j

i x(2)

i,i

i

!

i

i

4

i,i

k,j

i,i
i,i
i,i

i,k x(2)
i,k x(2)
i,k x(2)
i,k x(2)
i,k x(2)

    x =264
375
    x =264
375
    ||x||2 = px>x =ppi x2
    x =264
375
  ,j =pk x(1)
...
...
...
  ,j =pk x(1)
    x =264
375
  ,j =pk x(1)
  ,j =pk x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
  ,j =pk x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
. . . x1,m
x1,1
  ,j =pk x(1)
. . . xn,m
xn,1
    (x(1)x(2))i,j = x(1)
i,   x(2)
...
...
...
xn,1
. . . xn,m
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
id202
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
    ||x||f =ptrace(x>x) =qpi x 2
  ,j =pk x(1)
  ,j =pk x(1)
    ||x||f =ptrace(x>x) =qpi x 2
i,   x(2)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    (x(1)x(2))i,j = x(1)
. . . xn,m
xn,1
topics: special matrices	
ii,j =    1 if i = j
ii,j =    1 if i = j
    ||x||f =ptrace(x>x) =qpi x 2
ii,j =    1 if i = j
    ||x||f =ptrace(x>x) =qpi x2
ii,j =    1 if i = j
ii,j =    1 if i = j
  ,j =pk x(1)
    (x(1)x(2))i,j = x(1)
i,   x(2)
    identity matrix   : 	
ii,j =    1 if i = j
    i
    i
    i
    i
0 if i 6= j
    i
0 if i 6= j
0 if i 6= j
0 if i 6= j
0 if i 6= j
    ii,j =    1 if i = j
    i
!
ii,j =    1 if i = j
    ||x||f =ptrace(x>x) =qpi x 2
0 if i 6= j
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i 6= j
    diagonal matrix     : 	
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i 6= j
0 if i 6= j
    i
    x xi,j = 0 if i 6= j
0 if i 6= j
    x xi,j = 0 if i 6= j
    x xi,j = 0 if i < j
!
ii,j =    1 if i = j
    x xi,j = 0 if i < j
    x xi,j = 0 if i < j
    xi,j = 0 if i 6= j
    x xi,j = 0 if i < j
    lower triangular matrix     :  	
    x xi,j = 0 if i < j
    x xi,j = 0 if i 6= j
    i
    x xi,j = 0 if i < j
    x xi,j = xj,i (x> = x)
0 if i 6= j
    xi,j = 0 if i < j
    x xi,j = xj,i (x> = x)
    x xi,j = xj,i (x> = x)
    x xi,j = xj,i (x> = x)
!
    trace(x) =pi xi,i
    x xi,j = 0 if i < j
    x xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    trace(x) =pi xi,i
    symmetric matrix     :                            (i.e.                )	
    x xi,j = xj,i (x> = x)
    x xi,j = 0 if i 6= j
    trace(x) =pi xi,i
    xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    x xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    trace(x) =pi xi,i
!
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    x xi,j = 0 if i < j
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    trace(x) =pi xi,i
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2))trace(x(2)x(3)x(1))
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))
    x xi,j = xj,i (x> = x)
    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

i,k x(2)

i,i

i,i

5

    ||x||f =ptrace(x>x) =qpi x2
    x xi,j = 0 if i 6= j
    ii,j =    1 if i = j
0 if i 6= j
id202
    x xi,j = 0 if i < j
    xi,j = 0 if i 6= j
    x xi,j = xj,i (x> = x)
topics: operations on matrices	
    xi,j = 0 if i < j
    trace(x) =pi xi,i
    trace of matrix:	
    xi,j = xj,i (x> = x)
    trace(x) =pi xi,i
    trace of products: 	
!

    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

    trace(x(1)x(2)x(3)) = trace(x(3)x(1)x(2)) = trace(x(2)x(3)x(1))

!
    inverse of matrix:	

    doesn   t exist if determinant is 0	
    inverse of product:    

    transpose of matrix:	

    transpose of product:

1

x(1) 1

x(1) 1
x(1) 1

    x 1x = x x 1 = i
    x 1x = x x 1 = i
    x 1x = x x 1 = i
    (x(1)x(2)) 1 = x(2) 1
    (x(1)x(2)) 1 = x(2) 1
    (x(1)x(2)) 1 = x(2) 1
    x 1x = i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i

1

6

topics: operations on matrices	
    determinant	

    of triangular matrix:	

!

!

!

    of transpose of matrix:	

    of inverse of matrix:	

    of product of matrix: 

    x 1x = x x 1 = i
    x 1x = x x 1 = i
    x 1x = x x 1 = i
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
id202
    x 1x = x x 1 = i
x(1) 1
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
    (x(1)x(2)) 1 = x(2) 1
x(1) 1
    (x(1)x(2)) 1 = x(2) 1
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x>)i,j = xj,i
    (x(1)x(2))> = x(2)>x(1)>
    (x>)i,j = xj,i
    (x(1)x(2))> = x(2)>x(1)>
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    (x(1)x(2))> = x(2)>x(1)>
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x) =qi xi,i
    det (x>) = det (x)
    det (x>) = det (x)
    det (x>) = det (x)
    det (x 1) = det (x) 1
    det (x>) = det (x)
    det (x 1) = det (x) 1
    det (x 1) = det (x) 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x 1) = det (x) 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
    x> = x 1
    v>xv > 0
    x> = x 1
    v>xv > 0
    v>xv > 0
     
    v>xv > 0
     
     
    {x(t)}

7

    det (x) =qi xi,i
    det (x) =qi xi,i
    (x>)i,j = xj,i
    det (x>) = det (x)
    det (x>) = det (x)
    (x(1)x(2))> = x(2)>x(1)>
id202
    det (x) =qi xi,i
    det (x 1) = det (x) 1
    det (x 1) = det (x) 1
    det (x>) = det (x)
    det (x(1)x(2)) = det (x(1)) det (x(2))
topics: properties of matrices	
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x 1) = det (x) 1
    orthogonal matrix:	
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    x> = x 1
!
    x> = x 1
    v>xv > 0
    positive de   nite matrix: 	
    v>xv > 0 8v 2 rd
    v>xv > 0 8v 2 r
     
    if          , then positive semi-de   nite
     
     
    {x(t)}
    {x(t)}
    9w, t    x(t   ) =pt6=t    wtx(t)
    9w, t    x(t   ) =pt6=t    wtx(t)
    {x(t)}
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    x =pi  iuiu>i
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    det (x) =qi  i

8

    det (x 1) = det (x) 1
    x> = x 1
    x> = x 1
    det (x(1)x(2)) = det (x(1)) det (x(2))
    det (x(1)x(2)) = det (x(1)) det (x(2))
id202
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
    x> = x 1
    x> = x 1
     
topics: linear dependence, rank, range and nullspace	
     
    v>xv > 0 8v 2 r
    v>xv > 0 8v 2 r
    set of linearly dependent vectors          :	
    {x(t)}
    {x(t)}
     
     
!
    {x(t)}
    rank of matrix: number of linear independent columns	
    {x(t)}
    range of a matrix:	
!
    nullspace of a matrix:

    9w, t    such that x(t   ) =pt6=t    wtx(t)
    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    {x 2 rn | x /2 r(x)}
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    x =pi  iuiu>i
    x =pi  iuiu>i
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui et u>i uj = 1i=j}
    det (x) =qi  i
    det (x) =qi  i
    x =pi  iuiu>i
    { i, ui | xui =  iui et u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i

     i > 0

     i > 0

9

    {x(t)}

    x> = x 1
    x> = x 1
     
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    v>xv > 0 8v 2 r
id202
    v>xv > 0 8v 2 r
    {x(t)}
     
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
     
    9w, t    such that x(t   ) =pt6=t    wtx(t)
topics: eigenvalues and eigenvectors of a matrix	
    {x(t)}
    {x(t)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    eigenvalues and eigenvectors	
    9w, t    such that x(t   ) =pt6=t    wtx(t)
    {x 2 rn | x /2 r(x)}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
!
    { i, ui | xui =  iui and u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
!
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    properties	
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    x =pi  iuiu>i
    can write	
    det (x) =qi  i
    det (x) =qi  i
    det (x) =qi  i
    determinant of any matrix: 	
    det (x) =qi  i
    positive de   nite if	
     i > 0 8i
     i > 0
     i > 0
    rank of matrix is the number of non-zero eigenvalues
   
   
@
   
f(x +  , y)   f(x, y)
f(x, y) = lim
f(x, y) = lim
f(x, y) = lim
@x
 !0
@
 !0
 
 !0
f (x, y) = lim
@
@x
 !0

     i > 0
   

@
@x

@
@x

   

   

   

f(x +  , y)   f(x, y)
f(x +  , y)   f(x, y)
f (x +  , y)   f (x, y)
f(x, y +  )   f(x, y)

 
 
10
 

id128

    r(x) = {x 2 rn | 9w such that x =pj wjx  ,j}
    {x 2 rn | x /2 r(x)}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    {x 2 rn | x /2 r(x)}
    x =pi  iuiu>i
    { i, ui | xui =  iui and u>i uj = 1i=j}
    { i, ui | xui =  iui and u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i
    x =pi  iuiu>i
    det (x) =qi  i
    det (x) =qi  i

topics: derivative, partial derivative	
    derivative:	

     i > 0 8i
     i > 0 8i
     i > 0 8i
!
!

    direction and rate of increase of function	

d
dx

f (x +  )   f (x)

f (x) = lim
 !0
d
d
dx
dx

f(x +  )   f(x)
f(x +  )   f(x)
f(x) = lim
f(x) = lim
 !0
f (x +  , y)   f (x, y)
 !0

 

 
 
 

    partial derivative:	

@
@x

@
@y

f(x, y) = lim
f(x, y) = lim
 !0
 !0

f (x, y) = lim
 !0
@
@
@x
@x
f (x, y) = lim
 !0
@
@
@y
@y

f(x, y) = lim
f(x, y) = lim
 !0
 !0

!
!
!
!

f (x), . . . , @
@xd

f(x +  , y)   f(x, y)
f(x +  , y)   f(x, y)

f (x, y +  )   f (x, y)

 
 

f(x, y +  )   f(x, y)
f(x, y +  )   f(x, y)

 

 
 
f (x)
...

@

f(x)

375

@x1

f (x)i> =264
f(x)i> =264
f(x)i> =264

11

375
375

    direction and rate of increase for variable assuming others are    xed

@

    x y
    x y

    rf (x) =h @
    rf(x) =h @

@x1

i

i

i

)

)

(

|

 

 

u

u

i
,

i
,

   

   

   

   

   

   

   

   

   

   

   

{
 

x
=

d
e
t
(

r
x

    9w, t    x(t   ) =pt6=t    wtx(t)
    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
id128
{
{
{
9
x
    {x 2 rh | x /2 r(x)}
x
 
x
w
topics: derivative, partial derivative	
d  riv  e partielle et gradient 
=
>
2
2
    { i, ui | xui =  iui et u>i uj = 1i=j}
,
t
x
    example:
    x =pi  iuiu>i
p
p
   
0
r
r
       exemple%de%fonc7on%  %deux%variables:%
=
h
h
    det (x) =qi  i
x
=
f(x, y) = x2
 
(
{
     i > 0
t
x
x
x
   
     i > 0
q
y
)
   
2
/2
/2
%       d  riv  es%par7elles:%%%
=
   
=
@
 
r
r
r
f (x, y) = lim
p
@x
2x
 f(x, y)
 !0
=  x2
 
h
(
   
=
   
i
x
x
|
u
@
y2
 y
y
=
f (x, y) = lim
9
)
)
i
@y
t
 !0
}
}
w
   
e
x
y
traite%
traite%
treat	
treat	
    x y
    x y
t
w
comme%une%
comme%une%
as constant
as constant
constante%
constante%
machine learning
x

=
 f(x, y)
 
u
 x
e
t

u
>i

u
>i

x
u

x
u

u
>i

u
>i

u

u

|

|

|

(

t

t

i

i

i

i

i

i

i

i

i

i

i

i

hugo%larochelle%

ift615%

39%

)

(

   

   

   

    r(x) = {x 2 rh | 9w x =pj wjx  ,j}
    {x 2 rh | x /2 r(x)}
 
    { i, ui | xui =  iui et u>i uj = 1i=j}
    x =pi  iuiu>i
    det (x) =qi  i

r
{
x
(
x
t
)
}

=
{
x
2
r
p
 
h

f (x +  , y)   f (x, y)

|

f (x, y +  )   f (x, y)

9
 
w

x
=

x
12
=

6
6
@x

 !0

 

f (x, y +  )   f (x, y)

@
@y

f (x, y) = lim
 !0

id128
   
topics: gradient	
    x y
    gradient:
375

f (x)i> =264

@xd
       le%gradient%donne%la%direc7on%(vecteur)%ayant%le%taux%d   accroissement%de%

    rxf (x) =h @

f (x), . . . , @
descente de gradient 
@xd

f (x)
...
f (x)

@x1

@x1

 

@

@

la%fonc7on%le%plus%  lev  %

2

f(x, y)

x

y

13

3775
3775

!

@

@

...

...

@
@

@
@

@x1,1

@xn,1

f (x)

@2
@x2
1

f (x)

f (x)

@
@x1,m

f (x)

f (x)

f (x)

@
@xn,m

@
f(x)
@x1,m

    r2
    r2

...
@
f(x)
@xn,m

. . .
@
. . .
@x1,m
. . .
. . .
. . .
@
. . .
@xn,m

f (x)
@x1,1
f (x)
. . .
f(x)
...
@x1,1
...
...
. . .
f (x)
f (x)
f(x)
. . .
@xn,1
@xn,1

id128

topics: jacobian, hessian	
    hessian: 	
@2
@2
f (x)
f (x)
f(x)
. . .
@x2
@x2
1
1
!
...
...
...
@2
@2
f(x)
@xd@x1
@xd@x1

    rxf (x) =2664
    rxf (x) =2664
    rf(x) =2664
xf (x) =2664
xf (x) =2664
    r2f(x) =2664
    rxf (x) =264
    rf(x) =264
    rxf (x) =264

@
f(x)1
@
@x1
...
@x1
@
f(x)k
@x1
@
@x1
    (   ,f, p ),     f p ,
    (   ,f, p ),     f p ,
    (   ,f, p ),     f p ,

. . .
f (x)
. . .
f (x)
!
    if                                          is a vector, the jacobian is:
    f (x) = [f (x)1, . . . , f (x)k]>
    f(x) = [f(x)1, . . . , f(x)k]>
    f (x) = [f (x)1, . . . , f (x)k]>
. . .
f (x)1
f (x)1
@
. . .
f(x)1
@
@xd
f (x)1
. . .
@xd
...
...
...
...
. . .
. . .
. . .
. . .
f (x)k
f (x)k
@
f(x)k
. . .
@xd
@
@xd
. . .
f (x)k

3775
3775
375

@2
@2
f(x)
@x1@xd
@x1@xd
...
...
@2
@2
f (x)
f(x)
@x2
@x2
d
d

3775
3775
375

@2
. . .
. . .
@x1@xd
. . .
. . .

f (x)1
...
f (x)k

...
f (x)

. . .
@2
. . .
@x2
d

375

@xd@x1

@xd

@xd

@x1

@x1

@2

@

@

@

@

14

!

!

@

@

@

@

@

@x1

@x1

@x1

@x1,1

. . .
. . .

    f (x) = [f(x)1, . . . , f (x)k]>
. . .

f (x)1
@
f(x)1
...
id128
@xd
...
f (x)k
@
f(x)k
@xd

f(x)1
...
f(x)k
topics: gradient for matrices	
@x1
    if scalar function          takes a matrix      as input	
    f (x) x
    f(x) x
!
f (x)
f (x)
@
. . .
...
@x1,1
. . .
f (x)
f (x)
. . .
@
@xn,1

@
f (x)
...
...
@
f (x)
@xn,m
!
f (x)
    for functions that output vectors and take matrices as input, 
...
we organize into 3d tensors

    rxf (x) =264
    rxf (x) =2664

    rxf (x) =264
    rxf (x) =2664
    rxf(x) =2664
xf (x) =2664
    rxf (x) =264

. . .
    (   ,f, p ),     f p ,
    (   ,f, p ),     f p ,
. . .
       
       
. . .
f (x)
    e 2 f
    f (x) = [f (x)1, . . . , f (x)k]>
    e 2 f
        = {1, 2, 3, 4, 5, 6}

@
f(x)
@x1,m
...
@
f(x)
@xn,m

3775
3775

. . .
@
@x1,1
. . .
. . .
@
@xn,1

...
f (x)

    r2

375

@2
@x2
d

@2
@x2
1

f (x)

@xd@x1

@x1@xd

@x1,m

@xn,1

@2

@2

...

@

. . .

375

. . .
. . .

15

f(x)

. . .

f(x)

. . .
. . .

. . .

. . .
. . .

@

@

@

@

@

...

@2

...

@x1

@xd@x1
@2

@
@xd@x1
@x1

3775

@
f (x)1
f (x)1
. . .
@x1
f (x)
@xd
...
...
@xn,m
. . .
@
f (x)k
f (x)k
. . .
@xd

    r2f(x) =2664
    r2f (x) =2664
    rxf (x) =2664
    r2f(x) =2664
    rf (x) =264
    rf(x) =264
    rf(x) =264

    r2f (x) =2664
    r2f (x) =2664
    rf(x) =264
    rf (x) =264
    rf (x) =264

3775
3775
3775
3775
3775
    rf (x) =264
    rf (x) =264
375
375
375
375
375
375

...
...
    f(x) = [f(x)1, . . . , f(x)k]>
. . .
. . .
@x2
@2
f(x)
. . .
f(x)
d
@
@2
f (x)1
@x2
@xd@x1
f (x)
. . .
f (x)
@
    f (x) = [f (x)1, . . . , f (x)k]>
    f(x) = [f(x)1, . . . , f(x)k]>
f (x)
. . .
d
@x1
@
id203
@x2
f(x)1
f(x)1
. . .
    f (x) = [f (x)1, . . . , f (x)k]>
...
d
@xn,1
@xd
    f(x) = [f(x)1, . . . , f(x)k]>
...
...
@
@
f (x)1
. . .
f (x)1
    f (x) = [f (x)1, . . . , f (x)k]>
@
@
f(x)1
. . .
f(x)1
@
@x1
@xd
    (   ,f, p ),     f p ,
f (x)1
. . .
f (x)1
. . .
@x1
@xd
@
...
...
f (x)k
...
...
topics: id203 space	
@
@
@xd
@x1
f(x)1
. . .
f(x)1
@x1
...
...
. . .
@xd
@x1
@
@
@
. . .
. . .
f (x)1
f (x)1
f(x)k
. . .
f(x)k
...
...
. . .
    (   ,f, p ),     f p ,
@xd
@x1
    id203 space: triplet	
@x1
@xd
@
@
    (   ,f, p ),     f p ,
. . .
f (x)k
f (x)k
...
...
. . .
@
@
f(x)k
. . .
f(x)k
       
@xd
@x1
@
@
f (x)k
f (x)k
. . .
@xd
@x1
. . .
@
@
f(x)k
f(x)k
. . .
    (   ,f, p ),     f p ,
          is the space of possible outcomes	
@xd
@x1
    (   ,f, p ),     f p ,
@x1
    e 2 f
@
@
    (   ,f, p ),     f p ,
    e 2 f
. . .
f (x)k
f (x)k
          is the space of possible events	
    e 2 f
    (   ,f, p ),     f p ,
@x1
    (   ,f, p ),     f p ,
    e 2 f
    e 2 f
          is a id203 measure mapping an outcome to its id203 [0,1]	
        = {1, 2, 3, 4, 5, 6}
    e 2 f
        = {1, 2, 3, 4, 5, 6}
    e 2 f
        = {1, 2, 3, 4, 5, 6}
    example: throwing a die	
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
    e = {1, 5} 2 f
 	
        = {1, 2, 3, 4, 5, 6}
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
    e = {1, 5} 2 f
    e = {1, 5} 2 f
                                     (i.e. die is either 1 or 5)	
        = {1, 2, 3, 4, 5, 6}
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    e = {1, 5} 2 f
    e = {1, 5} 2 f
    e = {1, 5} 2 f
    p ({1, 5}) = 2
 	
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    e = {1, 5} 2 f
    properties:
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
1.
2.
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p ({!})   0 8! 2    
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    x o s

    (   , f , p ),     f p ,

    x o s

6
6
6

@xd

@xd

6

6

16

6

6

6

6

-

-

-

6

6

id203

    e 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
        = {1, 2, 3, 4, 5, 6}
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    e = {1, 5} 2 f
topics: random variable	
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    random variable: a function on outcomes	
    p!2    p ({!}) = 1
6
    p ({!})  = 0 8! 2    
    examples: 	
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
          is the value of the outcome 	
    x o s
          is 1 if the outcome is 1, 3 or 5, otherwise it   s 0	
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
          is 1 if the outcome is smaller than 4, otherwise it   s 0
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(s = s|o = o)
    p(s = s|o = o)

6

6

6

17

6

6

6

    marginal distribution:	

    p ({1, 5}) = 2
    e = {1, 5} 2 f
    p ({1, 5}) = 2
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p ({!})  = 0 8! 2    
    p ({1, 5}) = 2
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
id203
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p!2    p ({!}) = 1
    p ({!})  = 0 8! 2    
    p!2    p ({!}) = 1
    x o s
topics: distributions (joint, marginal, conditional)	
    x o s
    x o s
    x o s
    x o s
    x o s
    joint distribution:                                          (              for short)	
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    x o s
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = x, o = o, s = s) p(x, s, o)
    the id203 of a complete assignment of many random variables	
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(x = x, o = o, s = s) p(x, s, o)
    p(x = 1, o = 1, s = 0) = 0
    example: 	
    p(x = 1, o = 1, s = 0) = 0
    p(x = 1, o = 1, s = 0) = 0
    p(o, s) =px p(x, o, s)
    p(o = o, s = s)
    p(o = o, s = s)
    p(x = 1, o = 1, s = 0) = 0
    p(o = o, s = s)
    p(o = o, s = s)
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    the id203 of a partial assignment	
    p(o = 1, s = 0) = 1
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    example:	
    p(o = 1, s = 0) = 1
    p(s = s|o = o)
6
    p(s = s|o = o)
    p(o = 1, s = 0) = 1
    conditional distribution:	
6
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s = 1|o = 1) = 2
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(x) =qi p(xi|x1, . . . , xi 1)
po0 p(s=s|o=o0)p(o=o0)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)

    the id203 of some variables, assuming an assignment of other variables	
    example:

3
3

6

3

6

3

3

6

18

3

6

6

3

3

6

3

6

    p(o = 1, s = 0) = 1
    p(o = o, s = s)
    p(o = 1, s = 0) = 1
6
id203
    p(o = 1, s = 0) = 1
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = 1|o = 1) = 2
topics: id203 chain rule, bayes rule	
    p(s = 1|o = 1) = 2
    p(s = s|o = o)
    id203 chain rule:	
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s = 1|o = 1) = 2
    p(x) =qi p(xi|x1, . . . , xi 1)
    in general: 	
    p(x) =qi p(xi|x1, . . . , xi 1)
!
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
!
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    bayes rule:
po0 p(s=s|o=o0)p(o=o0)
    p(x1, x2) = p(x1)p(x2)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(x1, x2) = p(x1)p(x2)
po0 p(s=s|o=o0)p(o=o0)
    p(x1, x2) = p(x1)p(x2)

3

3

3

19

6

6

6

id203

    p(o, s) =px p(x, o, s)
    p(o, s) =px p(x, o, s)
    p(o, s) =px p(x, o, s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(o = 1, s = 0) = 1
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(s = s|o = o)
    p(s = s|o = o)
    p(s = s|o = o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
topics: independence between variables	
po0 p(s=s|o=o0)p(o=o0)
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    p(s = 1|o = 1) = 2
    independence: variables       and       are independent if	
    x1 x2 x3
    x1 x2 x3
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(s, o) = p(s|o)p(o) = p(o|s)p(s)
    p(x) =qi p(xi|x1, . . . , xi 1)
!
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(x) =qi p(xi|x1, . . . , xi 1)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
po0 p(s=s|o=o0)p(o=o0)
!
    p(o = o|s = s) = p(s=s|o=o)p(o=o)
po0 p(s=s|o=o0)p(o=o0)
    conditional independence: variables       and        are 
    x1 x2 x3
    x1 x2 x3
independent given       if 
    x1 x2 x3

3

3

3

3

3

!

    p(x1, x2) = p(x1)p(x2)
or
    p(x1|x2) = p(x1)
or
    p(x2|x1) = p(x2)
    p(x1, x2) = p(x1)p(x2)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2) = p(x1)
    p(x1|x2, x3) = p(x1|x3)
    p(x2|x1) = p(x2)
    p(x2|x1, x3) = p(x2|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x + y ] = e[x] + e[y ]
    p(x2|x1, x3) = p(x2|x3)
    e[xy ] = e[x]e[y ]

    e[x] =px x p(x = x)
    e[x] =px x p(x = x)

3

or
or

20

 	
 	
if independent, 	

    p(x2|x1) = p(x2)
    p(x1, x2) = p(x1)p(x2)
    p(x1|x2) = p(x1)
    p(x2|x1) = p(x2)
    p(x1|x2) = p(x1)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2) = p(x1)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2) = p(x1)
    p(x2|x1) = p(x2)
    p(x2|x1) = p(x2)
id203
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x2|x1) = p(x2)
    p(x1|x2, x3) = p(x1|x3)
    p(x1|x2, x3) = p(x1|x3)
    p(x2|x1) = p(x2)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x2|x1, x3) = p(x2|x3)
topics: expectation, variance	
    p(x1, x2|x3) = p(x1|x3)p(x2|x3)
    p(x2|x1, x3) = p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    p(x1|x2, x3) = p(x1|x3)
    e[x] =px x p(x = x)
    expectation:	
    p(x1|x2, x3) = p(x1|x3)
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    e[x + y ] = e[x] + e[y ]
    properties:	
    e[x] =px x p(x = x)
    p(x2|x1, x3) = p(x2|x3)
    e[x] =px x p(x = x)
    e[x + y ] = e[x] + e[y ]
    e[x] =px x p(x = x)
    e[xy ] = e[x]e[y ]
    e[x + y ] = e[x] + e[y ]
-
    e[x + y ] = e[x] + e[y ]
    e[x + y ] = e[x] + e[y ]
    e[f(x)] =px f(x) p(x = x)
    e[x + y ] = e[x] + e[y ]
    e[xy ] = e[x]e[y ]
-
    e[xy ] = e[x]e[y ]
    e[x + y ] = e[x] + e[y ]
    e[xy ] = e[x]e[y ]
    e[f (x)] =px f (x) p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
-
    e[xy ] = e[x]e[y ]
    e[xy ] = e[x]e[y ]
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    e[xy ] = e[x]e[y ]
    e[f(x)] =px f(x) p(x = x)
    e[f(x)] =px f(x) p(x = x)
!
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    e[f (x)] =px f (x) p(x = x)
    var[x] = e[x 2]   e[x]2
    var[x] =px(x   e(x))2 p(x = x)
    variance:	
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x + y ] = var[x] + var[y ]
    var[x] = e[x 2]   e[x]2
    properties:	
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
   
    var[x + y ] = var[x] + var[y ]
 	
    var[x] = e[x 2]   e[x]2
-
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
if independent,
    var[x + y ] = var[x] + var[y ]
-
   
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
    var[x + y ] = var[x] + var[y ]
= xx1 xx2
   
   
   
   
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
   
= xx1 xx2

(x1   e[x1])(x2   e[x2]) p(x1, x2)

21

    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    e[f (x)] =px f (x) p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
    var[x] =px(x   e(x))2 p(x = x)
id203
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x] = e[x 2]   e[x]2
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
    var[x + y ] = var[x] + var[y ]
topics: covariance matrix	
    var[x + y ] = var[x] + var[y ]
   
   
    covariance: 	
   
!

(x1   e[x1])(x2   e[x2]) p(x1, x2)

(x1   e[x1])(x2   e[x2]) p(x1, x2)

= xx1 xx2

= xx1 xx2

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
(x1   e[x1])(x2   e[x2]) p(x1, x2)
(x1   e[x1])(x2   e[x2]) p(x1, x2)

cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]

    cov(x1, x2) = 0
    var(x) = cov(x, x)

!
    if independent                          	
    cov(x1, x2) = 0
    cov(x1, x2) = 0
     	
    var(x) = cov(x, x)
    var(x) = cov(x, x)
    covariance matrix:
...
cov(x1, x1)

= xx1 xx2
= xx1 xx2
    cov(x1, x2) = 0
    cov(x) =264
    cov(x) =264
    var(x) = cov(x, x)
. . . cov(x1, xd)
    cov(x) =264
...
...
    cov(x) =264
. . . cov(x1, xd)
. . . cov(x1, xd)
...
...
...
...
...
cov(xd, x1)
. . . cov(xd, xd)
cov(xd, x1)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
. . . cov(xd, xd)
. . . cov(xd, xd)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx

. . . cov(x1, xd)
...
. . . cov(xd, xd)

cov(xd, x1)
    p (x = x)

...
cov(x1, x1)

    p (x = x)

375

22

cov(xd, x1)

cov(x1, x1)

cov(x1, x1)

375

375

375

...

...

...

...

...

...

cov(xd, x1)

375

cov(x1, x1)

    var(x) = cov(x, x)

= xx1 xx2
...

    cov(x1, x2) = 0
    var(x) = cov(x, x)

. . . cov(x1, xd)
...
. . . cov(xd, xd)

    cov(x1, x2) = 0
    var(x) = cov(x, x)
cov(x1, x1)
topics: continuous variables	
...
    for continuous variable      ,          is a density function	
cov(xd, x1)

    cov(x) =264
cov(x1, x1)
    cov(x) =264
. . . cov(x1, xd)
id203
...
375
    cov(x) =264
. . . cov(x1, xd)
cov(xd, x1)
    cov(x) =264
375
...
...
. . . cov(xd, xd)
cov(xd, x1)
. . . cov(x1, xd)
cov(x1, x1)
...
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
. . . cov(xd, xd)
. . . cov(xd, xd)
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
    p (x = x)
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
p(x) =
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
p(x) =
exp   1
2 (x     )>    1(x     )  e[x] =
    x 2 rd
1p(2   )d det(   )
   cov[x] =    
    x 2 rd
1p(2   )d det(   )

     	
    the id203                  is zero for continuous variables	
    p (x = x)
    in previous equations, summations would be replaced by integrals
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

    p (x = x)
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

   cov[x] =    
p(x) =

   cov[x] =    

p(x) =

4

   cov[x] =    

4

23

375

    var(x) = cov(x, x)

cov(xd, x1)
cov(x1, x1)

. . . cov(xd, xd)
. . . cov(x1, xd)
...
. . . cov(xd, xd)

    cov(x) =264
= xx1 xx2
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
    cov(x) =264
    var(x) = cov(x, x)
cov(x1, x1)
. . . cov(x1, xd)
375
    cov(x) =264
...
...
...
. . . cov(x1, xd)
cov(x1, x1)
    cov(x1, x2) = 0
(x1   e[x1])(x2   e[x2]) p(x1, x2)
id203
    p (x 2 a) =rx2a p(x)dx
375
...
...
cov(x1, x2) = e[(x1   e[x1])(x2   e[x2])]
. . . cov(x1, xd)
...
cov(xd, x1)
. . . cov(xd, xd)
    var(x) = cov(x, x)
...
(x1   e[x1])(x2   e[x2]) p(x1, x2)
. . . cov(xd, xd)
cov(xd, x1)
    p (x 2 a) =rx2a p(x)dx
topics: bernoulli, gaussian distributions	
    cov(x) =264
375
    p (x = x)
    p (x 2 a) =rx2a p(x)dx
. . . cov(x1, xd)
cov(x1, x1)
. . . cov(xd, xd)
    bernoulli variable: 	
...
...
    p (x 2 a) =rx2a p(x)dx
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
375
     	
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
. . . cov(xd, xd)
cov(xd, x1)
     	
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
...
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    p (x 2 a) =rx2a p(x)dx
375
. . . cov(x1, xd)
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
     	
exp   1
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 rd
. . . cov(xd, xd)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
exp   1
2 (x     )>    1(x     )  e[x] =
2 2     e[x] =    var[x] =  2
     	
    x 2 rd
1p(2   )d det(   )
p(x) =
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
exp   1
2 (x     )>    1(x     )  e[x] =
    gaussian variable:   
. . . cov(xd, xd)
   cov[x] =    
1p(2   )d det(   )
2 2     e[x] =    var[x] =  2
   cov[x] =    
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
2 (x     )>    1(x     )  e[x] =
exp   1
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
exp   1
    x 2 rd
1p(2   )d det(   )
2 (x     )>    1(x     )  e[x] =
4
exp   1
2 2     e[x] =    var[x] =  2
    x 2 r p(x) = 1p2    2 exp     (x   )2
     	
    x 2 rd
1p(2   )d det(   )
2 2     e[x] =    var[x] =  2
     
2 (x     )>    1(x     )  e[x] =
exp   1
2 (x     )>    1(x     )  e[x] =

2 (x     )>    1(x     )  e[x] =
2 (x     )>    1(x     )  e[x] =

4
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )

    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
. . . cov(x1, xd)

    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
   cov[x] =    

2 (x     )>    1(x     )  e[x] =

cov(xd, x1)
...

1p(2   )d det(   )

   cov[x] =    

p(x) =

p(x) =

p(x) =

        

...

4

24

...

375

375

cov(xd, x1)

...
. . . cov(xd, xd)

    cov(x) =264
    cov(x) =264
    cov(x) =264
375
    p (x = x)
...
    p (x 2 a) =rx2a p(x)dx
    p (x 2 a) =rx2a p(x)dx
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
id203
    p (x 2 a) =rx2a p(x)dx
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    p (x = x)
    p (x = x)
topics: multivariate gaussian distributions	
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    p (x = x)
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
2 (x     )>    1(x     ) 
exp   1
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    gaussian variable:   
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
    x 2 rd
1p(2   )d det(   )
    x 2 {0, 1} p(x = 1) =    p(x = 0) = 1    e[x] =    var[x] =   (1   )
    x 2 r p(x) = 1p2    2 exp     (x   )2
2 2     e[x] =    var[x] =  2
2 (x     )>    1(x     ) 
exp   1
exp   1
2 (x     )>    1(x     ) 
       
    x 2 rd
1p(2   )d det(   )
    x 2 rd
1p(2   )d det(   )
p(x) =
p(x) =
    e[x] =    cov[x] =    
2 (x     )>    1(x     ) 
exp   1
    x 2 rd
1p(2   )d det(   )
p(x) =
     	
    e[x] =    cov[x] =    
    e[x] =    cov[x] =    
    e[x] =    cov[x] =    

p(x) =

4

4

4

4

25

statistics

topics: estimate of the expectation and covariance matrix	
    sample mean:	
!
    sample variance:	
!
    sample covariance matrix:	
!
    these estimators are unbiased, i.e.:

t pt x(t)
    b   = 1
t pt x(t)
t pt x(t)
    b   = 1
t 1pt(x(t)  b  )2
    b   = 1
t pt x(t)
    b 2 = 1
    b   = 1
t 1pt(x(t)  b  )2
t 1pt(x(t)  b  )2
    b 2 = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b 2 = 1
t 1pt(x(t)  b  )2
    b    = 1
    b 2 = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
    e[b  ] =    e[b   ] =    
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
    e[b  ] =    e[b   ] =    
    e[b  ] =    e[b   ] =    
    b       2
    e[b  ] =    e[b 2] =  2 ehb   i =    
    b       2
    b       2
       2 [ 1.96b  +b  ,b   + 1.96b ]
    b       2
       2 [ 1.96b  +b  ,b   + 1.96b ]

t

t

t

26

topics: con   dence interval	
    con   dence interval of the sample mean (1d):	

    if t is big, the following estimator is approx. gaussian with mean 0 and variance 1	

t 1pt(x(t)  b  )2
    b 2 = 1
t pt x(t)
t 1pt(x(t)  b  )(x(t)  b  )>
    b   = 1
    b    = 1
statistics
t 1pt(x(t)  b  )2
    b 2 = 1
    e[b  ] =    e[b 2] =  2 ehb   i =    
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
    e[b  ] =    e[b 2] =  2 ehb   i =    
b     p
   
b 2/t
b     p
b 2/t
       2 [ 1.96b  +b  ,b   + 1.96b ]
       2b       1.96pb 2/t
   

   

   

    then we have that, with 95% id203, that

!
!
!

b    = arg max

27

p(x(1), . . . , x(t ))

b    = arg max

t

   

t 1pt(x(t)  b  )(x(t)  b  )>
t 1pt(x(t)  b  )(x(t)  b  )>
    b    = 1
    e[b  ] =    e[b   ] =    
t 1pt(x(t)     )2
    e[b  ] =    e[b   ] =    
statistics
    b       2
t 1pt(x(t)  b  )(x(t)  b  )>
       2 [ 1.96b  +b  ,b   + 1.96b ]
topics: maximum likelihood, i.i.d. hypothesis	
       2 [ 1.96b  +b  ,b   + 1.96b ]
    e[b  ] =    e[b   ] =    
    maximum likelihood estimator (id113):	
b    = arg max
    p(x(1), . . . , x(t )) =qt p(x(t))
       2 [ 1.96b  +b  ,b   + 1.96b ]
t b    = 1
    t 1
b    = arg max
    supervised learning example: (x, y)
p(x(1), . . . , x(t )) =yt
    training set: dtrain = {(x(t), y(t)}
    supervised learning example: (x, y)
p(x(t))
    training set: dtrain = {(x(t), y(t)}

    the sample mean is the id113 for a gaussian distribution 	
    the sample covariance matrix isnt, but this is	
machine learning

p(x(1), . . . , x(t ))
!
    independent and identically distributed variables

t pt(x(t)  b  )(x(t)  b  )>

p(x(1), . . . , x(t ))

   

!
!

   

   

!

p(x(1), . . . , x(t ))

   

b    = arg max
p(x(1), . . . , x(t )) =yt

machine learning

p(x(t))

28

   

   

   

   

p(x(1), . . . , x(t ))

       2b       1.96pb 2/t
       2b       1.96pb 2/t
b 2/t
       2b       1.96pb 2/t
   
b    = arg max
sampling
b    = arg max
b    = arg max
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
topics: monte carlo estimate	
p(x(1), . . . , x(t )) =yt
    monte carlo estimate:	
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t b    = 1
tpt(x(t)  b  )(x(t)  b  )>
    t 1
t b    = 1
    t 1
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
kpk f(x(k))
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
    e[f(x)] =px f(x) p(x)     1
    x(k) p(x)
    x(k) p(x)

    a method to approximate an expensive expectation	

    the        must be sampled from 

    x(k) p(x)

p(x(1), . . . , x(t ))

p(x(t))

p(x(t))

   

   

!
!

   

   

machine learning

machine learning

machine learning

p(x(1), . . . , x(t ))

p(x(t))

    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    f (x;    )
    dvalid dtest

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

29

   

   

   

   

   

   

   

p(x(t))

p(x(t))

p(x(t))
p(x(1), . . . , x(t ))

       2b       1.96pb 2/t

b    = arg max
p(x(1), . . . , x(t )) =yt
b     p
b    = arg max
b 2/t
   
p(x(1), . . . , x(t )) =yt
       2b       1.96pb 2/t
sampling
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
b    = arg max
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
   
topics: importance sampling	
t pt(x(t)  b  )(x(t)  b  )>
kpk f (x(k))
    e[f (x)] =px f (x) p(x)     1
p(x(1), . . . , x(t )) =yt
t b    = 1
    t 1
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
p(x(t))
    importance sampling:	
kpk f(x(k))
    e[f(x)] =px f(x) p(x)     1
t pt(x(t)  b  )(x(t)  b  )>
    x(k) p(x)
t b    = 1
    a sampling method for when           is expensive to sample from	
    t 1
    x(k) p(x)
t b    = 1
    t 1
    x(k) p(x)
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
kpk f (x(k)) p(x)
    e[f (x)] =px f (x) p(x)
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k)) p(x)
    e[f(x)] =px f(x) p(x)
q(x) q(x)     1
q(x) q(x)     1
    e[f(x)] =px f(x) p(x)
kpk f(x(k)) p(x)
            is easier to sample from and should be as similar as possible to 	
    supervised learning example: (x, y) x y
q(x) q(x)     1
    q(x)
    q(x)
- designing a good         is often hard to do
    training set: dtrain = {(x(t), y(t))}
machine learning
machine learning
machine learning
    f(x;    )
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    f(x;    )
   
t xt
    f(x;    )
    dvalid dtest
    dvalid dtest
    f (x;    )
    dvalid dtest

t pt(x(t)  b  )(x(t)  b  )>

l(f(x(t);    ), y(t)) +     (   )

machine learning
q(x)

    x(k) p(x)

    x(k) p(x)

arg min

    q(x)

q(x)

q(x)

!
!

1

30

   

   

   

   

   

   

   

   

p(x(t))

p(x(t))

p(x(1), . . . , x(t ))

       2b       1.96pb 2/t
       2b       1.96pb 2/t
b    = arg max
b    = arg max
t pt(x(t)  b  )(x(t)  b  )>
b    = arg max
b    = arg max
t b    = 1
    t 1
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
sampling
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
   
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
tpt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t b    = 1
topics: id115 (mcmc)	
    t 1
    x(k) p(x)
tpt(x(t)  b  )(x(t)  b  )>
tpt(x(t)  b  )(x(t)  b  )>
    e[f (x)] =px f (x) p(x)     1
kpk f (x(k))
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
t b    = 1
t b    = 1
    t 1
    t 1
    mcmc:	
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
kpk f(x(k))
    e[f(x)] =px f(x) p(x)     1
q(x) q(x)     1
    iterative method to generate the sequence of	
    x(k) p(x)
    x(k) p(x)
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
    the set of         will be dependent of each other	
    x(k) p(x)
    x(k) p(x)
q(x) q(x)     1
    q(x)
q(x)
!
machine learning
    q(x)
    supervised learning example: (x, y) x y
    x(1) t (x0 x)
 ! x(2) t (x0 x)
 ! x(3) t (x0 x)
!
 !       
    x(1) t (x0 x)
 ! x(2) t (x0 x)
t (x0 x)
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
 ! x(k)
    training set: dtrain = {(x(t), y(t))}
!
 !       
machine learning
                       is a transition operator, that must satisfy certain properties	
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    t (x0   x)
    f(x;    )
    k must be big for the set of samples be representative of distribution	
    f(x;    )
    f(x;    )
machine learning
    dvalid dtest
    supervised learning example: (x, y) x y
    usually, we drop the    rst samples, which are not reliable
    dvalid dtest
    dvalid dtest
    supervised learning example: (x, y) x y
   
    training set: dtrain = {(x(t), y(t))}
arg min
   
   
    training set: dtrain = {(x(t), y(t))}
t xt
l(f(x(t);    ), y(t)) +     (   )
arg min
    f (x;    )
    f (x;    )

t (x0 x)
 ! x(k)

 ! x(3) t (x0 x)

t xt
t xt

machine learning

machine learning

arg min

q(x)

1

1

1

   

l(f(x(t);    ), y(t)) +     (   )
31

l(f(x(t);    ), y(t)) +     (   )

q(x)

q(x)

    x(k) p(x)

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
p(x(1), . . . , x(t )) =yt
    e[f (x)] =px f (x) p(x)     1
    e[f(x)] =px f(x) p(x)
kpk f(x(k)) p(x)
    e[f(x)] =px f(x) p(x)     1
kpk f(x(k))
q(x) q(x)     1
    x(k) p(x)
t pt(x(t)  b  )(x(t)  b  )>
    e[f (x)] =px f (x) p(x)
kpk f (x(k)) p(x)
t b    = 1
    t 1
    x(k) p(x)
sampling
q(x) q(x)     1
    e[f (x)] =px f (x) p(x)
    q(x)
kpk f(x(k))
    e[f(x)] =px f(x) p(x)     1
    e[f(x)] =px f(x) p(x)
kpk f(x(k)) p(x)
q(x) q(x)     1
    q(x)
    q(x)
topics: id150	
t (x0 x)
    x(1) t (x0 x)
 ! x(2) t (x0 x)
 ! x(3) t (x0 x)
    x(k) p(x)
 ! x(k)
    q(x)
 !       
 ! x(3) t (x0 x)
    x(1) t (x0 x)
 ! x(2) t (x0 x)
t (x0 x)
 ! x(k)
 !       
 ! x(2) t (x0 x)
    x(1) t (x0 x)
    id150:	
kpk f(x(k)) p(x)
    e[f(x)] =px f(x) p(x)
q(x) q(x)     1
 ! x(3) t (x0 x)
    x(1) t (x0 x)
t (x0 x)
 ! x(2) t (x0 x)
 ! x(k)
 !       
q(x)
    t (x0   x)
    t (x0   x)
    mcmc method which uses the following transition operator	
    t (x0   x)
    q(x)
    t (x0   x)
    xi x0
- pick a variable       	
    xi x0
- obtain       by only resampling this variable according to   
t (x0 x)
 ! x(3) t (x0 x)
 ! x(2) t (x0 x)
    x(1) t (x0 x)
    xi x0
 ! x(k)
 !       
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    supervised learning example: (x, y) x y
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    t (x0   x)
machine learning
    training set: dtrain = {(x(t), y(t))}
    xi x0
return	
    supervised learning example: (x, y) x y
!
    f (x;    )
    p(xi|x1, . . . , xi 1, xi+1, . . . , xd)
    supervised learning example: (x, y) x y
    often, we simply cycle through the variables, in random order
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    supervised learning example: (x, y) x y
machine learning
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
   
    supervised learning example: (x, y) x y
    f(x;    )
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}

machine learning

machine learning

arg min

32

-

   
   
machine learning

p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
machine learning
t pt(x(t)  b  )(x(t)  b  )>
machine learning

t pt(x(t)  b  )(x(t)  b  )>
tpt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
topics: supervised learning	
    learning example: 	
    supervised learning example: (x, y)
    task to solve: predict target     from input	
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t)}
    classi   cation: target is a class id (from 0 to nb. of class - 1)	
    training set: dtrain = {(x(t), y(t)}
    training set: dtrain = {(x(t), y(t)}
    regression: target is a real number
   

machine learning
machine learning

34

supervised training example

entr  e x

sortie f(x)

cible y

six

deux!

t pt(x(t)  b  )(x(t)  b  )>

machine learning
machine learning
topics: unsupervised learning	
    learning example: 	
    supervised learning example: (x, y) x y
    no explicit target to predict	
    training set: dtrain = {(x(t), y(t)}
    id91: partition data into groups	
    feature extraction: learn meaningful features automatically	
    id84: learning a lower-dimensional representation of input

36

p(x(t))

p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
machine learning
t pt(x(t)  b  )(x(t)  b  )>
machine learning
t pt(x(t)  b  )(x(t)  b  )>
machine learning

   
t b    = 1
    t 1
t b    = 1
    t 1
tpt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
topics: semi-supervised learning	
    few supervised learning examples: 	
    supervised learning example: (x, y)
machine learning
    many unsupervised learning examples:	
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t)}
    task to solve: predict target     from input	
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t)}
    classi   cation: target is a class id (from 0 to nb. of class - 1)	
   
    training set: dtrain = {(x(t), y(t)}
    training set: dtrain = {(x(t), y(t)}
    regression: target is a real number	
    can the extra unsupervised data help?
   

machine learning

p(x(t))

37

apprentissage semi-supervis  

entr  e x2

classe verte

classe id7e

entr  e x1

p(x(1), . . . , x(t )) =yt

   

machine learning
machine learning

topics: learning algorithm, model, training set	
    learning algorithm        	

p(x(1), . . . , x(t )) =yt
p(x(1), . . . , x(t )) =yt
b    = arg max
p(x(1), . . . , x(t )) =yt
b    = arg max
   
p(x(1), . . . , x(t )) =yt
t pt(x(t)  b  )(x(t)  b  )>
machine learning
   
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
p(x(1), . . . , x(t )) =yt
t b    = 1
    t 1
t b    = 1
    t 1
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
t b    = 1
    t 1
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    takes as input a training set	
    training set: dtrain = {(x(t), y(t))}
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    outputs a model 	
    f (x;    )
   
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    training set: dtrain = {(x(t), y(t))}
    the model has learned the information present in 	
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    f (x;    )
    f(x;    )

    we can now use the model            on new inputs

    we then say the model            was trained on 	

machine learning
machine learning

machine learning

machine learning

39

machine learning

t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    t 1
machine learning
machine learning
machine learning
    supervised learning example: (x, y) x y
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    supervised learning example: (x, y) x y
topics: training, validation and test sets, generalization	
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    training set            serves to train a model	
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    validation set           serves to select hyper-parameters	
    dvalid dtest
    f(x;    )
    test set           serves to estimate the generalization 
    dvalid dtest
performance (error)	
!
    generalization is the behavior of the model on unseen 
examples	
    this is what we care about in machine learning!

40

machine learning

topics: capacity of a model, under   tting, over   tting, hyper-
parameter, model selection	
    capacity:    exibility of a model 	
    hyper-parameter: a parameter of a model that is not trained 
(speci   ed before training)	
    under   tting: state of model which could improve 
generalization with more training or capacity	
    over   tting: state of model which could improve 
generalization with less training or capacity	
    model selection: process of choosing the best hyper-
parameters on validation set

41

machine learning

topics: capacity of a model, under   tting, over   tting, hyper-
parameter, model selection

training

validation

under   tting

over   tting

0.5

0.4

0.3

0.2

0.1

0.0

training time or capacity

42

machine learning

topics: interaction between training set size/capacity/training 
time and training error/generalization error	
    if capacity increases:	
    training error will ?	
decrease
    generalization error will ?	

increase or decrease

    if training time increases:	

    training error will ?	
decrease
    generalization error will ?	

increase or decrease

    if training set size increases:	

    generalization error will ?	
decrease (or maybe stay the same)
    difference between the training and generalization error will ?

decrease

43

machine learning

t pt r   l(f (x(t);    ), y(t))    r      (   )

    l(f (x(t);    ), y(t))
       (   )
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
topics: bias-variance trade-off	
      =   1
       (   )
       (   )
       (   )
    l(f (x(t);    ), y(t))
    variance of trained model: does it vary a lot if the training set 
    l(f (x(t);    ), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
      =   1
      =   1
      =   1
       (   )
changes 	
       (   )
    {x 2 rd | rxf (x) = 0}
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
              +  
              +  
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
      =   1
    bias of trained model: is the average model close to the true 
    v>r2
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
              +  
solution?	
              +  
    v>r2
xf (x)v < 0 8v
    v>r2
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
xf (x)v > 0 8v
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf (x) = 0}
    generalization error can be seen as the sum of bias and the 
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
    v>r2
    v>r2
xf (x)v < 0 8v
xf (x)v < 0 8v
xf (x)v < 0 8v
    v>r2
xf (x)v > 0 8v
    v>r2
xf (x)v > 0 8v
variance
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
possible
    f    f
    (x(t), y(t))
    (x(t), y(t))
    (x(t), y(t))
      =  r   l(f (x(t);    ), y(t))    r      (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
    f    f
    f    f
    f    f
    (x(t), y(t))
    (x(t), y(t))
possible
    f    f
possible
    f    f
good trade-off

high variance/ 

low variance/ 

high bias

low bias

44

t pt(x(t)  b  )(x(t)  b  )>
t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
    f (x;    )
    supervised learning example: (x, y) x y
machine learning
    supervised learning example: (x, y) x y
machine learning
    dvalid dtest
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
    training set: dtrain = {(x(t), y(t))}
   
topics: empirical risk minimization, id173	
    f(x;    )
    f(x;    )
    empirical risk minimization	
    f(x;    )
    dvalid dtest
    dvalid dtest
    framework to design learning algorithms	
    dvalid dtest
   
    l(f (x(t);    ), y(t))
!
1
   
l(f(x(t);    ), y(t)) +     (   )
l(f(x(t);    ), y(t)) +     (   )
!
l(f(x(t);    ), y(t)) +     (   )
       (   )
!
      =   1
    l(f(x(t);    ), y(t))
                                  is a id168	
    l(f(x(t);    ), y(t))
               is a regularizer (penalizes certain values of     )	
              +  
       (   )
       (   )
    learning is cast as optimization	

t xt
t xt
t xt

1
arg min
1
arg min

arg min

   

   

   

   

arg min

1

t xt

t pt r   l(f (x(t);    ), y(t))    r      (   )

    ideally, we   d optimize classi   cation error, but it   s not smooth	
    id168 is a surrogate for what we truly should optimize (e.g. upper bound)

45

machine learning

topics: id119	
    id119: procedure to minimize a function	

    compute gradient	
    take step in opposite direction

46

machine learning

topics: id119

descent    
direction

 f(x)

 x

 

47

l(f (x(t);    ), y(t)) +     (   )

l(f(x(t);    ), y(t)) +     (   )

t xt
t xt
t xt

1

   

   

   
arg min

arg min
1

topics: id119	
    id119 for empirical risk minimization	

machine learning
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f(x(t);    ), y(t))    r      (   )

    f(x;    )
    dvalid dtest
   
    l(f (x(t);    ), y(t))
   
       (   )
      =   1
    l(f (x(t);    ), y(t))
    l(f(x(t);    ), y(t))
    initialize 	
              +  
       (   )
       (   )
    for n iterations	
      =   1
 	
      =   1
-
              +      
              +  
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))

5

48

machine learning

topics: local and global optima

49

   
   

1
1

l(f (x(t);    ), y(t)) +     (   )
l(f (x(t);    ), y(t)) +     (   )

t pt r   l(f(x(t);    ), y(t))    r      (   )
       (   )
    f (x;    )
      =   1
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    dvalid dtest
       (   )
    dvalid dtest
t pt r   l(f(x(t);    ), y(t))    r      (   )
    training set: dtrain = {(x(t), y(t))}
machine learning
    f (x;    )
      =   1
t pt r   l(f (x(t);    ), y(t))    r      (   )
              +  
   
      =   1
t xt
   
    f (x;    )
    dvalid dtest
t xt
arg min
arg min
    dvalid dtest
    {x 2 rd | rxf(x) = 0}
              +  
topics: critical points, local optima, saddle point, curvature	
   
              +  
t xt
   
    l(f (x(t);    ), y(t))
t xt
    critical points: 	
    l(f (x(t);    ), y(t))
    {x 2 rd | rxf (x) = 0}
    {x 2 rd | rxf(x) = 0}
    v>r2
xf(x)v > 0 8v
       (   )
    l(f (x(t);    ), y(t))
       (   )
!
    v>r2
xf (x)v > 0 8v
t pt r   l(f (x(t);    ), y(t))    r      (   )
    l(f (x(t);    ), y(t))
t pt r   l(f (x(t);    ), y(t))    r      (   )
       (   )
      =   1
    curvature in direction     : 	
    v>r2
    v>r2
xf(x)v > 0 8v
      =   1
xf(x)v < 0 8v
    v>r2
       (   )
      =   1
xf (x)v < 0 8v
              +  
              +  
!
      =   1
              +  
    v>r2
xf(x)v < 0 8v
    {x 2 rd | rxf (x) = 0}
    types of critical points:	
    {x 2 rd | rxf (x) = 0}
              +  
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    local minima:                                           (i.e.              positive de   nite)	
    {x 2 rd | rxf (x) = 0}
    v>r2
    v>r2
xf (x)v > 0 8v
xf (x)v > 0 8v
    local maxima:                                          (i.e.               negative de   nite)	
    v>r2
xf (x)v < 0 8v
    v>r2
5
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
    v>r2
xf (x)v < 0 8v
    saddle point:    curvature is positive in certain directions and negative in others
    v>r2
xf (x)v < 0 8v

t pt r   l(f (x(t);    ), y(t))    r      (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )

l(f (x(t);    ), y(t)) +     (   )

l(f (x(t);    ), y(t)) +     (   )

arg min

arg min

5

1

1

   

   

5
5

50

5
5

machine learning

topics: saddle point

51

   

   

   

1

t xt

l(f (x(t);    ), y(t)) +     (   )

t xt
t xt
arg min
    l(f (x(t);    ), y(t))
machine learning
    l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       (   )
t pt r   l(f (x(t);    ), y(t))    r      (   )
      =   1
       (   )
       (   )
topics: stochastic id119	
   
t pt r   l(f (x(t);    ), y(t))    r      (   )
t xt
              +  
      =   1
t pt r   l(f (x(t);    ), y(t))    r      (   )
l(f (x(t);    ), y(t)) +     (   )
arg min
      =   1
    {x 2 rd | rxf (x) = 0}
    algorithm that performs updates after each example	
              +  
   
    v>r2
xf (x)v > 0 8v
    initialize 	
    {x 2 rd | rxf (x) = 0}
              +  
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v < 0 8v
    for n iterations	
    v>r2
xf (x)v > 0 8v
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    v>r2
xf (x)v < 0 8v
for each training example	
    (x(t), y(t))
      =   1
     	
      =  r   l(f (x(t);    ), y(t))    r      (   )
     
              +      
    {x 2 rd | rxf (x) = 0}
    v>r2
xf (x)v > 0 8v
    v>r2
xf (x)v < 0 8v
5
      =  r   l(f (x(t);    ), y(t))    r      (   )
    (x(t), y(t))

t pt r   l(f (x(t);    ), y(t))    r      (   )

5

5

-

52

machine learning

topics: parametric vs. non-parametric	
    parametric model: its capacity is    xed and does not increase 
with the amount of training data	
    examples: linear classi   er, neural network with    xed number of hidden units, etc.	

    non-parametric model: the capacity increases with the 
amount of training data	
    examples: k nearest neighbors classi   er, neural network with adaptable hidden 

layer size, etc.

53

ef   cacit   computationnelle
fen  tres de parzen
r  gression lin  aire

  tiquette

  tiquette

?

?

entr  e

entr  e

(nombre d   exemples n > dimension d)

cpuramentra  nemento(ndo(dpr  dictiono(d)o(d)cpuramentra  nemento(nd)o(nd)pr  dictiono(nd)o(nd)cpuramentra  nemento(ndo(dpr  dictiono(d)o(d)cpuramentra  nemento(nd)o(nd)pr  dictiono(nd)o(nd)motivations for deep learning

supervised learning and local generalization

locally capture the variations

easy when there are 
only a few variations

curse of dimensionnality

   to generalize 
locally, need 
examples 
representative of 
each possible  
variation.

a parzen window example

  tiquette (label)

fen  tres de parzen

2

1.25
1

entr  e (input)
besoin d   exemples dans chaque r  gion de l   espace !

(in)ef   cacit   statistique : 
arbres de d  cision

id90 do not generalize to new variations 

!

y. bengio, o.delalleau et c.simard 
computational intelligence, 2010

arbre de d  cision (id90)
  tude de cas :   
  valuation d   une soutenance de th  se

n_slides < 20 

beurk ! 

dur  e < 30 

n_slides < 40 

n_slides < 30 

trop court ! 

trop vite ! 

bien ! 

f  licitations ! 

un arbre partitionne l   espace

dur  e

30

n_slides

20

30

40

n_slides < 20 

chaque r  gion est ind  pendante :
l   appr  ciation d   une soutenance ne 
beurk ! 
beurk !
d  pend que des autres soutenances 
pr  sentes dans la m  me r  gion
n_slides < 30 

n_slides < 40 

dur  e < 30 

trop court ! 
trop court ! 

trop vite ! 
trop vite ! 

bien ! 
bien ! 

f  licitations ! 
f  licitations ! 

dif   cult   : 
apprentissage de variations

entr  e x2

?

entr  e x1

th  or  me

pour bien approximer une fonction de 
la famille f en dimension d, un arbre 
de  d  cision  a  besoin  d   au  moins  cd 
exemples d   apprentissage. 

manifestation de la mal  diction de la dimensionalit  

la profondeur    la rescousse

combinaison 

choix parmi 

nombre de r  gions possibles : 54 = 625
d   la .... 

trop court

trop vite 

bien 

f  licitations 

nombre de r  gions possibles : 5

examples of deep learning

models

neural probabilistic language 
successes of 
this 
architecture 
and its 
descendents: 
beats localist 
state-of-the-art 
in nlp in most 
tasks 
(language 
model, 
chunking, 
semantic role 
labeling, pos)

embedding symbols

nearby words in semantic space

francejesusxboxreddishscratchedspainchristplaystationyellowishsmasheditalygoddreamcastgreenishrippedrussiaresurrectionps###brownishbrushedpolandprayersnesbluishhurledenglandyahwehwhcreamygrabbeddenmarkjosephusneswhitishtossedgermanymosesnintendoblackishsqueezedportugalsingamecubesilveryblastedswedenheavenpspgreyishtangledaustriasalvationamigapalerslashedconvolutional nets
    success of deep learning in id161 largely attributed to 
convolutional neural networks (circa 1990) + a few tricks (new 
class of id180 + dropout training) 

    rob fergus: nips 2013 tutorial - deep learning for computer 

vision https://www.youtube.com/watch?v=qgx57x0fbda

71

convnet in action

id163 1k competition, fall 2012
alex krizhevsky, ilya sutskever, geoffrey e. hinton, 2012

"
r
o
r
r
e

40"

35"

30"

25"

20"

15"

10"

5"

0"

c

r

r - x

a

e

l

e  
u .  o f  a

m  

a

st e r d

m

r i a  

e /i n

c

r

x

xf o r d  

o

i s i 

n  

e r v isio

p

u

s

72

convnet in action
    supervision (a.k.a. alexnet) id98 by the numbers	

    trained on 1.2 million images, roughly 1k images for each of the 1k classes.	
    trained with stochastic id119 on two nvidia gpus for about a week	
    650,000 neurons,         60,000,000 parameters,         630,000,000 connections

input image

90%$parameters$

output class prediction

73

convnet in action

    training paradigm:	

    recti   ed linear id180.	
    trained with dropout. 	
    dataset expansion (data augmentation) employed.
96 low-level learned features:

!"#$%&'(%)#$*+,$%-%$#./$0%'1

74

one year later

id163 1k competition, fall 2013

image from li deng, 2014

75

reaching human-level 

performance

    id163 1000-categories object recognition benchmark

before deep learning

after deep learning

human performance: 5% 

76

understanding id98s

image from zeiler and fergus, 2013

77

understanding id98s

image from zeiler and fergus, 2013

figure 2. visualization of features in a fully trained model. for layers 2-5 we show the top 9 activations in a random subset
of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.

78

layer 4layer 5dl for id103

    no improvement for 10+ 

years of research.	

    use of deep learning 

(dbns) quickly reduced 
the error from ~23% to 
<15% 	

    early results used dbns 
with greedy layer-wise 
unsupervised pretraining.	

    recently, unsupervised 

pretraining appears to be 
irrelevant.

image from li deng, 2014

year

79

