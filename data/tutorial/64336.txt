toward natural language 
semantics in learned 
representations

 

sam bowman
asst. prof. of linguistics and data science, nyu

ipam workshop: new deep learning techniques

context: deep learning in nlp

as in vision and elsewhere, deep learning techniques have 
yielded very fast progress on a few important data-rich tasks:

    reading comprehension questions

    near human performance (but brittle)

    translation

    large, perceptually obvious improvements over past 

systems.

    syntactic parsing

    measurable improvements on a longstanding state of the 

art

the question

can current neural network methods learn to do anything 
that resembles id152?

the question

can current neural network methods learn to do anything 
that resembles id152?

if we take this as a goal to work toward, what   s our metric?

proposal:
natural language 
id136 as a 
research task

natural language id136 (nli)
also known as recognizing id123 (rte)

  james byron dean refused to move without blue jeans

{entails, contradicts, neither}

james dean didn   t dance without pants

example: maccartney thesis    09

judging understanding with nli

to reliably perform well at nli, your representations of 
meaning  must handle with the full complexity of 
id152:*

    lexical entailment (cat vs. animal, cat vs. dog)
    quantification (all, most, fewer than eight)
    lexical ambiguity and scope ambiguity (bank, ...)
    modality (might, should, ...)
    common sense background knowledge

   

* without grounding to the outside world.

why not other tasks?

many tasks that have been used to evaluate sentence 
representation models don   t require all that much language 
understanding:

    id31
    sentence similarity

   

why not other tasks?

nli isn   t the only task to require high-quality natural language 
understanding, see also:

    machine translation
    id53
    goal-driven dialog
    id29
    syntactic parsing

 

   

but it   s the easiest of these.

outline

    background: nli as a research task for nlu
    part 1  data and early results
    part 2  more data, more results
    part 3  next steps: discovering structure
    conclusion

emnlp    15
best new data 
set award

part i

the stanford nli corpus

samuel r. bowman
gabor angeli
christopher potts
christopher d. manning

natural language id136 data

corpus

size

natural validated

   

   

   

fracas

rte

sick

dg

levy

~

   

   

~

.3k

7k

10k

728k

1,500k

ppdb2

100,000k

~

natural language id136 data

corpus

size

natural validated

   

   

   

   

fracas

rte

sick

snli

dg

levy

~

   

   

   

~

.3k

7k

10k

570k

728k

1,500k

ppdb2

100,000k

~

our data collection 
prompt

source captions from flickr30k: young, lai, hodosh, and hockenmaier, tacl    14

entailment

source captions from flickr30k: young, lai, hodosh, and hockenmaier, tacl    14

entailment

           neutral

source captions from flickr30k: young, lai, hodosh, and hockenmaier, tacl    14

entailment

           neutral

contradiction

source captions from flickr30k: young, lai, hodosh, and hockenmaier, tacl    14

what we got

some sample results

premise: two women are embracing while holding to go 
packages.

hypothesis: two woman are holding packages.

label: entailment

some sample results

premise: a man in a blue shirt standing in front of a garage-like 
structure painted with geometric designs.

hypothesis: a man is repainting a garage

label: neutral

some sample results

premise: a man selling donuts to a customer during a world 
exhibition event held in the city of angeles

hypothesis: a woman drinks her coffee in a small cafe.

label: contradiction

results on snli

some results on snli

model

most frequent class

big lexicalized classifier

test accuracy

34.2%

78.2%

two classes of neural network

    sentence encoder-based models

s1

s2

encoder

encoder

mlp

classifier

    attention and memory models

s1

encoder

attentional 
encoder

classifier

s2

encoder

some results on snli

model

most frequent class

big lexicalized classifier

300d cbow

300d bilstm

test accuracy

34.2%

78.2%

80.6%

81.5%

some results on snli

model

most frequent class

big lexicalized classifier

300d cbow

300d bilstm

reinforce-trained self-attention 
(tao shen et al.    18) 

self-attention/cross-attention + ensemble
(yi tay et al.    18) 

test accuracy

34.2%

78.2%

80.6%

81.5%

86.3%

89.3%

success?

    we   re not at human performance yet   
    ...but with 100+ published experiments, the best systems 

rarely stray too far from the standard toolkit:
    lstms
    attention
    pretrained id27s
    ensembling

part ii

the multi-genre nli corpus

adina williams
nikita nangia
samuel r. bowman

snli is showing its limitations

    little headroom left:

    sota: 89.3%
    human performance: ~96%

    many linguistic phenomena underattested or ignored

    tense
    beliefs
    modality (possibility/permission)

...

the multigenre nli corpus

genre

train

dev

test

captions (snli corpus)

(550,152)

(10,000)

(10,000)

fiction

government

slate

switchboard (telephone speech)

travel guides

77,348

77,350

77,306

83,348

77,350

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

the multigenre nli corpus

genre

train

dev

test

captions (snli corpus)

(550,152)

(10,000)

(10,000)

fiction

government

slate

switchboard (telephone speech)

travel guides

9/11 report

face-to-face speech

letters

oup (nonfiction books)

verbatim (magazine)

77,348

77,350

77,306

83,348

77,350

0

0

0

0

0

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

total

392,702

20,000

20,000

the multigenre nli corpus

genre

train

dev

test

captions (snli corpus)

(550,152)

(10,000)

(10,000)

fiction

government

slate

switchboard (telephone speech)

travel guides

9/11 report

face-to-face speech

letters

oup (nonfiction books)

verbatim (magazine)

77,348

77,350

77,306

83,348

77,350

0

0

0

0

0

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

2,000

genre-matched
evaluation

genre-mismatched
evaluation

total

392,702

20,000

20,000

what we got

typical dev set examples

premise: in contrast, suppliers that have continued to innovate 
and expand their use of the four practices, as well as other 
activities described in previous chapters, keep outperforming the 
industry as a whole.

hypothesis: the suppliers that continued to innovate in their use 
of the four practices consistently underperformed in the industry.

label: contradiction

genre: oxford university press (nonfiction books)

typical dev set examples

premise: someone else noticed it and i said well i guess that   s true 
and it was somewhat melodious in other words it wasn   t just you 
know it was really funny

hypothesis: no one noticed and it wasn   t funny at all.

label: contradiction

genre: switchboard (telephone speech)

typical dev set examples

premise: the father can beget new offspring safe from macbeth   s 
hand; the son is the palpable threat.

hypothesis: the son wants to kill him to marry his mom

label: neutral

genre: verbatim (magazine)

key findings

    inter-annotator agreement measures are identical 

between snli and multinli (within 0.5%): 
    multinli is not hard for humans.

    state-of-the-art snli models perform around 15 

percentage points worse when re-trained and tested on 
multinli. 
    multinli is hard for machine learning models.

key figures

early results

model

most frequent class

deep bilstms with gated skips
(chen et al.    17)

attention+convolutions
(gong et al.    18)

matched
test acc.

mismatched 
test acc.

36.5%

74.9%

35.6%

74.9%

80.0%

78.7%

nli as a pretraining task

conneau et al.    17; see also subramanian et al.    18

 

discussion: nli

    nli lets you judge the degree to which models can learn 

to understand natural language sentences.

    with snli, it   s now possible to train low-bias machine 

learning models like nns on nli.

    multinli makes it possible to test models    ability to 

understand american english in nearly its full range of 
uses.

    sentence encoders trained on nli, like infersent, are 

likely the best general-purpose encoders we have.

part iii
next steps:

learning syntax from semantics

adina williams
andrew drozdov
samuel r. bowman

tacl 2018

background: treelstms

treelstms replace the linear sequence of an 
lstm id56 with a binary tree from a trained 
parser.

treelstms outperform comparable lstm id56s 
by small but consistent margins on tasks like 
sentiment, translation, and nli.  

entailment

goal: learn syntax from semantics

mlp

parser

the old cat ate

what?
    build one model that can:

    parse sentences
    use resulting parses in a treeid56 text classifier

    train the full model (including the parser!) on snli or 

multinli

why?
    engineering objective: 

identify better parsing strategies for nlu

    scientific objective:

discover what syntactic structures are both 
valuable and learnable.

entailment

results to date

mlp

three 2017 papers on snli report that treelstms learned 
trees outperform ones based on trees from an external 
parser:

    yogatama et al.: 

    shift-reduce parser + reinforce

    maillard et al.: 

    chart parser + soft gating

    choi et al.:

    novel parser + straight through gumbel softmax

parser

limited analysis of the resulting parses so far.

the old cat ate

entailment

our findings

mlp

we reproduced the numeric results for the best two of these.

if thoroughly tuned for dev set performance, both learn:

    trivial left- or right-branching trees (id56-equivalent)

    ...or trivial balanced trees.

parser

the old cat ate

entailment

our findings

mlp

no categorical successes yet. 

open problems:

    the performance gain from discovering correct trees is 

small, and therefore difficult to optimize for with current 
tools. could better models improve this?

    how do we explore possible parsing strategies when it 

may take many gradient updates to the rest of the model 
to know if any strategy helps?

parser

the old cat ate

thanks!

questions, code, & data: 

nyu.edu/projects/bowman 

plus:

    adina williams is on the job market in cognitive science!
    nikita nangia and andrew drozdov are applying to phd 

programs in nlp!

