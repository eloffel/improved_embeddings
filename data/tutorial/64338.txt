unraveling the mysteries of 
stochastic id119 
on deep neural networks

pratik chaudhari

ucla vision lab

1

the question

measures disagreement of

predictions with ground truth

cat dog ...

x    = argmin

x

f (x )

weights aka

parameters

stochastic id119
xk +1 = xk       +fb(xk )

why is sgd 
so special?

2

many, many variants: 

adagrad, rmsprop, adam,


sag, svrg, catalyst,


appa, natasha, katyusha   

empirical evidence: wide    minima   

105

103

y
c
n
e
u
q
e
r
f

10

short negative tail

104

103

102

y
c
n
e
u
q
e
r
f

0
 0.5

 0.4

 0.3
 0.2
eigenvalues

 0.1

0.0

0
 5

0

10

20

eigenvalues

30

40

3

a bit of statistical physics

    energy landscape of a binary id88

many sharp minima

few wide minima,!
but generalize better!
[baldassi et al., '15]

    wide minima are a large deviations phenomenon

4

tilting the gibbs measure

    local id178 [chaudhari et al., iclr '17]

x    = argmin

x

f (x )

= argmax

x

e f (x )

x

    argmin
gaussian kernel!
of variance  

  log   g      e f (x )   

5

parle: parallelization of sgd
    state-of-the-art performance [chaudhari et al., sysml '18]

wide-resnet: cifar-10

all-id98: cifar-10 (25% data)

6

the question

why is sgd 
so special?

7

a continuous-time view of sgd

    diffusion matrix: variance of mini-batch gradients

var   +fb(x )    =

=

d (x )

b

1

b*,

1
n

nxk =1

+fk (x ) +fk (x )>   +f (x ) +f (x )>+-

    temperature: ratio of learning rate and step-size

  1 =

   
2b

8

a continuous-time view of sgd

    continuous-time limit of discrete-time updates
+q2  1d (x ) dw (t )

dx =  +f (x ) d t|{z},   

    fokker-planck (fp) equation gives the distribution on the   

weight space induced by sgd

will assume x 2            d

   t = div    +f    
|{z}drift

+   1div   d       
|          {z          }

di   usion

   

where x (t )        (t )

9

wasserstein gradient    ow

    heat equation                      performs steepest descent on the   

   t = div   i +      

dirichlet energy

1

2z    +   (x ) 2 dx

    it is also the steepest descent in the wasserstein metric for

 h (   ) =z   

8><>: h (   ) +
    negative id178 is a lyapunov functional for brownian motion

      
k +1 2 argmin

log     d   

   

converges to trajectories


of the heat equation

   2

2(   ,       
k )
2   

   ss
heat = argmin

   

 h (   )

9>=>;

10

wasserstein gradient    ow: with drift

    if          , the fokker-planck equation

d = i

   t = div   +f     +   1i +      

has the jordan-kinderleher-otto (jko) functional [jordan et al., '97]

   ss(x ) = argmin

   

as the lyapunov functional.

   x      ff (x )g
|        {z        }

energetic term

    1 h (   )

|     {z     }

entropic term

    fp is the steepest descent on jko in the wasserstein metric

11

what happens for non-isotropic noise?

   

+   1div   d       
|          {z          }
    fp monotonically minimizes the free energy

   t = div    +f    
|{z}drift
   x      f (x )g     1h (   )

   ss(x ) = argmin

di   usion

   

    rewrite as

f (   ) =   1kl (    ``    ss)

compare with |x - x*| for deterministic optimization.

12

sgd performs variational id136

theorem [chaudhari & soatto, iclr '18]
the functional 

f (   ) =   1kl (    ``    ss)

is minimized monotonically by trajectories of the

fokker-planck equation

   t = div   +f     +   1div (d    )   

with        as the steady-state distribution. moreover,

   ss

up to a constant.

  =    1 log    ss

13

some implications

    learning rate should scale linearly with batch-size

  1 =

   
2b

should not be small

    sampling with replacement regularizes better than without

also generalizes better

  1
w/o replacement =

   

2b 1  

b

n!

14

information bottleneck principle

    minimize mutual information of the representation with the training data   
[tishby '99, achille & soatto '17]

ib  (   ) =    x          ff (x )g     1 kl          `` prior   
    minimizing these functionals is hard, sgd does it naturally

15

potential phi vs. original loss f

    the solution of the variational problem is

   ss(x ) =

1
z 

e    (x )

    key point

   ss(x ) ,

e   f (x )

1
z 0 

most likely locations of

sgd are not the critical

points of the original loss

    the two losses are equal if and only if noise is isotropic

d (x ) = i ,  (x ) = f (x )

16

deep networks have highly non-isotropic noise

cifar-10

 (d ) = 0.27    0.84

rank(d ) = 0.34%

cifar-100

 (d ) = 0.98    2.16

rank(d ) = 0.47%

    evaluate neural architectures using the di   usion matrix

17

how different are cats and dogs, really?

18

sgd converges to limit cycles

theorem [chaudhari & soatto, iclr '18]
the most likely trajectories of sgd are 

  x = j (x ),

where the "leftover" vector    eld

j (x ) =  +f (x ) + d (x ) + (x )     1divd (x )

is such that

div j (x ) = 0.

19

trajectories of sgd

    run sgd for       epochs

105

fft of x i

k +1   x i

k

20

an example

+ (x ) = 0

force-   eld

saddle-point

j (x ) = 0

 j (x )  is small

very large  j (x ) 

21

most likely locations are not the critical points of the original loss

theorem [chaudhari & soatto, iclr '18]
the ito sde

is equivalent to an a-type sde

dx =  +f d t +q2  1d dw (t )
dx =     d + q    +  d t +q2  1d dw (t )
+f =   d + q    +      1 div   d + q   .

with the same steady-state                          if
   ss / e    (x )

22

knots in our understanding

architecture

optimization

generalization

23

punchline

is sgd special?

24

stochastic id119 performs variational id136,

converges to limit cycles for deep networks,

pratik chaudhari and stefano soatto.

arxiv:1710.11029, iclr '18

www.pratikac.info

thank you, questions?

25

