graph representation 
learning with graph 
convolutional networks

jure leskovec

networks: common language

movie 1

actor 1

movie 2

actor 2
movie 3

actor 4

friend

co-worker

mary

peter

brothers

friend

tom

actor 3

albert

protein 1

protein 2

protein 5

protein 9

|n|=4
|e|=4

jure leskovec, stanford university

2

example: node classification

?

?

?

?

?

machine 
learning

many possible ways to create node features:
   node degree, id95 score, motifs,    
   degree of neighbors, id95 of 

neighbors,    

jure leskovec, stanford university

3

machine learning lifecycle

network 

data

node 
features

learning 
algorithm  

model

feature 
engineering

automatically 
learn the features

downstream 
prediction task

(supervised) machine learning lifecycle: 
this feature, that feature. 
every single time!

jure leskovec, stanford university

4

id171 in graphs
this talk: id171 

for networks!

node!:#      &

u

vector

   &

feature representation, 

embedding

jure leskovec, stanford university

5

graphsage: 

graph convolutional 

networks

inductive representation learning on large graphs. 
w. hamilton, r. ying, j. leskovec. neural information processing systems (nips), 2017.
representation learning on graphs: methods and applications. 
w. hamilton, r. ying, j. leskovec. ieee data engineering bulletin, 2017.

jure leskovec, stanford university

6

from images to networks
convolutional neural networks (on grids)
single id98 layer with 3x3 filter:
single id98 layer with 3x3 filter:

(animation by  
vincent dumoulin)

image

transform information at the neighbors and combine it

   transform    messages       " from neighbors: #"	   "
   add them up:    #"	   "
"

graph

jure leskovec, stanford university

7

graph-structured data
graph-structured data

real-world graphs

hidden layer

what if our data looks like this?
what if our data looks like this?

but what if your graphs look like this?

input

input

hidden layer

or this:
or this:

or this:

relu

relu

   
   

   

   examples:

social networks, information networks, 
id13s, communication 
networks, web graph,    

jure leskovec, stanford university

end-to-end learning on graphs with id197s

thomas kipf

8

6

a na  ve approach

a na  ve approach

    take adjacency matrix     and feature matrix   

    concatenate them  

   join adjacency matrix and features
   feed them into a deep neural net:

    feed them into deep (fully connected) neural net 

[a, x]

    done?

a

c

b

d

e

a
b
c
d
e

a    b    c    d    e
feat
0     1     1     1     0          1     0
1     0     0     1     1          0     0
1     0     0     1     0          0     1
1     1     1     0     1          1     1
0     1     0     1     0          1     0

  !(#) parameters

   issues with this idea:

problems:
    huge number of parameters 
    no inductive learning possible

   not applicable to graphs of different sizes
   not invariant to node ordering

jure leskovec, stanford university

end-to-end learning on graphs with id197s

thomas kipf

9

?

id197
   id197:

   problem: for a given subgraph how to 

come with canonical node ordering?

niepert, mathias, mohamed ahmed, and konstantin kutzkov. "learning convolutional neural networks for graphs." icml. 2016. (image source)

jure leskovec, stanford

10

graph-structured data

what if our data looks like this?

input

desiderata
   invariant to node ordering 

   no graph isomorphism problem
   locality     operations depend 

on the neighbors of a given node

   number of model parameters should 

be independent of graph size

   model should be independent of graph 

structure and we should be able to 
transfer the model across graphs

jure leskovec, stanford university

11

graphsage

   adapt the id197 idea to inductive node 

embedding

   generalize beyond simple convolutions
   demonstrate that this generalization
   leads to significant performance 

gains

   allows the model to learn about local 

structures

jure leskovec, stanford

12

idea: graph defines computation

idea: node   s neighborhood defines a 

computation graph
!
!

determine node 
computation graph

propagate and

transform information

learn how to propagate information across 

the graph to compute node features

semi-supervised classification with id197. t. n. kipf, m. welling, iclr 2017

jure leskovec, stanford university

13

our approach: graphsage

w(2)

q(2)

q(1)

w(1)

q(1)

w(1)

   each node defines its own computational 

graph
   each edge in this graph is a 

transformation/aggregation function 

jure leskovec, stanford

14

our approach: graphsage

q(1)

w(1)

w(2)

q(2)

q(1)

w(1)

update for node !:
   #(%&')=*+,- .%   #%, 0 *+,-(1%   2%)
transform 6   s own 
9+1=> level
features of neighbors :
features from level 9
features of node 6
      #5 = attributes of node 6
         : aggregator function (e.g., avg., lstm, max-pooling)

2   4#

transform and aggregate

semi-supervised classification with id197. t. n. kipf, m. welling, iclr 2017

jure leskovec, stanford

15

graphsage algorithm

initialize representations as features

k =    search depth   

aggregate information from neighbors

concatenate neighborhood info with 
current representation and propagate

classification (cross-id178) loss

wl isomorphism test
   the classic weisfeiler-lehman graph 
isomorphism test is a special case of 
graphsage
neural nets:

   we replace the hash function with trainable 

hash

x

x

shervashidze, nino, et al. "weisfeiler-lehman graph kernels." journal of machine learning research (2011).

jure leskovec, stanford

17

graphsage: training

   assume parameter sharing:

w(2)
q(2)

w(2)
q(2)

w(2)
q(2)

w(1)
q(1)

   two types of parameters:

   aggregate function can have params.
   matrix w(k)

   adapt to inductive setting (e.g., unsupervised loss, 

neighborhood sampling, minibatch optimization)

   generalized notion of    aggregating neighborhood   

jure leskovec, stanford university

18

graphsage: benefits

   can use different aggregators !

   mean (simple element-wise mean), lstm (to a random 

order of nodes),  max-pooling (element-wise max)

   can use different id168s:

   cross id178, hinge loss, ranking loss

   model has a constant number of parameters
   fast scalable id136
   can be applied to any node in any network

jure leskovec, stanford university

19

graphsage performance: 

experiments

   compare graphsage to alternative methods

   id28 on features (no network information)
   node2vec, extended node2vec with features
   task: node classification, id21
   citation graph: 302,424 papers from 2000-05

   predict 6 subject codes; train on 2000-04, test on    05

   reddit posts: 232,965 posts, 50 communities, sep    14
   what community does a post belong to? train on first 20 
   protein-protein interaction networks: 24 ppi networks 

days, test on remaining 10 days

from different tissues 

   id21 of protein function: train on 20 networks, 

test on 2

darpa simplex pi meeting, february 6, 2018                      miner project

20

graphsage performance: results

graphsage performs best in all experiments.
achieves ~40% average improvement over raw features.

darpa simplex pi meeting, february 6, 2018                      miner project

21

application: pinterest
human curated collection of pins

pin: a visual bookmark someone 
has saved from the internet to a 
board they   ve created.
pin: image, text, link

board: a greater collection of ideas (pins having sth. in common). 

jure leskovec, stanford university

22

large-scale application

   semi-supervised node embedding for 

graph-based recommendations

   graph: 2b pins, 1b boards, 20b edges

pins

boars

q

jure leskovec, stanford university

23

pinterest graph

q

   graph is dynamic: need to apply to 
new nodes without model retraining
   rich node features: content, image

jure leskovec, stanford university

24

task: item-item recs

related pin recommendations
   given user is looking at pin q, what 

pin x are they going to save next:

query

positive

rnd. negative

hard negative

jure leskovec, stanford university

25

graphsage training
   leverage inductive capability, and 

train on individual subgraphs
   300 million nodes, 1 billion edges, 

1.2 billion pin pairs (q, x)

   large batch size: 2048 per minibatch

jure leskovec, stanford university

26

graphsage: id136

   use mapreduce for

model id136

   avoids repeated computation

jure leskovec, stanford university

27

experiments
related pin recommendations
   given user is looking at pin q, predict 
what pin x are they going to save next 
   baselines for comparison

   visual: vgg-16 visual features
   annotation: id97 model
   combined: combine visual and annotation
   rw: random-walk based algorithm
   graphsage
   setup: embed 2b pins, perform nearest 
neighbor to generate recommendations

jure leskovec, stanford university

28

results: ranking

task: given q, rank x as high as possible 
among 2b pins
   hit-rate: pct. p was among top-k
   mrr: mean reciprocal rank

method
visual

annotation
combined
graphsage

hit-rate

17%
14%
27%
46%

mrr
0.23
0.19
0.37
0.56

jure leskovec, stanford university

29

example recommendations

gs

jure leskovec, stanford university

30

graphsage: summary

   graph convolution networks

   generalize beyond simple convolutions

   fuses node features & graph info
   state-of-the-art accuracy for node 

classification and link prediction.

   model size independent of graph size; 

can scale to billions of nodes
   largest embedding to date (3b nodes, 20b edges)
   leads to significant performance gains

jure leskovec, stanford university

31

how can this technology 
be used for biomedical 

problems?

   two examples:

   pairs of nodes: predicting side-effects 

of drug combinations

   subgraph prediction: predicting which 

drug treats what disease

modeling polypharmacy side effects with id197. m. zitnik, m. 
agrawal, j. leskovec. bioarxiv, 2017.

jure leskovec, stanford university

32

polypharmacy side effects

patient   s medications

patient   s side effects

drug combination

polypharmacy side 

effect
,

polypharmacy side effects

patient   s medications

patient   s side effects

   polypharmacy is common to treat complex 

diseases and co-existing conditions

   high risk of side effects due to interactions
   15% of the u.s. population affected
   annual costs exceed $177 billion
   difficult to identify manually:

s

   rare, occur only in a subset of patients 
   not observed in clinical testing

polypharmacy side 

effect
,

network & indications data
   idea:  construct a heterogeneous graph of 
drugs and proteins
   train: fit a model to predict known 
associations of drug pairs and side effects
   test: given a query drug pair, predict 
candidate polypharmacy side effects

data:
   protein-protein interaction network [menche et al. science 15]

   19k nodes, 350k edges

   drug-protein and disease-protein links:

   9k proteins,  800k drug-protein links

   drug side effects: sider, offsides, twosides

jure leskovec, stanford university

35

heterogeneous graph

   predict labeled edges between drugs

link prediction task

   given a drug pair (",$),	predict how 
likely an edge (",'(,$) exists
   meaning: drug combination (",$)
leads to polypharmacy side effect '(

neural architecture: encoder

graph encoder:
   input: graph, additional 

node features

   output: node embeddings

neural architecture: decoder

graph decoder:
   input: query drug pairs 

and their embeddings
   output: predicted links 

prediction performance

   up to 54% improvement over baselines
   first time to computationally identify side 

effects of drugs

40

how can this technology 
be used for biomedical 

problems?

   two examples:

   pairs of nodes: predicting side-effects 

of drug combinations

   subgraph prediction: predicting which 

drug treats what disease

jure leskovec, stanford university

41

prediction problem

graph convolutional 
drug repurposing

goal: predict which diseases a new drug 

(molecule) could treat

jure leskovec, stanford university

42

insight: networks

   subgraphs of disease-associated proteins 
   subgraphs of drug target proteins

jure leskovec, stanford university

43

a rationale for graphs

a drug is likely to treat a disease if they 
are nearby in    pharmacological space   

[menche et al. science 2015; guney et al. nat commun 2016; hodos et al. systems biology and medicine 2016]

jure leskovec, stanford university

44

link prediction on subgraphs

   drug repurposing: link prediction 
problem on subgraphs
   predict new indications:

   obtain subgraphs by projecting drug and 
   predict links between subgraphs

disease on the graph

jure leskovec, stanford university

45

sugar: message passing

embedding for subgraph !:

jure leskovec, stanford university

46

neural network model

jure leskovec, stanford university

47

network & indications data
   protein-protein interaction network culled from 15 

knowledge databases [menche et al. science 15]
   19k nodes, 350k edges

   drug-protein and disease-protein links:

   drugbank, omim, disgenet, stitch db and others
   5k drugs, 20k diseases
   20k drug-protein links, 560k disease-protein links

   drug medical indications: 

   drugbank, medi-hps, dailymed, repodb and others
   6k drug-disease indications

   side information: molecular pathways, disease 

symptoms, side effects

jure leskovec, stanford university

48

experimental setup
   disease-centric cross-validation
   for each cross-validation fold:

   exclude all indications of test diseases
   use the remaining data to train a model

   query: given a disease, rank all 

drugs based on scores returned by 
the model

jure leskovec, stanford university

49

experimental results

comparison to current state of the art:
   up to 49% improvement over methods for drug 
   up to 172% improvement over methods for scoring 

repurposing
drug-disease pairs

jure leskovec, stanford university

50

integrating side information
including additional biomedical knowledge:

molecular 
pathways

genetics

metabolic 
pathways

jure leskovec, stanford university

51

drug repurposing @ spark

drug
n-acetyl-cysteine
xamoterol
plerixafor
sodium selenite 
ebselen
itraconazole
bestatin
bestatin
ketaprofen
sildenafil 
tacrolimus 
benzamil      
carvedilol 
benserazide
pioglitazone 
sirolimus

disease
cystic fibrosis
neurodegeneration
cancer           
cancer
c difficile
cancer
lymphedema
pulmonary arterial hypertension
lymphedema
lymphatic malformation
pulmonary arterial hypertension
psoriasis
chagas    disease
brca1 cancer
interstitial cystitis
dystrophic epidermolysis bullosa

given c difficile, where 
rank:   14/5000
does ebselen rank among 
rank:   26/5000
rank:   54/5000
all approved drugs?
rank:   36/5000
rank:   10/5000
rank:   26/5000
rank:   11/5000
rank:   16/5000
rank:   28/5000
rank:   26/5000
rank:   46/5000
rank: 114/5000
rank:     9/5000
rank:   41/5000
rank:   13/5000
rank:   46/5000

jure leskovec, stanford university

52

sugar   s predictions

drug
n-acetyl-cysteine
xamoterol
plerixafor
sodium selenite 
ebselen
itraconazole
bestatin
bestatin
ketaprofen
sildenafil 
tacrolimus 
benzamil      
carvedilol 
benserazide
pioglitazone 
sirolimus

disease
cystic fibrosis
neurodegeneration
cancer           
cancer
c difficile
cancer
lymphedema
pulmonary arterial hypertension
lymphedema
lymphatic malformation
pulmonary arterial hypertension
psoriasis
chagas    disease
brca1 cancer
interstitial cystitis
dystrophic epidermolysis bullosa

rank:   14/5000
rank:   26/5000
rank:   54/5000
rank:   36/5000
rank:   10/5000
rank:   26/5000
rank:   11/5000
rank:   16/5000
rank:   28/5000
rank:   26/5000
rank:   46/5000
rank: 114/5000
rank:     9/5000
rank:   41/5000
rank:   13/5000
rank:   46/5000

higher rank is better 
example: sugar predicted ebselen as 10th most likely candidate drug for c difficile

jure leskovec, stanford university

53

conclusion

results from the past 1-2 years have shown:
   representation learning paradigm can be 

extended to graphs

   no feature engineering necessary
   can effectively combine node attribute data 

with the network information

   state-of-the-art results in a number of 

domains/tasks

   use end-to-end training instead of 

multi-stage approaches for better performance

jure leskovec, stanford university

54

conclusion

next steps:
   multimodal & dynamic/evolving settings
   domain-specific adaptations 

(e.g. for recommender systems) 

   graph generation 
   prediction beyond simple parwise edges

   multi-hop edge prediction

   theory

jure leskovec, stanford university

55

phd students

industry partnerships

claire
donnat

mitchell
gordon

david
hallac

emma
pierson

geet
sethi

funding

rex
ying

tim
himabindu
althoff
lakkaraju
post-doctoral fellows

will

hamilton

david
jurgens

marinka
zitnik

michele
catasta

srijan
kumar

research
staff

stephen

bach

peter
kacin

rok
sosic

collaborators
dan jurafsky, linguistics, stanford university
christian danescu-miculescu-mizil, information science, cornell university
stephen boyd, electrical engineering, stanford university
david gleich, computer science, purdue university
vs subrahmanian, computer science, university of maryland
sarah kunz, medicine, harvard university
russ altman, medicine, stanford university
jochen profit, medicine, stanford university
eric horvitz, microsoft research
jon kleinberg, computer science, cornell university
sendhill mullainathan, economics, harvard university
scott delp, bioengineering, stanford university
jens ludwig, harris public policy, university of chicago

jure leskovec, stanford university

56

references

  

node2vec: scalable id171 for networks
a. grover, j. leskovec. kdd 2016.

   predicting multicellular function through multi-layer tissue 

networks. m. zitnik, j. leskovec. bioinformatics, 2017.
inductive representation learning on large graphs. 
w. hamilton, r. ying, j. leskovec. nips 2017

  

   representation learning on graphs: methods and applications. 

w. hamilton, r. ying, j. leskovec.
ieee data engineering bulletin, 2017.

   modeling polypharmacy side effects with graph convolutional 
networks. m. zitnik, m. agrawal, j. leskovec. bioarxiv, 2017.
   code:

   http://snap.stanford.edu/node2vec
   http://snap.stanford.edu/graphsage

jure leskovec, stanford university

57

