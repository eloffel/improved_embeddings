recurrent neural networks

these slides draw on material from geoff hinton   s course notes.
http://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf

1

1

modeling sequence data
    memoryless models for sequences:

this part draws heavily (i.e. verbatim) from  the  course notes (csc2535) of geoff hinton (2013) 

    autoregressive models: predict the next input in a sequence from a 
   xed number of previous inputs using    delay taps   .

wt-2

inputt-2

inputt-1

inputt

wt-1

    feed-forward neural networks: generalize autoregressive models by 
using non-linear hidden layers.

wt-2

wt-1
inputt-1

hiddenst
wt
inputt

inputt-2

wt-1

wt-1

wt-1

2

2

beyond memoryless models

    if the model could possess a dynamic hidden state:

    can store long term information.

    dynamic bayes networks (dbns) can possess internal 
dynamic state
    example 1:  linear dynamic system with 
   

hidden markov model:

gaussian noise model (kalman filter)

    example 2: discrete state, arbirary 

observation type (hidden markov model)
- state is not observed, must be inferred.
- represent id203 across n states with n 

numbers.

- ef   cient algorithms exist for id48 id136.

o
u
t
p
u
t
 
t
-
2

o
u
t
p
u
t
 
t
-
1

o
u
t
p
u
t
 
t

! time  !

3

3

limitations of id48s

    consider modelling sentence production with an id48.
    generation procedure: 

    at each time step t, sample state t given state t-1.
    everything important about the past outputs (output 0, . . . , output t-1) is must 

be summarized in this choice of state

    so with n states, it can only remember log(n) bits of the past.

    consider trying to generate the second half of the sentence 
with the    rst have already generated.
    syntax needs to    t (number and tense agreement).
    semantics need to    t.

    these aspects combined could be say 100 bits that need to 
be conveyed from the    rst half to the second. 2100 is big!

4

4

recurrent neural networks
    id56s are very powerful, because 
they combine two properties:
    distributed hidden state: can ef   ciently 
store a lot of information about the past.
- note: real valued activations not 1-of-n
    non-linear dynamics: can update their 
hidden state in complicated ways.

o
u
t
p
u
t
 
t
-
1

o
u
t
p
u
t
 
t
-
2

o
u
t
p
u
t
 
t

! time  !

i

n
p
u
t
 
t
-
2

i

n
p
u
t
 
t
-
1

i

n
p
u
t
 
t

! time  !

5

5

recurrent neural networks
   compare to: feed forward neural networks:
    information is propagated from the inputs to the outputs
    no notion of    time    necessary

output layer:

2nd hidden layer:

1st hidden layer:

x1

x2

x3

x4

x5

6

recurrent neural networks
    id56s can have arbitrary topology.

    no    xed direction of information    ow

    delays associated with speci   c connections

    every directed cycle must contain a delay. 

    possesses an internal dynamic state. 

x1

x2

x3

x4

x5

7

7

recurrent neural networks
    with directed cycles the network can do more than a feed-
forward network, it has dynamics.
    oscillate: useful for pattern generation, eg. for walking, swimming, etc. 
    settle to a point attractor: can represent semantics, like the meanings of words
    can behave chaotically: can think of this as skipping between stable oscillatory 

patterns. potentially useful for memory encoding and retrieval. 

    with internal state, the network can    remember    things for a 
long time.
    can decide to ignore input for a while if it wants to.
    but it is very hard to train an id56 to store information that   s not needed for a 

long time.

    in principal, the internal state can carry information about a potentially unbounded 
8

number of previous inputs.

8

recurrent neural networks
    how to make sense of recurrent connections.

    assume a unit time delay with each connection
    we can unroll the id56 in time to get a standard feedforward net that reuse 

the same weights at every layer. 

id56:

w 1

w 3

w 2

w 4

unroll

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

9

9

training id56s

    id56s are usually trained using the id26 through 
time algorithm.
    id26 through time: standard backprop through 
the unrolled id56 with the constraint that weights are shared.

time=3

time=2

time=1

time=0

n
o
i
t
a
g
a
p
o
r
p
 
t
u
p
n
i
 
d
r
a
w
r
o
f

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

b
a
c
k
w
a
r
d
 
e
r
r
o
r
 
p
r
o
p
a
g
a
t
i
o
n

10

10

id26 through time
    id26 with weight constraints: compute the 
gradients contributions for each weight at each time step.
    each time step = each layer in the unrolled feed-forward network.
     

let e be the error and w(i)

1 be weight w1 at time = i.

 e
 w1

=

 e
 w(3)
1

+

 e
 w(2)
1

+

 e
 w(1)
1

+

 e
 w(0)
1

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

11

11

id56 initialization

    need to specify the initial activations of the hidden units and 
output units. 
    could initialize them to a    xed value (such as 0.5).
    better to treat the initial state as  learned parameters.
    learn these the same way we do with other model 
parameters:
    start off with random guesses of the initial state values.
    backpropagate the prediction error through time all the way to the initial state 
values and compute the gradient of the error with respect to these initial state 
parameters.

    update these parameters by following the negative gradient.

12

12

input signals for id56s

    we can specify inputs in several ways:

    specify the initial states of all units.
    specify the initial states of a subset of units.
    specify the desired activations for a subset of 

units for multiple time steps.
- useful when modelling a sequential task.
-

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

13

13

input signals for id56s

    we can specify inputs in several ways:

    specify the initial states of all units.
    specify the initial states of a subset of units.
    specify the desired activations for a subset of 

units for multiple time steps.
- useful when modelling a sequential task.
-

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

13

13

training signals for id56s

    we can specify the target for training an id56 in a few 
different (task dependent) ways:
    specify the desired    nal activations of all units.
    specify the desired activations for all units for multiple time steps.

- useful when learning attractors.

    specify the desired activity of a subset of units.

- other units are input or hidden units.

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

14

14

training signals for id56s

    we can specify the target for training an id56 in a few 
different (task dependent) ways:
    specify the desired    nal activations of all units.
    specify the desired activations for all units for multiple time steps.

- useful when learning attractors.

    specify the desired activity of a subset of units.

- other units are input or hidden units.

time=3

time=2

time=1

time=0

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

w 1

w 3

w 2

w 4

14

14

example: binary addition
    can we train a feed-forward net to do binary addition? 
    problems:

    we must decide in advance the maximum number of digits in each number.
    the processing applied to the beginning of a long number does not generalize to 

the end of the long number because it uses different weights.

    feed-forward nets do not generalize well on binary addition.

11001100

hidden units

00100110

10100110

15

15

binary addition as a dynamic process

0 0 1 1 0 1 0 0
0 1 0 0 1 1 0 1
1 0 0 0 0 0 0 1

time

1
0

0
1

no carry 
print 1

1
0

0
1

0
0

0
0

no carry 
print 0

0
0

0
0

1
1

carry 
print 1

1
1

1
1

1
1

1
0

0
1

carry 
print 0

0
1

1
0

    moves from right to left over two input numbers.
    finite state atomaton

    decides what transition to make by looking at the next column.
    it prints after making the transition.

16

16

id56 for binary addition

    network has two input units and one output unit.
    each input corresponds to a digit from a distinct binary number. 

    desired output at each time step is the output 
for the column that was provided as input two 
time steps ago.
    takes one time step to update the hidden units based on the 

two input digits.

0 0 1 1 0 1 0 0
0 1 0 0 1 1 0 1
1 0 0 0 0 0 0 1

time

    takes another time step for the hidden units to cause the 

output.

17

17

id56 for binary addition

    id56 solution has 3 hidden units have 
all possible interconnections in all 
directions:
    allows hidden activity pattern at one time step to 

vote for the hidden activity pattern at the next 
time step.

    input units have feedforward 
connections that allow them to vote 
for the next hidden activity pattern.

x1

x2

18

18

id56 for binary addition

what the network learns:
    learns 4 distinct patterns of activity across the 3 hidden units.
    these patterns correspond to the nodes in the    nite state 
automaton.
    don   t confuse hidden units with states in the    nite state automaton.
    the automaton is restricted to be in exactly one state at each time. the hidden 

units are restricted to have exactly one vector of activity at each time. 
    an id56 can emulate a    nite state automaton, but it is 
exponentially more powerful.
    with n hidden neurons it has 2n possible binary activity vectors in the hidden units.
    important when the input stream has several separate things going on at once.

19

19

lots of promise, but ...

    recurrent neural networks are an extremely powerful class of 
model.
    unfortunately, it is very dif   cult to learn long-term 
dependencies in a recurrent network.
    exploding and vanishing gradients in id26 trough time is a huge issue 

that remains somewhat unresolved. 

20

20

exploding / vanishing gradient

    forward pass made stable by saturating 
nonlinearities, i.e. sigmoid activations
    forward pass determines the slopes of the linear 
function used for id26.

    backprop gradient is unbounded. 

    backward pass is completely linear.

-

if you double the error at the    nal layer, all the error 
derivatives will be double.

    can either shrink (stable) or explode (unstable).
if the weights are small, gradients shrink exponentially.
if the weights are large, gradients grow exponentially.

-
-

21

21

exploding / vanishing gradient

    understanding exploding / vanishing gradients:

    if we start a trajectory within an attractor, small changes in where we 
start make no difference to where we end up.
    but if we start close to the boundary, tiny changes can result in huge 
difference.

22

22

solutions for training id56s

    sophisticated optimization: 

    deal with the vanishing gradients problem by using a fancy optimizer that can 

detect directions with a tiny gradient but even smaller curvature. (eg. hessian-free 
optimization).

    echo state networks:

    initialize the input     hidden and hidden     hidden connections very carefully so 
that the hidden state has a huge reservoir of weakly coupled oscillators which can 
be selectively driven by the input.

    only learns the hidden     output connections.
    long short term memory (lstm):

    make the id56 out of modules that are designed to remember values for a long 

time.

    the lstm has recently become the id56 model of choice for many applications.

23

23

long short term memory networks
    hochreiter & schmidhuber (1997) solved the problem of 
getting an rmm to remember things for a long time           
(like hundreds of time steps).

    they designed a memory cell using logistic and linear units 
with gating (or multiplicative) interactions:
    write (input) gate: (on) information can get into the cell.
    keep (forget) gate: (on) information stays in the cell.
    read (output) gate: (on) information can be read by the cell.

24

24

index terms    recurrent neural networks, deep neural

id137

(y1, . . . , yt ) by iterating the following equations from t = 1
to t :

1. introduction

xt, ht   1

neural networks have a long history in id103,
usually in combination with id48 [1, 2].
they have gained attention in recent years with the dramatic
improvements in acoustic modelling yielded by deep feed-
forward networks [3, 4]. given that speech is an inherently
dynamic process, it seems natural to consider recurrent neu-
ral networks (id56s) as an alternative model. id48-id56
systems [5] have also seen a recent revival [6, 7], but do not
currently perform as well as deep networks.

,
t
x

1
   

t
h

instead of combining id56s with id48s, it is possible
to train id56s    end-to-end    for id103 [8, 9, 10].
this approach exploits the larger state-space and richer dy-
namics of id56s compared to id48s, and avoids the prob-
lem of using potentially incorrect alignments as training tar-
gets. the combination of long short-term memory [11], an
id56 architecture with an improved memory, with end-to-end
training has proved especially effective for cursive handwrit-
ing recognition [12, 13]. however it has so far made little

xt, ht   1

ht = h (wxhxt + whhht 1 + bh)
yt = whyht + by

xt, ht   1

long short-term memory cell

a crucial element of the recent success of hybrid id48-
neural network systems is the use of deep architectures, which
are able to build up progressively higher level representations
of acoustic data. deep id56s can be created by stacking mul-
tiple id56 hidden layers on top of each other, with the out-
put sequence of one layer forming the input sequence for the
next. assuming the same hidden layer function is used for
all n layers in the stack, the hidden vector sequences hn are
iteratively computed from n = 1 to n and t = 1 to t :

where the w terms denote weight matrices (e.g. wxh is the
input-hidden weight matrix), the b terms denote bias vectors
(e.g. bh is hidden bias vector) and h is the hidden layer func-
tion.
h is usually an elementwise application of a sigmoid
function. however we have found that the long short-term
memory (lstm) architecture [11], which uses purpose-built
memory cells to store information, is better at    nding and ex-
ploiting long range context. fig. 1 illustrates a single lstm
memory cell. for the version of lstm used in this paper [14]
h is implemented by the following composite function:

t = h whn 1hnhn 1

where we de   ne h0 = x. the network outputs yt are

hn

it =   (wxixt + whiht 1 + wcict 1 + bi)
ft =   (wxf xt + whf ht 1 + wcf ct 1 + bf )
ct = ftct 1 + it tanh (wxcxt + whcht 1 + bc)
ot =   (wxoxt + whoht 1 + wcoct + bo)
ht = ot tanh(ct)

yt = whn yhn

where   is the logistic sigmoid function, and i, f, o and c

25

deep bidirectional id56s can be implemented by replacing
each hidden sequence hn with the forward and backward se-
quences  !h n and   h n, and ensuring that every hidden layer

25

lstm cell equations:

fig. 1. long short-term memory cell

id137

    lstm has been around for a while (>15 years)

    long term memory properties have also been known for a while, but ...
    recently, lstm has emerged as the recurrent neural network 
of choice with state-of-the-art performance in: 
    cursive handwriting recognition: graves & schmidhuber   s (2009) 
lstm model achieved state-of-the-art performance for reading cursive 
writing.
    id103:  (among others) graves, mohamed & hinton 
(2013) showed state-of-the-art performance on timit phoneme 
recognition task.
    machine translation (mt): sutskever,  vinyals & le (2014) report near 
state-of-the-art results with an lstm-based, end-to-end mt system.

26

26

id103

    when we last discussed id103 we saw that deep 
learning methods (i.e. neural networks) are now the dominate 
paradigm for the acoustic model.

    the dream is a full deep learning system complete with end-to-
end training of the whole pipeline. 

    for this we can to replace the id48 language model with a 
model of word sequences: we can use lstm id56!

27

27

id103

    when we last discussed id103 we saw that deep 
learning methods (i.e. neural networks) are now the dominate 
paradigm for the acoustic model.

dsp

deep neural network

acoustic model

language 
model

    the dream is a full deep learning system complete with end-to-
end training of the whole pipeline. 

    for this we can to replace the id48 language model with a 
model of word sequences: we can use lstm id56!

27

27

id103

    when we last discussed id103 we saw that deep 
learning methods (i.e. neural networks) are now the dominate 
paradigm for the acoustic model.

dsp

deep neural network

acoustic model

language 
model

    the dream is a full deep learning system complete with end-to-
end training of the whole pipeline. 

dsp

deep neural network

27

27

id103

    when we last discussed id103 we saw that deep 
learning methods (i.e. neural networks) are now the dominate 
paradigm for the acoustic model.

dsp

deep neural network

acoustic model

language 
model

    the dream is a full deep learning system complete with end-to-
end training of the whole pipeline. 

dsp

deep neural network

    for this we can to replace the id48 language model with a 
model of word sequences: we can use lstm id56!

27

27

lstm for id103

next. assuming the same hidden layer function is used for
all n layers in the stack, the hidden vector sequences hn are
iteratively computed from n = 1 to n and t = 1 to t :

    graves, mohamed & hinton (2013) used a bidirectional lstm to 

incorporate both previous and future contextual information to predict 
the sequence of phonemes from the sequence of utterances. 

where we de   ne h0 = x. the network outputs yt are

vector only receives input from element m of the cell vector.
one shortcoming of conventional id56s is that they are
only able to make use of previous context. in speech recog-
nition, where whole utterances are transcribed at once, there
is no reason not to exploit future context as well. bidirec-
tional id56s (bid56s) [15] do this by processing the data in
both directions with two separate hidden layers, which are
then fed forwards to the same output layer. as illustrated in
fig. 2, a bid56 computes the forward hidden sequence  !h ,
the backward hidden sequence   h and the output sequence y
by iterating the backward layer from t = t to 1, the forward
layer from t = 1 to t and then updating the output layer:

t = h whn 1hnhn 1

hn

yt = whn yhn

fig. 1. long short-term memory cell

    h t = h   wx    h xt + w    h     h     h t 1 + b    h   
    h t = h   wx    h xt + w    h     h     h t+1 + b    h   
yt = w    h y    h t + w    h y    h t + by

deep bidirectional id56s can be implemented by replacing
each hidden sequence hn with the forward and backward se-
quences  !h n and   h n, and ensuring that every hidden layer
receives input from both the forward and backward layers at
the level below. if lstm is used for the hidden layers we get
deep bidirectional lstm, the main architecture used in this
paper. as far as we are aware this is the    rst time deep lstm
has been applied to id103, and we    nd that it
yields a dramatic improvement over single-layer lstm.

combing bid56s with lstm gives bidirectional lstm [16],
which can access long-range context in both input directions.

fig. 2. bidirectional id56

3. network training

we focus on end-to-end training, where id56s learn to map
directly from acoustic to phonetic sequences. one advantage

28

28

lstm for id103

tends to    simplify    neural networks, in the sense of reducing
the amount of information required to transmit the parame-
ters [23, 24], which improves generalisation.

    graves, mohamed & hinton (2013) used a bidirectional lstm to 

incorporate both previous and future contextual information to predict 
the sequence of phonemes from the sequence of utterances. {

table 1. timit phoneme recognition results.    epochs    is
the number of passes through the training set before conver-
gence.    per    is the phoneme error rate on the core test set.

phoneme recognition experiments were performed on the
timit corpus [25]. the standard 462 speaker set with all
sa records removed was used for training, and a separate
development set of 50 speakers was used for early stop-
ping. results are reported for the 24-speaker core test set.
the audio data was encoded using a fourier-transform-based
   lter-bank with 40 coef   cients (plus energy) distributed on
a mel-scale, together with their    rst and second temporal
derivatives. each input vector was therefore size 123. the
data were normalised so that every element of the input vec-

tanh id56 (cid:15482)

{lstms

network
ctc-3l-500h-tanh
ctc-1l-250h
ctc-1l-622h
ctc-2l-250h
ctc-3l-421h-uni
ctc-3l-250h
ctc-5l-250h
trans-3l-250h
pretrans-3l-250h

weights
3.7m
0.8m
3.8m
2.3m
3.8m
3.8m
6.8m
4.3m
4.3m

epochs
107
82
87
55
115
124
150
112
144

per
37.6%
23.9%
23.0%
21.0%
19.6%
18.6%
18.4%
18.3%
17.7%

29

29

ping. results are reported for the 24-speaker core test set.
the audio data was encoded using a fourier-transform-based
   lter-bank with 40 coef   cients (plus energy) distributed on
a mel-scale, together with their    rst and second temporal
derivatives. each input vector was therefore size 123. the
data were normalised so that every element of the input vec-
tors had zero mean and unit variance over the training set. all
61 phoneme labels were used during training and decoding
(so k = 61), then mapped to 39 classes for scoring [26].
note that all experiments were run only once, so the vari-
ance due to random weight initialisation and weight noise is

lstm for id103

ctc-5l-250h
trans-3l-250h
pretrans-3l-250h

18.4%
18.3%
17.7%

6.8m
4.3m
4.3m

150
112
144

graves, mohamed & hinton (2013)

as shown in table 1, nine id56s were evaluated, vary-
ing along three main dimensions: the training method used
(ctc, transducer or pretrained transducer), the number of
hidden levels (1   5), and the number of lstm cells in each
hidden layer. bidirectional lstm was used for all networks
except ctc-3l-500h-tanh, which had tanh units instead of
lstm cells, and ctc-3l-421h-uni where the lstm layers
were unidirectional. all networks were trained using stochas-
tic id119, with learning rate 10 4, momentum 0.9
and random initial weights drawn uniformly from [ 0.1, 0.1].

    input sensitivity of a deep ctc id56.  the heatmap (top) shows the derivatives of 
the    ah    and    p    outputs printed in red with respect to the    lterbank inputs (bottom).

    note that the sensitivity extends to surrounding segments; this may be because ctc 

(which lacks an explicit language model) attempts to learn linguistic dependencies 
from the acoustic data.

fig. 3. input sensitivity of a deep ctc id56. the heatmap
(top) shows the derivatives of the    ah    and    p    outputs printed
in red with respect to the    lterbank inputs (bottom). the
timit ground truth segmentation is shown below. note that
the sensitivity extends to surrounding segments; this may be

30

30

deep learning for id103

this section draws on the tutorials of vincent vanhoucke 
(icml 2013) and li deng (icml 2014) 

31

31

id103
    goal: transform speech (audio) input to text output

    this is a structured output problem!

    id103 task combines an acoustic model with a 
language model:

audio waveform

language model

argmax

w

p(w | o) = argmax

w

p(o | w )

word sequence

p(w )

$#$!"

!

"#

acoustic model

32

32

id103

    1990s-2009: the traditional pipeline

dsp

feature 
extraction

acoustic 
model

language 
model

    feature extraction:

    mel-frequency cepstral coef   cients (mfccs), perceptual linear prediction (plps), 

bottleneck features
    acoustic model:

    gaussian mixture models (diagonal covariances)

    language model:

    id48

33

33

id103

34

34

id103

    1990s-2009: the traditional pipeline

dsp

feature 
extraction

acoustic 
model

language 
model

34

34

id103

    1990s-2009: the traditional pipeline

dsp

feature 
extraction

acoustic 
model

language 
model

    2009-2014: the deep neural network pipeline.

dsp

deep neural network

acoustic model

language 
model

34

34

id103

    1990s-2009: the traditional pipeline

dsp

feature 
extraction

acoustic 
model

language 
model

    2009-2014: the deep neural network pipeline.

dsp

deep neural network

acoustic model

language 
model

     >2014 full dnn end-to-end system?

dsp

deep neural network

34

34

dl for id103

image from li deng, 2014

year

35

35

dl for id103

    no improvement for 10+ 

years of research.

image from li deng, 2014

year

35

35

dl for id103

    no improvement for 10+ 

years of research.

    use of deep learning 

(dbns) quickly reduced 
the error from ~23% to 
<15% 

image from li deng, 2014

year

35

35

dl for id103

    no improvement for 10+ 

years of research.

    use of deep learning 

(dbns) quickly reduced 
the error from ~23% to 
<15% 

    early results used dbns 
with greedy layer-wise 
unsupervised pretraining.

image from li deng, 2014

year

35

35

dl for id103

    no improvement for 10+ 

years of research.

    use of deep learning 

(dbns) quickly reduced 
the error from ~23% to 
<15% 

    early results used dbns 
with greedy layer-wise 
unsupervised pretraining.
    recently, unsupervised 

pretraining appears to be 
irrelevant.

image from li deng, 2014

year

35

35

dl for id103

an industry-wide revolution:

microsoft: 

ibm:

google:

u of toronto:

li deng, frank seide, dong yu, ...
brian kingsbury, tara sainath, ...
vincent vanhoucke, andrew senior, georg heigold, ...
goeff hinton, george dahl, abdel-rahman mohamed,  
navdeep jaitly

36

36

deep learning recipe: hybrid system

dsp

deep neural network

acoustic model

language 
model

audio

frame

state

text

    to combine with language model, need a model of

p(frame | state)

37

37

deep learning recipe: hybrid system

dsp

deep neural network

acoustic model

language 
model

audio

frame

state

text

    to combine with language model, need a model of

p(frame | state)

    can exploit baye   s rule:

p(frame | state)    

p(state | frame)

p(state)

37

37

deep learning recipe: hybrid system

dsp

deep neural network

acoustic model

language 
model

audio

frame

state

text

    to combine with language model, need a model of

p(frame | state)

    can exploit baye   s rule:

p(frame | state)    

p(state | frame)

p(state)

    plug any discriminative classi   er into 

p(state | frame)

37

37

deep learning recipe: hybrid system

dsp

deep neural network

acoustic model

language 
model

audio

frame

state

text

    to combine with language model, need a model of

p(frame | state)

    can exploit baye   s rule:

p(frame | state)    

p(state | frame)

p(state)

    plug any discriminative classi   er into 

p(state | frame)

idea: put a deep neural network here

37

37

dnn training
deep neural network

acoustic model

language 
model

dsp

frame

audio
    input: simple scaled spectrogram features
    model details:

    4-10 fully-connected layers, 1000-3000 units / layer.
    linear recti   ed activations / maxout, and softmax output.

state

text

    frame-by-frame training of dnn:

    with minibatch sgd. currently the dominate training paradigm. 
    trained simply to classify state values from frame-level audio data. 

    sequence based discriminative training:

    de   ne a smooth loss that takes into account work, phoneme or state-level 

errors. (kingsbury, 2009)

38

38

speaker adaptation

    hank liao, speaker adaptation of context dependent deep neural 
networks,  icassp 2013
    improvements in generalization due to dl seems to overwhelm gains due to 

speaker adaptation

    adaptation still works well with small networks but vanish as the networks grow.

number of parameters

<10m

31m

45m

60m

relative wer 
improvement

sources (icassp 2013)

32%, 10%

h. liao, o. abdel-hamid

15%

7%

5%

d. yu

d. yu

h. liao

39

39

multilingual id103

    id21 and multitask learning work very well.
    georg heigold  et al. multilingual acoustic models using distributed deep neural networks,  icassp 2013.
    j.-t. huang et. al. cross-language knowledge transfer using multilingual deep neural network with shared hidden 

layers, icassp 2013.
standard approach
language 1 text
language 4 text

language 1 audio
language 2 audio
input layer: window of acoustic feature frames

40

40

multilingual id103

    id21 and multitask learning work very well.
    georg heigold  et al. multilingual acoustic models using distributed deep neural networks,  icassp 2013.
    j.-t. huang et. al. cross-language knowledge transfer using multilingual deep neural network with shared hidden 

layers, icassp 2013.
standard approach
language 1 text
language 4 text

language 1 text

language 2 text

language 3 text

language 4 text

many 
hidden 
layers

shared feature 
representation

language 1 audio
language 2 audio
input layer: window of acoustic feature frames

lang. 1

lang. 2

lang. 3

lang. 4

input layer: window of acoustic feature frames

40

40

multilingual id103

    id21 and multitask learning work very well.
    georg heigold  et al. multilingual acoustic models using distributed deep neural networks,  icassp 2013.
    j.-t. huang et. al. cross-language knowledge transfer using multilingual deep neural network with shared hidden 

layers, icassp 2013.

language 1 text

language 2 text

language 3 text

language 4 text

chinese id103 character-error-rate 
when combined with transfer from european languages.

50"

45"

40"

35"

30"

25"

20"

15"

10"

5"

0"

image from li deng, 2014

1"

10"

100"

shared feature 
representation

many 
hidden 
layers

hours of 
training 
data

1000"

baseline"/"chn"only"

shl/mdnn"model"transfer"

rela>ve"cer"reduc>on"

lang. 1

lang. 2

lang. 3

lang. 4

target language: zh-cn
non-native source langs.: fra: 138h, deu: 195h, esp: 63h and ita: 93h of speech.

input layer: window of acoustic feature frames

41

41

convolutional nets for speech
convolutional networks
    t. n. sainath, et al. deep convolutional neural networks for lvcsr, icassp 2013.
    on a mel scale, a pitch change is mostly a shift in the frequency axis.
    convolutions in frequency seem like a natural way to represent invariance.
    most signi!cant reported improvement over basic dnns so far.
    convolutions in frequency seem like a natural way to represent this 

    insight exploited: on a mel scale, a pitch change is mostly a shift in the 

frequency axis. 

invariance.

    report signi   cant improvement over basic dnn!

image from vincent vanhoucke, 2013

42

42

id103

    1990s-2009: the traditional pipeline

dsp

feature 
extraction

acoustic 
model

language 
model

    2009-2014: the deep neural network pipeline.

dsp

deep neural network

acoustic model

language 
model

     >2014 full dnn end-to-end system?

dsp

deep neural network

43

43

