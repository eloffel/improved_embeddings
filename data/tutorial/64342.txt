sparse modeling  

in 

image processing and deep learning  

michael elad 

 
computer science department  
the technion - israel institute of technology 
haifa 32000, israel 
 

new deep learning techniques 
february 5-9, 2018 

the research leading to these results has been received funding 
from the european union's seventh framework program 
(fp/2007-2013) erc grant agreement erc-sparse- 320649 

   this lecture   

sparseland 

sparse 

representation 

theory 

csc 

convolutional 

sparse  
coding 

ml-csc   
multi-layered 
convolutional 
sparse coding 

  sparsity-inspired models 

deep-learning 

another underlying idea that will accompany us 

  

generative modeling of data sources enables  
o a systematic algorithm development, &   
o a theoretical analysis of their performance  

michael elad 
the computer-science department 
the technion 

 

2 

multi-layered convolutional  

sparse modeling 

 

michael elad 
the computer-science department 
the technion 

 

3 

   our data is structured 

 

stock market

 

text documents

 

biological signals

 

matrix data

 

     social networks

 

still images

 

seismic data

 

videos

 

radar imaging

o we are surrounded by various diverse 

sources of massive information 

o each of these sources have an internal 

structure, which can be exploited 

 

traffic info

o this structure, when identified, is the  

 

voice signals

engine behind our ability to process this data 

 

medical imaging

michael elad 
the computer-science department 
the technion 

 

 

3d objects

4 

   models 

o a model: a mathematical  

description of the underlying  
signal of interest, describing our 
beliefs regarding its structure 

o the following is a partial list of  

commonly used models for images 

principal-component-analysis 

   gaussian-mixture 

markov random field 

   laplacian smoothness 

dct concentration 

   wavelet sparsity 

o good models should be simple while 

piece-wise-smoothness 

matching the signals 
 

 

 

simplicity 

 

 

reliability 

 

o models are almost always imperfect 

   c2-smoothness 

besov-spaces 

   total-variation 

beltrami-flow 

michael elad 
the computer-science department 
the technion 

 

5 

   what this talk is all about?  

data models and their use 
o almost any task in data processing requires a model      

true for denoising, deblurring, super-resolution, inpainting, 
compression, anomaly-detection, sampling, recognition, 
separation, and more 

o sparse and redundant representations offer a new and 

highly effective model     we call it  
                                        sparseland  

o we shall describe this and descendant versions of it that 

lead all the way to     deep-learning 

michael elad 
the computer-science department 
the technion 

 

6 

multi-layered convolutional  

sparse modeling 

 

michael elad 
the computer-science department 
the technion 

 

7 

   a new emerging model 

signal   

processing 

wavelet 
theory 

multi-scale 

analysis 

signal 

transforms 

machine 
learning 

sparseland 

 

mathematics 

approximation 

theory 

linear  
algebra 

optimization 

theory 

semi-supervised 

interpolation 

learning 

compression 

id136 (solving 
inverse problems) 

source-

separation 

segmentation 

classification 

sensor-fusion 

summarizing 

prediction 

denoising 

recognition 

michael elad 
the computer-science department 
the technion 

 

id91 

identification 

anomaly 
detection 

synthesis 

8 

   the sparseland  model 

o task: model image patches of                                               

   

size 8  8 pixels 

o we assume that a dictionary of  

such image patches is given,  
containing 256 atom images 

  1 

  2 

  3 

o the sparseland  model assumption:                          
every image patch can be                                              
described as a linear                                    
combination of few atoms 

michael elad 
the computer-science department 
the technion 

 

9 

   the sparseland  model 

properties of this model:                      
        sparsity and redundancy 

   

o we start with a 8-by-8 pixels patch and 

  1 

  2 

  3 

represent it using 256 numbers         
       this is a redundant representation 

o however, out of those 256 elements in the 

representation, only 3 are non-zeros  
        this is a sparse representation 

o bottom line in this case: 64 numbers 

representing the patch are replaced by 6  
(3 for the indices of the non-zeros, and 3  
for their entries) 

michael elad 
the computer-science department 
the technion 

 

10 

   chemistry of data 

we could refer to the sparseland   
model as the chemistry of information: 

   

o our dictionary stands for the periodic table 

  1 

  2 

  3 

containing all the elements 

o our model follows a similar rationale:                                            

every molecule is built of few elements 

michael elad 
the computer-science department 
the technion 

 

11 

   sparseland : a formal description 

m 

m    

o every column in      

(dictionary) is a  
prototype signal (atom) 

n 

a dictionary 

     

x 

a sparse  
vector 

n 

o the vector     is 

generated  
with few non-
zeros at arbitrary 
locations and  
values 

   

o this is a generative model 

that describes how (we 
believe) signals are created 

michael elad 
the computer-science department 
the technion 

 

12 

      difficulties with sparseland 

o problem 1: given a signal, how                            

can we find its atom decomposition? 

o a simple example:  

   

  1 

  2 

  3 

    there are 2000 atoms in the dictionary 

    the signal is known to be built of 15 atoms 
 

                                                       possibilities  

 

   

if each of these takes 1nano-sec to test,                                      this 
will take ~7.5e20 years to finish !!!!!!  

o so, are we stuck?  

michael elad 
the computer-science department 
the technion 

 

13 

20002.4e3715                           atom decomposition made formal 

min       0  s. t.  x =        

min       0  s. t.             y 2        

approximation algorithms 

n 

     

m 

x 

   

relaxation methods 

greedy methods 

basis-pursuit 

thresholding/omp 

michael elad 
the computer-science department 
the technion 

 

    l0     counting number of 
non-zeros in the vector 

    this is a projection onto  

the sparseland model 

    these problems are known 

to be np-hard problem 

14 

      pursuit algorithms  

min       0  s. t.             y 2        

approximation algorithms 

basis pursuit 

matching pursuit 

thresholding 

 

 

 find the support greedily, 

one element at a time 

change the l0 into l1  
and then  the problem 
becomes convex and 
manageable  

min       1   

s. t.  

                  y 2        

 

multiply y by           
and apply shrinkage: 

    =                  y   

michael elad 
the computer-science department 
the technion 

 

15 

      difficulties with sparseland 

o there are various pursuit algorithms 
o here is an example using the basis pursuit (l1): 

 

 

 

 

 

 

   

  1 

  2 

  3 

o surprising fact: many of these algorithms are often  

accompanied by theoretical guarantees for their  
success, if the unknown is sparse enough 

michael elad 
the computer-science department 
the technion 

 

16 

   the mutual coherence 

o compute 

    t 

= 

     
assume 

normalized 

columns 

    t     

o the mutual coherence          is the largest off-diagonal  

entry in absolute value 

o we will pose all the theoretical results in this talk using  

this property, due to its simplicity 

o you may have heard of other ways to characterize the 

dictionary (restricted isometry property - rip, exact  
recovery condition - erc, babel function, spark,    ) 

michael elad 
the computer-science department 
the technion 

 

17 

   basis-pursuit success  

theorem: given a noisy signal y =        + v where  v 2        
and    is sufficiently sparse,   
 
then basis-pursuit:   min        1   s. t.              y 2        
leads to a stable result:              2

          <

2    

     +

    
    

    
    

4    2

 

 

1      4    0   1

donoho, elad & temlyakov (   06) 

     

m 

   

x 

y 

+ 

v 2        

min        1  
min        0  

s. t.  
s. t.  

           y 2         
           y 2         

michael elad 
the computer-science department 
the technion 

 

    

comments:  
o if    =0         =    
o this is a worst-case 

analysis     better 
bounds exist  

o similar theorems 

exist for many other 
pursuit algorithms 

18 

   difficulties with sparseland 

o problem 2: given a family of signals, how do                      

we find the dictionary to represent it well? 

o solution: learn! gather a large set of                                
  2 

  1 

   

  3 

signals (many thousands), and find the                                                          
dictionary that sparsifies them 

o such algorithms were developed in the                               

past 10 years (e.g., k-svd), and their                          
performance is surprisingly good 

o we will not discuss this matter further  

in this talk due to lack of time 

michael elad 
the computer-science department 
the technion 

 

19 

   difficulties with sparseland 

o problem 3: why is this model suitable to                   

describe various sources? e.g., is it good 
for images? audio? stocks?      

  1 

o general answer: yes, this model is                                

   

  2 

  3 

extremely effective in representing                                    
various sources 

    theoretical answer: clear connection  

to other models 

    empirical answer:  in a large variety of signal  

and image processing (and later machine  
learning), this model has been shown to lead  
to state-of-the-art results 

michael elad 
the computer-science department 
the technion 

 

20 

   difficulties with sparseland ? 

o problem 1: given an image patch, how   

can we find its atom decomposition ? 

   

  1 

  2 

  3 

o problem 2: given a family of signals,                                    
how do we find the dictionary to                                        
represent it well? 

o problem 3: is this model flexible                                      

enough to describe various sources?                                   
e.g., is it good for images? audio?      

michael elad 
the computer-science department 
the technion 

 

21 

 a new massive open online course  

michael elad 
the computer-science department 
the technion 

 

22 

   sparseland  for image processing 

o when handling images, sparseland  is typically deployed on small 
overlapping patches due to the desire to train the model to fit the 
data better 

 
 
 

o the model assumption is: each patch in the image is believed to 

have a sparse representation w.r.t. a common local dictionary 

o what is the corresponding global model? this brings us to     the 

convolutional sparse coding (csc)  

michael elad 
the computer-science department 
the technion 

 

23 

multi-layered convolutional  

sparse modeling 

 

joint work with 

michael elad 
the computer-science department 
the technion 

 

yaniv romano 

vardan papyan 

jeremias sulam 

24 

 

convolutional sparse coding (csc) 

i-th feature-map:   
an image of the 
same size as       
holding the sparse 
representation 
related to the i-filter 

     filters convolved with their 

sparse representations  

an image 
with      
pixels 

    

[    ] =   di

    [  i] 

i=1

the i-th filter of  
small size      

michael elad 
the computer-science department 
the technion 

 

29 

 

csc in matrix form

o here is an alternative global sparsity-based model formulation 

 

 

    

     =       i    i

 

=

i=1

    1                    1
   
        

=          

o     i                   is a banded and circulant  

matrix containing a single atom  
with all of its shifts 
 
 

    i = 
o     i             are the corresponding coefficients  

     

ordered as column vectors 

michael elad 
the computer-science department 
the technion 

 

     

30 

 

the csc dictionary

    1     2     3 = 

    l 

     = 

     

     

michael elad 
the computer-science department 
the technion 

 

31 

 

why csc?

(2         1)     

(2         1)     

     

     

 

stripe-dictionary

 

stripe vector

    i     
    i+1     

= 

     =          

every patch has a sparse 
representation w.r.t. to the 
same local dictionary (    ) just 

as assumed for images 

    i     =         i 

    i+1     =         i+1 

michael elad 
the computer-science department 
the technion 

 

    i     i+1 

32 

 

classical sparse theory for csc ? 

min

    

         0   s. t.                   2        

theorem: bp is guaranteed to    succeed       . if             <

    
    

     +

    
    

 

o assuming that      = 2 and      = 64 we have that [welch,    74] 

 

 

       0.063 

o success of pursuits is guaranteed as long as 

               0 <

1 +

0.063
o only few (4) non-zeros globally are  

  (    )

1

1 +

   

1
2

1
4

1

    4.2 

allowed!!! this is a very pessimistic result! 

michael elad 
the computer-science department 
the technion 

 

33 

 

 

 

 

 

 

 

moving to local sparsity: stripes  

   0,    norm:         0,   

s = max

i

       i 0 

     = 2 

min

    

s

        0,   

  s. t.                     2        

s

 is low     all      i are sparse     every 
        0,   
patch has a sparse representation over      

    i+1 

    i 

the main question we aim to address is this:  

 

can we generalize the vast theory of sparseland to this  
new notion of local sparsity? for example, could we  
provide guarantees for success for pursuit algorithms? 

michael elad 
the computer-science department 
the technion 

 

     

34 

success of the basis pursuit  

  bp = min

  

   

1
2

y     d   2

2 +       1 

local noise 
(per patch)  

theorem: for y = d   + e, if    = 4 e 2,   

p  , if  

     <

         ,   

    
    

     +

    
         

 

then basis pursuit performs very-well: 
1.

 the support of   bp is contained in that of    
p  
    bp                7.5 e 2,   
 every entry greater than 7.5 e 2,   
   bp is unique 

p  is found 

2.

3.

4.

michael elad 
the computer-science department 
the technion 

 

papyan, sulam 
& elad (   17) 

36 

multi-layered convolutional  

sparse modeling 

 

michael elad 
the computer-science department 
the technion 

 

40 

 

quick recall: the forward pass

         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
         ,     i ,     i = relu     2 +     2
           = relu     2 +     2

t relu     1 +     1
t relu     1 +     1
t relu     1 +     1
t relu     1 +     1
t relu     1 +     1
t relu     1 +     1
t relu     1 +     1
t relu     1 +     1

t      
t      
t      
t      
t      
t      
t      
t      

    2                2 

    2                2 

t                2          1 

    2

    1    1 

    2 

    1                1 

    1

t                1       

    0 

    1 

                 

relu 

relu 

    1 

michael elad 
the computer-science department 
the technion 

 

44 

from csc to multi-layered csc 
                 

    1                1 

    1                      1 
    1 

    0 

convolutional sparsity 

(csc) assumes an 

inherent structure is 

present in natural 

signals 

we propose to impose the 
same structure on the 
representations themselves 

    1                1 

    2                1          2 

    2                2 

    2 

    1    1 

    1 

 

multi-layer csc (ml-csc)

michael elad 
the computer-science department 
the technion 

 

45 

intuition: from atoms to molecules 

                 

    1                      1 

    1                1 

    2                1          2 

    2                2 

o we can chain the all the dictionaries  

into one effective dictionary 
    eff =     1    2    3               k            =     eff     k   

o this is a special sparseland  (indeed, a csc) model 

 

o however:  

    1                1 

    a key property in this model: sparsity of the intermediate representations 

    the effective atoms: atoms     molecules     cells     tissue     body-parts      

michael elad 
the computer-science department 
the technion 

 

46 

a small taste: model training (mnist) 

mnist dictionary: 
   d1:  32 filters of size 7  7, with stride of 2 (dense) 
   d2: 128 filters of size 5  5  32 with stride of 1 -  99.09 % sparse 
   d3: 1024 filters of size 7  7  128     99.89 % sparse 

    1 (7  7) 

    1    2 (15  15) 
 

    1    2    3  (28  28) 
  

michael elad 
the computer-science department 
the technion 

 

47 

a small taste: model training (cifar) 

    1 (5  5  3) 

    1    2 (13  13) 

    1    2    3  (32  32) 

cifar dictionary: 
    d1: 64 filters of size 5x5x3, stride of 2 

dense 

    d2: 256 filters of size 5x5x64, stride of 2 

82.99 % sparse 

    d3: 1024 filters of size 5x5x256  

90.66 % sparse 

michael elad 
the computer-science department 
the technion 

 

47 

ml-csc: pursuit 

o deep   coding problem                  (dictionaries are known): 
s       1
    1 0,   
s       2
    2 0,   
   
s       k
    k 0,   

     =     1    1
    1 =     2    2

find        j j=1

    k   1 =     k    k

        .     .  

k

   

 

  

 
o or, more realistically for noisy signals,  

             1    1 2        

    1 =     2    2

   

    k   1 =     k    k

s       1
    1 0,   
s       2
    2 0,   
   
s       k
    k 0,   

        find        j j=1

k

        .     .  

 

michael elad 
the computer-science department 
the technion 

 

 

 

48 

a small taste: pursuit 

y 

x 
  0 

  1 
94.51 % sparse 

(213 nnz) 

x=    1  1 

 

x=    1    2  2 

 

x=    1    2    3  3 

  2 

99.52% sparse 

(30 nnz) 

  3 

99.51% sparse 

(5 nnz) 

michael elad 
the computer-science department 
the technion 

 

49 

ml-csc: the simplest pursuit 

the simplest pursuit algorithm (single-layer case)  is  
the thr algorithm, which operates on a given input signal      by: 

     =          +       
              and      is sparse 

 
 
 

 

      =              t      

michael elad 
the computer-science department 
the technion 

 

50 

 

consider this for solving the dcp

o layered thresholding (lt): 

 
 

 
 

 

estimate     1 via the thr algorithm

     2 =       2     2
     2 =       2     2
     2 =       2     2

t       1     1
t       1     1
t       1     1

t      
t      
t      

 

estimate     2 via the thr algorithm

              

    :  find        j j=1

k

             1    1 2        

    1 =     2    2

   

    k   1 =     k    k

        .     .   
s       1
    1 0,   
s       2
    2 0,   
   
s       k
    k 0,   

o now let   s take a look at how conv. neural network operates: 
 

          = relu     2 +     2

t relu     1 +     1

t      

 

the layered (soft nonnegative) 

thresholding and the id98 forward pass 
 

algorithm are the very same thing !!!

michael elad 
the computer-science department 
the technion 

 

 

51 

 

theoretical path

m 

     =     1    1 
    1 =     2    2 

    

    k   1 =     k    k 

 

 

    i is     0,    sparse

a 

              

     

  

     

     

layered thr
 
(forward pass)

 

 

 

maybe other?

k

     i i=1

 

armed with this view of a generative source model, we 

may ask new and daring theoretical questions 

michael elad 
the computer-science department 
the technion 

 

52 

success of the layered-thr 

theorem: if      i 0,   

s <

1
2

1 +

1
       i

   

min
     i
max    
     i

1
       i

   

i   1
  l
max  
     i

then the layered hard thr (with the proper thresholds)     
finds the correct supports  and        i
p  and 
we have defined   l

lt         i 2,   

0 =      2,   

      l

i ,  where 

p

i =
  l

p

    i 0,   

      l

i   1 +        i

    i 0,   

s     1      i

max

 

the stability of the forward pass is guaranteed 
if the underlying representations are locally 

 

sparse and the noise is locally bounded

michael elad 
the computer-science department 
the technion 

 

papyan, romano & elad (   17) 

problems:  
1. contrast 
2. error growth 
3. error even if no noise 

54 

 

layered basis pursuit (bp)

o we chose the thresholding algorithm 
due to its simplicity, but we do know 
that there are better pursuit methods 
    how about using them? 

o lets use the basis pursuit instead     

    k   1 =     k    k

    1

lbp = min
    1

1
2

 

             1    1 2

2 +   1     1 1 

    2

lbp = min
    2

1
2

 

      1

lbp         2    2 2

2

+   2     2 1 

              

    :  find        j j=1

k

             1    1 2        

    1 =     2    2

   

        .     .   
s       1
    1 0,   
s       2
    2 0,   
   
s       k
    k 0,   

 

deconvolutional networks 
[zeiler, krishnan, taylor & fergus    10] 

michael elad 
the computer-science department 
the technion 

 

55 

success of the layered bp 

theorem: assuming that                ,   
then the layered basis pursuit performs very well:  

     +

 

     <

    
             

    
    

 

1.

2.

lbp is contained in that of     i 

 the support of      i
 the error is bounded:        i

      l

i = 7.5i      2,   

p  

i
j=1

p

lbp         i 2,   
    j 0,   

p

 

      l

i , where  

3. every entry in     i greater than  

i /
  l

p will be found 

    i 0,   

papyan, romano & elad (   17) 

michael elad 
the computer-science department 
the technion 

 

problems:  
1. contrast 
2.
3.

error growth 
error even if no noise 

56 

 

layered iterative thresholding

layered bp:        j

lbp = min
    j

 

1
2

      j   1

lbp         j    j 2

2

+   j     j 1

 

j 

layered iterative soft-thresholding: 

t 

t =       j/cj     j
    j

t      j   1         j    j

t   1

 

j 

 
t   1 +     j

note that our suggestion 
implies that groups of layers 
share the same dictionaries 

michael elad 
the computer-science department 
the technion 

 

can be seen as a very deep 
recurrent neural network 

[gregor & lecun    10] 

57 

 

what about learning?  

sparseland 

sparse 

representation 

theory 

csc 

convolutional 

sparse  
coding 

ml-csc   
multi-layered 
convolutional 
sparse coding 

o all these models rely on  proper dictionary learning algorithms to 

fulfil their mission:  

    sparseland: we have unsupervised and supervised such algorithms, and a 

beginning of theory to explain how these work 

    csc: we have few and only unsupervised methods, and even these are not 

fully stable/clear 

    ml-csc: one algorithm has been proposed (unsupervised)     see arxiv  

michael elad 
the computer-science department 
the technion 

 

52 

 

where are the labels? 

answer 2:  
answer 1:  

m 

     =     1    1 
    1 =     2    2 

    

    k   1 =     k    k 

 

 

    i is     0,    sparse

     

o in fact, this model could be augmented by a 
o we do not need labels because everything we 
synthesis of the corresponding label by:  
show refer to the unsupervised case, in which 
we operate on signals, not necessarily in the 
 
context of recognition 

l      =                  c +   wj

k
j=1

t  j

 

 

o this assumes that knowing the representations 

(or maybe their supports?) suffice for 
identifying the label  

l       

o thus, a successful pursuit algorithm can lead to 

an accurate recognition if the network is 
augmented by a fc classification layer 

we presented the ml-csc as a 
machine that produces signals x 

michael elad 
the computer-science department 
the technion 

 

52 

 

time to conclude

michael elad 
the computer-science department 
the technion 

 

58 

 

this talk

 

sparseland 

the desire to 
model data 

take home message 1: 

generative modeling of data 
sources enables algorithm 
development along with 
theoretically analyzing 
algorithms    performance  

novel view of 
convolutional  
sparse coding 

a novel interpretation 

and theoretical 

understanding of id98 

multi-layer 
convolutional  
sparse coding 

take home message 2: 

the multi-layer 

convolutional sparse 
coding model could be 

a new platform for 
understanding and 
developing deep-
learning solutions  

we spoke about the importance of models in signal/image 
the ml-csc was shown to enable a theoretical  
we propose a multi-layer extension of  
we presented a theoretical study of the csc  model and  
study of id98, along with new insights  
processing and described sparseland in details 
csc, shown to be tightly connected to id98 
how to operate locally while getting global optimality  

michael elad 
the computer-science department 
the technion 

 

59 

questions? 

more on these (including these slides and the relevant papers) can be 

found in http://www.cs.technion.ac.il/~elad  

michael elad 
the computer-science department 
the technion 

 

