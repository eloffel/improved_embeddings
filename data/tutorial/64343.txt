deep learning: approximation of functions

by composition

zuowei shen

department of mathematics

national university of singapore

outline

1 a brief introduction of approximation theory

2 deep learning: approximation of functions by composition

3 approximation of id98s and sparse coding

4 approximation in feature space

outline

1 a brief introduction of approximation theory

2 deep learning: approximation of functions by composition

3 approximation of id98s and sparse coding

4 approximation in feature space

a brief introduction of approximation theory

for a given function f : rd     r and   > 0, approximation is to
   nd a simple function g such that

(cid:107)f     g(cid:107) <  .

a brief introduction of approximation theory

for a given function f : rd     r and   > 0, approximation is to
   nd a simple function g such that

(cid:107)f     g(cid:107) <  .

function g : rn     r can be as simple as g(x) = a    x. to make
sense of this approximation, we need to    nd a map
t : rd (cid:55)    rn, such that

(cid:107)f     g     t(cid:107) <  .

a brief introduction of approximation theory

for a given function f : rd     r and   > 0, approximation is to
   nd a simple function g such that

(cid:107)f     g(cid:107) <  .

function g : rn     r can be as simple as g(x) = a    x. to make
sense of this approximation, we need to    nd a map
t : rd (cid:55)    rn, such that

(cid:107)f     g     t(cid:107) <  .

1 classical approximation: t is independent of data and n

depends on  .

2 deep learning: t depends on the data and n can be

independent of   (t is learned from data).

classical approximation

linear approximation: given a    nite set of generators
{  1, . . . ,   n}, e.g. splines, wavelet frames,    nite elements or
generators in reproducing kernel hilbert spaces. de   ne

t = [  1,   2, . . . ,   n](cid:62) : rd (cid:55)    rn and g(x) = a    x.

the linear approximation is to    nd a     rn such that

n(cid:88)

g     t =

ai  i     f

it is linear because f1     g1, f2     g2     f1 + f2     g1 + g2.

i=1

classical approximation

linear approximation: given a    nite set of generators
{  1, . . . ,   n}, e.g. splines, wavelet frames,    nite elements or
generators in reproducing kernel hilbert spaces. de   ne

t = [  1,   2, . . . ,   n](cid:62) : rd (cid:55)    rn and g(x) = a    x.

the linear approximation is to    nd a     rn such that

n(cid:88)

g     t =

ai  i     f

i=1

it is linear because f1     g1, f2     g2     f1 + f2     g1 + g2.
nonlinear approximation: given in   nite generators    = {  i}   
and de   ne

i=1

t = [  1,   2, . . . , ](cid:62) : rd (cid:55)       r    and g(x) = a    x

the nonlinear approximation of f is to    nd a    nitely supported a
such that g     t     f.
it is nonlinear because f1     g1, f2     g2 (cid:59) f1 + f2     g1 + g2 as
the support of the approximator g of f depends on f.

examples

consider a function space l2(rd), let {  i}   
basis of l2(rd).

i=1 be an orthonormal

examples

consider a function space l2(rd), let {  i}   
basis of l2(rd).
linear approximation
for a given n, t = [  1, . . . ,   n](cid:62) and g = a    x where aj = (cid:104)f,   j(cid:105). denote
h = span{  1, . . . ,   n}     l2(rd).
then,

i=1 be an orthonormal

n(cid:88)

g     t =

(cid:104)f,   i(cid:105)  i

is the orthogonal projection onto the space h and is the best approximation
of f from the space h.

i=1

examples

consider a function space l2(rd), let {  i}   
basis of l2(rd).
linear approximation
for a given n, t = [  1, . . . ,   n](cid:62) and g = a    x where aj = (cid:104)f,   j(cid:105). denote
h = span{  1, . . . ,   n}     l2(rd).
then,

i=1 be an orthonormal

n(cid:88)

g     t =

(cid:104)f,   i(cid:105)  i

i=1

is the orthogonal projection onto the space h and is the best approximation
of f from the space h.
g     t provides a good approximation of f when the sequence {(cid:104)f,   j(cid:105)}   
decays fast as j     +   .

j=1

examples

consider a function space l2(rd), let {  i}   
basis of l2(rd).
linear approximation
for a given n, t = [  1, . . . ,   n](cid:62) and g = a    x where aj = (cid:104)f,   j(cid:105). denote
h = span{  1, . . . ,   n}     l2(rd).
then,

i=1 be an orthonormal

n(cid:88)

g     t =

(cid:104)f,   i(cid:105)  i

i=1

is the orthogonal projection onto the space h and is the best approximation
of f from the space h.
g     t provides a good approximation of f when the sequence {(cid:104)f,   j(cid:105)}   
decays fast as j     +   .
therefore,

j=1

1 linear approximation provides a good approximation for smooth

functions.

2 when n =    , it reproduces any function in l2(rd).
3 advantage: it is a good approximation scheme for d is small, domain is

simple, function form is complicated but smooth.

4 disadvantage: if d is big and   is small, n is huge.

examples

nonlinear approximation

(cid:40)(cid:104)f,   j(cid:105),

aj =

0,

t = (  j)   

j=1 : rd (cid:55)    r    and g(x) = a    x and each aj is

for the largest n terms in the sequence {|(cid:104)f,   j(cid:105)|}   
otherwise.

j=1

examples

nonlinear approximation

(cid:40)(cid:104)f,   j(cid:105),

aj =

0,

t = (  j)   

j=1 : rd (cid:55)    r    and g(x) = a    x and each aj is

for the largest n terms in the sequence {|(cid:104)f,   j(cid:105)|}   
otherwise.

j=1

the approximation of f by g     t depends less on the decay of
the sequence {|(cid:104)f,   j(cid:105)|}   

j=1. therefore,

1

the nonlinear approximation is better than the linear
approximation when f is nonsmooth.

2 curse of dimensionality: if d is big and   is small, n is huge.

both linear and nonlinear approximations are schemes to
approximate a class of function where t is    xed and it
essentially changes a basis in order to represent or
approximate a certain class of functions.

both linear and nonlinear approximations do not suit for
approximating f when f is de   ned on a complex domain, e.g
manifold in a very high dimensional space.

however, in deep learning, t is constructed by given data that
is adaptive to the underlying function to be approximated. t
changes variables and maps domain of f to a feature domain
where approximation become simpler, robust, and ef   cient.

deep learning approximation is to    nd map t that maps the
domain of f to a    simple/ better domain    so that simple
classical approximation can be applied.

outline

1 a brief introduction of approximation theory

2 deep learning: approximation of functions by composition

3 approximation of id98s and sparse coding

4 approximation in feature space

approximation for deep learning

given data {(xi, f (xi))}m
i=1.
1 the key of deep learning is to construct a t by the given

data.

2 t can simplify the domain of f through the change of

variables.

3 t maps the key features of the domain of f and f , so that
it is easy to    nd g s.t. g     t gives a good approximation of
4
f.

what is the mathematics behind this?

settings: construct a map t : rd (cid:55)    rn and a simple function g
(e.g. g = a    x ) from data such that g     t provides a good
approximation of f.

approximation by compositions

question 1: for arbitrarily given   > 0, is there t : rd (cid:55)    r
such that (cid:107)f     g     t(cid:107)      ?

approximation by compositions

question 1: for arbitrarily given   > 0, is there t : rd (cid:55)    r
such that (cid:107)f     g     t(cid:107)      ?

answer: yes!
theorem
let f : rd     r and g : rn     r and assume im(f )     im(g). for
an arbitrarily given   > 0, there exists t : rd (cid:55)    rn such that

(cid:107)f     g     t(cid:107)      

t can be explicitly written out in terms of f and g.

t can be complex. this leads to

approximation by compositions

question 2: can t be a composition of simple maps? that is,
can we write t = t1                tj and each ti, i = 1, 2, . . . , j is
simple, e.g. perturbation of identity.

answer: yes!

theorem
denote f : rd     r and g : rn     r. for an arbitrarily given
  > 0, if im(f )     im(g), then there exists j simple maps
ti, i = 1, 2, . . . , j such that t = t1     t2 . . .     tj : rd (cid:55)    rn and

(cid:107)f     g     t1                tj(cid:107)      

ti, i = 1, 2, . . . , j can be written out explicitly in terms of t .

approximation by compositions

question 3: can ti, i = 1, 2, . . . , j be mathematically constructed by
some scheme?
answer: yes! ti, i = 1, . . . , j can be constructed by solving the
minimization problem:

min

t1,t2,...,tj

(cid:107)f     g     t1                tj(cid:107)

a constructive proof is given in paper.

approximation by compositions

question 3: can ti, i = 1, 2, . . . , j be mathematically constructed by
some scheme?
answer: yes! ti, i = 1, . . . , j can be constructed by solving the
minimization problem:

min

t1,t2,...,tj

(cid:107)f     g     t1                tj(cid:107)

a constructive proof is given in paper.
question 4: given training data {xi, f (xi)}m
numerical scheme to    nd   ti, i = 1, 2 . . . , j and   g such that
(cid:107)f       g       t1                  tj(cid:107)      , with high id203

i=1, can we design

by minimizing

m(cid:88)

i=1

1
m

(f (xi)       g       t1                  tj (xi))2?

answer: yes! we do have designed deep neural networks for that.
numerical simulations show it performs well.

ideas

one of the simplest ideas is

(cid:107)f       g       t1              tj(cid:107)
   (cid:107)f     g     t1                tj(cid:107) + (cid:107)g     t1                tj       g       t1              tj(cid:107)
=: bias

variance

+

li, shen and tai, deep learning: approximation of functions by composition, 2018.

this theory is complete, but does not answer all questions!
for example, we do not have approximation order in terms of
the number of nodes yet.

this theory is complete, but does not answer all questions!
for example, we do not have approximation order in terms of
the number of nodes yet.

there are many different machine learning architectures, e.g.
convolutional neural networks (id98s) and sparse coding based
classi   ers, are different from the architecture we designed here.

next, we will present approximation theory of id98s and sparse
coding based classi   ers for image classi   cation.

outline

1 a brief introduction of approximation theory

2 deep learning: approximation of functions by composition

3 approximation of id98s and sparse coding

4 approximation in feature space

approximation for image classi   cation

binary classi   cation

    =    0        1 and    0        1 =    .
let f to be the oracle classi   er, i.e. f :         rd     {0, 1} and

(cid:40)

f (x) =

if x        0,
if x        1.

0,
1,

construct feature map t and the classi   er g such that

f     g     t

is small.

approximation for image classi   cation

g is normally a fully connected layer followed by softmax
de   ned on feature space.
construct a feature map t , so that g     t approximates f
well, when constructing a fully connected layer to
approximate f from the data in image space is hard.

approximation for image classi   cation

g is normally a fully connected layer followed by softmax
de   ned on feature space.
construct a feature map t , so that g     t approximates f
well, when constructing a fully connected layer to
approximate f from the data in image space is hard.

g     t gives a good approximation of f if t satis   es

(cid:107)t (x)     t (y)(cid:107)     c(cid:107)x     y(cid:107),
(cid:107)t (x)     t (y)(cid:107) > c(cid:107)x     y(cid:107),

   x, y        ,
   x        0, y        1.

(3.1)
(3.2)

for some c, c > 0.

the above inequalities are not easy to prove for both id98s and
sparse coding based classi   ers especially when n     d.

accuracy of image classi   cation of id98s

the id98s achieve desired classi   cation accuracy with high
id203!
all the numerical results con   rmed it.

accuracy of image classi   cation of id98s

the id98s achieve desired classi   cation accuracy with high
id203!
all the numerical results con   rmed it.
settings:
given j sets of convolution kernels {wi}j
i=1 and bias {bi}j
i=1.
a j layer convolutional neural network is a nonlinear function

g     t

where

n(cid:88)

t (x) =   (wj (cid:126)   (wj   1 (cid:126)        (  (w1 (cid:126) x + b1))       + bj   1) + bj )

g(x) =

ai  (w(cid:62)

i x + bi),

and   (x) = max(0, x).

i=1

normally, the convolutional kernels {wi}j

i=1 have small size.

accuracy of image classi   cation of id98s

the id98s achieve desired classi   cation accuracy with high
id203!
all the numerical results con   rmed it.
settings:
given j sets of convolution kernels {wi}j
i=1 and bias {bi}j
i=1.
a j layer convolutional neural network is a nonlinear function

g     t

where

t (x) =   (wj (cid:126)   (wj   1 (cid:126)        (  (w1 (cid:126) x + b1))       + bj   1) + bj )

g(x) =

ai  (w(cid:62)

i x + bi),

and   (x) = max(0, x).

n(cid:88)

i=1

normally, the convolutional kernels {wi}j
given m training samples {(xi, yi)}m

i=1 have small size.

i=1,   t and a   g are learned from

m(cid:88)

i=1

min
g,t

1
m

(yi     g     t (xi))2.

accuracy of image classi   cation of id98s

question: whether can we have a rigorous proof for this
statement?

answer: yes!
theorem
for any given   > 0 and sample data z with sample size m, there
exists a id98 classi   er whose    lter size can be as small as 3, such
that the classi   er accuracy a satis   es

p(a     1      )     1       ( , m),

  ( , m)     0 as m     +   .

the dif   cult part is to prove the inequalities (3.1) and (3.2) for   t .

bao, shen, tai, wu and xiang, approximation and scaling analysis of convolutional
neural networks, 2017.

accuracy of image classi   cation of sparse coding

given m training samples {(xi, yi)}m
classi   er learn   d and   w via solving the problem

i=1, the sparse coding based

(cid:8)(cid:107)xi     dci(cid:107)2 +   (cid:107)ci(cid:107)0 +   (cid:107)yi     w ci(cid:107)2(cid:9)

min

(cid:107)dk(cid:107)=1,{ci}m

i=1,w

1
m

m(cid:88)

i=1

there are numerical algorithms with global convergence property to
solve the above minimization.

accuracy of image classi   cation of sparse coding

given m training samples {(xi, yi)}m
classi   er learn   d and   w via solving the problem

i=1, the sparse coding based

(cid:8)(cid:107)xi     dci(cid:107)2 +   (cid:107)ci(cid:107)0 +   (cid:107)yi     w ci(cid:107)2(cid:9)

min

(cid:107)dk(cid:107)=1,{ci}m

i=1,w

1
m

m(cid:88)

i=1

there are numerical algorithms with global convergence property to
solve the above minimization.
the sparse coding based classi   er is   g       t , where

  g(x) =   w x

and

  t (x)     arg min

c

(cid:107)x       dc(cid:107)2 +   (cid:107)c(cid:107)0.

there is no mathematical analysis of classi   cation accuracy of   g       t ,
i.e. (cid:107)f       g       t(cid:107).

bao, ji, quan, shen, dictionary learning for sparse coding: algorithms and convergence analysis, ieee
transactions on pattern analysis and machine intelligence, 38(7), (2016), 1356-1369.
bao, ji, quan, shen, l0 norm based dictionary learning by proximal methods with global convergence, ieee
conference id161 and pattern recognition (cvpr), columbus, (2014).

accuracy of image classi   cation of sparse coding

consider an orthogonal dictionary learning (odl) scheme

(cid:8)(cid:107)xi     dci(cid:107)2 +   (cid:107)ci(cid:107)1 +   (cid:107)yi     g(ci)(cid:107)2(cid:9) (3.3)

min

d(cid:62)d=i,{ci}m

i=1,g

1
m

m(cid:88)

i=1

where g is a fully connected layer.
the numerical algorithm to solve the above problem has global
convergence property.

the classi   cation accuracy is similar to the previous models.

dataset

classi   cation accuracies (%)
d-ksvd

face: extended yale b

face: ar face

object: caltech101

k-svd
93.10
86.50
68.70

94.10
88.80
68.60

idl
95.72
96.18
72.29

odl
96.12
96.37
72.54

accuracy of image classi   cation of sparse coding

consider an orthogonal dictionary learning (odl) scheme

(cid:8)(cid:107)xi     dci(cid:107)2 +   (cid:107)ci(cid:107)1 +   (cid:107)yi     g(ci)(cid:107)2(cid:9) (3.3)

min

d(cid:62)d=i,{ci}m

i=1,g

1
m

m(cid:88)

i=1

where g is a fully connected layer.
the numerical algorithm to solve the above problem has global
convergence property.

the classi   cation accuracy is similar to the previous models.

dataset

classi   cation accuracies (%)
d-ksvd

face: extended yale b

face: ar face

object: caltech101

k-svd
93.10
86.50
68.70

94.10
88.80
68.60

idl
95.72
96.18
72.29

odl
96.12
96.37
72.54

accuracy of image classi   cation of sparse coding

consider an orthogonal dictionary learning (odl) scheme

(cid:8)(cid:107)xi     dci(cid:107)2 +   (cid:107)ci(cid:107)1 +   (cid:107)yi     g(ci)(cid:107)2(cid:9) (3.3)

min

d(cid:62)d=i,{ci}m

i=1,g

1
m

m(cid:88)

i=1

where g is a fully connected layer.
the numerical algorithm to solve the above problem has global
convergence property.

the classi   cation accuracy is similar to the previous models.

dataset

classi   cation accuracies (%)
d-ksvd

face: extended yale b

face: ar face

object: caltech101

k-svd
93.10
86.50
68.70

94.10
88.80
68.60

idl
95.72
96.18
72.29

odl
96.12
96.37
72.54

for this model, we have mathematical analysis of accuracy.

sparse coding approximation of image classi   cation

let   g to be the fully connected layer and   d is the dictionary from
solving (3.3). de   ne

  t (x) = arg min

c

(cid:107)x       dc(cid:107)2 +   (cid:107)c(cid:107)1.

the sparse coding based classi   er from odl model is   g       t .

sparse coding approximation of image classi   cation

let   g to be the fully connected layer and   d is the dictionary from
solving (3.3). de   ne

  t (x) = arg min

c

(cid:107)x       dc(cid:107)2 +   (cid:107)c(cid:107)1.

the sparse coding based classi   er from odl model is   g       t .

theorem
consider the odl model. for any given   > 0 and sample data z
with sample size m, there exists a sparse coding based classi   er,
such that the classi   er accuracy a satis   es

p(a     1      )     1       ( , m),

  ( , m)     0 as m     +   .

to prove the two inequalities (3.1) and (3.2) of   t is not easy.
bao, ji and shen, classi   cation accuracy of sparse coding based classi   er, 2018.

data-driven tight frame is convolutional sparse coding

given an image g, the data driven tight frame model solves
2 +   2(cid:107)c(cid:107)0

2 + (cid:107)(i     ww t )c(cid:107)2

c,w (cid:107)w t c     g(cid:107)2
min
s.t. w tw = i.

when w(cid:62)w = i, the rows of w form a tight frame.

(3.4)

data-driven tight frame is convolutional sparse coding

given an image g, the data driven tight frame model solves
2 +   2(cid:107)c(cid:107)0

2 + (cid:107)(i     ww t )c(cid:107)2

c,w (cid:107)w t c     g(cid:107)2
min
s.t. w tw = i.

when w(cid:62)w = i, the rows of w form a tight frame.
the minimization model (3.4) is equivalent to

(3.4)

c,w (cid:107)wg     c(cid:107)2

min

2 +   2(cid:107)c(cid:107)0,

s.t. w(cid:62)w = i.

(3.5)

by the structure of w, each channel of w corresponds to a
convolutional kernel.

data-driven tight frame is convolutional sparse coding

given an image g, the data driven tight frame model solves
2 +   2(cid:107)c(cid:107)0

2 + (cid:107)(i     ww t )c(cid:107)2

c,w (cid:107)w t c     g(cid:107)2
min
s.t. w tw = i.

when w(cid:62)w = i, the rows of w form a tight frame.
the minimization model (3.4) is equivalent to

(3.4)

c,w (cid:107)wg     c(cid:107)2

min

2 +   2(cid:107)c(cid:107)0,

s.t. w(cid:62)w = i.

(3.5)

by the structure of w, each channel of w corresponds to a
convolutional kernel.
to solve (3.5), we use adm. for    xed w, c can be solved by
hard thresholding; for    xed c, w has an analytical solution and
easy to compute. thanks the convolution structure of w and the
tight frame property.
the iteration algorithm converges.

data-driven tight frame for image denoising

[1] cai, huang, ji, shen and ye, data-driven tight frame construction and image denois-
ing, applied and computational harmonic analysis, 37(1), (2014), 89-105.
[2] bao, ji and shen, convergence analysis for iterative data-driven tight frame construc-
tion scheme, applied and computational harmonic analysis, 38(3), (2015), 510-523.

outline

1 a brief introduction of approximation theory

2 deep learning: approximation of functions by composition

3 approximation of id98s and sparse coding

4 approximation in feature space

back to classical approximation on feature domain

classical approximation is still useful for approximation in
feature space.

back to classical approximation on feature domain

classical approximation is still useful for approximation in
feature space.

given noisy data {xi, yi}m

i=1 with

yi = (sf )(xi) + ni,

where {yi}m

i=1 are samples of f with noise ni.

back to classical approximation on feature domain

classical approximation is still useful for approximation in
feature space.

given noisy data {xi, yi}m

i=1 with

yi = (sf )(xi) + ni,

i=1 are samples of f with noise ni.

where {yi}m
by applying some data    tting scheme, e.g., the wavelet
frame scheme, one obtains the denoised result

{y   

i }m
i=1.

back to classical approximation on feature domain

classical approximation is still useful for approximation in
feature space.

given noisy data {xi, yi}m

i=1 with

yi = (sf )(xi) + ni,

i=1 are samples of f with noise ni.

where {yi}m
by applying some data    tting scheme, e.g., the wavelet
frame scheme, one obtains the denoised result

{y   

i }m
i=1.

let g be the function reconstructed by y   
approximation scheme.

i through some

back to classical approximation on feature domain

classical approximation is still useful for approximation in
feature space.

given noisy data {xi, yi}m

i=1 with

yi = (sf )(xi) + ni,

i=1 are samples of f with noise ni.

where {yi}m
by applying some data    tting scheme, e.g., the wavelet
frame scheme, one obtains the denoised result

{y   

i }m
i=1.

let g be the function reconstructed by y   
approximation scheme.
question:
1. what   s the error between g and f?
2. can we have g        f when the sampling data is
suf   ciently dense?

i through some

data    tting

let     := [0, 1]    [0, 1] and f     l2(   ). let    be the tensor
product of b-spline functions and denote the scaled functions
by   n,   := 2n  (2n         ). let (sf )[  ] = 2n(cid:104)f,   n,  (cid:105).

data    tting

let     := [0, 1]    [0, 1] and f     l2(   ). let    be the tensor
product of b-spline functions and denote the scaled functions
by   n,   := 2n  (2n         ). let (sf )[  ] = 2n(cid:104)f,   n,  (cid:105).

given noisy observations

y[  ] = (sf )[  ] + n  ,    = (  1,   2), 0       1,   2     2n     1.

the data    tting problem is to recover f on     from y.

wavelet frame

let    be a re   nable function and    := {  i, i = 1, . . . , r} be
the wavelet functions associated with    in l2(r2).

wavelet frame

let    be a re   nable function and    := {  i, i = 1, . . . , r} be
the wavelet functions associated with    in l2(r2).
denote the scaled functions by

  n,   := 2n  (2n         )

and   i,n,   := 2n  i(2n         ).

wavelet frame

let    be a re   nable function and    := {  i, i = 1, . . . , r} be
the wavelet functions associated with    in l2(r2).
denote the scaled functions by

  n,   := 2n  (2n         )

and   i,n,   := 2n  i(2n         ).

x(  ) := {  i,n,  } is a tight frame if

(cid:107)f(cid:107)2

2 =

|(cid:104)f,   i,n,  (cid:105)|2,   f     l2(r).

(cid:88)

i,n,  

unitary extension principle (uep): assume the masks hi
satisfy the following equalities

hi(m + 2k + (cid:96))hi(2k + (cid:96)) =   m, for any m, (cid:96)     z,

r(cid:88)

(cid:88)

i=0

k   z

2

x(  ) := {  i,n,  } is a tight frame.

wavelet frame

let    be a re   nable function and    := {  i, i = 1, . . . , r} be
the wavelet functions associated with    in l2(r2).
denote the scaled functions by

  n,   := 2n  (2n         )

and   i,n,   := 2n  i(2n         ).

x(  ) := {  i,n,  } is a tight frame if

(cid:107)f(cid:107)2

2 =

|(cid:104)f,   i,n,  (cid:105)|2,   f     l2(r).

(cid:88)

i,n,  

unitary extension principle (uep): assume the masks hi
satisfy the following equalities

hi(m + 2k + (cid:96))hi(2k + (cid:96)) =   m, for any m, (cid:96)     z,

r(cid:88)

(cid:88)

i=0

k   z

2

x(  ) := {  i,n,  } is a tight frame.

ron and shen, af   ne systems in l2(rd): the analysis of the analysis operator, journal
of functional analysis, 148(2), (1997), 408-447.
daubechies, han, ron and shen, framelets: mra-based constructions of wavelet
frames, applied and computational harmonic analysis, 14(1), (2003), 1-46.

examples of spline wavelets from uep

piecewise linear re   nable b-spline and the corresponding framelets.

re   nement mask h0 = [ 1

4 , 1

2 , 1

4 ]. high pass    lters h1 = [    1

4 , 1

2 ,     1

4 ] and h2 = [

   
4 , 0,    

2

   
2
4 ].

re   nement mask h0 = [ 1
h2 = [    1

piecewise cubic re   nable b-spline and the corresponding framelets.
16 ,     1
   
16 ] and h4 = [    1
6

4 , 1
8 , 1
16 , 1
4 , 3
   
16 , 0,    
8 ], h3 = [

16 ]. high pass    lters h1 = [ 1

   
6
8 , 0,

4 , 0,     1

4 , 3
8 ,     1

8 , 1

4 , 1

6

8 ,     1
4 , 0, 1

4 , 1
4 , 1

16 ],
8 ].

examples of spline wavelets from uep

piecewise linear re   nable b-spline and the corresponding framelets.

re   nement mask h0 = [ 1

4 , 1

2 , 1

4 ]. high pass    lters h1 = [    1

4 , 1

2 ,     1

4 ] and h2 = [

   
4 , 0,    

2

   
2
4 ].

piecewise cubic re   nable b-spline and the corresponding framelets.
16 ,     1
   
16 ] and h4 = [    1
6

16 ]. high pass    lters h1 = [ 1

   
6
8 , 0,

4 , 3
8 ,     1

4 , 1

re   nement mask h0 = [ 1
h2 = [    1

4 , 1
8 , 1
16 , 1
4 , 3
   
16 , 0,    
8 ], h3 = [
discrete wavelet transform w:

4 , 0,     1

8 , 1

6

8 ,     1
4 , 0, 1

4 , 1
4 , 1

16 ],
8 ].

{an,   := (cid:104)f,   n,  (cid:105)} w       {(cid:104)f,   i,n,  (cid:105)}r

i=0 = {hi     an,  }r

i=0,

where {hi} are the wavelet frame    lters.

wavelet frame based data    tting scheme

example: analysis-based wavelet approach for data    tting

let f   

n be a minimizer of the model

en(f ) := (cid:107)f     y(cid:107)2

2 + (cid:107)diag(  n)wnf(cid:107)1,

where    is a vector which scales the different wavelet
channels.

wavelet frame based data    tting scheme

example: analysis-based wavelet approach for data    tting

let f   

n be a minimizer of the model

en(f ) := (cid:107)f     y(cid:107)2

2 + (cid:107)diag(  n)wnf(cid:107)1,

where    is a vector which scales the different wavelet
channels.
let g   

n(  )  (2n         ).
f   

n :=(cid:80)

     in

wavelet frame based data    tting scheme

example: analysis-based wavelet approach for data    tting

let f   

n be a minimizer of the model

en(f ) := (cid:107)f     y(cid:107)2

2 + (cid:107)diag(  n)wnf(cid:107)1,

n :=(cid:80)

where    is a vector which scales the different wavelet
channels.
let g   
what   s the bound of (cid:107)g   

n(  )  (2n         ).
f   
n     f(cid:107)l2(   )?

     in

wavelet frame based data    tting scheme

example: analysis-based wavelet approach for data    tting

let f   

n be a minimizer of the model

en(f ) := (cid:107)f     y(cid:107)2

2 + (cid:107)diag(  n)wnf(cid:107)1,

n :=(cid:80)

where    is a vector which scales the different wavelet
channels.
let g   
what   s the bound of (cid:107)g   
does g   

n(  )  (2n         ).
f   
n     f(cid:107)l2(   )?

n converge to f in l2(   ) as n        ?

     in

regularity assumption of f: there exits    >    1 such that

(cid:88)

  

(cid:88)

n   0

2  n(cid:88)

i,  

|(cid:104)f,   0,  (cid:105)| +

|(cid:104)f,   i,n,  (cid:105)| <    .

regularity assumption of f: there exits    >    1 such that

(cid:88)

  

(cid:88)

n   0

2  n(cid:88)

i,  

|(cid:104)f,   0,  (cid:105)| +

|(cid:104)f,   i,n,  (cid:105)| <    .

then for an arbitrary given 0 <    < 1, the following
inequality

(cid:107)g   

n     f(cid:107)l2(   )     c12   n min{ 1+  
2 , 1

2} log

1
  

+ c2  2

holds with con   dence 1       .

regularity assumption of f: there exits    >    1 such that

(cid:88)

  

(cid:88)

n   0

2  n(cid:88)

i,  

|(cid:104)f,   0,  (cid:105)| +

|(cid:104)f,   i,n,  (cid:105)| <    .

then for an arbitrary given 0 <    < 1, the following
inequality

(cid:107)g   

n     f(cid:107)l2(   )     c12   n min{ 1+  
2 , 1

2} log

1
  

+ c2  2

holds with con   dence 1       .
when n        , one can design a data    tting scheme such
that

n      e(cid:0)(cid:107)g   

lim

n     f(cid:107)l2(   )

(cid:1) = 0.

regularity assumption of f: there exits    >    1 such that

(cid:88)

  

(cid:88)

n   0

2  n(cid:88)

i,  

|(cid:104)f,   0,  (cid:105)| +

|(cid:104)f,   i,n,  (cid:105)| <    .

then for an arbitrary given 0 <    < 1, the following
inequality

(cid:107)g   

n     f(cid:107)l2(   )     c12   n min{ 1+  
2 , 1

2} log

1
  

+ c2  2

holds with con   dence 1       .
when n        , one can design a data    tting scheme such
that

n      e(cid:0)(cid:107)g   

lim

n     f(cid:107)l2(   )

(cid:1) = 0.

in this case, data is given on uniform grids.

when the noisy data are obtained from nonuniform grids or
obtained by random sampling, can we have a similar
result?

when the noisy data are obtained from nonuniform grids or
obtained by random sampling, can we have a similar
result?
yes. it is technical but it has been carefully studied.

when the noisy data are obtained from nonuniform grids or
obtained by random sampling, can we have a similar
result?
yes. it is technical but it has been carefully studied.

cai, shen and ye, approximation of frame based missing data recovery, applied and computational harmonic

analysis, 31(2), (2011), 185-204.

yang, stahl and shen, an analysis of wavelet frame based scattered data reconstruction, applied and

computational harmonic analysis, 42(3), (2017), 480-507.

yang, dong and shen, approximation of analog signals from noisy data, manuscript, 2018.

thank you!

http://www.math.nus.edu.sg/   matzuows/

