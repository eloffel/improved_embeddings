introduction
ldmnet: model, algorithm, and complexity analysis
experiments

ldmnet: low dimensional manifold regularized neural

networks

wei zhu

duke university

feb 9, 2018

ipam workshp on deep learning

joint work with qiang qiu, jiaji huang, robert calderbank, guillermo

sapiro, and ingrid daubechies

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

neural networks and over   tting

deep neural networks (dnns) have achieved great success in machine learning
research and commercial applications. when large amounts of training data are
available, the capacity of dnns can easily be increased by adding more units or
layers to extract more e   ective high-level features.

however, big networks with millions of parameters can easily over   t even the
largest of datasets. as a result, the learned network will have low error rates on
the training data, but generalizes poorly onto the test data.

deep neural network

over   tting

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

id173s

many widely-used network id173s are data-independent.

weight decay
parameter sharing
dropout
dropconnect
early stopping
. . . . . .

most of the data-dependent id173s are motivated by the empirical
observation that data of interest typically lie close to a manifold.

tangent distance algorithm
tangent prop algorithm
manifold tangent classi   er
. . . . . .

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

ldmnet: low dimensional manifold regularized neural networks

low dimensional manifold regularized neural networks (ldmnet)
incorporates a feature id173 method that focuses on the geometry of
both the input data and the output features.
output features   i

manifold m

input data xi

i=1     rd1 are the input data. {  i = f  (xi)}n

{xi}n
i=1     rd2 are the output
features.
p = {(xi ,   i)}n
i=1     rd is the collection of the data-feature concatenation
(xi ,   i).
m     rd is the underlying manifold, discretely sampled by the point cloud
p.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

mpintroduction
ldmnet: model, algorithm, and complexity analysis
experiments

ldmnet: low dimensional manifold regularized neural networks

input data xi

output features   i

manifold m

l=1nl     rd1.

i=1 typically lie close to a collection of low dimensional
i=1     n =    l

the input data {xi}n
manifolds, i.e. {xi}n
the feature extractor, f  , of a good learning algorithm should be a smooth
function over n .
therefore the concatenation of the input data and output features,
p = {(xi ,   i)}n
m =    l
graph of f   over nl.

i=1, should sample a collection of low dimensional manifolds
l=1ml     rd, where d = d1 + d2, and ml = {(x, f  (x))}x   nl is the

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

mpintroduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

ldmnet: low dimensional manifold regularized neural networks

we suggest that network over   tting occurs when dim(ml) is too large after
training. therefore, to reduce over   tting, we explicitly use the dimensions of
ml as a regularizer in the following variational form:

min
  ,m
s.t.

dim(m(p))dp

j(  ) +   
|m|
{(xi , f  (xi))}n
l=1ml, m(p) denotes the manifold ml to which p

m
i=1     m,

l=1 |ml| is the volume of m.

belongs, and |m| =pl

where for any p     m =    l

z

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

feature of the mnist dataset

10,000 test data

weight decay

dropout

ldmnet

figure: test data of mnist and their features learned by the same network with
di   erent regularizers. all networks are trained from the same set of 1,000 images.
data are visualized in two dimensions using pca, and ten classes are distinguished by
di   erent colors.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

dimension of a manifold

question: how do we calculate dim(m(p)) from the point cloud
p = {(xi , f  (xi))}n

i=1     m

theorem
let m be a smooth submanifold isometrically embedded in rd. for any
p = (pi)d

i=1     m,

dx

dim(m) =

|   m  i(p)|2

,

on the manifold m. more speci   cally,    m  i =pk

where   i(p) = pi is the coordinate function, and    m is the gradient operator
s,t=1 gst    t   i    s, where k is

the intrinsic dimension of m, and g st is the inverse of the metric tensor.

i=1

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

dimension of a manifold

the metric tensor g = g11 =(cid:10)      

sanity check:
if m = s1, then k = dim(m) = 1, d = dim(r2) = 2, and
x =   (  ) = (cos   , sin   )t is the coordinate chart.
the gradient of   i,    m  i = g11   1  i    1 =    1  i    1 can be viewed as a vector in
the ambient space r2:

(cid:11) = 1 = g 11.

      ,      

     

   j
m  i =    1  

j

   1  i

therefore, we have

   m  1 =(cid:10)   1  
   m  2 =(cid:10)   1  

1

1

   1  1,    1  

   1  2,    1  

2

2

  ,    cos    sin   (cid:11) ,
   1  1(cid:11) =(cid:10)sin2
   1  2(cid:11) =(cid:10)    sin    cos   , cos2
  (cid:11) .

hence k   m  1k2 + k   m  2k2 = sin2    + cos2    = 1

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

ldmnet: low dimensional manifold regularized neural networks

z

min
  ,m
s.t.

j(  ) +   
|m|
{(xi , f  (xi))}n

m
i=1     m,

dim(m(p))dp

using theorem 1, the above variational functional can be reformulated as

dx

min
  ,m

j(  ) +   
|m|
{(xi , f  (xi))}n

j=1
i=1     m

k   m  jk2

l2(m)

wherepd

s.t.
j=1 k   m  jk2

l2(m) corresponds to the l1 norm of the local dimension.
how do we solve (2)? alternate direction of minimization with respect to m
and   .

(1)

(2)

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

alternate direction of minimization

dx

min
  ,m

s.t.

j(  ) +   
|m|
{(xi , f  (xi))}n

j=1
i=1     m

k   m  jk2

l2(m)

given (  (k),m(k)) at step k satisfying {(xi , f  (k)(xi))}n
consists of the following

i=1     m(k), step k + 1

update   (k+1) and the perturbed coordinate functions
(k+1)
  (k+1) = (  
1

) as the minimizers of (3) with the    xed m(k):

,       ,   

(k+1)
d

dx

j=1

min

  ,  

j(  ) +   

|m(k)|

k   m(k)   jk2

s.t.   (xi , f  (k)(xi)) = (xi , f  (xi)),

update m(k+1):

m(k+1) =   

(k+1)(m(k))

l2(m(k))
   i = 1, . . . , n

(3)

(4)

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

alternate direction of minimization

(k+1)
update   (k+1) and   (k+1) = (  
1

,       ,   

(k+1)
d

) with the    xed m(k):

min

  ,  

j(  ) +   

|m(k)|

k   m(k)   jk2

s.t.   (xi , f  (k)(xi)) = (xi , f  (xi)),

update m(k+1):

l2(m(k))
   i = 1, . . . , n

dx

j=1

m(k+1) =   

(k+1)(m(k))

the manifold update is trivial to implement, and the update of    and    is an
optimization problem with nonlinear constraint, which    can    be solved via the
alternating direction method of multipliers (admm).

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

alternate direction of minimization

dx

j=1

nx

i=1

min

  ,  

j(  ) +   

|m(k)|

k   m(k)   jk2

s.t.   (xi , f  (k)(xi)) = (xi , f  (xi)),

l2(m(k))
   i = 1, . . . , n

solve the variationl problem using admm. more speci   cally,

(k+1)
  

  

= arg min
    

k   m(k)   jkl2(m(k))

dx
nx

j=d1+1

i=1

+   |m(k)|
2  n

k    (xi , f  (k)(xi))     (f  (k)(xi)     z (k)
i

)k2
2.

  

(k+1) = arg min

  

j(  ) +   
2n

k  

(k+1)
  

(xi , f  (k)(xi))     (f  (xi)     z (k)
i

)k2
2.

z (k+1)
i

=z (k)

i +   

(k+1)
  

(xi , f  (k)(xi))     f  (k+1)(xi),

where    = (  x ,     ) = ((  1, . . . ,   d1), (  d1+1, . . . ,   d)), and zi is the dual
variable.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

admm

(k+1)
  

  

= arg min
    

k   m(k)   jkl2(m(k))

dx
nx

j=d1+1

i=1

+   |m(k)|
2  n

nx

i=1

k    (xi , f  (k)(xi))     (f  (k)(xi)     z (k)
i

)k2
2.

  

(k+1) = arg min

  

j(  ) +   
2n

k  

(k+1)
  

(xi , f  (k)(xi))     (f  (xi)     z (k)
i

)k2
2.

z (k+1)
i

=z (k)

i +   

(k+1)
  

(xi , f  (k)(xi))     f  (k+1)(xi),

among (5),(6) and (7), (7) is the easiest to implement, (6) can be solved by
stochastic id119 (sgd) with modi   ed back propagation, and (5)
can be solved by the point integral method (pim) [z. shi, j. sun].

shi, z., and sun, j. (2013). convergence of the point integral method for the poisson equation on manifolds ii: the dirichlet boundary.
arxiv preprint arxiv:1312.4424

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

(5)

(6)

(7)

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

back propagation for the    update

nx

  

(k+1) = arg min

  

= arg min

  

k  

(k+1)
  

nx

j(  ) +   
2n
   (f  (xi), yi) + 1
n

1
n

i=1

nx

ei(  ),

(xi , f  (k)(xi))     (f  (xi)     z (k)
i

i=1
)k2
(xi , f  (k)(xi))     (f  (xi)     z (k)
where ei(  ) =   
i
second term with respect to the output layer f  (xi) is:

2 k  

(k+1)
  

i=1

(cid:16)f  (xi)     z (k)

i       

(k+1)
  

(xi , f  (k)(xi))(cid:17)

   ei
   f  (xi) =   

)k2
2.

(8)

(9)

2. the gradient of the

this means that we need to only add the extra term (9) to the original gradient,
and then use the already known procedure to back-propagate the gradient.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

point integral method for the    update

(k+1)
  

  

= arg min
    

k   m(k)   jkl2(m(k))

dx
nx

j=d1+1

i=1

+   |m(k)|
2  n

k    (xi , f  (k)(xi))     (f  (k)(xi)     z (k)
i

)k2
2.

note that the objective funtion above is decoupled with respect to j, and each
  j update can be cast into:

min

u   h1(m)

k   muk2

l2(m) +   

|u(q)     v(q)|2

,

(10)

x

q   p

where u =   j, m = m(k),p = {pi = (xi , f  (k)(xi))}n
   =   |m(k)|/2  n. the euler-lagrange equation of (10) is:

i=1     m, and

                     mu(p) +   

x

q   p

  (p     q)(u(q)     v(q)) = 0, p     m

   u
   n = 0, p        m

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

point integral method

                     mu(p) +   
(cid:18)kx     yk2

4t

x

q   p

(cid:19)

   mu(y)  r

z

m

in the point integral method (pim) [z. shi, j. sun], the key observation is the
following integral approximation:

  (p     q)(u(q)     v(q)) = 0, p     m

   u
   n = 0, p        m
(cid:18)kx     yk2
(cid:19)

(cid:18)kx     yk2

4t

d  y .

4t

(u(x)     u(y)) r

   u
   n (y)  r

z
z

m

   m

(cid:19)

dy

dy         1
t

+ 2

z    

r

  r =

r(s)ds.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

the function r is a positive function de   ned on [0, +   ) with compact support
(or fast decay) and

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

local truncation error

theorem
let m be a smooth manifold and u     c 3(m), then

z

z

(cid:13)(cid:13)(cid:13)(cid:13)   1

t

m

(u(x)     u(y)) rt(x, y)dy + 2

   m
   mu(y)  rt(x, y)dy

   u
   n (y)  rt(x, y)d  y
= o(t1/4),

(cid:13)(cid:13)(cid:13)(cid:13)l2(m)

where

rt(x, y) =

1

(4  t)k/2 r

,   rt(x, y) =

1

(4  t)k/2

  r

(cid:18)kx     yk2

4t

(cid:19)

.

   

z
(cid:18)kx     yk2

m

4t

(cid:19)

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

point integral method for the    update

x

q   p

                     mu(p) +   
rt(p, q) = ct exp(cid:16)    |p   q|2
z

4t

   

m

x

q   p

  (p     q)(u(q)     v(q)) = 0, p     m

   u
   n = 0, p        m

(cid:17), we know the solution u should satisfy

after convolving the above equation with the heat kernel

   mu(q)rt(p, q)dq +   

rt(p, q) (u(q)     v(q)) = 0.

(11)

z

m

combined with theorem 2 and the neumann boundary condition, this implies
that u should approximately satisfy

(u(p)     u(q)) rt(p, q)dq +   tx

rt(p, q) (u(q)     v(q)) = 0

(12)

q   p

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

point integral method for the    update

rt(p, q) (u(q)     v(q)) = 0

z

m

(u(p)     u(q)) rt(p, q)dq +   tx
nx

rt,ij(ui     uj) +   t

q   p

|m|
n

j=1

nx
(cid:16)

j=1

assume that p = {p1, . . . , pn} samples the manifold m uniformly at random,
then the above equation can be discretized as

rt,ij(uj     vj) = 0,

(13)

(cid:17)

where ui = u(pi), and rt,ij = rt(pi , pj). combining the de   nition of    in (10),
we can write (13) in the matrix form

where      can be chosen instead of    as the hyperparameter to be tuned,
u = (u1, . . . , un)t, w is an n    n matrix

l +   
    

w

u =   
    

wv,

     = 2  /t,

(cid:18)

(cid:19)

,

wij = rt,ij = exp

   |pi     pj|2

4t

(14)

(15)

and l is the graph laplacian of w:

lii =x

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

wij ,

and lij =    wij

if

i 6= j.

(16)

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

algorithm for training ldmnet

algorithm 1 ldmnet training
input: training data {(xi , yi)}n
output: trained network weights      .
1: randomly initialize the network weights   (0). the dual variables z (0)

i=1     rd1    r, hyperparameters      and   , and a
neural network with the weights    and the output layer   i = f  (xi)     rd2.
i     rd2

are initialized to zero.
2: while not converge do
3:

4:

5:

1. compute the matrices w and l as in (15) and (16) with pi =
(xi , f  (k)(xi)).
2. update   (k+1) in (5): solve the linear systems (14), where

ui =   j(pi),

vi = f  (k)(xi)j     z (k)
i,j .

3. update   (k+1) in (6): run sgd for m epochs with an extra gradient
term (9).
4. update z (k+1) in (7).
5. k     k + 1.

6:
7:
8: end while
9:             (k).

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

model
algorithm
complexity analysis

complexity analysis

the additional computation in algorithm 1 comes from the update of weight
matrices and solving the linear system from pim once every m epochs of sgd.

the weight matrix w is truncated to only 20 nearest neighbors. to identify
those nearest neighbors, we    rst organize the data points {p1, . . . , pn}     rd
into a k-d tree. nearest neighbors can then be e   ciently identi   ed because
branches can be eliminated from the search space quickly. modern algorithms
to build a balanced k-d tree generally at worst converge in o(n log n) time,
and    nding nearest neighbours for one query point in a balanced k-d tree takes
o(log n) time on average. therefore the complexity of the weight update is
o(n log n).

since w and l are sparse symmetric matrices with a    xed maximum number
of non-zero entries in each row, the linear system can be solved e   ciently with
the preconditioned conjugate gradients method. after restricting the number of
id127s to a maximum of 50, the complexity of the    update is
o(n).

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

experimental setup

we compare the performance of ldmnet to widely-used network id173
techniques, weight decay and dropout, using the same underlying network
structure.

unless otherwise stated, all experiments use mini-batch sgd with momentum
on batches of 100 images. the momentum parameter is    xed at 0.9. the
networks are trained using a    xed learning rate r0 on the    rst 200 epochs, and
then r0/10 for another 100 epochs.
for ldmnet, the weight matrices and    are updated once every m = 2 epochs
of sgd. for dropout, the corresponding dropout layer is always chosen to
have a drop rate of 0.5. all other network hyperparameters are chosen
according to the error rate on the validation set.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

mnist

the mnist handwritten digit dataset contains approximately 60,000 training
images (28    28) and 10,000 test images.

layer

1
2
3
4
5
6
7
8

type
conv

max pool

conv

max pool

conv

relu (dropout)
fully connected

softmaxloss

parameters

size: 5    5    1    20
stride: 1, pad: 0

size: 2    2, stride: 2, pad: 0

size: 5    5    20    50

stride: 1, pad: 0

size: 2    2, stride: 2, pad: 0

size: 4    4    50    500

stride: 1, pad: 0

n/a

500    10

n/a

table: network structure in the mnist experiments. the outputs of layer 6 are the
extracted features, which will be fed into the softmax classi   er (layer 7 and 8).

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

mnist

training per

class
50
100
400
700
1000
3000
6000

weight
decay
91.32%
93.38%
97.23%
97.67%
98.06%
98.87%
99.15%

dropout
92.31%
94.05%
97.95%
98.07%
98.71%
99.21%
99.41%

ldmnet
95.57%
96.73%
98.41%
98.61%
98.89%
99.24%
99.39%

loss

accuracy

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

mnist

10,000 test data

weight decay

dropout

ldmnet

figure: test data of mnist and their features learned by the same network with
di   erent regularizers. all networks are trained from the same set of 1,000 images.
data are visualized in two dimensions using pca, and ten classes are distinguished by
di   erent colors.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

svhn and cifar-10

svhn and cifar-10 are benchmark rgb image datasets, both of which
contain 10 di   erent classes.
svhn

cifar-10

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

svhn and cifar 10

layer

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

type
conv
relu

max pool

conv
relu

max pool

conv
relu

max pool

fully connected
relu (dropout)
fully connected
relu (dropout)
fully connected

softmaxloss
wei zhu duke university

parameters

size: 5    5    3    96
stride: 1, pad: 2

size: 3    3, stride: 2, pad: 0

size: 5    5    96    128

stride: 1, pad: 2

size: 3    3, stride: 2, pad: 0

size: 4    4    128    256

stride: 1, pad: 0

n/a

n/a

n/a

size: 3    3, stride: 2, pad: 0

output: 2048

n/a

output: 2048
2048    10

n/a

n/a

ldmnet: low dimensional manifold regularized neural networks

table: network structure in the svhn and cifar-10 experiments. the outputs of

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

svhn and cifar 10

training per

class
50
100
400
700

weight
decay
71.46%
79.05%
87.38%
89.69%

dropout
71.94%
79.94%
87.16%
89.83%

ldmnet
74.64%
81.36%
88.03%
90.07%

table: svhn: testing accuracy for di   erent regularizers

training per

class
50
100
400
700

full data

weight
decay
34.70%
42.45%
56.19%
61.84%

dropout
35.94%
43.18%
56.79%
62.59%

87.72%

ldmnet
41.55%
48.73%
60.08%
65.59%
88.21%

table: cifar-10: testing accuracy for di   erent regularizers.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

the objective of the experiment is to match a probe image of a subject
captured in the near-infrared spectrum (nir) to the same subject from a
gallery of visible spectrum (vis) images. the casia nir-vis 2.0 benchmark
dataset is used to evaluate the performance.

figure: sample images of two subjects from the casia nir-vis 2.0 dataset after the
pre-procssing of alignment and cropping. top: nir. bottom: vis.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

despite recent breakthroughs for vis face recognition by training dnns from
millions of vis images, such approach cannot be simply transferred to nir-vis
face recognition.

unlike vis face images, we have only limited number of availabe nir
images.
the nir-vis face matching is a cross-modality comparison.

the authors in [lezama et al., 2017] introduced a way to transfer the
breakthrough in vis face recognition to the nir spectrum. their idea is to use
a dnn pre-trained on vis images as a feature extactor, while making two
independent modi   cations in the input and output of the dnn.

modify the input by    hallucinating    a vis image from the nir sample.
learn an embedding of the dnn features at the output

lezama, j., qiu, q., and sapiro, g. (2017). not afraid of the dark: nir-vis face recognition via cross-spectral hallucination and low-rank
embedding. in the ieee conference on id161 and pattern recognition (cvpr).

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

modify the input by    hallucinating    a vis image from the nir sample.
learn an embedding of the dnn features at the output

figure: proposed procedure in [lezama et al., 2017]

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

we follow the second idea in [lezama et al., 2017], and learn a nonlinear low
dimensional manifold embedding of the output features. we use the vgg-face
model as a feature extractor. we then put the 4,096 dimensional features into
a two-layer fully connected network to learn a nonlinear embedding using
di   erent id173s.

layer

1
2
3
4

type

fully connected
relu (dropout)
fully connected
relu (dropout)

parameters
output:2000

output:2000

n/a

n/a

table: fully connected network for the nir-vis nonlinear feature embedding. the
outputs of layer 4 are the extracted features.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

vgg-face
vgg-face + triplet [lezama et al., 2017]
vgg-face + low-rank [lezama et al., 2017]
vgg-face weight decay
vgg-face dropout
vgg-face ldmnet

accuracy (%)
74.51    1.28
75.96    2.90
80.69    1.02
63.87    1.33
66.97    1.31
85.02    0.86

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

nir-vis heterogeneous face recognition

vgg-face

weight decay

dropout

ldmnet

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

-510010055010-10-5-10-20-15-4040-20020402020400060-20-20-40-40-60-1010-51500105105-100-20-5-12-0.50.50100.51-0.50-1-1-1.5introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

conclusion

ldmnet is a general network id173 technique that focuses on the
geometry of both the input data and the output feature

ldmnet directly minimize the dimension of the manifolds without explicit
parametrization.

ldmnet signi   cantly outperforms the widely-used network regularizers.

limits: data augmentation, o(n log n) complexity.

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

introduction
ldmnet: model, algorithm, and complexity analysis
experiments

mnist
svhn and cifar-10
nir-vis heterogeneous face recognition

thank you

wei zhu duke university

ldmnet: low dimensional manifold regularized neural networks

