structured sparsity

in natural language processing:

models, algorithms, and applications

andr  e f. t. martins1,3 dani yogatama2 noah a. smith2

m  ario a. t. figueiredo1

1instituto de telecomunica  c  oes

instituto superior t  ecnico, lisboa, portugal

2language technologies institute, school of computer science

carnegie mellon university, pittsburgh, pa, usa

3priberam, lisboa, portugal

eacl 2014 tutorial, gothenburg, sweden, april 27, 2014

slides online at http://tiny.cc/ssnlp14

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

1 / 128

welcome

this tutorial is about sparsity, a topic of great relevance to nlp.

sparsity relates to feature selection, model compactness, runtime,
memory footprint, interpretability of our models.

new idea in the last 7 years: structured sparsity. this tutorial tries to
answer:

what is structured sparsity?

how do we apply it?

how has it been used so far?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

2 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

3 / 128

notation

many nlp problems involve mapping from one structured space to
another. notation:

input set x
for each x     x, candidate outputs are y(x)     y
mapping is hw : x     y

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

4 / 128

linear models

our predictor will take the form

hw(x) = arg max
y   y(x)

w(cid:62)f(x, y )

where:

f is a vector function that encodes all the relevant things about
(x, y ); the result of a theory, our knowledge, feature engineering, etc.
w     rd are the weights that parameterize the mapping.

nlp today: d is often in the tens or hundreds of millions.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

5 / 128

learning linear models

max ent, id88, crf, id166, even supervised generative models all    t
the linear modeling framework.

general training setup:

we observe a collection of examples {(cid:104)xn, yn(cid:105)}n
perform statistical analysis to discover w from the data.
ranges from    count and normalize    to complex optimization routines.

n=1.

optimization view:

(cid:98)w = arg min

w

1
n

(cid:124)

n(cid:88)

n=1

(cid:123)(cid:122)

l(w; xn, yn)

+    (w)

(cid:124)(cid:123)(cid:122)(cid:125)

regularizer

(cid:125)

empirical loss

this tutorial will focus on the regularizer,    .

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

6 / 128

what is sparsity?

the word    sparsity    has (at least) four related meanings in nlp!

1 data sparsity: n is too small to obtain a good estimate for w.

also known as    curse of dimensionality.   
(usually bad.)

2    id203    sparsity: i have a id203 distribution over events

(e.g., x    y), most of which receive zero id203.
(might be good or bad.)

3 sparsity in the dual: associated with id166s and other kernel-based

methods; implies that the predictor can be represented via kernel
calculations involving just a few training instances.

4 model sparsity: most dimensions of f are not needed for a good hw;

those dimensions of w can be zero, leading to a sparse w (model).

this tutorial is about sense #4: today, (model) sparsity is a good thing!

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

7 / 128

why sparsity is desirable in nlp

occam   s razor and interpretability.

the bet on sparsity (friedman et al., 2004): it   s often correct. when it
isn   t, there   s no good solution anyway!

models with just a few features are

easy to explain and implement

attractive as linguistic hypotheses

reminiscent of classical symbolic systems

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

8 / 128

a decision list from yarowsky (1995).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

9 / 128

why sparsity is desirable in nlp

computational savings.

wd = 0 is equivalent to erasing the feature from the model; smaller
e   ective d implies smaller memory footprint.

this, in turn, implies faster decoding runtime.

further, sometimes entire kinds of features can be eliminated, giving
asymptotic savings.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

10 / 128

why sparsity is desirable in nlp

generalization.

the challenge of learning is to extract from the data only what will
generalize to new examples.

forcing a learner to use few features is one way to discourage
over   tting.

text categorization experiments in kazama and tsujii (2003): +3
accuracy points with 1% as many features

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

11 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

12 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

12 / 128

filter-based feature selection

for each candidate feature fd , apply a heuristic to determine whether to
include it. (excluding fd equates to    xing wd = 0.)

examples:

count threshold: is |{n | fd (xn, yn) > 0}| >    ?
(ignore rare features.)

mutual information or correlation between features and labels

advantage: speed!

disadvantages:

ignores the learning algorithm

thresholds require tuning

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

13 / 128

ratnaparkhi (1996), on his pos tagger:

the behavior of a feature that occurs very sparsely in the
training set is often di   cult to predict, since its statistics may
not be reliable. therefore, the model uses the heuristic that any
feature which occurs less than 10 times in the data is unreliable,
and ignores features whose counts are less than 10.1 while there
are many smoothing algorithms which use techniques more
rigorous than a simple count cuto   , they have not yet been
investigated in conjunction with this tagger.

1except for features that look only at the current word, i.e., features of the

form wi =<word> and ti = <tag>. the count of 10 was chosen by inspection of
training and development data.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

14 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

15 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

15 / 128

wrapper-based feature selection

for each subset f     {1, 2, . . . d}, learn hwf for features {fd | d     f}.
2d     1 choices; so perform a search over subsets.
cons:

np-hard problem (amaldi and kann, 1998; davis et al., 1997)

must resort to greedy methods

even those require iterative calls to a black-box learner
danger of over   tting in choosing f.
(typically use development data or cross-validate.)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

16 / 128

della pietra et al. (1997) add features one at a time. step (3) involves
re-estimating parameters:

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

17 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

18 / 128

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

18 / 128

embedded methods for feature selection

formulate the learning problem as a trade-o    between

minimizing loss (   tting the training data, achieving good accuracy on
the training data, etc.)

choosing a desirable model (e.g., one with no more features than
needed)

n(cid:88)

n=1

min
w

1
n

l(w; xn, yn) +    (w)

key advantage: declarative statements of model    desirability    often lead
to well-understood, solvable optimization problems.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

19 / 128

useful papers on feature selection and sparsity

overview of many feature selection methods:
guyon and elissee    (2003)

greedy wrapper-based method used for max ent models in nlp:
della pietra et al. (1997)

early uses of sparsity in nlp:
kazama and tsujii (2003); goodman (2004)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

20 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

21 / 128

learning problem

recall that we formulate the learning problem as:

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

n(cid:88)
(cid:124)

i=1

+

l(w, xi , yi )

,

(cid:123)(cid:122)

total loss

(cid:125)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

22 / 128

id168s (i)

regression (y     r) typically uses the squared error loss:

(cid:16)

(cid:17)2

lse(w; x, y ) =

1
2

y     w(cid:62)f(x)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

23 / 128

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

23 / 128

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

23 / 128

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).
response vector: y = [y1, ..., yn ](cid:62).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

23 / 128

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).
response vector: y = [y1, ..., yn ](cid:62).
arguably, the most/best studied id168 (statistics, machine
learning, signal processing).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

23 / 128

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f(x, y ))

=     log
=     w(cid:62)f(x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f(x, y(cid:48)))

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

24 / 128

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f(x, y ))

=     log
=     w(cid:62)f(x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f(x, y(cid:48)))

partition function:

z (w, x) =

(cid:88)

y(cid:48)   y(x)

exp(w(cid:62)f(x, y(cid:48))).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

24 / 128

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f(x, y ))

=     log
=     w(cid:62)f(x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f(x, y(cid:48)))

partition function:

z (w, x) =

(cid:88)

y(cid:48)   y(x)

exp(w(cid:62)f(x, y(cid:48))).

related id168s: hinge loss (in id166) and the id88 loss.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

24 / 128

main id168s: summary

squared (id75)

log-linear (maxent, crf, logistic)

hinge (id166s)

id88

1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2
(cid:88)
(cid:0)w(cid:62)f(x, y(cid:48)) + c(y , y(cid:48))(cid:1)

exp(w(cid:62)f(x, y(cid:48)))

y(cid:48)   y

   w(cid:62)f(x, y ) + log

   w(cid:62)f(x, y ) + max
y(cid:48)   y

   w(cid:62)f(x, y ) + max
y(cid:48)   y

w(cid:62)f(x, y(cid:48))

(in the id166 loss, c(y , y(cid:48)) is a cost function.)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

25 / 128

main id168s: summary

squared (id75)

log-linear (maxent, crf, logistic)

hinge (id166s)

id88

1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2
(cid:88)
(cid:0)w(cid:62)f(x, y(cid:48)) + c(y , y(cid:48))(cid:1)

exp(w(cid:62)f(x, y(cid:48)))

y(cid:48)   y

   w(cid:62)f(x, y ) + log

   w(cid:62)f(x, y ) + max
y(cid:48)   y

   w(cid:62)f(x, y ) + max
y(cid:48)   y

w(cid:62)f(x, y(cid:48))

(in the id166 loss, c(y , y(cid:48)) is a cost function.)
the log-linear, hinge, and id88 losses are particular cases of general
family (martins et al., 2010).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

25 / 128

id173 formulations

(cid:98)w = arg min

w

n(cid:88)

n=1

tikhonov id173:

        (w) +

l(w; xn, yn)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

26 / 128

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

26 / 128

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

n=1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

26 / 128

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

equivalent, under mild conditions (namely convexity).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

26 / 128

n=1

id173

why regularize?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

27 / 128

id173

why regularize?

improve generalization by avoiding over-   tting.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

27 / 128

id173

why regularize?

improve generalization by avoiding over-   tting.
express prior knowledge about w.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

27 / 128

id173

why regularize?

improve generalization by avoiding over-   tting.
express prior knowledge about w.
select relevant features (via sparsity-inducing id173).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

27 / 128

id173

why regularize?

improve generalization by avoiding over-   tting.
express prior knowledge about w.
select relevant features (via sparsity-inducing id173).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

27 / 128

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

28 / 128

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

this interpretation underlies the id28 (lr) loss:
llr(w; xn, yn) =     log p (yn|xn; w).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

28 / 128

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

this interpretation underlies the id28 (lr) loss:
llr(w; xn, yn) =     log p (yn|xn; w).

same is true for the squared error (se) loss:
lse(w; xn, yn) = 1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2 =     log n(y|w(cid:62)f(x), 1)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

28 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).
pros: smooth and convex, thus benign for optimization.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).
pros: smooth and convex, thus benign for optimization.
cons: doesn   t promote sparsity (no explicit feature selection).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

classical regularizers: ridge

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)      

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

n=1

w

n(cid:88)

l(w; xn, yn) +    (w)

2

  
2

(cid:107)w(cid:107)2
2(cid:107)w(cid:107)2

(cid:1)

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).
pros: smooth and convex, thus benign for optimization.
cons: doesn   t promote sparsity (no explicit feature selection).
cons: only encodes trivial prior knowledge.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

29 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)

i=1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).

i=1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

in nlp: kazama and tsujii (2003); goodman (2004).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

in nlp: kazama and tsujii (2003); goodman (2004).
pros: encourages sparsity: embedded feature selection.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

in nlp: kazama and tsujii (2003); goodman (2004).
pros: encourages sparsity: embedded feature selection.
cons: convex, but non-smooth: challenging optimization.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

in nlp: kazama and tsujii (2003); goodman (2004).
pros: encourages sparsity: embedded feature selection.
cons: convex, but non-smooth: challenging optimization.
cons: only encodes trivial prior knowledge.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

30 / 128

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

31 / 128

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

31 / 128

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

contrast with the squared (cid:96)2 (ridge) regularizer (linear scaling):

(cid:98)w = arg min

w

(w     y )2 +

1
2

  
2

w 2 =

1

1 +   

y

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

31 / 128

the lasso and sparsity (ii)

why does the lasso yield sparsity?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

32 / 128

the lasso and sparsity (ii)

why does the lasso yield sparsity?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

32 / 128

norms: a quick review

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)

i |wi|)1 =(cid:80)

i |wi|.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)
(cid:107)w(cid:107)2 =(cid:0)(cid:80)

i |wi|)1 =(cid:80)
i |wi|2(cid:1)1/2 =(cid:112)(cid:80)

i |wi|.

i |wi|2.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)
(cid:107)w(cid:107)2 =(cid:0)(cid:80)
(cid:107)w(cid:107)p = ((cid:80)

i |wi|)1 =(cid:80)
i |wi|2(cid:1)1/2 =(cid:112)(cid:80)

i |wi|.

i |wi|2.

i |wi|p)1/p (called (cid:96)p norm, for p     1).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)
(cid:107)w(cid:107)2 =(cid:0)(cid:80)
(cid:107)w(cid:107)p = ((cid:80)

i |wi|.

i |wi|)1 =(cid:80)
i |wi|2(cid:1)1/2 =(cid:112)(cid:80)
i |wi|p)1/p (called (cid:96)p norm, for p     1).
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

i |wi|2.

(cid:107)w(cid:107)    = lim

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)
(cid:107)w(cid:107)2 =(cid:0)(cid:80)
(cid:107)w(cid:107)p = ((cid:80)

i |wi|.

i |wi|)1 =(cid:80)
i |wi|2(cid:1)1/2 =(cid:112)(cid:80)
i |wi|p)1/p (called (cid:96)p norm, for p     1).
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

i |wi|2.

(cid:107)w(cid:107)    = lim

fact: all norms are convex.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

norms: a quick review

a norm is a function satisfying:

(cid:107)  w(cid:107) = |  |(cid:107)w(cid:107), for any w (homogeneity);
(cid:107)w + w(cid:48)(cid:107)     (cid:107)w(cid:107) + (cid:107)w(cid:48)(cid:107), for any w, w(cid:48) (triangle inequality);
(cid:107)w(cid:107) = 0 if and only if w = 0.

examples of norms:

(cid:107)w(cid:107)1 = ((cid:80)
(cid:107)w(cid:107)2 =(cid:0)(cid:80)
(cid:107)w(cid:107)p = ((cid:80)

i |wi|.

i |wi|)1 =(cid:80)
i |wi|2(cid:1)1/2 =(cid:112)(cid:80)
i |wi|p)1/p (called (cid:96)p norm, for p     1).
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

i |wi|2.

(cid:107)w(cid:107)    = lim

fact: all norms are convex.
also important (but not a norm): (cid:107)w(cid:107)0 = lim
p   0

(cid:107)w(cid:107)p

p = |{i : wi

(cid:54)= 0}|

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

33 / 128

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

34 / 128

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

34 / 128

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

the    ideal    feature selection criterion (best subset):

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

(limit the number of features)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

34 / 128

relationship between (cid:96)1 and (cid:96)0 (ii)

the best subset selection problem

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

35 / 128

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

35 / 128

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem,

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

35 / 128

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem, also np-hard (muthukrishnan, 2005).

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

35 / 128

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem, also np-hard (muthukrishnan, 2005).

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

in some cases, one may replace (cid:96)0 with (cid:96)1 and obtain    similar    results:
central issue in compressive sensing (cs) (cand`es et al., 2006; donoho,
2006).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

35 / 128

take-home messages

sparsity is desirable for interpretability, computational savings, and
generalization
(cid:96)1-id173 gives an embedded method for feature selection
another view of (cid:96)1: a convex surrogate for direct penalization of
cardinality ((cid:96)0)
there are compelling algorithmic reasons for using convex surrogates
like (cid:96)1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

36 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

37 / 128

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

38 / 128

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

our main question: how can we promote less trivial sparsity patterns?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

38 / 128

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

our main question: how can we promote less trivial sparsity patterns?

we   ll talk about structured sparsity and group-lasso id173.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

38 / 128

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

39 / 128

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

group sparsity: discard entire groups of features

density inside each group
sparsity with respect to the groups which are selected
choice of groups: prior knowledge about the intended sparsity patterns

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

39 / 128

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

group sparsity: discard entire groups of features

density inside each group
sparsity with respect to the groups which are selected
choice of groups: prior knowledge about the intended sparsity patterns

leads to statistical gains if the prior assumptions are correct (stojnic
et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

39 / 128

tons of uses

feature template selection (martins et al., 2011b)

id72 (caruana, 1997; obozinski et al., 2010)

multiple kernel learning (lanckriet et al., 2004)

learning the structure of id114 (schmidt and murphy,
2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

40 / 128

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

41 / 128

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

41 / 128

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

goal: push entire columns to have zero weights

the groups are the columns of the grid

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

41 / 128

example 1: sparsity with multiple classes

assume the feature map decomposes as f(x, y ) = f(x)     ey
in words: we   re conjoining each input feature with each output class

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

42 / 128

input featureslabelsexample 1: sparsity with multiple classes

assume the feature map decomposes as f(x, y ) = f(x)     ey
in words: we   re conjoining each input feature with each output class

   standard    sparsity is wasteful   we still need to hash all the input features

what we want: discard some input features, along with each class they
conjoin with

solution: one group per input feature

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

42 / 128

input featureslabelsexample 2: id72

(caruana, 1997; obozinski et al., 2010)

same thing, except now rows are tasks and columns are features

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

43 / 128

shared featurestasksexample 2: id72

(caruana, 1997; obozinski et al., 2010)

same thing, except now rows are tasks and columns are features

what we want: discard features that are irrelevant for all tasks

solution: one group per feature

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

43 / 128

shared featurestasksgroup sparsity

d features

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

44 / 128

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

44 / 128

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1 (cid:107)wm(cid:107)2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

44 / 128

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1 (cid:107)wm(cid:107)2

intuitively: the (cid:96)1 norm of the (cid:96)2 norms
technically, still a norm (called a mixed norm, denoted (cid:96)2,1)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

44 / 128

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

intuitively: the (cid:96)1 norm of the (cid:96)2 norms
technically, still a norm (called a mixed norm, denoted (cid:96)2,1)
  m: prior weight for group gm (di   erent groups have di   erent sizes)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

44 / 128

id173 formulations (reminder)

tikhonov id173:

   (w) +

l(w; xn, yn)

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

ivanov id173

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

equivalent, under mild conditions (namely convexity).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

45 / 128

n=1

lasso versus group-lasso

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

46 / 128

lasso versus group-lasso

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

46 / 128

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

47 / 128

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

group sparsity corresponds to r = 1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

47 / 128

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

group sparsity corresponds to r = 1

this talk: q = 2
however q =     is also popular (quattoni et al., 2009; gra  ca et al., 2009;
wright et al., 2009; eisenstein et al., 2011)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

47 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

48 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

48 / 128

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

49 / 128

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

49 / 128

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

(cid:96)2-id173: one large group g1 = {1, . . . , d}
(cid:96)1-id173: d singleton groups gd = {d}

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

49 / 128

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

(cid:96)2-id173: one large group g1 = {1, . . . , d}
(cid:96)1-id173: d singleton groups gd = {d}

examples of non-trivial groups:

label-based groups (groups are columns of a matrix)

template-based groups (next)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

49 / 128

example: feature template selection

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)
the
dt
b-np

explore

vb
i-vp

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)
the
dt
b-np

explore

vb
i-vp

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"the feature""explore the""dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"the feature""explore the""dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

"dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

50 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

51 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

51 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

what is the sparsity pattern?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

what is the sparsity pattern?
if a group is discarded, all its descendants are also discarded

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

52 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

53 / 128

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

53 / 128

graph-structured groups

in general: groups can be represented as a directed acyclic graph

set inclusion induces a partial order on groups (jenatton et al., 2009)
feature space becomes a poset
sparsity patterns: given by this poset

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

54 / 128

example: coarse-to-   ne id173

1 de   ne a partial order between basic feature templates (e.g., p0 (cid:22) w0)
2 extend this partial order to all templates by lexicographic closure:

p0 (cid:22) p0p1 (cid:22) w0w1

goal: only include    ner features if coarser ones are also in the model

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

55 / 128

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

56 / 128

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex
choice of groups: problem dependent, opportunity to use prior
knowledge to favour certain structural patterns

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

56 / 128

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex
choice of groups: problem dependent, opportunity to use prior
knowledge to favour certain structural patterns
next: algorithms
we   ll see that optimization is easier with non-overlapping or
tree-structured groups than with arbitrary overlaps

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

56 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

57 / 128

learning the model

recall that learning involves solving

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

n(cid:88)
(cid:124)

i=1

+

l(w, xi , yi )

,

(cid:123)(cid:122)

total loss

(cid:125)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

58 / 128

learning the model

recall that learning involves solving

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

n(cid:88)
(cid:124)

i=1

+

l(w, xi , yi )

,

(cid:123)(cid:122)

total loss

(cid:125)

we   ll address two kinds of optimization algorithms:

batch algorithms (attacks the complete problem);

online algorithms (uses the training examples one by one)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

58 / 128

key concepts: convex functions

f is a convex function if:

          [0, 1], x and x(cid:48)     domain(f )
f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

59 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

60 / 128

batch algorithms

subgradient methods

proximal methods

alternating direction method of multipliers

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

61 / 128

key concepts: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

62 / 128

key concepts: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}

linear lower bound

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

62 / 128

key concepts: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

62 / 128

key concepts: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

non-di   erentiable case

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

62 / 128

key concepts: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

non-di   erentiable case

notation:      f (x) is a subgradient of f at x

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

62 / 128

subgradient methods

minw    (w) +   (w), where   (w) =(cid:80)n

i=1 l(w, xi , yi ) (loss)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

63 / 128

subgradient methods

minw    (w) +   (w), where   (w) =(cid:80)n

i=1 l(w, xi , yi ) (loss)

subgradient methods were invented by shor in the 1970   s (shor, 1985):

input: stepsize sequence (  t)t
initialize w
for t = 1, 2, . . . do

t=1

(sub-)gradient step: w     w       t

(cid:0)         (w) +        (w)(cid:1)

end for

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

63 / 128

subgradient methods

minw    (w) +   (w), where   (w) =(cid:80)n

i=1 l(w, xi , yi ) (loss)

subgradient methods were invented by shor in the 1970   s (shor, 1985):

input: stepsize sequence (  t)t
initialize w
for t = 1, 2, . . . do

t=1

(sub-)gradient step: w     w       t

(cid:0)         (w) +        (w)(cid:1)

end for

key disadvantages:

the step size   t needs to be annealed for convergence: very slow!
doesn   t explicitly capture the sparsity promoted by sparse regularizers.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

63 / 128

key concepts: proximity operators

let     : rd       r be a convex function.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

64 / 128

key concepts: proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2

2 is strictly convex.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

64 / 128

key concepts: proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2
classical examples:

2 is strictly convex.

squared (cid:96)2 id173,    (w) =   

2: scaling operation

2(cid:107)w(cid:107)2

prox   (w) =

1

1 +   

w

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

64 / 128

key concepts: proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2
classical examples:

2 is strictly convex.

squared (cid:96)2 id173,    (w) =   

2: scaling operation

2(cid:107)w(cid:107)2

prox   (w) =

1

w

1 +   

(cid:96)1 id173,    (w) =   (cid:107)w(cid:107)1: soft-thresholding;

prox   (w) = soft(w,   )

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

64 / 128

key concepts: proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

65 / 128

key concepts: proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

(cid:96)2 id173,    (w) =   (cid:107)w(cid:107)2: vector soft thresholding

(cid:26) 0
    (cid:107)w(cid:107)       
w(cid:107)w(cid:107) ((cid:107)w(cid:107)       )     (cid:107)w(cid:107) >   

prox   (w) =

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

65 / 128

key concepts: proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

(cid:96)2 id173,    (w) =   (cid:107)w(cid:107)2: vector soft thresholding

prox   (w) =

(cid:26) 0
    (cid:107)w(cid:107)       
w(cid:107)w(cid:107) ((cid:107)w(cid:107)       )     (cid:107)w(cid:107) >   
    w     s
+        w (cid:54)    s

(cid:26) 0

indicator function,    (w) =   s(w) =

prox   (w) = ps(w)

euclidean projection

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

65 / 128

m(cid:88)

   m(wm)

key concepts: proximity operators (iii)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.
indices in gm.

m=1

wm is a sub-vector of w with the

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

66 / 128

key concepts: proximity operators (iii)

m(cid:88)

   m(wm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.
indices in gm.

m=1

wm is a sub-vector of w with the

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]m = prox   m(wm)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

66 / 128

key concepts: proximity operators (iii)

m(cid:88)

   m(wm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.
indices in gm.

m=1

wm is a sub-vector of w with the

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]m = prox   m(wm)

tree-structured groups: (two groups are either non-overlapping or
one contais the other) prox    can be computed recursively (jenatton
et al., 2011).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

66 / 128

key concepts: proximity operators (iii)

m(cid:88)

   m(wm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.
indices in gm.

m=1

wm is a sub-vector of w with the

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]m = prox   m(wm)

tree-structured groups: (two groups are either non-overlapping or
one contais the other) prox    can be computed recursively (jenatton
et al., 2011).
arbitrary groups:

for    j (wm) = (cid:107)wm(cid:107)2: solved via convex smooth optimization (yuan
et al., 2011).
sequential proximity steps (martins et al., 2011a) (more later).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

66 / 128

proximal gradient

recall the problem: min

w

   (w) +   (w)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

projected gradient is a particular case, for prox    = ps.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

projected gradient is a particular case, for prox    = ps.
often called iterative shrinkage thresholding (ist).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

projected gradient is a particular case, for prox    = ps.
often called iterative shrinkage thresholding (ist).

can be derived with di   erent tools:

expectation-maximization (em) (figueiredo and nowak, 2003);

majorization-minimization (daubechies et al., 2004);

forward-backward splitting (combettes and wajs, 2006);

separable approximation (wright et al., 2009).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

67 / 128

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

68 / 128

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
monotonicity: if   t     1/l, then   (wt+1) +    (wt+1)       (wt) +    (wt).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

68 / 128

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
monotonicity: if   t     1/l, then   (wt+1) +    (wt+1)       (wt) +    (wt).
convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(cid:18) 1

(cid:19)

 

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

68 / 128

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

69 / 128

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))

= wt + bt   1

1+4 b2
t
2

(wt     wt   1)

bt+1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

69 / 128

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

(wt     wt   1)

bt+1

1+4 b2
t
2

= wt + bt   1

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))
(cid:19)

(cid:18) 1   

 

convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(vs o(1/ ) for ist)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

69 / 128

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

(wt     wt   1)

bt+1

1+4 b2
t
2

= wt + bt   1

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))
(cid:19)

(cid:18) 1   

 

convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(vs o(1/ ) for ist)

other ist variants: nesterov   s method (nesterov, 2007), sparsa (wright
et al., 2009), twist (two-step ist; bioucas-dias and figueiredo, 2007).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

69 / 128

alternating direction method of multipliers

combine bene   ts of id209 and augmented lagrangian
methods for constrained optimization (hestenes, 1969; powell, 1969).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

70 / 128

alternating direction method of multipliers

combine bene   ts of id209 and augmented lagrangian
methods for constrained optimization (hestenes, 1969; powell, 1969).

key ideas

break down the optimization problem into subproblems, each
depending on a subset of w.
each subproblem p receives a    copy    of the subvector w, denoted by
vp.
encode constraints forcing each vp to    agree    with the global solution
w.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

70 / 128

alternating direction method of multipliers

combine bene   ts of id209 and augmented lagrangian
methods for constrained optimization (hestenes, 1969; powell, 1969).

key ideas

break down the optimization problem into subproblems, each
depending on a subset of w.
each subproblem p receives a    copy    of the subvector w, denoted by
vp.
encode constraints forcing each vp to    agree    with the global solution
w.

particularly suitable for distributed optimization.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

70 / 128

alternating direction method of multipliers

original problem min

w

   (w) +   (w) where    (w) =

   m(wm) .

m(cid:88)

m=1

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

71 / 128

m(cid:88)

m=1

alternating direction method of multipliers

original problem min

w

   (w) +   (w) where    (w) =

   m(wm) .

admm objective min
w,v

   (v) +   (w) subject to av + bw = c

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

71 / 128

m(cid:88)

m=1

alternating direction method of multipliers

original problem min

w

   (w) +   (w) where    (w) =

   m(wm) .

admm objective min
w,v

   (v) +   (w) subject to av + bw = c

for example, in the overlapping group lasso case, we have a = i and
c = 0. the constraint becomes v =    bw.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

71 / 128

m(cid:88)

m=1

alternating direction method of multipliers

original problem min

w

   (w) +   (w) where    (w) =

   m(wm) .

admm objective min
w,v

   (v) +   (w) subject to av + bw = c

for example, in the overlapping group lasso case, we have a = i and
c = 0. the constraint becomes v =    bw.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

71 / 128

alternating direction method of multipliers

the augmented lagrangian is:

   (v) +  (w) + u(cid:62)(av + bw     c) +   

2(cid:107)av + bw     c(cid:107)2

2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

72 / 128

alternating direction method of multipliers

the augmented lagrangian is:

   (v) +  (w) + u(cid:62)(av + bw     c) +   

2(cid:107)av + bw     c(cid:107)2

2

admm iteratively solves:

  w = arg minw   (w) + u(cid:62)bw +   
  v = arg minv    (v) + u(cid:62)av +   
u = u +   (av + bw     c)

2(cid:107)av + bw     c(cid:107)2
2(cid:107)av + bw     c(cid:107)2

2

2

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

72 / 128

alternating direction method of multipliers

the augmented lagrangian is:

   (v) +  (w) + u(cid:62)(av + bw     c) +   

2(cid:107)av + bw     c(cid:107)2

2

admm iteratively solves:

  w = arg minw   (w) + u(cid:62)bw +   
  v = arg minv    (v) + u(cid:62)av +   
u = u +   (av + bw     c)

2(cid:107)av + bw     c(cid:107)2
2(cid:107)av + bw     c(cid:107)2

2

2

key advantage: the minimization of v can be done in parallel.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

72 / 128

alternating direction method of multipliers

convergence of admm in theory (boyd et al., 2010)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

73 / 128

alternating direction method of multipliers

convergence of admm in theory (boyd et al., 2010)

assumptions:

   and     are closed, proper, and convex.

the unaugmented lagrangian has a saddle point

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

73 / 128

alternating direction method of multipliers

convergence of admm in theory (boyd et al., 2010)

assumptions:

   and     are closed, proper, and convex.

the unaugmented lagrangian has a saddle point

as t        , we have:

residual convergence: av + bw     c     0.
primal convergence:   (wt) +    (vt)     p    where p    is the optimal
value.
dual convergence: ut     u   .

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

73 / 128

alternating direction method of multipliers

admm can handle various kinds of regularizers by adapting a and b.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

74 / 128

alternating direction method of multipliers

admm can handle various kinds of regularizers by adapting a and b.

admm is well suited for structured sparse models with group overlaps
because we can design a and b such that    (v) no longer has overlapping
groups. hence, we can solve each subproblem separately in parallel.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

74 / 128

alternating direction method of multipliers

admm can handle various kinds of regularizers by adapting a and b.

admm is well suited for structured sparse models with group overlaps
because we can design a and b such that    (v) no longer has overlapping
groups. hence, we can solve each subproblem separately in parallel.

practical considerations:

admm can be slow to converge in practice, but tens of iterations are
often enough to produce good results.

admm only produces weakly sparse solution (we only get sparsity in
the limit).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

74 / 128

alternating direction method of multipliers

recall that the admm objective is:

min
w,v

   struct(v) +   (w) subject to av + bw = c

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

75 / 128

alternating direction method of multipliers

recall that the admm objective is:

min
w,v

   struct(v) +   (w) subject to av + bw = c

we can introduce an additional lasso penalty (sparse group lasso;
friedman et al., 2010):

min
w,v

   struct(v) +    lasso(w) +   (w) subject to av + bw = c

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

75 / 128

alternating direction method of multipliers

recall that the admm objective is:

min
w,v

   struct(v) +   (w) subject to av + bw = c

we can introduce an additional lasso penalty (sparse group lasso;
friedman et al., 2010):

min
w,v

   struct(v) +    lasso(w) +   (w) subject to av + bw = c

we get sparse solutions and can still guarantee convergence (yogatama
and smith, 2014a).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

75 / 128

summary of algorithms

prox-grad (ist)
fista
admm

converges?

(cid:88)
(cid:88)
(cid:88)

rate?
   
o(1/ )
o(1/
 )
o(1/ )

sparse? groups? overlaps?
not easy
not easy

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
no

(cid:88)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

76 / 128

summary of algorithms

prox-grad (ist)
fista
admm

converges?

(cid:88)
(cid:88)
(cid:88)

rate?
   
o(1/ )
o(1/
 )
o(1/ )

sparse? groups? overlaps?
not easy
not easy

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
no

(cid:88)

note that we can still get sparsity for admm with sparse group lasso
(yogatama and smith, 2014a).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

76 / 128

some stu    we didn   t talk about

shooting method (fu, 1998);

grafting (perkins et al., 2003) and grafting-light (zhu et al., 2010);
(afonso et al., 2010; figueiredo and bioucas-dias, 2011).

forward stagewise regression (hastie et al., 2007).

homotopy/continuation method (osborne et al., 2000; efron et al.,
2004; figueiredo et al., 2007; hale et al., 2008).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

77 / 128

some stu    we didn   t talk about

shooting method (fu, 1998);

grafting (perkins et al., 2003) and grafting-light (zhu et al., 2010);
(afonso et al., 2010; figueiredo and bioucas-dias, 2011).

forward stagewise regression (hastie et al., 2007).

homotopy/continuation method (osborne et al., 2000; efron et al.,
2004; figueiredo et al., 2007; hale et al., 2008).

next: we   ll talk about online algorithms.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

77 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

78 / 128

why online?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

why online?

1 suitable for large datasets

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

why online?

1 suitable for large datasets

2 suitable for id170

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region
4 slower convergence, but this is    ne in machine learning

cf.    the tradeo   s of large scale learning    (bottou and bousquet, 2007)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region
4 slower convergence, but this is    ne in machine learning

cf.    the tradeo   s of large scale learning    (bottou and bousquet, 2007)

what we will say can be straighforwardly extended to the mini-batch case.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

79 / 128

plain stochastic (sub-)id119

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

empirical loss

l(w, xi , yi )

,

(cid:125)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

80 / 128

plain stochastic (sub-)id119

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

empirical loss

l(w, xi , yi )

,

(cid:125)

input: stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

take training pair (xt, yt)
(sub-)gradient step: w     w       t

end for

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

80 / 128

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

81 / 128

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

81 / 128

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

(cid:96)1-id173    (w) =   (cid:107)w(cid:107)1 =            (w) =   sign(w)

w     w       t  sign(w)

      t      l(w; xt, yt)

(cid:124)

(cid:123)(cid:122)

(cid:125)

constant penalty

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

81 / 128

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

(cid:96)1-id173    (w) =   (cid:107)w(cid:107)1 =            (w) =   sign(w)

w     w       t  sign(w)

      t      l(w; xt, yt)

(cid:124)

(cid:123)(cid:122)

(cid:125)

constant penalty

problem: iterates are never sparse!

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

81 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)2-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

82 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

plain sgd with (cid:96)1-id173

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

83 / 128

   sparse    online algorithms

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

84 / 128

   sparse    online algorithms

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

84 / 128

truncated gradient (langford et al., 2009)

input: laziness coe   cient k , stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

take training pair (xt, yt)
(sub-)gradient step: w     w       t      l(  ; xt, yt)
if t/k is integer then

truncation step: w     w     sign(w) (|w|       tk    )

(cid:123)(cid:122)

soft-thresholding

(cid:125)

(cid:124)

end if
end for

take gradients only with respect to the loss
every k rounds: a    lazy    soft-thresholding step
langford et al. (2009) also suggest other forms of truncation
converges to  -accurate objective after o(1/ 2) iterations

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

85 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

truncated gradient (langford et al., 2009)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

86 / 128

   sparse    online algorithms

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

87 / 128

online forward-backward splitting (duchi and

singer, 2009)

input: stepsize sequences (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(w; xt, yt)
proximal step: w     prox  t    (w)

end for

generalizes truncated gradient to arbitrary regularizers    

can tackle non-overlapping or hierarchical group-lasso, but arbitrary
overlaps are di   cult to handle (more later)

practical drawback: without a laziness parameter, iterates are
usually not very sparse
converges to  -accurate objective after o(1/ 2) iterations

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

88 / 128

   sparse    online algorithms

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

89 / 128

regularized dual averaging (xiao, 2010)

input: coe   cient   0
initialize w = 0
for t = 1, 2, . . . do

take training pair (xt, yt)
gradient step: s     s +    l(w; xt, yt)
proximal step: w       0

t    prox   (   s/t)

   

end for

based on the dual averaging technique (nesterov, 2009)
in practice: quite e   ective at getting sparse iterates (the proximal
steps are not vanishing)
o(c1/ 2 + c2/
and c2 is the variance of the stochastic gradients
drawback: requires storing two vectors (w and s), and s is not sparse

 ) convergence, where c1 is a lipschitz constant,

   

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

90 / 128

what about group sparsity?

both online forward-backward splitting (duchi and singer, 2009) and
regularized dual averaging (xiao, 2010) can handle groups

all that is necessary is to compute prox   (w)

easy for non-overlapping and tree-structured groups
but what about general overlapping groups?

martins et al. (2011a): a prox-grad algorithm that can handle arbitrary
overlapping groups

decompose    (w) =(cid:80)j

j=1    j (w) where each    j is non-overlapping

then apply prox   j sequentially
still convergent (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

91 / 128

   sparse    online algorithms

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

92 / 128

online proximal gradient (martins et al., 2011a)

input: gravity sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, stepsize sequence (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(  ; xt, yt)
sequential proximal steps:
for j = 1, 2, . . . do

w     prox  t   t    j (w)

end for

end for

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

93 / 128

online proximal gradient (martins et al., 2011a)

input: gravity sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, stepsize sequence (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(  ; xt, yt)
sequential proximal steps:
for j = 1, 2, . . . do

w     prox  t   t    j (w)

end for

end for

pac convergence.  -accurate solution after t     o(1/ 2) rounds
computational e   ciency. each gradient step is linear in the
number of features that    re.
each proximal step is linear in the number of groups m.
both are independent of d.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

93 / 128

implementation tricks (martins et al., 2011b)

budget driven shrinkage. instead of a id173 constant,
specify a budget on the number of selected groups. each proximal
step sets   t to meet this target.
sparseptron. let l(w) = w(cid:62)(f(x,   y )     f(x, y )) be the id88
loss. the algorithm becomes id88 with shrinkage.

debiasing. run a few iterations of sparseptron to identify the
relevant groups. then run a unregularized learner at a second stage.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

94 / 128

0510150246x 106# epochs# features  mirasparceptron + mira (b=30)implementation tricks (martins et al., 2011b)

budget driven shrinkage. instead of a id173 constant,
specify a budget on the number of selected groups. each proximal
step sets   t to meet this target.
sparseptron. let l(w) = w(cid:62)(f(x,   y )     f(x, y )) be the id88
loss. the algorithm becomes id88 with shrinkage.

debiasing. run a few iterations of sparseptron to identify the
relevant groups. then run a unregularized learner at a second stage.

memory e   ciency. only a
small active set of features need
to be maintained. entire groups
can be deleted after each
proximal step.
many irrelevant features are
never instantiated.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

94 / 128

0510150246x 106# epochs# features  mirasparceptron + mira (b=30)summary of algorithms

sparse? groups? overlaps?
not easy
not easy

prox-grad (ist)
fista
admm
online subgradient
truncated gradient
fobos
rda
online prox-grad

converges?

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

rate?
   
o(1/ )
o(1/
 )
o(1/ )
o(1/ 2)
o(1/ 2)
o(1/ 2)
o(1/ 2)
o(1/ 2)

(cid:88)
(cid:88)
no
no
(cid:88)

sort of

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
no
(cid:88)
(cid:88)
(cid:88)

(cid:88)
no
no

not easy
not easy

(cid:88)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

95 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

96 / 128

applications of structured sparsity in nlp

1 non-overlapping groups by feature template

2 tree-structured groups: coarse-to-   ne

3 arbitrarily overlapping groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

97 / 128

applications of structured sparsity in nlp

1 non-overlapping groups by feature template

2 tree-structured groups: coarse-to-   ne

3 arbitrarily overlapping groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

97 / 128

martins et al. (2011b): group by template

feature templates provide a straightforward way to de   ne non-overlapping
groups.

to achieve group sparsity, we optimize:

n(cid:88)

n=1

(cid:123)(cid:122)

min
w

1
n

(cid:124)

l(w; xn, yn)

+    (w)

(cid:124)(cid:123)(cid:122)(cid:125)

regularizer

(cid:125)

empirical loss

where we use the (cid:96)2,1 norm:

   (w) =   

m(cid:88)

m=1

  m(cid:107)wm(cid:107)2

for m groups/templates.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

98 / 128

id170 tasks (martins et al., 2011b)

chunking (conll 2000 shared task; sang and buchholz, 2000)
+0.5 f1 with 30 groups (out of 96)
ner (conll 2002/3 shared tasks on spanish, dutch, english; sang,
2002; sang and de meulder, 2003)
+1   2 f1 with 200 groups (out of 452)
id33 (conll-x shared task on several languages;
buchholz and marsi, 2006), 684 feature templates based on
mcdonald et al. (2005)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

99 / 128

which features get selected?

qualitative analysis of selected templates:

arabic danish

japanese

slovene

spanish

turkish

bilexical
lex.     pos
pos     lex.
pos     pos
middle pos
shape
direction
distance

++
+
++

++
++

++

+

+

++
++
+
+

+
+
++
++
++
+
+

+
++
++
+
+

+

+

++

+
+

+

++

+
+

(empty: none or very few templates selected; +: some templates
selected; ++: most or all templates selected.)

morphologically-rich languages with small datasets (turkish and
slovene) avoid lexical features.

in japanese, contextual pos appear to be especially relevant.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

100 / 128

which features get selected?

qualitative analysis of selected templates:

arabic danish

japanese

slovene

spanish

turkish

bilexical
lex.     pos
pos     lex.
pos     pos
middle pos
shape
direction
distance

++
+
++

++
++

++

+

+

++
++
+
+

+
+
++
++
++
+
+

+
++
++
+
+

+

+

++

+
+

+

++

+
+

(empty: none or very few templates selected; +: some templates
selected; ++: most or all templates selected.)

morphologically-rich languages with small datasets (turkish and
slovene) avoid lexical features.

in japanese, contextual pos appear to be especially relevant.
take this with a grain of salt: some patterns may be properties of
the datasets, not the languages!

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

100 / 128

sociolinguistic association discovery

(eisenstein et al., 2011)

dataset:

geotagged tweets from 9,250 authors
mapping of locations to the u.s. census    zip code tabulation areas
(zctas)
a ten-dimensional vector of statistics on demographic attributes

can we learn a compact set of terms used on twitter that associate
with demographics?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

101 / 128

sociolinguistic association discovery

(eisenstein et al., 2011)

setup: multi-output regression.

xn is a p-dimensional vector of independent variables; matrix is
x     rn  p
yn is a t -dimensional vector of dependent variables; matrix is
y     rn  t
wp,t is the regression coe   cient for the pth variable in the tth task;
matrix is w     rp  t
regularized objective with squared error loss typical for regression:

   (w) + (cid:107)y     xw(cid:107)2

f

min
w

regressions are run in both directions.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

102 / 128

structured sparsity with (cid:96)   ,1

drive entire rows of w to zero (turlach et al., 2005):    some
predictors are useless for any task   

t(cid:88)

t=1

   (w) =   

max

p

wp,t

optimization with blockwise coordinate ascent (liu et al., 2009) and
some tricks to maintain sparsity (eisenstein et al., 2011)
see also: duh et al. (2010) used multitask regression and (cid:96)2,1 to
select features useful for reranking across many instances (application
in machine translation).

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

103 / 128

predicting demographics from text

(eisenstein et al., 2011)

predict 10-dimensional zcta characterization from words tweeted in
that region (vocabulary is p = 5, 418)
measure pearson   s correlation between prediction and correct value
(average over tasks, cross-validated test sets)
compare with truncated svd, greatest variance across authors, most
frequent words

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

104 / 128

1021030.160.180.20.220.240.260.28number of featuresaverage correlation  multi   output lassosvdhighest variancemost frequentpredictive words (eisenstein et al., 2011)

.

m
a

.
r
f
a

e
t
i
h
w
-

.
g
n
a

.
g
n
a

l

.
g
n
a

l

l

n
.
a
p
b
s
i
r
h
u
+ - + + +

.
n
a
p
s

r
e
h
t
o

.
g
n
e

- + - +
-
-

+ - + - +
- + -

- + -
- + -
- +
- + -

- +
- + -

+

- + -
+ - + -

-
-

-

+

-
-

-

-
-

+ +
-

+ - + -

- + + - + + +
- + + - + + +
- + + - + +
+ - + +
-
-
+ - + +
- + + - + + +
-
+

+ - +

- +

-

- -
;)
:(
:)
:d
as
awesome + -
break
campus
dead
hell
shit
train
will
would
atlanta
famu
harlem
bbm
lls
lmaoo
lmaooo
lmaoooo
lmfaoo
lmfaooo
lml
odee

- +

y
l
i

m
a
f

r
e
t
n
e
r

.
c
n

i

.
d
e
m

omw
smfh
smh
w|

.

m
a

.
g
n
a

.
g
n
a

l

.
g
n
a

l

l

.
r
f
a

.
g
n
e

.
p
s
i
h

r
e
h
t
o

.
n
a
p
s

n
e
a
t
i
b
h
r
w
u
- + + - + + +
- + + - + + +
- +
+
+ - + + +
-

- +

+

+
+

-

+
+

-

+
+
+
+
+
+ -
+

-

+ - +

+ - +
- + - +
- + - +

con
la
si
- +
dats
- + + - + + +
deadass
haha
+ -
hahah
+ -
hahaha + -
ima
madd
nah
ova
sis
skool
wassup
wat
ya
yall
yep
yoo
yooo

-
-
-
- +
- +
- +
+ +
- + + - + + +
- + + - + + +
- +
- +

-
- + + - + + +
- +

+
+ - + +

- + -

- +

-

-

-

-

y
l
i

m
a
f

+

.
c
n

i

.
d
e
m

r
e
t
n
e
r

+
+
+
+

+

+ -
+
-

- +
+

+
+
+
+ -
+ -
+ -
+

-
+
+

table: demographically-indicative terms discovered by multi-output sparse
regression. statistically signi   cant (p < .05) associations are marked (+/-).

martins, yogatama, smith, figueiredo (ist, cmu)

http://tiny.cc/ssnlp14
signi   cant p < 0.05 positive (+) and negative (-) associations in a

structured sparsity in nlp

105 / 128

non-overlapping groups for    some    ambiguity

learning mappings from word types to labels (pos or semantic predicates)

semisupervised lexicon expansion with graph-based learning (das and
smith, 2012)

elitist lasso (squared (cid:96)1,2; kowalski and torr  esani, 2009) for per-word
sparsity

(cid:32)(cid:88)

(cid:88)

  

|wv ,y|

(cid:33)2

v

y

where v is a word and y is a label.
+3% accuracy on unknown-word frame prediction, with 35% as many
lexicon entries

unsupervised id52 with posterior id173 (gra  ca et al.,
2009)

incorporates (cid:96)   ,1 norm
+2   7% accuracy on 1-many pos evaluation

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

106 / 128

applications of structured sparsity in nlp

1 non-overlapping groups by feature template

2 tree-structured groups: coarse-to-   ne

3 arbitrarily overlapping groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

107 / 128

log-linear language models

(nelakanti et al., 2013)

setup: multinomial id28 (della pietra et al., 1997)

p(y | x) =

(cid:80)

exp(w(cid:62)
y f(x))
v   v exp(w(cid:62)

v f(x))

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

108 / 128

log-linear language models

(nelakanti et al., 2013)

setup: multinomial id28 (della pietra et al., 1997)

p(y | x) =

(cid:80)

exp(w(cid:62)
y f(x))
v   v exp(w(cid:62)

v f(x))

regularized objective with logistic loss:

    n(cid:88)

i=1

min
w

log p(yi | x1:k ; w) +    (w)

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

108 / 128

log-linear language models

(nelakanti et al., 2013)

setup: multinomial id28 (della pietra et al., 1997)

p(y | x) =

(cid:80)

exp(w(cid:62)
y f(x))
v   v exp(w(cid:62)

v f(x))

regularized objective with logistic loss:

    n(cid:88)

i=1

min
w

log p(yi | x1:k ; w) +    (w)

there are many choices for    (w). a key consideration is that the size of
w increases rapidly as k gets bigger.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

108 / 128

log-linear language models

(nelakanti et al., 2013)

encode history su   xes from length 0 to k in a tree; each is a feature.
tree-structured penalty: a longer su   x can only be included if all its
shorter su   xes are included.
can use (cid:96)2,1 or (cid:96)   ,1 norm

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

109 / 128

experimental results: ap-news

good generalization results (perplexity):

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

110 / 128

experimental results: ap-news

small model size:

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

111 / 128

groups from word clusters

(yogatama and smith, 2014a)

task: text classi   cation

model: bag-of-words id28

hierarchical clusters from brown et al. (1992): include the words in a
cluster only if its parent cluster is included.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

112 / 128

brown et al. (1992) clusters

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

113 / 128

regularize or add features?

20-newsgroups binary tasks:

dataset
science
sports
religion
computer

baseline
91.90 (ridge)
93.71 (elastic)
92.47 (ridge)
87.13 (elastic)

+ brown features

brown

ridge
lasso
90.51
86.96
82.66
88.94
94.98 96.93
55.72 96.65

elastic group lasso
91.14
85.43
96.93
67.57

93.04
93.71
92.89
86.36

caveat: we ought to use more data to learn the clusters!

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

114 / 128

applications of structured sparsity in nlp

1 non-overlapping groups by feature template

2 tree-structured groups: coarse-to-   ne

3 arbitrarily overlapping groups

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

115 / 128

groups from data

(yogatama and smith, 2014b)

task: text classi   cation

model: bag-of-words id28
groups: one group for every sentence in every training-set document

intuition: only some sentences are relevant
past work used latent    relevance    variables (yessenalina et al., 2010;
tackstrom and mcdonald, 2011)

use admm to handle thousands/millions of overlapping groups.
copy weights allow inspection to see which training sentences are
   selected   
additional (cid:96)1 penalty for strong sparsity

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

116 / 128

topic classi   cation (ibm vs. mac)

bars show log-odds e   ect of removing the sentence: sentence, elastic,
ridge, lasso.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

117 / 128

id31

(amazon dvds; blitzer et al., 2007)

bars show log-odds e   ect of removing the sentence: sentence, elastic,
ridge, lasso.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

118 / 128

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

batch algorithms

online algorithms

5 applications

6 conclusions

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

119 / 128

summary

sparsity is desirable in nlp: feature selection, runtime, memory
footprint, interpretability
beyond plain sparsity: structured sparsity can be promoted through
group-lasso id173

choice of groups re   ects prior knowledge about the desired sparsity
patterns.

we have seen examples for feature template selection, tree structures,
and data-driven groups, but many more are possible!

small/medium scale: many batch algorithms available, with fast
convergence (ist, fista, sparsa, ...)

large scale: distributed optimization algorithms (admm) or online
proximal-gradient algorithms suitable to explore large feature spaces

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

120 / 128

thank you!

questions?

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

121 / 128

acknowledgments

national science foundation (usa), career grant iis-1054319

funda  c  ao para a ci  encia e tecnologia (portugal), grants
pest-oe/eei/la0008/2011 and ptdc/eei-sii/2312/2012.

funda  c  ao para a ci  encia e tecnologia and information and
communication technologies institute (portugal/usa), through the
cmu-portugal program.

priberam: qren/por lisboa (portugal), eu/feder programme,
intelligo project, contract 2012/24803.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

122 / 128

references i

afonso, m., bioucas-dias, j., and figueiredo, m. (2010). fast image recovery using variable splitting and constrained

optimization. ieee transactions on image processing, 19:2345   2356.

amaldi, e. and kann, v. (1998). on the approximation of minimizing non zero variables or unsatis   ed relations in linear

systems. theoretical computer science, 209:237   260.

bakin, s. (1999). adaptive regression and model selection in data mining problems. phd thesis, australian national university.

beck, a. and teboulle, m. (2009). a fast iterative shrinkage-thresholding algorithm for linear inverse problems. siam journal

on imaging sciences, 2(1):183   202.

bioucas-dias, j. and figueiredo, m. (2007). a new twist: two-step iterativeshrinkage/thresholding algorithms for image

restoration. ieee transactions on image processing, 16:2992   3004.

blitzer, j., dredze, m., and pereira, f. (2007). biographies, bollywood, boom-boxes and blenders: id20 for

sentiment classi   cation. in proc. of acl.

bottou, l. and bousquet, o. (2007). the tradeo   s of large scale learning. nips, 20.

boyd, s., parikh, n., chu, e., peleato, b., and eckstein, j. (2010). distributed optimization and statistical learning via the

alternating direction method of multipliers. foundations and trends in machine learning, 3(1):1   122.

brown, p. f., desouza, p. v., mercer, r. l., pietra, v. j. d., and lai, j. c. (1992). class-based id165 models of natural

language. computational linguistics, 18(4):467   479.

buchholz, s. and marsi, e. (2006). conll-x shared task on multilingual id33. in proc. of conll.

cand`es, e., romberg, j., and tao, t. (2006). robust uncertainty principles: exact signal reconstruction from highly incomplete

frequency information. ieee transactions on id205, 52:489   509.

caruana, r. (1997). multitask learning. machine learning, 28(1):41   75.

cessie, s. l. and houwelingen, j. c. v. (1992). ridge estimators in id28. journal of the royal statistical society;

series c, 41:191   201.

chen, s. and rosenfeld, r. (1999). a gaussian prior for smoothing maximum id178 models. technical report,

cmu-cs-99-108.

claerbout, j. and muir, f. (1973). robust modelling of erratic data. geophysics, 38:826   844.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

123 / 128

references ii

combettes, p. and wajs, v. (2006). signal recovery by proximal forward-backward splitting. multiscale modeling and

simulation, 4:1168   1200.

das, d. and smith, n. a. (2012). graph-based lexicon expansion with sparsity-inducing penalties. in proceedings of naacl.

daubechies, i., defrise, m., and de mol, c. (2004). an iterative thresholding algorithm for linear inverse problems with a

sparsity constraint. communications on pure and applied mathematics, 11:1413   1457.

davis, g., mallat, s., and avellaneda, m. (1997). greedy adaptive approximation. journal of constructive approximation,

13:57   98.

della pietra, s., della pietra, v., and la   erty, j. (1997). inducing features of random    elds. ieee transactions on pattern

analysis and machine intelligence, 19:380   393.

donoho, d. (2006). compressed sensing. ieee transactions on id205, 52:1289   1306.

duchi, j. and singer, y. (2009). e   cient online and batch learning using forward backward splitting. jmlr, 10:2873   2908.

duh, k., sudoh, k., tsukada, h., isozaki, h., and nagata, m. (2010). n-best reranking by multitask learning. in proceedings of

the joint fifth workshop on id151 and metrics.

efron, b., hastie, t., johnstone, i., and tibshirani, r. (2004). least angle regression. annals of statistics, 32:407   499.

eisenstein, j., smith, n. a., and xing, e. p. (2011). discovering sociolinguistic associations with structured sparsity. in proc. of

acl.

figueiredo, m. and bioucas-dias, j. (2011). an alternating direction algorithm for (overlapping) group id173. in signal

processing with adaptive sparse structured representations   spars11. edinburgh, uk.

figueiredo, m. and nowak, r. (2003). an em algorithm for wavelet-based image restoration. ieee transactions on image

processing, 12:986   916.

figueiredo, m., nowak, r., and wright, s. (2007). gradient projection for sparse reconstruction: application to compressed

sensing and other inverse problems. ieee journal of selected topics in signal processing: special issue on convex
optimization methods for signal processing, 1:586   598.

friedman, j., hastie, t., rosset, s., tibshirani, r., and zhu, j. (2004). discussion of three boosting papers. annals of

statistics, 32(1):102   107.

friedman, j., hastie, t., and tibshirani, r. (2010). a note on the group lasso and a sparse group lasso. technical report,

stanford university.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

124 / 128

references iii

fu, w. (1998). penalized regressions: the bridge versus the lasso. journal of computational and graphical statistics, pages

397   416.

goodman, j. (2004). exponential priors for maximum id178 models. in proc. of naacl.

gra  ca, j., ganchev, k., taskar, b., and pereira, f. (2009). posterior vs. parameter sparsity in latent variable models. advances

in neural information processing systems.

guyon, i. and elissee   , a. (2003). an introduction to variable and feature selection. journal of machine learning research,

3:1157   1182.

hale, e., yin, w., and zhang, y. (2008). fixed-point continuation for l1-minimization: methodology and convergence. siam

journal on optimization, 19:1107   1130.

hastie, t., taylor, j., tibshirani, r., and walther, g. (2007). forward stagewise regression and the monotone lasso. electronic

journal of statistics, 1:1   29.

hestenes, m. r. (1969). multiplier and gradient methods. journal of optimization theory and applications, 4:303   320.

jenatton, r., audibert, j.-y., and bach, f. (2009). structured variable selection with sparsity-inducing norms. technical report,

arxiv:0904.3523.

jenatton, r., mairal, j., obozinski, g., and bach, f. (2011). proximal methods for hierarchical sparse coding. journal of

machine learning research, 12:2297   2334.

kazama, j. and tsujii, j. (2003). evaluation and extension of maximum id178 models with inequality constraints. in proc. of

emnlp.

kim, s. and xing, e. (2010). tree-guided group lasso for multi-task regression with structured sparsity. in proc. of icml.

kowalski, m. and torr  esani, b. (2009). sparsity and persistence: mixed norms provide simple signal models with dependent

coe   cients. signal, image and video processing, 3(3):251   264.

lanckriet, g. r. g., cristianini, n., bartlett, p., ghaoui, l. e., and jordan, m. i. (2004). learning the kernel matrix with

semide   nite programming. jmlr, 5:27   72.

langford, j., li, l., and zhang, t. (2009). sparse online learning via truncated gradient. jmlr, 10:777   801.

liu, h., palatucci, m., and zhang, j. (2009). blockwise coordinate descent procedures for the multi-task lasso, with applications
to neural semantic basis discovery. in proceedings of the 26th annual international conference on machine learning, pages
649   656. acm.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

125 / 128

references iv

mairal, j., jenatton, r., obozinski, g., and bach, f. (2010). network    ow algorithms for structured sparsity. in advances in

neural information processing systems.

martins, a. f. t., figueiredo, m. a. t., aguiar, p. m. q., smith, n. a., and xing, e. p. (2011a). online learning of structured

predictors with multiple kernels. in proc. of aistats.

martins, a. f. t., smith, n. a., aguiar, p. m. q., and figueiredo, m. a. t. (2011b). structured sparsity in structured

prediction. in proc. of empirical methods for natural language processing.

martins, a. f. t., smith, n. a., xing, e. p., aguiar, p. m. q., and figueiredo, m. a. t. (2010). turbo parsers: dependency

parsing by approximate variational id136. in proc. of emnlp.

mcdonald, r. t., pereira, f., ribarov, k., and hajic, j. (2005). non-projective id33 using spanning tree

algorithms. in proc. of hlt-emnlp.

muthukrishnan, s. (2005). data streams: algorithms and applications. now publishers, boston, ma.

nelakanti, a., archambeau, c., mairal, j., bach, f., and bouchard, g. (2013). structured penalties for log-linear language

models. in proc. of emnlp.

nesterov, y. (2007). gradient methods for minimizing composite objective function. technical report, core report.

nesterov, y. (2009). primal-dual subgradient methods for convex problems. mathematical programming, 120(1):221   259.

obozinski, g., taskar, b., and jordan, m. (2010). joint covariate selection and joint subspace selection for multiple

classi   cation problems. statistics and computing, 20(2):231   252.

osborne, m., presnell, b., and turlach, b. (2000). a new approach to variable selection in least squares problems. ima journal

of numerical analysis, 20:389   403.

perkins, s., lacker, k., and theiler, j. (2003). grafting: fast, incremental feature selection by id119 in function

space. journal of machine learning research, 3:1333   1356.

powell, m. j. d. (1969). a method for nonlinear constraints in minimization problems. in fletcher, r., editor, optimization,

pages 283   298. academic press.

quattoni, a., carreras, x., collins, m., and darrell, t. (2009). an e   cient projection for l1,    id173. in proc. of icml.
ratnaparkhi, a. (1996). a maximum id178 model for part-of-speech tagging. in proc. of emnlp.

sang, e. (2002). introduction to the conll-2002 shared task: language-independent id39. in proc. of

conll.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

126 / 128

references v

sang, e. and buchholz, s. (2000). introduction to the conll-2000 shared task: chunking. in proceedings of conll-2000 and

lll-2000.

sang, e. and de meulder, f. (2003). introduction to the conll-2003 shared task: language-independent named entity

recognition. in proc. of conll.

schaefer, r., roi, l., and wolfe, r. (1984). a ridge logistic estimator. communications in statistical theory and methods,

13:99   113.

schmidt, m. and murphy, k. (2010). convex structure learning in id148: beyond pairwise potentials. in proc. of

aistats.

shor, n. (1985). minimization methods for non-di   erentiable functions. springer.

stojnic, m., parvaresh, f., and hassibi, b. (2009). on the reconstruction of block-sparse signals with an optimal number of

measurements. signal processing, ieee transactions on, 57(8):3075   3085.

tackstrom, o. and mcdonald, r. (2011). discovering    ne-grained sentiment with latent variable id170 models.

in proc. of ecir.

taylor, h., bank, s., and mccoy, j. (1979). deconvolution with the (cid:96)1 norm. geophysics, 44:39   52.
tibshirani, r. (1996). regression shrinkage and selection via the lasso. journal of the royal statistical society b., pages

267   288.

tikhonov, a. (1943). on the stability of inverse problems. in dokl. akad. nauk sssr, volume 39, pages 195   198.

turlach, b. a., venables, w. n., and wright, s. j. (2005). simultaneous variable selection. technometrics, 47(3):349   363.

wiener, n. (1949). extrapolation, interpolation, and smoothing of stationary time series. wiley, new york.

williams, p. (1995). bayesian id173 and pruning using a laplace prior. neural computation, 7:117   143.

wright, s., nowak, r., and figueiredo, m. (2009). sparse reconstruction by separable approximation. ieee transactions on

signal processing, 57:2479   2493.

xiao, l. (2010). dual averaging methods for regularized stochastic learning and online optimization. journal of machine

learning research, 11:2543   2596.

yarowsky, d. (1995). unsupervised id51 rivaling supervised methods. in proc. of acl.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

127 / 128

references vi

yessenalina, a., yue, y., and cardie, c. (2010). multi-level structured models for document sentiment classi   cation. in proc. of

emnlp.

yogatama, d. and smith, n. a. (2014a). linguistic structured sparsity in text categorization. in proc. of acl.

yogatama, d. and smith, n. a. (2014b). making the most of bag of words: sentence id173 with alternating direction

method of multipliers. in proc. of icml.

yuan, l., liu, j., and ye, j. (2011). e   cient methods for overlapping group lasso. in advances in neural information

processing systems 24, pages 352   360.

yuan, m. and lin, y. (2006). model selection and estimation in regression with grouped variables. journal of the royal

statistical society (b), 68(1):49.

zhao, p., rocha, g., and yu, b. (2009). grouped and hierarchical model selection through composite absolute penalties. annals

of statistics, 37(6a):3468   3497.

zhu, j., lao, n., and xing, e. (2010). grafting-light: fast, incremental feature selection and structure learning of markov

random    elds. in proc. of international conference on knowledge discovery and data mining, pages 303   312.

martins, yogatama, smith, figueiredo (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp14

128 / 128

